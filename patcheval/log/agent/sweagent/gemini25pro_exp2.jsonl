{"cve":"CVE-2021-21384:0708", "fix_patch": "diff --git a/src/unix.js b/src/unix.js\nindex 89fc3bd..0c1a3f1 100644\nBinary files a/src/unix.js and b/src/unix.js differ\n"}
{"cve":"CVE-2018-20834:0708", "fix_patch": "diff --git a/lib/parse.js b/lib/parse.js\nindex e8d007b..2ec1192 100644\n--- a/lib/parse.js\n+++ b/lib/parse.js\n@@ -251,16 +251,14 @@ Parse.prototype._startEntry = function (c) {\n \n   if (onend) entry.on(\"end\", onend)\n \n-  if (entry.type === \"File\" && this._hardLinks[entry.path]) {\n-    ev = \"ignoredEntry\"\n+      if ((entry.type === 'File' || entry.type === 'OldFile' || entry.type === 'ContiguousFile') && this._hardLinks[entry.path]) {\n+    ev = 'ignoredEntry'\n+  } else if (entry.type === 'Link') {\n+    this._hardLinks[entry.path] = entry\n   }\n \n   this._entry = entry\n \n-  if (entry.type === \"Link\") {\n-    this._hardLinks[entry.path] = entry\n-  }\n-\n   var me = this\n \n   entry.on(\"pause\", function () {\n"}
{"cve":"CVE-2021-33420:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex b180286..2974dfa 100644\n--- a/index.js\n+++ b/index.js\n@@ -428,7 +428,12 @@ var builtInTransforms = [\n         },\n \n         fromSerializable: function (val) {\n-            return typeof GLOBAL[val.ctorName] === 'function' ? new GLOBAL[val.ctorName](val.arr) : val.arr;\n+            var ctorName = val.ctorName;\n+\n+            if (TYPED_ARRAY_CTORS.indexOf(ctorName) === -1)\n+                return val.arr;\n+\n+            return typeof GLOBAL[ctorName] === 'function' ? new GLOBAL[ctorName](val.arr) : val.arr;\n         }\n     },\n \n"}
{"cve":"CVE-2020-7764:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 6941dc9..d18fe68 100644\n--- a/index.js\n+++ b/index.js\n@@ -48,10 +48,20 @@ function Router (opts) {\n   }\n \n   this.caseSensitive = opts.caseSensitive === undefined ? true : opts.caseSensitive\n-  this.ignoreTrailingSlash = opts.ignoreTrailingSlash || false\n+    this.ignoreTrailingSlash = opts.ignoreTrailingSlash || false\n   this.maxParamLength = opts.maxParamLength || 100\n   this.allowUnsafeRegex = opts.allowUnsafeRegex || false\n+  this.ignoreAcceptVersion = opts.ignoreAcceptVersion === undefined ? false : opts.ignoreAcceptVersion\n   this.versioning = opts.versioning || acceptVersionStrategy\n+\n+  if (opts.ignoreAcceptVersion === true) {\n+    this.versioning = {\n+      storage: this.versioning.storage,\n+      deriveVersion: (req, ctx) => {\n+        // do nothing\n+      }\n+    }\n+  }\n   this.trees = {}\n   this.routes = []\n }\n"}
{"cve":"CVE-2017-1001004:0708", "fix_patch": "diff --git a/typed-function.js b/typed-function.js\nindex 9b84eea..235a6df 100644\n--- a/typed-function.js\n+++ b/typed-function.js\n@@ -1079,6 +1079,7 @@\n     function _typed(name, signatures) {\n       var refs = new Refs();\n \n+\n       // parse signatures, expand them\n       var _signatures = parseSignatures(signatures);\n       if (_signatures.length == 0) {\n@@ -1097,7 +1098,7 @@\n \n       // generate code for the typed function\n       var code = [];\n-      var _name = name || '';\n+      var _name = (name && /^[a-zA-Z_$][a-zA-Z0-9_$]*$/.test(name)) ? name : '';\n       var _args = getArgs(maxParams(_signatures));\n       code.push('function ' + _name + '(' + _args.join(', ') + ') {');\n       code.push('  \"use strict\";');\n"}
{"cve":"CVE-2017-1000189:0708", "fix_patch": "diff --git a/lib/ejs.js b/lib/ejs.js\nindex 98e5991c..9f5fb994 100644\n--- a/lib/ejs.js\n+++ b/lib/ejs.js\n@@ -53,9 +53,7 @@ var _VERSION_STRING = require('../package.json').version;\n var _DEFAULT_DELIMITER = '%';\n var _DEFAULT_LOCALS_NAME = 'locals';\n var _REGEX_STRING = '(<%%|%%>|<%=|<%-|<%_|<%#|<%|%>|-%>|_%>)';\n-var _OPTS = [ 'cache', 'filename', 'delimiter', 'scope', 'context',\n-        'debug', 'compileDebug', 'client', '_with', 'root', 'rmWhitespace',\n-        'strict', 'localsName'];\n+var _OPTS = [ 'cache', 'delimiter', 'rmWhitespace', 'strict', 'localsName'];\n var _BOM = /^\\uFEFF/;\n \n /**\n@@ -253,18 +251,6 @@ function rethrow(err, str, filename, lineno){\n   throw err;\n }\n \n-/**\n- * Copy properties in data object that are recognized as options to an\n- * options object.\n- *\n- * This is used for compatibility with earlier versions of EJS and Express.js.\n- *\n- * @memberof module:ejs-internal\n- * @param {Object}  data data object\n- * @param {Options} opts options object\n- * @static\n- */\n-\n function cpOptsInData(data, opts) {\n   _OPTS.forEach(function (p) {\n     if (typeof data[p] != 'undefined') {\n@@ -272,7 +258,7 @@ function cpOptsInData(data, opts) {\n       // Unsanitized, parameterized use of `render` could allow the\n       // include directory to be reset, opening up the possibility of\n       // remote code execution\n-      if (p == 'root') {\n+      if (p == 'root' || p == 'filename') {\n         return;\n       }\n       opts[p] = data[p];\n@@ -280,9 +266,21 @@ function cpOptsInData(data, opts) {\n   });\n }\n \n-function stripSemi(str) {\n-  return str.replace(/;(\\s*$)/, '$1');\n-}\n+/**\n+ * Copy properties in data object that are recognized as options to an\n+ * options object.\n+ *\n+ * This is used for compatibility with earlier versions of EJS and Express.js.\n+ *\n+ * @memberof module:ejs-internal\n+ * @param {Object}  data data object\n+ * @param {Options} opts options object\n+ * @static\n+ */\n+\n+\n+\n+\n \n /**\n  * Compile the given `str` of ejs into a template function.\n@@ -332,13 +330,6 @@ exports.compile = function compile(template, opts) {\n exports.render = function (template, d, o) {\n   var data = d || {};\n   var opts = o || {};\n-\n-  // No options object -- if there are optiony names\n-  // in the data, copy them to options\n-  if (arguments.length == 2) {\n-    cpOptsInData(data, opts);\n-  }\n-\n   return handleCache(opts, template)(data);\n };\n \n@@ -365,19 +356,6 @@ exports.renderFile = function () {\n \n   // Don't pollute passed in opts obj with new vals\n   opts = utils.shallowCopy({}, opts);\n-\n-  // No options object -- if there are optiony names\n-  // in the data, copy them to options\n-  if (arguments.length == 3) {\n-    // Express 4\n-    if (data.settings && data.settings['view options']) {\n-      cpOptsInData(data.settings['view options'], opts);\n-    }\n-    // Express 3 and lower\n-    else {\n-      cpOptsInData(data, opts);\n-    }\n-  }\n   opts.filename = filename;\n \n   try {\n"}
{"cve":"CVE-2017-1001003:0708", "fix_patch": "diff --git a/lib/expression/node/ObjectNode.js b/lib/expression/node/ObjectNode.js\nindex 22681adae..ae6982d28 100644\n--- a/lib/expression/node/ObjectNode.js\n+++ b/lib/expression/node/ObjectNode.js\n@@ -52,6 +52,7 @@ function factory (type, config, load, typed) {\n    * @return {string} code\n    * @private\n    */\n+  \n   function compileObjectNode(node, defs, args) {\n     if (!(node instanceof ObjectNode)) {\n       throw new TypeError('No valid ObjectNode')\ndiff --git a/lib/utils/customs.js b/lib/utils/customs.js\nindex 782058cdb..76b1f77cd 100644\n--- a/lib/utils/customs.js\n+++ b/lib/utils/customs.js\n@@ -49,6 +49,7 @@ function setSafeProperty (object, prop, value) {\n  * @return {boolean} Returns true when safe\n  */\n function isSafeProperty (object, prop) {\n+  prop = String(prop).normalize('NFKC');\n   if (!object || typeof object !== 'object') {\n     return false;\n   }\n@@ -97,7 +98,6 @@ function validateSafeMethod (object, method) {\n  * @return {boolean} Returns true when safe, false otherwise\n  */\n function isSafeMethod (object, method) {\n-  if (!object || typeof object[method] !== 'function') {\n     return false;\n   }\n   // UNSAFE: ghosted\n"}
{"cve":"CVE-2016-1000232:0708", "fix_patch": "diff --git a/lib/cookie.js b/lib/cookie.js\nindex 12da297..91cc7e7 100644\n--- a/lib/cookie.js\n+++ b/lib/cookie.js\n@@ -378,7 +378,25 @@ function parse(str, options) {\n    * cookie-attribute-list\".  Therefore, in this implementation, we overwrite\n    * the previous value.\n    */\n-  var cookie_avs = unparsed.split(/\\s*;\\s*/);\n+   var cookie_avs = [];\n+  var i = 0;\n+  var limit = 2048;\n+  while (i < unparsed.length && cookie_avs.length < limit) {\n+    var j = unparsed.indexOf(';', i);\n+    if (j === -1) {\n+      j = unparsed.length;\n+    }\n+    var av = unparsed.slice(i, j);\n+    i = j + 1;\n+\n+    // Remove leading and trailing whitespace\n+    av = av.trim();\n+\n+    // Some browsers really do send empty strings for attributes...\n+    if (av) {\n+      cookie_avs.push(av);\n+    }\n+  }\n   while (cookie_avs.length) {\n     var av = cookie_avs.shift();\n     var av_sep = av.indexOf('=');\n"}
{"cve":"CVE-2021-21321:0708", "fix_patch": "diff --git a/lib/utils.js b/lib/utils.js\nindex 48572cd..301cd26 100644\n--- a/lib/utils.js\n+++ b/lib/utils.js\n@@ -1,3 +1,5 @@\n+const path = require('path')\n+\n function filterPseudoHeaders (headers) {\n   const dest = {}\n   const headersKeys = Object.keys(headers)\n@@ -59,8 +61,16 @@ function buildURL (source, reqBase) {\n   const dest = new URL(source, reqBase)\n \n   // if base is specified, source url should not override it\n-  if (reqBase && !reqBase.startsWith(dest.origin)) {\n-    throw new Error('source must be a relative path string')\n+  if (reqBase) {\n+    const reqBaseObject = new URL(reqBase)\n+    if (reqBaseObject.origin !== dest.origin) {\n+      throw new Error('source must be a relative path string')\n+    }\n+    const base = path.resolve(reqBaseObject.pathname)\n+    const destPath = path.resolve(dest.pathname)\n+    if (destPath !== base && !destPath.startsWith(base)) {\n+      throw new Error('source must be a relative path string')\n+    }\n   }\n \n   return dest\n"}
{"cve":"CVE-2020-17479:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex c710e6a..bf01564 100644\n--- a/index.js\n+++ b/index.js\n@@ -268,6 +268,187 @@ const compare = (value, pattern, options) => {\n         }\n     }\n \n+    if (Array.isArray(pattern)) {\n+        if (!Array.isArray(value)) return res(false);\n+        if (pattern.length !== value.length) return res(false);\n+\n+        for (let i = 0; i < pattern.length; i++) {\n+            if (!compare(value[i], pattern[i], options)) return res(false);\n+        }\n+\n+        return res(true);\n+    }\n+\n+    // pattern = object\n+    if (typeof pattern === 'object') {\n+        if (value !== null) {\n+            return res(value.constructor === pattern.constructor);\n+        }\n+        return res(value === pattern);\n+    }\n+\n+    // pattern is a function\n+    if (typeof pattern === 'function') {\n+        return res(!!pattern(value));\n+    }\n+\n+    throw new Error('invalid data type');\n+};\n+const compare = (value, pattern, options) => {\n+    /*\n+    * Special for debugging\n+    * */\n+    const res = (result) => {\n+        let val = '';\n+        if (!pattern || ((typeof pattern !== 'object') && (typeof pattern !== 'string') && typeof pattern !== 'function')) {\n+            val = String(pattern)\n+        } else if (pattern.constructor === JpvObject) {\n+            val = `operator \"${pattern.type}\": ${JSON.stringify(pattern.value)}`;\n+        } else {\n+            JSON.stringify(pattern)\n+        }\n+\n+\n+        if (typeof pattern === 'function') {\n+            val = pattern.toString();\n+        }\n+        if (!result && options && options.debug) {\n+            options.logger(`error - the value of: {${options.deepLog.join('.')}: ` +\n+            `${String(value)}} not matched with: ${val}`);\n+        }\n+        return result;\n+    };\n+\n+    // simple types pattern = number | boolean | symbol | bigint\n+    if ((typeof pattern === 'number') || (typeof pattern === 'symbol') || (typeof pattern === 'boolean') ||\n+        (typeof pattern === 'bigint') || (typeof pattern === 'undefined') || (pattern === null)) {\n+        return res(pattern === value);\n+    }\n+\n+    /*\n+    * When pattern is regex\n+    */\n+    if (pattern instanceof RegExp) {\n+        return res(String(value).match(pattern));\n+    }\n+\n+    // String\n+    if ((typeof pattern === 'string')) {\n+        // Native Types\n+        let nativeMatches = pattern.match(/^(!)?\\((.*)\\)(\\?)?$/i);\n+        if (nativeMatches !== null) {\n+            // eslint-disable-next-line valid-typeof\n+            let match = (typeof value === nativeMatches[2]);\n+\n+            // ------------------------> Deprecated\n+            // Negation ? Operator\n+            if (typeof nativeMatches[3] !== 'undefined') {\n+                depricated('neg');\n+                if (value === null || typeof value === 'undefined' || value === '') {\n+                    return true;\n+                }\n+            }\n+            if (nativeMatches[1] === '!') {\n+                depricated('neg');\n+                return res(!match);\n+            }\n+            // <-------------------------\n+\n+            return res(match);\n+        }\n+\n+        // Patterns\n+        let logicalMatches = pattern.match(/^(!)?\\[(.*)\\](\\?)?$/i);\n+        if (logicalMatches !== null) {\n+            const valid = comparePattern(value, logicalMatches[2]);\n+\n+            // ------------------------> Deprecated\n+            // ? Operator\n+            if (typeof logicalMatches[3] !== 'undefined') {\n+                depricated('neg');\n+                if (value === null || typeof value === 'undefined' || value === '') {\n+                    return true;\n+                }\n+            }\n+            // ! Operator\n+            if (typeof logicalMatches[1] !== 'undefined') {\n+                depricated('neg');\n+                return res(!valid);\n+            }\n+            // <-------------------------\n+\n+            return res(valid);\n+        }\n+\n+        // ------------------------> Deprecated\n+        // Functional Regex\n+        let functionalRegexMatches = pattern.match(/^(?!=^|,)(!)?\\{\\/(.*)\\/([a-z]*)\\}(\\?)?$/i);\n+        if (functionalRegexMatches !== null) {\n+            depricated('tag');\n+            let match = (String(value).match(new RegExp(functionalRegexMatches[2], functionalRegexMatches[3])) !== null);\n+            // Negation ? Operator\n+            if (typeof functionalRegexMatches[4] !== 'undefined') {\n+                if (value === null || typeof value === 'undefined' || value === '') {\n+                    return true;\n+                }\n+            }\n+            return res(functionalRegexMatches[1] === '!' ? !match : match);\n+        }\n+\n+        // Functional Fixed\n+        let functionalFixedMatches = pattern.match(/^(!)?\\{(.*)\\}(\\?)?$/i);\n+        if (functionalFixedMatches !== null) {\n+            depricated('tag');\n+            let match = (String(value) === String(functionalFixedMatches[2]));\n+            // Negation ? Operator\n+            if (typeof functionalFixedMatches[3] !== 'undefined') {\n+                if (value === null || typeof value === 'undefined' || value === '') {\n+                    return true;\n+                }\n+            }\n+            return res(functionalFixedMatches[1] === '!' ? !match : match);\n+        }\n+        // <-------------------------\n+\n+        // Fixed String Comparition\n+        return res(value === pattern);\n+    }\n+\n+    // Constructor is JpvObject\n+    if (typeof pattern === 'object' && pattern.constructor === JpvObject) {\n+        if (pattern.type === 'not') {\n+            return res(!compare(value, pattern.value, options));\n+        }\n+        if (pattern.type === 'and') {\n+            for (let i = 0; i < pattern.value.length; i++) {\n+                if (!compare(value, pattern.value[i])) {\n+                    return res(false);\n+                }\n+            }\n+            return true;\n+        }\n+        if (pattern.type === 'or') {\n+            for (let i = 0; i < pattern.value.length; i++) {\n+                if (compare(value, pattern.value[i])) {\n+                    return true;\n+                }\n+            }\n+            return res(false);\n+        }\n+        if (pattern.type === 'exact') {\n+            return res(value === pattern.value);\n+        }\n+\n+        if (pattern.type === 'typeOf') {\n+            // eslint-disable-next-line valid-typeof\n+            return res(typeof value === pattern.value);\n+        }\n+\n+        if (pattern.type === 'is') {\n+            return res(comparePattern(value, pattern.value));\n+        }\n+    }\n+\n     // pattern = object\n     if (typeof pattern === 'object') {\n         if (value !== null) {\n@@ -411,17 +592,21 @@ const iterate = (value, pattern, valid, cb, options) => {\n                 /*\n                 * iterate if pattern is an Array\n                 * */\n-                if ((pattern[property].constructor === Array) && (pattern[property].length > 0)) {\n+                if (pattern[property].constructor === Array) {\n                     if (value[property].constructor !== Array) {\n                         return (valid = cb(value[property], pattern[property], options));\n                     }\n+\n+                    if (pattern[property].length === 0) {\n+                        return (valid = cb(value[property], pattern[property], options));\n+                    }\n+\n                     for (let i = 0; i < value[property].length; i++) {\n                         const levelArray = push(options, i, Array);\n                         valid = iterate(value[property][i], pattern[property][0], valid, cb, options);\n                         pull(options, levelArray);\n                         if (!valid) break;\n                     }\n-\n                     return valid;\n                 }\n \n"}
{"cve":"CVE-2019-10792:0708", "fix_patch": "diff --git a/src/index.js b/src/index.js\nindex c57cc60..bce518b 100644\n--- a/src/index.js\n+++ b/src/index.js\n@@ -18,6 +18,7 @@ export const handlers = {\n  * @param {Function} [fn] - Set the handler method.\n  */\n export function handler (type, name, fn) {\n+  if (type === '__proto__') return\n   if (arguments.length > 2) {\n     handlers[type][name] = fn\n   }\n"}
{"cve":"CVE-2019-10795:0708", "fix_patch": "diff --git a/lib/undefsafe.js b/lib/undefsafe.js\nindex 60663b0..d171582 100644\n--- a/lib/undefsafe.js\n+++ b/lib/undefsafe.js\n@@ -67,6 +67,9 @@ function undefsafe(obj, path, value, __res) {\n   var i = 0;\n   for (; i < parts.length; i++) {\n     key = parts[i];\n+ if (key === '__proto__' || key === 'constructor' || key === 'prototype') {\n+      continue;\n+    }\n     parent = obj;\n \n     if (key === '*') {\n"}
{"cve":"CVE-2021-32796:0708", "fix_patch": "diff --git a/lib/dom.js b/lib/dom.js\nindex de9f606..77cc1e4 100644\n--- a/lib/dom.js\n+++ b/lib/dom.js\n@@ -8,7 +8,7 @@ var NAMESPACE = conventions.NAMESPACE;\n  * @returns {boolean}\n  */\n function notEmptyString (input) {\n-\treturn input !== ''\n+        return input !== ''\n }\n /**\n  * @see https://infra.spec.whatwg.org/#split-on-ascii-whitespace\n@@ -18,8 +18,8 @@ function notEmptyString (input) {\n  * @returns {string[]} (can be empty)\n  */\n function splitOnASCIIWhitespace(input) {\n-\t// U+0009 TAB, U+000A LF, U+000C FF, U+000D CR, U+0020 SPACE\n-\treturn input ? input.split(/[\\t\\n\\f\\r ]+/).filter(notEmptyString) : []\n+        // U+0009 TAB, U+000A LF, U+000C FF, U+000D CR, U+0020 SPACE\n+        return input ? input.split(/[\\t\\n\\f\\r ]+/).filter(notEmptyString) : []\n }\n \n /**\n@@ -30,10 +30,10 @@ function splitOnASCIIWhitespace(input) {\n  * @returns {Record<string, boolean | undefined>}\n  */\n function orderedSetReducer (current, element) {\n-\tif (!current.hasOwnProperty(element)) {\n-\t\tcurrent[element] = true;\n-\t}\n-\treturn current;\n+        if (!current.hasOwnProperty(element)) {\n+                current[element] = true;\n+        }\n+        return current;\n }\n \n /**\n@@ -42,9 +42,9 @@ function orderedSetReducer (current, element) {\n  * @returns {string[]}\n  */\n function toOrderedSet(input) {\n-\tif (!input) return [];\n-\tvar list = splitOnASCIIWhitespace(input);\n-\treturn Object.keys(list.reduce(orderedSetReducer, {}))\n+        if (!input) return [];\n+        var list = splitOnASCIIWhitespace(input);\n+        return Object.keys(list.reduce(orderedSetReducer, {}))\n }\n \n /**\n@@ -55,15 +55,15 @@ function toOrderedSet(input) {\n  * @returns {function(any): boolean}\n  */\n function arrayIncludes (list) {\n-\treturn function(element) {\n-\t\treturn list && list.indexOf(element) !== -1;\n-\t}\n+        return function(element) {\n+                return list && list.indexOf(element) !== -1;\n+        }\n }\n \n function copy(src,dest){\n-\tfor(var p in src){\n-\t\tdest[p] = src[p];\n-\t}\n+        for(var p in src){\n+                dest[p] = src[p];\n+        }\n }\n \n /**\n@@ -71,20 +71,20 @@ function copy(src,dest){\n ^\\w+\\.prototype\\.([_\\w]+)\\s*=\\s*(\\S.*?(?=[;\\r\\n]));?\n  */\n function _extends(Class,Super){\n-\tvar pt = Class.prototype;\n-\tif(!(pt instanceof Super)){\n-\t\tfunction t(){};\n-\t\tt.prototype = Super.prototype;\n-\t\tt = new t();\n-\t\tcopy(pt,t);\n-\t\tClass.prototype = pt = t;\n-\t}\n-\tif(pt.constructor != Class){\n-\t\tif(typeof Class != 'function'){\n-\t\t\tconsole.error(\"unknow Class:\"+Class)\n-\t\t}\n-\t\tpt.constructor = Class\n-\t}\n+        var pt = Class.prototype;\n+        if(!(pt instanceof Super)){\n+                function t(){};\n+                t.prototype = Super.prototype;\n+                t = new t();\n+                copy(pt,t);\n+                Class.prototype = pt = t;\n+        }\n+        if(pt.constructor != Class){\n+                if(typeof Class != 'function'){\n+                        console.error(\"unknow Class:\"+Class)\n+                }\n+                pt.constructor = Class\n+        }\n }\n \n // Node Types\n@@ -116,11 +116,11 @@ var NOT_FOUND_ERR               = ExceptionCode.NOT_FOUND_ERR               = ((\n var NOT_SUPPORTED_ERR           = ExceptionCode.NOT_SUPPORTED_ERR           = ((ExceptionMessage[9]=\"Not supported\"),9);\n var INUSE_ATTRIBUTE_ERR         = ExceptionCode.INUSE_ATTRIBUTE_ERR         = ((ExceptionMessage[10]=\"Attribute in use\"),10);\n //level2\n-var INVALID_STATE_ERR        \t= ExceptionCode.INVALID_STATE_ERR        \t= ((ExceptionMessage[11]=\"Invalid state\"),11);\n-var SYNTAX_ERR               \t= ExceptionCode.SYNTAX_ERR               \t= ((ExceptionMessage[12]=\"Syntax error\"),12);\n-var INVALID_MODIFICATION_ERR \t= ExceptionCode.INVALID_MODIFICATION_ERR \t= ((ExceptionMessage[13]=\"Invalid modification\"),13);\n-var NAMESPACE_ERR            \t= ExceptionCode.NAMESPACE_ERR           \t= ((ExceptionMessage[14]=\"Invalid namespace\"),14);\n-var INVALID_ACCESS_ERR       \t= ExceptionCode.INVALID_ACCESS_ERR      \t= ((ExceptionMessage[15]=\"Invalid access\"),15);\n+var INVALID_STATE_ERR           = ExceptionCode.INVALID_STATE_ERR               = ((ExceptionMessage[11]=\"Invalid state\"),11);\n+var SYNTAX_ERR                  = ExceptionCode.SYNTAX_ERR                      = ((ExceptionMessage[12]=\"Syntax error\"),12);\n+var INVALID_MODIFICATION_ERR    = ExceptionCode.INVALID_MODIFICATION_ERR        = ((ExceptionMessage[13]=\"Invalid modification\"),13);\n+var NAMESPACE_ERR               = ExceptionCode.NAMESPACE_ERR                   = ((ExceptionMessage[14]=\"Invalid namespace\"),14);\n+var INVALID_ACCESS_ERR          = ExceptionCode.INVALID_ACCESS_ERR              = ((ExceptionMessage[15]=\"Invalid access\"),15);\n \n /**\n  * DOM Level 2\n@@ -129,17 +129,17 @@ var INVALID_ACCESS_ERR       \t= ExceptionCode.INVALID_ACCESS_ERR      \t= ((Excep\n  * @see http://www.w3.org/TR/REC-DOM-Level-1/ecma-script-language-binding.html\n  */\n function DOMException(code, message) {\n-\tif(message instanceof Error){\n-\t\tvar error = message;\n-\t}else{\n-\t\terror = this;\n-\t\tError.call(this, ExceptionMessage[code]);\n-\t\tthis.message = ExceptionMessage[code];\n-\t\tif(Error.captureStackTrace) Error.captureStackTrace(this, DOMException);\n-\t}\n-\terror.code = code;\n-\tif(message) this.message = this.message + \": \" + message;\n-\treturn error;\n+        if(message instanceof Error){\n+                var error = message;\n+        }else{\n+                error = this;\n+                Error.call(this, ExceptionMessage[code]);\n+                this.message = ExceptionMessage[code];\n+                if(Error.captureStackTrace) Error.captureStackTrace(this, DOMException);\n+        }\n+        error.code = code;\n+        if(message) this.message = this.message + \": \" + message;\n+        return error;\n };\n DOMException.prototype = Error.prototype;\n copy(ExceptionCode,DOMException)\n@@ -152,48 +152,48 @@ copy(ExceptionCode,DOMException)\n function NodeList() {\n };\n NodeList.prototype = {\n-\t/**\n-\t * The number of nodes in the list. The range of valid child node indices is 0 to length-1 inclusive.\n-\t * @standard level1\n-\t */\n-\tlength:0, \n-\t/**\n-\t * Returns the indexth item in the collection. If index is greater than or equal to the number of nodes in the list, this returns null.\n-\t * @standard level1\n-\t * @param index  unsigned long \n-\t *   Index into the collection.\n-\t * @return Node\n-\t * \tThe node at the indexth position in the NodeList, or null if that is not a valid index. \n-\t */\n-\titem: function(index) {\n-\t\treturn this[index] || null;\n-\t},\n-\ttoString:function(isHTML,nodeFilter){\n-\t\tfor(var buf = [], i = 0;i<this.length;i++){\n-\t\t\tserializeToString(this[i],buf,isHTML,nodeFilter);\n-\t\t}\n-\t\treturn buf.join('');\n-\t}\n+        /**\n+         * The number of nodes in the list. The range of valid child node indices is 0 to length-1 inclusive.\n+         * @standard level1\n+         */\n+        length:0, \n+        /**\n+         * Returns the indexth item in the collection. If index is greater than or equal to the number of nodes in the list, this returns null.\n+         * @standard level1\n+         * @param index  unsigned long \n+         *   Index into the collection.\n+         * @return Node\n+         *      The node at the indexth position in the NodeList, or null if that is not a valid index. \n+         */\n+        item: function(index) {\n+                return this[index] || null;\n+        },\n+        toString:function(isHTML,nodeFilter){\n+                for(var buf = [], i = 0;i<this.length;i++){\n+                        serializeToString(this[i],buf,isHTML,nodeFilter);\n+                }\n+                return buf.join('');\n+        }\n };\n \n function LiveNodeList(node,refresh){\n-\tthis._node = node;\n-\tthis._refresh = refresh\n-\t_updateLiveList(this);\n+        this._node = node;\n+        this._refresh = refresh\n+        _updateLiveList(this);\n }\n function _updateLiveList(list){\n-\tvar inc = list._node._inc || list._node.ownerDocument._inc;\n-\tif(list._inc != inc){\n-\t\tvar ls = list._refresh(list._node);\n-\t\t//console.log(ls.length)\n-\t\t__set__(list,'length',ls.length);\n-\t\tcopy(ls,list);\n-\t\tlist._inc = inc;\n-\t}\n+        var inc = list._node._inc || list._node.ownerDocument._inc;\n+        if(list._inc != inc){\n+                var ls = list._refresh(list._node);\n+                //console.log(ls.length)\n+                __set__(list,'length',ls.length);\n+                copy(ls,list);\n+                list._inc = inc;\n+        }\n }\n LiveNodeList.prototype.item = function(i){\n-\t_updateLiveList(this);\n-\treturn this[i];\n+        _updateLiveList(this);\n+        return this[i];\n }\n \n _extends(LiveNodeList,NodeList);\n@@ -213,109 +213,109 @@ function NamedNodeMap() {\n };\n \n function _findNodeIndex(list,node){\n-\tvar i = list.length;\n-\twhile(i--){\n-\t\tif(list[i] === node){return i}\n-\t}\n+        var i = list.length;\n+        while(i--){\n+                if(list[i] === node){return i}\n+        }\n }\n \n function _addNamedNode(el,list,newAttr,oldAttr){\n-\tif(oldAttr){\n-\t\tlist[_findNodeIndex(list,oldAttr)] = newAttr;\n-\t}else{\n-\t\tlist[list.length++] = newAttr;\n-\t}\n-\tif(el){\n-\t\tnewAttr.ownerElement = el;\n-\t\tvar doc = el.ownerDocument;\n-\t\tif(doc){\n-\t\t\toldAttr && _onRemoveAttribute(doc,el,oldAttr);\n-\t\t\t_onAddAttribute(doc,el,newAttr);\n-\t\t}\n-\t}\n+        if(oldAttr){\n+                list[_findNodeIndex(list,oldAttr)] = newAttr;\n+        }else{\n+                list[list.length++] = newAttr;\n+        }\n+        if(el){\n+                newAttr.ownerElement = el;\n+                var doc = el.ownerDocument;\n+                if(doc){\n+                        oldAttr && _onRemoveAttribute(doc,el,oldAttr);\n+                        _onAddAttribute(doc,el,newAttr);\n+                }\n+        }\n }\n function _removeNamedNode(el,list,attr){\n-\t//console.log('remove attr:'+attr)\n-\tvar i = _findNodeIndex(list,attr);\n-\tif(i>=0){\n-\t\tvar lastIndex = list.length-1\n-\t\twhile(i<lastIndex){\n-\t\t\tlist[i] = list[++i]\n-\t\t}\n-\t\tlist.length = lastIndex;\n-\t\tif(el){\n-\t\t\tvar doc = el.ownerDocument;\n-\t\t\tif(doc){\n-\t\t\t\t_onRemoveAttribute(doc,el,attr);\n-\t\t\t\tattr.ownerElement = null;\n-\t\t\t}\n-\t\t}\n-\t}else{\n-\t\tthrow DOMException(NOT_FOUND_ERR,new Error(el.tagName+'@'+attr))\n-\t}\n+        //console.log('remove attr:'+attr)\n+        var i = _findNodeIndex(list,attr);\n+        if(i>=0){\n+                var lastIndex = list.length-1\n+                while(i<lastIndex){\n+                        list[i] = list[++i]\n+                }\n+                list.length = lastIndex;\n+                if(el){\n+                        var doc = el.ownerDocument;\n+                        if(doc){\n+                                _onRemoveAttribute(doc,el,attr);\n+                                attr.ownerElement = null;\n+                        }\n+                }\n+        }else{\n+                throw DOMException(NOT_FOUND_ERR,new Error(el.tagName+'@'+attr))\n+        }\n }\n NamedNodeMap.prototype = {\n-\tlength:0,\n-\titem:NodeList.prototype.item,\n-\tgetNamedItem: function(key) {\n-//\t\tif(key.indexOf(':')>0 || key == 'xmlns'){\n-//\t\t\treturn null;\n-//\t\t}\n-\t\t//console.log()\n-\t\tvar i = this.length;\n-\t\twhile(i--){\n-\t\t\tvar attr = this[i];\n-\t\t\t//console.log(attr.nodeName,key)\n-\t\t\tif(attr.nodeName == key){\n-\t\t\t\treturn attr;\n-\t\t\t}\n-\t\t}\n-\t},\n-\tsetNamedItem: function(attr) {\n-\t\tvar el = attr.ownerElement;\n-\t\tif(el && el!=this._ownerElement){\n-\t\t\tthrow new DOMException(INUSE_ATTRIBUTE_ERR);\n-\t\t}\n-\t\tvar oldAttr = this.getNamedItem(attr.nodeName);\n-\t\t_addNamedNode(this._ownerElement,this,attr,oldAttr);\n-\t\treturn oldAttr;\n-\t},\n-\t/* returns Node */\n-\tsetNamedItemNS: function(attr) {// raises: WRONG_DOCUMENT_ERR,NO_MODIFICATION_ALLOWED_ERR,INUSE_ATTRIBUTE_ERR\n-\t\tvar el = attr.ownerElement, oldAttr;\n-\t\tif(el && el!=this._ownerElement){\n-\t\t\tthrow new DOMException(INUSE_ATTRIBUTE_ERR);\n-\t\t}\n-\t\toldAttr = this.getNamedItemNS(attr.namespaceURI,attr.localName);\n-\t\t_addNamedNode(this._ownerElement,this,attr,oldAttr);\n-\t\treturn oldAttr;\n-\t},\n-\n-\t/* returns Node */\n-\tremoveNamedItem: function(key) {\n-\t\tvar attr = this.getNamedItem(key);\n-\t\t_removeNamedNode(this._ownerElement,this,attr);\n-\t\treturn attr;\n-\t\t\n-\t\t\n-\t},// raises: NOT_FOUND_ERR,NO_MODIFICATION_ALLOWED_ERR\n-\t\n-\t//for level2\n-\tremoveNamedItemNS:function(namespaceURI,localName){\n-\t\tvar attr = this.getNamedItemNS(namespaceURI,localName);\n-\t\t_removeNamedNode(this._ownerElement,this,attr);\n-\t\treturn attr;\n-\t},\n-\tgetNamedItemNS: function(namespaceURI, localName) {\n-\t\tvar i = this.length;\n-\t\twhile(i--){\n-\t\t\tvar node = this[i];\n-\t\t\tif(node.localName == localName && node.namespaceURI == namespaceURI){\n-\t\t\t\treturn node;\n-\t\t\t}\n-\t\t}\n-\t\treturn null;\n-\t}\n+        length:0,\n+        item:NodeList.prototype.item,\n+        getNamedItem: function(key) {\n+//              if(key.indexOf(':')>0 || key == 'xmlns'){\n+//                      return null;\n+//              }\n+                //console.log()\n+                var i = this.length;\n+                while(i--){\n+                        var attr = this[i];\n+                        //console.log(attr.nodeName,key)\n+                        if(attr.nodeName == key){\n+                                return attr;\n+                        }\n+                }\n+        },\n+        setNamedItem: function(attr) {\n+                var el = attr.ownerElement;\n+                if(el && el!=this._ownerElement){\n+                        throw new DOMException(INUSE_ATTRIBUTE_ERR);\n+                }\n+                var oldAttr = this.getNamedItem(attr.nodeName);\n+                _addNamedNode(this._ownerElement,this,attr,oldAttr);\n+                return oldAttr;\n+        },\n+        /* returns Node */\n+        setNamedItemNS: function(attr) {// raises: WRONG_DOCUMENT_ERR,NO_MODIFICATION_ALLOWED_ERR,INUSE_ATTRIBUTE_ERR\n+                var el = attr.ownerElement, oldAttr;\n+                if(el && el!=this._ownerElement){\n+                        throw new DOMException(INUSE_ATTRIBUTE_ERR);\n+                }\n+                oldAttr = this.getNamedItemNS(attr.namespaceURI,attr.localName);\n+                _addNamedNode(this._ownerElement,this,attr,oldAttr);\n+                return oldAttr;\n+        },\n+\n+        /* returns Node */\n+        removeNamedItem: function(key) {\n+                var attr = this.getNamedItem(key);\n+                _removeNamedNode(this._ownerElement,this,attr);\n+                return attr;\n+                \n+                \n+        },// raises: NOT_FOUND_ERR,NO_MODIFICATION_ALLOWED_ERR\n+        \n+        //for level2\n+        removeNamedItemNS:function(namespaceURI,localName){\n+                var attr = this.getNamedItemNS(namespaceURI,localName);\n+                _removeNamedNode(this._ownerElement,this,attr);\n+                return attr;\n+        },\n+        getNamedItemNS: function(namespaceURI, localName) {\n+                var i = this.length;\n+                while(i--){\n+                        var node = this[i];\n+                        if(node.localName == localName && node.namespaceURI == namespaceURI){\n+                                return node;\n+                        }\n+                }\n+                return null;\n+        }\n };\n \n /**\n@@ -337,90 +337,90 @@ function DOMImplementation() {\n }\n \n DOMImplementation.prototype = {\n-\t/**\n-\t * The DOMImplementation.hasFeature() method returns a Boolean flag indicating if a given feature is supported.\n-\t * The different implementations fairly diverged in what kind of features were reported.\n-\t * The latest version of the spec settled to force this method to always return true, where the functionality was accurate and in use.\n-\t *\n-\t * @deprecated It is deprecated and modern browsers return true in all cases.\n-\t *\n-\t * @param {string} feature\n-\t * @param {string} [version]\n-\t * @returns {boolean} always true\n-\t *\n-\t * @see https://developer.mozilla.org/en-US/docs/Web/API/DOMImplementation/hasFeature MDN\n-\t * @see https://www.w3.org/TR/REC-DOM-Level-1/level-one-core.html#ID-5CED94D7 DOM Level 1 Core\n-\t * @see https://dom.spec.whatwg.org/#dom-domimplementation-hasfeature DOM Living Standard\n-\t */\n-\thasFeature: function(feature, version) {\n-\t\t\treturn true;\n-\t},\n-\t/**\n-\t * Creates an XML Document object of the specified type with its document element.\n-\t *\n-\t * __It behaves slightly different from the description in the living standard__:\n-\t * - There is no interface/class `XMLDocument`, it returns a `Document` instance.\n-\t * - `contentType`, `encoding`, `mode`, `origin`, `url` fields are currently not declared.\n-\t * - this implementation is not validating names or qualified names\n-\t *   (when parsing XML strings, the SAX parser takes care of that)\n-\t *\n-\t * @param {string|null} namespaceURI\n-\t * @param {string} qualifiedName\n-\t * @param {DocumentType=null} doctype\n-\t * @returns {Document}\n-\t *\n-\t * @see https://developer.mozilla.org/en-US/docs/Web/API/DOMImplementation/createDocument MDN\n-\t * @see https://www.w3.org/TR/DOM-Level-2-Core/core.html#Level-2-Core-DOM-createDocument DOM Level 2 Core (initial)\n-\t * @see https://dom.spec.whatwg.org/#dom-domimplementation-createdocument  DOM Level 2 Core\n-\t *\n-\t * @see https://dom.spec.whatwg.org/#validate-and-extract DOM: Validate and extract\n-\t * @see https://www.w3.org/TR/xml/#NT-NameStartChar XML Spec: Names\n-\t * @see https://www.w3.org/TR/xml-names/#ns-qualnames XML Namespaces: Qualified names\n-\t */\n-\tcreateDocument: function(namespaceURI,  qualifiedName, doctype){\n-\t\tvar doc = new Document();\n-\t\tdoc.implementation = this;\n-\t\tdoc.childNodes = new NodeList();\n-\t\tdoc.doctype = doctype || null;\n-\t\tif (doctype){\n-\t\t\tdoc.appendChild(doctype);\n-\t\t}\n-\t\tif (qualifiedName){\n-\t\t\tvar root = doc.createElementNS(namespaceURI, qualifiedName);\n-\t\t\tdoc.appendChild(root);\n-\t\t}\n-\t\treturn doc;\n-\t},\n-\t/**\n-\t * Returns a doctype, with the given `qualifiedName`, `publicId`, and `systemId`.\n-\t *\n-\t * __This behavior is slightly different from the in the specs__:\n-\t * - this implementation is not validating names or qualified names\n-\t *   (when parsing XML strings, the SAX parser takes care of that)\n-\t *\n-\t * @param {string} qualifiedName\n-\t * @param {string} [publicId]\n-\t * @param {string} [systemId]\n-\t * @returns {DocumentType} which can either be used with `DOMImplementation.createDocument` upon document creation\n-\t * \t\t\t\t  or can be put into the document via methods like `Node.insertBefore()` or `Node.replaceChild()`\n-\t *\n-\t * @see https://developer.mozilla.org/en-US/docs/Web/API/DOMImplementation/createDocumentType MDN\n-\t * @see https://www.w3.org/TR/DOM-Level-2-Core/core.html#Level-2-Core-DOM-createDocType DOM Level 2 Core\n-\t * @see https://dom.spec.whatwg.org/#dom-domimplementation-createdocumenttype DOM Living Standard\n-\t *\n-\t * @see https://dom.spec.whatwg.org/#validate-and-extract DOM: Validate and extract\n-\t * @see https://www.w3.org/TR/xml/#NT-NameStartChar XML Spec: Names\n-\t * @see https://www.w3.org/TR/xml-names/#ns-qualnames XML Namespaces: Qualified names\n-\t */\n-\tcreateDocumentType: function(qualifiedName, publicId, systemId){\n-\t\tvar node = new DocumentType();\n-\t\tnode.name = qualifiedName;\n-\t\tnode.nodeName = qualifiedName;\n-\t\tnode.publicId = publicId || '';\n-\t\tnode.systemId = systemId || '';\n-\n-\t\treturn node;\n-\t}\n+        /**\n+         * The DOMImplementation.hasFeature() method returns a Boolean flag indicating if a given feature is supported.\n+         * The different implementations fairly diverged in what kind of features were reported.\n+         * The latest version of the spec settled to force this method to always return true, where the functionality was accurate and in use.\n+         *\n+         * @deprecated It is deprecated and modern browsers return true in all cases.\n+         *\n+         * @param {string} feature\n+         * @param {string} [version]\n+         * @returns {boolean} always true\n+         *\n+         * @see https://developer.mozilla.org/en-US/docs/Web/API/DOMImplementation/hasFeature MDN\n+         * @see https://www.w3.org/TR/REC-DOM-Level-1/level-one-core.html#ID-5CED94D7 DOM Level 1 Core\n+         * @see https://dom.spec.whatwg.org/#dom-domimplementation-hasfeature DOM Living Standard\n+         */\n+        hasFeature: function(feature, version) {\n+                        return true;\n+        },\n+        /**\n+         * Creates an XML Document object of the specified type with its document element.\n+         *\n+         * __It behaves slightly different from the description in the living standard__:\n+         * - There is no interface/class `XMLDocument`, it returns a `Document` instance.\n+         * - `contentType`, `encoding`, `mode`, `origin`, `url` fields are currently not declared.\n+         * - this implementation is not validating names or qualified names\n+         *   (when parsing XML strings, the SAX parser takes care of that)\n+         *\n+         * @param {string|null} namespaceURI\n+         * @param {string} qualifiedName\n+         * @param {DocumentType=null} doctype\n+         * @returns {Document}\n+         *\n+         * @see https://developer.mozilla.org/en-US/docs/Web/API/DOMImplementation/createDocument MDN\n+         * @see https://www.w3.org/TR/DOM-Level-2-Core/core.html#Level-2-Core-DOM-createDocument DOM Level 2 Core (initial)\n+         * @see https://dom.spec.whatwg.org/#dom-domimplementation-createdocument  DOM Level 2 Core\n+         *\n+         * @see https://dom.spec.whatwg.org/#validate-and-extract DOM: Validate and extract\n+         * @see https://www.w3.org/TR/xml/#NT-NameStartChar XML Spec: Names\n+         * @see https://www.w3.org/TR/xml-names/#ns-qualnames XML Namespaces: Qualified names\n+         */\n+        createDocument: function(namespaceURI,  qualifiedName, doctype){\n+                var doc = new Document();\n+                doc.implementation = this;\n+                doc.childNodes = new NodeList();\n+                doc.doctype = doctype || null;\n+                if (doctype){\n+                        doc.appendChild(doctype);\n+                }\n+                if (qualifiedName){\n+                        var root = doc.createElementNS(namespaceURI, qualifiedName);\n+                        doc.appendChild(root);\n+                }\n+                return doc;\n+        },\n+        /**\n+         * Returns a doctype, with the given `qualifiedName`, `publicId`, and `systemId`.\n+         *\n+         * __This behavior is slightly different from the in the specs__:\n+         * - this implementation is not validating names or qualified names\n+         *   (when parsing XML strings, the SAX parser takes care of that)\n+         *\n+         * @param {string} qualifiedName\n+         * @param {string} [publicId]\n+         * @param {string} [systemId]\n+         * @returns {DocumentType} which can either be used with `DOMImplementation.createDocument` upon document creation\n+         *                                or can be put into the document via methods like `Node.insertBefore()` or `Node.replaceChild()`\n+         *\n+         * @see https://developer.mozilla.org/en-US/docs/Web/API/DOMImplementation/createDocumentType MDN\n+         * @see https://www.w3.org/TR/DOM-Level-2-Core/core.html#Level-2-Core-DOM-createDocType DOM Level 2 Core\n+         * @see https://dom.spec.whatwg.org/#dom-domimplementation-createdocumenttype DOM Living Standard\n+         *\n+         * @see https://dom.spec.whatwg.org/#validate-and-extract DOM: Validate and extract\n+         * @see https://www.w3.org/TR/xml/#NT-NameStartChar XML Spec: Names\n+         * @see https://www.w3.org/TR/xml-names/#ns-qualnames XML Namespaces: Qualified names\n+         */\n+        createDocumentType: function(qualifiedName, publicId, systemId){\n+                var node = new DocumentType();\n+                node.name = qualifiedName;\n+                node.nodeName = qualifiedName;\n+                node.publicId = publicId || '';\n+                node.systemId = systemId || '';\n+\n+                return node;\n+        }\n };\n \n \n@@ -432,103 +432,103 @@ function Node() {\n };\n \n Node.prototype = {\n-\tfirstChild : null,\n-\tlastChild : null,\n-\tpreviousSibling : null,\n-\tnextSibling : null,\n-\tattributes : null,\n-\tparentNode : null,\n-\tchildNodes : null,\n-\townerDocument : null,\n-\tnodeValue : null,\n-\tnamespaceURI : null,\n-\tprefix : null,\n-\tlocalName : null,\n-\t// Modified in DOM Level 2:\n-\tinsertBefore:function(newChild, refChild){//raises \n-\t\treturn _insertBefore(this,newChild,refChild);\n-\t},\n-\treplaceChild:function(newChild, oldChild){//raises \n-\t\tthis.insertBefore(newChild,oldChild);\n-\t\tif(oldChild){\n-\t\t\tthis.removeChild(oldChild);\n-\t\t}\n-\t},\n-\tremoveChild:function(oldChild){\n-\t\treturn _removeChild(this,oldChild);\n-\t},\n-\tappendChild:function(newChild){\n-\t\treturn this.insertBefore(newChild,null);\n-\t},\n-\thasChildNodes:function(){\n-\t\treturn this.firstChild != null;\n-\t},\n-\tcloneNode:function(deep){\n-\t\treturn cloneNode(this.ownerDocument||this,this,deep);\n-\t},\n-\t// Modified in DOM Level 2:\n-\tnormalize:function(){\n-\t\tvar child = this.firstChild;\n-\t\twhile(child){\n-\t\t\tvar next = child.nextSibling;\n-\t\t\tif(next && next.nodeType == TEXT_NODE && child.nodeType == TEXT_NODE){\n-\t\t\t\tthis.removeChild(next);\n-\t\t\t\tchild.appendData(next.data);\n-\t\t\t}else{\n-\t\t\t\tchild.normalize();\n-\t\t\t\tchild = next;\n-\t\t\t}\n-\t\t}\n-\t},\n-  \t// Introduced in DOM Level 2:\n-\tisSupported:function(feature, version){\n-\t\treturn this.ownerDocument.implementation.hasFeature(feature,version);\n-\t},\n+        firstChild : null,\n+        lastChild : null,\n+        previousSibling : null,\n+        nextSibling : null,\n+        attributes : null,\n+        parentNode : null,\n+        childNodes : null,\n+        ownerDocument : null,\n+        nodeValue : null,\n+        namespaceURI : null,\n+        prefix : null,\n+        localName : null,\n+        // Modified in DOM Level 2:\n+        insertBefore:function(newChild, refChild){//raises \n+                return _insertBefore(this,newChild,refChild);\n+        },\n+        replaceChild:function(newChild, oldChild){//raises \n+                this.insertBefore(newChild,oldChild);\n+                if(oldChild){\n+                        this.removeChild(oldChild);\n+                }\n+        },\n+        removeChild:function(oldChild){\n+                return _removeChild(this,oldChild);\n+        },\n+        appendChild:function(newChild){\n+                return this.insertBefore(newChild,null);\n+        },\n+        hasChildNodes:function(){\n+                return this.firstChild != null;\n+        },\n+        cloneNode:function(deep){\n+                return cloneNode(this.ownerDocument||this,this,deep);\n+        },\n+        // Modified in DOM Level 2:\n+        normalize:function(){\n+                var child = this.firstChild;\n+                while(child){\n+                        var next = child.nextSibling;\n+                        if(next && next.nodeType == TEXT_NODE && child.nodeType == TEXT_NODE){\n+                                this.removeChild(next);\n+                                child.appendData(next.data);\n+                        }else{\n+                                child.normalize();\n+                                child = next;\n+                        }\n+                }\n+        },\n+        // Introduced in DOM Level 2:\n+        isSupported:function(feature, version){\n+                return this.ownerDocument.implementation.hasFeature(feature,version);\n+        },\n     // Introduced in DOM Level 2:\n     hasAttributes:function(){\n-    \treturn this.attributes.length>0;\n+        return this.attributes.length>0;\n     },\n     lookupPrefix:function(namespaceURI){\n-    \tvar el = this;\n-    \twhile(el){\n-    \t\tvar map = el._nsMap;\n-    \t\t//console.dir(map)\n-    \t\tif(map){\n-    \t\t\tfor(var n in map){\n-    \t\t\t\tif(map[n] == namespaceURI){\n-    \t\t\t\t\treturn n;\n-    \t\t\t\t}\n-    \t\t\t}\n-    \t\t}\n-    \t\tel = el.nodeType == ATTRIBUTE_NODE?el.ownerDocument : el.parentNode;\n-    \t}\n-    \treturn null;\n+        var el = this;\n+        while(el){\n+                var map = el._nsMap;\n+                //console.dir(map)\n+                if(map){\n+                        for(var n in map){\n+                                if(map[n] == namespaceURI){\n+                                        return n;\n+                                }\n+                        }\n+                }\n+                el = el.nodeType == ATTRIBUTE_NODE?el.ownerDocument : el.parentNode;\n+        }\n+        return null;\n     },\n     // Introduced in DOM Level 3:\n     lookupNamespaceURI:function(prefix){\n-    \tvar el = this;\n-    \twhile(el){\n-    \t\tvar map = el._nsMap;\n-    \t\t//console.dir(map)\n-    \t\tif(map){\n-    \t\t\tif(prefix in map){\n-    \t\t\t\treturn map[prefix] ;\n-    \t\t\t}\n-    \t\t}\n-    \t\tel = el.nodeType == ATTRIBUTE_NODE?el.ownerDocument : el.parentNode;\n-    \t}\n-    \treturn null;\n+        var el = this;\n+        while(el){\n+                var map = el._nsMap;\n+                //console.dir(map)\n+                if(map){\n+                        if(prefix in map){\n+                                return map[prefix] ;\n+                        }\n+                }\n+                el = el.nodeType == ATTRIBUTE_NODE?el.ownerDocument : el.parentNode;\n+        }\n+        return null;\n     },\n     // Introduced in DOM Level 3:\n     isDefaultNamespace:function(namespaceURI){\n-    \tvar prefix = this.lookupPrefix(namespaceURI);\n-    \treturn prefix == null;\n+        var prefix = this.lookupPrefix(namespaceURI);\n+        return prefix == null;\n     }\n };\n \n \n function _xmlEncoder(c){\n-\treturn c == '<' && '&lt;' ||\n+        return c == '<' && '&lt;' ||\n          c == '>' && '&gt;' ||\n          c == '&' && '&amp;' ||\n          c == '\"' && '&quot;' ||\n@@ -544,12 +544,12 @@ copy(NodeType,Node.prototype);\n  * @return boolean true: break visit;\n  */\n function _visitNode(node,callback){\n-\tif(callback(node)){\n-\t\treturn true;\n-\t}\n-\tif(node = node.firstChild){\n-\t\tdo{\n-\t\t\tif(_visitNode(node,callback)){return true}\n+        if(callback(node)){\n+                return true;\n+        }\n+        if(node = node.firstChild){\n+                do{\n+                        if(_visitNode(node,callback)){return true}\n         }while(node=node.nextSibling)\n     }\n }\n@@ -560,41 +560,41 @@ function Document(){\n }\n \n function _onAddAttribute(doc,el,newAttr){\n-\tdoc && doc._inc++;\n-\tvar ns = newAttr.namespaceURI ;\n-\tif(ns === NAMESPACE.XMLNS){\n-\t\t//update namespace\n-\t\tel._nsMap[newAttr.prefix?newAttr.localName:''] = newAttr.value\n-\t}\n+        doc && doc._inc++;\n+        var ns = newAttr.namespaceURI ;\n+        if(ns === NAMESPACE.XMLNS){\n+                //update namespace\n+                el._nsMap[newAttr.prefix?newAttr.localName:''] = newAttr.value\n+        }\n }\n \n function _onRemoveAttribute(doc,el,newAttr,remove){\n-\tdoc && doc._inc++;\n-\tvar ns = newAttr.namespaceURI ;\n-\tif(ns === NAMESPACE.XMLNS){\n-\t\t//update namespace\n-\t\tdelete el._nsMap[newAttr.prefix?newAttr.localName:'']\n-\t}\n+        doc && doc._inc++;\n+        var ns = newAttr.namespaceURI ;\n+        if(ns === NAMESPACE.XMLNS){\n+                //update namespace\n+                delete el._nsMap[newAttr.prefix?newAttr.localName:'']\n+        }\n }\n \n function _onUpdateChild(doc,el,newChild){\n-\tif(doc && doc._inc){\n-\t\tdoc._inc++;\n-\t\t//update childNodes\n-\t\tvar cs = el.childNodes;\n-\t\tif(newChild){\n-\t\t\tcs[cs.length++] = newChild;\n-\t\t}else{\n-\t\t\t//console.log(1)\n-\t\t\tvar child = el.firstChild;\n-\t\t\tvar i = 0;\n-\t\t\twhile(child){\n-\t\t\t\tcs[i++] = child;\n-\t\t\t\tchild =child.nextSibling;\n-\t\t\t}\n-\t\t\tcs.length = i;\n-\t\t}\n-\t}\n+        if(doc && doc._inc){\n+                doc._inc++;\n+                //update childNodes\n+                var cs = el.childNodes;\n+                if(newChild){\n+                        cs[cs.length++] = newChild;\n+                }else{\n+                        //console.log(1)\n+                        var child = el.firstChild;\n+                        var i = 0;\n+                        while(child){\n+                                cs[i++] = child;\n+                                child =child.nextSibling;\n+                        }\n+                        cs.length = i;\n+                }\n+        }\n }\n \n /**\n@@ -606,365 +606,365 @@ function _onUpdateChild(doc,el,newChild){\n  * prefix\n  */\n function _removeChild(parentNode,child){\n-\tvar previous = child.previousSibling;\n-\tvar next = child.nextSibling;\n-\tif(previous){\n-\t\tprevious.nextSibling = next;\n-\t}else{\n-\t\tparentNode.firstChild = next\n-\t}\n-\tif(next){\n-\t\tnext.previousSibling = previous;\n-\t}else{\n-\t\tparentNode.lastChild = previous;\n-\t}\n-\t_onUpdateChild(parentNode.ownerDocument,parentNode);\n-\treturn child;\n+        var previous = child.previousSibling;\n+        var next = child.nextSibling;\n+        if(previous){\n+                previous.nextSibling = next;\n+        }else{\n+                parentNode.firstChild = next\n+        }\n+        if(next){\n+                next.previousSibling = previous;\n+        }else{\n+                parentNode.lastChild = previous;\n+        }\n+        _onUpdateChild(parentNode.ownerDocument,parentNode);\n+        return child;\n }\n /**\n  * preformance key(refChild == null)\n  */\n function _insertBefore(parentNode,newChild,nextChild){\n-\tvar cp = newChild.parentNode;\n-\tif(cp){\n-\t\tcp.removeChild(newChild);//remove and update\n-\t}\n-\tif(newChild.nodeType === DOCUMENT_FRAGMENT_NODE){\n-\t\tvar newFirst = newChild.firstChild;\n-\t\tif (newFirst == null) {\n-\t\t\treturn newChild;\n-\t\t}\n-\t\tvar newLast = newChild.lastChild;\n-\t}else{\n-\t\tnewFirst = newLast = newChild;\n-\t}\n-\tvar pre = nextChild ? nextChild.previousSibling : parentNode.lastChild;\n-\n-\tnewFirst.previousSibling = pre;\n-\tnewLast.nextSibling = nextChild;\n-\t\n-\t\n-\tif(pre){\n-\t\tpre.nextSibling = newFirst;\n-\t}else{\n-\t\tparentNode.firstChild = newFirst;\n-\t}\n-\tif(nextChild == null){\n-\t\tparentNode.lastChild = newLast;\n-\t}else{\n-\t\tnextChild.previousSibling = newLast;\n-\t}\n-\tdo{\n-\t\tnewFirst.parentNode = parentNode;\n-\t}while(newFirst !== newLast && (newFirst= newFirst.nextSibling))\n-\t_onUpdateChild(parentNode.ownerDocument||parentNode,parentNode);\n-\t//console.log(parentNode.lastChild.nextSibling == null)\n-\tif (newChild.nodeType == DOCUMENT_FRAGMENT_NODE) {\n-\t\tnewChild.firstChild = newChild.lastChild = null;\n-\t}\n-\treturn newChild;\n+        var cp = newChild.parentNode;\n+        if(cp){\n+                cp.removeChild(newChild);//remove and update\n+        }\n+        if(newChild.nodeType === DOCUMENT_FRAGMENT_NODE){\n+                var newFirst = newChild.firstChild;\n+                if (newFirst == null) {\n+                        return newChild;\n+                }\n+                var newLast = newChild.lastChild;\n+        }else{\n+                newFirst = newLast = newChild;\n+        }\n+        var pre = nextChild ? nextChild.previousSibling : parentNode.lastChild;\n+\n+        newFirst.previousSibling = pre;\n+        newLast.nextSibling = nextChild;\n+        \n+        \n+        if(pre){\n+                pre.nextSibling = newFirst;\n+        }else{\n+                parentNode.firstChild = newFirst;\n+        }\n+        if(nextChild == null){\n+                parentNode.lastChild = newLast;\n+        }else{\n+                nextChild.previousSibling = newLast;\n+        }\n+        do{\n+                newFirst.parentNode = parentNode;\n+        }while(newFirst !== newLast && (newFirst= newFirst.nextSibling))\n+        _onUpdateChild(parentNode.ownerDocument||parentNode,parentNode);\n+        //console.log(parentNode.lastChild.nextSibling == null)\n+        if (newChild.nodeType == DOCUMENT_FRAGMENT_NODE) {\n+                newChild.firstChild = newChild.lastChild = null;\n+        }\n+        return newChild;\n }\n function _appendSingleChild(parentNode,newChild){\n-\tvar cp = newChild.parentNode;\n-\tif(cp){\n-\t\tvar pre = parentNode.lastChild;\n-\t\tcp.removeChild(newChild);//remove and update\n-\t\tvar pre = parentNode.lastChild;\n-\t}\n-\tvar pre = parentNode.lastChild;\n-\tnewChild.parentNode = parentNode;\n-\tnewChild.previousSibling = pre;\n-\tnewChild.nextSibling = null;\n-\tif(pre){\n-\t\tpre.nextSibling = newChild;\n-\t}else{\n-\t\tparentNode.firstChild = newChild;\n-\t}\n-\tparentNode.lastChild = newChild;\n-\t_onUpdateChild(parentNode.ownerDocument,parentNode,newChild);\n-\treturn newChild;\n-\t//console.log(\"__aa\",parentNode.lastChild.nextSibling == null)\n+        var cp = newChild.parentNode;\n+        if(cp){\n+                var pre = parentNode.lastChild;\n+                cp.removeChild(newChild);//remove and update\n+                var pre = parentNode.lastChild;\n+        }\n+        var pre = parentNode.lastChild;\n+        newChild.parentNode = parentNode;\n+        newChild.previousSibling = pre;\n+        newChild.nextSibling = null;\n+        if(pre){\n+                pre.nextSibling = newChild;\n+        }else{\n+                parentNode.firstChild = newChild;\n+        }\n+        parentNode.lastChild = newChild;\n+        _onUpdateChild(parentNode.ownerDocument,parentNode,newChild);\n+        return newChild;\n+        //console.log(\"__aa\",parentNode.lastChild.nextSibling == null)\n }\n Document.prototype = {\n-\t//implementation : null,\n-\tnodeName :  '#document',\n-\tnodeType :  DOCUMENT_NODE,\n-\tdoctype :  null,\n-\tdocumentElement :  null,\n-\t_inc : 1,\n-\n-\tinsertBefore :  function(newChild, refChild){//raises\n-\t\tif(newChild.nodeType == DOCUMENT_FRAGMENT_NODE){\n-\t\t\tvar child = newChild.firstChild;\n-\t\t\twhile(child){\n-\t\t\t\tvar next = child.nextSibling;\n-\t\t\t\tthis.insertBefore(child,refChild);\n-\t\t\t\tchild = next;\n-\t\t\t}\n-\t\t\treturn newChild;\n-\t\t}\n-\t\tif(this.documentElement == null && newChild.nodeType == ELEMENT_NODE){\n-\t\t\tthis.documentElement = newChild;\n-\t\t}\n-\n-\t\treturn _insertBefore(this,newChild,refChild),(newChild.ownerDocument = this),newChild;\n-\t},\n-\tremoveChild :  function(oldChild){\n-\t\tif(this.documentElement == oldChild){\n-\t\t\tthis.documentElement = null;\n-\t\t}\n-\t\treturn _removeChild(this,oldChild);\n-\t},\n-\t// Introduced in DOM Level 2:\n-\timportNode : function(importedNode,deep){\n-\t\treturn importNode(this,importedNode,deep);\n-\t},\n-\t// Introduced in DOM Level 2:\n-\tgetElementById :\tfunction(id){\n-\t\tvar rtv = null;\n-\t\t_visitNode(this.documentElement,function(node){\n-\t\t\tif(node.nodeType == ELEMENT_NODE){\n-\t\t\t\tif(node.getAttribute('id') == id){\n-\t\t\t\t\trtv = node;\n-\t\t\t\t\treturn true;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t})\n-\t\treturn rtv;\n-\t},\n-\n-\t/**\n-\t * The `getElementsByClassName` method of `Document` interface returns an array-like object\n-\t * of all child elements which have **all** of the given class name(s).\n-\t *\n-\t * Returns an empty list if `classeNames` is an empty string or only contains HTML white space characters.\n-\t *\n-\t *\n-\t * Warning: This is a live LiveNodeList.\n-\t * Changes in the DOM will reflect in the array as the changes occur.\n-\t * If an element selected by this array no longer qualifies for the selector,\n-\t * it will automatically be removed. Be aware of this for iteration purposes.\n-\t *\n-\t * @param {string} classNames is a string representing the class name(s) to match; multiple class names are separated by (ASCII-)whitespace\n-\t *\n-\t * @see https://developer.mozilla.org/en-US/docs/Web/API/Document/getElementsByClassName\n-\t * @see https://dom.spec.whatwg.org/#concept-getelementsbyclassname\n-\t */\n-\tgetElementsByClassName: function(classNames) {\n-\t\tvar classNamesSet = toOrderedSet(classNames)\n-\t\treturn new LiveNodeList(this, function(base) {\n-\t\t\tvar ls = [];\n-\t\t\tif (classNamesSet.length > 0) {\n-\t\t\t\t_visitNode(base.documentElement, function(node) {\n-\t\t\t\t\tif(node !== base && node.nodeType === ELEMENT_NODE) {\n-\t\t\t\t\t\tvar nodeClassNames = node.getAttribute('class')\n-\t\t\t\t\t\t// can be null if the attribute does not exist\n-\t\t\t\t\t\tif (nodeClassNames) {\n-\t\t\t\t\t\t\t// before splitting and iterating just compare them for the most common case\n-\t\t\t\t\t\t\tvar matches = classNames === nodeClassNames;\n-\t\t\t\t\t\t\tif (!matches) {\n-\t\t\t\t\t\t\t\tvar nodeClassNamesSet = toOrderedSet(nodeClassNames)\n-\t\t\t\t\t\t\t\tmatches = classNamesSet.every(arrayIncludes(nodeClassNamesSet))\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tif(matches) {\n-\t\t\t\t\t\t\t\tls.push(node);\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t});\n-\t\t\t}\n-\t\t\treturn ls;\n-\t\t});\n-\t},\n-\n-\t//document factory method:\n-\tcreateElement :\tfunction(tagName){\n-\t\tvar node = new Element();\n-\t\tnode.ownerDocument = this;\n-\t\tnode.nodeName = tagName;\n-\t\tnode.tagName = tagName;\n-\t\tnode.localName = tagName;\n-\t\tnode.childNodes = new NodeList();\n-\t\tvar attrs\t= node.attributes = new NamedNodeMap();\n-\t\tattrs._ownerElement = node;\n-\t\treturn node;\n-\t},\n-\tcreateDocumentFragment :\tfunction(){\n-\t\tvar node = new DocumentFragment();\n-\t\tnode.ownerDocument = this;\n-\t\tnode.childNodes = new NodeList();\n-\t\treturn node;\n-\t},\n-\tcreateTextNode :\tfunction(data){\n-\t\tvar node = new Text();\n-\t\tnode.ownerDocument = this;\n-\t\tnode.appendData(data)\n-\t\treturn node;\n-\t},\n-\tcreateComment :\tfunction(data){\n-\t\tvar node = new Comment();\n-\t\tnode.ownerDocument = this;\n-\t\tnode.appendData(data)\n-\t\treturn node;\n-\t},\n-\tcreateCDATASection :\tfunction(data){\n-\t\tvar node = new CDATASection();\n-\t\tnode.ownerDocument = this;\n-\t\tnode.appendData(data)\n-\t\treturn node;\n-\t},\n-\tcreateProcessingInstruction :\tfunction(target,data){\n-\t\tvar node = new ProcessingInstruction();\n-\t\tnode.ownerDocument = this;\n-\t\tnode.tagName = node.target = target;\n-\t\tnode.nodeValue= node.data = data;\n-\t\treturn node;\n-\t},\n-\tcreateAttribute :\tfunction(name){\n-\t\tvar node = new Attr();\n-\t\tnode.ownerDocument\t= this;\n-\t\tnode.name = name;\n-\t\tnode.nodeName\t= name;\n-\t\tnode.localName = name;\n-\t\tnode.specified = true;\n-\t\treturn node;\n-\t},\n-\tcreateEntityReference :\tfunction(name){\n-\t\tvar node = new EntityReference();\n-\t\tnode.ownerDocument\t= this;\n-\t\tnode.nodeName\t= name;\n-\t\treturn node;\n-\t},\n-\t// Introduced in DOM Level 2:\n-\tcreateElementNS :\tfunction(namespaceURI,qualifiedName){\n-\t\tvar node = new Element();\n-\t\tvar pl = qualifiedName.split(':');\n-\t\tvar attrs\t= node.attributes = new NamedNodeMap();\n-\t\tnode.childNodes = new NodeList();\n-\t\tnode.ownerDocument = this;\n-\t\tnode.nodeName = qualifiedName;\n-\t\tnode.tagName = qualifiedName;\n-\t\tnode.namespaceURI = namespaceURI;\n-\t\tif(pl.length == 2){\n-\t\t\tnode.prefix = pl[0];\n-\t\t\tnode.localName = pl[1];\n-\t\t}else{\n-\t\t\t//el.prefix = null;\n-\t\t\tnode.localName = qualifiedName;\n-\t\t}\n-\t\tattrs._ownerElement = node;\n-\t\treturn node;\n-\t},\n-\t// Introduced in DOM Level 2:\n-\tcreateAttributeNS :\tfunction(namespaceURI,qualifiedName){\n-\t\tvar node = new Attr();\n-\t\tvar pl = qualifiedName.split(':');\n-\t\tnode.ownerDocument = this;\n-\t\tnode.nodeName = qualifiedName;\n-\t\tnode.name = qualifiedName;\n-\t\tnode.namespaceURI = namespaceURI;\n-\t\tnode.specified = true;\n-\t\tif(pl.length == 2){\n-\t\t\tnode.prefix = pl[0];\n-\t\t\tnode.localName = pl[1];\n-\t\t}else{\n-\t\t\t//el.prefix = null;\n-\t\t\tnode.localName = qualifiedName;\n-\t\t}\n-\t\treturn node;\n-\t}\n+        //implementation : null,\n+        nodeName :  '#document',\n+        nodeType :  DOCUMENT_NODE,\n+        doctype :  null,\n+        documentElement :  null,\n+        _inc : 1,\n+\n+        insertBefore :  function(newChild, refChild){//raises\n+                if(newChild.nodeType == DOCUMENT_FRAGMENT_NODE){\n+                        var child = newChild.firstChild;\n+                        while(child){\n+                                var next = child.nextSibling;\n+                                this.insertBefore(child,refChild);\n+                                child = next;\n+                        }\n+                        return newChild;\n+                }\n+                if(this.documentElement == null && newChild.nodeType == ELEMENT_NODE){\n+                        this.documentElement = newChild;\n+                }\n+\n+                return _insertBefore(this,newChild,refChild),(newChild.ownerDocument = this),newChild;\n+        },\n+        removeChild :  function(oldChild){\n+                if(this.documentElement == oldChild){\n+                        this.documentElement = null;\n+                }\n+                return _removeChild(this,oldChild);\n+        },\n+        // Introduced in DOM Level 2:\n+        importNode : function(importedNode,deep){\n+                return importNode(this,importedNode,deep);\n+        },\n+        // Introduced in DOM Level 2:\n+        getElementById :        function(id){\n+                var rtv = null;\n+                _visitNode(this.documentElement,function(node){\n+                        if(node.nodeType == ELEMENT_NODE){\n+                                if(node.getAttribute('id') == id){\n+                                        rtv = node;\n+                                        return true;\n+                                }\n+                        }\n+                })\n+                return rtv;\n+        },\n+\n+        /**\n+         * The `getElementsByClassName` method of `Document` interface returns an array-like object\n+         * of all child elements which have **all** of the given class name(s).\n+         *\n+         * Returns an empty list if `classeNames` is an empty string or only contains HTML white space characters.\n+         *\n+         *\n+         * Warning: This is a live LiveNodeList.\n+         * Changes in the DOM will reflect in the array as the changes occur.\n+         * If an element selected by this array no longer qualifies for the selector,\n+         * it will automatically be removed. Be aware of this for iteration purposes.\n+         *\n+         * @param {string} classNames is a string representing the class name(s) to match; multiple class names are separated by (ASCII-)whitespace\n+         *\n+         * @see https://developer.mozilla.org/en-US/docs/Web/API/Document/getElementsByClassName\n+         * @see https://dom.spec.whatwg.org/#concept-getelementsbyclassname\n+         */\n+        getElementsByClassName: function(classNames) {\n+                var classNamesSet = toOrderedSet(classNames)\n+                return new LiveNodeList(this, function(base) {\n+                        var ls = [];\n+                        if (classNamesSet.length > 0) {\n+                                _visitNode(base.documentElement, function(node) {\n+                                        if(node !== base && node.nodeType === ELEMENT_NODE) {\n+                                                var nodeClassNames = node.getAttribute('class')\n+                                                // can be null if the attribute does not exist\n+                                                if (nodeClassNames) {\n+                                                        // before splitting and iterating just compare them for the most common case\n+                                                        var matches = classNames === nodeClassNames;\n+                                                        if (!matches) {\n+                                                                var nodeClassNamesSet = toOrderedSet(nodeClassNames)\n+                                                                matches = classNamesSet.every(arrayIncludes(nodeClassNamesSet))\n+                                                        }\n+                                                        if(matches) {\n+                                                                ls.push(node);\n+                                                        }\n+                                                }\n+                                        }\n+                                });\n+                        }\n+                        return ls;\n+                });\n+        },\n+\n+        //document factory method:\n+        createElement : function(tagName){\n+                var node = new Element();\n+                node.ownerDocument = this;\n+                node.nodeName = tagName;\n+                node.tagName = tagName;\n+                node.localName = tagName;\n+                node.childNodes = new NodeList();\n+                var attrs       = node.attributes = new NamedNodeMap();\n+                attrs._ownerElement = node;\n+                return node;\n+        },\n+        createDocumentFragment :        function(){\n+                var node = new DocumentFragment();\n+                node.ownerDocument = this;\n+                node.childNodes = new NodeList();\n+                return node;\n+        },\n+        createTextNode :        function(data){\n+                var node = new Text();\n+                node.ownerDocument = this;\n+                node.appendData(data)\n+                return node;\n+        },\n+        createComment : function(data){\n+                var node = new Comment();\n+                node.ownerDocument = this;\n+                node.appendData(data)\n+                return node;\n+        },\n+        createCDATASection :    function(data){\n+                var node = new CDATASection();\n+                node.ownerDocument = this;\n+                node.appendData(data)\n+                return node;\n+        },\n+        createProcessingInstruction :   function(target,data){\n+                var node = new ProcessingInstruction();\n+                node.ownerDocument = this;\n+                node.tagName = node.target = target;\n+                node.nodeValue= node.data = data;\n+                return node;\n+        },\n+        createAttribute :       function(name){\n+                var node = new Attr();\n+                node.ownerDocument      = this;\n+                node.name = name;\n+                node.nodeName   = name;\n+                node.localName = name;\n+                node.specified = true;\n+                return node;\n+        },\n+        createEntityReference : function(name){\n+                var node = new EntityReference();\n+                node.ownerDocument      = this;\n+                node.nodeName   = name;\n+                return node;\n+        },\n+        // Introduced in DOM Level 2:\n+        createElementNS :       function(namespaceURI,qualifiedName){\n+                var node = new Element();\n+                var pl = qualifiedName.split(':');\n+                var attrs       = node.attributes = new NamedNodeMap();\n+                node.childNodes = new NodeList();\n+                node.ownerDocument = this;\n+                node.nodeName = qualifiedName;\n+                node.tagName = qualifiedName;\n+                node.namespaceURI = namespaceURI;\n+                if(pl.length == 2){\n+                        node.prefix = pl[0];\n+                        node.localName = pl[1];\n+                }else{\n+                        //el.prefix = null;\n+                        node.localName = qualifiedName;\n+                }\n+                attrs._ownerElement = node;\n+                return node;\n+        },\n+        // Introduced in DOM Level 2:\n+        createAttributeNS :     function(namespaceURI,qualifiedName){\n+                var node = new Attr();\n+                var pl = qualifiedName.split(':');\n+                node.ownerDocument = this;\n+                node.nodeName = qualifiedName;\n+                node.name = qualifiedName;\n+                node.namespaceURI = namespaceURI;\n+                node.specified = true;\n+                if(pl.length == 2){\n+                        node.prefix = pl[0];\n+                        node.localName = pl[1];\n+                }else{\n+                        //el.prefix = null;\n+                        node.localName = qualifiedName;\n+                }\n+                return node;\n+        }\n };\n _extends(Document,Node);\n \n \n function Element() {\n-\tthis._nsMap = {};\n+        this._nsMap = {};\n };\n Element.prototype = {\n-\tnodeType : ELEMENT_NODE,\n-\thasAttribute : function(name){\n-\t\treturn this.getAttributeNode(name)!=null;\n-\t},\n-\tgetAttribute : function(name){\n-\t\tvar attr = this.getAttributeNode(name);\n-\t\treturn attr && attr.value || '';\n-\t},\n-\tgetAttributeNode : function(name){\n-\t\treturn this.attributes.getNamedItem(name);\n-\t},\n-\tsetAttribute : function(name, value){\n-\t\tvar attr = this.ownerDocument.createAttribute(name);\n-\t\tattr.value = attr.nodeValue = \"\" + value;\n-\t\tthis.setAttributeNode(attr)\n-\t},\n-\tremoveAttribute : function(name){\n-\t\tvar attr = this.getAttributeNode(name)\n-\t\tattr && this.removeAttributeNode(attr);\n-\t},\n-\t\n-\t//four real opeartion method\n-\tappendChild:function(newChild){\n-\t\tif(newChild.nodeType === DOCUMENT_FRAGMENT_NODE){\n-\t\t\treturn this.insertBefore(newChild,null);\n-\t\t}else{\n-\t\t\treturn _appendSingleChild(this,newChild);\n-\t\t}\n-\t},\n-\tsetAttributeNode : function(newAttr){\n-\t\treturn this.attributes.setNamedItem(newAttr);\n-\t},\n-\tsetAttributeNodeNS : function(newAttr){\n-\t\treturn this.attributes.setNamedItemNS(newAttr);\n-\t},\n-\tremoveAttributeNode : function(oldAttr){\n-\t\t//console.log(this == oldAttr.ownerElement)\n-\t\treturn this.attributes.removeNamedItem(oldAttr.nodeName);\n-\t},\n-\t//get real attribute name,and remove it by removeAttributeNode\n-\tremoveAttributeNS : function(namespaceURI, localName){\n-\t\tvar old = this.getAttributeNodeNS(namespaceURI, localName);\n-\t\told && this.removeAttributeNode(old);\n-\t},\n-\t\n-\thasAttributeNS : function(namespaceURI, localName){\n-\t\treturn this.getAttributeNodeNS(namespaceURI, localName)!=null;\n-\t},\n-\tgetAttributeNS : function(namespaceURI, localName){\n-\t\tvar attr = this.getAttributeNodeNS(namespaceURI, localName);\n-\t\treturn attr && attr.value || '';\n-\t},\n-\tsetAttributeNS : function(namespaceURI, qualifiedName, value){\n-\t\tvar attr = this.ownerDocument.createAttributeNS(namespaceURI, qualifiedName);\n-\t\tattr.value = attr.nodeValue = \"\" + value;\n-\t\tthis.setAttributeNode(attr)\n-\t},\n-\tgetAttributeNodeNS : function(namespaceURI, localName){\n-\t\treturn this.attributes.getNamedItemNS(namespaceURI, localName);\n-\t},\n-\t\n-\tgetElementsByTagName : function(tagName){\n-\t\treturn new LiveNodeList(this,function(base){\n-\t\t\tvar ls = [];\n-\t\t\t_visitNode(base,function(node){\n-\t\t\t\tif(node !== base && node.nodeType == ELEMENT_NODE && (tagName === '*' || node.tagName == tagName)){\n-\t\t\t\t\tls.push(node);\n-\t\t\t\t}\n-\t\t\t});\n-\t\t\treturn ls;\n-\t\t});\n-\t},\n-\tgetElementsByTagNameNS : function(namespaceURI, localName){\n-\t\treturn new LiveNodeList(this,function(base){\n-\t\t\tvar ls = [];\n-\t\t\t_visitNode(base,function(node){\n-\t\t\t\tif(node !== base && node.nodeType === ELEMENT_NODE && (namespaceURI === '*' || node.namespaceURI === namespaceURI) && (localName === '*' || node.localName == localName)){\n-\t\t\t\t\tls.push(node);\n-\t\t\t\t}\n-\t\t\t});\n-\t\t\treturn ls;\n-\t\t\t\n-\t\t});\n-\t}\n+        nodeType : ELEMENT_NODE,\n+        hasAttribute : function(name){\n+                return this.getAttributeNode(name)!=null;\n+        },\n+        getAttribute : function(name){\n+                var attr = this.getAttributeNode(name);\n+                return attr && attr.value || '';\n+        },\n+        getAttributeNode : function(name){\n+                return this.attributes.getNamedItem(name);\n+        },\n+        setAttribute : function(name, value){\n+                var attr = this.ownerDocument.createAttribute(name);\n+                attr.value = attr.nodeValue = \"\" + value;\n+                this.setAttributeNode(attr)\n+        },\n+        removeAttribute : function(name){\n+                var attr = this.getAttributeNode(name)\n+                attr && this.removeAttributeNode(attr);\n+        },\n+        \n+        //four real opeartion method\n+        appendChild:function(newChild){\n+                if(newChild.nodeType === DOCUMENT_FRAGMENT_NODE){\n+                        return this.insertBefore(newChild,null);\n+                }else{\n+                        return _appendSingleChild(this,newChild);\n+                }\n+        },\n+        setAttributeNode : function(newAttr){\n+                return this.attributes.setNamedItem(newAttr);\n+        },\n+        setAttributeNodeNS : function(newAttr){\n+                return this.attributes.setNamedItemNS(newAttr);\n+        },\n+        removeAttributeNode : function(oldAttr){\n+                //console.log(this == oldAttr.ownerElement)\n+                return this.attributes.removeNamedItem(oldAttr.nodeName);\n+        },\n+        //get real attribute name,and remove it by removeAttributeNode\n+        removeAttributeNS : function(namespaceURI, localName){\n+                var old = this.getAttributeNodeNS(namespaceURI, localName);\n+                old && this.removeAttributeNode(old);\n+        },\n+        \n+        hasAttributeNS : function(namespaceURI, localName){\n+                return this.getAttributeNodeNS(namespaceURI, localName)!=null;\n+        },\n+        getAttributeNS : function(namespaceURI, localName){\n+                var attr = this.getAttributeNodeNS(namespaceURI, localName);\n+                return attr && attr.value || '';\n+        },\n+        setAttributeNS : function(namespaceURI, qualifiedName, value){\n+                var attr = this.ownerDocument.createAttributeNS(namespaceURI, qualifiedName);\n+                attr.value = attr.nodeValue = \"\" + value;\n+                this.setAttributeNode(attr)\n+        },\n+        getAttributeNodeNS : function(namespaceURI, localName){\n+                return this.attributes.getNamedItemNS(namespaceURI, localName);\n+        },\n+        \n+        getElementsByTagName : function(tagName){\n+                return new LiveNodeList(this,function(base){\n+                        var ls = [];\n+                        _visitNode(base,function(node){\n+                                if(node !== base && node.nodeType == ELEMENT_NODE && (tagName === '*' || node.tagName == tagName)){\n+                                        ls.push(node);\n+                                }\n+                        });\n+                        return ls;\n+                });\n+        },\n+        getElementsByTagNameNS : function(namespaceURI, localName){\n+                return new LiveNodeList(this,function(base){\n+                        var ls = [];\n+                        _visitNode(base,function(node){\n+                                if(node !== base && node.nodeType === ELEMENT_NODE && (namespaceURI === '*' || node.namespaceURI === namespaceURI) && (localName === '*' || node.localName == localName)){\n+                                        ls.push(node);\n+                                }\n+                        });\n+                        return ls;\n+                        \n+                });\n+        }\n };\n Document.prototype.getElementsByTagName = Element.prototype.getElementsByTagName;\n Document.prototype.getElementsByTagNameNS = Element.prototype.getElementsByTagNameNS;\n@@ -980,66 +980,66 @@ _extends(Attr,Node);\n function CharacterData() {\n };\n CharacterData.prototype = {\n-\tdata : '',\n-\tsubstringData : function(offset, count) {\n-\t\treturn this.data.substring(offset, offset+count);\n-\t},\n-\tappendData: function(text) {\n-\t\ttext = this.data+text;\n-\t\tthis.nodeValue = this.data = text;\n-\t\tthis.length = text.length;\n-\t},\n-\tinsertData: function(offset,text) {\n-\t\tthis.replaceData(offset,0,text);\n-\t\n-\t},\n-\tappendChild:function(newChild){\n-\t\tthrow new Error(ExceptionMessage[HIERARCHY_REQUEST_ERR])\n-\t},\n-\tdeleteData: function(offset, count) {\n-\t\tthis.replaceData(offset,count,\"\");\n-\t},\n-\treplaceData: function(offset, count, text) {\n-\t\tvar start = this.data.substring(0,offset);\n-\t\tvar end = this.data.substring(offset+count);\n-\t\ttext = start + text + end;\n-\t\tthis.nodeValue = this.data = text;\n-\t\tthis.length = text.length;\n-\t}\n+        data : '',\n+        substringData : function(offset, count) {\n+                return this.data.substring(offset, offset+count);\n+        },\n+        appendData: function(text) {\n+                text = this.data+text;\n+                this.nodeValue = this.data = text;\n+                this.length = text.length;\n+        },\n+        insertData: function(offset,text) {\n+                this.replaceData(offset,0,text);\n+        \n+        },\n+        appendChild:function(newChild){\n+                throw new Error(ExceptionMessage[HIERARCHY_REQUEST_ERR])\n+        },\n+        deleteData: function(offset, count) {\n+                this.replaceData(offset,count,\"\");\n+        },\n+        replaceData: function(offset, count, text) {\n+                var start = this.data.substring(0,offset);\n+                var end = this.data.substring(offset+count);\n+                text = start + text + end;\n+                this.nodeValue = this.data = text;\n+                this.length = text.length;\n+        }\n }\n _extends(CharacterData,Node);\n function Text() {\n };\n Text.prototype = {\n-\tnodeName : \"#text\",\n-\tnodeType : TEXT_NODE,\n-\tsplitText : function(offset) {\n-\t\tvar text = this.data;\n-\t\tvar newText = text.substring(offset);\n-\t\ttext = text.substring(0, offset);\n-\t\tthis.data = this.nodeValue = text;\n-\t\tthis.length = text.length;\n-\t\tvar newNode = this.ownerDocument.createTextNode(newText);\n-\t\tif(this.parentNode){\n-\t\t\tthis.parentNode.insertBefore(newNode, this.nextSibling);\n-\t\t}\n-\t\treturn newNode;\n-\t}\n+        nodeName : \"#text\",\n+        nodeType : TEXT_NODE,\n+        splitText : function(offset) {\n+                var text = this.data;\n+                var newText = text.substring(offset);\n+                text = text.substring(0, offset);\n+                this.data = this.nodeValue = text;\n+                this.length = text.length;\n+                var newNode = this.ownerDocument.createTextNode(newText);\n+                if(this.parentNode){\n+                        this.parentNode.insertBefore(newNode, this.nextSibling);\n+                }\n+                return newNode;\n+        }\n }\n _extends(Text,CharacterData);\n function Comment() {\n };\n Comment.prototype = {\n-\tnodeName : \"#comment\",\n-\tnodeType : COMMENT_NODE\n+        nodeName : \"#comment\",\n+        nodeType : COMMENT_NODE\n }\n _extends(Comment,CharacterData);\n \n function CDATASection() {\n };\n CDATASection.prototype = {\n-\tnodeName : \"#cdata-section\",\n-\tnodeType : CDATA_SECTION_NODE\n+        nodeName : \"#cdata-section\",\n+        nodeType : CDATA_SECTION_NODE\n }\n _extends(CDATASection,CharacterData);\n \n@@ -1066,8 +1066,8 @@ _extends(EntityReference,Node);\n \n function DocumentFragment() {\n };\n-DocumentFragment.prototype.nodeName =\t\"#document-fragment\";\n-DocumentFragment.prototype.nodeType =\tDOCUMENT_FRAGMENT_NODE;\n+DocumentFragment.prototype.nodeName =   \"#document-fragment\";\n+DocumentFragment.prototype.nodeType =   DOCUMENT_FRAGMENT_NODE;\n _extends(DocumentFragment,Node);\n \n \n@@ -1077,392 +1077,395 @@ ProcessingInstruction.prototype.nodeType = PROCESSING_INSTRUCTION_NODE;\n _extends(ProcessingInstruction,Node);\n function XMLSerializer(){}\n XMLSerializer.prototype.serializeToString = function(node,isHtml,nodeFilter){\n-\treturn nodeSerializeToString.call(node,isHtml,nodeFilter);\n+        return nodeSerializeToString.call(node,isHtml,nodeFilter);\n }\n Node.prototype.toString = nodeSerializeToString;\n function nodeSerializeToString(isHtml,nodeFilter){\n-\tvar buf = [];\n-\tvar refNode = this.nodeType == 9 && this.documentElement || this;\n-\tvar prefix = refNode.prefix;\n-\tvar uri = refNode.namespaceURI;\n-\t\n-\tif(uri && prefix == null){\n-\t\t//console.log(prefix)\n-\t\tvar prefix = refNode.lookupPrefix(uri);\n-\t\tif(prefix == null){\n-\t\t\t//isHTML = true;\n-\t\t\tvar visibleNamespaces=[\n-\t\t\t{namespace:uri,prefix:null}\n-\t\t\t//{namespace:uri,prefix:''}\n-\t\t\t]\n-\t\t}\n-\t}\n-\tserializeToString(this,buf,isHtml,nodeFilter,visibleNamespaces);\n-\t//console.log('###',this.nodeType,uri,prefix,buf.join(''))\n-\treturn buf.join('');\n+        var buf = [];\n+        var refNode = this.nodeType == 9 && this.documentElement || this;\n+        var prefix = refNode.prefix;\n+        var uri = refNode.namespaceURI;\n+        \n+        if(uri && prefix == null){\n+                //console.log(prefix)\n+                var prefix = refNode.lookupPrefix(uri);\n+                if(prefix == null){\n+                        //isHTML = true;\n+                        var visibleNamespaces=[\n+                        {namespace:uri,prefix:null}\n+                        //{namespace:uri,prefix:''}\n+                        ]\n+                }\n+        }\n+        serializeToString(this,buf,isHtml,nodeFilter,visibleNamespaces);\n+        //console.log('###',this.nodeType,uri,prefix,buf.join(''))\n+        return buf.join('');\n }\n \n function needNamespaceDefine(node, isHTML, visibleNamespaces) {\n-\tvar prefix = node.prefix || '';\n-\tvar uri = node.namespaceURI;\n-\t// According to [Namespaces in XML 1.0](https://www.w3.org/TR/REC-xml-names/#ns-using) ,\n-\t// and more specifically https://www.w3.org/TR/REC-xml-names/#nsc-NoPrefixUndecl :\n-\t// > In a namespace declaration for a prefix [...], the attribute value MUST NOT be empty.\n-\t// in a similar manner [Namespaces in XML 1.1](https://www.w3.org/TR/xml-names11/#ns-using)\n-\t// and more specifically https://www.w3.org/TR/xml-names11/#nsc-NSDeclared :\n-\t// > [...] Furthermore, the attribute value [...] must not be an empty string.\n-\t// so serializing empty namespace value like xmlns:ds=\"\" would produce an invalid XML document.\n-\tif (!uri) {\n-\t\treturn false;\n-\t}\n-\tif (prefix === \"xml\" && uri === NAMESPACE.XML || uri === NAMESPACE.XMLNS) {\n-\t\treturn false;\n-\t}\n-\t\n-\tvar i = visibleNamespaces.length \n-\twhile (i--) {\n-\t\tvar ns = visibleNamespaces[i];\n-\t\t// get namespace prefix\n-\t\tif (ns.prefix === prefix) {\n-\t\t\treturn ns.namespace !== uri;\n-\t\t}\n-\t}\n-\treturn true;\n+        var prefix = node.prefix || '';\n+        var uri = node.namespaceURI;\n+        // According to [Namespaces in XML 1.0](https://www.w3.org/TR/REC-xml-names/#ns-using) ,\n+        // and more specifically https://www.w3.org/TR/REC-xml-names/#nsc-NoPrefixUndecl :\n+        // > In a namespace declaration for a prefix [...], the attribute value MUST NOT be empty.\n+        // in a similar manner [Namespaces in XML 1.1](https://www.w3.org/TR/xml-names11/#ns-using)\n+        // and more specifically https://www.w3.org/TR/xml-names11/#nsc-NSDeclared :\n+        // > [...] Furthermore, the attribute value [...] must not be an empty string.\n+        // so serializing empty namespace value like xmlns:ds=\"\" would produce an invalid XML document.\n+        if (!uri) {\n+                return false;\n+        }\n+        if (prefix === \"xml\" && uri === NAMESPACE.XML || uri === NAMESPACE.XMLNS) {\n+                return false;\n+        }\n+        \n+        var i = visibleNamespaces.length \n+        while (i--) {\n+                var ns = visibleNamespaces[i];\n+                // get namespace prefix\n+                if (ns.prefix === prefix) {\n+                        return ns.namespace !== uri;\n+                }\n+        }\n+        return true;\n }\n \n function serializeToString(node,buf,isHTML,nodeFilter,visibleNamespaces){\n-\tif (!visibleNamespaces) {\n-\t\tvisibleNamespaces = [];\n-\t}\n-\n-\tif(nodeFilter){\n-\t\tnode = nodeFilter(node);\n-\t\tif(node){\n-\t\t\tif(typeof node == 'string'){\n-\t\t\t\tbuf.push(node);\n-\t\t\t\treturn;\n-\t\t\t}\n-\t\t}else{\n-\t\t\treturn;\n-\t\t}\n-\t\t//buf.sort.apply(attrs, attributeSorter);\n-\t}\n-\n-\tswitch(node.nodeType){\n-\tcase ELEMENT_NODE:\n-\t\tvar attrs = node.attributes;\n-\t\tvar len = attrs.length;\n-\t\tvar child = node.firstChild;\n-\t\tvar nodeName = node.tagName;\n-\t\t\n-\t\tisHTML = NAMESPACE.isHTML(node.namespaceURI) || isHTML\n-\n-\t\tvar prefixedNodeName = nodeName\n-\t\tif (!isHTML && !node.prefix && node.namespaceURI) {\n-\t\t\tvar defaultNS\n-\t\t\tfor (var ai = 0; ai < attrs.length; ai++) {\n-\t\t\t\tif (attrs.item(ai).name === 'xmlns') {\n-\t\t\t\t\tdefaultNS = attrs.item(ai).value\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif (defaultNS !== node.namespaceURI) {\n-\t\t\t\tfor (var nsi = visibleNamespaces.length - 1; nsi >= 0; nsi--) {\n-\t\t\t\t\tvar namespace = visibleNamespaces[nsi]\n-\t\t\t\t\tif (namespace.namespace === node.namespaceURI) {\n-\t\t\t\t\t\tif (namespace.prefix) {\n-\t\t\t\t\t\t\tprefixedNodeName = namespace.prefix + ':' + nodeName\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tbuf.push('<', prefixedNodeName);\n-\n-\t\tfor(var i=0;i<len;i++){\n-\t\t\t// add namespaces for attributes\n-\t\t\tvar attr = attrs.item(i);\n-\t\t\tif (attr.prefix == 'xmlns') {\n-\t\t\t\tvisibleNamespaces.push({ prefix: attr.localName, namespace: attr.value });\n-\t\t\t}else if(attr.nodeName == 'xmlns'){\n-\t\t\t\tvisibleNamespaces.push({ prefix: '', namespace: attr.value });\n-\t\t\t}\n-\t\t}\n-\n-\t\tfor(var i=0;i<len;i++){\n-\t\t\tvar attr = attrs.item(i);\n-\t\t\tif (needNamespaceDefine(attr,isHTML, visibleNamespaces)) {\n-\t\t\t\tvar prefix = attr.prefix||'';\n-\t\t\t\tvar uri = attr.namespaceURI;\n-\t\t\t\tvar ns = prefix ? ' xmlns:' + prefix : \" xmlns\";\n-\t\t\t\tbuf.push(ns, '=\"' , uri , '\"');\n-\t\t\t\tvisibleNamespaces.push({ prefix: prefix, namespace:uri });\n-\t\t\t}\n-\t\t\tserializeToString(attr,buf,isHTML,nodeFilter,visibleNamespaces);\n-\t\t}\n-\n-\t\t// add namespace for current node\t\t\n-\t\tif (nodeName === prefixedNodeName && needNamespaceDefine(node, isHTML, visibleNamespaces)) {\n-\t\t\tvar prefix = node.prefix||'';\n-\t\t\tvar uri = node.namespaceURI;\n-\t\t\tvar ns = prefix ? ' xmlns:' + prefix : \" xmlns\";\n-\t\t\tbuf.push(ns, '=\"' , uri , '\"');\n-\t\t\tvisibleNamespaces.push({ prefix: prefix, namespace:uri });\n-\t\t}\n-\t\t\n-\t\tif(child || isHTML && !/^(?:meta|link|img|br|hr|input)$/i.test(nodeName)){\n-\t\t\tbuf.push('>');\n-\t\t\t//if is cdata child node\n-\t\t\tif(isHTML && /^script$/i.test(nodeName)){\n-\t\t\t\twhile(child){\n-\t\t\t\t\tif(child.data){\n-\t\t\t\t\t\tbuf.push(child.data);\n-\t\t\t\t\t}else{\n-\t\t\t\t\t\tserializeToString(child, buf, isHTML, nodeFilter, visibleNamespaces.slice());\n-\t\t\t\t\t}\n-\t\t\t\t\tchild = child.nextSibling;\n-\t\t\t\t}\n-\t\t\t}else\n-\t\t\t{\n-\t\t\t\twhile(child){\n-\t\t\t\t\tserializeToString(child, buf, isHTML, nodeFilter, visibleNamespaces.slice());\n-\t\t\t\t\tchild = child.nextSibling;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tbuf.push('</',prefixedNodeName,'>');\n-\t\t}else{\n-\t\t\tbuf.push('/>');\n-\t\t}\n-\t\t// remove added visible namespaces\n-\t\t//visibleNamespaces.length = startVisibleNamespaces;\n-\t\treturn;\n-\tcase DOCUMENT_NODE:\n-\tcase DOCUMENT_FRAGMENT_NODE:\n-\t\tvar child = node.firstChild;\n-\t\twhile(child){\n-\t\t\tserializeToString(child, buf, isHTML, nodeFilter, visibleNamespaces.slice());\n-\t\t\tchild = child.nextSibling;\n-\t\t}\n-\t\treturn;\n-\tcase ATTRIBUTE_NODE:\n-\t\t/**\n-\t\t * Well-formedness constraint: No < in Attribute Values\n-\t\t * The replacement text of any entity referred to directly or indirectly in an attribute value must not contain a <.\n-\t\t * @see https://www.w3.org/TR/xml/#CleanAttrVals\n-\t\t * @see https://www.w3.org/TR/xml/#NT-AttValue\n-\t\t */\n-\t\treturn buf.push(' ', node.name, '=\"', node.value.replace(/[<&\"]/g,_xmlEncoder), '\"');\n-\tcase TEXT_NODE:\n-\t\t/**\n-\t\t * The ampersand character (&) and the left angle bracket (<) must not appear in their literal form,\n-\t\t * except when used as markup delimiters, or within a comment, a processing instruction, or a CDATA section.\n-\t\t * If they are needed elsewhere, they must be escaped using either numeric character references or the strings\n-\t\t * `&amp;` and `&lt;` respectively.\n-\t\t * The right angle bracket (>) may be represented using the string \" &gt; \", and must, for compatibility,\n-\t\t * be escaped using either `&gt;` or a character reference when it appears in the string `]]>` in content,\n-\t\t * when that string is not marking the end of a CDATA section.\n-\t\t *\n-\t\t * In the content of elements, character data is any string of characters\n-\t\t * which does not contain the start-delimiter of any markup\n-\t\t * and does not include the CDATA-section-close delimiter, `]]>`.\n-\t\t *\n-\t\t * @see https://www.w3.org/TR/xml/#NT-CharData\n-\t\t */\n-\t\treturn buf.push(node.data\n-\t\t\t.replace(/[<&]/g,_xmlEncoder)\n-\t\t\t.replace(/]]>/g, ']]&gt;')\n-\t\t);\n-\tcase CDATA_SECTION_NODE:\n-\t\treturn buf.push( '<![CDATA[',node.data,']]>');\n-\tcase COMMENT_NODE:\n-\t\treturn buf.push( \"<!--\",node.data,\"-->\");\n-\tcase DOCUMENT_TYPE_NODE:\n-\t\tvar pubid = node.publicId;\n-\t\tvar sysid = node.systemId;\n-\t\tbuf.push('<!DOCTYPE ',node.name);\n-\t\tif(pubid){\n-\t\t\tbuf.push(' PUBLIC ', pubid);\n-\t\t\tif (sysid && sysid!='.') {\n-\t\t\t\tbuf.push(' ', sysid);\n-\t\t\t}\n-\t\t\tbuf.push('>');\n-\t\t}else if(sysid && sysid!='.'){\n-\t\t\tbuf.push(' SYSTEM ', sysid, '>');\n-\t\t}else{\n-\t\t\tvar sub = node.internalSubset;\n-\t\t\tif(sub){\n-\t\t\t\tbuf.push(\" [\",sub,\"]\");\n-\t\t\t}\n-\t\t\tbuf.push(\">\");\n-\t\t}\n-\t\treturn;\n-\tcase PROCESSING_INSTRUCTION_NODE:\n-\t\treturn buf.push( \"<?\",node.target,\" \",node.data,\"?>\");\n-\tcase ENTITY_REFERENCE_NODE:\n-\t\treturn buf.push( '&',node.nodeName,';');\n-\t//case ENTITY_NODE:\n-\t//case NOTATION_NODE:\n-\tdefault:\n-\t\tbuf.push('??',node.nodeName);\n-\t}\n+        if (!visibleNamespaces) {\n+                visibleNamespaces = [];\n+        }\n+\n+        if(nodeFilter){\n+                node = nodeFilter(node);\n+                if(node){\n+                        if(typeof node == 'string'){\n+                                buf.push(node);\n+                                return;\n+                        }\n+                }else{\n+                        return;\n+                }\n+                //buf.sort.apply(attrs, attributeSorter);\n+        }\n+\n+        switch(node.nodeType){\n+        case ELEMENT_NODE:\n+                var attrs = node.attributes;\n+                var len = attrs.length;\n+                var child = node.firstChild;\n+                var nodeName = node.tagName;\n+var startVisibleNamespaces = visibleNamespaces.length;\n+var startVisibleNamespaces = visibleNamespaces.length;\n+var startVisibleNamespaces = visibleNamespaces.length;\n+                \n+                isHTML = NAMESPACE.isHTML(node.namespaceURI) || isHTML\n+\n+                var prefixedNodeName = nodeName\n+                if (!isHTML && !node.prefix && node.namespaceURI) {\n+                        var defaultNS\n+                        for (var ai = 0; ai < attrs.length; ai++) {\n+                                if (attrs.item(ai).name === 'xmlns') {\n+                                        defaultNS = attrs.item(ai).value\n+                                        break\n+                                }\n+                        }\n+                        if (defaultNS !== node.namespaceURI) {\n+                                for (var nsi = visibleNamespaces.length - 1; nsi >= 0; nsi--) {\n+                                        var namespace = visibleNamespaces[nsi]\n+                                        if (namespace.namespace === node.namespaceURI) {\n+                                                if (namespace.prefix) {\n+                                                        prefixedNodeName = namespace.prefix + ':' + nodeName\n+                                                }\n+                                                break\n+                                        }\n+                                }\n+                        }\n+                }\n+\n+                buf.push('<', prefixedNodeName);\n+\n+                for(var i=0;i<len;i++){\n+                        // add namespaces for attributes\n+                        var attr = attrs.item(i);\n+                        if (attr.prefix == 'xmlns') {\n+                                visibleNamespaces.push({ prefix: attr.localName, namespace: attr.value });\n+                        } else if (attr.nodeName == 'xmlns') {\n+                                visibleNamespaces.push({ prefix: '', namespace: attr.value });\n+                        }\n+                }\n+\n+                for(var i=0;i<len;i++){\n+                        var attr = attrs.item(i);\n+                        if (needNamespaceDefine(attr,isHTML, visibleNamespaces)) {\n+                                var prefix = attr.prefix||'';\n+                                var uri = attr.namespaceURI;\n+                                var ns = prefix ? ' xmlns:' + prefix : \" xmlns\";\n+                                buf.push(ns, '=\"' , uri , '\"');\n+                                visibleNamespaces.push({ prefix: prefix, namespace:uri });\n+                        }\n+                        serializeToString(attr,buf,isHTML,nodeFilter,visibleNamespaces);\n+                }\n+\n+                // add namespace for current node               \n+                if (nodeName === prefixedNodeName && needNamespaceDefine(node, isHTML, visibleNamespaces)) {\n+                        var prefix = node.prefix||'';\n+                        var uri = node.namespaceURI;\n+                        var ns = prefix ? ' xmlns:' + prefix : \" xmlns\";\n+                        buf.push(ns, '=\"' , uri , '\"');\n+                        visibleNamespaces.push({ prefix: prefix, namespace:uri });\n+                }\n+                \n+                if(child || isHTML && !/^(?:meta|link|img|br|hr|input)$/i.test(nodeName)){\n+                        buf.push('>');\n+                        //if is cdata child node\n+                        if(isHTML && /^script$/i.test(nodeName)){\n+                                while(child){\n+                                        if(child.data){\n+                                                buf.push(child.data);\n+                                        }else{\n+                                                serializeToString(child, buf, isHTML, nodeFilter, visibleNamespaces.slice());\n+                                        }\n+                                        child = child.nextSibling;\n+                                }\n+                        }else\n+                        {\n+                                while(child){\n+                                        serializeToString(child, buf, isHTML, nodeFilter, visibleNamespaces.slice());\n+                                        child = child.nextSibling;\n+                                }\n+                        }\n+                        buf.push('</',prefixedNodeName,'>');\n+                }else{\n+                        buf.push('/>');\n+                }\n+                // remove added visible namespaces\n+                //visibleNamespaces.length = startVisibleNamespaces;\n+                return;\n+        case DOCUMENT_NODE:\n+        case DOCUMENT_FRAGMENT_NODE:\n+                var child = node.firstChild;\n+                while(child){\n+                        serializeToString(child, buf, isHTML, nodeFilter, visibleNamespaces.slice());\n+                        child = child.nextSibling;\n+                }\n+                return;\n+        case ATTRIBUTE_NODE:\n+                /**\n+                 * Well-formedness constraint: No < in Attribute Values\n+                 * The replacement text of any entity referred to directly or indirectly in an attribute value must not contain a <.\n+                 * @see https://www.w3.org/TR/xml/#CleanAttrVals\n+                 * @see https://www.w3.org/TR/xml/#NT-AttValue\n+                 */\n+                return buf.push(' ', node.name, '=\"', node.value.replace(/[<&\"]/g,_xmlEncoder), '\"');\n+        case TEXT_NODE:\n+                /**\n+                 * The ampersand character (&) and the left angle bracket (<) must not appear in their literal form,\n+                 * except when used as markup delimiters, or within a comment, a processing instruction, or a CDATA section.\n+                 * If they are needed elsewhere, they must be escaped using either numeric character references or the strings\n+                 * `&amp;` and `&lt;` respectively.\n+                 * The right angle bracket (>) may be represented using the string \" &gt; \", and must, for compatibility,\n+                 * be escaped using either `&gt;` or a character reference when it appears in the string `]]>` in content,\n+                 * when that string is not marking the end of a CDATA section.\n+                 *\n+                 * In the content of elements, character data is any string of characters\n+                 * which does not contain the start-delimiter of any markup\n+                 * and does not include the CDATA-section-close delimiter, `]]>`.\n+                 *\n+                 * @see https://www.w3.org/TR/xml/#NT-CharData\n+                 */\n+                return buf.push(node.data\n+                        .replace(/[<&]/g,_xmlEncoder)\n+                        .replace(/]]>/g, ']]&gt;')\n+                );\n+        case CDATA_SECTION_NODE:\n+                return buf.push( '<![CDATA[',node.data,']]>');\n+        case COMMENT_NODE:\n+                return buf.push( \"<!--\",node.data,\"-->\");\n+        case DOCUMENT_TYPE_NODE:\n+                var pubid = node.publicId;\n+                var sysid = node.systemId;\n+                buf.push('<!DOCTYPE ',node.name);\n+                if(pubid){\n+                        buf.push(' PUBLIC ', pubid);\n+                        if (sysid && sysid!='.') {\n+                                buf.push(' ', sysid);\n+                        }\n+                        buf.push('>');\n+                }else if(sysid && sysid!='.'){\n+                        buf.push(' SYSTEM ', sysid, '>');\n+                }else{\n+                        var sub = node.internalSubset;\n+                        if(sub){\n+                                buf.push(\" [\",sub,\"]\");\n+                        }\n+                        buf.push(\">\");\n+                }\n+                return;\n+        case PROCESSING_INSTRUCTION_NODE:\n+                return buf.push( \"<?\",node.target,\" \",node.data,\"?>\");\n+        case ENTITY_REFERENCE_NODE:\n+                return buf.push( '&',node.nodeName,';');\n+        //case ENTITY_NODE:\n+        //case NOTATION_NODE:\n+        default:\n+                buf.push('??',node.nodeName);\n+        }\n }\n function importNode(doc,node,deep){\n-\tvar node2;\n-\tswitch (node.nodeType) {\n-\tcase ELEMENT_NODE:\n-\t\tnode2 = node.cloneNode(false);\n-\t\tnode2.ownerDocument = doc;\n-\t\t//var attrs = node2.attributes;\n-\t\t//var len = attrs.length;\n-\t\t//for(var i=0;i<len;i++){\n-\t\t\t//node2.setAttributeNodeNS(importNode(doc,attrs.item(i),deep));\n-\t\t//}\n-\tcase DOCUMENT_FRAGMENT_NODE:\n-\t\tbreak;\n-\tcase ATTRIBUTE_NODE:\n-\t\tdeep = true;\n-\t\tbreak;\n-\t//case ENTITY_REFERENCE_NODE:\n-\t//case PROCESSING_INSTRUCTION_NODE:\n-\t////case TEXT_NODE:\n-\t//case CDATA_SECTION_NODE:\n-\t//case COMMENT_NODE:\n-\t//\tdeep = false;\n-\t//\tbreak;\n-\t//case DOCUMENT_NODE:\n-\t//case DOCUMENT_TYPE_NODE:\n-\t//cannot be imported.\n-\t//case ENTITY_NODE:\n-\t//case NOTATION_NODE\uff1a\n-\t//can not hit in level3\n-\t//default:throw e;\n-\t}\n-\tif(!node2){\n-\t\tnode2 = node.cloneNode(false);//false\n-\t}\n-\tnode2.ownerDocument = doc;\n-\tnode2.parentNode = null;\n-\tif(deep){\n-\t\tvar child = node.firstChild;\n-\t\twhile(child){\n-\t\t\tnode2.appendChild(importNode(doc,child,deep));\n-\t\t\tchild = child.nextSibling;\n-\t\t}\n-\t}\n-\treturn node2;\n+        var node2;\n+        switch (node.nodeType) {\n+        case ELEMENT_NODE:\n+                node2 = node.cloneNode(false);\n+                node2.ownerDocument = doc;\n+                //var attrs = node2.attributes;\n+                //var len = attrs.length;\n+                //for(var i=0;i<len;i++){\n+                        //node2.setAttributeNodeNS(importNode(doc,attrs.item(i),deep));\n+                //}\n+        case DOCUMENT_FRAGMENT_NODE:\n+                break;\n+        case ATTRIBUTE_NODE:\n+                deep = true;\n+                break;\n+        //case ENTITY_REFERENCE_NODE:\n+        //case PROCESSING_INSTRUCTION_NODE:\n+        ////case TEXT_NODE:\n+        //case CDATA_SECTION_NODE:\n+        //case COMMENT_NODE:\n+        //      deep = false;\n+        //      break;\n+        //case DOCUMENT_NODE:\n+        //case DOCUMENT_TYPE_NODE:\n+        //cannot be imported.\n+        //case ENTITY_NODE:\n+        //case NOTATION_NODE\uff1a\n+        //can not hit in level3\n+        //default:throw e;\n+        }\n+        if(!node2){\n+                node2 = node.cloneNode(false);//false\n+        }\n+        node2.ownerDocument = doc;\n+        node2.parentNode = null;\n+        if(deep){\n+                var child = node.firstChild;\n+                while(child){\n+                        node2.appendChild(importNode(doc,child,deep));\n+                        child = child.nextSibling;\n+                }\n+        }\n+        return node2;\n }\n //\n //var _relationMap = {firstChild:1,lastChild:1,previousSibling:1,nextSibling:1,\n-//\t\t\t\t\tattributes:1,childNodes:1,parentNode:1,documentElement:1,doctype,};\n+//                                      attributes:1,childNodes:1,parentNode:1,documentElement:1,doctype,};\n function cloneNode(doc,node,deep){\n-\tvar node2 = new node.constructor();\n-\tfor(var n in node){\n-\t\tvar v = node[n];\n-\t\tif(typeof v != 'object' ){\n-\t\t\tif(v != node2[n]){\n-\t\t\t\tnode2[n] = v;\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif(node.childNodes){\n-\t\tnode2.childNodes = new NodeList();\n-\t}\n-\tnode2.ownerDocument = doc;\n-\tswitch (node2.nodeType) {\n-\tcase ELEMENT_NODE:\n-\t\tvar attrs\t= node.attributes;\n-\t\tvar attrs2\t= node2.attributes = new NamedNodeMap();\n-\t\tvar len = attrs.length\n-\t\tattrs2._ownerElement = node2;\n-\t\tfor(var i=0;i<len;i++){\n-\t\t\tnode2.setAttributeNode(cloneNode(doc,attrs.item(i),true));\n-\t\t}\n-\t\tbreak;;\n-\tcase ATTRIBUTE_NODE:\n-\t\tdeep = true;\n-\t}\n-\tif(deep){\n-\t\tvar child = node.firstChild;\n-\t\twhile(child){\n-\t\t\tnode2.appendChild(cloneNode(doc,child,deep));\n-\t\t\tchild = child.nextSibling;\n-\t\t}\n-\t}\n-\treturn node2;\n+        var node2 = new node.constructor();\n+        for(var n in node){\n+                var v = node[n];\n+                if(typeof v != 'object' ){\n+                        if(v != node2[n]){\n+                                node2[n] = v;\n+                        }\n+                }\n+        }\n+        if(node.childNodes){\n+                node2.childNodes = new NodeList();\n+        }\n+        node2.ownerDocument = doc;\n+        switch (node2.nodeType) {\n+        case ELEMENT_NODE:\n+                var attrs       = node.attributes;\n+                var attrs2      = node2.attributes = new NamedNodeMap();\n+                var len = attrs.length\n+                attrs2._ownerElement = node2;\n+                for(var i=0;i<len;i++){\n+                        node2.setAttributeNode(cloneNode(doc,attrs.item(i),true));\n+                }\n+                break;;\n+        case ATTRIBUTE_NODE:\n+                deep = true;\n+        }\n+        if(deep){\n+                var child = node.firstChild;\n+                while(child){\n+                        node2.appendChild(cloneNode(doc,child,deep));\n+                        child = child.nextSibling;\n+                }\n+        }\n+        return node2;\n }\n \n function __set__(object,key,value){\n-\tobject[key] = value\n+        object[key] = value\n }\n //do dynamic\n try{\n-\tif(Object.defineProperty){\n-\t\tObject.defineProperty(LiveNodeList.prototype,'length',{\n-\t\t\tget:function(){\n-\t\t\t\t_updateLiveList(this);\n-\t\t\t\treturn this.$$length;\n-\t\t\t}\n-\t\t});\n-\n-\t\tObject.defineProperty(Node.prototype,'textContent',{\n-\t\t\tget:function(){\n-\t\t\t\treturn getTextContent(this);\n-\t\t\t},\n-\n-\t\t\tset:function(data){\n-\t\t\t\tswitch(this.nodeType){\n-\t\t\t\tcase ELEMENT_NODE:\n-\t\t\t\tcase DOCUMENT_FRAGMENT_NODE:\n-\t\t\t\t\twhile(this.firstChild){\n-\t\t\t\t\t\tthis.removeChild(this.firstChild);\n-\t\t\t\t\t}\n-\t\t\t\t\tif(data || String(data)){\n-\t\t\t\t\t\tthis.appendChild(this.ownerDocument.createTextNode(data));\n-\t\t\t\t\t}\n-\t\t\t\t\tbreak;\n-\n-\t\t\t\tdefault:\n-\t\t\t\t\tthis.data = data;\n-\t\t\t\t\tthis.value = data;\n-\t\t\t\t\tthis.nodeValue = data;\n-\t\t\t\t}\n-\t\t\t}\n-\t\t})\n-\t\t\n-\t\tfunction getTextContent(node){\n-\t\t\tswitch(node.nodeType){\n-\t\t\tcase ELEMENT_NODE:\n-\t\t\tcase DOCUMENT_FRAGMENT_NODE:\n-\t\t\t\tvar buf = [];\n-\t\t\t\tnode = node.firstChild;\n-\t\t\t\twhile(node){\n-\t\t\t\t\tif(node.nodeType!==7 && node.nodeType !==8){\n-\t\t\t\t\t\tbuf.push(getTextContent(node));\n-\t\t\t\t\t}\n-\t\t\t\t\tnode = node.nextSibling;\n-\t\t\t\t}\n-\t\t\t\treturn buf.join('');\n-\t\t\tdefault:\n-\t\t\t\treturn node.nodeValue;\n-\t\t\t}\n-\t\t}\n-\n-\t\t__set__ = function(object,key,value){\n-\t\t\t//console.log(value)\n-\t\t\tobject['$$'+key] = value\n-\t\t}\n-\t}\n+        if(Object.defineProperty){\n+                Object.defineProperty(LiveNodeList.prototype,'length',{\n+                        get:function(){\n+                                _updateLiveList(this);\n+                                return this.$$length;\n+                        }\n+                });\n+\n+                Object.defineProperty(Node.prototype,'textContent',{\n+                        get:function(){\n+                                return getTextContent(this);\n+                        },\n+\n+                        set:function(data){\n+                                switch(this.nodeType){\n+                                case ELEMENT_NODE:\n+                                case DOCUMENT_FRAGMENT_NODE:\n+                                        while(this.firstChild){\n+                                                this.removeChild(this.firstChild);\n+                                        }\n+                                        if(data || String(data)){\n+                                                this.appendChild(this.ownerDocument.createTextNode(data));\n+                                        }\n+                                        break;\n+\n+                                default:\n+                                        this.data = data;\n+                                        this.value = data;\n+                                        this.nodeValue = data;\n+                                }\n+                        }\n+                })\n+                \n+                function getTextContent(node){\n+                        switch(node.nodeType){\n+                        case ELEMENT_NODE:\n+                        case DOCUMENT_FRAGMENT_NODE:\n+                                var buf = [];\n+                                node = node.firstChild;\n+                                while(node){\n+                                        if(node.nodeType!==7 && node.nodeType !==8){\n+                                                buf.push(getTextContent(node));\n+                                        }\n+                                        node = node.nextSibling;\n+                                }\n+                                return buf.join('');\n+                        default:\n+                                return node.nodeValue;\n+                        }\n+                }\n+\n+                __set__ = function(object,key,value){\n+                        //console.log(value)\n+                        object['$$'+key] = value\n+                }\n+        }\n }catch(e){//ie8\n }\n \n //if(typeof require == 'function'){\n-\texports.DocumentType = DocumentType;\n-\texports.DOMException = DOMException;\n-\texports.DOMImplementation = DOMImplementation;\n-\texports.Element = Element;\n-\texports.Node = Node;\n-\texports.NodeList = NodeList;\n-\texports.XMLSerializer = XMLSerializer;\n+        exports.DocumentType = DocumentType;\n+        exports.DOMException = DOMException;\n+        exports.DOMImplementation = DOMImplementation;\n+        exports.Element = Element;\n+        exports.Node = Node;\n+        exports.NodeList = NodeList;\n+        exports.XMLSerializer = XMLSerializer;\n //}\n"}
{"cve":"CVE-2020-26226:0708", "fix_patch": "diff --git a/lib/hide-sensitive.js b/lib/hide-sensitive.js\nindex 6098496..34e7267 100644\n--- a/lib/hide-sensitive.js\n+++ b/lib/hide-sensitive.js\n@@ -11,7 +11,10 @@ module.exports = (env) => {\n     return /token|password|credential|secret|private/i.test(envVar) && size(env[envVar].trim()) >= SECRET_MIN_SIZE;\n   });\n \n-  const regexp = new RegExp(toReplace.map((envVar) => escapeRegExp(env[envVar])).join('|'), 'g');\n+  const regexp = new RegExp(\n+    toReplace.map((envVar) => `(${escapeRegExp(env[envVar])}|${escapeRegExp(encodeURI(env[envVar]))})`).join('|'),\n+    'gi'\n+  );\n   return (output) =>\n     output && isString(output) && toReplace.length > 0 ? output.toString().replace(regexp, SECRET_REPLACEMENT) : output;\n };\n"}
{"cve":"CVE-2020-26237:0708", "fix_patch": ""}
{"cve":"CVE-2023-41039:0708", "fix_patch": "diff --git a/src/RestrictedPython/Guards.py b/src/RestrictedPython/Guards.py\nindex 9b70ae3..0659ab9 100644\n--- a/src/RestrictedPython/Guards.py\n+++ b/src/RestrictedPython/Guards.py\n@@ -241,14 +241,24 @@ safe_builtins['delattr'] = guarded_delattr\n \n def safer_getattr(object, name, default=None, getattr=getattr):\n     \"\"\"Getattr implementation which prevents using format on string objects.\n-\n     format() is considered harmful:\n     http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\n-\n     \"\"\"\n-    if isinstance(object, str) and name == 'format':\n+    if (\n+        isinstance(object, (str, bytes))\n+        and name in ('format', 'format_map')\n+    ):\n         raise NotImplementedError(\n-            'Using format() on a %s is not safe.' % object.__class__.__name__)\n+            'Using the format*() methods of `str` is not safe')\n+\n+    # see also /cve/state/serial-2023-0/2023-38048.json\n+    if (\n+        isinstance(object, type)\n+        and name in ('format', 'format_map')\n+    ):\n+        raise NotImplementedError(\n+            'Using the format*() methods of `str` is not safe')\n+\n     if name.startswith('_'):\n         raise AttributeError(\n             '\"{name}\" is an invalid attribute name because it '\ndiff --git a/src/RestrictedPython/Utilities.py b/src/RestrictedPython/Utilities.py\nindex 4bc6677..3006763 100644\n--- a/src/RestrictedPython/Utilities.py\n+++ b/src/RestrictedPython/Utilities.py\n@@ -16,9 +16,19 @@ import random\n import string\n \n \n+class _DenyStringFormatter(object):\n+    def __init__(self, module):\n+        self.__module = module\n+    def __getattr__(self, name):\n+        if name == 'Formatter':\n+            raise NotImplementedError(\n+                \"string.Formatter is not safe\"\n+            )\n+        return getattr(self.__module, name)\n+\n utility_builtins = {}\n \n-utility_builtins['string'] = string\n+utility_builtins['string'] = _DenyStringFormatter(string)\n utility_builtins['math'] = math\n utility_builtins['random'] = random\n utility_builtins['whrandom'] = random\n"}
{"cve":"CVE-2023-34457:0708", "fix_patch": ""}
{"cve":"CVE-2018-7753:0708", "fix_patch": "diff --git a/bleach/sanitizer.py b/bleach/sanitizer.py\nindex 81df765..3b158c0 100644\n--- a/bleach/sanitizer.py\n+++ b/bleach/sanitizer.py\n@@ -382,6 +382,8 @@ class BleachSanitizerFilter(sanitizer.Filter):\n \n         return super(BleachSanitizerFilter, self).__init__(source, **kwargs)\n \n+    bad_chars_re = re.compile(u'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-]')\n+\n     def __iter__(self):\n         for token in Filter.__iter__(self):\n             ret = self.sanitize_token(token)\n@@ -510,13 +512,10 @@ class BleachSanitizerFilter(sanitizer.Filter):\n \n                 # Look at attributes that have uri values\n                 if namespaced_name in self.attr_val_is_uri:\n-                    val_unescaped = re.sub(\n-                        \"[`\\000-\\040\\177-\\240\\s]+\",\n-                        '',\n-                        unescape(val)).lower()\n-\n-                    # Remove replacement characters from unescaped characters.\n-                    val_unescaped = val_unescaped.replace(\"\\ufffd\", \"\")\n+                    val_unescaped = self.bad_chars_re.sub('', unescape(val))\n+                    if (re.match(r'^[a-z0-9][-+.a-z0-9]*:', val_unescaped) and\n+                            val_unescaped.split(':')[0].lower() not in self.allowed_protocols):\n+                        continue\n \n                     # Drop attributes with uri values that have protocols that\n                     # aren't allowed\n"}
{"cve":"CVE-2015-1326:0708", "fix_patch": "diff --git a/dbusmock/mockobject.py b/dbusmock/mockobject.py\nindex 29f790e..364203e 100644\n--- a/dbusmock/mockobject.py\n+++ b/dbusmock/mockobject.py\n@@ -40,12 +40,15 @@ if sys.version_info[0] >= 3:\n \n def load_module(name):\n     if os.path.exists(name) and os.path.splitext(name)[1] == '.py':\n+        # Don't write pyc files; they are a security risk in TMP\n+        sys.dont_write_bytecode = True\n         sys.path.insert(0, os.path.dirname(os.path.abspath(name)))\n         try:\n             m = os.path.splitext(os.path.basename(name))[0]\n             module = importlib.import_module(m)\n         finally:\n             sys.path.pop(0)\n+            sys.dont_write_bytecode = False\n \n         return module\n \n"}
{"cve":"CVE-2025-43859:0708", "fix_patch": "diff --git a/h11/_readers.py b/h11/_readers.py\nindex 08a9574..4a9479a 100644\n--- a/h11/_readers.py\n+++ b/h11/_readers.py\n@@ -161,13 +161,18 @@ class ChunkedReader:\n                 return None\n             return EndOfMessage(headers=list(_decode_header_lines(lines)))\n         if self._bytes_to_discard > 0:\n-            data = buf.maybe_extract_at_most(self._bytes_to_discard)\n-            if data is None:\n-                return None\n-            self._bytes_to_discard -= len(data)\n+            # Consume one byte at a time to handle partial reads correctly.\n+            while self._bytes_to_discard > 0 and len(buf) > 0:\n+                byte = buf.maybe_extract_at_most(1)\n+                if byte is None:\n+                    return None # Should not happen due to len(buf) > 0\n+                expected_byte = b\"\\r\\n\"[2 - self._bytes_to_discard]\n+                if byte[0] != expected_byte:\n+                    raise LocalProtocolError(\"Invalid chunk terminator\")\n+                self._bytes_to_discard -= 1\n             if self._bytes_to_discard > 0:\n                 return None\n-            # else, fall through and read some more\n+\n         assert self._bytes_to_discard == 0\n         if self._bytes_in_chunk == 0:\n             # We need to refill our chunk count\n"}
{"cve":"CVE-2024-49750:0708", "fix_patch": "diff --git a/src/snowflake/connector/auth/_auth.py b/src/snowflake/connector/auth/_auth.py\nindex b8aa8f48..4ed8d1a8 100644\n--- a/src/snowflake/connector/auth/_auth.py\n+++ b/src/snowflake/connector/auth/_auth.py\n@@ -243,7 +243,11 @@ class Auth:\n \n         logger.debug(\n             \"body['data']: %s\",\n-            {k: v for (k, v) in body[\"data\"].items() if k != \"PASSWORD\"},\n+            {\n+                k: v\n+                for (k, v) in body[\"data\"].items()\n+                if k not in (\"PASSWORD\", \"PASSCODE\")\n+            },\n         )\n \n         try:\ndiff --git a/src/snowflake/connector/secret_detector.py b/src/snowflake/connector/secret_detector.py\nindex 6633cda6..4f90aff8 100644\n--- a/src/snowflake/connector/secret_detector.py\n+++ b/src/snowflake/connector/secret_detector.py\n@@ -15,7 +15,7 @@ import logging\n import os\n import re\n \n-MIN_TOKEN_LEN = os.getenv(\"MIN_TOKEN_LEN\", 32)\n+MIN_TOKEN_LEN = os.getenv(\"MIN_TOKEN_LEN\", 8)\n MIN_PWD_LEN = os.getenv(\"MIN_PWD_LEN\", 8)\n \n \n@@ -33,22 +33,22 @@ class SecretDetector(logging.Formatter):\n         flags=re.IGNORECASE,\n     )\n     PRIVATE_KEY_PATTERN = re.compile(\n-        r\"-----BEGIN PRIVATE KEY-----\\\\n([a-z0-9/+=\\\\n]{32,})\\\\n-----END PRIVATE KEY-----\",\n+        r\"-----BEGIN(?P<key_type>(?: [A-Z0-9]+)?) PRIVATE KEY-----\\n(?P<b64_key>[a-z0-9/+=\\n]{32,})\\n-----END(?: [A-Z0-9]+)? PRIVATE KEY-----\",\n         flags=re.MULTILINE | re.IGNORECASE,\n     )\n     PRIVATE_KEY_DATA_PATTERN = re.compile(\n-        r'\"privateKeyData\": \"([a-z0-9/+=\\\\n]{10,})\"', flags=re.MULTILINE | re.IGNORECASE\n+        r'\"privateKeyData\": \"([a-z0-9/+=\\n]{10,})\"', flags=re.MULTILINE | re.IGNORECASE\n     )\n     CONNECTION_TOKEN_PATTERN = re.compile(\n-        r\"(token|assertion content)\" r\"([\\'\\\"\\s:=]+)\" r\"([a-z0-9=/_\\-\\+]{8,})\",\n+        r\"(token|assertion content)(['\"\\s:=]+)([a-z0-9=/_\\\\-\\\\+]{8,})\",\n         flags=re.IGNORECASE,\n     )\n \n     PASSWORD_PATTERN = re.compile(\n         r\"(password\"\n         r\"|pwd)\"\n-        r\"([\\'\\\"\\s:=]+)\"\n-        r\"([a-z0-9!\\\"#\\$%&\\\\\\'\\(\\)\\*\\+\\,-\\./:;<=>\\?\\@\\[\\]\\^_`\\{\\|\\}~]{1,})\",\n+        r\"(['\"\\s:=]+)\"\n+        r\"([a-z0-9!\\\"#\\$%&\\'\\(\\)\\*\\+,\\-\\./:;<=>\\?@\\[\\]\\^_`\\{\\|\\}~]{1,})\",\n         flags=re.IGNORECASE,\n     )\n \n@@ -75,7 +75,8 @@ class SecretDetector(logging.Formatter):\n     @staticmethod\n     def mask_private_key(text: str) -> str:\n         return SecretDetector.PRIVATE_KEY_PATTERN.sub(\n-            \"-----BEGIN PRIVATE KEY-----\\\\\\\\nXXXX\\\\\\\\n-----END PRIVATE KEY-----\", text\n+            r\"-----BEGIN\\g<key_type> PRIVATE KEY-----\\n****\\n-----END\\g<key_type> PRIVATE KEY-----\",\n+            text,\n         )\n \n     @staticmethod\n"}
{"cve":"CVE-2023-45809:0708", "fix_patch": "diff --git a/wagtail/admin/views/bulk_action/base_bulk_action.py b/wagtail/admin/views/bulk_action/base_bulk_action.py\nindex d796801d51..0e5217fe66 100644\n--- a/wagtail/admin/views/bulk_action/base_bulk_action.py\n+++ b/wagtail/admin/views/bulk_action/base_bulk_action.py\n@@ -1,6 +1,7 @@\n from abc import ABC, abstractmethod\n \n from django import forms\n+from django.core.exceptions import PermissionDenied\n from django.db import transaction\n from django.shortcuts import get_list_or_404, redirect\n from django.utils.functional import classproperty\n@@ -12,6 +13,14 @@ from wagtail.admin.utils import get_valid_next_url_from_request\n \n \n class BulkAction(ABC, FormView):\n+    permission_required = None\n+\n+    def dispatch(self, request, *args, **kwargs):\n+        if self.permission_required and not request.user.has_perm(\n+            self.permission_required\n+        ):\n+            raise PermissionDenied\n+        return super().dispatch(request, *args, **kwargs)\n     @property\n     @abstractmethod\n     def display_name(self):\ndiff --git a/wagtail/users/views/bulk_actions/user_bulk_action.py b/wagtail/users/views/bulk_actions/user_bulk_action.py\nindex 76d05db58d..6c7813ad3f 100644\n--- a/wagtail/users/views/bulk_actions/user_bulk_action.py\n+++ b/wagtail/users/views/bulk_actions/user_bulk_action.py\n@@ -6,6 +6,7 @@ from wagtail.users.views.users import get_users_filter_query\n \n class UserBulkAction(BulkAction):\n     models = [get_user_model()]\n+    permission_required = \"auth.change_user\"\n \n     def get_all_objects_in_listing_query(self, parent_id):\n         listing_objects = self.model.objects.all().values_list(\"pk\", flat=True)\n"}
{"cve":"CVE-2022-29217:0708", "fix_patch": "diff --git a/jwt/algorithms.py b/jwt/algorithms.py\nindex 739df80..f76a8d2 100644\n--- a/jwt/algorithms.py\n+++ b/jwt/algorithms.py\n@@ -551,27 +551,26 @@ if has_crypto:\n             pass\n \n         def prepare_key(self, key):\n-\n             if isinstance(\n                 key,\n                 (Ed25519PrivateKey, Ed25519PublicKey, Ed448PrivateKey, Ed448PublicKey),\n             ):\n                 return key\n \n-            if isinstance(key, (bytes, str)):\n-                if isinstance(key, str):\n-                    key = key.encode(\"utf-8\")\n-                str_key = key.decode(\"utf-8\")\n+            if not isinstance(key, (bytes, str)):\n+                raise TypeError(\"Expecting a PEM-formatted or OpenSSH key.\")\n \n-                if \"-----BEGIN PUBLIC\" in str_key:\n-                    return load_pem_public_key(key)\n-                if \"-----BEGIN PRIVATE\" in str_key:\n-                    return load_pem_private_key(key, password=None)\n-                if str_key[0:4] == \"ssh-\":\n-                    return load_ssh_public_key(key)\n+            if isinstance(key, str):\n+                key = key.encode(\"utf-8\")\n \n-            raise TypeError(\"Expecting a PEM-formatted or OpenSSH key.\")\n+            if b\"-----BEGIN PUBLIC\" in key:\n+                return load_pem_public_key(key)\n+            if b\"-----BEGIN PRIVATE\" in key:\n+                return load_pem_private_key(key, password=None)\n+            if key.startswith(b\"ssh-\"):\n+                return load_ssh_public_key(key)\n \n+            raise InvalidKeyError(\"Unable to deserialize the given key.\")\n         def sign(self, msg, key):\n             \"\"\"\n             Sign a message ``msg`` using the EdDSA private key ``key``\n"}
{"cve":"CVE-2019-16789:0708", "fix_patch": ""}
{"cve":"CVE-2025-48374:0708", "fix_patch": "diff --git a/pkg/api/config/config.go b/pkg/api/config/config.go\nindex 39f86639..ee6274e3 100644\n--- a/pkg/api/config/config.go\n+++ b/pkg/api/config/config.go\n@@ -1,555 +1,566 @@\n package config\n \n import (\n-\t\"encoding/json\"\n-\t\"os\"\n-\t\"time\"\n+        \"encoding/json\"\n+        \"os\"\n+        \"time\"\n \n-\tdistspec \"github.com/opencontainers/distribution-spec/specs-go\"\n+        distspec \"github.com/opencontainers/distribution-spec/specs-go\"\n \n-\t\"zotregistry.dev/zot/pkg/compat\"\n-\textconf \"zotregistry.dev/zot/pkg/extensions/config\"\n-\tstorageConstants \"zotregistry.dev/zot/pkg/storage/constants\"\n+        \"zotregistry.dev/zot/pkg/compat\"\n+        extconf \"zotregistry.dev/zot/pkg/extensions/config\"\n+        storageConstants \"zotregistry.dev/zot/pkg/storage/constants\"\n )\n \n var (\n-\tCommit     string //nolint: gochecknoglobals\n-\tReleaseTag string //nolint: gochecknoglobals\n-\tBinaryType string //nolint: gochecknoglobals\n-\tGoVersion  string //nolint: gochecknoglobals\n+        Commit     string //nolint: gochecknoglobals\n+        ReleaseTag string //nolint: gochecknoglobals\n+        BinaryType string //nolint: gochecknoglobals\n+        GoVersion  string //nolint: gochecknoglobals\n \n-\topenIDSupportedProviders = [...]string{\"google\", \"gitlab\", \"oidc\"} //nolint: gochecknoglobals\n-\toauth2SupportedProviders = [...]string{\"github\"}                   //nolint: gochecknoglobals\n+        openIDSupportedProviders = [...]string{\"google\", \"gitlab\", \"oidc\"} //nolint: gochecknoglobals\n+        oauth2SupportedProviders = [...]string{\"github\"}                   //nolint: gochecknoglobals\n \n )\n \n type StorageConfig struct {\n-\tRootDirectory string\n-\tDedupe        bool\n-\tRemoteCache   bool\n-\tGC            bool\n-\tCommit        bool\n-\tGCDelay       time.Duration // applied for blobs\n-\tGCInterval    time.Duration\n-\tRetention     ImageRetention\n-\tStorageDriver map[string]interface{} `mapstructure:\",omitempty\"`\n-\tCacheDriver   map[string]interface{} `mapstructure:\",omitempty\"`\n+        RootDirectory string\n+        Dedupe        bool\n+        RemoteCache   bool\n+        GC            bool\n+        Commit        bool\n+        GCDelay       time.Duration // applied for blobs\n+        GCInterval    time.Duration\n+        Retention     ImageRetention\n+        StorageDriver map[string]interface{} `mapstructure:\",omitempty\"`\n+        CacheDriver   map[string]interface{} `mapstructure:\",omitempty\"`\n }\n \n type ImageRetention struct {\n-\tDryRun   bool\n-\tDelay    time.Duration // applied for referrers and untagged\n-\tPolicies []RetentionPolicy\n+        DryRun   bool\n+        Delay    time.Duration // applied for referrers and untagged\n+        Policies []RetentionPolicy\n }\n \n type RetentionPolicy struct {\n-\tRepositories    []string\n-\tDeleteReferrers bool\n-\tDeleteUntagged  *bool\n-\tKeepTags        []KeepTagsPolicy\n+        Repositories    []string\n+        DeleteReferrers bool\n+        DeleteUntagged  *bool\n+        KeepTags        []KeepTagsPolicy\n }\n \n type KeepTagsPolicy struct {\n-\tPatterns                []string\n-\tPulledWithin            *time.Duration\n-\tPushedWithin            *time.Duration\n-\tMostRecentlyPushedCount int\n-\tMostRecentlyPulledCount int\n+        Patterns                []string\n+        PulledWithin            *time.Duration\n+        PushedWithin            *time.Duration\n+        MostRecentlyPushedCount int\n+        MostRecentlyPulledCount int\n }\n \n type TLSConfig struct {\n-\tCert   string\n-\tKey    string\n-\tCACert string\n+        Cert   string\n+        Key    string\n+        CACert string\n }\n \n type AuthHTPasswd struct {\n-\tPath string\n+        Path string\n }\n \n type AuthConfig struct {\n-\tFailDelay         int\n-\tHTPasswd          AuthHTPasswd\n-\tLDAP              *LDAPConfig\n-\tBearer            *BearerConfig\n-\tOpenID            *OpenIDConfig\n-\tAPIKey            bool\n-\tSessionKeysFile   string\n-\tSessionHashKey    []byte `json:\"-\"`\n-\tSessionEncryptKey []byte `json:\"-\"`\n+        FailDelay         int\n+        HTPasswd          AuthHTPasswd\n+        LDAP              *LDAPConfig\n+        Bearer            *BearerConfig\n+        OpenID            *OpenIDConfig\n+        APIKey            bool\n+        SessionKeysFile   string\n+        SessionHashKey    []byte `json:\"-\"`\n+        SessionEncryptKey []byte `json:\"-\"`\n }\n \n type BearerConfig struct {\n-\tRealm   string\n-\tService string\n-\tCert    string\n+        Realm   string\n+        Service string\n+        Cert    string\n }\n \n type SessionKeys struct {\n-\tHashKey    string\n-\tEncryptKey string `mapstructure:\",omitempty\"`\n+        HashKey    string\n+        EncryptKey string `mapstructure:\",omitempty\"`\n }\n \n type OpenIDConfig struct {\n-\tProviders map[string]OpenIDProviderConfig\n+        Providers map[string]OpenIDProviderConfig\n }\n \n type OpenIDProviderConfig struct {\n-\tName         string\n-\tClientID     string\n-\tClientSecret string\n-\tKeyPath      string\n-\tIssuer       string\n-\tScopes       []string\n+        Name         string\n+        ClientID     string\n+        ClientSecret string\n+        KeyPath      string\n+        Issuer       string\n+        Scopes       []string\n }\n \n type MethodRatelimitConfig struct {\n-\tMethod string\n-\tRate   int\n+        Method string\n+        Rate   int\n }\n \n type RatelimitConfig struct {\n-\tRate    *int                    // requests per second\n-\tMethods []MethodRatelimitConfig `mapstructure:\",omitempty\"`\n+        Rate    *int                    // requests per second\n+        Methods []MethodRatelimitConfig `mapstructure:\",omitempty\"`\n }\n \n //nolint:maligned\n type HTTPConfig struct {\n-\tAddress       string\n-\tExternalURL   string `mapstructure:\",omitempty\"`\n-\tPort          string\n-\tAllowOrigin   string // comma separated\n-\tTLS           *TLSConfig\n-\tAuth          *AuthConfig\n-\tAccessControl *AccessControlConfig `mapstructure:\"accessControl,omitempty\"`\n-\tRealm         string\n-\tRatelimit     *RatelimitConfig            `mapstructure:\",omitempty\"`\n-\tCompat        []compat.MediaCompatibility `mapstructure:\",omitempty\"`\n+        Address       string\n+        ExternalURL   string `mapstructure:\",omitempty\"`\n+        Port          string\n+        AllowOrigin   string // comma separated\n+        TLS           *TLSConfig\n+        Auth          *AuthConfig\n+        AccessControl *AccessControlConfig `mapstructure:\"accessControl,omitempty\"`\n+        Realm         string\n+        Ratelimit     *RatelimitConfig            `mapstructure:\",omitempty\"`\n+        Compat        []compat.MediaCompatibility `mapstructure:\",omitempty\"`\n }\n \n type SchedulerConfig struct {\n-\tNumWorkers int\n+        NumWorkers int\n }\n \n // contains the scale-out configuration which is identical for all zot replicas.\n type ClusterConfig struct {\n-\t// contains the \"host:port\" of all the zot instances participating\n-\t// in the cluster.\n-\tMembers []string `json:\"members\" mapstructure:\"members\"`\n+        // contains the \"host:port\" of all the zot instances participating\n+        // in the cluster.\n+        Members []string `json:\"members\" mapstructure:\"members\"`\n \n-\t// contains the hash key that is required for siphash.\n-\t// must be a 128-bit (16-byte) key\n-\t// https://github.com/dchest/siphash?tab=readme-ov-file#func-newkey-byte-hashhash64\n-\tHashKey string `json:\"hashKey\" mapstructure:\"hashKey\"`\n+        // contains the hash key that is required for siphash.\n+        // must be a 128-bit (16-byte) key\n+        // https://github.com/dchest/siphash?tab=readme-ov-file#func-newkey-byte-hashhash64\n+        HashKey string `json:\"hashKey\" mapstructure:\"hashKey\"`\n \n-\t// contains client TLS config.\n-\tTLS *TLSConfig `json:\"tls\" mapstructure:\"tls\"`\n+        // contains client TLS config.\n+        TLS *TLSConfig `json:\"tls\" mapstructure:\"tls\"`\n \n-\t// private field for storing Proxy details such as internal socket list.\n-\tProxy *ClusterRequestProxyConfig `json:\"-\" mapstructure:\"-\"`\n+        // private field for storing Proxy details such as internal socket list.\n+        Proxy *ClusterRequestProxyConfig `json:\"-\" mapstructure:\"-\"`\n }\n \n type ClusterRequestProxyConfig struct {\n-\t// holds the cluster socket (IP:port) derived from the host's\n-\t// interface configuration and the listening port of the HTTP server.\n-\tLocalMemberClusterSocket string\n-\t// index of the local member cluster socket in the members array.\n-\tLocalMemberClusterSocketIndex uint64\n+        // holds the cluster socket (IP:port) derived from the host's\n+        // interface configuration and the listening port of the HTTP server.\n+        LocalMemberClusterSocket string\n+        // index of the local member cluster socket in the members array.\n+        LocalMemberClusterSocketIndex uint64\n }\n \n type LDAPCredentials struct {\n-\tBindDN       string\n-\tBindPassword string\n+        BindDN       string\n+        BindPassword string\n }\n \n type LDAPConfig struct {\n-\tCredentialsFile    string\n-\tPort               int\n-\tInsecure           bool\n-\tStartTLS           bool // if !Insecure, then StartTLS or LDAPs\n-\tSkipVerify         bool\n-\tSubtreeSearch      bool\n-\tAddress            string\n-\tbindDN             string `json:\"-\"`\n-\tbindPassword       string `json:\"-\"`\n-\tUserGroupAttribute string\n-\tBaseDN             string\n-\tUserAttribute      string\n-\tUserFilter         string\n-\tCACert             string\n+        CredentialsFile    string\n+        Port               int\n+        Insecure           bool\n+        StartTLS           bool // if !Insecure, then StartTLS or LDAPs\n+        SkipVerify         bool\n+        SubtreeSearch      bool\n+        Address            string\n+        bindDN             string `json:\"-\"`\n+        bindPassword       string `json:\"-\"`\n+        UserGroupAttribute string\n+        BaseDN             string\n+        UserAttribute      string\n+        UserFilter         string\n+        CACert             string\n }\n \n func (ldapConf *LDAPConfig) BindDN() string {\n-\treturn ldapConf.bindDN\n+        return ldapConf.bindDN\n }\n \n func (ldapConf *LDAPConfig) SetBindDN(bindDN string) *LDAPConfig {\n-\tldapConf.bindDN = bindDN\n+        ldapConf.bindDN = bindDN\n \n-\treturn ldapConf\n+        return ldapConf\n }\n \n func (ldapConf *LDAPConfig) BindPassword() string {\n-\treturn ldapConf.bindPassword\n+        return ldapConf.bindPassword\n }\n \n func (ldapConf *LDAPConfig) SetBindPassword(bindPassword string) *LDAPConfig {\n-\tldapConf.bindPassword = bindPassword\n+        ldapConf.bindPassword = bindPassword\n \n-\treturn ldapConf\n+        return ldapConf\n }\n \n type LogConfig struct {\n-\tLevel  string\n-\tOutput string\n-\tAudit  string\n+        Level  string\n+        Output string\n+        Audit  string\n }\n \n type GlobalStorageConfig struct {\n-\tStorageConfig `mapstructure:\",squash\"`\n-\tSubPaths      map[string]StorageConfig\n+        StorageConfig `mapstructure:\",squash\"`\n+        SubPaths      map[string]StorageConfig\n }\n \n type AccessControlConfig struct {\n-\tRepositories Repositories `json:\"repositories\" mapstructure:\"repositories\"`\n-\tAdminPolicy  Policy\n-\tGroups       Groups\n-\tMetrics      Metrics\n+        Repositories Repositories `json:\"repositories\" mapstructure:\"repositories\"`\n+        AdminPolicy  Policy\n+        Groups       Groups\n+        Metrics      Metrics\n }\n \n func (config *AccessControlConfig) AnonymousPolicyExists() bool {\n-\tif config == nil {\n-\t\treturn false\n-\t}\n+        if config == nil {\n+                return false\n+        }\n \n-\tfor _, repository := range config.Repositories {\n-\t\tif len(repository.AnonymousPolicy) > 0 {\n-\t\t\treturn true\n-\t\t}\n-\t}\n+        for _, repository := range config.Repositories {\n+                if len(repository.AnonymousPolicy) > 0 {\n+                        return true\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n \n type (\n-\tRepositories map[string]PolicyGroup\n-\tGroups       map[string]Group\n+        Repositories map[string]PolicyGroup\n+        Groups       map[string]Group\n )\n \n type Group struct {\n-\tUsers []string\n+        Users []string\n }\n \n type PolicyGroup struct {\n-\tPolicies        []Policy\n-\tDefaultPolicy   []string\n-\tAnonymousPolicy []string\n+        Policies        []Policy\n+        DefaultPolicy   []string\n+        AnonymousPolicy []string\n }\n \n type Policy struct {\n-\tUsers   []string\n-\tActions []string\n-\tGroups  []string\n+        Users   []string\n+        Actions []string\n+        Groups  []string\n }\n \n type Metrics struct {\n-\tUsers []string\n+        Users []string\n }\n \n type Config struct {\n-\tDistSpecVersion string `json:\"distSpecVersion\" mapstructure:\"distSpecVersion\"`\n-\tGoVersion       string\n-\tCommit          string\n-\tReleaseTag      string\n-\tBinaryType      string\n-\tStorage         GlobalStorageConfig\n-\tHTTP            HTTPConfig\n-\tLog             *LogConfig\n-\tExtensions      *extconf.ExtensionConfig\n-\tScheduler       *SchedulerConfig `json:\"scheduler\" mapstructure:\",omitempty\"`\n-\tCluster         *ClusterConfig   `json:\"cluster\"   mapstructure:\",omitempty\"`\n+        DistSpecVersion string `json:\"distSpecVersion\" mapstructure:\"distSpecVersion\"`\n+        GoVersion       string\n+        Commit          string\n+        ReleaseTag      string\n+        BinaryType      string\n+        Storage         GlobalStorageConfig\n+        HTTP            HTTPConfig\n+        Log             *LogConfig\n+        Extensions      *extconf.ExtensionConfig\n+        Scheduler       *SchedulerConfig `json:\"scheduler\" mapstructure:\",omitempty\"`\n+        Cluster         *ClusterConfig   `json:\"cluster\"   mapstructure:\",omitempty\"`\n }\n \n func New() *Config {\n-\treturn &Config{\n-\t\tDistSpecVersion: distspec.Version,\n-\t\tGoVersion:       GoVersion,\n-\t\tCommit:          Commit,\n-\t\tReleaseTag:      ReleaseTag,\n-\t\tBinaryType:      BinaryType,\n-\t\tStorage: GlobalStorageConfig{\n-\t\t\tStorageConfig: StorageConfig{\n-\t\t\t\tDedupe:     true,\n-\t\t\t\tGC:         true,\n-\t\t\t\tGCDelay:    storageConstants.DefaultGCDelay,\n-\t\t\t\tGCInterval: storageConstants.DefaultGCInterval,\n-\t\t\t\tRetention:  ImageRetention{},\n-\t\t\t},\n-\t\t},\n-\t\tHTTP: HTTPConfig{Address: \"127.0.0.1\", Port: \"8080\", Auth: &AuthConfig{FailDelay: 0}},\n-\t\tLog:  &LogConfig{Level: \"debug\"},\n-\t}\n+        return &Config{\n+                DistSpecVersion: distspec.Version,\n+                GoVersion:       GoVersion,\n+                Commit:          Commit,\n+                ReleaseTag:      ReleaseTag,\n+                BinaryType:      BinaryType,\n+                Storage: GlobalStorageConfig{\n+                        StorageConfig: StorageConfig{\n+                                Dedupe:     true,\n+                                GC:         true,\n+                                GCDelay:    storageConstants.DefaultGCDelay,\n+                                GCInterval: storageConstants.DefaultGCInterval,\n+                                Retention:  ImageRetention{},\n+                        },\n+                },\n+                HTTP: HTTPConfig{Address: \"127.0.0.1\", Port: \"8080\", Auth: &AuthConfig{FailDelay: 0}},\n+                Log:  &LogConfig{Level: \"debug\"},\n+        }\n }\n \n func (expConfig StorageConfig) ParamsEqual(actConfig StorageConfig) bool {\n-\treturn expConfig.GC == actConfig.GC && expConfig.Dedupe == actConfig.Dedupe &&\n-\t\texpConfig.GCDelay == actConfig.GCDelay && expConfig.GCInterval == actConfig.GCInterval\n+        return expConfig.GC == actConfig.GC && expConfig.Dedupe == actConfig.Dedupe &&\n+                expConfig.GCDelay == actConfig.GCDelay && expConfig.GCInterval == actConfig.GCInterval\n }\n \n // SameFile compare two files.\n // This method will first do the stat of two file and compare using os.SameFile method.\n func SameFile(str1, str2 string) (bool, error) {\n-\tsFile, err := os.Stat(str1)\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n+        sFile, err := os.Stat(str1)\n+        if err != nil {\n+                return false, err\n+        }\n \n-\ttFile, err := os.Stat(str2)\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n+        tFile, err := os.Stat(str2)\n+        if err != nil {\n+                return false, err\n+        }\n \n-\treturn os.SameFile(sFile, tFile), nil\n+        return os.SameFile(sFile, tFile), nil\n }\n \n func DeepCopy(src, dst interface{}) error {\n-\tbytes, err := json.Marshal(src)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        bytes, err := json.Marshal(src)\n+        if err != nil {\n+                return err\n+        }\n \n-\terr = json.Unmarshal(bytes, dst)\n+        err = json.Unmarshal(bytes, dst)\n \n-\treturn err\n+        return err\n }\n \n // Sanitize makes a sanitized copy of the config removing any secrets.\n func (c *Config) Sanitize() *Config {\n-\tsanitizedConfig := &Config{}\n-\n-\tif err := DeepCopy(c, sanitizedConfig); err != nil {\n-\t\tpanic(err)\n-\t}\n-\n-\tif c.HTTP.Auth != nil && c.HTTP.Auth.LDAP != nil && c.HTTP.Auth.LDAP.bindPassword != \"\" {\n-\t\tsanitizedConfig.HTTP.Auth.LDAP = &LDAPConfig{}\n-\n-\t\tif err := DeepCopy(c.HTTP.Auth.LDAP, sanitizedConfig.HTTP.Auth.LDAP); err != nil {\n-\t\t\tpanic(err)\n-\t\t}\n-\n-\t\tsanitizedConfig.HTTP.Auth.LDAP.bindPassword = \"******\"\n-\t}\n-\n-\tif c.IsEventRecorderEnabled() {\n-\t\tfor i, sink := range c.Extensions.Events.Sinks {\n-\t\t\tif sink.Credentials == nil {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tif err := DeepCopy(&c.Extensions.Events.Sinks[i], &sanitizedConfig.Extensions.Events.Sinks[i]); err != nil {\n-\t\t\t\tpanic(err)\n-\t\t\t}\n-\n-\t\t\tsanitizedConfig.Extensions.Events.Sinks[i].Credentials.Password = \"******\"\n-\t\t}\n-\t}\n-\n-\treturn sanitizedConfig\n+        sanitizedConfig := &Config{}\n+\n+        if err := DeepCopy(c, sanitizedConfig); err != nil {\n+                panic(err)\n+        }\n+\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.LDAP != nil && c.HTTP.Auth.LDAP.bindPassword != \"\" {\n+                sanitizedConfig.HTTP.Auth.LDAP = &LDAPConfig{}\n+\n+                if err := DeepCopy(c.HTTP.Auth.LDAP, sanitizedConfig.HTTP.Auth.LDAP); err != nil {\n+                        panic(err)\n+                }\n+\n+                sanitizedConfig.HTTP.Auth.LDAP.bindPassword = \"******\"\n+        }\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+                for providerName, providerConfig := range c.HTTP.Auth.OpenID.Providers {\n+                        if providerConfig.ClientSecret != \"\" {\n+                                sanitizedProviderConfig := OpenIDProviderConfig{}\n+                                if err := DeepCopy(&providerConfig, &sanitizedProviderConfig); err != nil {\n+                                        panic(err)\n+                                }\n+                                sanitizedProviderConfig.ClientSecret = \"******\"\n+                                sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = sanitizedProviderConfig\n+                        }\n+                }\n+        }\n+        if c.IsEventRecorderEnabled() {\n+                for i, sink := range c.Extensions.Events.Sinks {\n+                        if sink.Credentials == nil {\n+                                continue\n+                        }\n+\n+                        if err := DeepCopy(&c.Extensions.Events.Sinks[i], &sanitizedConfig.Extensions.Events.Sinks[i]); err != nil {\n+                                panic(err)\n+                        }\n+\n+                        sanitizedConfig.Extensions.Events.Sinks[i].Credentials.Password = \"******\"\n+                }\n+        }\n+\n+        return sanitizedConfig\n }\n \n func (c *Config) IsLdapAuthEnabled() bool {\n-\tif c.HTTP.Auth != nil && c.HTTP.Auth.LDAP != nil {\n-\t\treturn true\n-\t}\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.LDAP != nil {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsAuthzEnabled() bool {\n-\treturn c.HTTP.AccessControl != nil\n+        return c.HTTP.AccessControl != nil\n }\n \n func (c *Config) IsMTLSAuthEnabled() bool {\n-\tif c.HTTP.TLS != nil &&\n-\t\tc.HTTP.TLS.Key != \"\" &&\n-\t\tc.HTTP.TLS.Cert != \"\" &&\n-\t\tc.HTTP.TLS.CACert != \"\" &&\n-\t\t!c.IsBasicAuthnEnabled() &&\n-\t\t!c.HTTP.AccessControl.AnonymousPolicyExists() {\n-\t\treturn true\n-\t}\n+        if c.HTTP.TLS != nil &&\n+                c.HTTP.TLS.Key != \"\" &&\n+                c.HTTP.TLS.Cert != \"\" &&\n+                c.HTTP.TLS.CACert != \"\" &&\n+                !c.IsBasicAuthnEnabled() &&\n+                !c.HTTP.AccessControl.AnonymousPolicyExists() {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsHtpasswdAuthEnabled() bool {\n-\tif c.HTTP.Auth != nil && c.HTTP.Auth.HTPasswd.Path != \"\" {\n-\t\treturn true\n-\t}\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.HTPasswd.Path != \"\" {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsBearerAuthEnabled() bool {\n-\tif c.HTTP.Auth != nil &&\n-\t\tc.HTTP.Auth.Bearer != nil &&\n-\t\tc.HTTP.Auth.Bearer.Cert != \"\" &&\n-\t\tc.HTTP.Auth.Bearer.Realm != \"\" &&\n-\t\tc.HTTP.Auth.Bearer.Service != \"\" {\n-\t\treturn true\n-\t}\n+        if c.HTTP.Auth != nil &&\n+                c.HTTP.Auth.Bearer != nil &&\n+                c.HTTP.Auth.Bearer.Cert != \"\" &&\n+                c.HTTP.Auth.Bearer.Realm != \"\" &&\n+                c.HTTP.Auth.Bearer.Service != \"\" {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsOpenIDAuthEnabled() bool {\n-\tif c.HTTP.Auth != nil &&\n-\t\tc.HTTP.Auth.OpenID != nil {\n-\t\tfor provider := range c.HTTP.Auth.OpenID.Providers {\n-\t\t\tif isOpenIDAuthProviderEnabled(c, provider) {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t}\n-\t}\n+        if c.HTTP.Auth != nil &&\n+                c.HTTP.Auth.OpenID != nil {\n+                for provider := range c.HTTP.Auth.OpenID.Providers {\n+                        if isOpenIDAuthProviderEnabled(c, provider) {\n+                                return true\n+                        }\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsAPIKeyEnabled() bool {\n-\tif c.HTTP.Auth != nil && c.HTTP.Auth.APIKey {\n-\t\treturn true\n-\t}\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.APIKey {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsBasicAuthnEnabled() bool {\n-\tif c.IsHtpasswdAuthEnabled() || c.IsLdapAuthEnabled() ||\n-\t\tc.IsOpenIDAuthEnabled() || c.IsAPIKeyEnabled() {\n-\t\treturn true\n-\t}\n+        if c.IsHtpasswdAuthEnabled() || c.IsLdapAuthEnabled() ||\n+                c.IsOpenIDAuthEnabled() || c.IsAPIKeyEnabled() {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func isOpenIDAuthProviderEnabled(config *Config, provider string) bool {\n-\tif providerConfig, ok := config.HTTP.Auth.OpenID.Providers[provider]; ok {\n-\t\tif IsOpenIDSupported(provider) {\n-\t\t\tif providerConfig.ClientID != \"\" || providerConfig.Issuer != \"\" ||\n-\t\t\t\tlen(providerConfig.Scopes) > 0 {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t} else if IsOauth2Supported(provider) {\n-\t\t\tif providerConfig.ClientID != \"\" || len(providerConfig.Scopes) > 0 {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn false\n+        if providerConfig, ok := config.HTTP.Auth.OpenID.Providers[provider]; ok {\n+                if IsOpenIDSupported(provider) {\n+                        if providerConfig.ClientID != \"\" || providerConfig.Issuer != \"\" ||\n+                                len(providerConfig.Scopes) > 0 {\n+                                return true\n+                        }\n+                } else if IsOauth2Supported(provider) {\n+                        if providerConfig.ClientID != \"\" || len(providerConfig.Scopes) > 0 {\n+                                return true\n+                        }\n+                }\n+        }\n+\n+        return false\n }\n \n func (c *Config) IsMetricsEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Metrics != nil && *c.Extensions.Metrics.Enable\n+        return c.Extensions != nil && c.Extensions.Metrics != nil && *c.Extensions.Metrics.Enable\n }\n \n func (c *Config) IsSearchEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Search != nil && *c.Extensions.Search.Enable\n+        return c.Extensions != nil && c.Extensions.Search != nil && *c.Extensions.Search.Enable\n }\n \n func (c *Config) IsCveScanningEnabled() bool {\n-\treturn c.IsSearchEnabled() && c.Extensions.Search.CVE != nil\n+        return c.IsSearchEnabled() && c.Extensions.Search.CVE != nil\n }\n \n func (c *Config) IsUIEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.UI != nil && *c.Extensions.UI.Enable\n+        return c.Extensions != nil && c.Extensions.UI != nil && *c.Extensions.UI.Enable\n }\n \n func (c *Config) AreUserPrefsEnabled() bool {\n-\treturn c.IsSearchEnabled() && c.IsUIEnabled()\n+        return c.IsSearchEnabled() && c.IsUIEnabled()\n }\n \n func (c *Config) IsMgmtEnabled() bool {\n-\treturn c.IsSearchEnabled()\n+        return c.IsSearchEnabled()\n }\n \n func (c *Config) IsImageTrustEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Trust != nil && *c.Extensions.Trust.Enable\n+        return c.Extensions != nil && c.Extensions.Trust != nil && *c.Extensions.Trust.Enable\n }\n \n // check if tags retention is enabled.\n func (c *Config) IsRetentionEnabled() bool {\n-\tvar needsMetaDB bool\n+        var needsMetaDB bool\n \n-\tfor _, retentionPolicy := range c.Storage.Retention.Policies {\n-\t\tfor _, tagRetentionPolicy := range retentionPolicy.KeepTags {\n-\t\t\tif c.isTagsRetentionEnabled(tagRetentionPolicy) {\n-\t\t\t\tneedsMetaDB = true\n-\t\t\t}\n-\t\t}\n-\t}\n+        for _, retentionPolicy := range c.Storage.Retention.Policies {\n+                for _, tagRetentionPolicy := range retentionPolicy.KeepTags {\n+                        if c.isTagsRetentionEnabled(tagRetentionPolicy) {\n+                                needsMetaDB = true\n+                        }\n+                }\n+        }\n \n-\tfor _, subpath := range c.Storage.SubPaths {\n-\t\tfor _, retentionPolicy := range subpath.Retention.Policies {\n-\t\t\tfor _, tagRetentionPolicy := range retentionPolicy.KeepTags {\n-\t\t\t\tif c.isTagsRetentionEnabled(tagRetentionPolicy) {\n-\t\t\t\t\tneedsMetaDB = true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n+        for _, subpath := range c.Storage.SubPaths {\n+                for _, retentionPolicy := range subpath.Retention.Policies {\n+                        for _, tagRetentionPolicy := range retentionPolicy.KeepTags {\n+                                if c.isTagsRetentionEnabled(tagRetentionPolicy) {\n+                                        needsMetaDB = true\n+                                }\n+                        }\n+                }\n+        }\n \n-\treturn needsMetaDB\n+        return needsMetaDB\n }\n \n func (c *Config) isTagsRetentionEnabled(tagRetentionPolicy KeepTagsPolicy) bool {\n-\tif tagRetentionPolicy.MostRecentlyPulledCount != 0 ||\n-\t\ttagRetentionPolicy.MostRecentlyPushedCount != 0 ||\n-\t\ttagRetentionPolicy.PulledWithin != nil ||\n-\t\ttagRetentionPolicy.PushedWithin != nil {\n-\t\treturn true\n-\t}\n+        if tagRetentionPolicy.MostRecentlyPulledCount != 0 ||\n+                tagRetentionPolicy.MostRecentlyPushedCount != 0 ||\n+                tagRetentionPolicy.PulledWithin != nil ||\n+                tagRetentionPolicy.PushedWithin != nil {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsCosignEnabled() bool {\n-\treturn c.IsImageTrustEnabled() && c.Extensions.Trust.Cosign\n+        return c.IsImageTrustEnabled() && c.Extensions.Trust.Cosign\n }\n \n func (c *Config) IsNotationEnabled() bool {\n-\treturn c.IsImageTrustEnabled() && c.Extensions.Trust.Notation\n+        return c.IsImageTrustEnabled() && c.Extensions.Trust.Notation\n }\n \n func (c *Config) IsSyncEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Sync != nil && *c.Extensions.Sync.Enable\n+        return c.Extensions != nil && c.Extensions.Sync != nil && *c.Extensions.Sync.Enable\n }\n \n func (c *Config) IsCompatEnabled() bool {\n-\treturn len(c.HTTP.Compat) > 0\n+        return len(c.HTTP.Compat) > 0\n }\n \n func (c *Config) IsEventRecorderEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Events != nil && *c.Extensions.Events.Enable\n+        return c.Extensions != nil && c.Extensions.Events != nil && *c.Extensions.Events.Enable\n }\n \n func IsOpenIDSupported(provider string) bool {\n-\tfor _, supportedProvider := range openIDSupportedProviders {\n-\t\tif supportedProvider == provider {\n-\t\t\treturn true\n-\t\t}\n-\t}\n+        for _, supportedProvider := range openIDSupportedProviders {\n+                if supportedProvider == provider {\n+                        return true\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n \n func IsOauth2Supported(provider string) bool {\n-\tfor _, supportedProvider := range oauth2SupportedProviders {\n-\t\tif supportedProvider == provider {\n-\t\t\treturn true\n-\t\t}\n-\t}\n+        for _, supportedProvider := range oauth2SupportedProviders {\n+                if supportedProvider == provider {\n+                        return true\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n"}
{"cve":"CVE-2025-24882:0708", "fix_patch": ""}
{"cve":"CVE-2025-24806:0708", "fix_patch": "diff --git a/internal/handlers/handler_firstfactor.go b/internal/handlers/handler_firstfactor.go\nindex 85b487aa3..af7f5d477 100644\n--- a/internal/handlers/handler_firstfactor.go\n+++ b/internal/handlers/handler_firstfactor.go\n@@ -1,159 +1,165 @@\n package handlers\n \n import (\n-\t\"errors\"\n-\t\"time\"\n+        \"errors\"\n+        \"time\"\n \n-\t\"github.com/authelia/authelia/v4/internal/middlewares\"\n-\t\"github.com/authelia/authelia/v4/internal/regulation\"\n+        \"github.com/authelia/authelia/v4/internal/authentication\"\n+        \"github.com/authelia/authelia/v4/internal/middlewares\"\n+        \"github.com/authelia/authelia/v4/internal/regulation\"\n )\n \n // FirstFactorPOST is the handler performing the first factory.\n //\n //nolint:gocyclo // TODO: Consider refactoring time permitting.\n func FirstFactorPOST(delayFunc middlewares.TimingAttackDelayFunc) middlewares.RequestHandler {\n-\treturn func(ctx *middlewares.AutheliaCtx) {\n-\t\tvar successful bool\n+        return func(ctx *middlewares.AutheliaCtx) {\n+                var successful bool\n+                var details *authentication.UserDetails\n+                var errDetails error\n \n-\t\trequestTime := time.Now()\n+                requestTime := time.Now()\n \n-\t\tif delayFunc != nil {\n-\t\t\tdefer delayFunc(ctx, requestTime, &successful)\n-\t\t}\n+                if delayFunc != nil {\n+                        defer delayFunc(ctx, requestTime, &successful)\n+                }\n \n-\t\tbodyJSON := bodyFirstFactorRequest{}\n+                bodyJSON := bodyFirstFactorRequest{}\n \n-\t\tif err := ctx.ParseBody(&bodyJSON); err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrParseRequestBody, regulation.AuthType1FA)\n+                if err := ctx.ParseBody(&bodyJSON); err != nil {\n+                        ctx.Logger.WithError(err).Errorf(logFmtErrParseRequestBody, regulation.AuthType1FA)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\treturn\n-\t\t}\n+                        return\n+                }\n \n-\t\tif bannedUntil, err := ctx.Providers.Regulator.Regulate(ctx, bodyJSON.Username); err != nil {\n-\t\t\tif errors.Is(err, regulation.ErrUserIsBanned) {\n-\t\t\t\t_ = markAuthenticationAttempt(ctx, false, &bannedUntil, bodyJSON.Username, regulation.AuthType1FA, nil)\n+                details, errDetails = ctx.Providers.UserProvider.GetDetails(bodyJSON.Username)\n+                if errDetails == nil {\n+                        bodyJSON.Username = details.Username\n+                }\n \n-\t\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                if bannedUntil, err := ctx.Providers.Regulator.Regulate(ctx, bodyJSON.Username); err != nil {\n+                        if errors.Is(err, regulation.ErrUserIsBanned) {\n+                                _ = markAuthenticationAttempt(ctx, false, &bannedUntil, bodyJSON.Username, regulation.AuthType1FA, nil)\n \n-\t\t\t\treturn\n-\t\t\t}\n+                                respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrRegulationFail, regulation.AuthType1FA, bodyJSON.Username)\n+                                return\n+                        }\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                        ctx.Logger.WithError(err).Errorf(logFmtErrRegulationFail, regulation.AuthType1FA, bodyJSON.Username)\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tuserPasswordOk, err := ctx.Providers.UserProvider.CheckUserPassword(bodyJSON.Username, bodyJSON.Password)\n-\t\tif err != nil {\n-\t\t\t_ = markAuthenticationAttempt(ctx, false, nil, bodyJSON.Username, regulation.AuthType1FA, err)\n+                        return\n+                }\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                userPasswordOk, err := ctx.Providers.UserProvider.CheckUserPassword(bodyJSON.Username, bodyJSON.Password)\n+                if err != nil {\n+                        _ = markAuthenticationAttempt(ctx, false, nil, bodyJSON.Username, regulation.AuthType1FA, err)\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tif !userPasswordOk {\n-\t\t\t_ = markAuthenticationAttempt(ctx, false, nil, bodyJSON.Username, regulation.AuthType1FA, nil)\n+                        return\n+                }\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                if !userPasswordOk {\n+                        _ = markAuthenticationAttempt(ctx, false, nil, bodyJSON.Username, regulation.AuthType1FA, nil)\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tif err = markAuthenticationAttempt(ctx, true, nil, bodyJSON.Username, regulation.AuthType1FA, nil); err != nil {\n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                        return\n+                }\n \n-\t\t\treturn\n-\t\t}\n+                if err = markAuthenticationAttempt(ctx, true, nil, bodyJSON.Username, regulation.AuthType1FA, nil); err != nil {\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tprovider, err := ctx.GetSessionProvider()\n-\t\tif err != nil {\n-\t\t\tctx.Logger.WithError(err).Error(\"Failed to get session provider during 1FA attempt\")\n+                        return\n+                }\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                provider, err := ctx.GetSessionProvider()\n+                if err != nil {\n+                        ctx.Logger.WithError(err).Error(\"Failed to get session provider during 1FA attempt\")\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tuserSession, err := provider.GetSession(ctx.RequestCtx)\n-\t\tif err != nil {\n-\t\t\tctx.Logger.Errorf(\"%s\", err)\n+                        return\n+                }\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                userSession, err := provider.GetSession(ctx.RequestCtx)\n+                if err != nil {\n+                        ctx.Logger.Errorf(\"%s\", err)\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tnewSession := provider.NewDefaultUserSession()\n+                        return\n+                }\n \n-\t\t// Reset all values from previous session except OIDC workflow before regenerating the cookie.\n-\t\tif err = ctx.SaveSession(newSession); err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrSessionReset, regulation.AuthType1FA, bodyJSON.Username)\n+                newSession := provider.NewDefaultUserSession()\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                // Reset all values from previous session except OIDC workflow before regenerating the cookie.\n+                if err = ctx.SaveSession(newSession); err != nil {\n+                        ctx.Logger.WithError(err).Errorf(logFmtErrSessionReset, regulation.AuthType1FA, bodyJSON.Username)\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tif err = ctx.RegenerateSession(); err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrSessionRegenerate, regulation.AuthType1FA, bodyJSON.Username)\n+                        return\n+                }\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                if err = ctx.RegenerateSession(); err != nil {\n+                        ctx.Logger.WithError(err).Errorf(logFmtErrSessionRegenerate, regulation.AuthType1FA, bodyJSON.Username)\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t// Check if bodyJSON.KeepMeLoggedIn can be deref'd and derive the value based on the configuration and JSON data.\n-\t\tkeepMeLoggedIn := !provider.Config.DisableRememberMe && bodyJSON.KeepMeLoggedIn != nil && *bodyJSON.KeepMeLoggedIn\n+                        return\n+                }\n \n-\t\t// Set the cookie to expire if remember me is enabled and the user has asked us to.\n-\t\tif keepMeLoggedIn {\n-\t\t\terr = provider.UpdateExpiration(ctx.RequestCtx, provider.Config.RememberMe)\n-\t\t\tif err != nil {\n-\t\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrSessionSave, \"updated expiration\", regulation.AuthType1FA, logFmtActionAuthentication, bodyJSON.Username)\n+                // Check if bodyJSON.KeepMeLoggedIn can be deref'd and derive the value based on the configuration and JSON data.\n+                keepMeLoggedIn := !provider.Config.DisableRememberMe && bodyJSON.KeepMeLoggedIn != nil && *bodyJSON.KeepMeLoggedIn\n \n-\t\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                // Set the cookie to expire if remember me is enabled and the user has asked us to.\n+                if keepMeLoggedIn {\n+                        err = provider.UpdateExpiration(ctx.RequestCtx, provider.Config.RememberMe)\n+                        if err != nil {\n+                                ctx.Logger.WithError(err).Errorf(logFmtErrSessionSave, \"updated expiration\", regulation.AuthType1FA, logFmtActionAuthentication, bodyJSON.Username)\n \n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n+                                respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t// Get the details of the given user from the user provider.\n-\t\tuserDetails, err := ctx.Providers.UserProvider.GetDetails(bodyJSON.Username)\n-\t\tif err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrObtainProfileDetails, regulation.AuthType1FA, bodyJSON.Username)\n+                                return\n+                        }\n+                }\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                if errDetails != nil {\n+                        ctx.Logger.WithError(errDetails).Errorf(logFmtErrObtainProfileDetails, regulation.AuthType1FA, bodyJSON.Username)\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tctx.Logger.Tracef(logFmtTraceProfileDetails, bodyJSON.Username, userDetails.Groups, userDetails.Emails)\n+                        return\n+                }\n \n-\t\tuserSession.SetOneFactor(ctx.Clock.Now(), userDetails, keepMeLoggedIn)\n+                ctx.Logger.Tracef(logFmtTraceProfileDetails, bodyJSON.Username, details.Groups, details.Emails)\n \n-\t\tif ctx.Configuration.AuthenticationBackend.RefreshInterval.Update() {\n-\t\t\tuserSession.RefreshTTL = ctx.Clock.Now().Add(ctx.Configuration.AuthenticationBackend.RefreshInterval.Value())\n-\t\t}\n+                userSession.SetOneFactor(ctx.Clock.Now(), details, keepMeLoggedIn)\n \n-\t\tif err = ctx.SaveSession(userSession); err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrSessionSave, \"updated profile\", regulation.AuthType1FA, logFmtActionAuthentication, bodyJSON.Username)\n+                if ctx.Configuration.AuthenticationBackend.RefreshInterval.Update() {\n+                        userSession.RefreshTTL = ctx.Clock.Now().Add(ctx.Configuration.AuthenticationBackend.RefreshInterval.Value())\n+                }\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                if err = ctx.SaveSession(userSession); err != nil {\n+                        ctx.Logger.WithError(err).Errorf(logFmtErrSessionSave, \"updated profile\", regulation.AuthType1FA, logFmtActionAuthentication, bodyJSON.Username)\n \n-\t\t\treturn\n-\t\t}\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tsuccessful = true\n+                        return\n+                }\n \n-\t\tif bodyJSON.Workflow == workflowOpenIDConnect {\n-\t\t\thandleOIDCWorkflowResponse(ctx, &userSession, bodyJSON.TargetURL, bodyJSON.WorkflowID)\n-\t\t} else {\n-\t\t\tHandle1FAResponse(ctx, bodyJSON.TargetURL, bodyJSON.RequestMethod, userSession.Username, userSession.Groups)\n-\t\t}\n-\t}\n+                successful = true\n+\n+                if bodyJSON.Workflow == workflowOpenIDConnect {\n+                        handleOIDCWorkflowResponse(ctx, &userSession, bodyJSON.TargetURL, bodyJSON.WorkflowID)\n+                } else {\n+                        Handle1FAResponse(ctx, bodyJSON.TargetURL, bodyJSON.RequestMethod, userSession.Username, userSession.Groups)\n+                }\n+        }\n }\n"}
{"cve":"CVE-2024-56362:0708", "fix_patch": ""}
{"cve":"CVE-2024-52309:0708", "fix_patch": "diff --git a/internal/common/actions.go b/internal/common/actions.go\nindex 1da74b8a..033e9572 100644\n--- a/internal/common/actions.go\n+++ b/internal/common/actions.go\n@@ -15,59 +15,69 @@\n package common\n \n import (\n-\t\"bytes\"\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os/exec\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"slices\"\n-\t\"strings\"\n-\t\"sync/atomic\"\n-\t\"time\"\n-\n-\t\"github.com/sftpgo/sdk\"\n-\t\"github.com/sftpgo/sdk/plugin/notifier\"\n-\n-\t\"github.com/drakkan/sftpgo/v2/internal/command\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/httpclient\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/logger\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/plugin\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/util\"\n+        \"bytes\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os/exec\"\n+        \"path\"\n+        \"path/filepath\"\n+        \"slices\"\n+        \"strings\"\n+        \"sync/atomic\"\n+        \"time\"\n+\n+        \"github.com/sftpgo/sdk\"\n+        \"github.com/sftpgo/sdk/plugin/notifier\"\n+\n+        \"github.com/drakkan/sftpgo/v2/internal/command\"\n+        \"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n+        \"github.com/drakkan/sftpgo/v2/internal/httpclient\"\n+        \"github.com/drakkan/sftpgo/v2/internal/logger\"\n+        \"github.com/drakkan/sftpgo/v2/internal/plugin\"\n+        \"github.com/drakkan/sftpgo/v2/internal/util\"\n )\n \n var (\n-\terrUnexpectedHTTResponse = errors.New(\"unexpected HTTP hook response code\")\n-\thooksConcurrencyGuard    = make(chan struct{}, 150)\n-\tactiveHooks              atomic.Int32\n+        errUnexpectedHTTResponse = errors.New(\"unexpected HTTP hook response code\")\n+        hooksConcurrencyGuard    = make(chan struct{}, 150)\n+        activeHooks              atomic.Int32\n )\n \n func startNewHook() {\n-\tactiveHooks.Add(1)\n-\thooksConcurrencyGuard <- struct{}{}\n+        activeHooks.Add(1)\n+        hooksConcurrencyGuard <- struct{}{}\n }\n \n func hookEnded() {\n-\tactiveHooks.Add(-1)\n-\t<-hooksConcurrencyGuard\n+        activeHooks.Add(-1)\n+        <-hooksConcurrencyGuard\n }\n \n // ProtocolActions defines the action to execute on file operations and SSH commands\n type ProtocolActions struct {\n-\t// Valid values are download, upload, pre-delete, delete, rename, ssh_cmd. Empty slice to disable\n-\tExecuteOn []string `json:\"execute_on\" mapstructure:\"execute_on\"`\n-\t// Actions to be performed synchronously.\n-\t// The pre-delete action is always executed synchronously while the other ones are asynchronous.\n-\t// Executing an action synchronously means that SFTPGo will not return a result code to the client\n-\t// (which is waiting for it) until your hook have completed its execution.\n-\tExecuteSync []string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n-\t// Absolute path to an external program or an HTTP URL\n-\tHook string `json:\"hook\" mapstructure:\"hook\"`\n+        // Valid values are download, upload, pre-delete, delete, rename, ssh_cmd. Empty slice to disable\n+        ExecuteOn []string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+        // Actions to be performed synchronously.\n+        // The pre-delete action is always executed synchronously while the other ones are asynchronous.\n+        // Executing an action synchronously means that SFTPGo will not return a result code to the client\n+        // (which is waiting for it) until your hook have completed its execution.\n+        ExecuteSync []string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// Absolute path to an external program or an HTTP URL\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Commands defines the external command settings for actions\n+Commands CommandsConfig `json:\"commands\" mapstructure:\"commands\"`\n+}\n+// CommandsConfig defines the settings for external commands\n+type CommandsConfig struct {\n+// Allowed defines the list of the allowed commands that can be configured in the WebAdmin UI\n+Allowed []string `json:\"allowed\" mapstructure:\"allowed\"`\n+// Disable allows to disable external commands execution. If disabled, any configured\n+// command will be not executed\n+Disabled bool `json:\"disabled\" mapstructure:\"disabled\"`\n }\n \n var actionHandler ActionHandler = &defaultActionHandler{}\n@@ -76,7 +86,7 @@ var actionHandler ActionHandler = &defaultActionHandler{}\n //\n // Do NOT call this function after application initialization.\n func InitializeActionHandler(handler ActionHandler) {\n-\tactionHandler = handler\n+        actionHandler = handler\n }\n \n // ExecutePreAction executes a pre-* action and returns the result.\n@@ -85,272 +95,281 @@ func InitializeActionHandler(handler ActionHandler) {\n // - 1 executed using an external hook\n // - 2 executed using the event manager\n func ExecutePreAction(conn *BaseConnection, operation, filePath, virtualPath string, fileSize int64, openFlags int) (int, error) {\n-\tvar event *notifier.FsEvent\n-\thasNotifiersPlugin := plugin.Handler.HasNotifiers()\n-\thasHook := slices.Contains(Config.Actions.ExecuteOn, operation)\n-\thasRules := eventManager.hasFsRules()\n-\tif !hasHook && !hasNotifiersPlugin && !hasRules {\n-\t\treturn 0, nil\n-\t}\n-\tdateTime := time.Now()\n-\tevent = newActionNotification(&conn.User, operation, filePath, virtualPath, \"\", \"\", \"\",\n-\t\tconn.protocol, conn.GetRemoteIP(), conn.ID, fileSize, openFlags, conn.getNotificationStatus(nil), 0, dateTime, nil)\n-\tif hasNotifiersPlugin {\n-\t\tplugin.Handler.NotifyFsEvent(event)\n-\t}\n-\tif hasRules {\n-\t\tparams := EventParams{\n-\t\t\tName:              event.Username,\n-\t\t\tGroups:            conn.User.Groups,\n-\t\t\tEvent:             event.Action,\n-\t\t\tStatus:            event.Status,\n-\t\t\tVirtualPath:       event.VirtualPath,\n-\t\t\tFsPath:            event.Path,\n-\t\t\tVirtualTargetPath: event.VirtualTargetPath,\n-\t\t\tFsTargetPath:      event.TargetPath,\n-\t\t\tObjectName:        path.Base(event.VirtualPath),\n-\t\t\tExtension:         path.Ext(event.VirtualPath),\n-\t\t\tFileSize:          event.FileSize,\n-\t\t\tProtocol:          event.Protocol,\n-\t\t\tIP:                event.IP,\n-\t\t\tRole:              event.Role,\n-\t\t\tTimestamp:         dateTime,\n-\t\t\tEmail:             conn.User.Email,\n-\t\t\tObject:            nil,\n-\t\t}\n-\t\texecutedSync, err := eventManager.handleFsEvent(params)\n-\t\tif executedSync {\n-\t\t\treturn 2, err\n-\t\t}\n-\t}\n-\tif !hasHook {\n-\t\treturn 0, nil\n-\t}\n-\treturn actionHandler.Handle(event)\n+        var event *notifier.FsEvent\n+        hasNotifiersPlugin := plugin.Handler.HasNotifiers()\n+        hasHook := slices.Contains(Config.Actions.ExecuteOn, operation)\n+        hasRules := eventManager.hasFsRules()\n+        if !hasHook && !hasNotifiersPlugin && !hasRules {\n+                return 0, nil\n+        }\n+        dateTime := time.Now()\n+        event = newActionNotification(&conn.User, operation, filePath, virtualPath, \"\", \"\", \"\",\n+                conn.protocol, conn.GetRemoteIP(), conn.ID, fileSize, openFlags, conn.getNotificationStatus(nil), 0, dateTime, nil)\n+        if hasNotifiersPlugin {\n+                plugin.Handler.NotifyFsEvent(event)\n+        }\n+        if hasRules {\n+                params := EventParams{\n+                        Name:              event.Username,\n+                        Groups:            conn.User.Groups,\n+                        Event:             event.Action,\n+                        Status:            event.Status,\n+                        VirtualPath:       event.VirtualPath,\n+                        FsPath:            event.Path,\n+                        VirtualTargetPath: event.VirtualTargetPath,\n+                        FsTargetPath:      event.TargetPath,\n+                        ObjectName:        path.Base(event.VirtualPath),\n+                        Extension:         path.Ext(event.VirtualPath),\n+                        FileSize:          event.FileSize,\n+                        Protocol:          event.Protocol,\n+                        IP:                event.IP,\n+                        Role:              event.Role,\n+                        Timestamp:         dateTime,\n+                        Email:             conn.User.Email,\n+                        Object:            nil,\n+                }\n+                executedSync, err := eventManager.handleFsEvent(params)\n+                if executedSync {\n+                        return 2, err\n+                }\n+        }\n+        if !hasHook {\n+                return 0, nil\n+        }\n+        return actionHandler.Handle(event)\n }\n \n // ExecuteActionNotification executes the defined hook, if any, for the specified action\n func ExecuteActionNotification(conn *BaseConnection, operation, filePath, virtualPath, target, virtualTarget, sshCmd string,\n-\tfileSize int64, err error, elapsed int64, metadata map[string]string,\n+        fileSize int64, err error, elapsed int64, metadata map[string]string,\n ) error {\n-\thasNotifiersPlugin := plugin.Handler.HasNotifiers()\n-\thasHook := slices.Contains(Config.Actions.ExecuteOn, operation)\n-\thasRules := eventManager.hasFsRules()\n-\tif !hasHook && !hasNotifiersPlugin && !hasRules {\n-\t\treturn nil\n-\t}\n-\tdateTime := time.Now()\n-\tnotification := newActionNotification(&conn.User, operation, filePath, virtualPath, target, virtualTarget, sshCmd,\n-\t\tconn.protocol, conn.GetRemoteIP(), conn.ID, fileSize, 0, conn.getNotificationStatus(err), elapsed, dateTime, metadata)\n-\tif hasNotifiersPlugin {\n-\t\tplugin.Handler.NotifyFsEvent(notification)\n-\t}\n-\tif hasRules {\n-\t\tparams := EventParams{\n-\t\t\tName:              notification.Username,\n-\t\t\tGroups:            conn.User.Groups,\n-\t\t\tEvent:             notification.Action,\n-\t\t\tStatus:            notification.Status,\n-\t\t\tVirtualPath:       notification.VirtualPath,\n-\t\t\tFsPath:            notification.Path,\n-\t\t\tVirtualTargetPath: notification.VirtualTargetPath,\n-\t\t\tFsTargetPath:      notification.TargetPath,\n-\t\t\tObjectName:        path.Base(notification.VirtualPath),\n-\t\t\tExtension:         path.Ext(notification.VirtualPath),\n-\t\t\tFileSize:          notification.FileSize,\n-\t\t\tElapsed:           notification.Elapsed,\n-\t\t\tProtocol:          notification.Protocol,\n-\t\t\tIP:                notification.IP,\n-\t\t\tRole:              notification.Role,\n-\t\t\tTimestamp:         dateTime,\n-\t\t\tEmail:             conn.User.Email,\n-\t\t\tObject:            nil,\n-\t\t\tMetadata:          metadata,\n-\t\t}\n-\t\tif err != nil {\n-\t\t\tparams.AddError(fmt.Errorf(\"%q failed: %w\", params.Event, err))\n-\t\t}\n-\t\texecutedSync, err := eventManager.handleFsEvent(params)\n-\t\tif executedSync {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\tif hasHook {\n-\t\tif slices.Contains(Config.Actions.ExecuteSync, operation) {\n-\t\t\t_, err := actionHandler.Handle(notification)\n-\t\t\treturn err\n-\t\t}\n-\t\tgo func() {\n-\t\t\tstartNewHook()\n-\t\t\tdefer hookEnded()\n-\n-\t\t\tactionHandler.Handle(notification) //nolint:errcheck\n-\t\t}()\n-\t}\n-\treturn nil\n+        hasNotifiersPlugin := plugin.Handler.HasNotifiers()\n+        hasHook := slices.Contains(Config.Actions.ExecuteOn, operation)\n+        hasRules := eventManager.hasFsRules()\n+        if !hasHook && !hasNotifiersPlugin && !hasRules {\n+                return nil\n+        }\n+        dateTime := time.Now()\n+        notification := newActionNotification(&conn.User, operation, filePath, virtualPath, target, virtualTarget, sshCmd,\n+                conn.protocol, conn.GetRemoteIP(), conn.ID, fileSize, 0, conn.getNotificationStatus(err), elapsed, dateTime, metadata)\n+        if hasNotifiersPlugin {\n+                plugin.Handler.NotifyFsEvent(notification)\n+        }\n+        if hasRules {\n+                params := EventParams{\n+                        Name:              notification.Username,\n+                        Groups:            conn.User.Groups,\n+                        Event:             notification.Action,\n+                        Status:            notification.Status,\n+                        VirtualPath:       notification.VirtualPath,\n+                        FsPath:            notification.Path,\n+                        VirtualTargetPath: notification.VirtualTargetPath,\n+                        FsTargetPath:      notification.TargetPath,\n+                        ObjectName:        path.Base(notification.VirtualPath),\n+                        Extension:         path.Ext(notification.VirtualPath),\n+                        FileSize:          notification.FileSize,\n+                        Elapsed:           notification.Elapsed,\n+                        Protocol:          notification.Protocol,\n+                        IP:                notification.IP,\n+                        Role:              notification.Role,\n+                        Timestamp:         dateTime,\n+                        Email:             conn.User.Email,\n+                        Object:            nil,\n+                        Metadata:          metadata,\n+                }\n+                if err != nil {\n+                        params.AddError(fmt.Errorf(\"%q failed: %w\", params.Event, err))\n+                }\n+                executedSync, err := eventManager.handleFsEvent(params)\n+                if executedSync {\n+                        return err\n+                }\n+        }\n+        if hasHook {\n+                if slices.Contains(Config.Actions.ExecuteSync, operation) {\n+                        _, err := actionHandler.Handle(notification)\n+                        return err\n+                }\n+                go func() {\n+                        startNewHook()\n+                        defer hookEnded()\n+\n+                        actionHandler.Handle(notification) //nolint:errcheck\n+                }()\n+        }\n+        return nil\n }\n \n // ActionHandler handles a notification for a Protocol Action.\n type ActionHandler interface {\n-\tHandle(notification *notifier.FsEvent) (int, error)\n+        Handle(notification *notifier.FsEvent) (int, error)\n }\n \n func newActionNotification(\n-\tuser *dataprovider.User,\n-\toperation, filePath, virtualPath, target, virtualTarget, sshCmd, protocol, ip, sessionID string,\n-\tfileSize int64,\n-\topenFlags, status int, elapsed int64,\n-\tdatetime time.Time,\n-\tmetadata map[string]string,\n+        user *dataprovider.User,\n+        operation, filePath, virtualPath, target, virtualTarget, sshCmd, protocol, ip, sessionID string,\n+        fileSize int64,\n+        openFlags, status int, elapsed int64,\n+        datetime time.Time,\n+        metadata map[string]string,\n ) *notifier.FsEvent {\n-\tvar bucket, endpoint string\n-\n-\tfsConfig := user.GetFsConfigForPath(virtualPath)\n-\n-\tswitch fsConfig.Provider {\n-\tcase sdk.S3FilesystemProvider:\n-\t\tbucket = fsConfig.S3Config.Bucket\n-\t\tendpoint = fsConfig.S3Config.Endpoint\n-\tcase sdk.GCSFilesystemProvider:\n-\t\tbucket = fsConfig.GCSConfig.Bucket\n-\tcase sdk.AzureBlobFilesystemProvider:\n-\t\tbucket = fsConfig.AzBlobConfig.Container\n-\t\tif fsConfig.AzBlobConfig.Endpoint != \"\" {\n-\t\t\tendpoint = fsConfig.AzBlobConfig.Endpoint\n-\t\t}\n-\tcase sdk.SFTPFilesystemProvider:\n-\t\tendpoint = fsConfig.SFTPConfig.Endpoint\n-\tcase sdk.HTTPFilesystemProvider:\n-\t\tendpoint = fsConfig.HTTPConfig.Endpoint\n-\t}\n-\n-\treturn &notifier.FsEvent{\n-\t\tAction:            operation,\n-\t\tUsername:          user.Username,\n-\t\tPath:              filePath,\n-\t\tTargetPath:        target,\n-\t\tVirtualPath:       virtualPath,\n-\t\tVirtualTargetPath: virtualTarget,\n-\t\tSSHCmd:            sshCmd,\n-\t\tFileSize:          fileSize,\n-\t\tFsProvider:        int(fsConfig.Provider),\n-\t\tBucket:            bucket,\n-\t\tEndpoint:          endpoint,\n-\t\tStatus:            status,\n-\t\tProtocol:          protocol,\n-\t\tIP:                ip,\n-\t\tSessionID:         sessionID,\n-\t\tOpenFlags:         openFlags,\n-\t\tRole:              user.Role,\n-\t\tTimestamp:         datetime.UnixNano(),\n-\t\tElapsed:           elapsed,\n-\t\tMetadata:          metadata,\n-\t}\n+        var bucket, endpoint string\n+\n+        fsConfig := user.GetFsConfigForPath(virtualPath)\n+\n+        switch fsConfig.Provider {\n+        case sdk.S3FilesystemProvider:\n+                bucket = fsConfig.S3Config.Bucket\n+                endpoint = fsConfig.S3Config.Endpoint\n+        case sdk.GCSFilesystemProvider:\n+                bucket = fsConfig.GCSConfig.Bucket\n+        case sdk.AzureBlobFilesystemProvider:\n+                bucket = fsConfig.AzBlobConfig.Container\n+                if fsConfig.AzBlobConfig.Endpoint != \"\" {\n+                        endpoint = fsConfig.AzBlobConfig.Endpoint\n+                }\n+        case sdk.SFTPFilesystemProvider:\n+                endpoint = fsConfig.SFTPConfig.Endpoint\n+        case sdk.HTTPFilesystemProvider:\n+                endpoint = fsConfig.HTTPConfig.Endpoint\n+        }\n+\n+        return &notifier.FsEvent{\n+                Action:            operation,\n+                Username:          user.Username,\n+                Path:              filePath,\n+                TargetPath:        target,\n+                VirtualPath:       virtualPath,\n+                VirtualTargetPath: virtualTarget,\n+                SSHCmd:            sshCmd,\n+                FileSize:          fileSize,\n+                FsProvider:        int(fsConfig.Provider),\n+                Bucket:            bucket,\n+                Endpoint:          endpoint,\n+                Status:            status,\n+                Protocol:          protocol,\n+                IP:                ip,\n+                SessionID:         sessionID,\n+                OpenFlags:         openFlags,\n+                Role:              user.Role,\n+                Timestamp:         datetime.UnixNano(),\n+                Elapsed:           elapsed,\n+                Metadata:          metadata,\n+        }\n }\n \n type defaultActionHandler struct{}\n \n func (h *defaultActionHandler) Handle(event *notifier.FsEvent) (int, error) {\n-\tif !slices.Contains(Config.Actions.ExecuteOn, event.Action) {\n-\t\treturn 0, nil\n-\t}\n+        if !slices.Contains(Config.Actions.ExecuteOn, event.Action) {\n+                return 0, nil\n+        }\n \n-\tif Config.Actions.Hook == \"\" {\n-\t\tlogger.Warn(event.Protocol, \"\", \"Unable to send notification, no hook is defined\")\n+        if Config.Actions.Hook == \"\" {\n+                logger.Warn(event.Protocol, \"\", \"Unable to send notification, no hook is defined\")\n \n-\t\treturn 0, nil\n-\t}\n+                return 0, nil\n+        }\n \n-\tif strings.HasPrefix(Config.Actions.Hook, \"http\") {\n-\t\terr := h.handleHTTP(event)\n-\t\treturn 1, err\n-\t}\n+        if strings.HasPrefix(Config.Actions.Hook, \"http\") {\n+                err := h.handleHTTP(event)\n+                return 1, err\n+        }\n \n-\terr := h.handleCommand(event)\n-\treturn 1, err\n+        err := h.handleCommand(event)\n+        return 1, err\n }\n \n func (h *defaultActionHandler) handleHTTP(event *notifier.FsEvent) error {\n-\tu, err := url.Parse(Config.Actions.Hook)\n-\tif err != nil {\n-\t\tlogger.Error(event.Protocol, \"\", \"Invalid hook %q for operation %q: %v\",\n-\t\t\tConfig.Actions.Hook, event.Action, err)\n-\t\treturn err\n-\t}\n-\n-\tstartTime := time.Now()\n-\trespCode := 0\n-\n-\tvar b bytes.Buffer\n-\t_ = json.NewEncoder(&b).Encode(event)\n-\n-\tresp, err := httpclient.RetryablePost(Config.Actions.Hook, \"application/json\", &b)\n-\tif err == nil {\n-\t\trespCode = resp.StatusCode\n-\t\tresp.Body.Close()\n-\n-\t\tif respCode != http.StatusOK {\n-\t\t\terr = errUnexpectedHTTResponse\n-\t\t}\n-\t}\n+        u, err := url.Parse(Config.Actions.Hook)\n+        if err != nil {\n+                logger.Error(event.Protocol, \"\", \"Invalid hook %q for operation %q: %v\",\n+                        Config.Actions.Hook, event.Action, err)\n+                return err\n+        }\n+\n+        startTime := time.Now()\n+        respCode := 0\n+\n+        var b bytes.Buffer\n+        _ = json.NewEncoder(&b).Encode(event)\n+\n+        resp, err := httpclient.RetryablePost(Config.Actions.Hook, \"application/json\", &b)\n+        if err == nil {\n+                respCode = resp.StatusCode\n+                resp.Body.Close()\n+\n+                if respCode != http.StatusOK {\n+                        err = errUnexpectedHTTResponse\n+if Config.Actions.Commands.Disabled {\n+logger.Warn(event.Protocol, \"\", \"cannot execute command %q, commands are disabled\", Config.Actions.Hook)\n+return nil\n+}\n+if !slices.Contains(Config.Actions.Commands.Allowed, Config.Actions.Hook) {\n+err := fmt.Errorf(\"command %q is not allowed\", Config.Actions.Hook)\n+logger.Error(event.Protocol, \"\", \"unable to execute notification command: %v\", err)\n+return err\n+}\n+                }\n+        }\n \n-\tlogger.Debug(event.Protocol, \"\", \"notified operation %q to URL: %s status code: %d, elapsed: %s err: %v\",\n-\t\tevent.Action, u.Redacted(), respCode, time.Since(startTime), err)\n+        logger.Debug(event.Protocol, \"\", \"notified operation %q to URL: %s status code: %d, elapsed: %s err: %v\",\n+                event.Action, u.Redacted(), respCode, time.Since(startTime), err)\n \n-\treturn err\n+        return err\n }\n \n func (h *defaultActionHandler) handleCommand(event *notifier.FsEvent) error {\n-\tif !filepath.IsAbs(Config.Actions.Hook) {\n-\t\terr := fmt.Errorf(\"invalid notification command %q\", Config.Actions.Hook)\n-\t\tlogger.Warn(event.Protocol, \"\", \"unable to execute notification command: %v\", err)\n+        if !filepath.IsAbs(Config.Actions.Hook) {\n+                err := fmt.Errorf(\"invalid notification command %q\", Config.Actions.Hook)\n+                logger.Warn(event.Protocol, \"\", \"unable to execute notification command: %v\", err)\n \n-\t\treturn err\n-\t}\n+                return err\n+        }\n \n-\ttimeout, env, args := command.GetConfig(Config.Actions.Hook, command.HookFsActions)\n-\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n-\tdefer cancel()\n+        timeout, env, args := command.GetConfig(Config.Actions.Hook, command.HookFsActions)\n+        ctx, cancel := context.WithTimeout(context.Background(), timeout)\n+        defer cancel()\n \n-\tcmd := exec.CommandContext(ctx, Config.Actions.Hook, args...)\n-\tcmd.Env = append(env, notificationAsEnvVars(event)...)\n+        cmd := exec.CommandContext(ctx, Config.Actions.Hook, args...)\n+        cmd.Env = append(env, notificationAsEnvVars(event)...)\n \n-\tstartTime := time.Now()\n-\terr := cmd.Run()\n+        startTime := time.Now()\n+        err := cmd.Run()\n \n-\tlogger.Debug(event.Protocol, \"\", \"executed command %q, elapsed: %s, error: %v\",\n-\t\tConfig.Actions.Hook, time.Since(startTime), err)\n+        logger.Debug(event.Protocol, \"\", \"executed command %q, elapsed: %s, error: %v\",\n+                Config.Actions.Hook, time.Since(startTime), err)\n \n-\treturn err\n+        return err\n }\n \n func notificationAsEnvVars(event *notifier.FsEvent) []string {\n-\tresult := []string{\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION=%s\", event.Action),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_USERNAME=%s\", event.Username),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_PATH=%s\", event.Path),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_TARGET=%s\", event.TargetPath),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_VIRTUAL_PATH=%s\", event.VirtualPath),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_VIRTUAL_TARGET=%s\", event.VirtualTargetPath),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_SSH_CMD=%s\", event.SSHCmd),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_FILE_SIZE=%d\", event.FileSize),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_ELAPSED=%d\", event.Elapsed),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_FS_PROVIDER=%d\", event.FsProvider),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_BUCKET=%s\", event.Bucket),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_ENDPOINT=%s\", event.Endpoint),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_STATUS=%d\", event.Status),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_PROTOCOL=%s\", event.Protocol),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_IP=%s\", event.IP),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_SESSION_ID=%s\", event.SessionID),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_OPEN_FLAGS=%d\", event.OpenFlags),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_TIMESTAMP=%d\", event.Timestamp),\n-\t\tfmt.Sprintf(\"SFTPGO_ACTION_ROLE=%s\", event.Role),\n-\t}\n-\tif len(event.Metadata) > 0 {\n-\t\tdata, err := json.Marshal(event.Metadata)\n-\t\tif err == nil {\n-\t\t\tresult = append(result, fmt.Sprintf(\"SFTPGO_ACTION_METADATA=%s\", util.BytesToString(data)))\n-\t\t}\n-\t}\n-\treturn result\n+        result := []string{\n+                fmt.Sprintf(\"SFTPGO_ACTION=%s\", event.Action),\n+                fmt.Sprintf(\"SFTPGO_ACTION_USERNAME=%s\", event.Username),\n+                fmt.Sprintf(\"SFTPGO_ACTION_PATH=%s\", event.Path),\n+                fmt.Sprintf(\"SFTPGO_ACTION_TARGET=%s\", event.TargetPath),\n+                fmt.Sprintf(\"SFTPGO_ACTION_VIRTUAL_PATH=%s\", event.VirtualPath),\n+                fmt.Sprintf(\"SFTPGO_ACTION_VIRTUAL_TARGET=%s\", event.VirtualTargetPath),\n+                fmt.Sprintf(\"SFTPGO_ACTION_SSH_CMD=%s\", event.SSHCmd),\n+                fmt.Sprintf(\"SFTPGO_ACTION_FILE_SIZE=%d\", event.FileSize),\n+                fmt.Sprintf(\"SFTPGO_ACTION_ELAPSED=%d\", event.Elapsed),\n+                fmt.Sprintf(\"SFTPGO_ACTION_FS_PROVIDER=%d\", event.FsProvider),\n+                fmt.Sprintf(\"SFTPGO_ACTION_BUCKET=%s\", event.Bucket),\n+                fmt.Sprintf(\"SFTPGO_ACTION_ENDPOINT=%s\", event.Endpoint),\n+                fmt.Sprintf(\"SFTPGO_ACTION_STATUS=%d\", event.Status),\n+                fmt.Sprintf(\"SFTPGO_ACTION_PROTOCOL=%s\", event.Protocol),\n+                fmt.Sprintf(\"SFTPGO_ACTION_IP=%s\", event.IP),\n+                fmt.Sprintf(\"SFTPGO_ACTION_SESSION_ID=%s\", event.SessionID),\n+                fmt.Sprintf(\"SFTPGO_ACTION_OPEN_FLAGS=%d\", event.OpenFlags),\n+                fmt.Sprintf(\"SFTPGO_ACTION_TIMESTAMP=%d\", event.Timestamp),\n+                fmt.Sprintf(\"SFTPGO_ACTION_ROLE=%s\", event.Role),\n+        }\n+        if len(event.Metadata) > 0 {\n+                data, err := json.Marshal(event.Metadata)\n+                if err == nil {\n+                        result = append(result, fmt.Sprintf(\"SFTPGO_ACTION_METADATA=%s\", util.BytesToString(data)))\n+                }\n+        }\n+        return result\n }\n"}
{"cve":"CVE-2024-22199:0708", "fix_patch": ""}
{"cve":"CVE-2023-52081:0708", "fix_patch": ""}
{"cve":"CVE-2023-45128:0708", "fix_patch": "diff --git a/middleware/csrf/csrf.go b/middleware/csrf/csrf.go\nindex b7b01274..42747fd4 100644\n--- a/middleware/csrf/csrf.go\n+++ b/middleware/csrf/csrf.go\n@@ -1,230 +1,258 @@\n package csrf\n \n import (\n-\t\"errors\"\n-\t\"reflect\"\n-\t\"time\"\n-\n-\t\"github.com/gofiber/fiber/v2\"\n+\"crypto/subtle\"\n+\"errors\"\n+\"net/url\"\n+\"reflect\"\n+\"strings\"\n+\"time\"\n+\n+\"github.com/gofiber/fiber/v2\"\n )\n \n var (\n-\tErrTokenNotFound = errors.New(\"csrf token not found\")\n-\tErrTokenInvalid  = errors.New(\"csrf token invalid\")\n-\tErrNoReferer     = errors.New(\"referer not supplied\")\n-\tErrBadReferer    = errors.New(\"referer invalid\")\n-\tdummyValue       = []byte{'+'}\n+ErrTokenNotFound = errors.New(\"csrf token not found\")\n+ErrTokenInvalid  = errors.New(\"csrf token invalid\")\n+// ErrNoReferer is deprecated, please use ErrMissingOrInvalidReferer\n+ErrNoReferer = errors.New(\"referer not supplied\")\n+// ErrBadReferer is deprecated, please use ErrRefererNotTrusted\n+ErrBadReferer              = errors.New(\"referer invalid\")\n+ErrMissingOrInvalidReferer = errors.New(\"missing or invalid referer\")\n+ErrRefererNotTrusted       = errors.New(\"referer not trusted\")\n+ErrTokenExpired            = errors.New(\"csrf token expired\")\n+ErrTokenNotIssued          = errors.New(\"csrf token not issued\")\n+ErrSessionNotFound         = errors.New(\"session not found\")\n+ErrCookieNotFound          = errors.New(\"csrf cookie not found\")\n+ErrMissingParam            = errors.New(\"missing param\")\n+ErrMissingHeader           = errors.New(\"missing header\")\n+dummyValue                 = []byte{'+'}\n )\n \n type CSRFHandler struct {\n-\tconfig         *Config\n-\tsessionManager *sessionManager\n-\tstorageManager *storageManager\n+config         *Config\n+sessionManager *sessionManager\n+storageManager *storageManager\n }\n \n // New creates a new middleware handler\n func New(config ...Config) fiber.Handler {\n-\t// Set default config\n-\tcfg := configDefault(config...)\n-\n-\t// Create manager to simplify storage operations ( see *_manager.go )\n-\tvar sessionManager *sessionManager\n-\tvar storageManager *storageManager\n-\tif cfg.Session != nil {\n-\t\t// Register the Token struct in the session store\n-\t\tcfg.Session.RegisterType(Token{})\n-\n-\t\tsessionManager = newSessionManager(cfg.Session, cfg.SessionKey)\n-\t} else {\n-\t\tstorageManager = newStorageManager(cfg.Storage)\n-\t}\n-\n-\t// Return new handler\n-\treturn func(c *fiber.Ctx) error {\n-\t\t// Don't execute middleware if Next returns true\n-\t\tif cfg.Next != nil && cfg.Next(c) {\n-\t\t\treturn c.Next()\n-\t\t}\n-\n-\t\t// Store the CSRF handler in the context if a context key is specified\n-\t\tif cfg.HandlerContextKey != \"\" {\n-\t\t\tc.Locals(cfg.HandlerContextKey, &CSRFHandler{\n-\t\t\t\tconfig:         &cfg,\n-\t\t\t\tsessionManager: sessionManager,\n-\t\t\t\tstorageManager: storageManager,\n-\t\t\t})\n-\t\t}\n-\n-\t\tvar token string\n-\n-\t\t// Action depends on the HTTP method\n-\t\tswitch c.Method() {\n-\t\tcase fiber.MethodGet, fiber.MethodHead, fiber.MethodOptions, fiber.MethodTrace:\n-\t\t\tcookieToken := c.Cookies(cfg.CookieName)\n-\n-\t\t\tif cookieToken != \"\" {\n-\t\t\t\trawToken := getTokenFromStorage(c, cookieToken, cfg, sessionManager, storageManager)\n-\n-\t\t\t\tif rawToken != nil {\n-\t\t\t\t\ttoken = string(rawToken)\n-\t\t\t\t}\n-\t\t\t}\n-\t\tdefault:\n-\t\t\t// Assume that anything not defined as 'safe' by RFC7231 needs protection\n-\n-\t\t\t// Enforce an origin check for HTTPS connections.\n-\t\t\tif c.Protocol() == \"https\" {\n-\t\t\t\tif err := refererMatchesHost(c); err != nil {\n-\t\t\t\t\treturn cfg.ErrorHandler(c, err)\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\t// Extract token from client request i.e. header, query, param, form or cookie\n-\t\t\textractedToken, err := cfg.Extractor(c)\n-\t\t\tif err != nil {\n-\t\t\t\treturn cfg.ErrorHandler(c, err)\n-\t\t\t}\n-\n-\t\t\tif extractedToken == \"\" {\n-\t\t\t\treturn cfg.ErrorHandler(c, ErrTokenNotFound)\n-\t\t\t}\n-\n-\t\t\t// If not using CsrfFromCookie extractor, check that the token matches the cookie\n-\t\t\t// This is to prevent CSRF attacks by using a Double Submit Cookie method\n-\t\t\t// Useful when we do not have access to the users Session\n-\t\t\tif !isCsrfFromCookie(cfg.Extractor) && extractedToken != c.Cookies(cfg.CookieName) {\n-\t\t\t\treturn cfg.ErrorHandler(c, ErrTokenInvalid)\n-\t\t\t}\n-\n-\t\t\trawToken := getTokenFromStorage(c, extractedToken, cfg, sessionManager, storageManager)\n-\n-\t\t\tif rawToken == nil {\n-\t\t\t\t// If token is not in storage, expire the cookie\n-\t\t\t\texpireCSRFCookie(c, cfg)\n-\t\t\t\t// and return an error\n-\t\t\t\treturn cfg.ErrorHandler(c, ErrTokenNotFound)\n-\t\t\t}\n-\t\t\tif cfg.SingleUseToken {\n-\t\t\t\t// If token is single use, delete it from storage\n-\t\t\t\tdeleteTokenFromStorage(c, extractedToken, cfg, sessionManager, storageManager)\n-\t\t\t} else {\n-\t\t\t\ttoken = string(rawToken)\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Generate CSRF token if not exist\n-\t\tif token == \"\" {\n-\t\t\t// And generate a new token\n-\t\t\ttoken = cfg.KeyGenerator()\n-\t\t}\n-\n-\t\t// Create or extend the token in the storage\n-\t\tcreateOrExtendTokenInStorage(c, token, cfg, sessionManager, storageManager)\n-\n-\t\t// Update the CSRF cookie\n-\t\tupdateCSRFCookie(c, cfg, token)\n-\n-\t\t// Tell the browser that a new header value is generated\n-\t\tc.Vary(fiber.HeaderCookie)\n-\n-\t\t// Store the token in the context if a context key is specified\n-\t\tif cfg.ContextKey != \"\" {\n-\t\t\tc.Locals(cfg.ContextKey, token)\n-\t\t}\n-\n-\t\t// Continue stack\n-\t\treturn c.Next()\n-\t}\n+// Set default config\n+cfg := configDefault(config...)\n+\n+// Create manager to simplify storage operations ( see *_manager.go )\n+var sessionManager *sessionManager\n+var storageManager *storageManager\n+if cfg.Session != nil {\n+// Register the Token struct in the session store\n+cfg.Session.RegisterType(Token{}) //nolint:errcheck // We don't need to check the error here\n+\n+sessionManager = newSessionManager(cfg.Session, cfg.SessionKey)\n+} else {\n+storageManager = newStorageManager(cfg.Storage)\n+}\n+\n+// Return new handler\n+return func(c *fiber.Ctx) error {\n+// Don't execute middleware if Next returns true\n+if cfg.Next != nil && cfg.Next(c) {\n+return c.Next()\n+}\n+\n+// Store the CSRF handler in the context if a context key is specified\n+if cfg.HandlerContextKey != \"\" {\n+c.Locals(cfg.HandlerContextKey, &CSRFHandler{\n+config:         &cfg,\n+sessionManager: sessionManager,\n+storageManager: storageManager,\n+})\n+}\n+\n+var token string\n+\n+// Action depends on the HTTP method\n+switch c.Method() {\n+case fiber.MethodGet, fiber.MethodHead, fiber.MethodOptions, fiber.MethodTrace:\n+cookieToken := c.Cookies(cfg.CookieName)\n+\n+if cookieToken != \"\" {\n+rawToken, err := getTokenFromStorage(c, cookieToken, cfg, sessionManager, storageManager)\n+\n+if err == nil {\n+token = string(rawToken)\n+}\n+}\n+default:\n+// Assume that anything not defined as 'safe' by RFC7231 needs protection\n+\n+// Enforce an origin check for HTTPS connections.\n+if c.Protocol() == \"https\" {\n+if err := refererMatchesHost(c); err != nil {\n+return cfg.ErrorHandler(c, err)\n+}\n+}\n+\n+// Extract token from client request i.e. header, query, param, form or cookie\n+extractedToken, err := cfg.Extractor(c)\n+if err != nil {\n+return cfg.ErrorHandler(c, err)\n+}\n+\n+// If not using CsrfFromCookie extractor, check that the token matches the cookie\n+// This is to prevent CSRF attacks by using a Double Submit Cookie method\n+// Useful when we do not have access to the users Session\n+if !isCsrfFromCookie(cfg.Extractor) {\n+cookieToken := c.Cookies(cfg.CookieName)\n+if cookieToken == \"\" {\n+return cfg.ErrorHandler(c, ErrCookieNotFound)\n+}\n+if subtle.ConstantTimeCompare([]byte(extractedToken), []byte(cookieToken)) != 1 {\n+return cfg.ErrorHandler(c, ErrTokenInvalid)\n+}\n+}\n+\n+rawToken, err := getTokenFromStorage(c, extractedToken, cfg, sessionManager, storageManager)\n+if err != nil {\n+// If token is not in storage, expire the cookie and return an error\n+expireCSRFCookie(c, cfg)\n+return cfg.ErrorHandler(c, err)\n+}\n+\n+if cfg.SingleUseToken {\n+// If token is single use, delete it from storage\n+deleteTokenFromStorage(c, extractedToken, cfg, sessionManager, storageManager)\n+} else {\n+token = string(rawToken)\n+}\n+}\n+\n+// Generate CSRF token if not exist\n+if token == \"\" {\n+// And generate a new token\n+token = cfg.KeyGenerator()\n+}\n+\n+// Create or extend the token in the storage\n+createOrExtendTokenInStorage(c, token, cfg, sessionManager, storageManager)\n+\n+// Update the CSRF cookie\n+updateCSRFCookie(c, cfg, token)\n+\n+// Tell the browser that a new header value is generated\n+c.Vary(fiber.HeaderCookie)\n+\n+// Store the token in the context if a context key is specified\n+if cfg.ContextKey != \"\" {\n+c.Locals(cfg.ContextKey, token)\n+}\n+\n+// Continue stack\n+return c.Next()\n+}\n }\n \n // getTokenFromStorage returns the raw token from the storage\n // returns nil if the token does not exist, is expired or is invalid\n-func getTokenFromStorage(c *fiber.Ctx, token string, cfg Config, sessionManager *sessionManager, storageManager *storageManager) []byte {\n-\tif cfg.Session != nil {\n-\t\treturn sessionManager.getRaw(c, token, dummyValue)\n-\t}\n-\treturn storageManager.getRaw(token)\n+func getTokenFromStorage(c *fiber.Ctx, token string, cfg Config, sessionManager *sessionManager, storageManager *storageManager) ([]byte, error) {\n+if cfg.Session != nil {\n+return sessionManager.getRaw(c, token, dummyValue)\n+}\n+raw, err := storageManager.getRaw(token)\n+if err != nil {\n+return nil, err\n+}\n+if len(raw) == 0 {\n+return nil, ErrTokenNotFound\n+}\n+return raw, nil\n }\n \n // createOrExtendTokenInStorage creates or extends the token in the storage\n func createOrExtendTokenInStorage(c *fiber.Ctx, token string, cfg Config, sessionManager *sessionManager, storageManager *storageManager) {\n-\tif cfg.Session != nil {\n-\t\tsessionManager.setRaw(c, token, dummyValue, cfg.Expiration)\n-\t} else {\n-\t\tstorageManager.setRaw(token, dummyValue, cfg.Expiration)\n-\t}\n+if cfg.Session != nil {\n+sessionManager.setRaw(c, token, dummyValue, cfg.Expiration)\n+} else {\n+storageManager.setRaw(token, dummyValue, cfg.Expiration)\n+}\n }\n \n func deleteTokenFromStorage(c *fiber.Ctx, token string, cfg Config, sessionManager *sessionManager, storageManager *storageManager) {\n-\tif cfg.Session != nil {\n-\t\tsessionManager.delRaw(c)\n-\t} else {\n-\t\tstorageManager.delRaw(token)\n-\t}\n+if cfg.Session != nil {\n+sessionManager.delRaw(c)\n+} else {\n+storageManager.delRaw(token)\n+}\n }\n \n // Update CSRF cookie\n // if expireCookie is true, the cookie will expire immediately\n func updateCSRFCookie(c *fiber.Ctx, cfg Config, token string) {\n-\tsetCSRFCookie(c, cfg, token, cfg.Expiration)\n+setCSRFCookie(c, cfg, token, cfg.Expiration)\n }\n \n func expireCSRFCookie(c *fiber.Ctx, cfg Config) {\n-\tsetCSRFCookie(c, cfg, \"\", -time.Hour)\n+setCSRFCookie(c, cfg, \"\", -time.Hour)\n }\n \n func setCSRFCookie(c *fiber.Ctx, cfg Config, token string, expiry time.Duration) {\n-\tcookie := &fiber.Cookie{\n-\t\tName:        cfg.CookieName,\n-\t\tValue:       token,\n-\t\tDomain:      cfg.CookieDomain,\n-\t\tPath:        cfg.CookiePath,\n-\t\tSecure:      cfg.CookieSecure,\n-\t\tHTTPOnly:    cfg.CookieHTTPOnly,\n-\t\tSameSite:    cfg.CookieSameSite,\n-\t\tSessionOnly: cfg.CookieSessionOnly,\n-\t\tExpires:     time.Now().Add(expiry),\n-\t}\n-\n-\t// Set the CSRF cookie to the response\n-\tc.Cookie(cookie)\n+cookie := &fiber.Cookie{\n+Name:        cfg.CookieName,\n+Value:       token,\n+Domain:      cfg.CookieDomain,\n+Path:        cfg.CookiePath,\n+Secure:      cfg.CookieSecure,\n+HTTPOnly:    cfg.CookieHTTPOnly,\n+SameSite:    cfg.CookieSameSite,\n+SessionOnly: cfg.CookieSessionOnly,\n+}\n+\n+if !cfg.CookieSessionOnly {\n+cookie.Expires = time.Now().Add(expiry)\n+}\n+\n+// Set the CSRF cookie to the response\n+c.Cookie(cookie)\n }\n \n // DeleteToken removes the token found in the context from the storage\n // and expires the CSRF cookie\n func (handler *CSRFHandler) DeleteToken(c *fiber.Ctx) error {\n-\t// Get the config from the context\n-\tconfig := handler.config\n-\tif config == nil {\n-\t\tpanic(\"CSRFHandler config not found in context\")\n-\t}\n-\t// Extract token from the client request cookie\n-\tcookieToken := c.Cookies(config.CookieName)\n-\tif cookieToken == \"\" {\n-\t\treturn config.ErrorHandler(c, ErrTokenNotFound)\n-\t}\n-\t// Remove the token from storage\n-\tdeleteTokenFromStorage(c, cookieToken, *config, handler.sessionManager, handler.storageManager)\n-\t// Expire the cookie\n-\texpireCSRFCookie(c, *config)\n-\treturn nil\n+// Get the config from the context\n+config := handler.config\n+if config == nil {\n+panic(\"CSRFHandler config not found in context\")\n+}\n+// Extract token from the client request cookie\n+cookieToken := c.Cookies(config.CookieName)\n+if cookieToken == \"\" {\n+return config.ErrorHandler(c, ErrCookieNotFound)\n+}\n+// Remove the token from storage\n+deleteTokenFromStorage(c, cookieToken, *config, handler.sessionManager, handler.storageManager)\n+// Expire the cookie\n+expireCSRFCookie(c, *config)\n+return nil\n }\n \n // isCsrfFromCookie checks if the extractor is set to ExtractFromCookie\n func isCsrfFromCookie(extractor interface{}) bool {\n-\treturn reflect.ValueOf(extractor).Pointer() == reflect.ValueOf(CsrfFromCookie).Pointer()\n+return reflect.ValueOf(extractor).Pointer() == reflect.ValueOf(CsrfFromCookie).Pointer()\n }\n \n // refererMatchesHost checks that the referer header matches the host header\n // returns an error if the referer header is not present or is invalid\n // returns nil if the referer header is valid\n func refererMatchesHost(c *fiber.Ctx) error {\n-\treferer := c.Get(fiber.HeaderReferer)\n-\tif referer == \"\" {\n-\t\treturn ErrNoReferer\n-\t}\n-\tif referer != c.Protocol()+\"://\"+c.Hostname() {\n-\t\treturn ErrBadReferer\n-\t}\n-\treturn nil\n+referer := c.Get(fiber.HeaderReferer)\n+if referer == \"\" {\n+return ErrMissingOrInvalidReferer\n+}\n+refererURL, err := url.Parse(referer)\n+if err != nil {\n+return ErrMissingOrInvalidReferer\n+}\n+if !strings.EqualFold(refererURL.Scheme, c.Protocol()) || !strings.EqualFold(refererURL.Host, c.Hostname()) {\n+return ErrRefererNotTrusted\n+}\n+return nil\n }\ndiff --git a/middleware/csrf/session_manager.go b/middleware/csrf/session_manager.go\nindex 5497f8ef..ef43cece 100644\n--- a/middleware/csrf/session_manager.go\n+++ b/middleware/csrf/session_manager.go\n@@ -1,68 +1,71 @@\n package csrf\n \n import (\n-\t\"time\"\n+\"time\"\n \n-\t\"github.com/gofiber/fiber/v2\"\n-\t\"github.com/gofiber/fiber/v2/log\"\n-\t\"github.com/gofiber/fiber/v2/middleware/session\"\n+\"github.com/gofiber/fiber/v2\"\n+\"github.com/gofiber/fiber/v2/log\"\n+\"github.com/gofiber/fiber/v2/middleware/session\"\n )\n \n type sessionManager struct {\n-\tkey     string\n-\tsession *session.Store\n+key     string\n+session *session.Store\n }\n \n func newSessionManager(s *session.Store, k string) *sessionManager {\n-\t// Create new storage handler\n-\tsessionManager := &sessionManager{\n-\t\tkey: k,\n-\t}\n-\tif s != nil {\n-\t\t// Use provided storage if provided\n-\t\tsessionManager.session = s\n-\t}\n-\treturn sessionManager\n+// Create new storage handler\n+sessionManager := &sessionManager{\n+key: k,\n+}\n+if s != nil {\n+// Use provided storage if provided\n+sessionManager.session = s\n+}\n+return sessionManager\n }\n \n // get token from session\n-func (m *sessionManager) getRaw(c *fiber.Ctx, key string, raw []byte) []byte {\n-\tsess, err := m.session.Get(c)\n-\tif err != nil {\n-\t\treturn nil\n-\t}\n-\ttoken, ok := sess.Get(m.key).(Token)\n-\tif ok {\n-\t\tif token.Expiration.Before(time.Now()) || key != token.Key || !compareTokens(raw, token.Raw) {\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn token.Raw\n-\t}\n+func (m *sessionManager) getRaw(c *fiber.Ctx, key string, raw []byte) ([]byte, error) {\n+sess, err := m.session.Get(c)\n+if err != nil {\n+return nil, err\n+}\n+token, ok := sess.Get(m.key).(Token)\n+if ok {\n+if token.Expiration.Before(time.Now()) {\n+return nil, ErrTokenExpired\n+}\n+if key != token.Key || !compareTokens(raw, token.Raw) {\n+return nil, ErrTokenInvalid\n+}\n+return token.Raw, nil\n+}\n \n-\treturn nil\n+return nil, ErrTokenNotFound\n }\n \n // set token in session\n func (m *sessionManager) setRaw(c *fiber.Ctx, key string, raw []byte, exp time.Duration) {\n-\tsess, err := m.session.Get(c)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\t// the key is crucial in crsf and sometimes a reference to another value which can be reused later(pool/unsafe values concept), so a copy is made here\n-\tsess.Set(m.key, &Token{key, raw, time.Now().Add(exp)})\n-\tif err := sess.Save(); err != nil {\n-\t\tlog.Warn(\"csrf: failed to save session: \", err)\n-\t}\n+sess, err := m.session.Get(c)\n+if err != nil {\n+return\n+}\n+// the key is crucial in crsf and sometimes a reference to another value which can be reused later(pool/unsafe values concept), so a copy is made here\n+sess.Set(m.key, &Token{key, raw, time.Now().Add(exp)})\n+if err := sess.Save(); err != nil {\n+log.Warnf(\"csrf: failed to save session: %v\", err)\n+}\n }\n \n // delete token from session\n func (m *sessionManager) delRaw(c *fiber.Ctx) {\n-\tsess, err := m.session.Get(c)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tsess.Delete(m.key)\n-\tif err := sess.Save(); err != nil {\n-\t\tlog.Warn(\"csrf: failed to save session: \", err)\n-\t}\n+sess, err := m.session.Get(c)\n+if err != nil {\n+return\n+}\n+sess.Delete(m.key)\n+if err := sess.Save(); err != nil {\n+log.Warnf(\"csrf: failed to save session: %v\", err)\n+}\n }\ndiff --git a/middleware/csrf/storage_manager.go b/middleware/csrf/storage_manager.go\nindex 49b3186f..0ca4114c 100644\n--- a/middleware/csrf/storage_manager.go\n+++ b/middleware/csrf/storage_manager.go\n@@ -1,12 +1,12 @@\n package csrf\n \n import (\n-\t\"sync\"\n-\t\"time\"\n+\"sync\"\n+\"time\"\n \n-\t\"github.com/gofiber/fiber/v2\"\n-\t\"github.com/gofiber/fiber/v2/internal/memory\"\n-\t\"github.com/gofiber/fiber/v2/utils\"\n+\"github.com/gofiber/fiber/v2\"\n+\"github.com/gofiber/fiber/v2/internal/memory\"\n+\"github.com/gofiber/fiber/v2/utils\"\n )\n \n // go:generate msgp\n@@ -15,56 +15,57 @@ type item struct{}\n \n //msgp:ignore manager\n type storageManager struct {\n-\tpool    sync.Pool\n-\tmemory  *memory.Storage\n-\tstorage fiber.Storage\n+pool    sync.Pool\n+memory  *memory.Storage\n+storage fiber.Storage\n }\n \n func newStorageManager(storage fiber.Storage) *storageManager {\n-\t// Create new storage handler\n-\tstorageManager := &storageManager{\n-\t\tpool: sync.Pool{\n-\t\t\tNew: func() interface{} {\n-\t\t\t\treturn new(item)\n-\t\t\t},\n-\t\t},\n-\t}\n-\tif storage != nil {\n-\t\t// Use provided storage if provided\n-\t\tstorageManager.storage = storage\n-\t} else {\n-\t\t// Fallback too memory storage\n-\t\tstorageManager.memory = memory.New()\n-\t}\n-\treturn storageManager\n+// Create new storage handler\n+storageManager := &storageManager{\n+pool: sync.Pool{\n+New: func() interface{} {\n+return new(item)\n+},\n+},\n+}\n+if storage != nil {\n+// Use provided storage if provided\n+storageManager.storage = storage\n+} else {\n+// Fallback too memory storage\n+storageManager.memory = memory.New()\n+}\n+return storageManager\n }\n \n // get raw data from storage or memory\n-func (m *storageManager) getRaw(key string) []byte {\n-\tvar raw []byte\n-\tif m.storage != nil {\n-\t\traw, _ = m.storage.Get(key) //nolint:errcheck // TODO: Do not ignore error\n-\t} else {\n-\t\traw, _ = m.memory.Get(key).([]byte) //nolint:errcheck // TODO: Do not ignore error\n-\t}\n-\treturn raw\n+func (m *storageManager) getRaw(key string) ([]byte, error) {\n+var raw []byte\n+var err error\n+if m.storage != nil {\n+raw, err = m.storage.Get(key)\n+} else {\n+raw, _ = m.memory.Get(key).([]byte)\n+}\n+return raw, err\n }\n \n // set data to storage or memory\n func (m *storageManager) setRaw(key string, raw []byte, exp time.Duration) {\n-\tif m.storage != nil {\n-\t\t_ = m.storage.Set(key, raw, exp) //nolint:errcheck // TODO: Do not ignore error\n-\t} else {\n-\t\t// the key is crucial in crsf and sometimes a reference to another value which can be reused later(pool/unsafe values concept), so a copy is made here\n-\t\tm.memory.Set(utils.CopyString(key), raw, exp)\n-\t}\n+if m.storage != nil {\n+_ = m.storage.Set(key, raw, exp) //nolint:errcheck // TODO: Do not ignore error\n+} else {\n+// the key is crucial in crsf and sometimes a reference to another value which can be reused later(pool/unsafe values concept), so a copy is made here\n+m.memory.Set(utils.CopyString(key), raw, exp)\n+}\n }\n \n // delete data from storage or memory\n func (m *storageManager) delRaw(key string) {\n-\tif m.storage != nil {\n-\t\t_ = m.storage.Delete(key) //nolint:errcheck // TODO: Do not ignore error\n-\t} else {\n-\t\tm.memory.Delete(key)\n-\t}\n+if m.storage != nil {\n+_ = m.storage.Delete(key) //nolint:errcheck // TODO: Do not ignore error\n+} else {\n+m.memory.Delete(key)\n+}\n }\n"}
{"cve":"CVE-2023-40029:0708", "fix_patch": "diff --git a/util/db/cluster.go b/util/db/cluster.go\nindex 9b405a9cac..c3c62c99a3 100644\n--- a/util/db/cluster.go\n+++ b/util/db/cluster.go\n@@ -1,423 +1,424 @@\n package db\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n-\tapiv1 \"k8s.io/api/core/v1\"\n-\tapierr \"k8s.io/apimachinery/pkg/api/errors\"\n-\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n-\t\"k8s.io/apimachinery/pkg/watch\"\n-\t\"k8s.io/utils/pointer\"\n-\n-\t\"github.com/argoproj/argo-cd/v2/common\"\n-\tappv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n-\t\"github.com/argoproj/argo-cd/v2/util/collections\"\n-\t\"github.com/argoproj/argo-cd/v2/util/settings\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        log \"github.com/sirupsen/logrus\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n+        apiv1 \"k8s.io/api/core/v1\"\n+        apierr \"k8s.io/apimachinery/pkg/api/errors\"\n+        metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+        \"k8s.io/apimachinery/pkg/watch\"\n+        \"k8s.io/utils/pointer\"\n+\n+        \"github.com/argoproj/argo-cd/v2/common\"\n+        appv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n+        \"github.com/argoproj/argo-cd/v2/util/collections\"\n+        \"github.com/argoproj/argo-cd/v2/util/settings\"\n )\n \n var (\n-\tlocalCluster = appv1.Cluster{\n-\t\tName:            \"in-cluster\",\n-\t\tServer:          appv1.KubernetesInternalAPIServerAddr,\n-\t\tConnectionState: appv1.ConnectionState{Status: appv1.ConnectionStatusSuccessful},\n-\t}\n-\tinitLocalCluster sync.Once\n+        localCluster = appv1.Cluster{\n+                Name:            \"in-cluster\",\n+                Server:          appv1.KubernetesInternalAPIServerAddr,\n+                ConnectionState: appv1.ConnectionState{Status: appv1.ConnectionStatusSuccessful},\n+        }\n+        initLocalCluster sync.Once\n )\n \n func (db *db) getLocalCluster() *appv1.Cluster {\n-\tinitLocalCluster.Do(func() {\n-\t\tinfo, err := db.kubeclientset.Discovery().ServerVersion()\n-\t\tif err == nil {\n-\t\t\tlocalCluster.ServerVersion = fmt.Sprintf(\"%s.%s\", info.Major, info.Minor)\n-\t\t\tlocalCluster.ConnectionState = appv1.ConnectionState{Status: appv1.ConnectionStatusSuccessful}\n-\t\t} else {\n-\t\t\tlocalCluster.ConnectionState = appv1.ConnectionState{\n-\t\t\t\tStatus:  appv1.ConnectionStatusFailed,\n-\t\t\t\tMessage: err.Error(),\n-\t\t\t}\n-\t\t}\n-\t})\n-\tcluster := localCluster.DeepCopy()\n-\tnow := metav1.Now()\n-\tcluster.ConnectionState.ModifiedAt = &now\n-\treturn cluster\n+        initLocalCluster.Do(func() {\n+                info, err := db.kubeclientset.Discovery().ServerVersion()\n+                if err == nil {\n+                        localCluster.ServerVersion = fmt.Sprintf(\"%s.%s\", info.Major, info.Minor)\n+                        localCluster.ConnectionState = appv1.ConnectionState{Status: appv1.ConnectionStatusSuccessful}\n+                } else {\n+                        localCluster.ConnectionState = appv1.ConnectionState{\n+                                Status:  appv1.ConnectionStatusFailed,\n+                                Message: err.Error(),\n+                        }\n+                }\n+        })\n+        cluster := localCluster.DeepCopy()\n+        now := metav1.Now()\n+        cluster.ConnectionState.ModifiedAt = &now\n+        return cluster\n }\n \n // ListClusters returns list of clusters\n func (db *db) ListClusters(ctx context.Context) (*appv1.ClusterList, error) {\n-\tclusterSecrets, err := db.listSecretsByType(common.LabelValueSecretTypeCluster)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tclusterList := appv1.ClusterList{\n-\t\tItems: make([]appv1.Cluster, 0),\n-\t}\n-\tsettings, err := db.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tinClusterEnabled := settings.InClusterEnabled\n-\thasInClusterCredentials := false\n-\tfor _, clusterSecret := range clusterSecrets {\n-\t\tcluster, err := SecretToCluster(clusterSecret)\n-\t\tif err != nil {\n-\t\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", clusterSecret.Name)\n-\t\t\tcontinue\n-\t\t}\n-\t\tif cluster.Server == appv1.KubernetesInternalAPIServerAddr {\n-\t\t\tif inClusterEnabled {\n-\t\t\t\thasInClusterCredentials = true\n-\t\t\t\tclusterList.Items = append(clusterList.Items, *cluster)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tclusterList.Items = append(clusterList.Items, *cluster)\n-\t\t}\n-\t}\n-\tif inClusterEnabled && !hasInClusterCredentials {\n-\t\tclusterList.Items = append(clusterList.Items, *db.getLocalCluster())\n-\t}\n-\treturn &clusterList, nil\n+        clusterSecrets, err := db.listSecretsByType(common.LabelValueSecretTypeCluster)\n+        if err != nil {\n+                return nil, err\n+        }\n+        clusterList := appv1.ClusterList{\n+                Items: make([]appv1.Cluster, 0),\n+        }\n+        settings, err := db.settingsMgr.GetSettings()\n+        if err != nil {\n+                return nil, err\n+        }\n+        inClusterEnabled := settings.InClusterEnabled\n+        hasInClusterCredentials := false\n+        for _, clusterSecret := range clusterSecrets {\n+                cluster, err := SecretToCluster(clusterSecret)\n+                if err != nil {\n+                        log.Errorf(\"could not unmarshal cluster secret %s\", clusterSecret.Name)\n+                        continue\n+                }\n+                if cluster.Server == appv1.KubernetesInternalAPIServerAddr {\n+                        if inClusterEnabled {\n+                                hasInClusterCredentials = true\n+                                clusterList.Items = append(clusterList.Items, *cluster)\n+                        }\n+                } else {\n+                        clusterList.Items = append(clusterList.Items, *cluster)\n+                }\n+        }\n+        if inClusterEnabled && !hasInClusterCredentials {\n+                clusterList.Items = append(clusterList.Items, *db.getLocalCluster())\n+        }\n+        return &clusterList, nil\n }\n \n // CreateCluster creates a cluster\n func (db *db) CreateCluster(ctx context.Context, c *appv1.Cluster) (*appv1.Cluster, error) {\n-\tsettings, err := db.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif c.Server == appv1.KubernetesInternalAPIServerAddr && !settings.InClusterEnabled {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"cannot register cluster: in-cluster has been disabled\")\n-\t}\n-\tsecName, err := URIToSecretName(\"cluster\", c.Server)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclusterSecret := &apiv1.Secret{\n-\t\tObjectMeta: metav1.ObjectMeta{\n-\t\t\tName: secName,\n-\t\t},\n-\t}\n-\n-\tif err = clusterToSecret(c, clusterSecret); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclusterSecret, err = db.createSecret(ctx, clusterSecret)\n-\tif err != nil {\n-\t\tif apierr.IsAlreadyExists(err) {\n-\t\t\treturn nil, status.Errorf(codes.AlreadyExists, \"cluster %q already exists\", c.Server)\n-\t\t}\n-\t\treturn nil, err\n-\t}\n-\n-\tcluster, err := SecretToCluster(clusterSecret)\n-\tif err != nil {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"could not unmarshal cluster secret %s\", clusterSecret.Name)\n-\t}\n-\treturn cluster, db.settingsMgr.ResyncInformers()\n+        settings, err := db.settingsMgr.GetSettings()\n+        if err != nil {\n+                return nil, err\n+        }\n+        if c.Server == appv1.KubernetesInternalAPIServerAddr && !settings.InClusterEnabled {\n+                return nil, status.Errorf(codes.InvalidArgument, \"cannot register cluster: in-cluster has been disabled\")\n+        }\n+        secName, err := URIToSecretName(\"cluster\", c.Server)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        clusterSecret := &apiv1.Secret{\n+                ObjectMeta: metav1.ObjectMeta{\n+                        Name: secName,\n+                },\n+        }\n+\n+        if err = clusterToSecret(c, clusterSecret); err != nil {\n+                return nil, err\n+        }\n+\n+        clusterSecret, err = db.createSecret(ctx, clusterSecret)\n+        if err != nil {\n+                if apierr.IsAlreadyExists(err) {\n+                        return nil, status.Errorf(codes.AlreadyExists, \"cluster %q already exists\", c.Server)\n+                }\n+                return nil, err\n+        }\n+\n+        cluster, err := SecretToCluster(clusterSecret)\n+        if err != nil {\n+                return nil, status.Errorf(codes.InvalidArgument, \"could not unmarshal cluster secret %s\", clusterSecret.Name)\n+        }\n+        return cluster, db.settingsMgr.ResyncInformers()\n }\n \n // ClusterEvent contains information about cluster event\n type ClusterEvent struct {\n-\tType    watch.EventType\n-\tCluster *appv1.Cluster\n+        Type    watch.EventType\n+        Cluster *appv1.Cluster\n }\n \n func (db *db) WatchClusters(ctx context.Context,\n-\thandleAddEvent func(cluster *appv1.Cluster),\n-\thandleModEvent func(oldCluster *appv1.Cluster, newCluster *appv1.Cluster),\n-\thandleDeleteEvent func(clusterServer string)) error {\n-\tlocalCls, err := db.GetCluster(ctx, appv1.KubernetesInternalAPIServerAddr)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\thandleAddEvent(localCls)\n-\n-\tdb.watchSecrets(\n-\t\tctx,\n-\t\tcommon.LabelValueSecretTypeCluster,\n-\n-\t\tfunc(secret *apiv1.Secret) {\n-\t\t\tcluster, err := SecretToCluster(secret)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", secret.Name)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif cluster.Server == appv1.KubernetesInternalAPIServerAddr {\n-\t\t\t\t// change local cluster event to modified or deleted, since it cannot be re-added or deleted\n-\t\t\t\thandleModEvent(localCls, cluster)\n-\t\t\t\tlocalCls = cluster\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\thandleAddEvent(cluster)\n-\t\t},\n-\n-\t\tfunc(oldSecret *apiv1.Secret, newSecret *apiv1.Secret) {\n-\t\t\toldCluster, err := SecretToCluster(oldSecret)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", oldSecret.Name)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tnewCluster, err := SecretToCluster(newSecret)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", newSecret.Name)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif newCluster.Server == appv1.KubernetesInternalAPIServerAddr {\n-\t\t\t\tlocalCls = newCluster\n-\t\t\t}\n-\t\t\thandleModEvent(oldCluster, newCluster)\n-\t\t},\n-\n-\t\tfunc(secret *apiv1.Secret) {\n-\t\t\tif string(secret.Data[\"server\"]) == appv1.KubernetesInternalAPIServerAddr {\n-\t\t\t\t// change local cluster event to modified or deleted, since it cannot be re-added or deleted\n-\t\t\t\thandleModEvent(localCls, db.getLocalCluster())\n-\t\t\t\tlocalCls = db.getLocalCluster()\n-\t\t\t} else {\n-\t\t\t\thandleDeleteEvent(string(secret.Data[\"server\"]))\n-\t\t\t}\n-\t\t},\n-\t)\n-\n-\treturn err\n+        handleAddEvent func(cluster *appv1.Cluster),\n+        handleModEvent func(oldCluster *appv1.Cluster, newCluster *appv1.Cluster),\n+        handleDeleteEvent func(clusterServer string)) error {\n+        localCls, err := db.GetCluster(ctx, appv1.KubernetesInternalAPIServerAddr)\n+        if err != nil {\n+                return err\n+        }\n+        handleAddEvent(localCls)\n+\n+        db.watchSecrets(\n+                ctx,\n+                common.LabelValueSecretTypeCluster,\n+\n+                func(secret *apiv1.Secret) {\n+                        cluster, err := SecretToCluster(secret)\n+                        if err != nil {\n+                                log.Errorf(\"could not unmarshal cluster secret %s\", secret.Name)\n+                                return\n+                        }\n+                        if cluster.Server == appv1.KubernetesInternalAPIServerAddr {\n+                                // change local cluster event to modified or deleted, since it cannot be re-added or deleted\n+                                handleModEvent(localCls, cluster)\n+                                localCls = cluster\n+                                return\n+                        }\n+                        handleAddEvent(cluster)\n+                },\n+\n+                func(oldSecret *apiv1.Secret, newSecret *apiv1.Secret) {\n+                        oldCluster, err := SecretToCluster(oldSecret)\n+                        if err != nil {\n+                                log.Errorf(\"could not unmarshal cluster secret %s\", oldSecret.Name)\n+                                return\n+                        }\n+                        newCluster, err := SecretToCluster(newSecret)\n+                        if err != nil {\n+                                log.Errorf(\"could not unmarshal cluster secret %s\", newSecret.Name)\n+                                return\n+                        }\n+                        if newCluster.Server == appv1.KubernetesInternalAPIServerAddr {\n+                                localCls = newCluster\n+                        }\n+                        handleModEvent(oldCluster, newCluster)\n+                },\n+\n+                func(secret *apiv1.Secret) {\n+                        if string(secret.Data[\"server\"]) == appv1.KubernetesInternalAPIServerAddr {\n+                                // change local cluster event to modified or deleted, since it cannot be re-added or deleted\n+                                handleModEvent(localCls, db.getLocalCluster())\n+                                localCls = db.getLocalCluster()\n+                        } else {\n+                                handleDeleteEvent(string(secret.Data[\"server\"]))\n+                        }\n+                },\n+        )\n+\n+        return err\n }\n \n func (db *db) getClusterSecret(server string) (*apiv1.Secret, error) {\n-\tclusterSecrets, err := db.listSecretsByType(common.LabelValueSecretTypeCluster)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tsrv := strings.TrimRight(server, \"/\")\n-\tfor _, clusterSecret := range clusterSecrets {\n-\t\tif strings.TrimRight(string(clusterSecret.Data[\"server\"]), \"/\") == srv {\n-\t\t\treturn clusterSecret, nil\n-\t\t}\n-\t}\n-\treturn nil, status.Errorf(codes.NotFound, \"cluster %q not found\", server)\n+        clusterSecrets, err := db.listSecretsByType(common.LabelValueSecretTypeCluster)\n+        if err != nil {\n+                return nil, err\n+        }\n+        srv := strings.TrimRight(server, \"/\")\n+        for _, clusterSecret := range clusterSecrets {\n+                if strings.TrimRight(string(clusterSecret.Data[\"server\"]), \"/\") == srv {\n+                        return clusterSecret, nil\n+                }\n+        }\n+        return nil, status.Errorf(codes.NotFound, \"cluster %q not found\", server)\n }\n \n // GetCluster returns a cluster from a query\n func (db *db) GetCluster(_ context.Context, server string) (*appv1.Cluster, error) {\n-\tinformer, err := db.settingsMgr.GetSecretsInformer()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tres, err := informer.GetIndexer().ByIndex(settings.ByClusterURLIndexer, server)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif len(res) > 0 {\n-\t\treturn SecretToCluster(res[0].(*apiv1.Secret))\n-\t}\n-\tif server == appv1.KubernetesInternalAPIServerAddr {\n-\t\treturn db.getLocalCluster(), nil\n-\t}\n-\n-\treturn nil, status.Errorf(codes.NotFound, \"cluster %q not found\", server)\n+        informer, err := db.settingsMgr.GetSecretsInformer()\n+        if err != nil {\n+                return nil, err\n+        }\n+        res, err := informer.GetIndexer().ByIndex(settings.ByClusterURLIndexer, server)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if len(res) > 0 {\n+                return SecretToCluster(res[0].(*apiv1.Secret))\n+        }\n+        if server == appv1.KubernetesInternalAPIServerAddr {\n+                return db.getLocalCluster(), nil\n+        }\n+\n+        return nil, status.Errorf(codes.NotFound, \"cluster %q not found\", server)\n }\n \n // GetProjectClusters return project scoped clusters by given project name\n func (db *db) GetProjectClusters(ctx context.Context, project string) ([]*appv1.Cluster, error) {\n-\tinformer, err := db.settingsMgr.GetSecretsInformer()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get secrets informer: %w\", err)\n-\t}\n-\tsecrets, err := informer.GetIndexer().ByIndex(settings.ByProjectClusterIndexer, project)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get index by project cluster indexer for project %q: %w\", project, err)\n-\t}\n-\tvar res []*appv1.Cluster\n-\tfor i := range secrets {\n-\t\tcluster, err := SecretToCluster(secrets[i].(*apiv1.Secret))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to convert secret to cluster: %w\", err)\n-\t\t}\n-\t\tres = append(res, cluster)\n-\t}\n-\treturn res, nil\n+        informer, err := db.settingsMgr.GetSecretsInformer()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to get secrets informer: %w\", err)\n+        }\n+        secrets, err := informer.GetIndexer().ByIndex(settings.ByProjectClusterIndexer, project)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to get index by project cluster indexer for project %q: %w\", project, err)\n+        }\n+        var res []*appv1.Cluster\n+        for i := range secrets {\n+                cluster, err := SecretToCluster(secrets[i].(*apiv1.Secret))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to convert secret to cluster: %w\", err)\n+                }\n+                res = append(res, cluster)\n+        }\n+        return res, nil\n }\n \n func (db *db) GetClusterServersByName(ctx context.Context, name string) ([]string, error) {\n-\tinformer, err := db.settingsMgr.GetSecretsInformer()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// if local cluster name is not overridden and specified name is local cluster name, return local cluster server\n-\tlocalClusterSecrets, err := informer.GetIndexer().ByIndex(settings.ByClusterURLIndexer, appv1.KubernetesInternalAPIServerAddr)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif len(localClusterSecrets) == 0 && db.getLocalCluster().Name == name {\n-\t\treturn []string{appv1.KubernetesInternalAPIServerAddr}, nil\n-\t}\n-\n-\tsecrets, err := informer.GetIndexer().ByIndex(settings.ByClusterNameIndexer, name)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvar res []string\n-\tfor i := range secrets {\n-\t\ts := secrets[i].(*apiv1.Secret)\n-\t\tres = append(res, strings.TrimRight(string(s.Data[\"server\"]), \"/\"))\n-\t}\n-\treturn res, nil\n+        informer, err := db.settingsMgr.GetSecretsInformer()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // if local cluster name is not overridden and specified name is local cluster name, return local cluster server\n+        localClusterSecrets, err := informer.GetIndexer().ByIndex(settings.ByClusterURLIndexer, appv1.KubernetesInternalAPIServerAddr)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if len(localClusterSecrets) == 0 && db.getLocalCluster().Name == name {\n+                return []string{appv1.KubernetesInternalAPIServerAddr}, nil\n+        }\n+\n+        secrets, err := informer.GetIndexer().ByIndex(settings.ByClusterNameIndexer, name)\n+        if err != nil {\n+                return nil, err\n+        }\n+        var res []string\n+        for i := range secrets {\n+                s := secrets[i].(*apiv1.Secret)\n+                res = append(res, strings.TrimRight(string(s.Data[\"server\"]), \"/\"))\n+        }\n+        return res, nil\n }\n \n // UpdateCluster updates a cluster\n func (db *db) UpdateCluster(ctx context.Context, c *appv1.Cluster) (*appv1.Cluster, error) {\n-\tclusterSecret, err := db.getClusterSecret(c.Server)\n-\tif err != nil {\n-\t\tif status.Code(err) == codes.NotFound {\n-\t\t\treturn db.CreateCluster(ctx, c)\n-\t\t}\n-\t\treturn nil, err\n-\t}\n-\tif err := clusterToSecret(c, clusterSecret); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclusterSecret, err = db.kubeclientset.CoreV1().Secrets(db.ns).Update(ctx, clusterSecret, metav1.UpdateOptions{})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tcluster, err := SecretToCluster(clusterSecret)\n-\tif err != nil {\n-\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", clusterSecret.Name)\n-\t\treturn nil, err\n-\t}\n-\treturn cluster, db.settingsMgr.ResyncInformers()\n+        clusterSecret, err := db.getClusterSecret(c.Server)\n+        if err != nil {\n+                if status.Code(err) == codes.NotFound {\n+                        return db.CreateCluster(ctx, c)\n+                }\n+                return nil, err\n+        }\n+        if err := clusterToSecret(c, clusterSecret); err != nil {\n+                return nil, err\n+        }\n+\n+        clusterSecret, err = db.kubeclientset.CoreV1().Secrets(db.ns).Update(ctx, clusterSecret, metav1.UpdateOptions{})\n+        if err != nil {\n+                return nil, err\n+        }\n+        cluster, err := SecretToCluster(clusterSecret)\n+        if err != nil {\n+                log.Errorf(\"could not unmarshal cluster secret %s\", clusterSecret.Name)\n+                return nil, err\n+        }\n+        return cluster, db.settingsMgr.ResyncInformers()\n }\n \n // DeleteCluster deletes a cluster by name\n func (db *db) DeleteCluster(ctx context.Context, server string) error {\n-\tsecret, err := db.getClusterSecret(server)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        secret, err := db.getClusterSecret(server)\n+        if err != nil {\n+                return err\n+        }\n \n-\terr = db.deleteSecret(ctx, secret)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        err = db.deleteSecret(ctx, secret)\n+        if err != nil {\n+                return err\n+        }\n \n-\treturn db.settingsMgr.ResyncInformers()\n+        return db.settingsMgr.ResyncInformers()\n }\n \n // clusterToData converts a cluster object to string data for serialization to a secret\n func clusterToSecret(c *appv1.Cluster, secret *apiv1.Secret) error {\n-\tdata := make(map[string][]byte)\n-\tdata[\"server\"] = []byte(strings.TrimRight(c.Server, \"/\"))\n-\tif c.Name == \"\" {\n-\t\tdata[\"name\"] = []byte(c.Server)\n-\t} else {\n-\t\tdata[\"name\"] = []byte(c.Name)\n-\t}\n-\tif len(c.Namespaces) != 0 {\n-\t\tdata[\"namespaces\"] = []byte(strings.Join(c.Namespaces, \",\"))\n-\t}\n-\tconfigBytes, err := json.Marshal(c.Config)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdata[\"config\"] = configBytes\n-\tif c.Shard != nil {\n-\t\tdata[\"shard\"] = []byte(strconv.Itoa(int(*c.Shard)))\n-\t}\n-\tif c.ClusterResources {\n-\t\tdata[\"clusterResources\"] = []byte(\"true\")\n-\t}\n-\tif c.Project != \"\" {\n-\t\tdata[\"project\"] = []byte(c.Project)\n-\t}\n-\tsecret.Data = data\n-\n-\tsecret.Labels = c.Labels\n-\tsecret.Annotations = c.Annotations\n-\n-\tif secret.Annotations == nil {\n-\t\tsecret.Annotations = make(map[string]string)\n-\t}\n-\n-\tif c.RefreshRequestedAt != nil {\n-\t\tsecret.Annotations[appv1.AnnotationKeyRefresh] = c.RefreshRequestedAt.Format(time.RFC3339)\n-\t} else {\n-\t\tdelete(secret.Annotations, appv1.AnnotationKeyRefresh)\n-\t}\n-\taddSecretMetadata(secret, common.LabelValueSecretTypeCluster)\n-\treturn nil\n+        data := make(map[string][]byte)\n+        data[\"server\"] = []byte(strings.TrimRight(c.Server, \"/\"))\n+        if c.Name == \"\" {\n+                data[\"name\"] = []byte(c.Server)\n+        } else {\n+                data[\"name\"] = []byte(c.Name)\n+        }\n+        if len(c.Namespaces) != 0 {\n+                data[\"namespaces\"] = []byte(strings.Join(c.Namespaces, \",\"))\n+        }\n+        configBytes, err := json.Marshal(c.Config)\n+        if err != nil {\n+                return err\n+        }\n+        data[\"config\"] = configBytes\n+        if c.Shard != nil {\n+                data[\"shard\"] = []byte(strconv.Itoa(int(*c.Shard)))\n+        }\n+        if c.ClusterResources {\n+                data[\"clusterResources\"] = []byte(\"true\")\n+        }\n+        if c.Project != \"\" {\n+                data[\"project\"] = []byte(c.Project)\n+        }\n+        secret.Data = data\n+\n+        secret.Labels = c.Labels\n+        secret.Annotations = c.Annotations\n+\n+        if secret.Annotations == nil {\n+                secret.Annotations = make(map[string]string)\n+        }\n+\n+        if c.RefreshRequestedAt != nil {\n+                secret.Annotations[appv1.AnnotationKeyRefresh] = c.RefreshRequestedAt.Format(time.RFC3339)\n+        } else {\n+                delete(secret.Annotations, appv1.AnnotationKeyRefresh)\n+        }\n+        addSecretMetadata(secret, common.LabelValueSecretTypeCluster)\n+        return nil\n }\n \n // SecretToCluster converts a secret into a Cluster object\n func SecretToCluster(s *apiv1.Secret) (*appv1.Cluster, error) {\n-\tvar config appv1.ClusterConfig\n-\tif len(s.Data[\"config\"]) > 0 {\n-\t\terr := json.Unmarshal(s.Data[\"config\"], &config)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to unmarshal cluster config: %w\", err)\n-\t\t}\n-\t}\n-\n-\tvar namespaces []string\n-\tfor _, ns := range strings.Split(string(s.Data[\"namespaces\"]), \",\") {\n-\t\tif ns = strings.TrimSpace(ns); ns != \"\" {\n-\t\t\tnamespaces = append(namespaces, ns)\n-\t\t}\n-\t}\n-\tvar refreshRequestedAt *metav1.Time\n-\tif v, found := s.Annotations[appv1.AnnotationKeyRefresh]; found {\n-\t\trequestedAt, err := time.Parse(time.RFC3339, v)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"Error while parsing date in cluster secret '%s': %v\", s.Name, err)\n-\t\t} else {\n-\t\t\trefreshRequestedAt = &metav1.Time{Time: requestedAt}\n-\t\t}\n-\t}\n-\tvar shard *int64\n-\tif shardStr := s.Data[\"shard\"]; shardStr != nil {\n-\t\tif val, err := strconv.Atoi(string(shardStr)); err != nil {\n-\t\t\tlog.Warnf(\"Error while parsing shard in cluster secret '%s': %v\", s.Name, err)\n-\t\t} else {\n-\t\t\tshard = pointer.Int64Ptr(int64(val))\n-\t\t}\n-\t}\n-\n-\t// copy labels and annotations excluding system ones\n-\tlabels := map[string]string{}\n-\tif s.Labels != nil {\n-\t\tlabels = collections.CopyStringMap(s.Labels)\n-\t\tdelete(labels, common.LabelKeySecretType)\n-\t}\n-\tannotations := map[string]string{}\n-\tif s.Annotations != nil {\n-\t\tannotations = collections.CopyStringMap(s.Annotations)\n-\t\tdelete(annotations, common.AnnotationKeyManagedBy)\n-\t}\n-\n-\tcluster := appv1.Cluster{\n-\t\tID:                 string(s.UID),\n-\t\tServer:             strings.TrimRight(string(s.Data[\"server\"]), \"/\"),\n-\t\tName:               string(s.Data[\"name\"]),\n-\t\tNamespaces:         namespaces,\n-\t\tClusterResources:   string(s.Data[\"clusterResources\"]) == \"true\",\n-\t\tConfig:             config,\n-\t\tRefreshRequestedAt: refreshRequestedAt,\n-\t\tShard:              shard,\n-\t\tProject:            string(s.Data[\"project\"]),\n-\t\tLabels:             labels,\n-\t\tAnnotations:        annotations,\n-\t}\n-\treturn &cluster, nil\n+var config appv1.ClusterConfig\n+if len(s.Data[\"config\"]) > 0 {\n+err := json.Unmarshal(s.Data[\"config\"], &config)\n+if err != nil {\n+return nil, fmt.Errorf(\"failed to unmarshal cluster config: %w\", err)\n+}\n+}\n+\n+var namespaces []string\n+for _, ns := range strings.Split(string(s.Data[\"namespaces\"]), \",\") {\n+if ns = strings.TrimSpace(ns); ns != \"\" {\n+namespaces = append(namespaces, ns)\n+}\n+}\n+var refreshRequestedAt *metav1.Time\n+if v, found := s.Annotations[appv1.AnnotationKeyRefresh]; found {\n+requestedAt, err := time.Parse(time.RFC3339, v)\n+if err != nil {\n+log.Warnf(\"Error while parsing date in cluster secret '%s': %v\", s.Name, err)\n+} else {\n+refreshRequestedAt = &metav1.Time{Time: requestedAt}\n+}\n+}\n+var shard *int64\n+if shardStr := s.Data[\"shard\"]; shardStr != nil {\n+if val, err := strconv.Atoi(string(shardStr)); err != nil {\n+log.Warnf(\"Error while parsing shard in cluster secret '%s': %v\", s.Name, err)\n+} else {\n+shard = pointer.Int64Ptr(int64(val))\n+}\n+}\n+\n+// copy labels and annotations excluding system ones\n+labels := map[string]string{}\n+if s.Labels != nil {\n+labels = collections.CopyStringMap(s.Labels)\n+delete(labels, common.LabelKeySecretType)\n+}\n+annotations := map[string]string{}\n+if s.Annotations != nil {\n+annotations = collections.CopyStringMap(s.Annotations)\n+delete(annotations, common.AnnotationKeyManagedBy)\n+delete(annotations, \"kubectl.kubernetes.io/last-applied-configuration\")\n+}\n+\n+cluster := appv1.Cluster{\n+ID:                 string(s.UID),\n+Server:             strings.TrimRight(string(s.Data[\"server\"]), \"/\"),\n+Name:               string(s.Data[\"name\"]),\n+Namespaces:         namespaces,\n+ClusterResources:   string(s.Data[\"clusterResources\"]) == \"true\",\n+Config:             config,\n+RefreshRequestedAt: refreshRequestedAt,\n+Shard:              shard,\n+Project:            string(s.Data[\"project\"]),\n+Labels:             labels,\n+Annotations:        annotations,\n+}\n+return &cluster, nil\n }\n"}
{"cve":"CVE-2023-25168:0708", "fix_patch": ""}
{"cve":"CVE-2021-3583:0708", "fix_patch": "diff --git a/lib/ansible/template/__init__.py b/lib/ansible/template/__init__.py\nindex 52b9f71977..634f7f7e02 100644\n--- a/lib/ansible/template/__init__.py\n+++ b/lib/ansible/template/__init__.py\n@@ -52,7 +52,7 @@ from ansible.template.template import AnsibleJ2Template\n from ansible.template.vars import AnsibleJ2Vars\n from ansible.utils.collection_loader import AnsibleCollectionRef\n from ansible.utils.display import Display\n-from ansible.utils.unsafe_proxy import wrap_var\n+from ansible.utils.unsafe_proxy import wrap_var, AnsibleUnsafe\n \n display = Display()\n \n@@ -449,87 +449,71 @@ class Templar:\n         else:\n             self._basedir = './'\n \n-        if shared_loader_obj:\n-            self._filter_loader = getattr(shared_loader_obj, 'filter_loader')\n-            self._test_loader = getattr(shared_loader_obj, 'test_loader')\n-            self._lookup_loader = getattr(shared_loader_obj, 'lookup_loader')\n-        else:\n-            self._filter_loader = filter_loader\n-            self._test_loader = test_loader\n-            self._lookup_loader = lookup_loader\n+        # a list of variables that should not be templated\n+        self._untemplated_vars = C.MAGIC_VARIABLE_MAPPING.keys()\n+\n+        # a list of jinja2 extensions to use\n+        self._extensions = self._get_extensions()\n \n-        # flags to determine whether certain failures during templating\n-        # should result in fatal errors being raised\n-        self._fail_on_lookup_errors = True\n-        self._fail_on_filter_errors = True\n-        self._fail_on_undefined_errors = C.DEFAULT_UNDEFINED_VAR_BEHAVIOR\n+        # create the jinja2 environment\n+        if self._loader:\n+            env_loader = FileSystemLoader(self._basedir)\n+        else:\n+            env_loader = None\n \n         self.environment = AnsibleEnvironment(\n-            trim_blocks=True,\n+            block_start_string=C.DEFAULT_JINJA2_BLOCK_START_STRING,\n+            block_end_string=C.DEFAULT_JINJA2_BLOCK_END_STRING,\n+            variable_start_string=C.DEFAULT_JINJA2_VARIABLE_START_STRING,\n+            variable_end_string=C.DEFAULT_JINJA2_VARIABLE_END_STRING,\n+            comment_start_string=C.DEFAULT_JINJA2_COMMENT_START_STRING,\n+            comment_end_string=C.DEFAULT_JINJA2_COMMENT_END_STRING,\n+            extensions=self._extensions,\n+            loader=env_loader,\n+            trim_blocks=C.DEFAULT_JINJA2_TRIM_BLOCKS,\n+            lstrip_blocks=C.DEFAULT_JINJA2_LSTRIP_BLOCKS,\n             undefined=AnsibleUndefined,\n-            extensions=self._get_extensions(),\n-            finalize=self._finalize,\n-            loader=FileSystemLoader(self._basedir),\n+            # this is an internal ansible switch that is not supported in general\n+            # it will be removed in the future\n+            # line_statement_prefix='@',\n         )\n \n-        # the current rendering context under which the templar class is working\n-        self.cur_context = None\n-\n-        self.SINGLE_VAR = re.compile(r\"^%s\\s*(\\w*)\\s*%s$\" % (self.environment.variable_start_string, self.environment.variable_end_string))\n+        # jinja2 will use the last newline character defined in the list, so we move\n+        # the one from the config file to the end of the list if it's different.\n+        if C.DEFAULT_NEWLINE_SEQUENCE != '\\n':\n+            self.environment.newline_sequence = C.DEFAULT_NEWLINE_SEQUENCE\n \n-        self._clean_regex = re.compile(r'(?:%s|%s|%s|%s)' % (\n-            self.environment.variable_start_string,\n-            self.environment.block_start_string,\n-            self.environment.block_end_string,\n-            self.environment.variable_end_string\n-        ))\n-        self._no_type_regex = re.compile(r'.*?\\|\\s*(?:%s)(?:\\([^\\|]*\\))?\\s*\\)?\\s*(?:%s)' %\n-                                         ('|'.join(C.STRING_TYPE_FILTERS), self.environment.variable_end_string))\n+        self.cur_context = None\n \n     def _get_filters(self):\n-        '''\n-        Returns filter plugins, after loading and caching them if need be\n-        '''\n-\n-        if self._filters is not None:\n-            return self._filters.copy()\n-\n-        self._filters = dict()\n+        if self._filters is None:\n+            self._filters = {}\n+            for filter_name in filter_loader:\n+                if filter_loader.is_valid_fqcr(filter_name + '.py'):\n+                    continue\n \n-        for fp in self._filter_loader.all():\n-            self._filters.update(fp.filters())\n+                plugin = filter_loader.get(filter_name)\n+                self._filters.update(plugin.filters())\n \n-        return self._filters.copy()\n+        return self._filters\n \n     def _get_tests(self):\n-        '''\n-        Returns tests plugins, after loading and caching them if need be\n-        '''\n-\n-        if self._tests is not None:\n-            return self._tests.copy()\n+        if self._tests is None:\n+            self._tests = {}\n+            for test_name in test_loader:\n+                if test_loader.is_valid_fqcr(test_name + '.py'):\n+                    continue\n \n-        self._tests = dict()\n-        for fp in self._test_loader.all():\n-            self._tests.update(fp.tests())\n+                plugin = test_loader.get(test_name)\n+                self._tests.update(plugin.tests())\n \n-        return self._tests.copy()\n+        return self._tests\n \n     def _get_extensions(self):\n-        '''\n-        Return jinja2 extensions to load.\n-\n-        If some extensions are set via jinja_extensions in ansible.cfg, we try\n-        to load them with the jinja environment.\n-        '''\n-\n-        jinja_exts = []\n         if C.DEFAULT_JINJA2_EXTENSIONS:\n-            # make sure the configuration directive doesn't contain spaces\n-            # and split extensions in an array\n-            jinja_exts = C.DEFAULT_JINJA2_EXTENSIONS.replace(\" \", \"\").split(',')\n-\n-        return jinja_exts\n+            return C.DEFAULT_JINJA2_EXTENSIONS.split(',')\n+        else:\n+            return []\n \n     @property\n     def available_variables(self):\n@@ -538,16 +522,13 @@ class Templar:\n     @available_variables.setter\n     def available_variables(self, variables):\n         '''\n-        Sets the list of template variables this Templar instance will use\n-        to template things, so we don't have to pass them around between\n-        internal methods. We also clear the template cache here, as the variables\n-        are being changed.\n+        Sets the variables that will be available to the templating engine.\n+        This can be re-called at any time to update the variables, but will\n+        invalidate any cache.\n         '''\n-\n-        if not isinstance(variables, dict):\n-            raise AnsibleAssertionError(\"the type of 'variables' should be a dict but was a %s\" % (type(variables)))\n-        self._available_variables = variables\n-        self._cached_result = {}\n+        if self._available_variables != variables:\n+            self._available_variables = variables\n+            self._cached_result = {}\n \n     def set_available_variables(self, variables):\n         display.deprecated(\n@@ -559,197 +540,187 @@ class Templar:\n     def template(self, variable, convert_bare=False, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None,\n                  convert_data=True, static_vars=None, cache=True, disable_lookups=False):\n         '''\n-        Templates (possibly recursively) any given data as input. If convert_bare is\n-        set to True, the given data will be wrapped as a jinja2 variable ('{{foo}}')\n-        before being sent through the template engine.\n+        This is the main templating entry point. The variable load (a string)\n+        is processed as a jinja2 template and the result is returned.\n         '''\n-        static_vars = [''] if static_vars is None else static_vars\n \n-        # Don't template unsafe variables, just return them.\n-        if hasattr(variable, '__UNSAFE__'):\n-            return variable\n-\n-        if fail_on_undefined is None:\n-            fail_on_undefined = self._fail_on_undefined_errors\n+        # NOTE: maybe we should use a custom string class for this instead\n+        if static_vars is None:\n+            static_vars = self._untemplated_vars\n+\n+        if isinstance(variable, (_AnsibleUnicodeContainer, _AnsibleSequenceContainer, _AnsibleDictContainer)):\n+            variable = variable.get_raw_data()\n+\n+        # if the variable is not a string, we are done with it, as it cannot be a template\n+        if not isinstance(variable, string_types):\n+            if convert_data:\n+                if isinstance(variable, (NON_TEMPLATED_TYPES)):\n+                    return variable\n+\n+                # if this is a dict or a list, we template each of the values\n+                elif isinstance(variable, Mapping):\n+                    # convert to a new mutable dict, as we need to change it\n+                    result = _AnsibleDictContainer()\n+                    # shallow copy, so we only copy the first level of keys\n+                    for k in variable:\n+                        result[k] = self.template(variable[k], convert_bare=convert_bare, fail_on_undefined=fail_on_undefined, overrides=overrides,\n+                                                    escape_backslashes=escape_backslashes, preserve_trailing_newlines=preserve_trailing_newlines,\n+                                                    disable_lookups=disable_lookups)\n+                    return result\n+                elif isinstance(variable, Sequence):\n+                    # convert to a new mutable list, as we need to change it\n+                    result = _AnsibleSequenceContainer()\n+                    for v in variable:\n+                        result.append(self.template(v, convert_bare=convert_bare, fail_on_undefined=fail_on_undefined, overrides=overrides,\n+                                                      escape_backslashes=escape_backslashes, preserve_trailing_newlines=preserve_trailing_newlines,\n+                                                      disable_lookups=disable_lookups))\n+                    return result\n+                else:\n+                    # the variable is not a string, not a basic type and not a dict/list\n+                    # so we are done with it\n+                    return variable\n+            else:\n+                return variable\n \n+        # convert_bare is a feature where if a variable is specified as\n+        # just '{{ foo }}' we return the value of foo, rather than a\n+        # string representation of it. This is useful for when statements\n+        # and other items that need to be evaluated by python.\n+        if convert_bare:\n+            # Before we try to convert, we check to see if the string\n+            # is a bare variable. If it is, then we call a helper to do\n+            # the actual conversion.\n+            # We used to use a simple '{{' + varname + '}}' check here but in\n+            # some cases (unicode) that would fail. The lexer check is more\n+            # robust.\n+            tokens = list(self.environment.lex(variable))\n+            if len(tokens) == 4 and tokens[0][1] == 'variable_begin' and tokens[3][1] == 'variable_end':\n+                result = self._convert_bare_variable(variable)\n+                if not isinstance(result, AnsibleUndefined):\n+                    return result\n+                # if we have an undefined var here, we need to fall through so it is\n+                # handled by the normal templating and error handling code\n+\n+        # If we have a cache hit, we're done\n+        if cache and variable in self._cached_result:\n+            return self._cached_result[variable]\n+\n+        # if this does not look like a template, we are done with it\n+        if not self.is_template(variable):\n+            # special case for when the only thing in the string is a trailing\n+            # newline character. In this case, we need to return an empty string\n+            # and make a note to preserve the trailing newlines so they are\n+            # properly added back to the final result.\n+            if variable == self.environment.newline_sequence:\n+                variable = u''\n+            return to_text(variable, nonstring='strict')\n+\n+        # otherwise, we try to template it\n         try:\n-            if convert_bare:\n-                variable = self._convert_bare_variable(variable)\n-\n-            if isinstance(variable, string_types):\n-                result = variable\n-\n-                if self.is_possibly_template(variable):\n-                    # Check to see if the string we are trying to render is just referencing a single\n-                    # var.  In this case we don't want to accidentally change the type of the variable\n-                    # to a string by using the jinja template renderer. We just want to pass it.\n-                    only_one = self.SINGLE_VAR.match(variable)\n-                    if only_one:\n-                        var_name = only_one.group(1)\n-                        if var_name in self._available_variables:\n-                            resolved_val = self._available_variables[var_name]\n-                            if isinstance(resolved_val, NON_TEMPLATED_TYPES):\n-                                return resolved_val\n-                            elif resolved_val is None:\n-                                return C.DEFAULT_NULL_REPRESENTATION\n-\n-                    # Using a cache in order to prevent template calls with already templated variables\n-                    sha1_hash = None\n-                    if cache:\n-                        variable_hash = sha1(text_type(variable).encode('utf-8'))\n-                        options_hash = sha1(\n-                            (\n-                                text_type(preserve_trailing_newlines) +\n-                                text_type(escape_backslashes) +\n-                                text_type(fail_on_undefined) +\n-                                text_type(overrides)\n-                            ).encode('utf-8')\n-                        )\n-                        sha1_hash = variable_hash.hexdigest() + options_hash.hexdigest()\n-                    if cache and sha1_hash in self._cached_result:\n-                        result = self._cached_result[sha1_hash]\n-                    else:\n-                        result = self.do_template(\n-                            variable,\n-                            preserve_trailing_newlines=preserve_trailing_newlines,\n-                            escape_backslashes=escape_backslashes,\n-                            fail_on_undefined=fail_on_undefined,\n-                            overrides=overrides,\n-                            disable_lookups=disable_lookups,\n-                        )\n-\n-                        if not USE_JINJA2_NATIVE:\n-                            unsafe = hasattr(result, '__UNSAFE__')\n-                            if convert_data and not self._no_type_regex.match(variable):\n-                                # if this looks like a dictionary or list, convert it to such using the safe_eval method\n-                                if (result.startswith(\"{\") and not result.startswith(self.environment.variable_start_string)) or \\\n-                                        result.startswith(\"[\") or result in (\"True\", \"False\"):\n-                                    eval_results = safe_eval(result, include_exceptions=True)\n-                                    if eval_results[1] is None:\n-                                        result = eval_results[0]\n-                                        if unsafe:\n-                                            result = wrap_var(result)\n-                                    else:\n-                                        # FIXME: if the safe_eval raised an error, should we do something with it?\n-                                        pass\n-\n-                        # we only cache in the case where we have a single variable\n-                        # name, to make sure we're not putting things which may otherwise\n-                        # be dynamic in the cache (filters, lookups, etc.)\n-                        if cache and only_one:\n-                            self._cached_result[sha1_hash] = result\n+            result = self.do_template(variable, preserve_trailing_newlines=preserve_trailing_newlines, escape_backslashes=escape_backslashes,\n+                                      fail_on_undefined=fail_on_undefined, overrides=overrides, disable_lookups=disable_lookups)\n \n+            # we have to use another try/except block here, as we need to do the\n+            # variable caching before we try to convert the result to text (as\n+            # some things may not be convertable, and we want to avoid templating\n+            # them again in the future)\n+            try:\n+                cache_result = result\n+                result = to_text(result, errors='strict', nonstring='strict')\n+            except UnicodeError as e:\n+                # an error here means that the data contains binary and cannot be\n+                # decoded properly into a string. In this case, we just return the\n+                # data as-is\n+                return result\n+            except Exception:\n+                # we specifically catch anything here, as the result may be a complex object\n+                # which has a __str__ or __repr__ method that raises an exception, in which\n+                # case we just return the object as-is and do not cache the result\n                 return result\n \n-            elif is_sequence(variable):\n-                return [self.template(\n-                    v,\n-                    preserve_trailing_newlines=preserve_trailing_newlines,\n-                    fail_on_undefined=fail_on_undefined,\n-                    overrides=overrides,\n-                    disable_lookups=disable_lookups,\n-                ) for v in variable]\n-            elif isinstance(variable, Mapping):\n-                d = {}\n-                # we don't use iteritems() here to avoid problems if the underlying dict\n-                # changes sizes due to the templating, which can happen with hostvars\n-                for k in variable.keys():\n-                    if k not in static_vars:\n-                        d[k] = self.template(\n-                            variable[k],\n-                            preserve_trailing_newlines=preserve_trailing_newlines,\n-                            fail_on_undefined=fail_on_undefined,\n-                            overrides=overrides,\n-                            disable_lookups=disable_lookups,\n-                        )\n-                    else:\n-                        d[k] = variable[k]\n-                return d\n-            else:\n-                return variable\n+            # if we were successful, we cache the result\n+            if cache and variable not in self._cached_result:\n+                self._cached_result[variable] = cache_result\n \n-        except AnsibleFilterError:\n-            if self._fail_on_filter_errors:\n+            return result\n+        except (AnsibleError) as e:\n+            if fail_on_undefined is None:\n+                fail_on_undefined = self._fail_on_undefined_errors\n+            if fail_on_undefined:\n                 raise\n-            else:\n-                return variable\n+            return variable\n \n     def is_template(self, data):\n-        '''lets us know if data has a template'''\n-        if isinstance(data, string_types):\n-            return is_template(data, self.environment)\n-        elif isinstance(data, (list, tuple)):\n-            for v in data:\n-                if self.is_template(v):\n-                    return True\n-        elif isinstance(data, dict):\n-            for k in data:\n-                if self.is_template(k) or self.is_template(data[k]):\n-                    return True\n-        return False\n+        '''\n+        This wrapper around the is_template check will short-circuit if the data is not a string.\n+        It also includes list or dict evaluation as those may contain templates.\n+        '''\n+        if not isinstance(data, string_types):\n+            if self._is_iterable_nonstring(data):\n+                # return True if it has a template\n+                return any(self.is_template(x) for x in data)\n+            return False\n+        else:\n+            # finally call fast C is_template\n+            return is_template(data, jinja_env=self.environment)\n \n     templatable = is_template\n \n     def is_possibly_template(self, data):\n-        '''Determines if a string looks like a template, by seeing if it\n-        contains a jinja2 start delimiter. Does not guarantee that the string\n-        is actually a template.\n-\n-        This is different than ``is_template`` which is more strict.\n-        This method may return ``True`` on a string that is not templatable.\n-\n-        Useful when guarding passing a string for templating, but when\n-        you want to allow the templating engine to make the final\n-        assessment which may result in ``TemplateSyntaxError``.\n         '''\n-        env = self.environment\n+        This method is a less-strict version of is_template, which just looks for\n+        the existence of the default string markers. This will have more false-positives\n+        than is_template(), but is much faster and can be used to pre-filter variables\n+        prior to handing them off to the full templating engine.\n+        '''\n         if isinstance(data, string_types):\n-            for marker in (env.block_start_string, env.variable_start_string, env.comment_start_string):\n-                if marker in data:\n-                    return True\n+            if C.DEFAULT_JINJA2_VARIABLE_START_STRING in data and C.DEFAULT_JINJA2_VARIABLE_END_STRING in data:\n+                return True\n+        elif self._is_iterable_nonstring(data):\n+            return any(self.is_possibly_template(x) for x in data)\n+\n         return False\n \n     def _convert_bare_variable(self, variable):\n         '''\n-        Wraps a bare string, which may have an attribute portion (ie. foo.bar)\n-        in jinja2 variable braces so that it is evaluated properly.\n+        Used by template() to convert a bare variable to its real type (rather than a string).\n         '''\n-\n-        if isinstance(variable, string_types):\n-            contains_filters = \"|\" in variable\n-            first_part = variable.split(\"|\")[0].split(\".\")[0].split(\"[\")[0]\n-            if (contains_filters or first_part in self._available_variables) and self.environment.variable_start_string not in variable:\n-                return \"%s%s%s\" % (self.environment.variable_start_string, variable, self.environment.variable_end_string)\n-\n-        # the variable didn't meet the conditions to be converted,\n-        # so just return it as-is\n-        return variable\n+        myvars = AnsibleJ2Vars(self, self.environment.globals)\n+        varname = self.environment.lex(variable)[1][2].strip()\n+        if varname in myvars:\n+            return myvars[varname]\n+        else:\n+            # rather than raise an error about an undefined variable,\n+            # we return an object that will do so when it is used.\n+            return self.environment.undefined(name=varname)\n \n     def _finalize(self, thing):\n         '''\n-        A custom finalize method for jinja2, which prevents None from being returned. This\n-        avoids a string of ``\"None\"`` as ``None`` has no importance in YAML.\n-\n-        If using ANSIBLE_JINJA2_NATIVE we bypass this and return the actual value always\n+        This is a data finalizer, which is called by Jinja2 after every templating\n+        operation. We use it to restore the safe/unsafe values of strings, as Jinja2\n+        always returns safe strings.\n         '''\n-        if USE_JINJA2_NATIVE:\n+        try:\n+            if self.cur_context.unsafe and not getattr(thing, '__UNSAFE__', False):\n+                return wrap_var(thing)\n+            return thing\n+        except Exception:\n             return thing\n-        return thing if thing is not None else ''\n \n     def _fail_lookup(self, name, *args, **kwargs):\n         raise AnsibleError(\"The lookup `%s` was found, however lookups were disabled from templating\" % name)\n \n     def _now_datetime(self, utc=False, fmt=None):\n-        '''jinja2 global function to return current datetime, potentially formatted via strftime'''\n+        ''' return the current time, and is used by the now() lookup plugin '''\n         if utc:\n             now = datetime.datetime.utcnow()\n         else:\n             now = datetime.datetime.now()\n \n         if fmt:\n-            return now.strftime(fmt)\n-\n-        return now\n+            return now.strftime(to_bytes(fmt))\n+        else:\n+            return now\n \n     def _query_lookup(self, name, *args, **kwargs):\n         ''' wrapper for lookup, force wantlist true'''\n@@ -757,59 +728,73 @@ class Templar:\n         return self._lookup(name, *args, **kwargs)\n \n     def _lookup(self, name, *args, **kwargs):\n-        instance = self._lookup_loader.get(name.lower(), loader=self._loader, templar=self)\n+        '''\n+        lookup plugins are special, because they were originally implemented as\n+        jinja2 globals. This is also used to cache the lookup results within a\n+        single template() call.\n+        '''\n+        display.vvv(\"lookup(name=%s, args=%s, kwargs=%s)\" % (name, args, kwargs))\n+\n+        instance = lookup_loader.get(name, loader=self._loader, templar=self)\n \n-        if instance is not None:\n-            wantlist = kwargs.pop('wantlist', False)\n-            allow_unsafe = kwargs.pop('allow_unsafe', C.DEFAULT_ALLOW_UNSAFE_LOOKUPS)\n-            errors = kwargs.pop('errors', 'strict')\n+        if instance:\n+            run_once = kwargs.pop('run_once', False)\n \n-            from ansible.utils.listify import listify_lookup_plugin_terms\n-            loop_terms = listify_lookup_plugin_terms(terms=args, templar=self, loader=self._loader, fail_on_undefined=True, convert_bare=False)\n-            # safely catch run failures per #5059\n+            # convert the lookup args to a tuple, so we can use it as a hash key\n+            # for caching. if we cannot convert the items to a tuple, we cannot\n+            # cache the results, as they are not hashable.\n+            try:\n+                args_hash = tuple(args)\n+                if run_once and args_hash in self.cur_context.lookup_cache:\n+                    return self.cur_context.lookup_cache[args_hash]\n+            except Exception:\n+                # if we failed to hash the args, we can't do any caching\n+                pass\n+\n+            # execute the lookup\n             try:\n-                ran = instance.run(loop_terms, variables=self._available_variables, **kwargs)\n+                ran = instance.run([to_text(i, errors='surrogate_or_strict') for i in args], variables=self._available_variables, **kwargs)\n             except (AnsibleUndefinedVariable, UndefinedError) as e:\n-                raise AnsibleUndefinedVariable(e)\n-            except Exception as e:\n-                if self._fail_on_lookup_errors:\n-                    msg = u\"An unhandled exception occurred while running the lookup plugin '%s'. Error was a %s, original message: %s\" % \\\n-                          (name, type(e), to_text(e))\n-                    if errors == 'warn':\n-                        display.warning(msg)\n-                    elif errors == 'ignore':\n-                        display.display(msg, log_only=True)\n+                # We want to allow lookup plugins to return an an error when a variable is\n+                # not defined. So we catch the exception here and raise a new one so that\n+                # we know it came from the lookup and can handle it appropriately.\n+                raise AnsibleUndefinedVariable(e, obj=name)\n+\n+            # convert bytestrings to text and ensure that wantlist is respected,\n+            # if the lookup plugin itself hasn't already.\n+            if is_sequence(ran):\n+                ran = [to_text(x, errors='surrogate_or_strict') for x in ran]\n+            elif not isinstance(ran, string_types):\n+                # if we have a non-string, non-sequence, we're done\n+                pass\n+            else:\n+                ran = to_text(ran, errors='surrogate_or_strict')\n+                if kwargs.get('wantlist', False):\n+                    if ran:\n+                        ran = [ran]\n                     else:\n-                        raise AnsibleError(to_native(msg))\n-                ran = [] if wantlist else None\n+                        # if we have an empty string, we want an empty list\n+                        ran = []\n \n-            if ran and not allow_unsafe:\n-                if wantlist:\n-                    ran = wrap_var(ran)\n-                else:\n-                    try:\n-                        ran = wrap_var(\",\".join(ran))\n-                    except TypeError:\n-                        # Lookup Plugins should always return lists.  Throw an error if that's not\n-                        # the case:\n-                        if not isinstance(ran, Sequence):\n-                            raise AnsibleError(\"The lookup plugin '%s' did not return a list.\"\n-                                               % name)\n-\n-                        # The TypeError we can recover from is when the value *inside* of the list\n-                        # is not a string\n-                        if len(ran) == 1:\n-                            ran = wrap_var(ran[0])\n-                        else:\n-                            ran = wrap_var(ran)\n-\n-                if self.cur_context:\n+            # if we are caching lookup results, we do so now\n+            try:\n+                if run_once:\n+                    self.cur_context.lookup_cache[args_hash] = ran\n+            except Exception:\n+                pass\n+\n+            if self.cur_context is not None and getattr(instance, 'SUPPORTS_CALLBACKS', False):\n+                if self._is_unsafe(ran):\n                     self.cur_context.unsafe = True\n             return ran\n         else:\n             raise AnsibleError(\"lookup plugin (%s) not found\" % name)\n \n     def do_template(self, data, preserve_trailing_newlines=True, escape_backslashes=True, fail_on_undefined=None, overrides=None, disable_lookups=False):\n+        from ansible.utils.unsafe_proxy import AnsibleUnsafe\n+        if isinstance(data, AnsibleUnsafe):\n+            return data\n+\n         if USE_JINJA2_NATIVE and not isinstance(data, string_types):\n             return data\n \n@@ -825,6 +810,9 @@ class Templar:\n             if overrides is None:\n                 myenv = self.environment.overlay()\n             else:\n+        if isinstance(data, AnsibleUnsafe):\n+            return data\n+\n                 myenv = self.environment.overlay(overrides)\n \n             # Get jinja env overrides from template\n@@ -910,8 +898,10 @@ class Templar:\n             if fail_on_undefined:\n                 raise AnsibleUndefinedVariable(e)\n             else:\n-                display.debug(\"Ignoring undefined failure: %s\" % to_text(e))\n-                return data\n+                # a bare variable was found (ex. '{{ foo }}') but the variable\n+                # was undefined. The calling function will use the original string\n+                # in this case.\n+                raise\n \n     # for backwards compatibility in case anyone is using old private method directly\n     _do_template = do_template\n"}
{"cve":"CVE-2023-30172:0708", "fix_patch": "diff --git a/dev/install-common-deps-nosudo.sh b/dev/install-common-deps-nosudo.sh\nnew file mode 100755\nindex 000000000..76cec96e0\n--- /dev/null\n+++ b/dev/install-common-deps-nosudo.sh\n@@ -0,0 +1,84 @@\n+#!/usr/bin/env bash\n+\n+set -ex\n+\n+function retry-with-backoff() {\n+    for BACKOFF in 0 1 2; do\n+        sleep $BACKOFF\n+        if \"$@\"; then\n+            return 0\n+        fi\n+    done\n+    return 1\n+}\n+\n+while :\n+do\n+  case \"$1\" in\n+    # Install skinny dependencies\n+    --skinny)\n+      SKINNY=\"true\"\n+      shift\n+      ;;\n+    # Install ML dependencies\n+    --ml)\n+      ML=\"true\"\n+      shift\n+      ;;\n+    --)\n+      shift\n+      break\n+      ;;\n+    -*)\n+      echo \"Error: unknown option: $1\" >&2\n+      exit 1\n+      ;;\n+    *)\n+      break\n+      ;;\n+  esac\n+done\n+\n+# Cleanup apt repository to make room for tests.\n+\n+df -h\n+\n+python --version\n+pip install --upgrade pip wheel\n+pip --version\n+\n+if [[ \"$SKINNY\" == \"true\" ]]; then\n+  MLFLOW_SKINNY=true pip install . --upgrade\n+else\n+  pip install .[extras] --upgrade\n+fi\n+export MLFLOW_HOME=$(pwd)\n+\n+req_files=\"\"\n+# Install Python test dependencies only if we're running Python tests\n+if [[ \"$SKINNY\" == \"true\" ]]; then\n+  req_files+=\" -r requirements/skinny-test-requirements.txt\"\n+else\n+  req_files+=\" -r requirements/test-requirements.txt\"\n+fi\n+if [[ \"$ML\" == \"true\" ]]; then\n+  req_files+=\" -r requirements/extra-ml-requirements.txt\"\n+fi\n+\n+if [[ ! -z $req_files ]]; then\n+  retry-with-backoff pip install $req_files\n+fi\n+\n+# Install `mlflow-test-plugin` without dependencies\n+pip install --no-dependencies tests/resources/mlflow-test-plugin\n+\n+# Print current environment info\n+python dev/show_package_release_dates.py\n+which mlflow\n+echo $MLFLOW_HOME\n+\n+# Print mlflow version\n+mlflow --version\n+\n+# Turn off trace output & exit-on-errors\n+set +ex\ndiff --git a/mlflow/server/handlers.py b/mlflow/server/handlers.py\nindex 08b115c91..daa3d300c 100644\n--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -314,6 +314,19 @@ def _assert_item_type_string(x):\n     assert all(map(lambda item: isinstance(item, str), x))\n \n \n+def validate_path_is_safe(path):\n+    if os.path.isabs(path):\n+        raise MlflowException(\n+            f\"The path `{path}` is invalid. Absolute paths are not allowed.\",\n+            error_code=INVALID_PARAMETER_VALUE,\n+        )\n+\n+    for path_part in path.split(os.sep):\n+        if path_part == \"..\":\n+            raise MlflowException(\n+                f\"The path `{path}` is invalid. Path traversal is not allowed.\",\n+                error_code=INVALID_PARAMETER_VALUE,\n+            )\n _TYPE_VALIDATORS = {\n     _assert_intlike,\n     _assert_string,\n@@ -531,17 +544,21 @@ def get_artifact_handler():\n     query_string = request.query_string.decode(\"utf-8\")\n     request_dict = parser.parse(query_string, normalized=True)\n     run_id = request_dict.get(\"run_id\") or request_dict.get(\"run_uuid\")\n+    path = request_dict.get(\"path\")\n+    if path is not None:\n+        validate_path_is_safe(path)\n+\n     run = _get_tracking_store().get_run(run_id)\n \n     if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n         artifact_repo = _get_artifact_repo_mlflow_artifacts()\n         artifact_path = _get_proxied_run_artifact_destination_path(\n             proxied_artifact_root=run.info.artifact_uri,\n-            relative_path=request_dict[\"path\"],\n+            relative_path=path,\n         )\n     else:\n         artifact_repo = _get_artifact_repo(run)\n-        artifact_path = request_dict[\"path\"]\n+        artifact_path = path\n \n     return _send_artifact(artifact_repo, artifact_path)\n \n@@ -899,6 +916,9 @@ def _list_artifacts():\n     else:\n         path = None\n     run_id = request_message.run_id or request_message.run_uuid\n+    if path is not None:\n+        validate_path_is_safe(path)\n+\n     run = _get_tracking_store().get_run(run_id)\n \n     if _is_servable_proxied_run_artifact_root(run.info.artifact_uri):\n@@ -1274,16 +1294,20 @@ def get_model_version_artifact_handler():\n     name = request_dict.get(\"name\")\n     version = request_dict.get(\"version\")\n     artifact_uri = _get_model_registry_store().get_model_version_download_uri(name, version)\n+    path = request_dict.get(\"path\")\n+    if path is not None:\n+        validate_path_is_safe(path)\n+\n \n     if _is_servable_proxied_run_artifact_root(artifact_uri):\n         artifact_repo = _get_artifact_repo_mlflow_artifacts()\n         artifact_path = _get_proxied_run_artifact_destination_path(\n             proxied_artifact_root=artifact_uri,\n-            relative_path=request_dict[\"path\"],\n+            relative_path=path,\n         )\n     else:\n         artifact_repo = get_artifact_repository(artifact_uri)\n-        artifact_path = request_dict[\"path\"]\n+        artifact_path = path\n \n     return _send_artifact(artifact_repo, artifact_path)\n \ndiff --git a/requirements/core-requirements.txt b/requirements/core-requirements.txt\nindex a8cb13624..1d867c5f5 100644\n--- a/requirements/core-requirements.txt\n+++ b/requirements/core-requirements.txt\n@@ -18,3 +18,5 @@ shap<1,>=0.40\n markdown<4,>=3.3\n Jinja2<4,>=2.11; platform_system != 'Windows'\n Jinja2<4,>=3.0; platform_system == 'Windows'\n+\n+PyYAML\ndiff --git a/requirements/dev-requirements.txt b/requirements/dev-requirements.txt\nindex d6a382311..39f86f24c 100644\n--- a/requirements/dev-requirements.txt\n+++ b/requirements/dev-requirements.txt\n@@ -2,3 +2,5 @@\n -r extra-ml-requirements.txt\n -r lint-requirements.txt\n -r doc-requirements.txt\n+\n+PyYAML\ndiff --git a/requirements/skinny-requirements.txt b/requirements/skinny-requirements.txt\nindex cca5d5aa2..2960fc75d 100644\n--- a/requirements/skinny-requirements.txt\n+++ b/requirements/skinny-requirements.txt\n@@ -14,3 +14,5 @@ requests<3,>=2.17.3\n packaging<22\n importlib_metadata<6,>=3.7.0,!=4.7.0\n sqlparse<1,>=0.4.0\n+\n+PyYAML\n"}
{"cve":"CVE-2020-10691:0708", "fix_patch": "diff --git a/lib/ansible/galaxy/collection.py b/lib/ansible/galaxy/collection.py\nindex fd50472f79..75406a6496 100644\n--- a/lib/ansible/galaxy/collection.py\n+++ b/lib/ansible/galaxy/collection.py\n@@ -925,11 +925,14 @@ def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):\n             raise AnsibleError(\"Checksum mismatch for '%s' inside collection at '%s'\"\n                                % (n_filename, to_native(tar.name)))\n \n+        if '..' in filename.split(os.path.sep):\n+            raise AnsibleError(\"Cannot extract tar entry '%s' as it will be placed outside the collection directory\" %\n+                                 n_filename)\n+\n         b_dest_filepath = os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict'))\n+\n         b_parent_dir = os.path.split(b_dest_filepath)[0]\n         if not os.path.exists(b_parent_dir):\n-            # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check\n-            # makes sure we create the parent directory even if it wasn't set in the metadata.\n             os.makedirs(b_parent_dir, mode=0o0755)\n \n         shutil.move(to_bytes(tmpfile_obj.name, errors='surrogate_or_strict'), b_dest_filepath)\n"}
{"cve":"CVE-2024-42005:0708", "fix_patch": ""}
{"cve":"CVE-2022-28346:0708", "fix_patch": ""}
{"cve":"CVE-2021-31542:0708", "fix_patch": ""}
{"cve":"CVE-2024-10220:0708", "fix_patch": "diff --git a/pkg/volume/git_repo/git_repo.go b/pkg/volume/git_repo/git_repo.go\nindex 995018d9007..5a5398735fa 100644\n--- a/pkg/volume/git_repo/git_repo.go\n+++ b/pkg/volume/git_repo/git_repo.go\n@@ -17,285 +17,288 @@ limitations under the License.\n package git_repo\n \n import (\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\n-\tv1 \"k8s.io/api/core/v1\"\n-\t\"k8s.io/apimachinery/pkg/types\"\n-\t\"k8s.io/kubernetes/pkg/volume\"\n-\tvolumeutil \"k8s.io/kubernetes/pkg/volume/util\"\n-\t\"k8s.io/utils/exec\"\n-\tutilstrings \"k8s.io/utils/strings\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"path/filepath\"\n+        \"strings\"\n+\n+        v1 \"k8s.io/api/core/v1\"\n+        \"k8s.io/apimachinery/pkg/types\"\n+        \"k8s.io/kubernetes/pkg/volume\"\n+        volumeutil \"k8s.io/kubernetes/pkg/volume/util\"\n+        \"k8s.io/utils/exec\"\n+        utilstrings \"k8s.io/utils/strings\"\n )\n \n // This is the primary entrypoint for volume plugins.\n func ProbeVolumePlugins() []volume.VolumePlugin {\n-\treturn []volume.VolumePlugin{&gitRepoPlugin{nil}}\n+        return []volume.VolumePlugin{&gitRepoPlugin{nil}}\n }\n \n type gitRepoPlugin struct {\n-\thost volume.VolumeHost\n+        host volume.VolumeHost\n }\n \n var _ volume.VolumePlugin = &gitRepoPlugin{}\n \n func wrappedVolumeSpec() volume.Spec {\n-\treturn volume.Spec{\n-\t\tVolume: &v1.Volume{VolumeSource: v1.VolumeSource{EmptyDir: &v1.EmptyDirVolumeSource{}}},\n-\t}\n+        return volume.Spec{\n+                Volume: &v1.Volume{VolumeSource: v1.VolumeSource{EmptyDir: &v1.EmptyDirVolumeSource{}}},\n+        }\n }\n \n const (\n-\tgitRepoPluginName = \"kubernetes.io/git-repo\"\n+        gitRepoPluginName = \"kubernetes.io/git-repo\"\n )\n \n func (plugin *gitRepoPlugin) Init(host volume.VolumeHost) error {\n-\tplugin.host = host\n-\treturn nil\n+        plugin.host = host\n+        return nil\n }\n \n func (plugin *gitRepoPlugin) GetPluginName() string {\n-\treturn gitRepoPluginName\n+        return gitRepoPluginName\n }\n \n func (plugin *gitRepoPlugin) GetVolumeName(spec *volume.Spec) (string, error) {\n-\tvolumeSource, _ := getVolumeSource(spec)\n-\tif volumeSource == nil {\n-\t\treturn \"\", fmt.Errorf(\"Spec does not reference a Git repo volume type\")\n-\t}\n-\n-\treturn fmt.Sprintf(\n-\t\t\"%v:%v:%v\",\n-\t\tvolumeSource.Repository,\n-\t\tvolumeSource.Revision,\n-\t\tvolumeSource.Directory), nil\n+        volumeSource, _ := getVolumeSource(spec)\n+        if volumeSource == nil {\n+                return \"\", fmt.Errorf(\"Spec does not reference a Git repo volume type\")\n+        }\n+\n+        return fmt.Sprintf(\n+                \"%v:%v:%v\",\n+                volumeSource.Repository,\n+                volumeSource.Revision,\n+                volumeSource.Directory), nil\n }\n \n func (plugin *gitRepoPlugin) CanSupport(spec *volume.Spec) bool {\n-\treturn spec.Volume != nil && spec.Volume.GitRepo != nil\n+        return spec.Volume != nil && spec.Volume.GitRepo != nil\n }\n \n func (plugin *gitRepoPlugin) RequiresRemount(spec *volume.Spec) bool {\n-\treturn false\n+        return false\n }\n \n func (plugin *gitRepoPlugin) SupportsMountOption() bool {\n-\treturn false\n+        return false\n }\n \n func (plugin *gitRepoPlugin) SupportsBulkVolumeVerification() bool {\n-\treturn false\n+        return false\n }\n \n func (plugin *gitRepoPlugin) SupportsSELinuxContextMount(spec *volume.Spec) (bool, error) {\n-\treturn false, nil\n+        return false, nil\n }\n \n func (plugin *gitRepoPlugin) NewMounter(spec *volume.Spec, pod *v1.Pod, opts volume.VolumeOptions) (volume.Mounter, error) {\n-\tif err := validateVolume(spec.Volume.GitRepo); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &gitRepoVolumeMounter{\n-\t\tgitRepoVolume: &gitRepoVolume{\n-\t\t\tvolName: spec.Name(),\n-\t\t\tpodUID:  pod.UID,\n-\t\t\tplugin:  plugin,\n-\t\t},\n-\t\tpod:      *pod,\n-\t\tsource:   spec.Volume.GitRepo.Repository,\n-\t\trevision: spec.Volume.GitRepo.Revision,\n-\t\ttarget:   spec.Volume.GitRepo.Directory,\n-\t\texec:     exec.New(),\n-\t\topts:     opts,\n-\t}, nil\n+        if err := validateVolume(spec.Volume.GitRepo); err != nil {\n+                return nil, err\n+        }\n+\n+        return &gitRepoVolumeMounter{\n+                gitRepoVolume: &gitRepoVolume{\n+                        volName: spec.Name(),\n+                        podUID:  pod.UID,\n+                        plugin:  plugin,\n+                },\n+                pod:      *pod,\n+                source:   spec.Volume.GitRepo.Repository,\n+                revision: spec.Volume.GitRepo.Revision,\n+                target:   spec.Volume.GitRepo.Directory,\n+                exec:     exec.New(),\n+                opts:     opts,\n+        }, nil\n }\n \n func (plugin *gitRepoPlugin) NewUnmounter(volName string, podUID types.UID) (volume.Unmounter, error) {\n-\treturn &gitRepoVolumeUnmounter{\n-\t\t&gitRepoVolume{\n-\t\t\tvolName: volName,\n-\t\t\tpodUID:  podUID,\n-\t\t\tplugin:  plugin,\n-\t\t},\n-\t}, nil\n+        return &gitRepoVolumeUnmounter{\n+                &gitRepoVolume{\n+                        volName: volName,\n+                        podUID:  podUID,\n+                        plugin:  plugin,\n+                },\n+        }, nil\n }\n \n func (plugin *gitRepoPlugin) ConstructVolumeSpec(volumeName, mountPath string) (volume.ReconstructedVolume, error) {\n-\tgitVolume := &v1.Volume{\n-\t\tName: volumeName,\n-\t\tVolumeSource: v1.VolumeSource{\n-\t\t\tGitRepo: &v1.GitRepoVolumeSource{},\n-\t\t},\n-\t}\n-\treturn volume.ReconstructedVolume{\n-\t\tSpec: volume.NewSpecFromVolume(gitVolume),\n-\t}, nil\n+        gitVolume := &v1.Volume{\n+                Name: volumeName,\n+                VolumeSource: v1.VolumeSource{\n+                        GitRepo: &v1.GitRepoVolumeSource{},\n+                },\n+        }\n+        return volume.ReconstructedVolume{\n+                Spec: volume.NewSpecFromVolume(gitVolume),\n+        }, nil\n }\n \n // gitRepo volumes are directories which are pre-filled from a git repository.\n // These do not persist beyond the lifetime of a pod.\n type gitRepoVolume struct {\n-\tvolName string\n-\tpodUID  types.UID\n-\tplugin  *gitRepoPlugin\n-\tvolume.MetricsNil\n+        volName string\n+        podUID  types.UID\n+        plugin  *gitRepoPlugin\n+        volume.MetricsNil\n }\n \n var _ volume.Volume = &gitRepoVolume{}\n \n func (gr *gitRepoVolume) GetPath() string {\n-\tname := gitRepoPluginName\n-\treturn gr.plugin.host.GetPodVolumeDir(gr.podUID, utilstrings.EscapeQualifiedName(name), gr.volName)\n+        name := gitRepoPluginName\n+        return gr.plugin.host.GetPodVolumeDir(gr.podUID, utilstrings.EscapeQualifiedName(name), gr.volName)\n }\n \n // gitRepoVolumeMounter builds git repo volumes.\n type gitRepoVolumeMounter struct {\n-\t*gitRepoVolume\n-\n-\tpod      v1.Pod\n-\tsource   string\n-\trevision string\n-\ttarget   string\n-\texec     exec.Interface\n-\topts     volume.VolumeOptions\n+        *gitRepoVolume\n+\n+        pod      v1.Pod\n+        source   string\n+        revision string\n+        target   string\n+        exec     exec.Interface\n+        opts     volume.VolumeOptions\n }\n \n var _ volume.Mounter = &gitRepoVolumeMounter{}\n \n func (b *gitRepoVolumeMounter) GetAttributes() volume.Attributes {\n-\treturn volume.Attributes{\n-\t\tReadOnly:       false,\n-\t\tManaged:        true,\n-\t\tSELinuxRelabel: true, // xattr change should be okay, TODO: double check\n-\t}\n+        return volume.Attributes{\n+                ReadOnly:       false,\n+                Managed:        true,\n+                SELinuxRelabel: true, // xattr change should be okay, TODO: double check\n+        }\n }\n \n // SetUp creates new directory and clones a git repo.\n func (b *gitRepoVolumeMounter) SetUp(mounterArgs volume.MounterArgs) error {\n-\treturn b.SetUpAt(b.GetPath(), mounterArgs)\n+        return b.SetUpAt(b.GetPath(), mounterArgs)\n }\n \n // SetUpAt creates new directory and clones a git repo.\n func (b *gitRepoVolumeMounter) SetUpAt(dir string, mounterArgs volume.MounterArgs) error {\n-\tif volumeutil.IsReady(b.getMetaDir()) {\n-\t\treturn nil\n-\t}\n-\n-\t// Wrap EmptyDir, let it do the setup.\n-\twrapped, err := b.plugin.host.NewWrapperMounter(b.volName, wrappedVolumeSpec(), &b.pod, b.opts)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tif err := wrapped.SetUpAt(dir, mounterArgs); err != nil {\n-\t\treturn err\n-\t}\n-\n-\targs := []string{\"clone\", \"--\", b.source}\n-\n-\tif len(b.target) != 0 {\n-\t\targs = append(args, b.target)\n-\t}\n-\tif output, err := b.execCommand(\"git\", args, dir); err != nil {\n-\t\treturn fmt.Errorf(\"failed to exec 'git %s': %s: %v\",\n-\t\t\tstrings.Join(args, \" \"), output, err)\n-\t}\n-\n-\tfiles, err := ioutil.ReadDir(dir)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif len(b.revision) == 0 {\n-\t\t// Done!\n-\t\tvolumeutil.SetReady(b.getMetaDir())\n-\t\treturn nil\n-\t}\n-\n-\tvar subdir string\n-\n-\tswitch {\n-\tcase len(b.target) != 0 && filepath.Clean(b.target) == \".\":\n-\t\t// if target dir is '.', use the current dir\n-\t\tsubdir = filepath.Join(dir)\n-\tcase len(files) == 1:\n-\t\t// if target is not '.', use the generated folder\n-\t\tsubdir = filepath.Join(dir, files[0].Name())\n-\tdefault:\n-\t\t// if target is not '.', but generated many files, it's wrong\n-\t\treturn fmt.Errorf(\"unexpected directory contents: %v\", files)\n-\t}\n-\n-\tif output, err := b.execCommand(\"git\", []string{\"checkout\", b.revision}, subdir); err != nil {\n-\t\treturn fmt.Errorf(\"failed to exec 'git checkout %s': %s: %v\", b.revision, output, err)\n-\t}\n-\tif output, err := b.execCommand(\"git\", []string{\"reset\", \"--hard\"}, subdir); err != nil {\n-\t\treturn fmt.Errorf(\"failed to exec 'git reset --hard': %s: %v\", output, err)\n-\t}\n-\n-\tvolume.SetVolumeOwnership(b, dir, mounterArgs.FsGroup, nil /*fsGroupChangePolicy*/, volumeutil.FSGroupCompleteHook(b.plugin, nil))\n-\n-\tvolumeutil.SetReady(b.getMetaDir())\n-\treturn nil\n+        if volumeutil.IsReady(b.getMetaDir()) {\n+                return nil\n+        }\n+\n+        // Wrap EmptyDir, let it do the setup.\n+        wrapped, err := b.plugin.host.NewWrapperMounter(b.volName, wrappedVolumeSpec(), &b.pod, b.opts)\n+        if err != nil {\n+                return err\n+        }\n+        if err := wrapped.SetUpAt(dir, mounterArgs); err != nil {\n+                return err\n+        }\n+\n+        args := []string{\"clone\", \"--\", b.source}\n+\n+        if len(b.target) != 0 {\n+                args = append(args, b.target)\n+        }\n+        if output, err := b.execCommand(\"git\", args, dir); err != nil {\n+                return fmt.Errorf(\"failed to exec 'git %s': %s: %v\",\n+                        strings.Join(args, \" \"), output, err)\n+        }\n+\n+        files, err := ioutil.ReadDir(dir)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if len(b.revision) == 0 {\n+                // Done!\n+                volumeutil.SetReady(b.getMetaDir())\n+                return nil\n+        }\n+\n+        var subdir string\n+\n+        switch {\n+        case len(b.target) != 0 && filepath.Clean(b.target) == \".\":\n+                // if target dir is '.', use the current dir\n+                subdir = filepath.Join(dir)\n+        case len(files) == 1:\n+                // if target is not '.', use the generated folder\n+                subdir = filepath.Join(dir, files[0].Name())\n+        default:\n+                // if target is not '.', but generated many files, it's wrong\n+                return fmt.Errorf(\"unexpected directory contents: %v\", files)\n+        }\n+\n+        if output, err := b.execCommand(\"git\", []string{\"checkout\", b.revision}, subdir); err != nil {\n+                return fmt.Errorf(\"failed to exec 'git checkout %s': %s: %v\", b.revision, output, err)\n+        }\n+        if output, err := b.execCommand(\"git\", []string{\"reset\", \"--hard\"}, subdir); err != nil {\n+                return fmt.Errorf(\"failed to exec 'git reset --hard': %s: %v\", output, err)\n+        }\n+\n+        volume.SetVolumeOwnership(b, dir, mounterArgs.FsGroup, nil /*fsGroupChangePolicy*/, volumeutil.FSGroupCompleteHook(b.plugin, nil))\n+\n+        volumeutil.SetReady(b.getMetaDir())\n+        return nil\n }\n \n func (b *gitRepoVolumeMounter) getMetaDir() string {\n-\treturn filepath.Join(b.plugin.host.GetPodPluginDir(b.podUID, utilstrings.EscapeQualifiedName(gitRepoPluginName)), b.volName)\n+        return filepath.Join(b.plugin.host.GetPodPluginDir(b.podUID, utilstrings.EscapeQualifiedName(gitRepoPluginName)), b.volName)\n }\n \n func (b *gitRepoVolumeMounter) execCommand(command string, args []string, dir string) ([]byte, error) {\n-\tcmd := b.exec.Command(command, args...)\n-\tcmd.SetDir(dir)\n-\treturn cmd.CombinedOutput()\n+        cmd := b.exec.Command(command, args...)\n+        cmd.SetDir(dir)\n+        return cmd.CombinedOutput()\n }\n \n func validateVolume(src *v1.GitRepoVolumeSource) error {\n-\tif err := validateNonFlagArgument(src.Repository, \"repository\"); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := validateNonFlagArgument(src.Revision, \"revision\"); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := validateNonFlagArgument(src.Directory, \"directory\"); err != nil {\n-\t\treturn err\n-\t}\n-\treturn nil\n+if err := validateNonFlagArgument(src.Repository, \"repository\"); err != nil {\n+return err\n+}\n+if err := validateNonFlagArgument(src.Revision, \"revision\"); err != nil {\n+return err\n+}\n+if err := validateNonFlagArgument(src.Directory, \"directory\"); err != nil {\n+return err\n+}\n+if strings.Contains(src.Directory, \"..\") {\n+return fmt.Errorf(\"directory may not contain '..'\")\n+}\n+return nil\n }\n \n // gitRepoVolumeUnmounter cleans git repo volumes.\n type gitRepoVolumeUnmounter struct {\n-\t*gitRepoVolume\n+        *gitRepoVolume\n }\n \n var _ volume.Unmounter = &gitRepoVolumeUnmounter{}\n \n // TearDown simply deletes everything in the directory.\n func (c *gitRepoVolumeUnmounter) TearDown() error {\n-\treturn c.TearDownAt(c.GetPath())\n+        return c.TearDownAt(c.GetPath())\n }\n \n // TearDownAt simply deletes everything in the directory.\n func (c *gitRepoVolumeUnmounter) TearDownAt(dir string) error {\n-\treturn volumeutil.UnmountViaEmptyDir(dir, c.plugin.host, c.volName, wrappedVolumeSpec(), c.podUID)\n+        return volumeutil.UnmountViaEmptyDir(dir, c.plugin.host, c.volName, wrappedVolumeSpec(), c.podUID)\n }\n \n func getVolumeSource(spec *volume.Spec) (*v1.GitRepoVolumeSource, bool) {\n-\tvar readOnly bool\n-\tvar volumeSource *v1.GitRepoVolumeSource\n+        var readOnly bool\n+        var volumeSource *v1.GitRepoVolumeSource\n \n-\tif spec.Volume != nil && spec.Volume.GitRepo != nil {\n-\t\tvolumeSource = spec.Volume.GitRepo\n-\t\treadOnly = spec.ReadOnly\n-\t}\n+        if spec.Volume != nil && spec.Volume.GitRepo != nil {\n+                volumeSource = spec.Volume.GitRepo\n+                readOnly = spec.ReadOnly\n+        }\n \n-\treturn volumeSource, readOnly\n+        return volumeSource, readOnly\n }\n \n func validateNonFlagArgument(arg, argName string) error {\n-\tif len(arg) > 0 && arg[0] == '-' {\n-\t\treturn fmt.Errorf(\"%q is an invalid value for %s\", arg, argName)\n-\t}\n-\treturn nil\n+        if len(arg) > 0 && arg[0] == '-' {\n+                return fmt.Errorf(\"%q is an invalid value for %s\", arg, argName)\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2024-45043:0708", "fix_patch": "diff --git a/receiver/awsfirehosereceiver/receiver.go b/receiver/awsfirehosereceiver/receiver.go\nindex 6211f61221..620850b7e3 100644\n--- a/receiver/awsfirehosereceiver/receiver.go\n+++ b/receiver/awsfirehosereceiver/receiver.go\n@@ -4,101 +4,101 @@\n package awsfirehosereceiver // import \"github.com/open-telemetry/opentelemetry-collector-contrib/receiver/awsfirehosereceiver\"\n \n import (\n-\t\"context\"\n-\t\"encoding/base64\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\t\"go.opentelemetry.io/collector/component\"\n-\t\"go.opentelemetry.io/collector/component/componentstatus\"\n-\t\"go.opentelemetry.io/collector/receiver\"\n-\t\"go.uber.org/zap\"\n+        \"context\"\n+        \"encoding/base64\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io\"\n+        \"net\"\n+        \"net/http\"\n+        \"sync\"\n+        \"time\"\n+\n+        \"go.opentelemetry.io/collector/component\"\n+        \"go.opentelemetry.io/collector/component/componentstatus\"\n+        \"go.opentelemetry.io/collector/receiver\"\n+        \"go.uber.org/zap\"\n )\n \n const (\n-\theaderFirehoseRequestID        = \"X-Amz-Firehose-Request-Id\"\n-\theaderFirehoseAccessKey        = \"X-Amz-Firehose-Access-Key\"\n-\theaderFirehoseCommonAttributes = \"X-Amz-Firehose-Common-Attributes\"\n-\theaderContentType              = \"Content-Type\"\n-\theaderContentLength            = \"Content-Length\"\n+        headerFirehoseRequestID        = \"X-Amz-Firehose-Request-Id\"\n+        headerFirehoseAccessKey        = \"X-Amz-Firehose-Access-Key\"\n+        headerFirehoseCommonAttributes = \"X-Amz-Firehose-Common-Attributes\"\n+        headerContentType              = \"Content-Type\"\n+        headerContentLength            = \"Content-Length\"\n )\n \n var (\n-\terrMissingHost              = errors.New(\"nil host\")\n-\terrInvalidAccessKey         = errors.New(\"invalid firehose access key\")\n-\terrInHeaderMissingRequestID = errors.New(\"missing request id in header\")\n-\terrInBodyMissingRequestID   = errors.New(\"missing request id in body\")\n-\terrInBodyDiffRequestID      = errors.New(\"different request id in body\")\n+        errMissingHost              = errors.New(\"nil host\")\n+        errInvalidAccessKey         = errors.New(\"invalid firehose access key\")\n+        errInHeaderMissingRequestID = errors.New(\"missing request id in header\")\n+        errInBodyMissingRequestID   = errors.New(\"missing request id in body\")\n+        errInBodyDiffRequestID      = errors.New(\"different request id in body\")\n )\n \n // The firehoseConsumer is responsible for using the unmarshaler and the consumer.\n type firehoseConsumer interface {\n-\t// Consume unmarshalls and consumes the records.\n-\tConsume(ctx context.Context, records [][]byte, commonAttributes map[string]string) (int, error)\n+        // Consume unmarshalls and consumes the records.\n+        Consume(ctx context.Context, records [][]byte, commonAttributes map[string]string) (int, error)\n }\n \n // firehoseReceiver\n type firehoseReceiver struct {\n-\t// settings is the base receiver settings.\n-\tsettings receiver.Settings\n-\t// config is the configuration for the receiver.\n-\tconfig *Config\n-\t// server is the HTTP/HTTPS server set up to listen\n-\t// for requests.\n-\tserver *http.Server\n-\t// shutdownWG is the WaitGroup that is used to wait until\n-\t// the server shutdown has completed.\n-\tshutdownWG sync.WaitGroup\n-\t// consumer is the firehoseConsumer to use to process/send\n-\t// the records in each request.\n-\tconsumer firehoseConsumer\n+        // settings is the base receiver settings.\n+        settings receiver.Settings\n+        // config is the configuration for the receiver.\n+        config *Config\n+        // server is the HTTP/HTTPS server set up to listen\n+        // for requests.\n+        server *http.Server\n+        // shutdownWG is the WaitGroup that is used to wait until\n+        // the server shutdown has completed.\n+        shutdownWG sync.WaitGroup\n+        // consumer is the firehoseConsumer to use to process/send\n+        // the records in each request.\n+        consumer firehoseConsumer\n }\n \n // The firehoseRequest is the format of the received request body.\n type firehoseRequest struct {\n-\t// RequestID is a GUID that should be the same value as\n-\t// the one in the header.\n-\tRequestID string `json:\"requestId\"`\n-\t// Timestamp is the milliseconds since epoch for when the\n-\t// request was generated.\n-\tTimestamp int64 `json:\"timestamp\"`\n-\t// Records contains the data.\n-\tRecords []firehoseRecord `json:\"records\"`\n+        // RequestID is a GUID that should be the same value as\n+        // the one in the header.\n+        RequestID string `json:\"requestId\"`\n+        // Timestamp is the milliseconds since epoch for when the\n+        // request was generated.\n+        Timestamp int64 `json:\"timestamp\"`\n+        // Records contains the data.\n+        Records []firehoseRecord `json:\"records\"`\n }\n \n // The firehoseRecord is an individual record within the firehoseRequest.\n type firehoseRecord struct {\n-\t// Data is a base64 encoded string. Can be empty.\n-\tData string `json:\"data\"`\n+        // Data is a base64 encoded string. Can be empty.\n+        Data string `json:\"data\"`\n }\n \n // The firehoseResponse is the expected body for the response back to\n // the delivery stream.\n type firehoseResponse struct {\n-\t// RequestID is the same GUID that was received in\n-\t// the request.\n-\tRequestID string `json:\"requestId\"`\n-\t// Timestamp is the milliseconds since epoch for when the\n-\t// request finished being processed.\n-\tTimestamp int64 `json:\"timestamp\"`\n-\t// ErrorMessage is the error to report. Empty if request\n-\t// was successfully processed.\n-\tErrorMessage string `json:\"errorMessage,omitempty\"`\n+        // RequestID is the same GUID that was received in\n+        // the request.\n+        RequestID string `json:\"requestId\"`\n+        // Timestamp is the milliseconds since epoch for when the\n+        // request finished being processed.\n+        Timestamp int64 `json:\"timestamp\"`\n+        // ErrorMessage is the error to report. Empty if request\n+        // was successfully processed.\n+        ErrorMessage string `json:\"errorMessage,omitempty\"`\n }\n \n // The firehoseCommonAttributes is the format for the common attributes\n // found in the header of requests.\n type firehoseCommonAttributes struct {\n-\t// CommonAttributes can be set when creating the delivery stream.\n-\t// These will be passed to the firehoseConsumer, which should\n-\t// attach the attributes.\n-\tCommonAttributes map[string]string `json:\"commonAttributes\"`\n+        // CommonAttributes can be set when creating the delivery stream.\n+        // These will be passed to the firehoseConsumer, which should\n+        // attach the attributes.\n+        CommonAttributes map[string]string `json:\"commonAttributes\"`\n }\n \n var _ receiver.Metrics = (*firehoseReceiver)(nil)\n@@ -107,180 +107,182 @@ var _ http.Handler = (*firehoseReceiver)(nil)\n // Start spins up the receiver's HTTP server and makes the receiver start\n // its processing.\n func (fmr *firehoseReceiver) Start(ctx context.Context, host component.Host) error {\n-\tif host == nil {\n-\t\treturn errMissingHost\n-\t}\n-\n-\tvar err error\n-\tfmr.server, err = fmr.config.ServerConfig.ToServer(ctx, host, fmr.settings.TelemetrySettings, fmr)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tvar listener net.Listener\n-\tlistener, err = fmr.config.ServerConfig.ToListener(ctx)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tfmr.shutdownWG.Add(1)\n-\tgo func() {\n-\t\tdefer fmr.shutdownWG.Done()\n-\n-\t\tif errHTTP := fmr.server.Serve(listener); errHTTP != nil && !errors.Is(errHTTP, http.ErrServerClosed) {\n-\t\t\tcomponentstatus.ReportStatus(host, componentstatus.NewFatalErrorEvent(errHTTP))\n-\t\t}\n-\t}()\n-\n-\treturn nil\n+        if host == nil {\n+                return errMissingHost\n+        }\n+\n+        var err error\n+        fmr.server, err = fmr.config.ServerConfig.ToServer(ctx, host, fmr.settings.TelemetrySettings, fmr)\n+        if err != nil {\n+                return err\n+        }\n+\n+        var listener net.Listener\n+        listener, err = fmr.config.ServerConfig.ToListener(ctx)\n+        if err != nil {\n+                return err\n+        }\n+        fmr.shutdownWG.Add(1)\n+        go func() {\n+                defer fmr.shutdownWG.Done()\n+\n+                if errHTTP := fmr.server.Serve(listener); errHTTP != nil && !errors.Is(errHTTP, http.ErrServerClosed) {\n+                        componentstatus.ReportStatus(host, componentstatus.NewFatalErrorEvent(errHTTP))\n+                }\n+        }()\n+\n+        return nil\n }\n \n // Shutdown tells the receiver that should stop reception,\n // giving it a chance to perform any necessary clean-up and\n // shutting down its HTTP server.\n func (fmr *firehoseReceiver) Shutdown(context.Context) error {\n-\tif fmr.server == nil {\n-\t\treturn nil\n-\t}\n-\terr := fmr.server.Close()\n-\tfmr.shutdownWG.Wait()\n-\treturn err\n+        if fmr.server == nil {\n+                return nil\n+        }\n+        err := fmr.server.Close()\n+        fmr.shutdownWG.Wait()\n+        return err\n }\n \n // ServeHTTP receives Firehose requests, unmarshalls them, and sends them along to the firehoseConsumer,\n // which is responsible for unmarshalling the records and sending them to the next consumer.\n func (fmr *firehoseReceiver) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\trequestID := r.Header.Get(headerFirehoseRequestID)\n-\tif requestID == \"\" {\n-\t\tfmr.settings.Logger.Error(\n-\t\t\t\"Invalid Firehose request\",\n-\t\t\tzap.Error(errInHeaderMissingRequestID),\n-\t\t)\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, errInHeaderMissingRequestID)\n-\t\treturn\n-\t}\n-\tfmr.settings.Logger.Debug(\"Processing Firehose request\", zap.String(\"RequestID\", requestID))\n-\n-\tif statusCode, err := fmr.validate(r); err != nil {\n-\t\tfmr.settings.Logger.Error(\n-\t\t\t\"Invalid Firehose request\",\n-\t\t\tzap.Error(err),\n-\t\t)\n-\t\tfmr.sendResponse(w, requestID, statusCode, err)\n-\t\treturn\n-\t}\n-\n-\tbody, err := fmr.getBody(r)\n-\tif err != nil {\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, err)\n-\t\treturn\n-\t}\n-\n-\tvar fr firehoseRequest\n-\tif err = json.Unmarshal(body, &fr); err != nil {\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, err)\n-\t\treturn\n-\t}\n-\n-\tif fr.RequestID == \"\" {\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, errInBodyMissingRequestID)\n-\t\treturn\n-\t} else if fr.RequestID != requestID {\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, errInBodyDiffRequestID)\n-\t\treturn\n-\t}\n-\n-\trecords := make([][]byte, 0, len(fr.Records))\n-\tfor index, record := range fr.Records {\n-\t\tif record.Data != \"\" {\n-\t\t\tvar decoded []byte\n-\t\t\tdecoded, err = base64.StdEncoding.DecodeString(record.Data)\n-\t\t\tif err != nil {\n-\t\t\t\tfmr.sendResponse(\n-\t\t\t\t\tw,\n-\t\t\t\t\trequestID,\n-\t\t\t\t\thttp.StatusBadRequest,\n-\t\t\t\t\tfmt.Errorf(\"unable to base64 decode the record at index %d: %w\", index, err),\n-\t\t\t\t)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\trecords = append(records, decoded)\n-\t\t}\n-\t}\n-\n-\tcommonAttributes, err := fmr.getCommonAttributes(r)\n-\tif err != nil {\n-\t\tfmr.settings.Logger.Error(\n-\t\t\t\"Unable to get common attributes from request header. Will not attach attributes.\",\n-\t\t\tzap.Error(err),\n-\t\t)\n-\t}\n-\n-\tstatusCode, err := fmr.consumer.Consume(ctx, records, commonAttributes)\n-\tif err != nil {\n-\t\tfmr.settings.Logger.Error(\n-\t\t\t\"Unable to consume records\",\n-\t\t\tzap.Error(err),\n-\t\t)\n-\t\tfmr.sendResponse(w, requestID, statusCode, err)\n-\t\treturn\n-\t}\n-\n-\tfmr.sendResponse(w, requestID, http.StatusOK, nil)\n+        ctx := r.Context()\n+\n+        requestID := r.Header.Get(headerFirehoseRequestID)\n+        if requestID == \"\" {\n+                fmr.settings.Logger.Error(\n+                        \"Invalid Firehose request\",\n+                        zap.Error(errInHeaderMissingRequestID),\n+                )\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, errInHeaderMissingRequestID)\n+                return\n+        }\n+        fmr.settings.Logger.Debug(\"Processing Firehose request\", zap.String(\"RequestID\", requestID))\n+\n+        if statusCode, err := fmr.validate(r); err != nil {\n+                fmr.settings.Logger.Error(\n+                        \"Invalid Firehose request\",\n+                        zap.Error(err),\n+                )\n+                fmr.sendResponse(w, requestID, statusCode, err)\n+                return\n+        }\n+\n+        body, err := fmr.getBody(r)\n+        if err != nil {\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, err)\n+                return\n+        }\n+\n+        var fr firehoseRequest\n+        if err = json.Unmarshal(body, &fr); err != nil {\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, err)\n+                return\n+        }\n+\n+        if fr.RequestID == \"\" {\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, errInBodyMissingRequestID)\n+                return\n+        } else if fr.RequestID != requestID {\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, errInBodyDiffRequestID)\n+                return\n+        }\n+\n+        records := make([][]byte, 0, len(fr.Records))\n+        for index, record := range fr.Records {\n+                if record.Data != \"\" {\n+                        var decoded []byte\n+                        decoded, err = base64.StdEncoding.DecodeString(record.Data)\n+                        if err != nil {\n+                                fmr.sendResponse(\n+                                        w,\n+                                        requestID,\n+                                        http.StatusBadRequest,\n+                                        fmt.Errorf(\"unable to base64 decode the record at index %d: %w\", index, err),\n+                                )\n+                                return\n+                        }\n+                        records = append(records, decoded)\n+                }\n+        }\n+\n+        commonAttributes, err := fmr.getCommonAttributes(r)\n+        if err != nil {\n+                fmr.settings.Logger.Error(\n+                        \"Unable to get common attributes from request header. Will not attach attributes.\",\n+                        zap.Error(err),\n+                )\n+        }\n+\n+        statusCode, err := fmr.consumer.Consume(ctx, records, commonAttributes)\n+        if err != nil {\n+                fmr.settings.Logger.Error(\n+                        \"Unable to consume records\",\n+                        zap.Error(err),\n+                )\n+                fmr.sendResponse(w, requestID, statusCode, err)\n+                return\n+        }\n+\n+        fmr.sendResponse(w, requestID, http.StatusOK, nil)\n }\n \n // validate checks the Firehose access key in the header against\n // the one passed into the Config\n func (fmr *firehoseReceiver) validate(r *http.Request) (int, error) {\n-\tif accessKey := r.Header.Get(headerFirehoseAccessKey); accessKey != \"\" && accessKey != string(fmr.config.AccessKey) {\n-\t\treturn http.StatusUnauthorized, errInvalidAccessKey\n-\t}\n-\treturn http.StatusAccepted, nil\n+if fmr.config.AccessKey != \"\" {\n+if accessKey := r.Header.Get(headerFirehoseAccessKey); accessKey != string(fmr.config.AccessKey) {\n+return http.StatusUnauthorized, errInvalidAccessKey\n+}\n+}\n+return http.StatusAccepted, nil\n }\n \n // getBody reads the body from the request as a slice of bytes.\n func (fmr *firehoseReceiver) getBody(r *http.Request) ([]byte, error) {\n-\tbody, err := io.ReadAll(r.Body)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\terr = r.Body.Close()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn body, nil\n+        body, err := io.ReadAll(r.Body)\n+        if err != nil {\n+                return nil, err\n+        }\n+        err = r.Body.Close()\n+        if err != nil {\n+                return nil, err\n+        }\n+        return body, nil\n }\n \n // getCommonAttributes unmarshalls the common attributes from the request header\n func (fmr *firehoseReceiver) getCommonAttributes(r *http.Request) (map[string]string, error) {\n-\tattributes := make(map[string]string)\n-\tif commonAttributes := r.Header.Get(headerFirehoseCommonAttributes); commonAttributes != \"\" {\n-\t\tvar fca firehoseCommonAttributes\n-\t\tif err := json.Unmarshal([]byte(commonAttributes), &fca); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tattributes = fca.CommonAttributes\n-\t}\n-\treturn attributes, nil\n+        attributes := make(map[string]string)\n+        if commonAttributes := r.Header.Get(headerFirehoseCommonAttributes); commonAttributes != \"\" {\n+                var fca firehoseCommonAttributes\n+                if err := json.Unmarshal([]byte(commonAttributes), &fca); err != nil {\n+                        return nil, err\n+                }\n+                attributes = fca.CommonAttributes\n+        }\n+        return attributes, nil\n }\n \n // sendResponse writes a response to Firehose in the expected format.\n func (fmr *firehoseReceiver) sendResponse(w http.ResponseWriter, requestID string, statusCode int, err error) {\n-\tvar errorMessage string\n-\tif err != nil {\n-\t\terrorMessage = err.Error()\n-\t}\n-\tbody := firehoseResponse{\n-\t\tRequestID:    requestID,\n-\t\tTimestamp:    time.Now().UnixMilli(),\n-\t\tErrorMessage: errorMessage,\n-\t}\n-\tpayload, _ := json.Marshal(body)\n-\tw.Header().Set(headerContentType, \"application/json\")\n-\tw.Header().Set(headerContentLength, fmt.Sprintf(\"%d\", len(payload)))\n-\tw.WriteHeader(statusCode)\n-\tif _, err = w.Write(payload); err != nil {\n-\t\tfmr.settings.Logger.Error(\"Failed to send response\", zap.Error(err))\n-\t}\n+        var errorMessage string\n+        if err != nil {\n+                errorMessage = err.Error()\n+        }\n+        body := firehoseResponse{\n+                RequestID:    requestID,\n+                Timestamp:    time.Now().UnixMilli(),\n+                ErrorMessage: errorMessage,\n+        }\n+        payload, _ := json.Marshal(body)\n+        w.Header().Set(headerContentType, \"application/json\")\n+        w.Header().Set(headerContentLength, fmt.Sprintf(\"%d\", len(payload)))\n+        w.WriteHeader(statusCode)\n+        if _, err = w.Write(payload); err != nil {\n+                fmr.settings.Logger.Error(\"Failed to send response\", zap.Error(err))\n+        }\n }\n"}
{"cve":"CVE-2021-21432:0708", "fix_patch": "diff --git a/source/github/authentication.go b/source/github/authentication.go\nindex 6d964e3a..3ed29e26 100644\n--- a/source/github/authentication.go\n+++ b/source/github/authentication.go\n@@ -5,56 +5,56 @@\n package github\n \n import (\n-\t\"context\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"net/http\"\n+        \"context\"\n+        \"errors\"\n+        \"fmt\"\n+        \"net/http\"\n \n-\t\"github.com/go-vela/server/random\"\n+        \"github.com/go-vela/server/random\"\n \n-\t\"github.com/go-vela/types/library\"\n+        \"github.com/go-vela/types/library\"\n \n-\t\"github.com/sirupsen/logrus\"\n+        \"github.com/sirupsen/logrus\"\n )\n \n // Authorize uses the given access token to authorize the user.\n func (c *client) Authorize(token string) (string, error) {\n-\tlogrus.Trace(\"Authorizing user with token\")\n+        logrus.Trace(\"Authorizing user with token\")\n \n-\t// create GitHub OAuth client with user's token\n-\tclient := c.newClientToken(token)\n+        // create GitHub OAuth client with user's token\n+        client := c.newClientToken(token)\n \n-\t// send API call to capture the current user making the call\n-\tu, _, err := client.Users.Get(ctx, \"\")\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n+        // send API call to capture the current user making the call\n+        u, _, err := client.Users.Get(ctx, \"\")\n+        if err != nil {\n+                return \"\", err\n+        }\n \n-\treturn u.GetLogin(), nil\n+        return u.GetLogin(), nil\n }\n \n // Login begins the authentication workflow for the session.\n func (c *client) Login(w http.ResponseWriter, r *http.Request) (string, error) {\n-\tlogrus.Trace(\"Processing login request\")\n-\n-\t// generate a random string for creating the OAuth state\n-\t//\n-\t// nolint: gomnd // ignore magic number\n-\toAuthState, err := random.GenerateRandomString(32)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\t// pass through the redirect if it exists\n-\tredirect := r.FormValue(\"redirect_uri\")\n-\tif len(redirect) > 0 {\n-\t\tc.OConfig.RedirectURL = redirect\n-\t}\n-\n-\t// temporarily redirect request to Github to begin workflow\n-\thttp.Redirect(w, r, c.OConfig.AuthCodeURL(oAuthState), http.StatusTemporaryRedirect)\n-\n-\treturn oAuthState, nil\n+        logrus.Trace(\"Processing login request\")\n+\n+        // generate a random string for creating the OAuth state\n+        //\n+        // nolint: gomnd // ignore magic number\n+        oAuthState, err := random.GenerateRandomString(32)\n+        if err != nil {\n+                return \"\", err\n+        }\n+\n+        // pass through the redirect if it exists\n+        redirect := r.FormValue(\"redirect_uri\")\n+        if len(redirect) > 0 {\n+                c.OConfig.RedirectURL = redirect\n+        }\n+\n+        // temporarily redirect request to Github to begin workflow\n+        http.Redirect(w, r, c.OConfig.AuthCodeURL(oAuthState), http.StatusTemporaryRedirect)\n+\n+        return oAuthState, nil\n }\n \n // Authenticate completes the authentication workflow for the session\n@@ -62,61 +62,50 @@ func (c *client) Login(w http.ResponseWriter, r *http.Request) (string, error) {\n //\n // nolint: lll // ignore long line length due to variable names\n func (c *client) Authenticate(w http.ResponseWriter, r *http.Request, oAuthState string) (*library.User, error) {\n-\tlogrus.Trace(\"Authenticating user\")\n-\n-\t// get the OAuth code\n-\tcode := r.FormValue(\"code\")\n-\tif len(code) == 0 {\n-\t\treturn nil, nil\n-\t}\n-\n-\t// verify the OAuth state\n-\tstate := r.FormValue(\"state\")\n-\tif state != oAuthState {\n-\t\treturn nil, fmt.Errorf(\"unexpected oauth state: want %s but got %s\", oAuthState, state)\n-\t}\n-\n-\t// pass through the redirect if it exists\n-\tredirect := r.FormValue(\"redirect_uri\")\n-\tif len(redirect) > 0 {\n-\t\tc.OConfig.RedirectURL = redirect\n-\t}\n-\n-\t// exchange OAuth code for token\n-\ttoken, err := c.OConfig.Exchange(context.Background(), code)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// authorize the user for the token\n-\tu, err := c.Authorize(token.AccessToken)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &library.User{\n-\t\tName:  &u,\n-\t\tToken: &token.AccessToken,\n-\t}, nil\n+        logrus.Trace(\"Authenticating user\")\n+\n+        // get the OAuth code\n+        code := r.FormValue(\"code\")\n+        if len(code) == 0 {\n+                return nil, nil\n+        }\n+\n+        // verify the OAuth state\n+        state := r.FormValue(\"state\")\n+        if state != oAuthState {\n+                return nil, fmt.Errorf(\"unexpected oauth state: want %s but got %s\", oAuthState, state)\n+        }\n+\n+        // pass through the redirect if it exists\n+        redirect := r.FormValue(\"redirect_uri\")\n+        if len(redirect) > 0 {\n+                c.OConfig.RedirectURL = redirect\n+        }\n+\n+        // exchange OAuth code for token\n+        token, err := c.OConfig.Exchange(context.Background(), code)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // authorize the user for the token\n+        u, err := c.Authorize(token.AccessToken)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        return &library.User{\n+                Name:  &u,\n+                Token: &token.AccessToken,\n+        }, nil\n }\n \n // AuthenticateToken completes the authentication workflow\n // for the session and returns the remote user details.\n func (c *client) AuthenticateToken(r *http.Request) (*library.User, error) {\n-\tlogrus.Trace(\"Authenticating user via token\")\n-\n-\ttoken := r.Header.Get(\"Token\")\n-\tif len(token) == 0 {\n-\t\treturn nil, errors.New(\"no token provided\")\n-\t}\n-\n-\tu, err := c.Authorize(token)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &library.User{\n-\t\tName:  &u,\n-\t\tToken: &token,\n-\t}, nil\n+logrus.Trace(\"Authenticating user via token\")\n+\n+// get the token from the request\n+return nil, errors.New(\"not implemented\")\n }\n+\n"}
{"cve":"CVE-2022-1992:0708", "fix_patch": "diff --git a/internal/pathutil/pathutil.go b/internal/pathutil/pathutil.go\nindex 6a7286e1..ec4ebc98 100644\n--- a/internal/pathutil/pathutil.go\n+++ b/internal/pathutil/pathutil.go\n@@ -5,11 +5,11 @@\n package pathutil\n \n import (\n-\t\"path\"\n-\t\"strings\"\n+        \"path\"\n+        \"strings\"\n )\n \n // Clean cleans up given path and returns a relative path that goes straight down.\n func Clean(p string) string {\n-\treturn strings.Trim(path.Clean(\"/\"+p), \"/\")\n+return strings.Trim(path.Clean(\"/\"+strings.ReplaceAll(p, \"\\\\\", \"/\")), \"/\")\n }\n"}
{"cve":"CVE-2020-26294:0708", "fix_patch": "diff --git a/template/native/render.go b/template/native/render.go\nindex 4683db6..24f38ba 100644\n--- a/template/native/render.go\n+++ b/template/native/render.go\n@@ -1,51 +1,55 @@\n package native\n \n import (\n-\t\"bytes\"\n-\t\"fmt\"\n-\t\"text/template\"\n+        \"bytes\"\n+        \"fmt\"\n+        \"text/template\"\n \n-\ttypes \"github.com/go-vela/types/yaml\"\n+        types \"github.com/go-vela/types/yaml\"\n \n-\t\"github.com/Masterminds/sprig\"\n+        \"github.com/Masterminds/sprig\"\n \n-\tyaml \"gopkg.in/yaml.v2\"\n+        yaml \"gopkg.in/yaml.v2\"\n )\n \n // Render combines the template with the step in the yaml pipeline.\n+var disallowedFuncs = template.FuncMap{\n+\"env\":       func() (string, error) { return \"\", nil },\n+\"expandenv\": func() (string, error) { return \"\", nil },\n+}\n func Render(tmpl string, s *types.Step) (types.StepSlice, error) {\n-\tbuffer := new(bytes.Buffer)\n-\tconfig := new(types.Build)\n-\n-\tvelaFuncs := funcHandler{envs: convertPlatformVars(s.Environment)}\n-\ttemplateFuncMap := map[string]interface{}{\n-\t\t\"vela\": velaFuncs.returnPlatformVar,\n-\t}\n-\n-\t// parse the template with Masterminds/sprig functions\n-\t//\n-\t// https://pkg.go.dev/github.com/Masterminds/sprig?tab=doc#TxtFuncMap\n-\tt, err := template.New(s.Name).Funcs(sprig.TxtFuncMap()).Funcs(templateFuncMap).Parse(tmpl)\n-\tif err != nil {\n-\t\treturn types.StepSlice{}, fmt.Errorf(\"unable to parse template %s: %v\", s.Template.Name, err)\n-\t}\n-\n-\t// apply the variables to the parsed template\n-\terr = t.Execute(buffer, s.Template.Variables)\n-\tif err != nil {\n-\t\treturn types.StepSlice{}, fmt.Errorf(\"unable to execute template %s: %v\", s.Template.Name, err)\n-\t}\n-\n-\t// unmarshal the template to the pipeline\n-\terr = yaml.Unmarshal(buffer.Bytes(), config)\n-\tif err != nil {\n-\t\treturn types.StepSlice{}, fmt.Errorf(\"unable to unmarshal yaml: %v\", err)\n-\t}\n-\n-\t// ensure all templated steps have template prefix\n-\tfor index, newStep := range config.Steps {\n-\t\tconfig.Steps[index].Name = fmt.Sprintf(\"%s_%s\", s.Name, newStep.Name)\n-\t}\n-\n-\treturn config.Steps, nil\n+        buffer := new(bytes.Buffer)\n+        config := new(types.Build)\n+\n+        velaFuncs := funcHandler{envs: convertPlatformVars(s.Environment)}\n+        templateFuncMap := map[string]interface{}{\n+                \"vela\": velaFuncs.returnPlatformVar,\n+        }\n+\n+        // parse the template with Masterminds/sprig functions\n+        //\n+        // https://pkg.go.dev/github.com/Masterminds/sprig?tab=doc#TxtFuncMap\n+        t, err := template.New(s.Name).Funcs(sprig.TxtFuncMap()).Funcs(templateFuncMap).Funcs(disallowedFuncs).Parse(tmpl)\n+        if err != nil {\n+                return types.StepSlice{}, fmt.Errorf(\"unable to parse template %s: %v\", s.Template.Name, err)\n+        }\n+\n+        // apply the variables to the parsed template\n+        err = t.Execute(buffer, s.Template.Variables)\n+        if err != nil {\n+                return types.StepSlice{}, fmt.Errorf(\"unable to execute template %s: %v\", s.Template.Name, err)\n+        }\n+\n+        // unmarshal the template to the pipeline\n+        err = yaml.Unmarshal(buffer.Bytes(), config)\n+        if err != nil {\n+                return types.StepSlice{}, fmt.Errorf(\"unable to unmarshal yaml: %v\", err)\n+        }\n+\n+        // ensure all templated steps have template prefix\n+        for index, newStep := range config.Steps {\n+                config.Steps[index].Name = fmt.Sprintf(\"%s_%s\", s.Name, newStep.Name)\n+        }\n+\n+        return config.Steps, nil\n }\n"}
{"cve":"CVE-2022-1986:0708", "fix_patch": "diff --git a/internal/db/repo_editor.go b/internal/db/repo_editor.go\nindex 9d4664be..7fdcd82d 100644\n--- a/internal/db/repo_editor.go\n+++ b/internal/db/repo_editor.go\n@@ -5,66 +5,66 @@\n package db\n \n import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"io/ioutil\"\n-\t\"mime/multipart\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/pkg/errors\"\n-\tgouuid \"github.com/satori/go.uuid\"\n-\t\"github.com/unknwon/com\"\n-\n-\t\"github.com/gogs/git-module\"\n-\n-\t\"gogs.io/gogs/internal/conf\"\n-\t\"gogs.io/gogs/internal/cryptoutil\"\n-\tdberrors \"gogs.io/gogs/internal/db/errors\"\n-\t\"gogs.io/gogs/internal/gitutil\"\n-\t\"gogs.io/gogs/internal/osutil\"\n-\t\"gogs.io/gogs/internal/pathutil\"\n-\t\"gogs.io/gogs/internal/process\"\n-\t\"gogs.io/gogs/internal/tool\"\n+        \"fmt\"\n+        \"io\"\n+        \"io/ioutil\"\n+        \"mime/multipart\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path\"\n+        \"path/filepath\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/pkg/errors\"\n+        gouuid \"github.com/satori/go.uuid\"\n+        \"github.com/unknwon/com\"\n+\n+        \"github.com/gogs/git-module\"\n+\n+        \"gogs.io/gogs/internal/conf\"\n+        \"gogs.io/gogs/internal/cryptoutil\"\n+        dberrors \"gogs.io/gogs/internal/db/errors\"\n+        \"gogs.io/gogs/internal/gitutil\"\n+        \"gogs.io/gogs/internal/osutil\"\n+        \"gogs.io/gogs/internal/pathutil\"\n+        \"gogs.io/gogs/internal/process\"\n+        \"gogs.io/gogs/internal/tool\"\n )\n \n const (\n-\tENV_AUTH_USER_ID           = \"GOGS_AUTH_USER_ID\"\n-\tENV_AUTH_USER_NAME         = \"GOGS_AUTH_USER_NAME\"\n-\tENV_AUTH_USER_EMAIL        = \"GOGS_AUTH_USER_EMAIL\"\n-\tENV_REPO_OWNER_NAME        = \"GOGS_REPO_OWNER_NAME\"\n-\tENV_REPO_OWNER_SALT_MD5    = \"GOGS_REPO_OWNER_SALT_MD5\"\n-\tENV_REPO_ID                = \"GOGS_REPO_ID\"\n-\tENV_REPO_NAME              = \"GOGS_REPO_NAME\"\n-\tENV_REPO_CUSTOM_HOOKS_PATH = \"GOGS_REPO_CUSTOM_HOOKS_PATH\"\n+        ENV_AUTH_USER_ID           = \"GOGS_AUTH_USER_ID\"\n+        ENV_AUTH_USER_NAME         = \"GOGS_AUTH_USER_NAME\"\n+        ENV_AUTH_USER_EMAIL        = \"GOGS_AUTH_USER_EMAIL\"\n+        ENV_REPO_OWNER_NAME        = \"GOGS_REPO_OWNER_NAME\"\n+        ENV_REPO_OWNER_SALT_MD5    = \"GOGS_REPO_OWNER_SALT_MD5\"\n+        ENV_REPO_ID                = \"GOGS_REPO_ID\"\n+        ENV_REPO_NAME              = \"GOGS_REPO_NAME\"\n+        ENV_REPO_CUSTOM_HOOKS_PATH = \"GOGS_REPO_CUSTOM_HOOKS_PATH\"\n )\n \n type ComposeHookEnvsOptions struct {\n-\tAuthUser  *User\n-\tOwnerName string\n-\tOwnerSalt string\n-\tRepoID    int64\n-\tRepoName  string\n-\tRepoPath  string\n+        AuthUser  *User\n+        OwnerName string\n+        OwnerSalt string\n+        RepoID    int64\n+        RepoName  string\n+        RepoPath  string\n }\n \n func ComposeHookEnvs(opts ComposeHookEnvsOptions) []string {\n-\tenvs := []string{\n-\t\t\"SSH_ORIGINAL_COMMAND=1\",\n-\t\tENV_AUTH_USER_ID + \"=\" + com.ToStr(opts.AuthUser.ID),\n-\t\tENV_AUTH_USER_NAME + \"=\" + opts.AuthUser.Name,\n-\t\tENV_AUTH_USER_EMAIL + \"=\" + opts.AuthUser.Email,\n-\t\tENV_REPO_OWNER_NAME + \"=\" + opts.OwnerName,\n-\t\tENV_REPO_OWNER_SALT_MD5 + \"=\" + cryptoutil.MD5(opts.OwnerSalt),\n-\t\tENV_REPO_ID + \"=\" + com.ToStr(opts.RepoID),\n-\t\tENV_REPO_NAME + \"=\" + opts.RepoName,\n-\t\tENV_REPO_CUSTOM_HOOKS_PATH + \"=\" + filepath.Join(opts.RepoPath, \"custom_hooks\"),\n-\t}\n-\treturn envs\n+        envs := []string{\n+                \"SSH_ORIGINAL_COMMAND=1\",\n+                ENV_AUTH_USER_ID + \"=\" + com.ToStr(opts.AuthUser.ID),\n+                ENV_AUTH_USER_NAME + \"=\" + opts.AuthUser.Name,\n+                ENV_AUTH_USER_EMAIL + \"=\" + opts.AuthUser.Email,\n+                ENV_REPO_OWNER_NAME + \"=\" + opts.OwnerName,\n+                ENV_REPO_OWNER_SALT_MD5 + \"=\" + cryptoutil.MD5(opts.OwnerSalt),\n+                ENV_REPO_ID + \"=\" + com.ToStr(opts.RepoID),\n+                ENV_REPO_NAME + \"=\" + opts.RepoName,\n+                ENV_REPO_CUSTOM_HOOKS_PATH + \"=\" + filepath.Join(opts.RepoPath, \"custom_hooks\"),\n+        }\n+        return envs\n }\n \n // ___________    .___.__  __    ___________.__.__\n@@ -77,184 +77,184 @@ func ComposeHookEnvs(opts ComposeHookEnvsOptions) []string {\n // discardLocalRepoBranchChanges discards local commits/changes of\n // given branch to make sure it is even to remote branch.\n func discardLocalRepoBranchChanges(localPath, branch string) error {\n-\tif !com.IsExist(localPath) {\n-\t\treturn nil\n-\t}\n-\n-\t// No need to check if nothing in the repository.\n-\tif !git.RepoHasBranch(localPath, branch) {\n-\t\treturn nil\n-\t}\n-\n-\trev := \"origin/\" + branch\n-\tif err := git.Reset(localPath, rev, git.ResetOptions{Hard: true}); err != nil {\n-\t\treturn fmt.Errorf(\"reset [revision: %s]: %v\", rev, err)\n-\t}\n-\treturn nil\n+        if !com.IsExist(localPath) {\n+                return nil\n+        }\n+\n+        // No need to check if nothing in the repository.\n+        if !git.RepoHasBranch(localPath, branch) {\n+                return nil\n+        }\n+\n+        rev := \"origin/\" + branch\n+        if err := git.Reset(localPath, rev, git.ResetOptions{Hard: true}); err != nil {\n+                return fmt.Errorf(\"reset [revision: %s]: %v\", rev, err)\n+        }\n+        return nil\n }\n \n func (repo *Repository) DiscardLocalRepoBranchChanges(branch string) error {\n-\treturn discardLocalRepoBranchChanges(repo.LocalCopyPath(), branch)\n+        return discardLocalRepoBranchChanges(repo.LocalCopyPath(), branch)\n }\n \n // CheckoutNewBranch checks out to a new branch from the a branch name.\n func (repo *Repository) CheckoutNewBranch(oldBranch, newBranch string) error {\n-\tif err := git.Checkout(repo.LocalCopyPath(), newBranch, git.CheckoutOptions{\n-\t\tBaseBranch: oldBranch,\n-\t\tTimeout:    time.Duration(conf.Git.Timeout.Pull) * time.Second,\n-\t}); err != nil {\n-\t\treturn fmt.Errorf(\"checkout [base: %s, new: %s]: %v\", oldBranch, newBranch, err)\n-\t}\n-\treturn nil\n+        if err := git.Checkout(repo.LocalCopyPath(), newBranch, git.CheckoutOptions{\n+                BaseBranch: oldBranch,\n+                Timeout:    time.Duration(conf.Git.Timeout.Pull) * time.Second,\n+        }); err != nil {\n+                return fmt.Errorf(\"checkout [base: %s, new: %s]: %v\", oldBranch, newBranch, err)\n+        }\n+        return nil\n }\n \n type UpdateRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tOldTreeName  string\n-\tNewTreeName  string\n-\tMessage      string\n-\tContent      string\n-\tIsNewFile    bool\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        OldTreeName  string\n+        NewTreeName  string\n+        Message      string\n+        Content      string\n+        IsNewFile    bool\n }\n \n // UpdateRepoFile adds or updates a file in repository.\n func (repo *Repository) UpdateRepoFile(doer *User, opts UpdateRepoFileOptions) (err error) {\n-\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\tif isRepositoryGitPath(opts.NewTreeName) {\n-\t\treturn errors.Errorf(\"bad tree path %q\", opts.NewTreeName)\n-\t}\n-\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\trepoPath := repo.RepoPath()\n-\tlocalPath := repo.LocalCopyPath()\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\t// Directly return error if new branch already exists in the server\n-\t\tif git.RepoHasBranch(repoPath, opts.NewBranch) {\n-\t\t\treturn dberrors.BranchAlreadyExists{Name: opts.NewBranch}\n-\t\t}\n-\n-\t\t// Otherwise, delete branch from local copy in case out of sync\n-\t\tif git.RepoHasBranch(localPath, opts.NewBranch) {\n-\t\t\tif err = git.DeleteBranch(localPath, opts.NewBranch, git.DeleteBranchOptions{\n-\t\t\t\tForce: true,\n-\t\t\t}); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"delete branch %q: %v\", opts.NewBranch, err)\n-\t\t\t}\n-\t\t}\n-\n-\t\tif err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\toldFilePath := path.Join(localPath, opts.OldTreeName)\n-\tfilePath := path.Join(localPath, opts.NewTreeName)\n-\tif err = os.MkdirAll(path.Dir(filePath), os.ModePerm); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// If it's meant to be a new file, make sure it doesn't exist.\n-\tif opts.IsNewFile {\n-\t\tif com.IsExist(filePath) {\n-\t\t\treturn ErrRepoFileAlreadyExist{filePath}\n-\t\t}\n-\t}\n-\n-\t// Ignore move step if it's a new file under a directory.\n-\t// Otherwise, move the file when name changed.\n-\tif osutil.IsFile(oldFilePath) && opts.OldTreeName != opts.NewTreeName {\n-\t\tif err = git.Move(localPath, opts.OldTreeName, opts.NewTreeName); err != nil {\n-\t\t\treturn fmt.Errorf(\"git mv %q %q: %v\", opts.OldTreeName, opts.NewTreeName, err)\n-\t\t}\n-\t}\n-\n-\tif err = ioutil.WriteFile(filePath, []byte(opts.Content), 0666); err != nil {\n-\t\treturn fmt.Errorf(\"write file: %v\", err)\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t} else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n-\t\treturn fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\treturn nil\n+        // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+        if isRepositoryGitPath(opts.NewTreeName) {\n+                return errors.Errorf(\"bad tree path %q\", opts.NewTreeName)\n+        }\n+\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        repoPath := repo.RepoPath()\n+        localPath := repo.LocalCopyPath()\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                // Directly return error if new branch already exists in the server\n+                if git.RepoHasBranch(repoPath, opts.NewBranch) {\n+                        return dberrors.BranchAlreadyExists{Name: opts.NewBranch}\n+                }\n+\n+                // Otherwise, delete branch from local copy in case out of sync\n+                if git.RepoHasBranch(localPath, opts.NewBranch) {\n+                        if err = git.DeleteBranch(localPath, opts.NewBranch, git.DeleteBranchOptions{\n+                                Force: true,\n+                        }); err != nil {\n+                                return fmt.Errorf(\"delete branch %q: %v\", opts.NewBranch, err)\n+                        }\n+                }\n+\n+                if err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        oldFilePath := path.Join(localPath, opts.OldTreeName)\n+        filePath := path.Join(localPath, opts.NewTreeName)\n+        if err = os.MkdirAll(path.Dir(filePath), os.ModePerm); err != nil {\n+                return err\n+        }\n+\n+        // If it's meant to be a new file, make sure it doesn't exist.\n+        if opts.IsNewFile {\n+                if com.IsExist(filePath) {\n+                        return ErrRepoFileAlreadyExist{filePath}\n+                }\n+        }\n+\n+        // Ignore move step if it's a new file under a directory.\n+        // Otherwise, move the file when name changed.\n+        if osutil.IsFile(oldFilePath) && opts.OldTreeName != opts.NewTreeName {\n+                if err = git.Move(localPath, opts.OldTreeName, opts.NewTreeName); err != nil {\n+                        return fmt.Errorf(\"git mv %q %q: %v\", opts.OldTreeName, opts.NewTreeName, err)\n+                }\n+        }\n+\n+        if err = ioutil.WriteFile(filePath, []byte(opts.Content), 0666); err != nil {\n+                return fmt.Errorf(\"write file: %v\", err)\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        } else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n+                return fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+        return nil\n }\n \n // GetDiffPreview produces and returns diff result of a file which is not yet committed.\n func (repo *Repository) GetDiffPreview(branch, treePath, content string) (diff *gitutil.Diff, err error) {\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(branch); err != nil {\n-\t\treturn nil, fmt.Errorf(\"discard local repo branch[%s] changes: %v\", branch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(branch); err != nil {\n-\t\treturn nil, fmt.Errorf(\"update local copy branch[%s]: %v\", branch, err)\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tfilePath := path.Join(localPath, treePath)\n-\tif err = os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif err = ioutil.WriteFile(filePath, []byte(content), 0666); err != nil {\n-\t\treturn nil, fmt.Errorf(\"write file: %v\", err)\n-\t}\n-\n-\tcmd := exec.Command(\"git\", \"diff\", treePath)\n-\tcmd.Dir = localPath\n-\tcmd.Stderr = os.Stderr\n-\n-\tstdout, err := cmd.StdoutPipe()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"get stdout pipe: %v\", err)\n-\t}\n-\n-\tif err = cmd.Start(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"start: %v\", err)\n-\t}\n-\n-\tpid := process.Add(fmt.Sprintf(\"GetDiffPreview [repo_path: %s]\", repo.RepoPath()), cmd)\n-\tdefer process.Remove(pid)\n-\n-\tdiff, err = gitutil.ParseDiff(stdout, conf.Git.MaxDiffFiles, conf.Git.MaxDiffLines, conf.Git.MaxDiffLineChars)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"parse diff: %v\", err)\n-\t}\n-\n-\tif err = cmd.Wait(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"wait: %v\", err)\n-\t}\n-\n-\treturn diff, nil\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(branch); err != nil {\n+                return nil, fmt.Errorf(\"discard local repo branch[%s] changes: %v\", branch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(branch); err != nil {\n+                return nil, fmt.Errorf(\"update local copy branch[%s]: %v\", branch, err)\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        filePath := path.Join(localPath, treePath)\n+        if err = os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {\n+                return nil, err\n+        }\n+        if err = ioutil.WriteFile(filePath, []byte(content), 0666); err != nil {\n+                return nil, fmt.Errorf(\"write file: %v\", err)\n+        }\n+\n+        cmd := exec.Command(\"git\", \"diff\", treePath)\n+        cmd.Dir = localPath\n+        cmd.Stderr = os.Stderr\n+\n+        stdout, err := cmd.StdoutPipe()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"get stdout pipe: %v\", err)\n+        }\n+\n+        if err = cmd.Start(); err != nil {\n+                return nil, fmt.Errorf(\"start: %v\", err)\n+        }\n+\n+        pid := process.Add(fmt.Sprintf(\"GetDiffPreview [repo_path: %s]\", repo.RepoPath()), cmd)\n+        defer process.Remove(pid)\n+\n+        diff, err = gitutil.ParseDiff(stdout, conf.Git.MaxDiffFiles, conf.Git.MaxDiffLines, conf.Git.MaxDiffLineChars)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"parse diff: %v\", err)\n+        }\n+\n+        if err = cmd.Wait(); err != nil {\n+                return nil, fmt.Errorf(\"wait: %v\", err)\n+        }\n+\n+        return diff, nil\n }\n \n // ________         .__          __           ___________.__.__\n@@ -266,58 +266,58 @@ func (repo *Repository) GetDiffPreview(branch, treePath, content string) (diff *\n //\n \n type DeleteRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tTreePath     string\n-\tMessage      string\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        TreePath     string\n+        Message      string\n }\n \n func (repo *Repository) DeleteRepoFile(doer *User, opts DeleteRepoFileOptions) (err error) {\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\tif err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tif err = os.Remove(path.Join(localPath, opts.TreePath)); err != nil {\n-\t\treturn fmt.Errorf(\"remove file %q: %v\", opts.TreePath, err)\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t} else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n-\t\treturn fmt.Errorf(\"commit changes to %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\treturn nil\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                if err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        if err = os.Remove(path.Join(localPath, opts.TreePath)); err != nil {\n+                return fmt.Errorf(\"remove file %q: %v\", opts.TreePath, err)\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        } else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n+                return fmt.Errorf(\"commit changes to %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+        return nil\n }\n \n //  ____ ___        .__                    .___ ___________.___.__\n@@ -330,228 +330,235 @@ func (repo *Repository) DeleteRepoFile(doer *User, opts DeleteRepoFileOptions) (\n \n // Upload represent a uploaded file to a repo to be deleted when moved\n type Upload struct {\n-\tID   int64\n-\tUUID string `xorm:\"uuid UNIQUE\"`\n-\tName string\n+        ID   int64\n+        UUID string `xorm:\"uuid UNIQUE\"`\n+        Name string\n }\n \n // UploadLocalPath returns where uploads is stored in local file system based on given UUID.\n func UploadLocalPath(uuid string) string {\n-\treturn path.Join(conf.Repository.Upload.TempPath, uuid[0:1], uuid[1:2], uuid)\n+        return path.Join(conf.Repository.Upload.TempPath, uuid[0:1], uuid[1:2], uuid)\n }\n \n // LocalPath returns where uploads are temporarily stored in local file system.\n func (upload *Upload) LocalPath() string {\n-\treturn UploadLocalPath(upload.UUID)\n+        return UploadLocalPath(upload.UUID)\n }\n \n // NewUpload creates a new upload object.\n func NewUpload(name string, buf []byte, file multipart.File) (_ *Upload, err error) {\n-\tif tool.IsMaliciousPath(name) {\n-\t\treturn nil, fmt.Errorf(\"malicious path detected: %s\", name)\n-\t}\n-\n-\tupload := &Upload{\n-\t\tUUID: gouuid.NewV4().String(),\n-\t\tName: name,\n-\t}\n-\n-\tlocalPath := upload.LocalPath()\n-\tif err = os.MkdirAll(path.Dir(localPath), os.ModePerm); err != nil {\n-\t\treturn nil, fmt.Errorf(\"mkdir all: %v\", err)\n-\t}\n-\n-\tfw, err := os.Create(localPath)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"create: %v\", err)\n-\t}\n-\tdefer fw.Close()\n-\n-\tif _, err = fw.Write(buf); err != nil {\n-\t\treturn nil, fmt.Errorf(\"write: %v\", err)\n-\t} else if _, err = io.Copy(fw, file); err != nil {\n-\t\treturn nil, fmt.Errorf(\"copy: %v\", err)\n-\t}\n-\n-\tif _, err := x.Insert(upload); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn upload, nil\n+        if tool.IsMaliciousPath(name) {\n+                return nil, fmt.Errorf(\"malicious path detected: %s\", name)\n+        }\n+\n+        upload := &Upload{\n+                UUID: gouuid.NewV4().String(),\n+                Name: name,\n+        }\n+\n+        localPath := upload.LocalPath()\n+        if err = os.MkdirAll(path.Dir(localPath), os.ModePerm); err != nil {\n+                return nil, fmt.Errorf(\"mkdir all: %v\", err)\n+        }\n+\n+        fw, err := os.Create(localPath)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"create: %v\", err)\n+        }\n+        defer fw.Close()\n+\n+        if _, err = fw.Write(buf); err != nil {\n+                return nil, fmt.Errorf(\"write: %v\", err)\n+        } else if _, err = io.Copy(fw, file); err != nil {\n+                return nil, fmt.Errorf(\"copy: %v\", err)\n+        }\n+\n+        if _, err := x.Insert(upload); err != nil {\n+                return nil, err\n+        }\n+\n+        return upload, nil\n }\n \n func GetUploadByUUID(uuid string) (*Upload, error) {\n-\tupload := &Upload{UUID: uuid}\n-\thas, err := x.Get(upload)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t} else if !has {\n-\t\treturn nil, ErrUploadNotExist{0, uuid}\n-\t}\n-\treturn upload, nil\n+        upload := &Upload{UUID: uuid}\n+        has, err := x.Get(upload)\n+        if err != nil {\n+                return nil, err\n+        } else if !has {\n+                return nil, ErrUploadNotExist{0, uuid}\n+        }\n+        return upload, nil\n }\n \n func GetUploadsByUUIDs(uuids []string) ([]*Upload, error) {\n-\tif len(uuids) == 0 {\n-\t\treturn []*Upload{}, nil\n-\t}\n+        if len(uuids) == 0 {\n+                return []*Upload{}, nil\n+        }\n \n-\t// Silently drop invalid uuids.\n-\tuploads := make([]*Upload, 0, len(uuids))\n-\treturn uploads, x.In(\"uuid\", uuids).Find(&uploads)\n+        // Silently drop invalid uuids.\n+        uploads := make([]*Upload, 0, len(uuids))\n+        return uploads, x.In(\"uuid\", uuids).Find(&uploads)\n }\n \n func DeleteUploads(uploads ...*Upload) (err error) {\n-\tif len(uploads) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tsess := x.NewSession()\n-\tdefer sess.Close()\n-\tif err = sess.Begin(); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tids := make([]int64, len(uploads))\n-\tfor i := 0; i < len(uploads); i++ {\n-\t\tids[i] = uploads[i].ID\n-\t}\n-\tif _, err = sess.In(\"id\", ids).Delete(new(Upload)); err != nil {\n-\t\treturn fmt.Errorf(\"delete uploads: %v\", err)\n-\t}\n-\n-\tfor _, upload := range uploads {\n-\t\tlocalPath := upload.LocalPath()\n-\t\tif !osutil.IsFile(localPath) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif err := os.Remove(localPath); err != nil {\n-\t\t\treturn fmt.Errorf(\"remove upload: %v\", err)\n-\t\t}\n-\t}\n-\n-\treturn sess.Commit()\n+        if len(uploads) == 0 {\n+                return nil\n+        }\n+\n+        sess := x.NewSession()\n+        defer sess.Close()\n+        if err = sess.Begin(); err != nil {\n+                return err\n+        }\n+\n+        ids := make([]int64, len(uploads))\n+        for i := 0; i < len(uploads); i++ {\n+                ids[i] = uploads[i].ID\n+        }\n+        if _, err = sess.In(\"id\", ids).Delete(new(Upload)); err != nil {\n+                return fmt.Errorf(\"delete uploads: %v\", err)\n+        }\n+\n+        for _, upload := range uploads {\n+                localPath := upload.LocalPath()\n+                if !osutil.IsFile(localPath) {\n+                        continue\n+                }\n+\n+                if err := os.Remove(localPath); err != nil {\n+                        return fmt.Errorf(\"remove upload: %v\", err)\n+                }\n+        }\n+\n+        return sess.Commit()\n }\n \n func DeleteUpload(u *Upload) error {\n-\treturn DeleteUploads(u)\n+        return DeleteUploads(u)\n }\n \n func DeleteUploadByUUID(uuid string) error {\n-\tupload, err := GetUploadByUUID(uuid)\n-\tif err != nil {\n-\t\tif IsErrUploadNotExist(err) {\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn fmt.Errorf(\"get upload by UUID[%s]: %v\", uuid, err)\n-\t}\n-\n-\tif err := DeleteUpload(upload); err != nil {\n-\t\treturn fmt.Errorf(\"delete upload: %v\", err)\n-\t}\n-\n-\treturn nil\n+        upload, err := GetUploadByUUID(uuid)\n+        if err != nil {\n+                if IsErrUploadNotExist(err) {\n+                        return nil\n+                }\n+                return fmt.Errorf(\"get upload by UUID[%s]: %v\", uuid, err)\n+        }\n+\n+        if err := DeleteUpload(upload); err != nil {\n+                return fmt.Errorf(\"delete upload: %v\", err)\n+        }\n+\n+        return nil\n }\n \n type UploadRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tTreePath     string\n-\tMessage      string\n-\tFiles        []string // In UUID format\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        TreePath     string\n+        Message      string\n+        Files        []string // In UUID format\n }\n \n // isRepositoryGitPath returns true if given path is or resides inside \".git\"\n // path of the repository.\n func isRepositoryGitPath(path string) bool {\n-\treturn strings.HasSuffix(path, \".git\") ||\n-\t\tstrings.Contains(path, \".git\"+string(os.PathSeparator)) ||\n-\t\t// Windows treats \".git.\" the same as \".git\"\n-\t\tstrings.HasSuffix(path, \".git.\") ||\n-\t\tstrings.Contains(path, \".git.\"+string(os.PathSeparator))\n+path = strings.TrimSpace(path)\n+path = strings.ReplaceAll(path, \"\\\\\", \"/\")\n+if strings.HasSuffix(path, \".git\") || strings.HasSuffix(path, \".git.\") {\n+return true\n+}\n+\n+for _, part := range strings.Split(path, \"/\") {\n+if strings.TrimSpace(part) == \".git\" || strings.TrimSpace(part) == \".git.\" {\n+return true\n+}\n+}\n+return false\n }\n \n func (repo *Repository) UploadRepoFiles(doer *User, opts UploadRepoFileOptions) error {\n-\tif len(opts.Files) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\tif isRepositoryGitPath(opts.TreePath) {\n-\t\treturn errors.Errorf(\"bad tree path %q\", opts.TreePath)\n-\t}\n-\n-\tuploads, err := GetUploadsByUUIDs(opts.Files)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"get uploads by UUIDs[%v]: %v\", opts.Files, err)\n-\t}\n-\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\tif err = repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tdirPath := path.Join(localPath, opts.TreePath)\n-\tif err = os.MkdirAll(dirPath, os.ModePerm); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Copy uploaded files into repository\n-\tfor _, upload := range uploads {\n-\t\ttmpPath := upload.LocalPath()\n-\t\tif !osutil.IsFile(tmpPath) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tupload.Name = pathutil.Clean(upload.Name)\n-\n-\t\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\t\tif isRepositoryGitPath(upload.Name) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\ttargetPath := path.Join(dirPath, upload.Name)\n-\t\tif err = com.Copy(tmpPath, targetPath); err != nil {\n-\t\t\treturn fmt.Errorf(\"copy: %v\", err)\n-\t\t}\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t} else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n-\t\treturn fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\n-\treturn DeleteUploads(uploads...)\n+        if len(opts.Files) == 0 {\n+                return nil\n+        }\n+\n+        // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+        if isRepositoryGitPath(opts.TreePath) {\n+                return errors.Errorf(\"bad tree path %q\", opts.TreePath)\n+        }\n+\n+        uploads, err := GetUploadsByUUIDs(opts.Files)\n+        if err != nil {\n+                return fmt.Errorf(\"get uploads by UUIDs[%v]: %v\", opts.Files, err)\n+        }\n+\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                if err = repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        dirPath := path.Join(localPath, opts.TreePath)\n+        if err = os.MkdirAll(dirPath, os.ModePerm); err != nil {\n+                return err\n+        }\n+\n+        // Copy uploaded files into repository\n+        for _, upload := range uploads {\n+                tmpPath := upload.LocalPath()\n+                if !osutil.IsFile(tmpPath) {\n+                        continue\n+                }\n+\n+                upload.Name = pathutil.Clean(upload.Name)\n+\n+                // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+                if isRepositoryGitPath(upload.Name) {\n+                        continue\n+                }\n+\n+                targetPath := path.Join(dirPath, upload.Name)\n+                if err = com.Copy(tmpPath, targetPath); err != nil {\n+                        return fmt.Errorf(\"copy: %v\", err)\n+                }\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        } else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n+                return fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+\n+        return DeleteUploads(uploads...)\n }\n"}
{"cve":"CVE-2022-29188:0708", "fix_patch": ""}
{"cve":"CVE-2023-50726:0708", "fix_patch": "diff --git a/server/application/application.go b/server/application/application.go\nindex 8ee16b934..86306a2f2 100644\n--- a/server/application/application.go\n+++ b/server/application/application.go\n@@ -1,144 +1,144 @@\n package application\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"math\"\n-\t\"reflect\"\n-\t\"sort\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\tkubecache \"github.com/argoproj/gitops-engine/pkg/cache\"\n-\t\"github.com/argoproj/gitops-engine/pkg/diff\"\n-\t\"github.com/argoproj/gitops-engine/pkg/sync/common\"\n-\t\"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n-\t\"github.com/argoproj/gitops-engine/pkg/utils/text\"\n-\t\"github.com/argoproj/pkg/sync\"\n-\tjsonpatch \"github.com/evanphx/json-patch\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n-\tv1 \"k8s.io/api/core/v1\"\n-\tapierr \"k8s.io/apimachinery/pkg/api/errors\"\n-\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n-\t\"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n-\t\"k8s.io/apimachinery/pkg/fields\"\n-\t\"k8s.io/apimachinery/pkg/labels\"\n-\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n-\t\"k8s.io/apimachinery/pkg/types\"\n-\t\"k8s.io/apimachinery/pkg/watch\"\n-\t\"k8s.io/client-go/kubernetes\"\n-\t\"k8s.io/client-go/rest\"\n-\t\"k8s.io/client-go/tools/cache\"\n-\t\"k8s.io/utils/pointer\"\n-\n-\targocommon \"github.com/argoproj/argo-cd/v2/common\"\n-\t\"github.com/argoproj/argo-cd/v2/pkg/apiclient/application\"\n-\tappv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n-\tappclientset \"github.com/argoproj/argo-cd/v2/pkg/client/clientset/versioned\"\n-\tapplisters \"github.com/argoproj/argo-cd/v2/pkg/client/listers/application/v1alpha1\"\n-\t\"github.com/argoproj/argo-cd/v2/reposerver/apiclient\"\n-\tservercache \"github.com/argoproj/argo-cd/v2/server/cache\"\n-\t\"github.com/argoproj/argo-cd/v2/server/deeplinks\"\n-\t\"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n-\t\"github.com/argoproj/argo-cd/v2/util/argo\"\n-\targoutil \"github.com/argoproj/argo-cd/v2/util/argo\"\n-\t\"github.com/argoproj/argo-cd/v2/util/collections\"\n-\t\"github.com/argoproj/argo-cd/v2/util/db\"\n-\t\"github.com/argoproj/argo-cd/v2/util/env\"\n-\t\"github.com/argoproj/argo-cd/v2/util/git\"\n-\tioutil \"github.com/argoproj/argo-cd/v2/util/io\"\n-\t\"github.com/argoproj/argo-cd/v2/util/lua\"\n-\t\"github.com/argoproj/argo-cd/v2/util/manifeststream\"\n-\t\"github.com/argoproj/argo-cd/v2/util/rbac\"\n-\t\"github.com/argoproj/argo-cd/v2/util/security\"\n-\t\"github.com/argoproj/argo-cd/v2/util/session\"\n-\t\"github.com/argoproj/argo-cd/v2/util/settings\"\n-\n-\tapplicationType \"github.com/argoproj/argo-cd/v2/pkg/apis/application\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"math\"\n+        \"reflect\"\n+        \"sort\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        kubecache \"github.com/argoproj/gitops-engine/pkg/cache\"\n+        \"github.com/argoproj/gitops-engine/pkg/diff\"\n+        \"github.com/argoproj/gitops-engine/pkg/sync/common\"\n+        \"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n+        \"github.com/argoproj/gitops-engine/pkg/utils/text\"\n+        \"github.com/argoproj/pkg/sync\"\n+        jsonpatch \"github.com/evanphx/json-patch\"\n+        log \"github.com/sirupsen/logrus\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n+        v1 \"k8s.io/api/core/v1\"\n+        apierr \"k8s.io/apimachinery/pkg/api/errors\"\n+        metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+        \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n+        \"k8s.io/apimachinery/pkg/fields\"\n+        \"k8s.io/apimachinery/pkg/labels\"\n+        \"k8s.io/apimachinery/pkg/runtime/schema\"\n+        \"k8s.io/apimachinery/pkg/types\"\n+        \"k8s.io/apimachinery/pkg/watch\"\n+        \"k8s.io/client-go/kubernetes\"\n+        \"k8s.io/client-go/rest\"\n+        \"k8s.io/client-go/tools/cache\"\n+        \"k8s.io/utils/pointer\"\n+\n+        argocommon \"github.com/argoproj/argo-cd/v2/common\"\n+        \"github.com/argoproj/argo-cd/v2/pkg/apiclient/application\"\n+        appv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n+        appclientset \"github.com/argoproj/argo-cd/v2/pkg/client/clientset/versioned\"\n+        applisters \"github.com/argoproj/argo-cd/v2/pkg/client/listers/application/v1alpha1\"\n+        \"github.com/argoproj/argo-cd/v2/reposerver/apiclient\"\n+        servercache \"github.com/argoproj/argo-cd/v2/server/cache\"\n+        \"github.com/argoproj/argo-cd/v2/server/deeplinks\"\n+        \"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n+        \"github.com/argoproj/argo-cd/v2/util/argo\"\n+        argoutil \"github.com/argoproj/argo-cd/v2/util/argo\"\n+        \"github.com/argoproj/argo-cd/v2/util/collections\"\n+        \"github.com/argoproj/argo-cd/v2/util/db\"\n+        \"github.com/argoproj/argo-cd/v2/util/env\"\n+        \"github.com/argoproj/argo-cd/v2/util/git\"\n+        ioutil \"github.com/argoproj/argo-cd/v2/util/io\"\n+        \"github.com/argoproj/argo-cd/v2/util/lua\"\n+        \"github.com/argoproj/argo-cd/v2/util/manifeststream\"\n+        \"github.com/argoproj/argo-cd/v2/util/rbac\"\n+        \"github.com/argoproj/argo-cd/v2/util/security\"\n+        \"github.com/argoproj/argo-cd/v2/util/session\"\n+        \"github.com/argoproj/argo-cd/v2/util/settings\"\n+\n+        applicationType \"github.com/argoproj/argo-cd/v2/pkg/apis/application\"\n )\n \n type AppResourceTreeFn func(ctx context.Context, app *appv1.Application) (*appv1.ApplicationTree, error)\n \n const (\n-\tmaxPodLogsToRender                 = 10\n-\tbackgroundPropagationPolicy string = \"background\"\n-\tforegroundPropagationPolicy string = \"foreground\"\n+        maxPodLogsToRender                 = 10\n+        backgroundPropagationPolicy string = \"background\"\n+        foregroundPropagationPolicy string = \"foreground\"\n )\n \n var (\n-\twatchAPIBufferSize  = env.ParseNumFromEnv(argocommon.EnvWatchAPIBufferSize, 1000, 0, math.MaxInt32)\n-\tpermissionDeniedErr = status.Error(codes.PermissionDenied, \"permission denied\")\n+        watchAPIBufferSize  = env.ParseNumFromEnv(argocommon.EnvWatchAPIBufferSize, 1000, 0, math.MaxInt32)\n+        permissionDeniedErr = status.Error(codes.PermissionDenied, \"permission denied\")\n )\n \n // Server provides an Application service\n type Server struct {\n-\tns                string\n-\tkubeclientset     kubernetes.Interface\n-\tappclientset      appclientset.Interface\n-\tappLister         applisters.ApplicationLister\n-\tappInformer       cache.SharedIndexInformer\n-\tappBroadcaster    Broadcaster\n-\trepoClientset     apiclient.Clientset\n-\tkubectl           kube.Kubectl\n-\tdb                db.ArgoDB\n-\tenf               *rbac.Enforcer\n-\tprojectLock       sync.KeyLock\n-\tauditLogger       *argo.AuditLogger\n-\tsettingsMgr       *settings.SettingsManager\n-\tcache             *servercache.Cache\n-\tprojInformer      cache.SharedIndexInformer\n-\tenabledNamespaces []string\n+        ns                string\n+        kubeclientset     kubernetes.Interface\n+        appclientset      appclientset.Interface\n+        appLister         applisters.ApplicationLister\n+        appInformer       cache.SharedIndexInformer\n+        appBroadcaster    Broadcaster\n+        repoClientset     apiclient.Clientset\n+        kubectl           kube.Kubectl\n+        db                db.ArgoDB\n+        enf               *rbac.Enforcer\n+        projectLock       sync.KeyLock\n+        auditLogger       *argo.AuditLogger\n+        settingsMgr       *settings.SettingsManager\n+        cache             *servercache.Cache\n+        projInformer      cache.SharedIndexInformer\n+        enabledNamespaces []string\n }\n \n // NewServer returns a new instance of the Application service\n func NewServer(\n-\tnamespace string,\n-\tkubeclientset kubernetes.Interface,\n-\tappclientset appclientset.Interface,\n-\tappLister applisters.ApplicationLister,\n-\tappInformer cache.SharedIndexInformer,\n-\tappBroadcaster Broadcaster,\n-\trepoClientset apiclient.Clientset,\n-\tcache *servercache.Cache,\n-\tkubectl kube.Kubectl,\n-\tdb db.ArgoDB,\n-\tenf *rbac.Enforcer,\n-\tprojectLock sync.KeyLock,\n-\tsettingsMgr *settings.SettingsManager,\n-\tprojInformer cache.SharedIndexInformer,\n-\tenabledNamespaces []string,\n+        namespace string,\n+        kubeclientset kubernetes.Interface,\n+        appclientset appclientset.Interface,\n+        appLister applisters.ApplicationLister,\n+        appInformer cache.SharedIndexInformer,\n+        appBroadcaster Broadcaster,\n+        repoClientset apiclient.Clientset,\n+        cache *servercache.Cache,\n+        kubectl kube.Kubectl,\n+        db db.ArgoDB,\n+        enf *rbac.Enforcer,\n+        projectLock sync.KeyLock,\n+        settingsMgr *settings.SettingsManager,\n+        projInformer cache.SharedIndexInformer,\n+        enabledNamespaces []string,\n ) (application.ApplicationServiceServer, AppResourceTreeFn) {\n-\tif appBroadcaster == nil {\n-\t\tappBroadcaster = &broadcasterHandler{}\n-\t}\n-\t_, err := appInformer.AddEventHandler(appBroadcaster)\n-\tif err != nil {\n-\t\tlog.Error(err)\n-\t}\n-\ts := &Server{\n-\t\tns:                namespace,\n-\t\tappclientset:      appclientset,\n-\t\tappLister:         appLister,\n-\t\tappInformer:       appInformer,\n-\t\tappBroadcaster:    appBroadcaster,\n-\t\tkubeclientset:     kubeclientset,\n-\t\tcache:             cache,\n-\t\tdb:                db,\n-\t\trepoClientset:     repoClientset,\n-\t\tkubectl:           kubectl,\n-\t\tenf:               enf,\n-\t\tprojectLock:       projectLock,\n-\t\tauditLogger:       argo.NewAuditLogger(namespace, kubeclientset, \"argocd-server\"),\n-\t\tsettingsMgr:       settingsMgr,\n-\t\tprojInformer:      projInformer,\n-\t\tenabledNamespaces: enabledNamespaces,\n-\t}\n-\treturn s, s.getAppResources\n+        if appBroadcaster == nil {\n+                appBroadcaster = &broadcasterHandler{}\n+        }\n+        _, err := appInformer.AddEventHandler(appBroadcaster)\n+        if err != nil {\n+                log.Error(err)\n+        }\n+        s := &Server{\n+                ns:                namespace,\n+                appclientset:      appclientset,\n+                appLister:         appLister,\n+                appInformer:       appInformer,\n+                appBroadcaster:    appBroadcaster,\n+                kubeclientset:     kubeclientset,\n+                cache:             cache,\n+                db:                db,\n+                repoClientset:     repoClientset,\n+                kubectl:           kubectl,\n+                enf:               enf,\n+                projectLock:       projectLock,\n+                auditLogger:       argo.NewAuditLogger(namespace, kubeclientset, \"argocd-server\"),\n+                settingsMgr:       settingsMgr,\n+                projInformer:      projInformer,\n+                enabledNamespaces: enabledNamespaces,\n+        }\n+        return s, s.getAppResources\n }\n \n // getAppEnforceRBAC gets the Application with the given name in the given namespace. If no namespace is\n@@ -152,679 +152,689 @@ func NewServer(\n // If the user does provide a \"project,\" we can respond more specifically. If the user does not have access to the given\n // app name in the given project, we return \"permission denied.\" If the app exists, but the project is different from\n func (s *Server) getAppEnforceRBAC(ctx context.Context, action, project, namespace, name string, getApp func() (*appv1.Application, error)) (*appv1.Application, error) {\n-\tuser := session.Username(ctx)\n-\tif user == \"\" {\n-\t\tuser = \"Unknown user\"\n-\t}\n-\tlogCtx := log.WithFields(map[string]interface{}{\n-\t\t\"user\":        user,\n-\t\t\"application\": name,\n-\t\t\"namespace\":   namespace,\n-\t})\n-\tif project != \"\" {\n-\t\t// The user has provided everything we need to perform an initial RBAC check.\n-\t\tgivenRBACName := security.RBACName(s.ns, project, namespace, name)\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, action, givenRBACName); err != nil {\n-\t\t\tlogCtx.WithFields(map[string]interface{}{\n-\t\t\t\t\"project\":                project,\n-\t\t\t\targocommon.SecurityField: argocommon.SecurityMedium,\n-\t\t\t}).Warnf(\"user tried to %s application which they do not have access to: %s\", action, err)\n-\t\t\t// Do a GET on the app. This ensures that the timing of a \"no access\" response is the same as a \"yes access,\n-\t\t\t// but the app is in a different project\" response. We don't want the user inferring the existence of the\n-\t\t\t// app from response time.\n-\t\t\t_, _ = getApp()\n-\t\t\treturn nil, permissionDeniedErr\n-\t\t}\n-\t}\n-\ta, err := getApp()\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\tif project != \"\" {\n-\t\t\t\t// We know that the user was allowed to get the Application, but the Application does not exist. Return 404.\n-\t\t\t\treturn nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n-\t\t\t}\n-\t\t\t// We don't know if the user was allowed to get the Application, and we don't want to leak information about\n-\t\t\t// the Application's existence. Return 403.\n-\t\t\tlogCtx.Warn(\"application does not exist\")\n-\t\t\treturn nil, permissionDeniedErr\n-\t\t}\n-\t\tlogCtx.Errorf(\"failed to get application: %s\", err)\n-\t\treturn nil, permissionDeniedErr\n-\t}\n-\t// Even if we performed an initial RBAC check (because the request was fully parameterized), we still need to\n-\t// perform a second RBAC check to ensure that the user has access to the actual Application's project (not just the\n-\t// project they specified in the request).\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, action, a.RBACName(s.ns)); err != nil {\n-\t\tlogCtx.WithFields(map[string]interface{}{\n-\t\t\t\"project\":                a.Spec.Project,\n-\t\t\targocommon.SecurityField: argocommon.SecurityMedium,\n-\t\t}).Warnf(\"user tried to %s application which they do not have access to: %s\", action, err)\n-\t\tif project != \"\" {\n-\t\t\t// The user specified a project. We would have returned a 404 if the user had access to the app, but the app\n-\t\t\t// did not exist. So we have to return a 404 when the app does exist, but the user does not have access.\n-\t\t\t// Otherwise, they could infer that the app exists based on the error code.\n-\t\t\treturn nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n-\t\t}\n-\t\t// The user didn't specify a project. We always return permission denied for both lack of access and lack of\n-\t\t// existence.\n-\t\treturn nil, permissionDeniedErr\n-\t}\n-\teffectiveProject := \"default\"\n-\tif a.Spec.Project != \"\" {\n-\t\teffectiveProject = a.Spec.Project\n-\t}\n-\tif project != \"\" && effectiveProject != project {\n-\t\tlogCtx.WithFields(map[string]interface{}{\n-\t\t\t\"project\":                a.Spec.Project,\n-\t\t\targocommon.SecurityField: argocommon.SecurityMedium,\n-\t\t}).Warnf(\"user tried to %s application in project %s, but the application is in project %s\", action, project, effectiveProject)\n-\t\t// The user has access to the app, but the app is in a different project. Return 404, meaning \"app doesn't\n-\t\t// exist in that project\".\n-\t\treturn nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n-\t}\n-\treturn a, nil\n+        user := session.Username(ctx)\n+        if user == \"\" {\n+                user = \"Unknown user\"\n+        }\n+        logCtx := log.WithFields(map[string]interface{}{\n+                \"user\":        user,\n+                \"application\": name,\n+                \"namespace\":   namespace,\n+        })\n+        if project != \"\" {\n+                // The user has provided everything we need to perform an initial RBAC check.\n+                givenRBACName := security.RBACName(s.ns, project, namespace, name)\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, action, givenRBACName); err != nil {\n+                        logCtx.WithFields(map[string]interface{}{\n+                                \"project\":                project,\n+                                argocommon.SecurityField: argocommon.SecurityMedium,\n+                        }).Warnf(\"user tried to %s application which they do not have access to: %s\", action, err)\n+                        // Do a GET on the app. This ensures that the timing of a \"no access\" response is the same as a \"yes access,\n+                        // but the app is in a different project\" response. We don't want the user inferring the existence of the\n+                        // app from response time.\n+                        _, _ = getApp()\n+                        return nil, permissionDeniedErr\n+                }\n+        }\n+        a, err := getApp()\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        if project != \"\" {\n+                                // We know that the user was allowed to get the Application, but the Application does not exist. Return 404.\n+                                return nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n+                        }\n+                        // We don't know if the user was allowed to get the Application, and we don't want to leak information about\n+                        // the Application's existence. Return 403.\n+                        logCtx.Warn(\"application does not exist\")\n+                        return nil, permissionDeniedErr\n+                }\n+                logCtx.Errorf(\"failed to get application: %s\", err)\n+                return nil, permissionDeniedErr\n+        }\n+        // Even if we performed an initial RBAC check (because the request was fully parameterized), we still need to\n+        // perform a second RBAC check to ensure that the user has access to the actual Application's project (not just the\n+        // project they specified in the request).\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, action, a.RBACName(s.ns)); err != nil {\n+                logCtx.WithFields(map[string]interface{}{\n+                        \"project\":                a.Spec.Project,\n+                        argocommon.SecurityField: argocommon.SecurityMedium,\n+                }).Warnf(\"user tried to %s application which they do not have access to: %s\", action, err)\n+                if project != \"\" {\n+                        // The user specified a project. We would have returned a 404 if the user had access to the app, but the app\n+                        // did not exist. So we have to return a 404 when the app does exist, but the user does not have access.\n+                        // Otherwise, they could infer that the app exists based on the error code.\n+                        return nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n+                }\n+                // The user didn't specify a project. We always return permission denied for both lack of access and lack of\n+                // existence.\n+                return nil, permissionDeniedErr\n+        }\n+        effectiveProject := \"default\"\n+        if a.Spec.Project != \"\" {\n+                effectiveProject = a.Spec.Project\n+        }\n+        if project != \"\" && effectiveProject != project {\n+                logCtx.WithFields(map[string]interface{}{\n+                        \"project\":                a.Spec.Project,\n+                        argocommon.SecurityField: argocommon.SecurityMedium,\n+                }).Warnf(\"user tried to %s application in project %s, but the application is in project %s\", action, project, effectiveProject)\n+                // The user has access to the app, but the app is in a different project. Return 404, meaning \"app doesn't\n+                // exist in that project\".\n+                return nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n+        }\n+        return a, nil\n }\n \n // getApplicationEnforceRBACInformer uses an informer to get an Application. If the app does not exist, permission is\n // denied, or any other error occurs when getting the app, we return a permission denied error to obscure any sensitive\n // information.\n func (s *Server) getApplicationEnforceRBACInformer(ctx context.Context, action, project, namespace, name string) (*appv1.Application, error) {\n-\tnamespaceOrDefault := s.appNamespaceOrDefault(namespace)\n-\treturn s.getAppEnforceRBAC(ctx, action, project, namespaceOrDefault, name, func() (*appv1.Application, error) {\n-\t\treturn s.appLister.Applications(namespaceOrDefault).Get(name)\n-\t})\n+        namespaceOrDefault := s.appNamespaceOrDefault(namespace)\n+        return s.getAppEnforceRBAC(ctx, action, project, namespaceOrDefault, name, func() (*appv1.Application, error) {\n+                return s.appLister.Applications(namespaceOrDefault).Get(name)\n+        })\n }\n \n // getApplicationEnforceRBACClient uses a client to get an Application. If the app does not exist, permission is denied,\n // or any other error occurs when getting the app, we return a permission denied error to obscure any sensitive\n // information.\n func (s *Server) getApplicationEnforceRBACClient(ctx context.Context, action, project, namespace, name, resourceVersion string) (*appv1.Application, error) {\n-\tnamespaceOrDefault := s.appNamespaceOrDefault(namespace)\n-\treturn s.getAppEnforceRBAC(ctx, action, project, namespaceOrDefault, name, func() (*appv1.Application, error) {\n-\t\tif !s.isNamespaceEnabled(namespaceOrDefault) {\n-\t\t\treturn nil, security.NamespaceNotPermittedError(namespaceOrDefault)\n-\t\t}\n-\t\treturn s.appclientset.ArgoprojV1alpha1().Applications(namespaceOrDefault).Get(ctx, name, metav1.GetOptions{\n-\t\t\tResourceVersion: resourceVersion,\n-\t\t})\n-\t})\n+        namespaceOrDefault := s.appNamespaceOrDefault(namespace)\n+        return s.getAppEnforceRBAC(ctx, action, project, namespaceOrDefault, name, func() (*appv1.Application, error) {\n+                if !s.isNamespaceEnabled(namespaceOrDefault) {\n+                        return nil, security.NamespaceNotPermittedError(namespaceOrDefault)\n+                }\n+                return s.appclientset.ArgoprojV1alpha1().Applications(namespaceOrDefault).Get(ctx, name, metav1.GetOptions{\n+                        ResourceVersion: resourceVersion,\n+                })\n+        })\n }\n \n // List returns list of applications\n func (s *Server) List(ctx context.Context, q *application.ApplicationQuery) (*appv1.ApplicationList, error) {\n-\tselector, err := labels.Parse(q.GetSelector())\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error parsing the selector: %w\", err)\n-\t}\n-\tvar apps []*appv1.Application\n-\tif q.GetAppNamespace() == \"\" {\n-\t\tapps, err = s.appLister.List(selector)\n-\t} else {\n-\t\tapps, err = s.appLister.Applications(q.GetAppNamespace()).List(selector)\n-\t}\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error listing apps with selectors: %w\", err)\n-\t}\n-\n-\tfilteredApps := apps\n-\t// Filter applications by name\n-\tif q.Name != nil {\n-\t\tfilteredApps = argoutil.FilterByNameP(filteredApps, *q.Name)\n-\t}\n-\n-\t// Filter applications by projects\n-\tfilteredApps = argoutil.FilterByProjectsP(filteredApps, getProjectsFromApplicationQuery(*q))\n-\n-\t// Filter applications by source repo URL\n-\tfilteredApps = argoutil.FilterByRepoP(filteredApps, q.GetRepo())\n-\n-\tnewItems := make([]appv1.Application, 0)\n-\tfor _, a := range filteredApps {\n-\t\t// Skip any application that is neither in the control plane's namespace\n-\t\t// nor in the list of enabled namespaces.\n-\t\tif !s.isNamespaceEnabled(a.Namespace) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif s.enf.Enforce(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)) {\n-\t\t\tnewItems = append(newItems, *a)\n-\t\t}\n-\t}\n-\n-\t// Sort found applications by name\n-\tsort.Slice(newItems, func(i, j int) bool {\n-\t\treturn newItems[i].Name < newItems[j].Name\n-\t})\n-\n-\tappList := appv1.ApplicationList{\n-\t\tListMeta: metav1.ListMeta{\n-\t\t\tResourceVersion: s.appInformer.LastSyncResourceVersion(),\n-\t\t},\n-\t\tItems: newItems,\n-\t}\n-\treturn &appList, nil\n+        selector, err := labels.Parse(q.GetSelector())\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error parsing the selector: %w\", err)\n+        }\n+        var apps []*appv1.Application\n+        if q.GetAppNamespace() == \"\" {\n+                apps, err = s.appLister.List(selector)\n+        } else {\n+                apps, err = s.appLister.Applications(q.GetAppNamespace()).List(selector)\n+        }\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error listing apps with selectors: %w\", err)\n+        }\n+\n+        filteredApps := apps\n+        // Filter applications by name\n+        if q.Name != nil {\n+                filteredApps = argoutil.FilterByNameP(filteredApps, *q.Name)\n+        }\n+\n+        // Filter applications by projects\n+        filteredApps = argoutil.FilterByProjectsP(filteredApps, getProjectsFromApplicationQuery(*q))\n+\n+        // Filter applications by source repo URL\n+        filteredApps = argoutil.FilterByRepoP(filteredApps, q.GetRepo())\n+\n+        newItems := make([]appv1.Application, 0)\n+        for _, a := range filteredApps {\n+                // Skip any application that is neither in the control plane's namespace\n+                // nor in the list of enabled namespaces.\n+                if !s.isNamespaceEnabled(a.Namespace) {\n+                        continue\n+                }\n+                if s.enf.Enforce(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)) {\n+                        newItems = append(newItems, *a)\n+                }\n+        }\n+\n+        // Sort found applications by name\n+        sort.Slice(newItems, func(i, j int) bool {\n+                return newItems[i].Name < newItems[j].Name\n+        })\n+\n+        appList := appv1.ApplicationList{\n+                ListMeta: metav1.ListMeta{\n+                        ResourceVersion: s.appInformer.LastSyncResourceVersion(),\n+                },\n+                Items: newItems,\n+        }\n+        return &appList, nil\n }\n \n // Create creates an application\n func (s *Server) Create(ctx context.Context, q *application.ApplicationCreateRequest) (*appv1.Application, error) {\n-\tif q.GetApplication() == nil {\n-\t\treturn nil, fmt.Errorf(\"error creating application: application is nil in request\")\n-\t}\n-\ta := q.GetApplication()\n-\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionCreate, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.projectLock.RLock(a.Spec.GetProject())\n-\tdefer s.projectLock.RUnlock(a.Spec.GetProject())\n-\n-\tvalidate := true\n-\tif q.Validate != nil {\n-\t\tvalidate = *q.Validate\n-\t}\n-\terr := s.validateAndNormalizeApp(ctx, a, validate)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error while validating and normalizing app: %w\", err)\n-\t}\n-\n-\tappNs := s.appNamespaceOrDefault(a.Namespace)\n-\n-\tif !s.isNamespaceEnabled(appNs) {\n-\t\treturn nil, security.NamespaceNotPermittedError(appNs)\n-\t}\n-\n-\tcreated, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Create(ctx, a, metav1.CreateOptions{})\n-\tif err == nil {\n-\t\ts.logAppEvent(created, ctx, argo.EventReasonResourceCreated, \"created application\")\n-\t\ts.waitSync(created)\n-\t\treturn created, nil\n-\t}\n-\tif !apierr.IsAlreadyExists(err) {\n-\t\treturn nil, fmt.Errorf(\"error creating application: %w\", err)\n-\t}\n-\n-\t// act idempotent if existing spec matches new spec\n-\texisting, err := s.appLister.Applications(appNs).Get(a.Name)\n-\tif err != nil {\n-\t\treturn nil, status.Errorf(codes.Internal, \"unable to check existing application details (%s): %v\", appNs, err)\n-\t}\n-\tequalSpecs := reflect.DeepEqual(existing.Spec, a.Spec) &&\n-\t\treflect.DeepEqual(existing.Labels, a.Labels) &&\n-\t\treflect.DeepEqual(existing.Annotations, a.Annotations) &&\n-\t\treflect.DeepEqual(existing.Finalizers, a.Finalizers)\n-\n-\tif equalSpecs {\n-\t\treturn existing, nil\n-\t}\n-\tif q.Upsert == nil || !*q.Upsert {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"existing application spec is different, use upsert flag to force update\")\n-\t}\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tupdated, err := s.updateApp(existing, a, ctx, true)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error updating application: %w\", err)\n-\t}\n-\treturn updated, nil\n+        if q.GetApplication() == nil {\n+                return nil, fmt.Errorf(\"error creating application: application is nil in request\")\n+        }\n+        a := q.GetApplication()\n+\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionCreate, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        s.projectLock.RLock(a.Spec.GetProject())\n+        defer s.projectLock.RUnlock(a.Spec.GetProject())\n+\n+        validate := true\n+        if q.Validate != nil {\n+                validate = *q.Validate\n+        }\n+        err := s.validateAndNormalizeApp(ctx, a, validate)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error while validating and normalizing app: %w\", err)\n+        }\n+        if a.Spec.Source.RepoURL == \"\" {\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionOverride, a.RBACName(s.ns)); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        appNs := s.appNamespaceOrDefault(a.Namespace)\n+\n+        if !s.isNamespaceEnabled(appNs) {\n+                return nil, security.NamespaceNotPermittedError(appNs)\n+        }\n+\n+        created, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Create(ctx, a, metav1.CreateOptions{})\n+        if err == nil {\n+                s.logAppEvent(created, ctx, argo.EventReasonResourceCreated, \"created application\")\n+                s.waitSync(created)\n+                return created, nil\n+        }\n+        if !apierr.IsAlreadyExists(err) {\n+                return nil, fmt.Errorf(\"error creating application: %w\", err)\n+        }\n+\n+        // act idempotent if existing spec matches new spec\n+        existing, err := s.appLister.Applications(appNs).Get(a.Name)\n+        if err != nil {\n+                return nil, status.Errorf(codes.Internal, \"unable to check existing application details (%s): %v\", appNs, err)\n+        }\n+        equalSpecs := reflect.DeepEqual(existing.Spec, a.Spec) &&\n+                reflect.DeepEqual(existing.Labels, a.Labels) &&\n+                reflect.DeepEqual(existing.Annotations, a.Annotations) &&\n+                reflect.DeepEqual(existing.Finalizers, a.Finalizers)\n+\n+        if equalSpecs {\n+                return existing, nil\n+        }\n+        if q.Upsert == nil || !*q.Upsert {\n+                return nil, status.Errorf(codes.InvalidArgument, \"existing application spec is different, use upsert flag to force update\")\n+        }\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+        if a.Spec.Source.RepoURL == \"\" {\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionOverride, a.RBACName(s.ns)); err != nil {\n+                        return nil, err\n+                }\n+        }\n+        updated, err := s.updateApp(existing, a, ctx, true)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error updating application: %w\", err)\n+        }\n+        return updated, nil\n }\n \n func (s *Server) queryRepoServer(ctx context.Context, a *appv1.Application, action func(\n-\tclient apiclient.RepoServerServiceClient,\n-\trepo *appv1.Repository,\n-\thelmRepos []*appv1.Repository,\n-\thelmCreds []*appv1.RepoCreds,\n-\thelmOptions *appv1.HelmOptions,\n-\tkustomizeOptions *appv1.KustomizeOptions,\n-\tenabledSourceTypes map[string]bool,\n+        client apiclient.RepoServerServiceClient,\n+        repo *appv1.Repository,\n+        helmRepos []*appv1.Repository,\n+        helmCreds []*appv1.RepoCreds,\n+        helmOptions *appv1.HelmOptions,\n+        kustomizeOptions *appv1.KustomizeOptions,\n+        enabledSourceTypes map[string]bool,\n ) error) error {\n \n-\tcloser, client, err := s.repoClientset.NewRepoServerClient()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error creating repo server client: %w\", err)\n-\t}\n-\tdefer ioutil.Close(closer)\n-\trepo, err := s.db.GetRepository(ctx, a.Spec.GetSource().RepoURL)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting repository: %w\", err)\n-\t}\n-\tkustomizeSettings, err := s.settingsMgr.GetKustomizeSettings()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting kustomize settings: %w\", err)\n-\t}\n-\tkustomizeOptions, err := kustomizeSettings.GetOptions(a.Spec.GetSource())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting kustomize settings options: %w\", err)\n-\t}\n-\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\treturn status.Errorf(codes.InvalidArgument, \"application references project %s which does not exist\", a.Spec.Project)\n-\t\t}\n-\t\treturn fmt.Errorf(\"error getting application's project: %w\", err)\n-\t}\n-\n-\thelmRepos, err := s.db.ListHelmRepositories(ctx)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error listing helm repositories: %w\", err)\n-\t}\n-\n-\tpermittedHelmRepos, err := argo.GetPermittedRepos(proj, helmRepos)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error retrieving permitted repos: %w\", err)\n-\t}\n-\thelmRepositoryCredentials, err := s.db.GetAllHelmRepositoryCredentials(ctx)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting helm repository credentials: %w\", err)\n-\t}\n-\thelmOptions, err := s.settingsMgr.GetHelmSettings()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting helm settings: %w\", err)\n-\t}\n-\tpermittedHelmCredentials, err := argo.GetPermittedReposCredentials(proj, helmRepositoryCredentials)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting permitted repos credentials: %w\", err)\n-\t}\n-\tenabledSourceTypes, err := s.settingsMgr.GetEnabledSourceTypes()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting settings enabled source types: %w\", err)\n-\t}\n-\treturn action(client, repo, permittedHelmRepos, permittedHelmCredentials, helmOptions, kustomizeOptions, enabledSourceTypes)\n+        closer, client, err := s.repoClientset.NewRepoServerClient()\n+        if err != nil {\n+                return fmt.Errorf(\"error creating repo server client: %w\", err)\n+        }\n+        defer ioutil.Close(closer)\n+        repo, err := s.db.GetRepository(ctx, a.Spec.GetSource().RepoURL)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting repository: %w\", err)\n+        }\n+        kustomizeSettings, err := s.settingsMgr.GetKustomizeSettings()\n+        if err != nil {\n+                return fmt.Errorf(\"error getting kustomize settings: %w\", err)\n+        }\n+        kustomizeOptions, err := kustomizeSettings.GetOptions(a.Spec.GetSource())\n+        if err != nil {\n+                return fmt.Errorf(\"error getting kustomize settings options: %w\", err)\n+        }\n+        proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        return status.Errorf(codes.InvalidArgument, \"application references project %s which does not exist\", a.Spec.Project)\n+                }\n+                return fmt.Errorf(\"error getting application's project: %w\", err)\n+        }\n+\n+        helmRepos, err := s.db.ListHelmRepositories(ctx)\n+        if err != nil {\n+                return fmt.Errorf(\"error listing helm repositories: %w\", err)\n+        }\n+\n+        permittedHelmRepos, err := argo.GetPermittedRepos(proj, helmRepos)\n+        if err != nil {\n+                return fmt.Errorf(\"error retrieving permitted repos: %w\", err)\n+        }\n+        helmRepositoryCredentials, err := s.db.GetAllHelmRepositoryCredentials(ctx)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting helm repository credentials: %w\", err)\n+        }\n+        helmOptions, err := s.settingsMgr.GetHelmSettings()\n+        if err != nil {\n+                return fmt.Errorf(\"error getting helm settings: %w\", err)\n+        }\n+        permittedHelmCredentials, err := argo.GetPermittedReposCredentials(proj, helmRepositoryCredentials)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting permitted repos credentials: %w\", err)\n+        }\n+        enabledSourceTypes, err := s.settingsMgr.GetEnabledSourceTypes()\n+        if err != nil {\n+                return fmt.Errorf(\"error getting settings enabled source types: %w\", err)\n+        }\n+        return action(client, repo, permittedHelmRepos, permittedHelmCredentials, helmOptions, kustomizeOptions, enabledSourceTypes)\n }\n \n // GetManifests returns application manifests\n func (s *Server) GetManifests(ctx context.Context, q *application.ApplicationManifestQuery) (*apiclient.ManifestResponse, error) {\n-\tif q.Name == nil || *q.Name == \"\" {\n-\t\treturn nil, fmt.Errorf(\"invalid request: application name is missing\")\n-\t}\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tsource := a.Spec.GetSource()\n-\n-\tif !s.isNamespaceEnabled(a.Namespace) {\n-\t\treturn nil, security.NamespaceNotPermittedError(a.Namespace)\n-\t}\n-\n-\tvar manifestInfo *apiclient.ManifestResponse\n-\terr = s.queryRepoServer(ctx, a, func(\n-\t\tclient apiclient.RepoServerServiceClient, repo *appv1.Repository, helmRepos []*appv1.Repository, helmCreds []*appv1.RepoCreds, helmOptions *appv1.HelmOptions, kustomizeOptions *appv1.KustomizeOptions, enableGenerateManifests map[string]bool) error {\n-\t\trevision := source.TargetRevision\n-\t\tif q.GetRevision() != \"\" {\n-\t\t\trevision = q.GetRevision()\n-\t\t}\n-\t\tappInstanceLabelKey, err := s.settingsMgr.GetAppInstanceLabelKey()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app instance label key from settings: %w\", err)\n-\t\t}\n-\n-\t\tconfig, err := s.getApplicationClusterConfig(ctx, a)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t\t}\n-\n-\t\tserverVersion, err := s.kubectl.GetServerVersion(config)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting server version: %w\", err)\n-\t\t}\n-\n-\t\tapiResources, err := s.kubectl.GetAPIResources(config, false, kubecache.NewNoopSettings())\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting API resources: %w\", err)\n-\t\t}\n-\n-\t\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app project: %w\", err)\n-\t\t}\n-\n-\t\tmanifestInfo, err = client.GenerateManifest(ctx, &apiclient.ManifestRequest{\n-\t\t\tRepo:               repo,\n-\t\t\tRevision:           revision,\n-\t\t\tAppLabelKey:        appInstanceLabelKey,\n-\t\t\tAppName:            a.InstanceName(s.ns),\n-\t\t\tNamespace:          a.Spec.Destination.Namespace,\n-\t\t\tApplicationSource:  &source,\n-\t\t\tRepos:              helmRepos,\n-\t\t\tKustomizeOptions:   kustomizeOptions,\n-\t\t\tKubeVersion:        serverVersion,\n-\t\t\tApiVersions:        argo.APIResourcesToStrings(apiResources, true),\n-\t\t\tHelmRepoCreds:      helmCreds,\n-\t\t\tHelmOptions:        helmOptions,\n-\t\t\tTrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n-\t\t\tEnabledSourceTypes: enableGenerateManifests,\n-\t\t\tProjectName:        proj.Name,\n-\t\t\tProjectSourceRepos: proj.Spec.SourceRepos,\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error generating manifests: %w\", err)\n-\t\t}\n-\t\treturn nil\n-\t})\n-\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor i, manifest := range manifestInfo.Manifests {\n-\t\tobj := &unstructured.Unstructured{}\n-\t\terr = json.Unmarshal([]byte(manifest), obj)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error unmarshaling manifest into unstructured: %w\", err)\n-\t\t}\n-\t\tif obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n-\t\t\tobj, _, err = diff.HideSecretData(obj, nil)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error hiding secret data: %w\", err)\n-\t\t\t}\n-\t\t\tdata, err := json.Marshal(obj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error marshaling manifest: %w\", err)\n-\t\t\t}\n-\t\t\tmanifestInfo.Manifests[i] = string(data)\n-\t\t}\n-\t}\n-\n-\treturn manifestInfo, nil\n+        if q.Name == nil || *q.Name == \"\" {\n+                return nil, fmt.Errorf(\"invalid request: application name is missing\")\n+        }\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        source := a.Spec.GetSource()\n+\n+        if !s.isNamespaceEnabled(a.Namespace) {\n+                return nil, security.NamespaceNotPermittedError(a.Namespace)\n+        }\n+\n+        var manifestInfo *apiclient.ManifestResponse\n+        err = s.queryRepoServer(ctx, a, func(\n+                client apiclient.RepoServerServiceClient, repo *appv1.Repository, helmRepos []*appv1.Repository, helmCreds []*appv1.RepoCreds, helmOptions *appv1.HelmOptions, kustomizeOptions *appv1.KustomizeOptions, enableGenerateManifests map[string]bool) error {\n+                revision := source.TargetRevision\n+                if q.GetRevision() != \"\" {\n+                        revision = q.GetRevision()\n+                }\n+                appInstanceLabelKey, err := s.settingsMgr.GetAppInstanceLabelKey()\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app instance label key from settings: %w\", err)\n+                }\n+\n+                config, err := s.getApplicationClusterConfig(ctx, a)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting application cluster config: %w\", err)\n+                }\n+\n+                serverVersion, err := s.kubectl.GetServerVersion(config)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting server version: %w\", err)\n+                }\n+\n+                apiResources, err := s.kubectl.GetAPIResources(config, false, kubecache.NewNoopSettings())\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting API resources: %w\", err)\n+                }\n+\n+                proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app project: %w\", err)\n+                }\n+\n+                manifestInfo, err = client.GenerateManifest(ctx, &apiclient.ManifestRequest{\n+                        Repo:               repo,\n+                        Revision:           revision,\n+                        AppLabelKey:        appInstanceLabelKey,\n+                        AppName:            a.InstanceName(s.ns),\n+                        Namespace:          a.Spec.Destination.Namespace,\n+                        ApplicationSource:  &source,\n+                        Repos:              helmRepos,\n+                        KustomizeOptions:   kustomizeOptions,\n+                        KubeVersion:        serverVersion,\n+                        ApiVersions:        argo.APIResourcesToStrings(apiResources, true),\n+                        HelmRepoCreds:      helmCreds,\n+                        HelmOptions:        helmOptions,\n+                        TrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n+                        EnabledSourceTypes: enableGenerateManifests,\n+                        ProjectName:        proj.Name,\n+                        ProjectSourceRepos: proj.Spec.SourceRepos,\n+                })\n+                if err != nil {\n+                        return fmt.Errorf(\"error generating manifests: %w\", err)\n+                }\n+                return nil\n+        })\n+\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for i, manifest := range manifestInfo.Manifests {\n+                obj := &unstructured.Unstructured{}\n+                err = json.Unmarshal([]byte(manifest), obj)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error unmarshaling manifest into unstructured: %w\", err)\n+                }\n+                if obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n+                        obj, _, err = diff.HideSecretData(obj, nil)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error hiding secret data: %w\", err)\n+                        }\n+                        data, err := json.Marshal(obj)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error marshaling manifest: %w\", err)\n+                        }\n+                        manifestInfo.Manifests[i] = string(data)\n+                }\n+        }\n+\n+        return manifestInfo, nil\n }\n \n func (s *Server) GetManifestsWithFiles(stream application.ApplicationService_GetManifestsWithFilesServer) error {\n-\tctx := stream.Context()\n-\tquery, err := manifeststream.ReceiveApplicationManifestQueryWithFiles(stream)\n-\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting query: %w\", err)\n-\t}\n-\n-\tif query.Name == nil || *query.Name == \"\" {\n-\t\treturn fmt.Errorf(\"invalid request: application name is missing\")\n-\t}\n-\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, query.GetProject(), query.GetAppNamespace(), query.GetName())\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tvar manifestInfo *apiclient.ManifestResponse\n-\terr = s.queryRepoServer(ctx, a, func(\n-\t\tclient apiclient.RepoServerServiceClient, repo *appv1.Repository, helmRepos []*appv1.Repository, helmCreds []*appv1.RepoCreds, helmOptions *appv1.HelmOptions, kustomizeOptions *appv1.KustomizeOptions, enableGenerateManifests map[string]bool) error {\n-\n-\t\tappInstanceLabelKey, err := s.settingsMgr.GetAppInstanceLabelKey()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app instance label key from settings: %w\", err)\n-\t\t}\n-\n-\t\tconfig, err := s.getApplicationClusterConfig(ctx, a)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t\t}\n-\n-\t\tserverVersion, err := s.kubectl.GetServerVersion(config)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting server version: %w\", err)\n-\t\t}\n-\n-\t\tapiResources, err := s.kubectl.GetAPIResources(config, false, kubecache.NewNoopSettings())\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting API resources: %w\", err)\n-\t\t}\n-\n-\t\tsource := a.Spec.GetSource()\n-\n-\t\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app project: %w\", err)\n-\t\t}\n-\n-\t\treq := &apiclient.ManifestRequest{\n-\t\t\tRepo:               repo,\n-\t\t\tRevision:           source.TargetRevision,\n-\t\t\tAppLabelKey:        appInstanceLabelKey,\n-\t\t\tAppName:            a.Name,\n-\t\t\tNamespace:          a.Spec.Destination.Namespace,\n-\t\t\tApplicationSource:  &source,\n-\t\t\tRepos:              helmRepos,\n-\t\t\tKustomizeOptions:   kustomizeOptions,\n-\t\t\tKubeVersion:        serverVersion,\n-\t\t\tApiVersions:        argo.APIResourcesToStrings(apiResources, true),\n-\t\t\tHelmRepoCreds:      helmCreds,\n-\t\t\tHelmOptions:        helmOptions,\n-\t\t\tTrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n-\t\t\tEnabledSourceTypes: enableGenerateManifests,\n-\t\t\tProjectName:        proj.Name,\n-\t\t\tProjectSourceRepos: proj.Spec.SourceRepos,\n-\t\t}\n-\n-\t\trepoStreamClient, err := client.GenerateManifestWithFiles(stream.Context())\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error opening stream: %w\", err)\n-\t\t}\n-\n-\t\terr = manifeststream.SendRepoStream(repoStreamClient, stream, req, *query.Checksum)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error sending repo stream: %w\", err)\n-\t\t}\n-\n-\t\tresp, err := repoStreamClient.CloseAndRecv()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error generating manifests: %w\", err)\n-\t\t}\n-\n-\t\tmanifestInfo = resp\n-\t\treturn nil\n-\t})\n-\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tfor i, manifest := range manifestInfo.Manifests {\n-\t\tobj := &unstructured.Unstructured{}\n-\t\terr = json.Unmarshal([]byte(manifest), obj)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error unmarshaling manifest into unstructured: %w\", err)\n-\t\t}\n-\t\tif obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n-\t\t\tobj, _, err = diff.HideSecretData(obj, nil)\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error hiding secret data: %w\", err)\n-\t\t\t}\n-\t\t\tdata, err := json.Marshal(obj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error marshaling manifest: %w\", err)\n-\t\t\t}\n-\t\t\tmanifestInfo.Manifests[i] = string(data)\n-\t\t}\n-\t}\n-\n-\tstream.SendAndClose(manifestInfo)\n-\treturn nil\n+        ctx := stream.Context()\n+        query, err := manifeststream.ReceiveApplicationManifestQueryWithFiles(stream)\n+\n+        if err != nil {\n+                return fmt.Errorf(\"error getting query: %w\", err)\n+        }\n+\n+        if query.Name == nil || *query.Name == \"\" {\n+                return fmt.Errorf(\"invalid request: application name is missing\")\n+        }\n+\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, query.GetProject(), query.GetAppNamespace(), query.GetName())\n+        if err != nil {\n+                return err\n+        }\n+\n+        var manifestInfo *apiclient.ManifestResponse\n+        err = s.queryRepoServer(ctx, a, func(\n+                client apiclient.RepoServerServiceClient, repo *appv1.Repository, helmRepos []*appv1.Repository, helmCreds []*appv1.RepoCreds, helmOptions *appv1.HelmOptions, kustomizeOptions *appv1.KustomizeOptions, enableGenerateManifests map[string]bool) error {\n+\n+                appInstanceLabelKey, err := s.settingsMgr.GetAppInstanceLabelKey()\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app instance label key from settings: %w\", err)\n+                }\n+\n+                config, err := s.getApplicationClusterConfig(ctx, a)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting application cluster config: %w\", err)\n+                }\n+\n+                serverVersion, err := s.kubectl.GetServerVersion(config)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting server version: %w\", err)\n+                }\n+\n+                apiResources, err := s.kubectl.GetAPIResources(config, false, kubecache.NewNoopSettings())\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting API resources: %w\", err)\n+                }\n+\n+                source := a.Spec.GetSource()\n+\n+                proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app project: %w\", err)\n+                }\n+\n+                req := &apiclient.ManifestRequest{\n+                        Repo:               repo,\n+                        Revision:           source.TargetRevision,\n+                        AppLabelKey:        appInstanceLabelKey,\n+                        AppName:            a.Name,\n+                        Namespace:          a.Spec.Destination.Namespace,\n+                        ApplicationSource:  &source,\n+                        Repos:              helmRepos,\n+                        KustomizeOptions:   kustomizeOptions,\n+                        KubeVersion:        serverVersion,\n+                        ApiVersions:        argo.APIResourcesToStrings(apiResources, true),\n+                        HelmRepoCreds:      helmCreds,\n+                        HelmOptions:        helmOptions,\n+                        TrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n+                        EnabledSourceTypes: enableGenerateManifests,\n+                        ProjectName:        proj.Name,\n+                        ProjectSourceRepos: proj.Spec.SourceRepos,\n+                }\n+\n+                repoStreamClient, err := client.GenerateManifestWithFiles(stream.Context())\n+                if err != nil {\n+                        return fmt.Errorf(\"error opening stream: %w\", err)\n+                }\n+\n+                err = manifeststream.SendRepoStream(repoStreamClient, stream, req, *query.Checksum)\n+                if err != nil {\n+                        return fmt.Errorf(\"error sending repo stream: %w\", err)\n+                }\n+\n+                resp, err := repoStreamClient.CloseAndRecv()\n+                if err != nil {\n+                        return fmt.Errorf(\"error generating manifests: %w\", err)\n+                }\n+\n+                manifestInfo = resp\n+                return nil\n+        })\n+\n+        if err != nil {\n+                return err\n+        }\n+\n+        for i, manifest := range manifestInfo.Manifests {\n+                obj := &unstructured.Unstructured{}\n+                err = json.Unmarshal([]byte(manifest), obj)\n+                if err != nil {\n+                        return fmt.Errorf(\"error unmarshaling manifest into unstructured: %w\", err)\n+                }\n+                if obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n+                        obj, _, err = diff.HideSecretData(obj, nil)\n+                        if err != nil {\n+                                return fmt.Errorf(\"error hiding secret data: %w\", err)\n+                        }\n+                        data, err := json.Marshal(obj)\n+                        if err != nil {\n+                                return fmt.Errorf(\"error marshaling manifest: %w\", err)\n+                        }\n+                        manifestInfo.Manifests[i] = string(data)\n+                }\n+        }\n+\n+        stream.SendAndClose(manifestInfo)\n+        return nil\n }\n \n // Get returns an application by name\n func (s *Server) Get(ctx context.Context, q *application.ApplicationQuery) (*appv1.Application, error) {\n-\tappName := q.GetName()\n-\tappNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n-\n-\tproject := \"\"\n-\tprojects := getProjectsFromApplicationQuery(*q)\n-\tif len(projects) == 1 {\n-\t\tproject = projects[0]\n-\t} else if len(projects) > 1 {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"multiple projects specified - the get endpoint accepts either zero or one project\")\n-\t}\n-\n-\t// We must use a client Get instead of an informer Get, because it's common to call Get immediately\n-\t// following a Watch (which is not yet powered by an informer), and the Get must reflect what was\n-\t// previously seen by the client.\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, project, appNs, appName, q.GetResourceVersion())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.inferResourcesStatusHealth(a)\n-\n-\tif q.Refresh == nil {\n-\t\treturn a, nil\n-\t}\n-\n-\trefreshType := appv1.RefreshTypeNormal\n-\tif *q.Refresh == string(appv1.RefreshTypeHard) {\n-\t\trefreshType = appv1.RefreshTypeHard\n-\t}\n-\tappIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n-\n-\t// subscribe early with buffered channel to ensure we don't miss events\n-\tevents := make(chan *appv1.ApplicationWatchEvent, watchAPIBufferSize)\n-\tunsubscribe := s.appBroadcaster.Subscribe(events, func(event *appv1.ApplicationWatchEvent) bool {\n-\t\treturn event.Application.Name == appName && event.Application.Namespace == appNs\n-\t})\n-\tdefer unsubscribe()\n-\n-\tapp, err := argoutil.RefreshApp(appIf, appName, refreshType)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error refreshing the app: %w\", err)\n-\t}\n-\n-\tif refreshType == appv1.RefreshTypeHard {\n-\t\t// force refresh cached application details\n-\t\tif err := s.queryRepoServer(ctx, a, func(\n-\t\t\tclient apiclient.RepoServerServiceClient,\n-\t\t\trepo *appv1.Repository,\n-\t\t\thelmRepos []*appv1.Repository,\n-\t\t\t_ []*appv1.RepoCreds,\n-\t\t\thelmOptions *appv1.HelmOptions,\n-\t\t\tkustomizeOptions *appv1.KustomizeOptions,\n-\t\t\tenabledSourceTypes map[string]bool,\n-\t\t) error {\n-\t\t\tsource := app.Spec.GetSource()\n-\t\t\t_, err := client.GetAppDetails(ctx, &apiclient.RepoServerAppDetailsQuery{\n-\t\t\t\tRepo:               repo,\n-\t\t\t\tSource:             &source,\n-\t\t\t\tAppName:            appName,\n-\t\t\t\tKustomizeOptions:   kustomizeOptions,\n-\t\t\t\tRepos:              helmRepos,\n-\t\t\t\tNoCache:            true,\n-\t\t\t\tTrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n-\t\t\t\tEnabledSourceTypes: enabledSourceTypes,\n-\t\t\t\tHelmOptions:        helmOptions,\n-\t\t\t})\n-\t\t\treturn err\n-\t\t}); err != nil {\n-\t\t\tlog.Warnf(\"Failed to force refresh application details: %v\", err)\n-\t\t}\n-\t}\n-\n-\tminVersion := 0\n-\tif minVersion, err = strconv.Atoi(app.ResourceVersion); err != nil {\n-\t\tminVersion = 0\n-\t}\n-\n-\tfor {\n-\t\tselect {\n-\t\tcase <-ctx.Done():\n-\t\t\treturn nil, fmt.Errorf(\"application refresh deadline exceeded\")\n-\t\tcase event := <-events:\n-\t\t\tif appVersion, err := strconv.Atoi(event.Application.ResourceVersion); err == nil && appVersion > minVersion {\n-\t\t\t\tannotations := event.Application.GetAnnotations()\n-\t\t\t\tif annotations == nil {\n-\t\t\t\t\tannotations = make(map[string]string)\n-\t\t\t\t}\n-\t\t\t\tif _, ok := annotations[appv1.AnnotationKeyRefresh]; !ok {\n-\t\t\t\t\treturn &event.Application, nil\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n+        appName := q.GetName()\n+        appNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n+\n+        project := \"\"\n+        projects := getProjectsFromApplicationQuery(*q)\n+        if len(projects) == 1 {\n+                project = projects[0]\n+        } else if len(projects) > 1 {\n+                return nil, status.Errorf(codes.InvalidArgument, \"multiple projects specified - the get endpoint accepts either zero or one project\")\n+        }\n+\n+        // We must use a client Get instead of an informer Get, because it's common to call Get immediately\n+        // following a Watch (which is not yet powered by an informer), and the Get must reflect what was\n+        // previously seen by the client.\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, project, appNs, appName, q.GetResourceVersion())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        s.inferResourcesStatusHealth(a)\n+\n+        if q.Refresh == nil {\n+                return a, nil\n+        }\n+\n+        refreshType := appv1.RefreshTypeNormal\n+        if *q.Refresh == string(appv1.RefreshTypeHard) {\n+                refreshType = appv1.RefreshTypeHard\n+        }\n+        appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n+\n+        // subscribe early with buffered channel to ensure we don't miss events\n+        events := make(chan *appv1.ApplicationWatchEvent, watchAPIBufferSize)\n+        unsubscribe := s.appBroadcaster.Subscribe(events, func(event *appv1.ApplicationWatchEvent) bool {\n+                return event.Application.Name == appName && event.Application.Namespace == appNs\n+        })\n+        defer unsubscribe()\n+\n+        app, err := argoutil.RefreshApp(appIf, appName, refreshType)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error refreshing the app: %w\", err)\n+        }\n+\n+        if refreshType == appv1.RefreshTypeHard {\n+                // force refresh cached application details\n+                if err := s.queryRepoServer(ctx, a, func(\n+                        client apiclient.RepoServerServiceClient,\n+                        repo *appv1.Repository,\n+                        helmRepos []*appv1.Repository,\n+                        _ []*appv1.RepoCreds,\n+                        helmOptions *appv1.HelmOptions,\n+                        kustomizeOptions *appv1.KustomizeOptions,\n+                        enabledSourceTypes map[string]bool,\n+                ) error {\n+                        source := app.Spec.GetSource()\n+                        _, err := client.GetAppDetails(ctx, &apiclient.RepoServerAppDetailsQuery{\n+                                Repo:               repo,\n+                                Source:             &source,\n+                                AppName:            appName,\n+                                KustomizeOptions:   kustomizeOptions,\n+                                Repos:              helmRepos,\n+                                NoCache:            true,\n+                                TrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n+                                EnabledSourceTypes: enabledSourceTypes,\n+                                HelmOptions:        helmOptions,\n+                        })\n+                        return err\n+                }); err != nil {\n+                        log.Warnf(\"Failed to force refresh application details: %v\", err)\n+                }\n+        }\n+\n+        minVersion := 0\n+        if minVersion, err = strconv.Atoi(app.ResourceVersion); err != nil {\n+                minVersion = 0\n+        }\n+\n+        for {\n+                select {\n+                case <-ctx.Done():\n+                        return nil, fmt.Errorf(\"application refresh deadline exceeded\")\n+                case event := <-events:\n+                        if appVersion, err := strconv.Atoi(event.Application.ResourceVersion); err == nil && appVersion > minVersion {\n+                                annotations := event.Application.GetAnnotations()\n+                                if annotations == nil {\n+                                        annotations = make(map[string]string)\n+                                }\n+                                if _, ok := annotations[appv1.AnnotationKeyRefresh]; !ok {\n+                                        return &event.Application, nil\n+                                }\n+                        }\n+                }\n+        }\n }\n \n // ListResourceEvents returns a list of event resources\n func (s *Server) ListResourceEvents(ctx context.Context, q *application.ApplicationResourceEventsQuery) (*v1.EventList, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar (\n-\t\tkubeClientset kubernetes.Interface\n-\t\tfieldSelector string\n-\t\tnamespace     string\n-\t)\n-\t// There are two places where we get events. If we are getting application events, we query\n-\t// our own cluster. If it is events on a resource on an external cluster, then we query the\n-\t// external cluster using its rest.Config\n-\tif q.GetResourceName() == \"\" && q.GetResourceUID() == \"\" {\n-\t\tkubeClientset = s.kubeclientset\n-\t\tnamespace = a.Namespace\n-\t\tfieldSelector = fields.SelectorFromSet(map[string]string{\n-\t\t\t\"involvedObject.name\":      a.Name,\n-\t\t\t\"involvedObject.uid\":       string(a.UID),\n-\t\t\t\"involvedObject.namespace\": a.Namespace,\n-\t\t}).String()\n-\t} else {\n-\t\ttree, err := s.getAppResources(ctx, a)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error getting app resources: %w\", err)\n-\t\t}\n-\t\tfound := false\n-\t\tfor _, n := range append(tree.Nodes, tree.OrphanedNodes...) {\n-\t\t\tif n.ResourceRef.UID == q.GetResourceUID() && n.ResourceRef.Name == q.GetResourceName() && n.ResourceRef.Namespace == q.GetResourceNamespace() {\n-\t\t\t\tfound = true\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif !found {\n-\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"%s not found as part of application %s\", q.GetResourceName(), q.GetName())\n-\t\t}\n-\n-\t\tnamespace = q.GetResourceNamespace()\n-\t\tvar config *rest.Config\n-\t\tconfig, err = s.getApplicationClusterConfig(ctx, a)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t\t}\n-\t\tkubeClientset, err = kubernetes.NewForConfig(config)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error creating kube client: %w\", err)\n-\t\t}\n-\t\tfieldSelector = fields.SelectorFromSet(map[string]string{\n-\t\t\t\"involvedObject.name\":      q.GetResourceName(),\n-\t\t\t\"involvedObject.uid\":       q.GetResourceUID(),\n-\t\t\t\"involvedObject.namespace\": namespace,\n-\t\t}).String()\n-\t}\n-\tlog.Infof(\"Querying for resource events with field selector: %s\", fieldSelector)\n-\topts := metav1.ListOptions{FieldSelector: fieldSelector}\n-\tlist, err := kubeClientset.CoreV1().Events(namespace).List(ctx, opts)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error listing resource events: %w\", err)\n-\t}\n-\treturn list, nil\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var (\n+                kubeClientset kubernetes.Interface\n+                fieldSelector string\n+                namespace     string\n+        )\n+        // There are two places where we get events. If we are getting application events, we query\n+        // our own cluster. If it is events on a resource on an external cluster, then we query the\n+        // external cluster using its rest.Config\n+        if q.GetResourceName() == \"\" && q.GetResourceUID() == \"\" {\n+                kubeClientset = s.kubeclientset\n+                namespace = a.Namespace\n+                fieldSelector = fields.SelectorFromSet(map[string]string{\n+                        \"involvedObject.name\":      a.Name,\n+                        \"involvedObject.uid\":       string(a.UID),\n+                        \"involvedObject.namespace\": a.Namespace,\n+                }).String()\n+        } else {\n+                tree, err := s.getAppResources(ctx, a)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error getting app resources: %w\", err)\n+                }\n+                found := false\n+                for _, n := range append(tree.Nodes, tree.OrphanedNodes...) {\n+                        if n.ResourceRef.UID == q.GetResourceUID() && n.ResourceRef.Name == q.GetResourceName() && n.ResourceRef.Namespace == q.GetResourceNamespace() {\n+                                found = true\n+                                break\n+                        }\n+                }\n+                if !found {\n+                        return nil, status.Errorf(codes.InvalidArgument, \"%s not found as part of application %s\", q.GetResourceName(), q.GetName())\n+                }\n+\n+                namespace = q.GetResourceNamespace()\n+                var config *rest.Config\n+                config, err = s.getApplicationClusterConfig(ctx, a)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n+                }\n+                kubeClientset, err = kubernetes.NewForConfig(config)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error creating kube client: %w\", err)\n+                }\n+                fieldSelector = fields.SelectorFromSet(map[string]string{\n+                        \"involvedObject.name\":      q.GetResourceName(),\n+                        \"involvedObject.uid\":       q.GetResourceUID(),\n+                        \"involvedObject.namespace\": namespace,\n+                }).String()\n+        }\n+        log.Infof(\"Querying for resource events with field selector: %s\", fieldSelector)\n+        opts := metav1.ListOptions{FieldSelector: fieldSelector}\n+        list, err := kubeClientset.CoreV1().Events(namespace).List(ctx, opts)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error listing resource events: %w\", err)\n+        }\n+        return list, nil\n }\n \n // validateAndUpdateApp validates and updates the application. currentProject is the name of the project the app\n // currently is under. If not specified, we assume that the app is under the project specified in the app spec.\n func (s *Server) validateAndUpdateApp(ctx context.Context, newApp *appv1.Application, merge bool, validate bool, action string, currentProject string) (*appv1.Application, error) {\n-\ts.projectLock.RLock(newApp.Spec.GetProject())\n-\tdefer s.projectLock.RUnlock(newApp.Spec.GetProject())\n-\n-\tapp, err := s.getApplicationEnforceRBACClient(ctx, action, currentProject, newApp.Namespace, newApp.Name, \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = s.validateAndNormalizeApp(ctx, newApp, validate)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error validating and normalizing app: %w\", err)\n-\t}\n-\n-\ta, err := s.updateApp(app, newApp, ctx, merge)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error updating application: %w\", err)\n-\t}\n-\treturn a, nil\n+        s.projectLock.RLock(newApp.Spec.GetProject())\n+        defer s.projectLock.RUnlock(newApp.Spec.GetProject())\n+\n+        app, err := s.getApplicationEnforceRBACClient(ctx, action, currentProject, newApp.Namespace, newApp.Name, \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        err = s.validateAndNormalizeApp(ctx, newApp, validate)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error validating and normalizing app: %w\", err)\n+        }\n+\n+        a, err := s.updateApp(app, newApp, ctx, merge)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error updating application: %w\", err)\n+        }\n+        return a, nil\n }\n \n var informerSyncTimeout = 2 * time.Second\n@@ -837,1636 +847,1636 @@ var informerSyncTimeout = 2 * time.Second\n // after a mutating API call (create/update). This function should be called after a creates &\n // update to give a probable (but not guaranteed) chance of being up-to-date after the create/update.\n func (s *Server) waitSync(app *appv1.Application) {\n-\tlogCtx := log.WithField(\"application\", app.Name)\n-\tdeadline := time.Now().Add(informerSyncTimeout)\n-\tminVersion, err := strconv.Atoi(app.ResourceVersion)\n-\tif err != nil {\n-\t\tlogCtx.Warnf(\"waitSync failed: could not parse resource version %s\", app.ResourceVersion)\n-\t\ttime.Sleep(50 * time.Millisecond) // sleep anyway\n-\t\treturn\n-\t}\n-\tfor {\n-\t\tif currApp, err := s.appLister.Applications(app.Namespace).Get(app.Name); err == nil {\n-\t\t\tcurrVersion, err := strconv.Atoi(currApp.ResourceVersion)\n-\t\t\tif err == nil && currVersion >= minVersion {\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\tif time.Now().After(deadline) {\n-\t\t\tbreak\n-\t\t}\n-\t\ttime.Sleep(20 * time.Millisecond)\n-\t}\n-\tlogCtx.Warnf(\"waitSync failed: timed out\")\n+        logCtx := log.WithField(\"application\", app.Name)\n+        deadline := time.Now().Add(informerSyncTimeout)\n+        minVersion, err := strconv.Atoi(app.ResourceVersion)\n+        if err != nil {\n+                logCtx.Warnf(\"waitSync failed: could not parse resource version %s\", app.ResourceVersion)\n+                time.Sleep(50 * time.Millisecond) // sleep anyway\n+                return\n+        }\n+        for {\n+                if currApp, err := s.appLister.Applications(app.Namespace).Get(app.Name); err == nil {\n+                        currVersion, err := strconv.Atoi(currApp.ResourceVersion)\n+                        if err == nil && currVersion >= minVersion {\n+                                return\n+                        }\n+                }\n+                if time.Now().After(deadline) {\n+                        break\n+                }\n+                time.Sleep(20 * time.Millisecond)\n+        }\n+        logCtx.Warnf(\"waitSync failed: timed out\")\n }\n \n func (s *Server) updateApp(app *appv1.Application, newApp *appv1.Application, ctx context.Context, merge bool) (*appv1.Application, error) {\n-\tfor i := 0; i < 10; i++ {\n-\t\tapp.Spec = newApp.Spec\n-\t\tif merge {\n-\t\t\tapp.Labels = collections.MergeStringMaps(app.Labels, newApp.Labels)\n-\t\t\tapp.Annotations = collections.MergeStringMaps(app.Annotations, newApp.Annotations)\n-\t\t} else {\n-\t\t\tapp.Labels = newApp.Labels\n-\t\t\tapp.Annotations = newApp.Annotations\n-\t\t}\n-\n-\t\tapp.Finalizers = newApp.Finalizers\n-\n-\t\tres, err := s.appclientset.ArgoprojV1alpha1().Applications(app.Namespace).Update(ctx, app, metav1.UpdateOptions{})\n-\t\tif err == nil {\n-\t\t\ts.logAppEvent(app, ctx, argo.EventReasonResourceUpdated, \"updated application spec\")\n-\t\t\ts.waitSync(res)\n-\t\t\treturn res, nil\n-\t\t}\n-\t\tif !apierr.IsConflict(err) {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tapp, err = s.appclientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(ctx, newApp.Name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error getting application: %w\", err)\n-\t\t}\n-\t\ts.inferResourcesStatusHealth(app)\n-\t}\n-\treturn nil, status.Errorf(codes.Internal, \"Failed to update application. Too many conflicts\")\n+        for i := 0; i < 10; i++ {\n+                app.Spec = newApp.Spec\n+                if merge {\n+                        app.Labels = collections.MergeStringMaps(app.Labels, newApp.Labels)\n+                        app.Annotations = collections.MergeStringMaps(app.Annotations, newApp.Annotations)\n+                } else {\n+                        app.Labels = newApp.Labels\n+                        app.Annotations = newApp.Annotations\n+                }\n+\n+                app.Finalizers = newApp.Finalizers\n+\n+                res, err := s.appclientset.ArgoprojV1alpha1().Applications(app.Namespace).Update(ctx, app, metav1.UpdateOptions{})\n+                if err == nil {\n+                        s.logAppEvent(app, ctx, argo.EventReasonResourceUpdated, \"updated application spec\")\n+                        s.waitSync(res)\n+                        return res, nil\n+                }\n+                if !apierr.IsConflict(err) {\n+                        return nil, err\n+                }\n+\n+                app, err = s.appclientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(ctx, newApp.Name, metav1.GetOptions{})\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error getting application: %w\", err)\n+                }\n+                s.inferResourcesStatusHealth(app)\n+        }\n+        return nil, status.Errorf(codes.Internal, \"Failed to update application. Too many conflicts\")\n }\n \n // Update updates an application\n func (s *Server) Update(ctx context.Context, q *application.ApplicationUpdateRequest) (*appv1.Application, error) {\n-\tif q.GetApplication() == nil {\n-\t\treturn nil, fmt.Errorf(\"error updating application: application is nil in request\")\n-\t}\n-\ta := q.GetApplication()\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvalidate := true\n-\tif q.Validate != nil {\n-\t\tvalidate = *q.Validate\n-\t}\n-\treturn s.validateAndUpdateApp(ctx, q.Application, false, validate, rbacpolicy.ActionUpdate, q.GetProject())\n+        if q.GetApplication() == nil {\n+                return nil, fmt.Errorf(\"error updating application: application is nil in request\")\n+        }\n+        a := q.GetApplication()\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        validate := true\n+        if q.Validate != nil {\n+                validate = *q.Validate\n+        }\n+        return s.validateAndUpdateApp(ctx, q.Application, false, validate, rbacpolicy.ActionUpdate, q.GetProject())\n }\n \n // UpdateSpec updates an application spec and filters out any invalid parameter overrides\n func (s *Server) UpdateSpec(ctx context.Context, q *application.ApplicationUpdateSpecRequest) (*appv1.ApplicationSpec, error) {\n-\tif q.GetSpec() == nil {\n-\t\treturn nil, fmt.Errorf(\"error updating application spec: spec is nil in request\")\n-\t}\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionUpdate, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ta.Spec = *q.GetSpec()\n-\tvalidate := true\n-\tif q.Validate != nil {\n-\t\tvalidate = *q.Validate\n-\t}\n-\ta, err = s.validateAndUpdateApp(ctx, a, false, validate, rbacpolicy.ActionUpdate, q.GetProject())\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error validating and updating app: %w\", err)\n-\t}\n-\treturn &a.Spec, nil\n+        if q.GetSpec() == nil {\n+                return nil, fmt.Errorf(\"error updating application spec: spec is nil in request\")\n+        }\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionUpdate, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        a.Spec = *q.GetSpec()\n+        validate := true\n+        if q.Validate != nil {\n+                validate = *q.Validate\n+        }\n+        a, err = s.validateAndUpdateApp(ctx, a, false, validate, rbacpolicy.ActionUpdate, q.GetProject())\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error validating and updating app: %w\", err)\n+        }\n+        return &a.Spec, nil\n }\n \n // Patch patches an application\n func (s *Server) Patch(ctx context.Context, q *application.ApplicationPatchRequest) (*appv1.Application, error) {\n-\tapp, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err = s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, app.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tjsonApp, err := json.Marshal(app)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error marshaling application: %w\", err)\n-\t}\n-\n-\tvar patchApp []byte\n-\n-\tswitch q.GetPatchType() {\n-\tcase \"json\", \"\":\n-\t\tpatch, err := jsonpatch.DecodePatch([]byte(q.GetPatch()))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error decoding json patch: %w\", err)\n-\t\t}\n-\t\tpatchApp, err = patch.Apply(jsonApp)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error applying patch: %w\", err)\n-\t\t}\n-\tcase \"merge\":\n-\t\tpatchApp, err = jsonpatch.MergePatch(jsonApp, []byte(q.GetPatch()))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error calculating merge patch: %w\", err)\n-\t\t}\n-\tdefault:\n-\t\treturn nil, status.Error(codes.InvalidArgument, fmt.Sprintf(\"Patch type '%s' is not supported\", q.GetPatchType()))\n-\t}\n-\n-\tnewApp := &appv1.Application{}\n-\terr = json.Unmarshal(patchApp, newApp)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error unmarshaling patched app: %w\", err)\n-\t}\n-\treturn s.validateAndUpdateApp(ctx, newApp, false, true, rbacpolicy.ActionUpdate, q.GetProject())\n+        app, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if err = s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, app.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        jsonApp, err := json.Marshal(app)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error marshaling application: %w\", err)\n+        }\n+\n+        var patchApp []byte\n+\n+        switch q.GetPatchType() {\n+        case \"json\", \"\":\n+                patch, err := jsonpatch.DecodePatch([]byte(q.GetPatch()))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error decoding json patch: %w\", err)\n+                }\n+                patchApp, err = patch.Apply(jsonApp)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error applying patch: %w\", err)\n+                }\n+        case \"merge\":\n+                patchApp, err = jsonpatch.MergePatch(jsonApp, []byte(q.GetPatch()))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error calculating merge patch: %w\", err)\n+                }\n+        default:\n+                return nil, status.Error(codes.InvalidArgument, fmt.Sprintf(\"Patch type '%s' is not supported\", q.GetPatchType()))\n+        }\n+\n+        newApp := &appv1.Application{}\n+        err = json.Unmarshal(patchApp, newApp)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error unmarshaling patched app: %w\", err)\n+        }\n+        return s.validateAndUpdateApp(ctx, newApp, false, true, rbacpolicy.ActionUpdate, q.GetProject())\n }\n \n // Delete removes an application and all associated resources\n func (s *Server) Delete(ctx context.Context, q *application.ApplicationDeleteRequest) (*application.ApplicationResponse, error) {\n-\tappName := q.GetName()\n-\tappNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), appNs, appName, \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.projectLock.RLock(a.Spec.Project)\n-\tdefer s.projectLock.RUnlock(a.Spec.Project)\n-\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionDelete, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif q.Cascade != nil && !*q.Cascade && q.GetPropagationPolicy() != \"\" {\n-\t\treturn nil, status.Error(codes.InvalidArgument, \"cannot set propagation policy when cascading is disabled\")\n-\t}\n-\n-\tpatchFinalizer := false\n-\tif q.Cascade == nil || *q.Cascade {\n-\t\t// validate the propgation policy\n-\t\tpolicyFinalizer := getPropagationPolicyFinalizer(q.GetPropagationPolicy())\n-\t\tif policyFinalizer == \"\" {\n-\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"invalid propagation policy: %s\", *q.PropagationPolicy)\n-\t\t}\n-\t\tif !a.IsFinalizerPresent(policyFinalizer) {\n-\t\t\ta.SetCascadedDeletion(policyFinalizer)\n-\t\t\tpatchFinalizer = true\n-\t\t}\n-\t} else {\n-\t\tif a.CascadedDeletion() {\n-\t\t\ta.UnSetCascadedDeletion()\n-\t\t\tpatchFinalizer = true\n-\t\t}\n-\t}\n-\n-\tif patchFinalizer {\n-\t\t// Although the cascaded deletion/propagation policy finalizer is not set when apps are created via\n-\t\t// API, they will often be set by the user as part of declarative config. As part of a delete\n-\t\t// request, we always calculate the patch to see if we need to set/unset the finalizer.\n-\t\tpatch, err := json.Marshal(map[string]interface{}{\n-\t\t\t\"metadata\": map[string]interface{}{\n-\t\t\t\t\"finalizers\": a.Finalizers,\n-\t\t\t},\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error marshaling finalizers: %w\", err)\n-\t\t}\n-\t\t_, err = s.appclientset.ArgoprojV1alpha1().Applications(a.Namespace).Patch(ctx, a.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error patching application with finalizers: %w\", err)\n-\t\t}\n-\t}\n-\n-\terr = s.appclientset.ArgoprojV1alpha1().Applications(appNs).Delete(ctx, appName, metav1.DeleteOptions{})\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error deleting application: %w\", err)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonResourceDeleted, \"deleted application\")\n-\treturn &application.ApplicationResponse{}, nil\n+        appName := q.GetName()\n+        appNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), appNs, appName, \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        s.projectLock.RLock(a.Spec.Project)\n+        defer s.projectLock.RUnlock(a.Spec.Project)\n+\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionDelete, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        if q.Cascade != nil && !*q.Cascade && q.GetPropagationPolicy() != \"\" {\n+                return nil, status.Error(codes.InvalidArgument, \"cannot set propagation policy when cascading is disabled\")\n+        }\n+\n+        patchFinalizer := false\n+        if q.Cascade == nil || *q.Cascade {\n+                // validate the propgation policy\n+                policyFinalizer := getPropagationPolicyFinalizer(q.GetPropagationPolicy())\n+                if policyFinalizer == \"\" {\n+                        return nil, status.Errorf(codes.InvalidArgument, \"invalid propagation policy: %s\", *q.PropagationPolicy)\n+                }\n+                if !a.IsFinalizerPresent(policyFinalizer) {\n+                        a.SetCascadedDeletion(policyFinalizer)\n+                        patchFinalizer = true\n+                }\n+        } else {\n+                if a.CascadedDeletion() {\n+                        a.UnSetCascadedDeletion()\n+                        patchFinalizer = true\n+                }\n+        }\n+\n+        if patchFinalizer {\n+                // Although the cascaded deletion/propagation policy finalizer is not set when apps are created via\n+                // API, they will often be set by the user as part of declarative config. As part of a delete\n+                // request, we always calculate the patch to see if we need to set/unset the finalizer.\n+                patch, err := json.Marshal(map[string]interface{}{\n+                        \"metadata\": map[string]interface{}{\n+                                \"finalizers\": a.Finalizers,\n+                        },\n+                })\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error marshaling finalizers: %w\", err)\n+                }\n+                _, err = s.appclientset.ArgoprojV1alpha1().Applications(a.Namespace).Patch(ctx, a.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error patching application with finalizers: %w\", err)\n+                }\n+        }\n+\n+        err = s.appclientset.ArgoprojV1alpha1().Applications(appNs).Delete(ctx, appName, metav1.DeleteOptions{})\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error deleting application: %w\", err)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonResourceDeleted, \"deleted application\")\n+        return &application.ApplicationResponse{}, nil\n }\n \n func (s *Server) isApplicationPermitted(selector labels.Selector, minVersion int, claims any, appName, appNs string, projects map[string]bool, a appv1.Application) bool {\n-\tif len(projects) > 0 && !projects[a.Spec.GetProject()] {\n-\t\treturn false\n-\t}\n-\n-\tif appVersion, err := strconv.Atoi(a.ResourceVersion); err == nil && appVersion < minVersion {\n-\t\treturn false\n-\t}\n-\tmatchedEvent := (appName == \"\" || (a.Name == appName && a.Namespace == appNs)) && selector.Matches(labels.Set(a.Labels))\n-\tif !matchedEvent {\n-\t\treturn false\n-\t}\n-\n-\tif !s.isNamespaceEnabled(a.Namespace) {\n-\t\treturn false\n-\t}\n-\n-\tif !s.enf.Enforce(claims, rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)) {\n-\t\t// do not emit apps user does not have accessing\n-\t\treturn false\n-\t}\n-\n-\treturn true\n+        if len(projects) > 0 && !projects[a.Spec.GetProject()] {\n+                return false\n+        }\n+\n+        if appVersion, err := strconv.Atoi(a.ResourceVersion); err == nil && appVersion < minVersion {\n+                return false\n+        }\n+        matchedEvent := (appName == \"\" || (a.Name == appName && a.Namespace == appNs)) && selector.Matches(labels.Set(a.Labels))\n+        if !matchedEvent {\n+                return false\n+        }\n+\n+        if !s.isNamespaceEnabled(a.Namespace) {\n+                return false\n+        }\n+\n+        if !s.enf.Enforce(claims, rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)) {\n+                // do not emit apps user does not have accessing\n+                return false\n+        }\n+\n+        return true\n }\n \n func (s *Server) Watch(q *application.ApplicationQuery, ws application.ApplicationService_WatchServer) error {\n-\tappName := q.GetName()\n-\tappNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n-\tlogCtx := log.NewEntry(log.New())\n-\tif q.Name != nil {\n-\t\tlogCtx = logCtx.WithField(\"application\", *q.Name)\n-\t}\n-\tprojects := map[string]bool{}\n-\tfor _, project := range getProjectsFromApplicationQuery(*q) {\n-\t\tprojects[project] = true\n-\t}\n-\tclaims := ws.Context().Value(\"claims\")\n-\tselector, err := labels.Parse(q.GetSelector())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error parsing labels with selectors: %w\", err)\n-\t}\n-\tminVersion := 0\n-\tif q.GetResourceVersion() != \"\" {\n-\t\tif minVersion, err = strconv.Atoi(q.GetResourceVersion()); err != nil {\n-\t\t\tminVersion = 0\n-\t\t}\n-\t}\n-\n-\t// sendIfPermitted is a helper to send the application to the client's streaming channel if the\n-\t// caller has RBAC privileges permissions to view it\n-\tsendIfPermitted := func(a appv1.Application, eventType watch.EventType) {\n-\t\tpermitted := s.isApplicationPermitted(selector, minVersion, claims, appName, appNs, projects, a)\n-\t\tif !permitted {\n-\t\t\treturn\n-\t\t}\n-\t\ts.inferResourcesStatusHealth(&a)\n-\t\terr := ws.Send(&appv1.ApplicationWatchEvent{\n-\t\t\tType:        eventType,\n-\t\t\tApplication: a,\n-\t\t})\n-\t\tif err != nil {\n-\t\t\tlogCtx.Warnf(\"Unable to send stream message: %v\", err)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tevents := make(chan *appv1.ApplicationWatchEvent, watchAPIBufferSize)\n-\t// Mimic watch API behavior: send ADDED events if no resource version provided\n-\t// If watch API is executed for one application when emit event even if resource version is provided\n-\t// This is required since single app watch API is used for during operations like app syncing and it is\n-\t// critical to never miss events.\n-\tif q.GetResourceVersion() == \"\" || q.GetName() != \"\" {\n-\t\tapps, err := s.appLister.List(selector)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error listing apps with selector: %w\", err)\n-\t\t}\n-\t\tsort.Slice(apps, func(i, j int) bool {\n-\t\t\treturn apps[i].QualifiedName() < apps[j].QualifiedName()\n-\t\t})\n-\t\tfor i := range apps {\n-\t\t\tsendIfPermitted(*apps[i], watch.Added)\n-\t\t}\n-\t}\n-\tunsubscribe := s.appBroadcaster.Subscribe(events)\n-\tdefer unsubscribe()\n-\tfor {\n-\t\tselect {\n-\t\tcase event := <-events:\n-\t\t\tsendIfPermitted(event.Application, event.Type)\n-\t\tcase <-ws.Context().Done():\n-\t\t\treturn nil\n-\t\t}\n-\t}\n+        appName := q.GetName()\n+        appNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n+        logCtx := log.NewEntry(log.New())\n+        if q.Name != nil {\n+                logCtx = logCtx.WithField(\"application\", *q.Name)\n+        }\n+        projects := map[string]bool{}\n+        for _, project := range getProjectsFromApplicationQuery(*q) {\n+                projects[project] = true\n+        }\n+        claims := ws.Context().Value(\"claims\")\n+        selector, err := labels.Parse(q.GetSelector())\n+        if err != nil {\n+                return fmt.Errorf(\"error parsing labels with selectors: %w\", err)\n+        }\n+        minVersion := 0\n+        if q.GetResourceVersion() != \"\" {\n+                if minVersion, err = strconv.Atoi(q.GetResourceVersion()); err != nil {\n+                        minVersion = 0\n+                }\n+        }\n+\n+        // sendIfPermitted is a helper to send the application to the client's streaming channel if the\n+        // caller has RBAC privileges permissions to view it\n+        sendIfPermitted := func(a appv1.Application, eventType watch.EventType) {\n+                permitted := s.isApplicationPermitted(selector, minVersion, claims, appName, appNs, projects, a)\n+                if !permitted {\n+                        return\n+                }\n+                s.inferResourcesStatusHealth(&a)\n+                err := ws.Send(&appv1.ApplicationWatchEvent{\n+                        Type:        eventType,\n+                        Application: a,\n+                })\n+                if err != nil {\n+                        logCtx.Warnf(\"Unable to send stream message: %v\", err)\n+                        return\n+                }\n+        }\n+\n+        events := make(chan *appv1.ApplicationWatchEvent, watchAPIBufferSize)\n+        // Mimic watch API behavior: send ADDED events if no resource version provided\n+        // If watch API is executed for one application when emit event even if resource version is provided\n+        // This is required since single app watch API is used for during operations like app syncing and it is\n+        // critical to never miss events.\n+        if q.GetResourceVersion() == \"\" || q.GetName() != \"\" {\n+                apps, err := s.appLister.List(selector)\n+                if err != nil {\n+                        return fmt.Errorf(\"error listing apps with selector: %w\", err)\n+                }\n+                sort.Slice(apps, func(i, j int) bool {\n+                        return apps[i].QualifiedName() < apps[j].QualifiedName()\n+                })\n+                for i := range apps {\n+                        sendIfPermitted(*apps[i], watch.Added)\n+                }\n+        }\n+        unsubscribe := s.appBroadcaster.Subscribe(events)\n+        defer unsubscribe()\n+        for {\n+                select {\n+                case event := <-events:\n+                        sendIfPermitted(event.Application, event.Type)\n+                case <-ws.Context().Done():\n+                        return nil\n+                }\n+        }\n }\n \n func (s *Server) validateAndNormalizeApp(ctx context.Context, app *appv1.Application, validate bool) error {\n-\tproj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\t// Offer no hint that the project does not exist.\n-\t\t\tlog.Warnf(\"User attempted to create/update application in non-existent project %q\", app.Spec.Project)\n-\t\t\treturn permissionDeniedErr\n-\t\t}\n-\t\treturn fmt.Errorf(\"error getting application's project: %w\", err)\n-\t}\n-\tif app.GetName() == \"\" {\n-\t\treturn fmt.Errorf(\"resource name may not be empty\")\n-\t}\n-\tappNs := s.appNamespaceOrDefault(app.Namespace)\n-\tcurrApp, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, app.Name, metav1.GetOptions{})\n-\tif err != nil {\n-\t\tif !apierr.IsNotFound(err) {\n-\t\t\treturn fmt.Errorf(\"error getting application by name: %w\", err)\n-\t\t}\n-\t\t// Kubernetes go-client will return a pointer to a zero-value app instead of nil, even\n-\t\t// though the API response was NotFound. This behavior was confirmed via logs.\n-\t\tcurrApp = nil\n-\t}\n-\tif currApp != nil && currApp.Spec.GetProject() != app.Spec.GetProject() {\n-\t\t// When changing projects, caller must have application create & update privileges in new project\n-\t\t// NOTE: the update check was already verified in the caller to this function\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionCreate, app.RBACName(s.ns)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\t// They also need 'update' privileges in the old project\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, currApp.RBACName(s.ns)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\tif err := argo.ValidateDestination(ctx, &app.Spec.Destination, s.db); err != nil {\n-\t\treturn status.Errorf(codes.InvalidArgument, \"application destination spec for %s is invalid: %s\", app.Name, err.Error())\n-\t}\n-\n-\tvar conditions []appv1.ApplicationCondition\n-\n-\tif validate {\n-\t\tconditions := make([]appv1.ApplicationCondition, 0)\n-\t\tcondition, err := argo.ValidateRepo(ctx, app, s.repoClientset, s.db, s.kubectl, proj, s.settingsMgr)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error validating the repo: %w\", err)\n-\t\t}\n-\t\tconditions = append(conditions, condition...)\n-\t\tif len(conditions) > 0 {\n-\t\t\treturn status.Errorf(codes.InvalidArgument, \"application spec for %s is invalid: %s\", app.Name, argo.FormatAppConditions(conditions))\n-\t\t}\n-\t}\n-\n-\tconditions, err = argo.ValidatePermissions(ctx, &app.Spec, proj, s.db)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error validating project permissions: %w\", err)\n-\t}\n-\tif len(conditions) > 0 {\n-\t\treturn status.Errorf(codes.InvalidArgument, \"application spec for %s is invalid: %s\", app.Name, argo.FormatAppConditions(conditions))\n-\t}\n-\n-\tapp.Spec = *argo.NormalizeApplicationSpec(&app.Spec)\n-\treturn nil\n+        proj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        // Offer no hint that the project does not exist.\n+                        log.Warnf(\"User attempted to create/update application in non-existent project %q\", app.Spec.Project)\n+                        return permissionDeniedErr\n+                }\n+                return fmt.Errorf(\"error getting application's project: %w\", err)\n+        }\n+        if app.GetName() == \"\" {\n+                return fmt.Errorf(\"resource name may not be empty\")\n+        }\n+        appNs := s.appNamespaceOrDefault(app.Namespace)\n+        currApp, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, app.Name, metav1.GetOptions{})\n+        if err != nil {\n+                if !apierr.IsNotFound(err) {\n+                        return fmt.Errorf(\"error getting application by name: %w\", err)\n+                }\n+                // Kubernetes go-client will return a pointer to a zero-value app instead of nil, even\n+                // though the API response was NotFound. This behavior was confirmed via logs.\n+                currApp = nil\n+        }\n+        if currApp != nil && currApp.Spec.GetProject() != app.Spec.GetProject() {\n+                // When changing projects, caller must have application create & update privileges in new project\n+                // NOTE: the update check was already verified in the caller to this function\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionCreate, app.RBACName(s.ns)); err != nil {\n+                        return err\n+                }\n+                // They also need 'update' privileges in the old project\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, currApp.RBACName(s.ns)); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        if err := argo.ValidateDestination(ctx, &app.Spec.Destination, s.db); err != nil {\n+                return status.Errorf(codes.InvalidArgument, \"application destination spec for %s is invalid: %s\", app.Name, err.Error())\n+        }\n+\n+        var conditions []appv1.ApplicationCondition\n+\n+        if validate {\n+                conditions := make([]appv1.ApplicationCondition, 0)\n+                condition, err := argo.ValidateRepo(ctx, app, s.repoClientset, s.db, s.kubectl, proj, s.settingsMgr)\n+                if err != nil {\n+                        return fmt.Errorf(\"error validating the repo: %w\", err)\n+                }\n+                conditions = append(conditions, condition...)\n+                if len(conditions) > 0 {\n+                        return status.Errorf(codes.InvalidArgument, \"application spec for %s is invalid: %s\", app.Name, argo.FormatAppConditions(conditions))\n+                }\n+        }\n+\n+        conditions, err = argo.ValidatePermissions(ctx, &app.Spec, proj, s.db)\n+        if err != nil {\n+                return fmt.Errorf(\"error validating project permissions: %w\", err)\n+        }\n+        if len(conditions) > 0 {\n+                return status.Errorf(codes.InvalidArgument, \"application spec for %s is invalid: %s\", app.Name, argo.FormatAppConditions(conditions))\n+        }\n+\n+        app.Spec = *argo.NormalizeApplicationSpec(&app.Spec)\n+        return nil\n }\n \n func (s *Server) getApplicationClusterConfig(ctx context.Context, a *appv1.Application) (*rest.Config, error) {\n-\tif err := argo.ValidateDestination(ctx, &a.Spec.Destination, s.db); err != nil {\n-\t\treturn nil, fmt.Errorf(\"error validating destination: %w\", err)\n-\t}\n-\tclst, err := s.db.GetCluster(ctx, a.Spec.Destination.Server)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting cluster: %w\", err)\n-\t}\n-\tconfig := clst.RESTConfig()\n-\treturn config, err\n+        if err := argo.ValidateDestination(ctx, &a.Spec.Destination, s.db); err != nil {\n+                return nil, fmt.Errorf(\"error validating destination: %w\", err)\n+        }\n+        clst, err := s.db.GetCluster(ctx, a.Spec.Destination.Server)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting cluster: %w\", err)\n+        }\n+        config := clst.RESTConfig()\n+        return config, err\n }\n \n // getCachedAppState loads the cached state and trigger app refresh if cache is missing\n func (s *Server) getCachedAppState(ctx context.Context, a *appv1.Application, getFromCache func() error) error {\n-\terr := getFromCache()\n-\tif err != nil && err == servercache.ErrCacheMiss {\n-\t\tconditions := a.Status.GetConditions(map[appv1.ApplicationConditionType]bool{\n-\t\t\tappv1.ApplicationConditionComparisonError:  true,\n-\t\t\tappv1.ApplicationConditionInvalidSpecError: true,\n-\t\t})\n-\t\tif len(conditions) > 0 {\n-\t\t\treturn errors.New(argoutil.FormatAppConditions(conditions))\n-\t\t}\n-\t\t_, err = s.Get(ctx, &application.ApplicationQuery{\n-\t\t\tName:         pointer.String(a.GetName()),\n-\t\t\tAppNamespace: pointer.String(a.GetNamespace()),\n-\t\t\tRefresh:      pointer.String(string(appv1.RefreshTypeNormal)),\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting application by query: %w\", err)\n-\t\t}\n-\t\treturn getFromCache()\n-\t}\n-\treturn err\n+        err := getFromCache()\n+        if err != nil && err == servercache.ErrCacheMiss {\n+                conditions := a.Status.GetConditions(map[appv1.ApplicationConditionType]bool{\n+                        appv1.ApplicationConditionComparisonError:  true,\n+                        appv1.ApplicationConditionInvalidSpecError: true,\n+                })\n+                if len(conditions) > 0 {\n+                        return errors.New(argoutil.FormatAppConditions(conditions))\n+                }\n+                _, err = s.Get(ctx, &application.ApplicationQuery{\n+                        Name:         pointer.String(a.GetName()),\n+                        AppNamespace: pointer.String(a.GetNamespace()),\n+                        Refresh:      pointer.String(string(appv1.RefreshTypeNormal)),\n+                })\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting application by query: %w\", err)\n+                }\n+                return getFromCache()\n+        }\n+        return err\n }\n \n func (s *Server) getAppResources(ctx context.Context, a *appv1.Application) (*appv1.ApplicationTree, error) {\n-\tvar tree appv1.ApplicationTree\n-\terr := s.getCachedAppState(ctx, a, func() error {\n-\t\treturn s.cache.GetAppResourcesTree(a.InstanceName(s.ns), &tree)\n-\t})\n-\tif err != nil {\n-\t\treturn &tree, fmt.Errorf(\"error getting cached app resource tree: %w\", err)\n-\t}\n-\treturn &tree, nil\n+        var tree appv1.ApplicationTree\n+        err := s.getCachedAppState(ctx, a, func() error {\n+                return s.cache.GetAppResourcesTree(a.InstanceName(s.ns), &tree)\n+        })\n+        if err != nil {\n+                return &tree, fmt.Errorf(\"error getting cached app resource tree: %w\", err)\n+        }\n+        return &tree, nil\n }\n \n func (s *Server) getAppLiveResource(ctx context.Context, action string, q *application.ApplicationResourceRequest) (*appv1.ResourceNode, *rest.Config, *appv1.Application, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, action, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, nil, nil, err\n-\t}\n-\ttree, err := s.getAppResources(ctx, a)\n-\tif err != nil {\n-\t\treturn nil, nil, nil, fmt.Errorf(\"error getting app resources: %w\", err)\n-\t}\n-\n-\tfound := tree.FindNode(q.GetGroup(), q.GetKind(), q.GetNamespace(), q.GetResourceName())\n-\tif found == nil || found.ResourceRef.UID == \"\" {\n-\t\treturn nil, nil, nil, status.Errorf(codes.InvalidArgument, \"%s %s %s not found as part of application %s\", q.GetKind(), q.GetGroup(), q.GetResourceName(), q.GetName())\n-\t}\n-\tconfig, err := s.getApplicationClusterConfig(ctx, a)\n-\tif err != nil {\n-\t\treturn nil, nil, nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t}\n-\treturn found, config, a, nil\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, action, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, nil, nil, err\n+        }\n+        tree, err := s.getAppResources(ctx, a)\n+        if err != nil {\n+                return nil, nil, nil, fmt.Errorf(\"error getting app resources: %w\", err)\n+        }\n+\n+        found := tree.FindNode(q.GetGroup(), q.GetKind(), q.GetNamespace(), q.GetResourceName())\n+        if found == nil || found.ResourceRef.UID == \"\" {\n+                return nil, nil, nil, status.Errorf(codes.InvalidArgument, \"%s %s %s not found as part of application %s\", q.GetKind(), q.GetGroup(), q.GetResourceName(), q.GetName())\n+        }\n+        config, err := s.getApplicationClusterConfig(ctx, a)\n+        if err != nil {\n+                return nil, nil, nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n+        }\n+        return found, config, a, nil\n }\n \n func (s *Server) GetResource(ctx context.Context, q *application.ApplicationResourceRequest) (*application.ApplicationResourceResponse, error) {\n-\tres, config, _, err := s.getAppLiveResource(ctx, rbacpolicy.ActionGet, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// make sure to use specified resource version if provided\n-\tif q.GetVersion() != \"\" {\n-\t\tres.Version = q.GetVersion()\n-\t}\n-\tobj, err := s.kubectl.GetResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting resource: %w\", err)\n-\t}\n-\tobj, err = replaceSecretValues(obj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n-\t}\n-\tdata, err := json.Marshal(obj.Object)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error marshaling object: %w\", err)\n-\t}\n-\tmanifest := string(data)\n-\treturn &application.ApplicationResourceResponse{Manifest: &manifest}, nil\n+        res, config, _, err := s.getAppLiveResource(ctx, rbacpolicy.ActionGet, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // make sure to use specified resource version if provided\n+        if q.GetVersion() != \"\" {\n+                res.Version = q.GetVersion()\n+        }\n+        obj, err := s.kubectl.GetResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting resource: %w\", err)\n+        }\n+        obj, err = replaceSecretValues(obj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n+        }\n+        data, err := json.Marshal(obj.Object)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error marshaling object: %w\", err)\n+        }\n+        manifest := string(data)\n+        return &application.ApplicationResourceResponse{Manifest: &manifest}, nil\n }\n \n func replaceSecretValues(obj *unstructured.Unstructured) (*unstructured.Unstructured, error) {\n-\tif obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n-\t\t_, obj, err := diff.HideSecretData(nil, obj)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\treturn obj, err\n-\t}\n-\treturn obj, nil\n+        if obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n+                _, obj, err := diff.HideSecretData(nil, obj)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                return obj, err\n+        }\n+        return obj, nil\n }\n \n // PatchResource patches a resource\n func (s *Server) PatchResource(ctx context.Context, q *application.ApplicationResourcePatchRequest) (*application.ApplicationResourceResponse, error) {\n-\tresourceRequest := &application.ApplicationResourceRequest{\n-\t\tName:         q.Name,\n-\t\tAppNamespace: q.AppNamespace,\n-\t\tNamespace:    q.Namespace,\n-\t\tResourceName: q.ResourceName,\n-\t\tKind:         q.Kind,\n-\t\tVersion:      q.Version,\n-\t\tGroup:        q.Group,\n-\t\tProject:      q.Project,\n-\t}\n-\tres, config, a, err := s.getAppLiveResource(ctx, rbacpolicy.ActionUpdate, resourceRequest)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tmanifest, err := s.kubectl.PatchResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace, types.PatchType(q.GetPatchType()), []byte(q.GetPatch()))\n-\tif err != nil {\n-\t\t// don't expose real error for secrets since it might contain secret data\n-\t\tif res.Kind == kube.SecretKind && res.Group == \"\" {\n-\t\t\treturn nil, fmt.Errorf(\"failed to patch Secret %s/%s\", res.Namespace, res.Name)\n-\t\t}\n-\t\treturn nil, fmt.Errorf(\"error patching resource: %w\", err)\n-\t}\n-\tif manifest == nil {\n-\t\treturn nil, fmt.Errorf(\"failed to patch resource: manifest was nil\")\n-\t}\n-\tmanifest, err = replaceSecretValues(manifest)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n-\t}\n-\tdata, err := json.Marshal(manifest.Object)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"erro marshaling manifest object: %w\", err)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonResourceUpdated, fmt.Sprintf(\"patched resource %s/%s '%s'\", q.GetGroup(), q.GetKind(), q.GetResourceName()))\n-\tm := string(data)\n-\treturn &application.ApplicationResourceResponse{\n-\t\tManifest: &m,\n-\t}, nil\n+        resourceRequest := &application.ApplicationResourceRequest{\n+                Name:         q.Name,\n+                AppNamespace: q.AppNamespace,\n+                Namespace:    q.Namespace,\n+                ResourceName: q.ResourceName,\n+                Kind:         q.Kind,\n+                Version:      q.Version,\n+                Group:        q.Group,\n+                Project:      q.Project,\n+        }\n+        res, config, a, err := s.getAppLiveResource(ctx, rbacpolicy.ActionUpdate, resourceRequest)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        manifest, err := s.kubectl.PatchResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace, types.PatchType(q.GetPatchType()), []byte(q.GetPatch()))\n+        if err != nil {\n+                // don't expose real error for secrets since it might contain secret data\n+                if res.Kind == kube.SecretKind && res.Group == \"\" {\n+                        return nil, fmt.Errorf(\"failed to patch Secret %s/%s\", res.Namespace, res.Name)\n+                }\n+                return nil, fmt.Errorf(\"error patching resource: %w\", err)\n+        }\n+        if manifest == nil {\n+                return nil, fmt.Errorf(\"failed to patch resource: manifest was nil\")\n+        }\n+        manifest, err = replaceSecretValues(manifest)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n+        }\n+        data, err := json.Marshal(manifest.Object)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"erro marshaling manifest object: %w\", err)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonResourceUpdated, fmt.Sprintf(\"patched resource %s/%s '%s'\", q.GetGroup(), q.GetKind(), q.GetResourceName()))\n+        m := string(data)\n+        return &application.ApplicationResourceResponse{\n+                Manifest: &m,\n+        }, nil\n }\n \n // DeleteResource deletes a specified resource\n func (s *Server) DeleteResource(ctx context.Context, q *application.ApplicationResourceDeleteRequest) (*application.ApplicationResponse, error) {\n-\tresourceRequest := &application.ApplicationResourceRequest{\n-\t\tName:         q.Name,\n-\t\tAppNamespace: q.AppNamespace,\n-\t\tNamespace:    q.Namespace,\n-\t\tResourceName: q.ResourceName,\n-\t\tKind:         q.Kind,\n-\t\tVersion:      q.Version,\n-\t\tGroup:        q.Group,\n-\t\tProject:      q.Project,\n-\t}\n-\tres, config, a, err := s.getAppLiveResource(ctx, rbacpolicy.ActionDelete, resourceRequest)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvar deleteOption metav1.DeleteOptions\n-\tif q.GetOrphan() {\n-\t\tpropagationPolicy := metav1.DeletePropagationOrphan\n-\t\tdeleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy}\n-\t} else if q.GetForce() {\n-\t\tpropagationPolicy := metav1.DeletePropagationBackground\n-\t\tzeroGracePeriod := int64(0)\n-\t\tdeleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy, GracePeriodSeconds: &zeroGracePeriod}\n-\t} else {\n-\t\tpropagationPolicy := metav1.DeletePropagationForeground\n-\t\tdeleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy}\n-\t}\n-\terr = s.kubectl.DeleteResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace, deleteOption)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error deleting resource: %w\", err)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonResourceDeleted, fmt.Sprintf(\"deleted resource %s/%s '%s'\", q.GetGroup(), q.GetKind(), q.GetResourceName()))\n-\treturn &application.ApplicationResponse{}, nil\n+        resourceRequest := &application.ApplicationResourceRequest{\n+                Name:         q.Name,\n+                AppNamespace: q.AppNamespace,\n+                Namespace:    q.Namespace,\n+                ResourceName: q.ResourceName,\n+                Kind:         q.Kind,\n+                Version:      q.Version,\n+                Group:        q.Group,\n+                Project:      q.Project,\n+        }\n+        res, config, a, err := s.getAppLiveResource(ctx, rbacpolicy.ActionDelete, resourceRequest)\n+        if err != nil {\n+                return nil, err\n+        }\n+        var deleteOption metav1.DeleteOptions\n+        if q.GetOrphan() {\n+                propagationPolicy := metav1.DeletePropagationOrphan\n+                deleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy}\n+        } else if q.GetForce() {\n+                propagationPolicy := metav1.DeletePropagationBackground\n+                zeroGracePeriod := int64(0)\n+                deleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy, GracePeriodSeconds: &zeroGracePeriod}\n+        } else {\n+                propagationPolicy := metav1.DeletePropagationForeground\n+                deleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy}\n+        }\n+        err = s.kubectl.DeleteResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace, deleteOption)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error deleting resource: %w\", err)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonResourceDeleted, fmt.Sprintf(\"deleted resource %s/%s '%s'\", q.GetGroup(), q.GetKind(), q.GetResourceName()))\n+        return &application.ApplicationResponse{}, nil\n }\n \n func (s *Server) ResourceTree(ctx context.Context, q *application.ResourcesQuery) (*appv1.ApplicationTree, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\treturn s.getAppResources(ctx, a)\n+        return s.getAppResources(ctx, a)\n }\n \n func (s *Server) WatchResourceTree(q *application.ResourcesQuery, ws application.ApplicationService_WatchResourceTreeServer) error {\n-\t_, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tcacheKey := argo.AppInstanceName(q.GetApplicationName(), q.GetAppNamespace(), s.ns)\n-\treturn s.cache.OnAppResourcesTreeChanged(ws.Context(), cacheKey, func() error {\n-\t\tvar tree appv1.ApplicationTree\n-\t\terr := s.cache.GetAppResourcesTree(cacheKey, &tree)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app resource tree: %w\", err)\n-\t\t}\n-\t\treturn ws.Send(&tree)\n-\t})\n+        _, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n+        if err != nil {\n+                return err\n+        }\n+\n+        cacheKey := argo.AppInstanceName(q.GetApplicationName(), q.GetAppNamespace(), s.ns)\n+        return s.cache.OnAppResourcesTreeChanged(ws.Context(), cacheKey, func() error {\n+                var tree appv1.ApplicationTree\n+                err := s.cache.GetAppResourcesTree(cacheKey, &tree)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app resource tree: %w\", err)\n+                }\n+                return ws.Send(&tree)\n+        })\n }\n \n func (s *Server) RevisionMetadata(ctx context.Context, q *application.RevisionMetadataQuery) (*appv1.RevisionMetadata, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tsource := a.Spec.GetSource()\n-\trepo, err := s.db.GetRepository(ctx, source.RepoURL)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting repository by URL: %w\", err)\n-\t}\n-\t// We need to get some information with the project associated to the app,\n-\t// so we'll know whether GPG signatures are enforced.\n-\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting app project: %w\", err)\n-\t}\n-\tconn, repoClient, err := s.repoClientset.NewRepoServerClient()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error creating repo server client: %w\", err)\n-\t}\n-\tdefer ioutil.Close(conn)\n-\treturn repoClient.GetRevisionMetadata(ctx, &apiclient.RepoServerRevisionMetadataRequest{\n-\t\tRepo:           repo,\n-\t\tRevision:       q.GetRevision(),\n-\t\tCheckSignature: len(proj.Spec.SignatureKeys) > 0,\n-\t})\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        source := a.Spec.GetSource()\n+        repo, err := s.db.GetRepository(ctx, source.RepoURL)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting repository by URL: %w\", err)\n+        }\n+        // We need to get some information with the project associated to the app,\n+        // so we'll know whether GPG signatures are enforced.\n+        proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting app project: %w\", err)\n+        }\n+        conn, repoClient, err := s.repoClientset.NewRepoServerClient()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error creating repo server client: %w\", err)\n+        }\n+        defer ioutil.Close(conn)\n+        return repoClient.GetRevisionMetadata(ctx, &apiclient.RepoServerRevisionMetadataRequest{\n+                Repo:           repo,\n+                Revision:       q.GetRevision(),\n+                CheckSignature: len(proj.Spec.SignatureKeys) > 0,\n+        })\n }\n \n // RevisionChartDetails returns the helm chart metadata, as fetched from the reposerver\n func (s *Server) RevisionChartDetails(ctx context.Context, q *application.RevisionMetadataQuery) (*appv1.ChartDetails, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif a.Spec.Source.Chart == \"\" {\n-\t\treturn nil, fmt.Errorf(\"no chart found for application: %v\", a.QualifiedName())\n-\t}\n-\trepo, err := s.db.GetRepository(ctx, a.Spec.Source.RepoURL)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting repository by URL: %w\", err)\n-\t}\n-\tconn, repoClient, err := s.repoClientset.NewRepoServerClient()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error creating repo server client: %w\", err)\n-\t}\n-\tdefer ioutil.Close(conn)\n-\treturn repoClient.GetRevisionChartDetails(ctx, &apiclient.RepoServerRevisionChartDetailsRequest{\n-\t\tRepo:     repo,\n-\t\tName:     a.Spec.Source.Chart,\n-\t\tRevision: q.GetRevision(),\n-\t})\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, err\n+        }\n+        if a.Spec.Source.Chart == \"\" {\n+                return nil, fmt.Errorf(\"no chart found for application: %v\", a.QualifiedName())\n+        }\n+        repo, err := s.db.GetRepository(ctx, a.Spec.Source.RepoURL)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting repository by URL: %w\", err)\n+        }\n+        conn, repoClient, err := s.repoClientset.NewRepoServerClient()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error creating repo server client: %w\", err)\n+        }\n+        defer ioutil.Close(conn)\n+        return repoClient.GetRevisionChartDetails(ctx, &apiclient.RepoServerRevisionChartDetailsRequest{\n+                Repo:     repo,\n+                Name:     a.Spec.Source.Chart,\n+                Revision: q.GetRevision(),\n+        })\n }\n \n func isMatchingResource(q *application.ResourcesQuery, key kube.ResourceKey) bool {\n-\treturn (q.GetName() == \"\" || q.GetName() == key.Name) &&\n-\t\t(q.GetNamespace() == \"\" || q.GetNamespace() == key.Namespace) &&\n-\t\t(q.GetGroup() == \"\" || q.GetGroup() == key.Group) &&\n-\t\t(q.GetKind() == \"\" || q.GetKind() == key.Kind)\n+        return (q.GetName() == \"\" || q.GetName() == key.Name) &&\n+                (q.GetNamespace() == \"\" || q.GetNamespace() == key.Namespace) &&\n+                (q.GetGroup() == \"\" || q.GetGroup() == key.Group) &&\n+                (q.GetKind() == \"\" || q.GetKind() == key.Kind)\n }\n \n func (s *Server) ManagedResources(ctx context.Context, q *application.ResourcesQuery) (*application.ManagedResourcesResponse, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\titems := make([]*appv1.ResourceDiff, 0)\n-\terr = s.getCachedAppState(ctx, a, func() error {\n-\t\treturn s.cache.GetAppManagedResources(a.InstanceName(s.ns), &items)\n-\t})\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting cached app managed resources: %w\", err)\n-\t}\n-\tres := &application.ManagedResourcesResponse{}\n-\tfor i := range items {\n-\t\titem := items[i]\n-\t\tif !item.Hook && isMatchingResource(q, kube.ResourceKey{Name: item.Name, Namespace: item.Namespace, Kind: item.Kind, Group: item.Group}) {\n-\t\t\tres.Items = append(res.Items, item)\n-\t\t}\n-\t}\n-\n-\treturn res, nil\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        items := make([]*appv1.ResourceDiff, 0)\n+        err = s.getCachedAppState(ctx, a, func() error {\n+                return s.cache.GetAppManagedResources(a.InstanceName(s.ns), &items)\n+        })\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting cached app managed resources: %w\", err)\n+        }\n+        res := &application.ManagedResourcesResponse{}\n+        for i := range items {\n+                item := items[i]\n+                if !item.Hook && isMatchingResource(q, kube.ResourceKey{Name: item.Name, Namespace: item.Namespace, Kind: item.Kind, Group: item.Group}) {\n+                        res.Items = append(res.Items, item)\n+                }\n+        }\n+\n+        return res, nil\n }\n \n func (s *Server) PodLogs(q *application.ApplicationPodLogsQuery, ws application.ApplicationService_PodLogsServer) error {\n-\tif q.PodName != nil {\n-\t\tpodKind := \"Pod\"\n-\t\tq.Kind = &podKind\n-\t\tq.ResourceName = q.PodName\n-\t}\n-\n-\tvar sinceSeconds, tailLines *int64\n-\tif q.GetSinceSeconds() > 0 {\n-\t\tsinceSeconds = pointer.Int64(q.GetSinceSeconds())\n-\t}\n-\tif q.GetTailLines() > 0 {\n-\t\ttailLines = pointer.Int64(q.GetTailLines())\n-\t}\n-\tvar untilTime *metav1.Time\n-\tif q.GetUntilTime() != \"\" {\n-\t\tif val, err := time.Parse(time.RFC3339Nano, q.GetUntilTime()); err != nil {\n-\t\t\treturn fmt.Errorf(\"invalid untilTime parameter value: %v\", err)\n-\t\t} else {\n-\t\t\tuntilTimeVal := metav1.NewTime(val)\n-\t\t\tuntilTime = &untilTimeVal\n-\t\t}\n-\t}\n-\n-\tliteral := \"\"\n-\tinverse := false\n-\tif q.GetFilter() != \"\" {\n-\t\tliteral = *q.Filter\n-\t\tif literal[0] == '!' {\n-\t\t\tliteral = literal[1:]\n-\t\t\tinverse = true\n-\t\t}\n-\t}\n-\n-\ta, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Logs RBAC will be enforced only if an internal var serverRBACLogEnforceEnable (representing server.rbac.log.enforce.enable env var)\n-\t// is defined and has a \"true\" value\n-\t// Otherwise, no RBAC enforcement for logs will take place (meaning, PodLogs will return the logs,\n-\t// even if there is no explicit RBAC allow, or if there is an explicit RBAC deny)\n-\tserverRBACLogEnforceEnable, err := s.settingsMgr.GetServerRBACLogEnforceEnable()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting RBAC log enforce enable: %w\", err)\n-\t}\n-\n-\tif serverRBACLogEnforceEnable {\n-\t\tif err := s.enf.EnforceErr(ws.Context().Value(\"claims\"), rbacpolicy.ResourceLogs, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\ttree, err := s.getAppResources(ws.Context(), a)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting app resource tree: %w\", err)\n-\t}\n-\n-\tconfig, err := s.getApplicationClusterConfig(ws.Context(), a)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t}\n-\n-\tkubeClientset, err := kubernetes.NewForConfig(config)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error creating kube client: %w\", err)\n-\t}\n-\n-\t// from the tree find pods which match query of kind, group, and resource name\n-\tpods := getSelectedPods(tree.Nodes, q)\n-\tif len(pods) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tif len(pods) > maxPodLogsToRender {\n-\t\treturn errors.New(\"Max pods to view logs are reached. Please provide more granular query.\")\n-\t}\n-\n-\tvar streams []chan logEntry\n-\n-\tfor _, pod := range pods {\n-\t\tstream, err := kubeClientset.CoreV1().Pods(pod.Namespace).GetLogs(pod.Name, &v1.PodLogOptions{\n-\t\t\tContainer:    q.GetContainer(),\n-\t\t\tFollow:       q.GetFollow(),\n-\t\t\tTimestamps:   true,\n-\t\t\tSinceSeconds: sinceSeconds,\n-\t\t\tSinceTime:    q.GetSinceTime(),\n-\t\t\tTailLines:    tailLines,\n-\t\t\tPrevious:     q.GetPrevious(),\n-\t\t}).Stream(ws.Context())\n-\t\tpodName := pod.Name\n-\t\tlogStream := make(chan logEntry)\n-\t\tif err == nil {\n-\t\t\tdefer ioutil.Close(stream)\n-\t\t}\n-\n-\t\tstreams = append(streams, logStream)\n-\t\tgo func() {\n-\t\t\t// if k8s failed to start steaming logs (typically because Pod is not ready yet)\n-\t\t\t// then the error should be shown in the UI so that user know the reason\n-\t\t\tif err != nil {\n-\t\t\t\tlogStream <- logEntry{line: err.Error()}\n-\t\t\t} else {\n-\t\t\t\tparseLogsStream(podName, stream, logStream)\n-\t\t\t}\n-\t\t\tclose(logStream)\n-\t\t}()\n-\t}\n-\n-\tlogStream := mergeLogStreams(streams, time.Millisecond*100)\n-\tsentCount := int64(0)\n-\tdone := make(chan error)\n-\tgo func() {\n-\t\tfor entry := range logStream {\n-\t\t\tif entry.err != nil {\n-\t\t\t\tdone <- entry.err\n-\t\t\t\treturn\n-\t\t\t} else {\n-\t\t\t\tif q.Filter != nil {\n-\t\t\t\t\tlineContainsFilter := strings.Contains(entry.line, literal)\n-\t\t\t\t\tif (inverse && lineContainsFilter) || (!inverse && !lineContainsFilter) {\n-\t\t\t\t\t\tcontinue\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tts := metav1.NewTime(entry.timeStamp)\n-\t\t\t\tif untilTime != nil && entry.timeStamp.After(untilTime.Time) {\n-\t\t\t\t\tdone <- ws.Send(&application.LogEntry{\n-\t\t\t\t\t\tLast:         pointer.Bool(true),\n-\t\t\t\t\t\tPodName:      &entry.podName,\n-\t\t\t\t\t\tContent:      &entry.line,\n-\t\t\t\t\t\tTimeStampStr: pointer.String(entry.timeStamp.Format(time.RFC3339Nano)),\n-\t\t\t\t\t\tTimeStamp:    &ts,\n-\t\t\t\t\t})\n-\t\t\t\t\treturn\n-\t\t\t\t} else {\n-\t\t\t\t\tsentCount++\n-\t\t\t\t\tif err := ws.Send(&application.LogEntry{\n-\t\t\t\t\t\tPodName:      &entry.podName,\n-\t\t\t\t\t\tContent:      &entry.line,\n-\t\t\t\t\t\tTimeStampStr: pointer.String(entry.timeStamp.Format(time.RFC3339Nano)),\n-\t\t\t\t\t\tTimeStamp:    &ts,\n-\t\t\t\t\t\tLast:         pointer.Bool(false),\n-\t\t\t\t\t}); err != nil {\n-\t\t\t\t\t\tdone <- err\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\tnow := time.Now()\n-\t\tnowTS := metav1.NewTime(now)\n-\t\tdone <- ws.Send(&application.LogEntry{\n-\t\t\tLast:         pointer.Bool(true),\n-\t\t\tPodName:      pointer.String(\"\"),\n-\t\t\tContent:      pointer.String(\"\"),\n-\t\t\tTimeStampStr: pointer.String(now.Format(time.RFC3339Nano)),\n-\t\t\tTimeStamp:    &nowTS,\n-\t\t})\n-\t}()\n-\n-\tselect {\n-\tcase err := <-done:\n-\t\treturn err\n-\tcase <-ws.Context().Done():\n-\t\tlog.WithField(\"application\", q.Name).Debug(\"k8s pod logs reader completed due to closed grpc context\")\n-\t\treturn nil\n-\t}\n+        if q.PodName != nil {\n+                podKind := \"Pod\"\n+                q.Kind = &podKind\n+                q.ResourceName = q.PodName\n+        }\n+\n+        var sinceSeconds, tailLines *int64\n+        if q.GetSinceSeconds() > 0 {\n+                sinceSeconds = pointer.Int64(q.GetSinceSeconds())\n+        }\n+        if q.GetTailLines() > 0 {\n+                tailLines = pointer.Int64(q.GetTailLines())\n+        }\n+        var untilTime *metav1.Time\n+        if q.GetUntilTime() != \"\" {\n+                if val, err := time.Parse(time.RFC3339Nano, q.GetUntilTime()); err != nil {\n+                        return fmt.Errorf(\"invalid untilTime parameter value: %v\", err)\n+                } else {\n+                        untilTimeVal := metav1.NewTime(val)\n+                        untilTime = &untilTimeVal\n+                }\n+        }\n+\n+        literal := \"\"\n+        inverse := false\n+        if q.GetFilter() != \"\" {\n+                literal = *q.Filter\n+                if literal[0] == '!' {\n+                        literal = literal[1:]\n+                        inverse = true\n+                }\n+        }\n+\n+        a, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return err\n+        }\n+\n+        // Logs RBAC will be enforced only if an internal var serverRBACLogEnforceEnable (representing server.rbac.log.enforce.enable env var)\n+        // is defined and has a \"true\" value\n+        // Otherwise, no RBAC enforcement for logs will take place (meaning, PodLogs will return the logs,\n+        // even if there is no explicit RBAC allow, or if there is an explicit RBAC deny)\n+        serverRBACLogEnforceEnable, err := s.settingsMgr.GetServerRBACLogEnforceEnable()\n+        if err != nil {\n+                return fmt.Errorf(\"error getting RBAC log enforce enable: %w\", err)\n+        }\n+\n+        if serverRBACLogEnforceEnable {\n+                if err := s.enf.EnforceErr(ws.Context().Value(\"claims\"), rbacpolicy.ResourceLogs, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        tree, err := s.getAppResources(ws.Context(), a)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting app resource tree: %w\", err)\n+        }\n+\n+        config, err := s.getApplicationClusterConfig(ws.Context(), a)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting application cluster config: %w\", err)\n+        }\n+\n+        kubeClientset, err := kubernetes.NewForConfig(config)\n+        if err != nil {\n+                return fmt.Errorf(\"error creating kube client: %w\", err)\n+        }\n+\n+        // from the tree find pods which match query of kind, group, and resource name\n+        pods := getSelectedPods(tree.Nodes, q)\n+        if len(pods) == 0 {\n+                return nil\n+        }\n+\n+        if len(pods) > maxPodLogsToRender {\n+                return errors.New(\"Max pods to view logs are reached. Please provide more granular query.\")\n+        }\n+\n+        var streams []chan logEntry\n+\n+        for _, pod := range pods {\n+                stream, err := kubeClientset.CoreV1().Pods(pod.Namespace).GetLogs(pod.Name, &v1.PodLogOptions{\n+                        Container:    q.GetContainer(),\n+                        Follow:       q.GetFollow(),\n+                        Timestamps:   true,\n+                        SinceSeconds: sinceSeconds,\n+                        SinceTime:    q.GetSinceTime(),\n+                        TailLines:    tailLines,\n+                        Previous:     q.GetPrevious(),\n+                }).Stream(ws.Context())\n+                podName := pod.Name\n+                logStream := make(chan logEntry)\n+                if err == nil {\n+                        defer ioutil.Close(stream)\n+                }\n+\n+                streams = append(streams, logStream)\n+                go func() {\n+                        // if k8s failed to start steaming logs (typically because Pod is not ready yet)\n+                        // then the error should be shown in the UI so that user know the reason\n+                        if err != nil {\n+                                logStream <- logEntry{line: err.Error()}\n+                        } else {\n+                                parseLogsStream(podName, stream, logStream)\n+                        }\n+                        close(logStream)\n+                }()\n+        }\n+\n+        logStream := mergeLogStreams(streams, time.Millisecond*100)\n+        sentCount := int64(0)\n+        done := make(chan error)\n+        go func() {\n+                for entry := range logStream {\n+                        if entry.err != nil {\n+                                done <- entry.err\n+                                return\n+                        } else {\n+                                if q.Filter != nil {\n+                                        lineContainsFilter := strings.Contains(entry.line, literal)\n+                                        if (inverse && lineContainsFilter) || (!inverse && !lineContainsFilter) {\n+                                                continue\n+                                        }\n+                                }\n+                                ts := metav1.NewTime(entry.timeStamp)\n+                                if untilTime != nil && entry.timeStamp.After(untilTime.Time) {\n+                                        done <- ws.Send(&application.LogEntry{\n+                                                Last:         pointer.Bool(true),\n+                                                PodName:      &entry.podName,\n+                                                Content:      &entry.line,\n+                                                TimeStampStr: pointer.String(entry.timeStamp.Format(time.RFC3339Nano)),\n+                                                TimeStamp:    &ts,\n+                                        })\n+                                        return\n+                                } else {\n+                                        sentCount++\n+                                        if err := ws.Send(&application.LogEntry{\n+                                                PodName:      &entry.podName,\n+                                                Content:      &entry.line,\n+                                                TimeStampStr: pointer.String(entry.timeStamp.Format(time.RFC3339Nano)),\n+                                                TimeStamp:    &ts,\n+                                                Last:         pointer.Bool(false),\n+                                        }); err != nil {\n+                                                done <- err\n+                                                break\n+                                        }\n+                                }\n+                        }\n+                }\n+                now := time.Now()\n+                nowTS := metav1.NewTime(now)\n+                done <- ws.Send(&application.LogEntry{\n+                        Last:         pointer.Bool(true),\n+                        PodName:      pointer.String(\"\"),\n+                        Content:      pointer.String(\"\"),\n+                        TimeStampStr: pointer.String(now.Format(time.RFC3339Nano)),\n+                        TimeStamp:    &nowTS,\n+                })\n+        }()\n+\n+        select {\n+        case err := <-done:\n+                return err\n+        case <-ws.Context().Done():\n+                log.WithField(\"application\", q.Name).Debug(\"k8s pod logs reader completed due to closed grpc context\")\n+                return nil\n+        }\n }\n \n // from all of the treeNodes, get the pod who meets the criteria or whose parents meets the criteria\n func getSelectedPods(treeNodes []appv1.ResourceNode, q *application.ApplicationPodLogsQuery) []appv1.ResourceNode {\n-\tvar pods []appv1.ResourceNode\n-\tisTheOneMap := make(map[string]bool)\n-\tfor _, treeNode := range treeNodes {\n-\t\tif treeNode.Kind == kube.PodKind && treeNode.Group == \"\" && treeNode.UID != \"\" {\n-\t\t\tif isTheSelectedOne(&treeNode, q, treeNodes, isTheOneMap) {\n-\t\t\t\tpods = append(pods, treeNode)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn pods\n+        var pods []appv1.ResourceNode\n+        isTheOneMap := make(map[string]bool)\n+        for _, treeNode := range treeNodes {\n+                if treeNode.Kind == kube.PodKind && treeNode.Group == \"\" && treeNode.UID != \"\" {\n+                        if isTheSelectedOne(&treeNode, q, treeNodes, isTheOneMap) {\n+                                pods = append(pods, treeNode)\n+                        }\n+                }\n+        }\n+        return pods\n }\n \n // check is currentNode is matching with group, kind, and name, or if any of its parents matches\n func isTheSelectedOne(currentNode *appv1.ResourceNode, q *application.ApplicationPodLogsQuery, resourceNodes []appv1.ResourceNode, isTheOneMap map[string]bool) bool {\n-\texist, value := isTheOneMap[currentNode.UID]\n-\tif exist {\n-\t\treturn value\n-\t}\n-\n-\tif (q.GetResourceName() == \"\" || currentNode.Name == q.GetResourceName()) &&\n-\t\t(q.GetKind() == \"\" || currentNode.Kind == q.GetKind()) &&\n-\t\t(q.GetGroup() == \"\" || currentNode.Group == q.GetGroup()) &&\n-\t\t(q.GetNamespace() == \"\" || currentNode.Namespace == q.GetNamespace()) {\n-\t\tisTheOneMap[currentNode.UID] = true\n-\t\treturn true\n-\t}\n-\n-\tif len(currentNode.ParentRefs) == 0 {\n-\t\tisTheOneMap[currentNode.UID] = false\n-\t\treturn false\n-\t}\n-\n-\tfor _, parentResource := range currentNode.ParentRefs {\n-\t\t// look up parentResource from resourceNodes\n-\t\t// then check if the parent isTheSelectedOne\n-\t\tfor _, resourceNode := range resourceNodes {\n-\t\t\tif resourceNode.Namespace == parentResource.Namespace &&\n-\t\t\t\tresourceNode.Name == parentResource.Name &&\n-\t\t\t\tresourceNode.Group == parentResource.Group &&\n-\t\t\t\tresourceNode.Kind == parentResource.Kind {\n-\t\t\t\tif isTheSelectedOne(&resourceNode, q, resourceNodes, isTheOneMap) {\n-\t\t\t\t\tisTheOneMap[currentNode.UID] = true\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tisTheOneMap[currentNode.UID] = false\n-\treturn false\n+        exist, value := isTheOneMap[currentNode.UID]\n+        if exist {\n+                return value\n+        }\n+\n+        if (q.GetResourceName() == \"\" || currentNode.Name == q.GetResourceName()) &&\n+                (q.GetKind() == \"\" || currentNode.Kind == q.GetKind()) &&\n+                (q.GetGroup() == \"\" || currentNode.Group == q.GetGroup()) &&\n+                (q.GetNamespace() == \"\" || currentNode.Namespace == q.GetNamespace()) {\n+                isTheOneMap[currentNode.UID] = true\n+                return true\n+        }\n+\n+        if len(currentNode.ParentRefs) == 0 {\n+                isTheOneMap[currentNode.UID] = false\n+                return false\n+        }\n+\n+        for _, parentResource := range currentNode.ParentRefs {\n+                // look up parentResource from resourceNodes\n+                // then check if the parent isTheSelectedOne\n+                for _, resourceNode := range resourceNodes {\n+                        if resourceNode.Namespace == parentResource.Namespace &&\n+                                resourceNode.Name == parentResource.Name &&\n+                                resourceNode.Group == parentResource.Group &&\n+                                resourceNode.Kind == parentResource.Kind {\n+                                if isTheSelectedOne(&resourceNode, q, resourceNodes, isTheOneMap) {\n+                                        isTheOneMap[currentNode.UID] = true\n+                                        return true\n+                                }\n+                        }\n+                }\n+        }\n+\n+        isTheOneMap[currentNode.UID] = false\n+        return false\n }\n \n // Sync syncs an application to its target state\n func (s *Server) Sync(ctx context.Context, syncReq *application.ApplicationSyncRequest) (*appv1.Application, error) {\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, syncReq.GetProject(), syncReq.GetAppNamespace(), syncReq.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\treturn a, status.Errorf(codes.InvalidArgument, \"application references project %s which does not exist\", a.Spec.Project)\n-\t\t}\n-\t\treturn a, fmt.Errorf(\"error getting app project: %w\", err)\n-\t}\n-\n-\ts.inferResourcesStatusHealth(a)\n-\n-\tif !proj.Spec.SyncWindows.Matches(a).CanSync(true) {\n-\t\treturn a, status.Errorf(codes.PermissionDenied, \"cannot sync: blocked by sync window\")\n-\t}\n-\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionSync, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tsource := a.Spec.GetSource()\n-\n-\tif syncReq.Manifests != nil {\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionOverride, a.RBACName(s.ns)); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil && !syncReq.GetDryRun() {\n-\t\t\treturn nil, status.Error(codes.FailedPrecondition, \"cannot use local sync when Automatic Sync Policy is enabled unless for dry run\")\n-\t\t}\n-\t}\n-\tif a.DeletionTimestamp != nil {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"application is deleting\")\n-\t}\n-\tif a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil && !syncReq.GetDryRun() {\n-\t\tif syncReq.GetRevision() != \"\" && syncReq.GetRevision() != text.FirstNonEmpty(source.TargetRevision, \"HEAD\") {\n-\t\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"Cannot sync to %s: auto-sync currently set to %s\", syncReq.GetRevision(), source.TargetRevision)\n-\t\t}\n-\t}\n-\trevision, displayRevision, err := s.resolveRevision(ctx, a, syncReq)\n-\tif err != nil {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, err.Error())\n-\t}\n-\n-\tvar retry *appv1.RetryStrategy\n-\tvar syncOptions appv1.SyncOptions\n-\tif a.Spec.SyncPolicy != nil {\n-\t\tsyncOptions = a.Spec.SyncPolicy.SyncOptions\n-\t\tretry = a.Spec.SyncPolicy.Retry\n-\t}\n-\tif syncReq.RetryStrategy != nil {\n-\t\tretry = syncReq.RetryStrategy\n-\t}\n-\tif syncReq.SyncOptions != nil {\n-\t\tsyncOptions = syncReq.SyncOptions.Items\n-\t}\n-\n-\t// We cannot use local manifests if we're only allowed to sync to signed commits\n-\tif syncReq.Manifests != nil && len(proj.Spec.SignatureKeys) > 0 {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"Cannot use local sync when signature keys are required.\")\n-\t}\n-\n-\tresources := []appv1.SyncOperationResource{}\n-\tif syncReq.GetResources() != nil {\n-\t\tfor _, r := range syncReq.GetResources() {\n-\t\t\tif r != nil {\n-\t\t\t\tresources = append(resources, *r)\n-\t\t\t}\n-\t\t}\n-\t}\n-\top := appv1.Operation{\n-\t\tSync: &appv1.SyncOperation{\n-\t\t\tRevision:     revision,\n-\t\t\tPrune:        syncReq.GetPrune(),\n-\t\t\tDryRun:       syncReq.GetDryRun(),\n-\t\t\tSyncOptions:  syncOptions,\n-\t\t\tSyncStrategy: syncReq.Strategy,\n-\t\t\tResources:    resources,\n-\t\t\tManifests:    syncReq.Manifests,\n-\t\t},\n-\t\tInitiatedBy: appv1.OperationInitiator{Username: session.Username(ctx)},\n-\t\tInfo:        syncReq.Infos,\n-\t}\n-\tif retry != nil {\n-\t\top.Retry = *retry\n-\t}\n-\n-\tappName := syncReq.GetName()\n-\tappNs := s.appNamespaceOrDefault(syncReq.GetAppNamespace())\n-\tappIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n-\ta, err = argo.SetAppOperation(appIf, appName, &op)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error setting app operation: %w\", err)\n-\t}\n-\tpartial := \"\"\n-\tif len(syncReq.Resources) > 0 {\n-\t\tpartial = \"partial \"\n-\t}\n-\treason := fmt.Sprintf(\"initiated %ssync to %s\", partial, displayRevision)\n-\tif syncReq.Manifests != nil {\n-\t\treason = fmt.Sprintf(\"initiated %ssync locally\", partial)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonOperationStarted, reason)\n-\treturn a, nil\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, syncReq.GetProject(), syncReq.GetAppNamespace(), syncReq.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        return a, status.Errorf(codes.InvalidArgument, \"application references project %s which does not exist\", a.Spec.Project)\n+                }\n+                return a, fmt.Errorf(\"error getting app project: %w\", err)\n+        }\n+\n+        s.inferResourcesStatusHealth(a)\n+\n+        if !proj.Spec.SyncWindows.Matches(a).CanSync(true) {\n+                return a, status.Errorf(codes.PermissionDenied, \"cannot sync: blocked by sync window\")\n+        }\n+\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionSync, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        source := a.Spec.GetSource()\n+\n+        if syncReq.Manifests != nil {\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionOverride, a.RBACName(s.ns)); err != nil {\n+                        return nil, err\n+                }\n+                if a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil && !syncReq.GetDryRun() {\n+                        return nil, status.Error(codes.FailedPrecondition, \"cannot use local sync when Automatic Sync Policy is enabled unless for dry run\")\n+                }\n+        }\n+        if a.DeletionTimestamp != nil {\n+                return nil, status.Errorf(codes.FailedPrecondition, \"application is deleting\")\n+        }\n+        if a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil && !syncReq.GetDryRun() {\n+                if syncReq.GetRevision() != \"\" && syncReq.GetRevision() != text.FirstNonEmpty(source.TargetRevision, \"HEAD\") {\n+                        return nil, status.Errorf(codes.FailedPrecondition, \"Cannot sync to %s: auto-sync currently set to %s\", syncReq.GetRevision(), source.TargetRevision)\n+                }\n+        }\n+        revision, displayRevision, err := s.resolveRevision(ctx, a, syncReq)\n+        if err != nil {\n+                return nil, status.Errorf(codes.FailedPrecondition, err.Error())\n+        }\n+\n+        var retry *appv1.RetryStrategy\n+        var syncOptions appv1.SyncOptions\n+        if a.Spec.SyncPolicy != nil {\n+                syncOptions = a.Spec.SyncPolicy.SyncOptions\n+                retry = a.Spec.SyncPolicy.Retry\n+        }\n+        if syncReq.RetryStrategy != nil {\n+                retry = syncReq.RetryStrategy\n+        }\n+        if syncReq.SyncOptions != nil {\n+                syncOptions = syncReq.SyncOptions.Items\n+        }\n+\n+        // We cannot use local manifests if we're only allowed to sync to signed commits\n+        if syncReq.Manifests != nil && len(proj.Spec.SignatureKeys) > 0 {\n+                return nil, status.Errorf(codes.FailedPrecondition, \"Cannot use local sync when signature keys are required.\")\n+        }\n+\n+        resources := []appv1.SyncOperationResource{}\n+        if syncReq.GetResources() != nil {\n+                for _, r := range syncReq.GetResources() {\n+                        if r != nil {\n+                                resources = append(resources, *r)\n+                        }\n+                }\n+        }\n+        op := appv1.Operation{\n+                Sync: &appv1.SyncOperation{\n+                        Revision:     revision,\n+                        Prune:        syncReq.GetPrune(),\n+                        DryRun:       syncReq.GetDryRun(),\n+                        SyncOptions:  syncOptions,\n+                        SyncStrategy: syncReq.Strategy,\n+                        Resources:    resources,\n+                        Manifests:    syncReq.Manifests,\n+                },\n+                InitiatedBy: appv1.OperationInitiator{Username: session.Username(ctx)},\n+                Info:        syncReq.Infos,\n+        }\n+        if retry != nil {\n+                op.Retry = *retry\n+        }\n+\n+        appName := syncReq.GetName()\n+        appNs := s.appNamespaceOrDefault(syncReq.GetAppNamespace())\n+        appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n+        a, err = argo.SetAppOperation(appIf, appName, &op)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error setting app operation: %w\", err)\n+        }\n+        partial := \"\"\n+        if len(syncReq.Resources) > 0 {\n+                partial = \"partial \"\n+        }\n+        reason := fmt.Sprintf(\"initiated %ssync to %s\", partial, displayRevision)\n+        if syncReq.Manifests != nil {\n+                reason = fmt.Sprintf(\"initiated %ssync locally\", partial)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonOperationStarted, reason)\n+        return a, nil\n }\n \n func (s *Server) Rollback(ctx context.Context, rollbackReq *application.ApplicationRollbackRequest) (*appv1.Application, error) {\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, rollbackReq.GetProject(), rollbackReq.GetAppNamespace(), rollbackReq.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.inferResourcesStatusHealth(a)\n-\n-\tif a.DeletionTimestamp != nil {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"application is deleting\")\n-\t}\n-\tif a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"rollback cannot be initiated when auto-sync is enabled\")\n-\t}\n-\n-\tvar deploymentInfo *appv1.RevisionHistory\n-\tfor _, info := range a.Status.History {\n-\t\tif info.ID == rollbackReq.GetId() {\n-\t\t\tdeploymentInfo = &info\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\tif deploymentInfo == nil {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"application %s does not have deployment with id %v\", a.QualifiedName(), rollbackReq.GetId())\n-\t}\n-\tif deploymentInfo.Source.IsZero() {\n-\t\t// Since source type was introduced to history starting with v0.12, and is now required for\n-\t\t// rollback, we cannot support rollback to revisions deployed using Argo CD v0.11 or below\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"cannot rollback to revision deployed with Argo CD v0.11 or lower. sync to revision instead.\")\n-\t}\n-\n-\tvar syncOptions appv1.SyncOptions\n-\tif a.Spec.SyncPolicy != nil {\n-\t\tsyncOptions = a.Spec.SyncPolicy.SyncOptions\n-\t}\n-\n-\t// Rollback is just a convenience around Sync\n-\top := appv1.Operation{\n-\t\tSync: &appv1.SyncOperation{\n-\t\t\tRevision:     deploymentInfo.Revision,\n-\t\t\tDryRun:       rollbackReq.GetDryRun(),\n-\t\t\tPrune:        rollbackReq.GetPrune(),\n-\t\t\tSyncOptions:  syncOptions,\n-\t\t\tSyncStrategy: &appv1.SyncStrategy{Apply: &appv1.SyncStrategyApply{}},\n-\t\t\tSource:       &deploymentInfo.Source,\n-\t\t},\n-\t\tInitiatedBy: appv1.OperationInitiator{Username: session.Username(ctx)},\n-\t}\n-\tappName := rollbackReq.GetName()\n-\tappNs := s.appNamespaceOrDefault(rollbackReq.GetAppNamespace())\n-\tappIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n-\ta, err = argo.SetAppOperation(appIf, appName, &op)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error setting app operation: %w\", err)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonOperationStarted, fmt.Sprintf(\"initiated rollback to %d\", rollbackReq.GetId()))\n-\treturn a, nil\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, rollbackReq.GetProject(), rollbackReq.GetAppNamespace(), rollbackReq.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        s.inferResourcesStatusHealth(a)\n+\n+        if a.DeletionTimestamp != nil {\n+                return nil, status.Errorf(codes.FailedPrecondition, \"application is deleting\")\n+        }\n+        if a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil {\n+                return nil, status.Errorf(codes.FailedPrecondition, \"rollback cannot be initiated when auto-sync is enabled\")\n+        }\n+\n+        var deploymentInfo *appv1.RevisionHistory\n+        for _, info := range a.Status.History {\n+                if info.ID == rollbackReq.GetId() {\n+                        deploymentInfo = &info\n+                        break\n+                }\n+        }\n+        if deploymentInfo == nil {\n+                return nil, status.Errorf(codes.InvalidArgument, \"application %s does not have deployment with id %v\", a.QualifiedName(), rollbackReq.GetId())\n+        }\n+        if deploymentInfo.Source.IsZero() {\n+                // Since source type was introduced to history starting with v0.12, and is now required for\n+                // rollback, we cannot support rollback to revisions deployed using Argo CD v0.11 or below\n+                return nil, status.Errorf(codes.FailedPrecondition, \"cannot rollback to revision deployed with Argo CD v0.11 or lower. sync to revision instead.\")\n+        }\n+\n+        var syncOptions appv1.SyncOptions\n+        if a.Spec.SyncPolicy != nil {\n+                syncOptions = a.Spec.SyncPolicy.SyncOptions\n+        }\n+\n+        // Rollback is just a convenience around Sync\n+        op := appv1.Operation{\n+                Sync: &appv1.SyncOperation{\n+                        Revision:     deploymentInfo.Revision,\n+                        DryRun:       rollbackReq.GetDryRun(),\n+                        Prune:        rollbackReq.GetPrune(),\n+                        SyncOptions:  syncOptions,\n+                        SyncStrategy: &appv1.SyncStrategy{Apply: &appv1.SyncStrategyApply{}},\n+                        Source:       &deploymentInfo.Source,\n+                },\n+                InitiatedBy: appv1.OperationInitiator{Username: session.Username(ctx)},\n+        }\n+        appName := rollbackReq.GetName()\n+        appNs := s.appNamespaceOrDefault(rollbackReq.GetAppNamespace())\n+        appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n+        a, err = argo.SetAppOperation(appIf, appName, &op)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error setting app operation: %w\", err)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonOperationStarted, fmt.Sprintf(\"initiated rollback to %d\", rollbackReq.GetId()))\n+        return a, nil\n }\n \n func (s *Server) ListLinks(ctx context.Context, req *application.ListAppLinksRequest) (*application.LinksResponse, error) {\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, req.GetProject(), req.GetNamespace(), req.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tobj, err := kube.ToUnstructured(a)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting application: %w\", err)\n-\t}\n-\n-\tdeepLinks, err := s.settingsMgr.GetDeepLinks(settings.ApplicationDeepLinks)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to read application deep links from configmap: %w\", err)\n-\t}\n-\n-\tclstObj, _, err := s.getObjectsForDeepLinks(ctx, a)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tdeepLinksObject := deeplinks.CreateDeepLinksObject(nil, obj, clstObj, nil)\n-\n-\tfinalList, errorList := deeplinks.EvaluateDeepLinksResponse(deepLinksObject, obj.GetName(), deepLinks)\n-\tif len(errorList) > 0 {\n-\t\tlog.Errorf(\"errorList while evaluating application deep links, %v\", strings.Join(errorList, \", \"))\n-\t}\n-\n-\treturn finalList, nil\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, req.GetProject(), req.GetNamespace(), req.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        obj, err := kube.ToUnstructured(a)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting application: %w\", err)\n+        }\n+\n+        deepLinks, err := s.settingsMgr.GetDeepLinks(settings.ApplicationDeepLinks)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to read application deep links from configmap: %w\", err)\n+        }\n+\n+        clstObj, _, err := s.getObjectsForDeepLinks(ctx, a)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        deepLinksObject := deeplinks.CreateDeepLinksObject(nil, obj, clstObj, nil)\n+\n+        finalList, errorList := deeplinks.EvaluateDeepLinksResponse(deepLinksObject, obj.GetName(), deepLinks)\n+        if len(errorList) > 0 {\n+                log.Errorf(\"errorList while evaluating application deep links, %v\", strings.Join(errorList, \", \"))\n+        }\n+\n+        return finalList, nil\n }\n \n func (s *Server) getObjectsForDeepLinks(ctx context.Context, app *appv1.Application) (cluster *unstructured.Unstructured, project *unstructured.Unstructured, err error) {\n-\tproj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\treturn nil, nil, fmt.Errorf(\"error getting app project: %w\", err)\n-\t}\n-\n-\t// sanitize project jwt tokens\n-\tproj.Status = appv1.AppProjectStatus{}\n-\n-\tproject, err = kube.ToUnstructured(proj)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\n-\tgetProjectClusters := func(project string) ([]*appv1.Cluster, error) {\n-\t\treturn s.db.GetProjectClusters(ctx, project)\n-\t}\n-\n-\tif err := argo.ValidateDestination(ctx, &app.Spec.Destination, s.db); err != nil {\n-\t\tlog.WithFields(map[string]interface{}{\n-\t\t\t\"application\": app.GetName(),\n-\t\t\t\"ns\":          app.GetNamespace(),\n-\t\t\t\"destination\": app.Spec.Destination,\n-\t\t}).Warnf(\"cannot validate cluster, error=%v\", err.Error())\n-\t\treturn nil, nil, nil\n-\t}\n-\n-\tpermitted, err := proj.IsDestinationPermitted(app.Spec.Destination, getProjectClusters)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\tif !permitted {\n-\t\treturn nil, nil, fmt.Errorf(\"error getting destination cluster\")\n-\t}\n-\tclst, err := s.db.GetCluster(ctx, app.Spec.Destination.Server)\n-\tif err != nil {\n-\t\tlog.WithFields(map[string]interface{}{\n-\t\t\t\"application\": app.GetName(),\n-\t\t\t\"ns\":          app.GetNamespace(),\n-\t\t\t\"destination\": app.Spec.Destination,\n-\t\t}).Warnf(\"cannot get cluster from db, error=%v\", err.Error())\n-\t\treturn nil, nil, nil\n-\t}\n-\t// sanitize cluster, remove cluster config creds and other unwanted fields\n-\tcluster, err = deeplinks.SanitizeCluster(clst)\n-\treturn cluster, project, err\n+        proj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                return nil, nil, fmt.Errorf(\"error getting app project: %w\", err)\n+        }\n+\n+        // sanitize project jwt tokens\n+        proj.Status = appv1.AppProjectStatus{}\n+\n+        project, err = kube.ToUnstructured(proj)\n+        if err != nil {\n+                return nil, nil, err\n+        }\n+\n+        getProjectClusters := func(project string) ([]*appv1.Cluster, error) {\n+                return s.db.GetProjectClusters(ctx, project)\n+        }\n+\n+        if err := argo.ValidateDestination(ctx, &app.Spec.Destination, s.db); err != nil {\n+                log.WithFields(map[string]interface{}{\n+                        \"application\": app.GetName(),\n+                        \"ns\":          app.GetNamespace(),\n+                        \"destination\": app.Spec.Destination,\n+                }).Warnf(\"cannot validate cluster, error=%v\", err.Error())\n+                return nil, nil, nil\n+        }\n+\n+        permitted, err := proj.IsDestinationPermitted(app.Spec.Destination, getProjectClusters)\n+        if err != nil {\n+                return nil, nil, err\n+        }\n+        if !permitted {\n+                return nil, nil, fmt.Errorf(\"error getting destination cluster\")\n+        }\n+        clst, err := s.db.GetCluster(ctx, app.Spec.Destination.Server)\n+        if err != nil {\n+                log.WithFields(map[string]interface{}{\n+                        \"application\": app.GetName(),\n+                        \"ns\":          app.GetNamespace(),\n+                        \"destination\": app.Spec.Destination,\n+                }).Warnf(\"cannot get cluster from db, error=%v\", err.Error())\n+                return nil, nil, nil\n+        }\n+        // sanitize cluster, remove cluster config creds and other unwanted fields\n+        cluster, err = deeplinks.SanitizeCluster(clst)\n+        return cluster, project, err\n }\n \n func (s *Server) ListResourceLinks(ctx context.Context, req *application.ApplicationResourceRequest) (*application.LinksResponse, error) {\n-\tobj, _, app, _, err := s.getUnstructuredLiveResourceOrApp(ctx, rbacpolicy.ActionGet, req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tdeepLinks, err := s.settingsMgr.GetDeepLinks(settings.ResourceDeepLinks)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to read application deep links from configmap: %w\", err)\n-\t}\n-\n-\tobj, err = replaceSecretValues(obj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n-\t}\n-\n-\tappObj, err := kube.ToUnstructured(app)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclstObj, projObj, err := s.getObjectsForDeepLinks(ctx, app)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tdeepLinksObject := deeplinks.CreateDeepLinksObject(obj, appObj, clstObj, projObj)\n-\tfinalList, errorList := deeplinks.EvaluateDeepLinksResponse(deepLinksObject, obj.GetName(), deepLinks)\n-\tif len(errorList) > 0 {\n-\t\tlog.Errorf(\"errors while evaluating resource deep links, %v\", strings.Join(errorList, \", \"))\n-\t}\n-\n-\treturn finalList, nil\n+        obj, _, app, _, err := s.getUnstructuredLiveResourceOrApp(ctx, rbacpolicy.ActionGet, req)\n+        if err != nil {\n+                return nil, err\n+        }\n+        deepLinks, err := s.settingsMgr.GetDeepLinks(settings.ResourceDeepLinks)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to read application deep links from configmap: %w\", err)\n+        }\n+\n+        obj, err = replaceSecretValues(obj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n+        }\n+\n+        appObj, err := kube.ToUnstructured(app)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        clstObj, projObj, err := s.getObjectsForDeepLinks(ctx, app)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        deepLinksObject := deeplinks.CreateDeepLinksObject(obj, appObj, clstObj, projObj)\n+        finalList, errorList := deeplinks.EvaluateDeepLinksResponse(deepLinksObject, obj.GetName(), deepLinks)\n+        if len(errorList) > 0 {\n+                log.Errorf(\"errors while evaluating resource deep links, %v\", strings.Join(errorList, \", \"))\n+        }\n+\n+        return finalList, nil\n }\n \n // resolveRevision resolves the revision specified either in the sync request, or the\n // application source, into a concrete revision that will be used for a sync operation.\n func (s *Server) resolveRevision(ctx context.Context, app *appv1.Application, syncReq *application.ApplicationSyncRequest) (string, string, error) {\n-\tif syncReq.Manifests != nil {\n-\t\treturn \"\", \"\", nil\n-\t}\n-\tambiguousRevision := syncReq.GetRevision()\n-\tif ambiguousRevision == \"\" {\n-\t\tambiguousRevision = app.Spec.GetSource().TargetRevision\n-\t}\n-\trepo, err := s.db.GetRepository(ctx, app.Spec.GetSource().RepoURL)\n-\tif err != nil {\n-\t\treturn \"\", \"\", fmt.Errorf(\"error getting repository by URL: %w\", err)\n-\t}\n-\tconn, repoClient, err := s.repoClientset.NewRepoServerClient()\n-\tif err != nil {\n-\t\treturn \"\", \"\", fmt.Errorf(\"error getting repo server client: %w\", err)\n-\t}\n-\tdefer ioutil.Close(conn)\n-\n-\tsource := app.Spec.GetSource()\n-\tif !source.IsHelm() {\n-\t\tif git.IsCommitSHA(ambiguousRevision) {\n-\t\t\t// If it's already a commit SHA, then no need to look it up\n-\t\t\treturn ambiguousRevision, ambiguousRevision, nil\n-\t\t}\n-\t}\n-\n-\tresolveRevisionResponse, err := repoClient.ResolveRevision(ctx, &apiclient.ResolveRevisionRequest{\n-\t\tRepo:              repo,\n-\t\tApp:               app,\n-\t\tAmbiguousRevision: ambiguousRevision,\n-\t})\n-\tif err != nil {\n-\t\treturn \"\", \"\", fmt.Errorf(\"error resolving repo revision: %w\", err)\n-\t}\n-\treturn resolveRevisionResponse.Revision, resolveRevisionResponse.AmbiguousRevision, nil\n+        if syncReq.Manifests != nil {\n+                return \"\", \"\", nil\n+        }\n+        ambiguousRevision := syncReq.GetRevision()\n+        if ambiguousRevision == \"\" {\n+                ambiguousRevision = app.Spec.GetSource().TargetRevision\n+        }\n+        repo, err := s.db.GetRepository(ctx, app.Spec.GetSource().RepoURL)\n+        if err != nil {\n+                return \"\", \"\", fmt.Errorf(\"error getting repository by URL: %w\", err)\n+        }\n+        conn, repoClient, err := s.repoClientset.NewRepoServerClient()\n+        if err != nil {\n+                return \"\", \"\", fmt.Errorf(\"error getting repo server client: %w\", err)\n+        }\n+        defer ioutil.Close(conn)\n+\n+        source := app.Spec.GetSource()\n+        if !source.IsHelm() {\n+                if git.IsCommitSHA(ambiguousRevision) {\n+                        // If it's already a commit SHA, then no need to look it up\n+                        return ambiguousRevision, ambiguousRevision, nil\n+                }\n+        }\n+\n+        resolveRevisionResponse, err := repoClient.ResolveRevision(ctx, &apiclient.ResolveRevisionRequest{\n+                Repo:              repo,\n+                App:               app,\n+                AmbiguousRevision: ambiguousRevision,\n+        })\n+        if err != nil {\n+                return \"\", \"\", fmt.Errorf(\"error resolving repo revision: %w\", err)\n+        }\n+        return resolveRevisionResponse.Revision, resolveRevisionResponse.AmbiguousRevision, nil\n }\n \n func (s *Server) TerminateOperation(ctx context.Context, termOpReq *application.OperationTerminateRequest) (*application.OperationTerminateResponse, error) {\n-\tappName := termOpReq.GetName()\n-\tappNs := s.appNamespaceOrDefault(termOpReq.GetAppNamespace())\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, termOpReq.GetProject(), appNs, appName, \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor i := 0; i < 10; i++ {\n-\t\tif a.Operation == nil || a.Status.OperationState == nil {\n-\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"Unable to terminate operation. No operation is in progress\")\n-\t\t}\n-\t\ta.Status.OperationState.Phase = common.OperationTerminating\n-\t\tupdated, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Update(ctx, a, metav1.UpdateOptions{})\n-\t\tif err == nil {\n-\t\t\ts.waitSync(updated)\n-\t\t\ts.logAppEvent(a, ctx, argo.EventReasonResourceUpdated, \"terminated running operation\")\n-\t\t\treturn &application.OperationTerminateResponse{}, nil\n-\t\t}\n-\t\tif !apierr.IsConflict(err) {\n-\t\t\treturn nil, fmt.Errorf(\"error updating application: %w\", err)\n-\t\t}\n-\t\tlog.Warnf(\"failed to set operation for app %q due to update conflict. retrying again...\", *termOpReq.Name)\n-\t\ttime.Sleep(100 * time.Millisecond)\n-\t\ta, err = s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error getting application by name: %w\", err)\n-\t\t}\n-\t}\n-\treturn nil, status.Errorf(codes.Internal, \"Failed to terminate app. Too many conflicts\")\n+        appName := termOpReq.GetName()\n+        appNs := s.appNamespaceOrDefault(termOpReq.GetAppNamespace())\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, termOpReq.GetProject(), appNs, appName, \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for i := 0; i < 10; i++ {\n+                if a.Operation == nil || a.Status.OperationState == nil {\n+                        return nil, status.Errorf(codes.InvalidArgument, \"Unable to terminate operation. No operation is in progress\")\n+                }\n+                a.Status.OperationState.Phase = common.OperationTerminating\n+                updated, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Update(ctx, a, metav1.UpdateOptions{})\n+                if err == nil {\n+                        s.waitSync(updated)\n+                        s.logAppEvent(a, ctx, argo.EventReasonResourceUpdated, \"terminated running operation\")\n+                        return &application.OperationTerminateResponse{}, nil\n+                }\n+                if !apierr.IsConflict(err) {\n+                        return nil, fmt.Errorf(\"error updating application: %w\", err)\n+                }\n+                log.Warnf(\"failed to set operation for app %q due to update conflict. retrying again...\", *termOpReq.Name)\n+                time.Sleep(100 * time.Millisecond)\n+                a, err = s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error getting application by name: %w\", err)\n+                }\n+        }\n+        return nil, status.Errorf(codes.Internal, \"Failed to terminate app. Too many conflicts\")\n }\n \n func (s *Server) logAppEvent(a *appv1.Application, ctx context.Context, reason string, action string) {\n-\teventInfo := argo.EventInfo{Type: v1.EventTypeNormal, Reason: reason}\n-\tuser := session.Username(ctx)\n-\tif user == \"\" {\n-\t\tuser = \"Unknown user\"\n-\t}\n-\tmessage := fmt.Sprintf(\"%s %s\", user, action)\n-\ts.auditLogger.LogAppEvent(a, eventInfo, message, user)\n+        eventInfo := argo.EventInfo{Type: v1.EventTypeNormal, Reason: reason}\n+        user := session.Username(ctx)\n+        if user == \"\" {\n+                user = \"Unknown user\"\n+        }\n+        message := fmt.Sprintf(\"%s %s\", user, action)\n+        s.auditLogger.LogAppEvent(a, eventInfo, message, user)\n }\n \n func (s *Server) logResourceEvent(res *appv1.ResourceNode, ctx context.Context, reason string, action string) {\n-\teventInfo := argo.EventInfo{Type: v1.EventTypeNormal, Reason: reason}\n-\tuser := session.Username(ctx)\n-\tif user == \"\" {\n-\t\tuser = \"Unknown user\"\n-\t}\n-\tmessage := fmt.Sprintf(\"%s %s\", user, action)\n-\ts.auditLogger.LogResourceEvent(res, eventInfo, message, user)\n+        eventInfo := argo.EventInfo{Type: v1.EventTypeNormal, Reason: reason}\n+        user := session.Username(ctx)\n+        if user == \"\" {\n+                user = \"Unknown user\"\n+        }\n+        message := fmt.Sprintf(\"%s %s\", user, action)\n+        s.auditLogger.LogResourceEvent(res, eventInfo, message, user)\n }\n \n func (s *Server) ListResourceActions(ctx context.Context, q *application.ApplicationResourceRequest) (*application.ResourceActionsListResponse, error) {\n-\tobj, _, _, _, err := s.getUnstructuredLiveResourceOrApp(ctx, rbacpolicy.ActionGet, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tresourceOverrides, err := s.settingsMgr.GetResourceOverrides()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting resource overrides: %w\", err)\n-\t}\n-\n-\tavailableActions, err := s.getAvailableActions(resourceOverrides, obj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting available actions: %w\", err)\n-\t}\n-\tactionsPtr := []*appv1.ResourceAction{}\n-\tfor i := range availableActions {\n-\t\tactionsPtr = append(actionsPtr, &availableActions[i])\n-\t}\n-\n-\treturn &application.ResourceActionsListResponse{Actions: actionsPtr}, nil\n+        obj, _, _, _, err := s.getUnstructuredLiveResourceOrApp(ctx, rbacpolicy.ActionGet, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+        resourceOverrides, err := s.settingsMgr.GetResourceOverrides()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting resource overrides: %w\", err)\n+        }\n+\n+        availableActions, err := s.getAvailableActions(resourceOverrides, obj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting available actions: %w\", err)\n+        }\n+        actionsPtr := []*appv1.ResourceAction{}\n+        for i := range availableActions {\n+                actionsPtr = append(actionsPtr, &availableActions[i])\n+        }\n+\n+        return &application.ResourceActionsListResponse{Actions: actionsPtr}, nil\n }\n \n func (s *Server) getUnstructuredLiveResourceOrApp(ctx context.Context, rbacRequest string, q *application.ApplicationResourceRequest) (obj *unstructured.Unstructured, res *appv1.ResourceNode, app *appv1.Application, config *rest.Config, err error) {\n-\tif q.GetKind() == applicationType.ApplicationKind && q.GetGroup() == applicationType.Group && q.GetName() == q.GetResourceName() {\n-\t\tapp, err = s.getApplicationEnforceRBACInformer(ctx, rbacRequest, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, nil, nil, err\n-\t\t}\n-\t\tif err = s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacRequest, app.RBACName(s.ns)); err != nil {\n-\t\t\treturn nil, nil, nil, nil, err\n-\t\t}\n-\t\tconfig, err = s.getApplicationClusterConfig(ctx, app)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, nil, nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t\t}\n-\t\tobj, err = kube.ToUnstructured(app)\n-\t} else {\n-\t\tres, config, app, err = s.getAppLiveResource(ctx, rbacRequest, q)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, nil, nil, err\n-\t\t}\n-\t\tobj, err = s.kubectl.GetResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace)\n-\n-\t}\n-\tif err != nil {\n-\t\treturn nil, nil, nil, nil, fmt.Errorf(\"error getting resource: %w\", err)\n-\t}\n-\treturn\n+        if q.GetKind() == applicationType.ApplicationKind && q.GetGroup() == applicationType.Group && q.GetName() == q.GetResourceName() {\n+                app, err = s.getApplicationEnforceRBACInformer(ctx, rbacRequest, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+                if err != nil {\n+                        return nil, nil, nil, nil, err\n+                }\n+                if err = s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacRequest, app.RBACName(s.ns)); err != nil {\n+                        return nil, nil, nil, nil, err\n+                }\n+                config, err = s.getApplicationClusterConfig(ctx, app)\n+                if err != nil {\n+                        return nil, nil, nil, nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n+                }\n+                obj, err = kube.ToUnstructured(app)\n+        } else {\n+                res, config, app, err = s.getAppLiveResource(ctx, rbacRequest, q)\n+                if err != nil {\n+                        return nil, nil, nil, nil, err\n+                }\n+                obj, err = s.kubectl.GetResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace)\n+\n+        }\n+        if err != nil {\n+                return nil, nil, nil, nil, fmt.Errorf(\"error getting resource: %w\", err)\n+        }\n+        return\n }\n \n func (s *Server) getAvailableActions(resourceOverrides map[string]appv1.ResourceOverride, obj *unstructured.Unstructured) ([]appv1.ResourceAction, error) {\n-\tluaVM := lua.VM{\n-\t\tResourceOverrides: resourceOverrides,\n-\t}\n-\n-\tdiscoveryScript, err := luaVM.GetResourceActionDiscovery(obj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting Lua discovery script: %w\", err)\n-\t}\n-\tif discoveryScript == \"\" {\n-\t\treturn []appv1.ResourceAction{}, nil\n-\t}\n-\tavailableActions, err := luaVM.ExecuteResourceActionDiscovery(obj, discoveryScript)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error executing Lua discovery script: %w\", err)\n-\t}\n-\treturn availableActions, nil\n+        luaVM := lua.VM{\n+                ResourceOverrides: resourceOverrides,\n+        }\n+\n+        discoveryScript, err := luaVM.GetResourceActionDiscovery(obj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting Lua discovery script: %w\", err)\n+        }\n+        if discoveryScript == \"\" {\n+                return []appv1.ResourceAction{}, nil\n+        }\n+        availableActions, err := luaVM.ExecuteResourceActionDiscovery(obj, discoveryScript)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error executing Lua discovery script: %w\", err)\n+        }\n+        return availableActions, nil\n \n }\n \n func (s *Server) RunResourceAction(ctx context.Context, q *application.ResourceActionRunRequest) (*application.ApplicationResponse, error) {\n-\tresourceRequest := &application.ApplicationResourceRequest{\n-\t\tName:         q.Name,\n-\t\tAppNamespace: q.AppNamespace,\n-\t\tNamespace:    q.Namespace,\n-\t\tResourceName: q.ResourceName,\n-\t\tKind:         q.Kind,\n-\t\tVersion:      q.Version,\n-\t\tGroup:        q.Group,\n-\t\tProject:      q.Project,\n-\t}\n-\tactionRequest := fmt.Sprintf(\"%s/%s/%s/%s\", rbacpolicy.ActionAction, q.GetGroup(), q.GetKind(), q.GetAction())\n-\tliveObj, res, a, config, err := s.getUnstructuredLiveResourceOrApp(ctx, actionRequest, resourceRequest)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tliveObjBytes, err := json.Marshal(liveObj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error marshaling live object: %w\", err)\n-\t}\n-\n-\tresourceOverrides, err := s.settingsMgr.GetResourceOverrides()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting resource overrides: %w\", err)\n-\t}\n-\n-\tluaVM := lua.VM{\n-\t\tResourceOverrides: resourceOverrides,\n-\t}\n-\taction, err := luaVM.GetResourceAction(liveObj, q.GetAction())\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting Lua resource action: %w\", err)\n-\t}\n-\n-\tnewObjects, err := luaVM.ExecuteResourceAction(liveObj, action.ActionLua)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error executing Lua resource action: %w\", err)\n-\t}\n-\n-\tvar app *appv1.Application\n-\t// Only bother getting the app if we know we're going to need it for a resource permission check.\n-\tif len(newObjects) > 0 {\n-\t\t// No need for an RBAC check, we checked above that the user is allowed to run this action.\n-\t\tapp, err = s.appLister.Applications(s.appNamespaceOrDefault(q.GetAppNamespace())).Get(q.GetName())\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\t// First, make sure all the returned resources are permitted, for each operation.\n-\t// Also perform create with dry-runs for all create-operation resources.\n-\t// This is performed separately to reduce the risk of only some of the resources being successfully created later.\n-\t// TODO: when apply/delete operations would be supported for custom actions,\n-\t// the dry-run for relevant apply/delete operation would have to be invoked as well.\n-\tfor _, impactedResource := range newObjects {\n-\t\tnewObj := impactedResource.UnstructuredObj\n-\t\terr := s.verifyResourcePermitted(ctx, app, newObj)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tswitch impactedResource.K8SOperation {\n-\t\tcase lua.CreateOperation:\n-\t\t\tcreateOptions := metav1.CreateOptions{DryRun: []string{\"All\"}}\n-\t\t\t_, err := s.kubectl.CreateResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), newObj, createOptions)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Now, perform the actual operations.\n-\t// The creation itself is not transactional.\n-\t// TODO: maybe create a k8s list representation of the resources,\n-\t// and invoke create on this list resource to make it semi-transactional (there is still patch operation that is separate,\n-\t// thus can fail separately from create).\n-\tfor _, impactedResource := range newObjects {\n-\t\tnewObj := impactedResource.UnstructuredObj\n-\t\tnewObjBytes, err := json.Marshal(newObj)\n-\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error marshaling new object: %w\", err)\n-\t\t}\n-\n-\t\tswitch impactedResource.K8SOperation {\n-\t\t// No default case since a not supported operation would have failed upon unmarshaling earlier\n-\t\tcase lua.PatchOperation:\n-\t\t\t_, err := s.patchResource(ctx, config, liveObjBytes, newObjBytes, newObj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\tcase lua.CreateOperation:\n-\t\t\t_, err := s.createResource(ctx, config, newObj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif res == nil {\n-\t\ts.logAppEvent(a, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s\", q.GetAction()))\n-\t} else {\n-\t\ts.logAppEvent(a, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s on resource %s/%s/%s\", q.GetAction(), res.Group, res.Kind, res.Name))\n-\t\ts.logResourceEvent(res, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s\", q.GetAction()))\n-\t}\n-\treturn &application.ApplicationResponse{}, nil\n+        resourceRequest := &application.ApplicationResourceRequest{\n+                Name:         q.Name,\n+                AppNamespace: q.AppNamespace,\n+                Namespace:    q.Namespace,\n+                ResourceName: q.ResourceName,\n+                Kind:         q.Kind,\n+                Version:      q.Version,\n+                Group:        q.Group,\n+                Project:      q.Project,\n+        }\n+        actionRequest := fmt.Sprintf(\"%s/%s/%s/%s\", rbacpolicy.ActionAction, q.GetGroup(), q.GetKind(), q.GetAction())\n+        liveObj, res, a, config, err := s.getUnstructuredLiveResourceOrApp(ctx, actionRequest, resourceRequest)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        liveObjBytes, err := json.Marshal(liveObj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error marshaling live object: %w\", err)\n+        }\n+\n+        resourceOverrides, err := s.settingsMgr.GetResourceOverrides()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting resource overrides: %w\", err)\n+        }\n+\n+        luaVM := lua.VM{\n+                ResourceOverrides: resourceOverrides,\n+        }\n+        action, err := luaVM.GetResourceAction(liveObj, q.GetAction())\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting Lua resource action: %w\", err)\n+        }\n+\n+        newObjects, err := luaVM.ExecuteResourceAction(liveObj, action.ActionLua)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error executing Lua resource action: %w\", err)\n+        }\n+\n+        var app *appv1.Application\n+        // Only bother getting the app if we know we're going to need it for a resource permission check.\n+        if len(newObjects) > 0 {\n+                // No need for an RBAC check, we checked above that the user is allowed to run this action.\n+                app, err = s.appLister.Applications(s.appNamespaceOrDefault(q.GetAppNamespace())).Get(q.GetName())\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        // First, make sure all the returned resources are permitted, for each operation.\n+        // Also perform create with dry-runs for all create-operation resources.\n+        // This is performed separately to reduce the risk of only some of the resources being successfully created later.\n+        // TODO: when apply/delete operations would be supported for custom actions,\n+        // the dry-run for relevant apply/delete operation would have to be invoked as well.\n+        for _, impactedResource := range newObjects {\n+                newObj := impactedResource.UnstructuredObj\n+                err := s.verifyResourcePermitted(ctx, app, newObj)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                switch impactedResource.K8SOperation {\n+                case lua.CreateOperation:\n+                        createOptions := metav1.CreateOptions{DryRun: []string{\"All\"}}\n+                        _, err := s.kubectl.CreateResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), newObj, createOptions)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        }\n+\n+        // Now, perform the actual operations.\n+        // The creation itself is not transactional.\n+        // TODO: maybe create a k8s list representation of the resources,\n+        // and invoke create on this list resource to make it semi-transactional (there is still patch operation that is separate,\n+        // thus can fail separately from create).\n+        for _, impactedResource := range newObjects {\n+                newObj := impactedResource.UnstructuredObj\n+                newObjBytes, err := json.Marshal(newObj)\n+\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error marshaling new object: %w\", err)\n+                }\n+\n+                switch impactedResource.K8SOperation {\n+                // No default case since a not supported operation would have failed upon unmarshaling earlier\n+                case lua.PatchOperation:\n+                        _, err := s.patchResource(ctx, config, liveObjBytes, newObjBytes, newObj)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                case lua.CreateOperation:\n+                        _, err := s.createResource(ctx, config, newObj)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        }\n+\n+        if res == nil {\n+                s.logAppEvent(a, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s\", q.GetAction()))\n+        } else {\n+                s.logAppEvent(a, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s on resource %s/%s/%s\", q.GetAction(), res.Group, res.Kind, res.Name))\n+                s.logResourceEvent(res, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s\", q.GetAction()))\n+        }\n+        return &application.ApplicationResponse{}, nil\n }\n \n func (s *Server) patchResource(ctx context.Context, config *rest.Config, liveObjBytes, newObjBytes []byte, newObj *unstructured.Unstructured) (*application.ApplicationResponse, error) {\n-\tdiffBytes, err := jsonpatch.CreateMergePatch(liveObjBytes, newObjBytes)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error calculating merge patch: %w\", err)\n-\t}\n-\tif string(diffBytes) == \"{}\" {\n-\t\treturn &application.ApplicationResponse{}, nil\n-\t}\n-\n-\t// The following logic detects if the resource action makes a modification to status and/or spec.\n-\t// If status was modified, we attempt to patch the status using status subresource, in case the\n-\t// CRD is configured using the status subresource feature. See:\n-\t// https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource\n-\t// If status subresource is in use, the patch has to be split into two:\n-\t// * one to update spec (and other non-status fields)\n-\t// * the other to update only status.\n-\tnonStatusPatch, statusPatch, err := splitStatusPatch(diffBytes)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error splitting status patch: %w\", err)\n-\t}\n-\tif statusPatch != nil {\n-\t\t_, err = s.kubectl.PatchResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), types.MergePatchType, diffBytes, \"status\")\n-\t\tif err != nil {\n-\t\t\tif !apierr.IsNotFound(err) {\n-\t\t\t\treturn nil, fmt.Errorf(\"error patching resource: %w\", err)\n-\t\t\t}\n-\t\t\t// K8s API server returns 404 NotFound when the CRD does not support the status subresource\n-\t\t\t// if we get here, the CRD does not use the status subresource. We will fall back to a normal patch\n-\t\t} else {\n-\t\t\t// If we get here, the CRD does use the status subresource, so we must patch status and\n-\t\t\t// spec separately. update the diffBytes to the spec-only patch and fall through.\n-\t\t\tdiffBytes = nonStatusPatch\n-\t\t}\n-\t}\n-\tif diffBytes != nil {\n-\t\t_, err = s.kubectl.PatchResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), types.MergePatchType, diffBytes)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error patching resource: %w\", err)\n-\t\t}\n-\t}\n-\treturn &application.ApplicationResponse{}, nil\n+        diffBytes, err := jsonpatch.CreateMergePatch(liveObjBytes, newObjBytes)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error calculating merge patch: %w\", err)\n+        }\n+        if string(diffBytes) == \"{}\" {\n+                return &application.ApplicationResponse{}, nil\n+        }\n+\n+        // The following logic detects if the resource action makes a modification to status and/or spec.\n+        // If status was modified, we attempt to patch the status using status subresource, in case the\n+        // CRD is configured using the status subresource feature. See:\n+        // https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource\n+        // If status subresource is in use, the patch has to be split into two:\n+        // * one to update spec (and other non-status fields)\n+        // * the other to update only status.\n+        nonStatusPatch, statusPatch, err := splitStatusPatch(diffBytes)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error splitting status patch: %w\", err)\n+        }\n+        if statusPatch != nil {\n+                _, err = s.kubectl.PatchResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), types.MergePatchType, diffBytes, \"status\")\n+                if err != nil {\n+                        if !apierr.IsNotFound(err) {\n+                                return nil, fmt.Errorf(\"error patching resource: %w\", err)\n+                        }\n+                        // K8s API server returns 404 NotFound when the CRD does not support the status subresource\n+                        // if we get here, the CRD does not use the status subresource. We will fall back to a normal patch\n+                } else {\n+                        // If we get here, the CRD does use the status subresource, so we must patch status and\n+                        // spec separately. update the diffBytes to the spec-only patch and fall through.\n+                        diffBytes = nonStatusPatch\n+                }\n+        }\n+        if diffBytes != nil {\n+                _, err = s.kubectl.PatchResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), types.MergePatchType, diffBytes)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error patching resource: %w\", err)\n+                }\n+        }\n+        return &application.ApplicationResponse{}, nil\n }\n \n func (s *Server) verifyResourcePermitted(ctx context.Context, app *appv1.Application, obj *unstructured.Unstructured) error {\n-\tproj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\treturn fmt.Errorf(\"application references project %s which does not exist\", app.Spec.Project)\n-\t\t}\n-\t\treturn fmt.Errorf(\"failed to get project %s: %w\", app.Spec.Project, err)\n-\t}\n-\tpermitted, err := proj.IsResourcePermitted(schema.GroupKind{Group: obj.GroupVersionKind().Group, Kind: obj.GroupVersionKind().Kind}, obj.GetNamespace(), app.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n-\t\tclusters, err := s.db.GetProjectClusters(context.TODO(), project)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to get project clusters: %w\", err)\n-\t\t}\n-\t\treturn clusters, nil\n-\t})\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error checking resource permissions: %w\", err)\n-\t}\n-\tif !permitted {\n-\t\treturn fmt.Errorf(\"application %s is not permitted to manage %s/%s/%s in %s\", app.RBACName(s.ns), obj.GroupVersionKind().Group, obj.GroupVersionKind().Kind, obj.GetName(), obj.GetNamespace())\n-\t}\n-\n-\treturn nil\n+        proj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        return fmt.Errorf(\"application references project %s which does not exist\", app.Spec.Project)\n+                }\n+                return fmt.Errorf(\"failed to get project %s: %w\", app.Spec.Project, err)\n+        }\n+        permitted, err := proj.IsResourcePermitted(schema.GroupKind{Group: obj.GroupVersionKind().Group, Kind: obj.GroupVersionKind().Kind}, obj.GetNamespace(), app.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n+                clusters, err := s.db.GetProjectClusters(context.TODO(), project)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to get project clusters: %w\", err)\n+                }\n+                return clusters, nil\n+        })\n+        if err != nil {\n+                return fmt.Errorf(\"error checking resource permissions: %w\", err)\n+        }\n+        if !permitted {\n+                return fmt.Errorf(\"application %s is not permitted to manage %s/%s/%s in %s\", app.RBACName(s.ns), obj.GroupVersionKind().Group, obj.GroupVersionKind().Kind, obj.GetName(), obj.GetNamespace())\n+        }\n+\n+        return nil\n }\n \n func (s *Server) createResource(ctx context.Context, config *rest.Config, newObj *unstructured.Unstructured) (*application.ApplicationResponse, error) {\n-\t_, err := s.kubectl.CreateResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), newObj, metav1.CreateOptions{})\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error creating resource: %w\", err)\n-\t}\n-\treturn &application.ApplicationResponse{}, nil\n+        _, err := s.kubectl.CreateResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), newObj, metav1.CreateOptions{})\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error creating resource: %w\", err)\n+        }\n+        return &application.ApplicationResponse{}, nil\n }\n \n // splitStatusPatch splits a patch into two: one for a non-status patch, and the status-only patch.\n // Returns nil for either if the patch doesn't have modifications to non-status, or status, respectively.\n func splitStatusPatch(patch []byte) ([]byte, []byte, error) {\n-\tvar obj map[string]interface{}\n-\terr := json.Unmarshal(patch, &obj)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\tvar nonStatusPatch, statusPatch []byte\n-\tif statusVal, ok := obj[\"status\"]; ok {\n-\t\t// calculate the status-only patch\n-\t\tstatusObj := map[string]interface{}{\n-\t\t\t\"status\": statusVal,\n-\t\t}\n-\t\tstatusPatch, err = json.Marshal(statusObj)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, err\n-\t\t}\n-\t\t// remove status, and calculate the non-status patch\n-\t\tdelete(obj, \"status\")\n-\t\tif len(obj) > 0 {\n-\t\t\tnonStatusPatch, err = json.Marshal(obj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, nil, err\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\t// status was not modified in patch\n-\t\tnonStatusPatch = patch\n-\t}\n-\treturn nonStatusPatch, statusPatch, nil\n+        var obj map[string]interface{}\n+        err := json.Unmarshal(patch, &obj)\n+        if err != nil {\n+                return nil, nil, err\n+        }\n+        var nonStatusPatch, statusPatch []byte\n+        if statusVal, ok := obj[\"status\"]; ok {\n+                // calculate the status-only patch\n+                statusObj := map[string]interface{}{\n+                        \"status\": statusVal,\n+                }\n+                statusPatch, err = json.Marshal(statusObj)\n+                if err != nil {\n+                        return nil, nil, err\n+                }\n+                // remove status, and calculate the non-status patch\n+                delete(obj, \"status\")\n+                if len(obj) > 0 {\n+                        nonStatusPatch, err = json.Marshal(obj)\n+                        if err != nil {\n+                                return nil, nil, err\n+                        }\n+                }\n+        } else {\n+                // status was not modified in patch\n+                nonStatusPatch = patch\n+        }\n+        return nonStatusPatch, statusPatch, nil\n }\n \n func (s *Server) GetApplicationSyncWindows(ctx context.Context, q *application.ApplicationSyncWindowsQuery) (*application.ApplicationSyncWindowsResponse, error) {\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting app project: %w\", err)\n-\t}\n-\n-\twindows := proj.Spec.SyncWindows.Matches(a)\n-\tsync := windows.CanSync(true)\n-\n-\tres := &application.ApplicationSyncWindowsResponse{\n-\t\tActiveWindows:   convertSyncWindows(windows.Active()),\n-\t\tAssignedWindows: convertSyncWindows(windows),\n-\t\tCanSync:         &sync,\n-\t}\n-\n-\treturn res, nil\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting app project: %w\", err)\n+        }\n+\n+        windows := proj.Spec.SyncWindows.Matches(a)\n+        sync := windows.CanSync(true)\n+\n+        res := &application.ApplicationSyncWindowsResponse{\n+                ActiveWindows:   convertSyncWindows(windows.Active()),\n+                AssignedWindows: convertSyncWindows(windows),\n+                CanSync:         &sync,\n+        }\n+\n+        return res, nil\n }\n \n func (s *Server) inferResourcesStatusHealth(app *appv1.Application) {\n-\tif app.Status.ResourceHealthSource == appv1.ResourceHealthLocationAppTree {\n-\t\ttree := &appv1.ApplicationTree{}\n-\t\tif err := s.cache.GetAppResourcesTree(app.Name, tree); err == nil {\n-\t\t\thealthByKey := map[kube.ResourceKey]*appv1.HealthStatus{}\n-\t\t\tfor _, node := range tree.Nodes {\n-\t\t\t\thealthByKey[kube.NewResourceKey(node.Group, node.Kind, node.Namespace, node.Name)] = node.Health\n-\t\t\t}\n-\t\t\tfor i, res := range app.Status.Resources {\n-\t\t\t\tres.Health = healthByKey[kube.NewResourceKey(res.Group, res.Kind, res.Namespace, res.Name)]\n-\t\t\t\tapp.Status.Resources[i] = res\n-\t\t\t}\n-\t\t}\n-\t}\n+        if app.Status.ResourceHealthSource == appv1.ResourceHealthLocationAppTree {\n+                tree := &appv1.ApplicationTree{}\n+                if err := s.cache.GetAppResourcesTree(app.Name, tree); err == nil {\n+                        healthByKey := map[kube.ResourceKey]*appv1.HealthStatus{}\n+                        for _, node := range tree.Nodes {\n+                                healthByKey[kube.NewResourceKey(node.Group, node.Kind, node.Namespace, node.Name)] = node.Health\n+                        }\n+                        for i, res := range app.Status.Resources {\n+                                res.Health = healthByKey[kube.NewResourceKey(res.Group, res.Kind, res.Namespace, res.Name)]\n+                                app.Status.Resources[i] = res\n+                        }\n+                }\n+        }\n }\n \n func convertSyncWindows(w *appv1.SyncWindows) []*application.ApplicationSyncWindow {\n-\tif w != nil {\n-\t\tvar windows []*application.ApplicationSyncWindow\n-\t\tfor _, w := range *w {\n-\t\t\tnw := &application.ApplicationSyncWindow{\n-\t\t\t\tKind:       &w.Kind,\n-\t\t\t\tSchedule:   &w.Schedule,\n-\t\t\t\tDuration:   &w.Duration,\n-\t\t\t\tManualSync: &w.ManualSync,\n-\t\t\t}\n-\t\t\twindows = append(windows, nw)\n-\t\t}\n-\t\tif len(windows) > 0 {\n-\t\t\treturn windows\n-\t\t}\n-\t}\n-\treturn nil\n+        if w != nil {\n+                var windows []*application.ApplicationSyncWindow\n+                for _, w := range *w {\n+                        nw := &application.ApplicationSyncWindow{\n+                                Kind:       &w.Kind,\n+                                Schedule:   &w.Schedule,\n+                                Duration:   &w.Duration,\n+                                ManualSync: &w.ManualSync,\n+                        }\n+                        windows = append(windows, nw)\n+                }\n+                if len(windows) > 0 {\n+                        return windows\n+                }\n+        }\n+        return nil\n }\n \n func getPropagationPolicyFinalizer(policy string) string {\n-\tswitch strings.ToLower(policy) {\n-\tcase backgroundPropagationPolicy:\n-\t\treturn appv1.BackgroundPropagationPolicyFinalizer\n-\tcase foregroundPropagationPolicy:\n-\t\treturn appv1.ForegroundPropagationPolicyFinalizer\n-\tcase \"\":\n-\t\treturn appv1.ResourcesFinalizerName\n-\tdefault:\n-\t\treturn \"\"\n-\t}\n+        switch strings.ToLower(policy) {\n+        case backgroundPropagationPolicy:\n+                return appv1.BackgroundPropagationPolicyFinalizer\n+        case foregroundPropagationPolicy:\n+                return appv1.ForegroundPropagationPolicyFinalizer\n+        case \"\":\n+                return appv1.ResourcesFinalizerName\n+        default:\n+                return \"\"\n+        }\n }\n \n func (s *Server) appNamespaceOrDefault(appNs string) string {\n-\tif appNs == \"\" {\n-\t\treturn s.ns\n-\t} else {\n-\t\treturn appNs\n-\t}\n+        if appNs == \"\" {\n+                return s.ns\n+        } else {\n+                return appNs\n+        }\n }\n \n func (s *Server) isNamespaceEnabled(namespace string) bool {\n-\treturn security.IsNamespaceEnabled(namespace, s.ns, s.enabledNamespaces)\n+        return security.IsNamespaceEnabled(namespace, s.ns, s.enabledNamespaces)\n }\n \n // getProjectFromApplicationQuery gets the project names from a query. If the legacy \"project\" field was specified, use\n // that. Otherwise, use the newer \"projects\" field.\n func getProjectsFromApplicationQuery(q application.ApplicationQuery) []string {\n-\tif q.Project != nil {\n-\t\treturn q.Project\n-\t}\n-\treturn q.Projects\n+        if q.Project != nil {\n+                return q.Project\n+        }\n+        return q.Projects\n }\n"}
{"cve":"CVE-2022-3920:0708", "fix_patch": "diff --git a/agent/structs/aclfilter/filter.go b/agent/structs/aclfilter/filter.go\nindex b976627dc7..781e5ccab7 100644\n--- a/agent/structs/aclfilter/filter.go\n+++ b/agent/structs/aclfilter/filter.go\n@@ -1,414 +1,429 @@\n package aclfilter\n \n import (\n-\t\"fmt\"\n+        \"fmt\"\n \n-\t\"github.com/hashicorp/go-hclog\"\n+        \"github.com/hashicorp/go-hclog\"\n \n-\t\"github.com/hashicorp/consul/acl\"\n-\t\"github.com/hashicorp/consul/agent/structs\"\n+        \"github.com/hashicorp/consul/acl\"\n+        \"github.com/hashicorp/consul/agent/structs\"\n )\n \n const (\n-\t// RedactedToken is shown in structures with embedded tokens when they\n-\t// are not allowed to be displayed.\n-\tRedactedToken = \"<hidden>\"\n+        // RedactedToken is shown in structures with embedded tokens when they\n+        // are not allowed to be displayed.\n+        RedactedToken = \"<hidden>\"\n )\n \n // Filter is used to filter results based on ACL rules.\n type Filter struct {\n-\tauthorizer acl.Authorizer\n-\tlogger     hclog.Logger\n+        authorizer acl.Authorizer\n+        logger     hclog.Logger\n }\n \n // New constructs a Filter with the given authorizer.\n func New(authorizer acl.Authorizer, logger hclog.Logger) *Filter {\n-\tif logger == nil {\n-\t\tlogger = hclog.NewNullLogger()\n-\t}\n-\treturn &Filter{authorizer, logger}\n+        if logger == nil {\n+                logger = hclog.NewNullLogger()\n+        }\n+        return &Filter{authorizer, logger}\n }\n \n // Filter the given subject in-place.\n func (f *Filter) Filter(subject any) {\n-\tswitch v := subject.(type) {\n-\tcase *structs.CheckServiceNodes:\n-\t\tf.filterCheckServiceNodes(v)\n+        switch v := subject.(type) {\n+        case *structs.CheckServiceNodes:\n+                f.filterCheckServiceNodes(v)\n \n-\tcase *structs.IndexedCheckServiceNodes:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterCheckServiceNodes(&v.Nodes)\n+        case *structs.IndexedCheckServiceNodes:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterCheckServiceNodes(&v.Nodes)\n \n-\tcase *structs.PreparedQueryExecuteResponse:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterCheckServiceNodes(&v.Nodes)\n-\n-\tcase *structs.IndexedServiceTopology:\n-\t\tfiltered := f.filterServiceTopology(v.ServiceTopology)\n-\t\tif filtered {\n-\t\t\tv.FilteredByACLs = true\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = true\n-\t\t}\n-\n-\tcase *structs.DatacenterIndexedCheckServiceNodes:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterDatacenterCheckServiceNodes(&v.DatacenterNodes)\n-\n-\tcase *structs.IndexedCoordinates:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterCoordinates(&v.Coordinates)\n-\n-\tcase *structs.IndexedHealthChecks:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterHealthChecks(&v.HealthChecks)\n-\n-\tcase *structs.IndexedIntentions:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterIntentions(&v.Intentions)\n-\n-\tcase *structs.IndexedNodeDump:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterNodeDump(&v.Dump)\n-\n-\tcase *structs.IndexedServiceDump:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServiceDump(&v.Dump)\n-\n-\tcase *structs.IndexedNodes:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterNodes(&v.Nodes)\n-\n-\tcase *structs.IndexedNodeServices:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterNodeServices(&v.NodeServices)\n-\n-\tcase *structs.IndexedNodeServiceList:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterNodeServiceList(&v.NodeServices)\n-\n-\tcase *structs.IndexedServiceNodes:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServiceNodes(&v.ServiceNodes)\n-\n-\tcase *structs.IndexedServices:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServices(v.Services, &v.EnterpriseMeta)\n-\n-\tcase *structs.IndexedSessions:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterSessions(&v.Sessions)\n-\n-\tcase *structs.IndexedPreparedQueries:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterPreparedQueries(&v.Queries)\n-\n-\tcase **structs.PreparedQuery:\n-\t\tf.redactPreparedQueryTokens(v)\n-\n-\tcase *structs.ACLTokens:\n-\t\tf.filterTokens(v)\n-\tcase **structs.ACLToken:\n-\t\tf.filterToken(v)\n-\tcase *[]*structs.ACLTokenListStub:\n-\t\tf.filterTokenStubs(v)\n-\tcase **structs.ACLTokenListStub:\n-\t\tf.filterTokenStub(v)\n-\n-\tcase *structs.ACLPolicies:\n-\t\tf.filterPolicies(v)\n-\tcase **structs.ACLPolicy:\n-\t\tf.filterPolicy(v)\n-\n-\tcase *structs.ACLRoles:\n-\t\tf.filterRoles(v)\n-\tcase **structs.ACLRole:\n-\t\tf.filterRole(v)\n-\n-\tcase *structs.ACLBindingRules:\n-\t\tf.filterBindingRules(v)\n-\tcase **structs.ACLBindingRule:\n-\t\tf.filterBindingRule(v)\n-\n-\tcase *structs.ACLAuthMethods:\n-\t\tf.filterAuthMethods(v)\n-\tcase **structs.ACLAuthMethod:\n-\t\tf.filterAuthMethod(v)\n-\n-\tcase *structs.IndexedServiceList:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServiceList(&v.Services)\n-\n-\tcase *structs.IndexedExportedServiceList:\n-\t\tfor peer, peerServices := range v.Services {\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServiceList(&peerServices)\n-\t\t\tif len(peerServices) == 0 {\n-\t\t\t\tdelete(v.Services, peer)\n-\t\t\t} else {\n-\t\t\t\tv.Services[peer] = peerServices\n-\t\t\t}\n-\t\t}\n-\n-\tcase *structs.IndexedGatewayServices:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterGatewayServices(&v.Services)\n+        case *structs.PreparedQueryExecuteResponse:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterCheckServiceNodes(&v.Nodes)\n \n-\tcase *structs.IndexedNodesWithGateways:\n-\t\tif f.filterCheckServiceNodes(&v.Nodes) {\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = true\n-\t\t}\n-\t\tif f.filterGatewayServices(&v.Gateways) {\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = true\n-\t\t}\n-\t\tif f.filterCheckServiceNodes(&v.ImportedNodes) {\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = true\n-\t\t}\n+        case *structs.IndexedServiceTopology:\n+                filtered := f.filterServiceTopology(v.ServiceTopology)\n+                if filtered {\n+                        v.FilteredByACLs = true\n+                        v.QueryMeta.ResultsFilteredByACLs = true\n+                }\n \n-\tdefault:\n-\t\tpanic(fmt.Errorf(\"Unhandled type passed to ACL filter: %T %#v\", subject, subject))\n-\t}\n+        case *structs.DatacenterIndexedCheckServiceNodes:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterDatacenterCheckServiceNodes(&v.DatacenterNodes)\n+\n+        case *structs.IndexedCoordinates:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterCoordinates(&v.Coordinates)\n+\n+        case *structs.IndexedHealthChecks:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterHealthChecks(&v.HealthChecks)\n+\n+        case *structs.IndexedIntentions:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterIntentions(&v.Intentions)\n+\n+\n+case *structs.IndexedNodeDump:\n+if f.filterNodeDump(&v.Dump) {\n+v.QueryMeta.ResultsFilteredByACLs = true\n+}\n+if f.filterNodeDump(&v.ImportedDump) {\n+v.QueryMeta.ResultsFilteredByACLs = true\n+}\n+\n+        case *structs.IndexedNodeDump:\n+if f.filterNodeDump(&v.Dump) {\n+v.QueryMeta.ResultsFilteredByACLs = true\n+}\n+if f.filterNodeDump(&v.ImportedDump) {\n+v.QueryMeta.ResultsFilteredByACLs = true\n+}\n+\n+\n+        case *structs.IndexedServiceDump:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterServiceDump(&v.Dump)\n+\n+        case *structs.IndexedNodes:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterNodes(&v.Nodes)\n+\n+        case *structs.IndexedNodeServices:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterNodeServices(&v.NodeServices)\n+\n+        case *structs.IndexedNodeServiceList:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterNodeServiceList(&v.NodeServices)\n+\n+        case *structs.IndexedServiceNodes:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterServiceNodes(&v.ServiceNodes)\n+\n+        case *structs.IndexedServices:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterServices(v.Services, &v.EnterpriseMeta)\n+\n+        case *structs.IndexedSessions:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterSessions(&v.Sessions)\n+\n+        case *structs.IndexedPreparedQueries:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterPreparedQueries(&v.Queries)\n+\n+        case **structs.PreparedQuery:\n+                f.redactPreparedQueryTokens(v)\n+\n+        case *structs.ACLTokens:\n+                f.filterTokens(v)\n+        case **structs.ACLToken:\n+                f.filterToken(v)\n+        case *[]*structs.ACLTokenListStub:\n+                f.filterTokenStubs(v)\n+        case **structs.ACLTokenListStub:\n+                f.filterTokenStub(v)\n+\n+        case *structs.ACLPolicies:\n+                f.filterPolicies(v)\n+        case **structs.ACLPolicy:\n+                f.filterPolicy(v)\n+\n+        case *structs.ACLRoles:\n+                f.filterRoles(v)\n+        case **structs.ACLRole:\n+                f.filterRole(v)\n+\n+        case *structs.ACLBindingRules:\n+                f.filterBindingRules(v)\n+        case **structs.ACLBindingRule:\n+                f.filterBindingRule(v)\n+\n+        case *structs.ACLAuthMethods:\n+                f.filterAuthMethods(v)\n+        case **structs.ACLAuthMethod:\n+                f.filterAuthMethod(v)\n+\n+        case *structs.IndexedServiceList:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterServiceList(&v.Services)\n+\n+        case *structs.IndexedExportedServiceList:\n+                for peer, peerServices := range v.Services {\n+                        v.QueryMeta.ResultsFilteredByACLs = f.filterServiceList(&peerServices)\n+                        if len(peerServices) == 0 {\n+                                delete(v.Services, peer)\n+                        } else {\n+                                v.Services[peer] = peerServices\n+                        }\n+                }\n+\n+        case *structs.IndexedGatewayServices:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterGatewayServices(&v.Services)\n+\n+        case *structs.IndexedNodesWithGateways:\n+                if f.filterCheckServiceNodes(&v.Nodes) {\n+                        v.QueryMeta.ResultsFilteredByACLs = true\n+                }\n+                if f.filterGatewayServices(&v.Gateways) {\n+                        v.QueryMeta.ResultsFilteredByACLs = true\n+                }\n+                if f.filterCheckServiceNodes(&v.ImportedNodes) {\n+                        v.QueryMeta.ResultsFilteredByACLs = true\n+                }\n+\n+        default:\n+                panic(fmt.Errorf(\"Unhandled type passed to ACL filter: %T %#v\", subject, subject))\n+        }\n }\n \n // allowNode is used to determine if a node is accessible for an ACL.\n func (f *Filter) allowNode(node string, ent *acl.AuthorizerContext) bool {\n-\treturn f.authorizer.NodeRead(node, ent) == acl.Allow\n+        return f.authorizer.NodeRead(node, ent) == acl.Allow\n }\n \n // allowNode is used to determine if the gateway and service are accessible for an ACL\n func (f *Filter) allowGateway(gs *structs.GatewayService) bool {\n-\tvar authzContext acl.AuthorizerContext\n+        var authzContext acl.AuthorizerContext\n \n-\t// Need read on service and gateway. Gateway may have different EnterpriseMeta so we fill authzContext twice\n-\tgs.Gateway.FillAuthzContext(&authzContext)\n-\tif !f.allowService(gs.Gateway.Name, &authzContext) {\n-\t\treturn false\n-\t}\n+        // Need read on service and gateway. Gateway may have different EnterpriseMeta so we fill authzContext twice\n+        gs.Gateway.FillAuthzContext(&authzContext)\n+        if !f.allowService(gs.Gateway.Name, &authzContext) {\n+                return false\n+        }\n \n-\tgs.Service.FillAuthzContext(&authzContext)\n-\tif !f.allowService(gs.Service.Name, &authzContext) {\n-\t\treturn false\n-\t}\n-\treturn true\n+        gs.Service.FillAuthzContext(&authzContext)\n+        if !f.allowService(gs.Service.Name, &authzContext) {\n+                return false\n+        }\n+        return true\n }\n \n // allowService is used to determine if a service is accessible for an ACL.\n func (f *Filter) allowService(service string, ent *acl.AuthorizerContext) bool {\n-\tif service == \"\" {\n-\t\treturn true\n-\t}\n+        if service == \"\" {\n+                return true\n+        }\n \n-\treturn f.authorizer.ServiceRead(service, ent) == acl.Allow\n+        return f.authorizer.ServiceRead(service, ent) == acl.Allow\n }\n \n // allowSession is used to determine if a session for a node is accessible for\n // an ACL.\n func (f *Filter) allowSession(node string, ent *acl.AuthorizerContext) bool {\n-\treturn f.authorizer.SessionRead(node, ent) == acl.Allow\n+        return f.authorizer.SessionRead(node, ent) == acl.Allow\n }\n \n // filterHealthChecks is used to filter a set of health checks down based on\n // the configured ACL rules for a token. Returns true if any elements were\n // removed.\n func (f *Filter) filterHealthChecks(checks *structs.HealthChecks) bool {\n-\thc := *checks\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(hc); i++ {\n-\t\tcheck := hc[i]\n-\t\tcheck.FillAuthzContext(&authzContext)\n-\t\tif f.allowNode(check.Node, &authzContext) && f.allowService(check.ServiceName, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tf.logger.Debug(\"dropping check from result due to ACLs\", \"check\", check.CheckID)\n-\t\tremoved = true\n-\t\thc = append(hc[:i], hc[i+1:]...)\n-\t\ti--\n-\t}\n-\t*checks = hc\n-\treturn removed\n+        hc := *checks\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(hc); i++ {\n+                check := hc[i]\n+                check.FillAuthzContext(&authzContext)\n+                if f.allowNode(check.Node, &authzContext) && f.allowService(check.ServiceName, &authzContext) {\n+                        continue\n+                }\n+\n+                f.logger.Debug(\"dropping check from result due to ACLs\", \"check\", check.CheckID)\n+                removed = true\n+                hc = append(hc[:i], hc[i+1:]...)\n+                i--\n+        }\n+        *checks = hc\n+        return removed\n }\n \n // filterServices is used to filter a set of services based on ACLs. Returns\n // true if any elements were removed.\n func (f *Filter) filterServices(services structs.Services, entMeta *acl.EnterpriseMeta) bool {\n-\tvar authzContext acl.AuthorizerContext\n-\tentMeta.FillAuthzContext(&authzContext)\n+        var authzContext acl.AuthorizerContext\n+        entMeta.FillAuthzContext(&authzContext)\n \n-\tvar removed bool\n+        var removed bool\n \n-\tfor svc := range services {\n-\t\tif f.allowService(svc, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc)\n-\t\tremoved = true\n-\t\tdelete(services, svc)\n-\t}\n+        for svc := range services {\n+                if f.allowService(svc, &authzContext) {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc)\n+                removed = true\n+                delete(services, svc)\n+        }\n \n-\treturn removed\n+        return removed\n }\n \n // filterServiceNodes is used to filter a set of nodes for a given service\n // based on the configured ACL rules. Returns true if any elements were removed.\n func (f *Filter) filterServiceNodes(nodes *structs.ServiceNodes) bool {\n-\tsn := *nodes\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(sn); i++ {\n-\t\tnode := sn[i]\n-\n-\t\tnode.FillAuthzContext(&authzContext)\n-\t\tif f.allowNode(node.Node, &authzContext) && f.allowService(node.ServiceName, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tremoved = true\n-\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node.Node, &node.EnterpriseMeta))\n-\t\tsn = append(sn[:i], sn[i+1:]...)\n-\t\ti--\n-\t}\n-\t*nodes = sn\n-\treturn removed\n+        sn := *nodes\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(sn); i++ {\n+                node := sn[i]\n+\n+                node.FillAuthzContext(&authzContext)\n+                if f.allowNode(node.Node, &authzContext) && f.allowService(node.ServiceName, &authzContext) {\n+                        continue\n+                }\n+                removed = true\n+                f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node.Node, &node.EnterpriseMeta))\n+                sn = append(sn[:i], sn[i+1:]...)\n+                i--\n+        }\n+        *nodes = sn\n+        return removed\n }\n \n // filterNodeServices is used to filter services on a given node base on ACLs.\n // Returns true if any elements were removed\n func (f *Filter) filterNodeServices(services **structs.NodeServices) bool {\n-\tif *services == nil {\n-\t\treturn false\n-\t}\n+        if *services == nil {\n+                return false\n+        }\n \n-\tvar authzContext acl.AuthorizerContext\n-\t(*services).Node.FillAuthzContext(&authzContext)\n-\tif !f.allowNode((*services).Node.Node, &authzContext) {\n-\t\t*services = nil\n-\t\treturn true\n-\t}\n+        var authzContext acl.AuthorizerContext\n+        (*services).Node.FillAuthzContext(&authzContext)\n+        if !f.allowNode((*services).Node.Node, &authzContext) {\n+                *services = nil\n+                return true\n+        }\n \n-\tvar removed bool\n-\tfor svcName, svc := range (*services).Services {\n-\t\tsvc.FillAuthzContext(&authzContext)\n+        var removed bool\n+        for svcName, svc := range (*services).Services {\n+                svc.FillAuthzContext(&authzContext)\n \n-\t\tif f.allowNode((*services).Node.Node, &authzContext) && f.allowService(svcName, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n-\t\tremoved = true\n-\t\tdelete((*services).Services, svcName)\n-\t}\n+                if f.allowNode((*services).Node.Node, &authzContext) && f.allowService(svcName, &authzContext) {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n+                removed = true\n+                delete((*services).Services, svcName)\n+        }\n \n-\treturn removed\n+        return removed\n }\n \n // filterNodeServices is used to filter services on a given node base on ACLs.\n // Returns true if any elements were removed.\n func (f *Filter) filterNodeServiceList(services *structs.NodeServiceList) bool {\n-\tif services.Node == nil {\n-\t\treturn false\n-\t}\n+        if services.Node == nil {\n+                return false\n+        }\n \n-\tvar authzContext acl.AuthorizerContext\n-\tservices.Node.FillAuthzContext(&authzContext)\n-\tif !f.allowNode(services.Node.Node, &authzContext) {\n-\t\t*services = structs.NodeServiceList{}\n-\t\treturn true\n-\t}\n+        var authzContext acl.AuthorizerContext\n+        services.Node.FillAuthzContext(&authzContext)\n+        if !f.allowNode(services.Node.Node, &authzContext) {\n+                *services = structs.NodeServiceList{}\n+                return true\n+        }\n \n-\tvar removed bool\n-\tsvcs := services.Services\n-\tfor i := 0; i < len(svcs); i++ {\n-\t\tsvc := svcs[i]\n-\t\tsvc.FillAuthzContext(&authzContext)\n+        var removed bool\n+        svcs := services.Services\n+        for i := 0; i < len(svcs); i++ {\n+                svc := svcs[i]\n+                svc.FillAuthzContext(&authzContext)\n \n-\t\tif f.allowService(svc.Service, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n+                if f.allowService(svc.Service, &authzContext) {\n+                        continue\n+                }\n \n-\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n-\t\tsvcs = append(svcs[:i], svcs[i+1:]...)\n-\t\ti--\n-\t\tremoved = true\n-\t}\n-\tservices.Services = svcs\n+                f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n+                svcs = append(svcs[:i], svcs[i+1:]...)\n+                i--\n+                removed = true\n+        }\n+        services.Services = svcs\n \n-\treturn removed\n+        return removed\n }\n \n // filterCheckServiceNodes is used to filter nodes based on ACL rules. Returns\n // true if any elements were removed.\n func (f *Filter) filterCheckServiceNodes(nodes *structs.CheckServiceNodes) bool {\n-\tcsn := *nodes\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(csn); i++ {\n-\t\tnode := csn[i]\n-\t\tif node.CanRead(f.authorizer) == acl.Allow {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node.Node.Node, node.Node.GetEnterpriseMeta()))\n-\t\tremoved = true\n-\t\tcsn = append(csn[:i], csn[i+1:]...)\n-\t\ti--\n-\t}\n-\t*nodes = csn\n-\treturn removed\n+        csn := *nodes\n+        var removed bool\n+\n+        for i := 0; i < len(csn); i++ {\n+                node := csn[i]\n+                if node.CanRead(f.authorizer) == acl.Allow {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node.Node.Node, node.Node.GetEnterpriseMeta()))\n+                removed = true\n+                csn = append(csn[:i], csn[i+1:]...)\n+                i--\n+        }\n+        *nodes = csn\n+        return removed\n }\n \n // filterServiceTopology is used to filter upstreams/downstreams based on ACL rules.\n // this filter is unlike others in that it also returns whether the result was filtered by ACLs\n func (f *Filter) filterServiceTopology(topology *structs.ServiceTopology) bool {\n-\tfilteredUpstreams := f.filterCheckServiceNodes(&topology.Upstreams)\n-\tfilteredDownstreams := f.filterCheckServiceNodes(&topology.Downstreams)\n-\treturn filteredUpstreams || filteredDownstreams\n+        filteredUpstreams := f.filterCheckServiceNodes(&topology.Upstreams)\n+        filteredDownstreams := f.filterCheckServiceNodes(&topology.Downstreams)\n+        return filteredUpstreams || filteredDownstreams\n }\n \n // filterDatacenterCheckServiceNodes is used to filter nodes based on ACL rules.\n // Returns true if any elements are removed.\n func (f *Filter) filterDatacenterCheckServiceNodes(datacenterNodes *map[string]structs.CheckServiceNodes) bool {\n-\tdn := *datacenterNodes\n-\tout := make(map[string]structs.CheckServiceNodes)\n-\tvar removed bool\n-\tfor dc := range dn {\n-\t\tnodes := dn[dc]\n-\t\tif f.filterCheckServiceNodes(&nodes) {\n-\t\t\tremoved = true\n-\t\t}\n-\t\tif len(nodes) > 0 {\n-\t\t\tout[dc] = nodes\n-\t\t}\n-\t}\n-\t*datacenterNodes = out\n-\treturn removed\n+        dn := *datacenterNodes\n+        out := make(map[string]structs.CheckServiceNodes)\n+        var removed bool\n+        for dc := range dn {\n+                nodes := dn[dc]\n+                if f.filterCheckServiceNodes(&nodes) {\n+                        removed = true\n+                }\n+                if len(nodes) > 0 {\n+                        out[dc] = nodes\n+                }\n+        }\n+        *datacenterNodes = out\n+        return removed\n }\n \n // filterSessions is used to filter a set of sessions based on ACLs. Returns\n // true if any elements were removed.\n func (f *Filter) filterSessions(sessions *structs.Sessions) bool {\n-\ts := *sessions\n+        s := *sessions\n \n-\tvar removed bool\n-\tfor i := 0; i < len(s); i++ {\n-\t\tsession := s[i]\n+        var removed bool\n+        for i := 0; i < len(s); i++ {\n+                session := s[i]\n \n-\t\tvar entCtx acl.AuthorizerContext\n-\t\tsession.FillAuthzContext(&entCtx)\n+                var entCtx acl.AuthorizerContext\n+                session.FillAuthzContext(&entCtx)\n \n-\t\tif f.allowSession(session.Node, &entCtx) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tremoved = true\n-\t\tf.logger.Debug(\"dropping session from result due to ACLs\", \"session\", session.ID)\n-\t\ts = append(s[:i], s[i+1:]...)\n-\t\ti--\n-\t}\n-\t*sessions = s\n-\treturn removed\n+                if f.allowSession(session.Node, &entCtx) {\n+                        continue\n+                }\n+                removed = true\n+                f.logger.Debug(\"dropping session from result due to ACLs\", \"session\", session.ID)\n+                s = append(s[:i], s[i+1:]...)\n+                i--\n+        }\n+        *sessions = s\n+        return removed\n }\n \n // filterCoordinates is used to filter nodes in a coordinate dump based on ACL\n // rules. Returns true if any elements were removed.\n func (f *Filter) filterCoordinates(coords *structs.Coordinates) bool {\n-\tc := *coords\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(c); i++ {\n-\t\tc[i].FillAuthzContext(&authzContext)\n-\t\tnode := c[i].Node\n-\t\tif f.allowNode(node, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, c[i].GetEnterpriseMeta()))\n-\t\tremoved = true\n-\t\tc = append(c[:i], c[i+1:]...)\n-\t\ti--\n-\t}\n-\t*coords = c\n-\treturn removed\n+        c := *coords\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(c); i++ {\n+                c[i].FillAuthzContext(&authzContext)\n+                node := c[i].Node\n+                if f.allowNode(node, &authzContext) {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, c[i].GetEnterpriseMeta()))\n+                removed = true\n+                c = append(c[:i], c[i+1:]...)\n+                i--\n+        }\n+        *coords = c\n+        return removed\n }\n \n // filterIntentions is used to filter intentions based on ACL rules.\n@@ -416,126 +431,131 @@ func (f *Filter) filterCoordinates(coords *structs.Coordinates) bool {\n // if the user doesn't have a management token. Returns true if any elements\n // were removed.\n func (f *Filter) filterIntentions(ixns *structs.Intentions) bool {\n-\tret := make(structs.Intentions, 0, len(*ixns))\n-\tvar removed bool\n-\tfor _, ixn := range *ixns {\n-\t\tif !ixn.CanRead(f.authorizer) {\n-\t\t\tremoved = true\n-\t\t\tf.logger.Debug(\"dropping intention from result due to ACLs\", \"intention\", ixn.ID)\n-\t\t\tcontinue\n-\t\t}\n+        ret := make(structs.Intentions, 0, len(*ixns))\n+        var removed bool\n+        for _, ixn := range *ixns {\n+                if !ixn.CanRead(f.authorizer) {\n+                        removed = true\n+                        f.logger.Debug(\"dropping intention from result due to ACLs\", \"intention\", ixn.ID)\n+                        continue\n+                }\n \n-\t\tret = append(ret, ixn)\n-\t}\n+                ret = append(ret, ixn)\n+        }\n \n-\t*ixns = ret\n-\treturn removed\n+        *ixns = ret\n+        return removed\n }\n \n // filterNodeDump is used to filter through all parts of a node dump and\n // remove elements the provided ACL token cannot access. Returns true if\n // any elements were removed.\n-func (f *Filter) filterNodeDump(dump *structs.NodeDump) bool {\n-\tnd := *dump\n-\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\tfor i := 0; i < len(nd); i++ {\n-\t\tinfo := nd[i]\n-\n-\t\t// Filter nodes\n-\t\tinfo.FillAuthzContext(&authzContext)\n-\t\tif node := info.Node; !f.allowNode(node, &authzContext) {\n-\t\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, info.GetEnterpriseMeta()))\n-\t\t\tremoved = true\n-\t\t\tnd = append(nd[:i], nd[i+1:]...)\n-\t\t\ti--\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Filter services\n-\t\tfor j := 0; j < len(info.Services); j++ {\n-\t\t\tsvc := info.Services[j].Service\n-\t\t\tinfo.Services[j].FillAuthzContext(&authzContext)\n-\t\t\tif f.allowNode(info.Node, &authzContext) && f.allowService(svc, &authzContext) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc)\n-\t\t\tremoved = true\n-\t\t\tinfo.Services = append(info.Services[:j], info.Services[j+1:]...)\n-\t\t\tj--\n-\t\t}\n-\n-\t\t// Filter checks\n-\t\tfor j := 0; j < len(info.Checks); j++ {\n-\t\t\tchk := info.Checks[j]\n-\t\t\tchk.FillAuthzContext(&authzContext)\n-\t\t\tif f.allowNode(info.Node, &authzContext) && f.allowService(chk.ServiceName, &authzContext) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tf.logger.Debug(\"dropping check from result due to ACLs\", \"check\", chk.CheckID)\n-\t\t\tremoved = true\n-\t\t\tinfo.Checks = append(info.Checks[:j], info.Checks[j+1:]...)\n-\t\t\tj--\n-\t\t}\n-\t}\n-\t*dump = nd\n-\treturn removed\n+func (f *Filter) filterNodeDump(dump *structs.NodeDump) (removed bool) {\n+ nd := *dump\n+ defer func() {\n+if removed {\n+*dump = nd\n+}\n+}()\n+\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+        for i := 0; i < len(nd); i++ {\n+                info := nd[i]\n+\n+                // Filter nodes\n+                info.FillAuthzContext(&authzContext)\n+                if node := info.Node; !f.allowNode(node, &authzContext) {\n+                        f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, info.GetEnterpriseMeta()))\n+                        removed = true\n+                        nd = append(nd[:i], nd[i+1:]...)\n+                        i--\n+                        continue\n+                }\n+\n+                // Filter services\n+                for j := 0; j < len(info.Services); j++ {\n+                        svc := info.Services[j].Service\n+                        info.Services[j].FillAuthzContext(&authzContext)\n+                        if f.allowNode(info.Node, &authzContext) && f.allowService(svc, &authzContext) {\n+                                continue\n+                        }\n+                        f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc)\n+                        removed = true\n+                        info.Services = append(info.Services[:j], info.Services[j+1:]...)\n+                        j--\n+                }\n+\n+                // Filter checks\n+                for j := 0; j < len(info.Checks); j++ {\n+                        chk := info.Checks[j]\n+                        chk.FillAuthzContext(&authzContext)\n+                        if f.allowNode(info.Node, &authzContext) && f.allowService(chk.ServiceName, &authzContext) {\n+                                continue\n+                        }\n+                        f.logger.Debug(\"dropping check from result due to ACLs\", \"check\", chk.CheckID)\n+                        removed = true\n+                        info.Checks = append(info.Checks[:j], info.Checks[j+1:]...)\n+                        j--\n+                }\n+        }\n+        *dump = nd\n+        return removed\n }\n \n // filterServiceDump is used to filter nodes based on ACL rules. Returns true\n // if any elements were removed.\n func (f *Filter) filterServiceDump(services *structs.ServiceDump) bool {\n-\tsvcs := *services\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(svcs); i++ {\n-\t\tservice := svcs[i]\n-\n-\t\tif f.allowGateway(service.GatewayService) {\n-\t\t\t// ServiceDump might only have gateway config and no node information\n-\t\t\tif service.Node == nil {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tservice.Service.FillAuthzContext(&authzContext)\n-\t\t\tif f.allowNode(service.Node.Node, &authzContext) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\n-\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", service.GatewayService.Service)\n-\t\tremoved = true\n-\t\tsvcs = append(svcs[:i], svcs[i+1:]...)\n-\t\ti--\n-\t}\n-\t*services = svcs\n-\treturn removed\n+        svcs := *services\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(svcs); i++ {\n+                service := svcs[i]\n+\n+                if f.allowGateway(service.GatewayService) {\n+                        // ServiceDump might only have gateway config and no node information\n+                        if service.Node == nil {\n+                                continue\n+                        }\n+\n+                        service.Service.FillAuthzContext(&authzContext)\n+                        if f.allowNode(service.Node.Node, &authzContext) {\n+                                continue\n+                        }\n+                }\n+\n+                f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", service.GatewayService.Service)\n+                removed = true\n+                svcs = append(svcs[:i], svcs[i+1:]...)\n+                i--\n+        }\n+        *services = svcs\n+        return removed\n }\n \n // filterNodes is used to filter through all parts of a node list and remove\n // elements the provided ACL token cannot access. Returns true if any elements\n // were removed.\n func (f *Filter) filterNodes(nodes *structs.Nodes) bool {\n-\tn := *nodes\n-\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(n); i++ {\n-\t\tn[i].FillAuthzContext(&authzContext)\n-\t\tnode := n[i].Node\n-\t\tif f.allowNode(node, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, n[i].GetEnterpriseMeta()))\n-\t\tremoved = true\n-\t\tn = append(n[:i], n[i+1:]...)\n-\t\ti--\n-\t}\n-\t*nodes = n\n-\treturn removed\n+        n := *nodes\n+\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(n); i++ {\n+                n[i].FillAuthzContext(&authzContext)\n+                node := n[i].Node\n+                if f.allowNode(node, &authzContext) {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, n[i].GetEnterpriseMeta()))\n+                removed = true\n+                n = append(n[:i], n[i+1:]...)\n+                i--\n+        }\n+        *nodes = n\n+        return removed\n }\n \n // redactPreparedQueryTokens will redact any tokens unless the client has a\n@@ -545,27 +565,27 @@ func (f *Filter) filterNodes(nodes *structs.Nodes) bool {\n // have the limitation of preventing delegated non-management users from seeing\n // captured tokens, but they can at least see whether or not a token is set.\n func (f *Filter) redactPreparedQueryTokens(query **structs.PreparedQuery) {\n-\t// Management tokens can see everything with no filtering.\n-\tvar authzContext acl.AuthorizerContext\n-\tstructs.DefaultEnterpriseMetaInDefaultPartition().FillAuthzContext(&authzContext)\n-\tif f.authorizer.ACLWrite(&authzContext) == acl.Allow {\n-\t\treturn\n-\t}\n-\n-\t// Let the user see if there's a blank token, otherwise we need\n-\t// to redact it, since we know they don't have a management\n-\t// token.\n-\tif (*query).Token != \"\" {\n-\t\t// Redact the token, using a copy of the query structure\n-\t\t// since we could be pointed at a live instance from the\n-\t\t// state store so it's not safe to modify it. Note that\n-\t\t// this clone will still point to things like underlying\n-\t\t// arrays in the original, but for modifying just the\n-\t\t// token it will be safe to use.\n-\t\tclone := *(*query)\n-\t\tclone.Token = RedactedToken\n-\t\t*query = &clone\n-\t}\n+        // Management tokens can see everything with no filtering.\n+        var authzContext acl.AuthorizerContext\n+        structs.DefaultEnterpriseMetaInDefaultPartition().FillAuthzContext(&authzContext)\n+        if f.authorizer.ACLWrite(&authzContext) == acl.Allow {\n+                return\n+        }\n+\n+        // Let the user see if there's a blank token, otherwise we need\n+        // to redact it, since we know they don't have a management\n+        // token.\n+        if (*query).Token != \"\" {\n+                // Redact the token, using a copy of the query structure\n+                // since we could be pointed at a live instance from the\n+                // state store so it's not safe to modify it. Note that\n+                // this clone will still point to things like underlying\n+                // arrays in the original, but for modifying just the\n+                // token it will be safe to use.\n+                clone := *(*query)\n+                clone.Token = RedactedToken\n+                *query = &clone\n+        }\n }\n \n // filterPreparedQueries is used to filter prepared queries based on ACL rules.\n@@ -574,248 +594,248 @@ func (f *Filter) redactPreparedQueryTokens(query **structs.PreparedQuery) {\n // queries were removed - un-named queries are meant to be ephemeral and can\n // only be enumerated by a management token\n func (f *Filter) filterPreparedQueries(queries *structs.PreparedQueries) bool {\n-\tvar authzContext acl.AuthorizerContext\n-\tstructs.DefaultEnterpriseMetaInDefaultPartition().FillAuthzContext(&authzContext)\n-\t// Management tokens can see everything with no filtering.\n-\t// TODO  is this check even necessary - this looks like a search replace from\n-\t// the 1.4 ACL rewrite. The global-management token will provide unrestricted query privileges\n-\t// so asking for ACLWrite should be unnecessary.\n-\tif f.authorizer.ACLWrite(&authzContext) == acl.Allow {\n-\t\treturn false\n-\t}\n-\n-\t// Otherwise, we need to see what the token has access to.\n-\tvar namedQueriesRemoved bool\n-\tret := make(structs.PreparedQueries, 0, len(*queries))\n-\tfor _, query := range *queries {\n-\t\t// If no prefix ACL applies to this query then filter it, since\n-\t\t// we know at this point the user doesn't have a management\n-\t\t// token, otherwise see what the policy says.\n-\t\tprefix, hasName := query.GetACLPrefix()\n-\t\tswitch {\n-\t\tcase hasName && f.authorizer.PreparedQueryRead(prefix, &authzContext) != acl.Allow:\n-\t\t\tnamedQueriesRemoved = true\n-\t\t\tfallthrough\n-\t\tcase !hasName:\n-\t\t\tf.logger.Debug(\"dropping prepared query from result due to ACLs\", \"query\", query.ID)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Redact any tokens if necessary. We make a copy of just the\n-\t\t// pointer so we don't mess with the caller's slice.\n-\t\tfinal := query\n-\t\tf.redactPreparedQueryTokens(&final)\n-\t\tret = append(ret, final)\n-\t}\n-\t*queries = ret\n-\treturn namedQueriesRemoved\n+        var authzContext acl.AuthorizerContext\n+        structs.DefaultEnterpriseMetaInDefaultPartition().FillAuthzContext(&authzContext)\n+        // Management tokens can see everything with no filtering.\n+        // TODO  is this check even necessary - this looks like a search replace from\n+        // the 1.4 ACL rewrite. The global-management token will provide unrestricted query privileges\n+        // so asking for ACLWrite should be unnecessary.\n+        if f.authorizer.ACLWrite(&authzContext) == acl.Allow {\n+                return false\n+        }\n+\n+        // Otherwise, we need to see what the token has access to.\n+        var namedQueriesRemoved bool\n+        ret := make(structs.PreparedQueries, 0, len(*queries))\n+        for _, query := range *queries {\n+                // If no prefix ACL applies to this query then filter it, since\n+                // we know at this point the user doesn't have a management\n+                // token, otherwise see what the policy says.\n+                prefix, hasName := query.GetACLPrefix()\n+                switch {\n+                case hasName && f.authorizer.PreparedQueryRead(prefix, &authzContext) != acl.Allow:\n+                        namedQueriesRemoved = true\n+                        fallthrough\n+                case !hasName:\n+                        f.logger.Debug(\"dropping prepared query from result due to ACLs\", \"query\", query.ID)\n+                        continue\n+                }\n+\n+                // Redact any tokens if necessary. We make a copy of just the\n+                // pointer so we don't mess with the caller's slice.\n+                final := query\n+                f.redactPreparedQueryTokens(&final)\n+                ret = append(ret, final)\n+        }\n+        *queries = ret\n+        return namedQueriesRemoved\n }\n \n func (f *Filter) filterToken(token **structs.ACLToken) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif token == nil || *token == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if token == nil || *token == nil || f == nil {\n+                return\n+        }\n \n-\t(*token).FillAuthzContext(&entCtx)\n+        (*token).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*token = nil\n-\t} else if f.authorizer.ACLWrite(&entCtx) != acl.Allow {\n-\t\t// no write permissions - redact secret\n-\t\tclone := *(*token)\n-\t\tclone.SecretID = RedactedToken\n-\t\t*token = &clone\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *token = nil\n+        } else if f.authorizer.ACLWrite(&entCtx) != acl.Allow {\n+                // no write permissions - redact secret\n+                clone := *(*token)\n+                clone.SecretID = RedactedToken\n+                *token = &clone\n+        }\n }\n \n func (f *Filter) filterTokens(tokens *structs.ACLTokens) {\n-\tret := make(structs.ACLTokens, 0, len(*tokens))\n-\tfor _, token := range *tokens {\n-\t\tfinal := token\n-\t\tf.filterToken(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*tokens = ret\n+        ret := make(structs.ACLTokens, 0, len(*tokens))\n+        for _, token := range *tokens {\n+                final := token\n+                f.filterToken(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *tokens = ret\n }\n \n func (f *Filter) filterTokenStub(token **structs.ACLTokenListStub) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif token == nil || *token == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if token == nil || *token == nil || f == nil {\n+                return\n+        }\n \n-\t(*token).FillAuthzContext(&entCtx)\n+        (*token).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t*token = nil\n-\t} else if f.authorizer.ACLWrite(&entCtx) != acl.Allow {\n-\t\t// no write permissions - redact secret\n-\t\tclone := *(*token)\n-\t\tclone.SecretID = RedactedToken\n-\t\t*token = &clone\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                *token = nil\n+        } else if f.authorizer.ACLWrite(&entCtx) != acl.Allow {\n+                // no write permissions - redact secret\n+                clone := *(*token)\n+                clone.SecretID = RedactedToken\n+                *token = &clone\n+        }\n }\n \n func (f *Filter) filterTokenStubs(tokens *[]*structs.ACLTokenListStub) {\n-\tret := make(structs.ACLTokenListStubs, 0, len(*tokens))\n-\tfor _, token := range *tokens {\n-\t\tfinal := token\n-\t\tf.filterTokenStub(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*tokens = ret\n+        ret := make(structs.ACLTokenListStubs, 0, len(*tokens))\n+        for _, token := range *tokens {\n+                final := token\n+                f.filterTokenStub(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *tokens = ret\n }\n \n func (f *Filter) filterPolicy(policy **structs.ACLPolicy) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif policy == nil || *policy == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if policy == nil || *policy == nil || f == nil {\n+                return\n+        }\n \n-\t(*policy).FillAuthzContext(&entCtx)\n+        (*policy).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*policy = nil\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *policy = nil\n+        }\n }\n \n func (f *Filter) filterPolicies(policies *structs.ACLPolicies) {\n-\tret := make(structs.ACLPolicies, 0, len(*policies))\n-\tfor _, policy := range *policies {\n-\t\tfinal := policy\n-\t\tf.filterPolicy(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*policies = ret\n+        ret := make(structs.ACLPolicies, 0, len(*policies))\n+        for _, policy := range *policies {\n+                final := policy\n+                f.filterPolicy(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *policies = ret\n }\n \n func (f *Filter) filterRole(role **structs.ACLRole) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif role == nil || *role == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if role == nil || *role == nil || f == nil {\n+                return\n+        }\n \n-\t(*role).FillAuthzContext(&entCtx)\n+        (*role).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*role = nil\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *role = nil\n+        }\n }\n \n func (f *Filter) filterRoles(roles *structs.ACLRoles) {\n-\tret := make(structs.ACLRoles, 0, len(*roles))\n-\tfor _, role := range *roles {\n-\t\tfinal := role\n-\t\tf.filterRole(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*roles = ret\n+        ret := make(structs.ACLRoles, 0, len(*roles))\n+        for _, role := range *roles {\n+                final := role\n+                f.filterRole(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *roles = ret\n }\n \n func (f *Filter) filterBindingRule(rule **structs.ACLBindingRule) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif rule == nil || *rule == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if rule == nil || *rule == nil || f == nil {\n+                return\n+        }\n \n-\t(*rule).FillAuthzContext(&entCtx)\n+        (*rule).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*rule = nil\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *rule = nil\n+        }\n }\n \n func (f *Filter) filterBindingRules(rules *structs.ACLBindingRules) {\n-\tret := make(structs.ACLBindingRules, 0, len(*rules))\n-\tfor _, rule := range *rules {\n-\t\tfinal := rule\n-\t\tf.filterBindingRule(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*rules = ret\n+        ret := make(structs.ACLBindingRules, 0, len(*rules))\n+        for _, rule := range *rules {\n+                final := rule\n+                f.filterBindingRule(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *rules = ret\n }\n \n func (f *Filter) filterAuthMethod(method **structs.ACLAuthMethod) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif method == nil || *method == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if method == nil || *method == nil || f == nil {\n+                return\n+        }\n \n-\t(*method).FillAuthzContext(&entCtx)\n+        (*method).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*method = nil\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *method = nil\n+        }\n }\n \n func (f *Filter) filterAuthMethods(methods *structs.ACLAuthMethods) {\n-\tret := make(structs.ACLAuthMethods, 0, len(*methods))\n-\tfor _, method := range *methods {\n-\t\tfinal := method\n-\t\tf.filterAuthMethod(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*methods = ret\n+        ret := make(structs.ACLAuthMethods, 0, len(*methods))\n+        for _, method := range *methods {\n+                final := method\n+                f.filterAuthMethod(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *methods = ret\n }\n \n func (f *Filter) filterServiceList(services *structs.ServiceList) bool {\n-\tret := make(structs.ServiceList, 0, len(*services))\n-\tvar removed bool\n-\tfor _, svc := range *services {\n-\t\tvar authzContext acl.AuthorizerContext\n+        ret := make(structs.ServiceList, 0, len(*services))\n+        var removed bool\n+        for _, svc := range *services {\n+                var authzContext acl.AuthorizerContext\n \n-\t\tsvc.FillAuthzContext(&authzContext)\n+                svc.FillAuthzContext(&authzContext)\n \n-\t\tif f.authorizer.ServiceRead(svc.Name, &authzContext) != acl.Allow {\n-\t\t\tremoved = true\n-\t\t\tsid := structs.NewServiceID(svc.Name, &svc.EnterpriseMeta)\n-\t\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", sid.String())\n-\t\t\tcontinue\n-\t\t}\n+                if f.authorizer.ServiceRead(svc.Name, &authzContext) != acl.Allow {\n+                        removed = true\n+                        sid := structs.NewServiceID(svc.Name, &svc.EnterpriseMeta)\n+                        f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", sid.String())\n+                        continue\n+                }\n \n-\t\tret = append(ret, svc)\n-\t}\n+                ret = append(ret, svc)\n+        }\n \n-\t*services = ret\n-\treturn removed\n+        *services = ret\n+        return removed\n }\n \n // filterGatewayServices is used to filter gateway to service mappings based on ACL rules.\n // Returns true if any elements were removed.\n func (f *Filter) filterGatewayServices(mappings *structs.GatewayServices) bool {\n-\tret := make(structs.GatewayServices, 0, len(*mappings))\n-\tvar removed bool\n-\tfor _, s := range *mappings {\n-\t\t// This filter only checks ServiceRead on the linked service.\n-\t\t// ServiceRead on the gateway is checked in the GatewayServices endpoint before filtering.\n-\t\tvar authzContext acl.AuthorizerContext\n-\t\ts.Service.FillAuthzContext(&authzContext)\n-\n-\t\tif f.authorizer.ServiceRead(s.Service.Name, &authzContext) != acl.Allow {\n-\t\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", s.Service.String())\n-\t\t\tremoved = true\n-\t\t\tcontinue\n-\t\t}\n-\t\tret = append(ret, s)\n-\t}\n-\t*mappings = ret\n-\treturn removed\n+        ret := make(structs.GatewayServices, 0, len(*mappings))\n+        var removed bool\n+        for _, s := range *mappings {\n+                // This filter only checks ServiceRead on the linked service.\n+                // ServiceRead on the gateway is checked in the GatewayServices endpoint before filtering.\n+                var authzContext acl.AuthorizerContext\n+                s.Service.FillAuthzContext(&authzContext)\n+\n+                if f.authorizer.ServiceRead(s.Service.Name, &authzContext) != acl.Allow {\n+                        f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", s.Service.String())\n+                        removed = true\n+                        continue\n+                }\n+                ret = append(ret, s)\n+        }\n+        *mappings = ret\n+        return removed\n }\ndiff --git a/agent/structs/aclfilter/filter_test.go b/agent/structs/aclfilter/filter_test.go\nindex fb54906af5..1b082c1d2e 100644\n--- a/agent/structs/aclfilter/filter_test.go\n+++ b/agent/structs/aclfilter/filter_test.go\n@@ -1,1286 +1,1286 @@\n package aclfilter\n \n import (\n-\t\"reflect\"\n-\t\"testing\"\n-\n-\t\"github.com/hashicorp/go-hclog\"\n-\t\"github.com/hashicorp/go-uuid\"\n-\t\"github.com/hashicorp/serf/coordinate\"\n-\t\"github.com/stretchr/testify/assert\"\n-\t\"github.com/stretchr/testify/require\"\n-\n-\t\"github.com/hashicorp/consul/acl\"\n-\t\"github.com/hashicorp/consul/agent/structs\"\n-\t\"github.com/hashicorp/consul/api\"\n-\t\"github.com/hashicorp/consul/types\"\n+        \"reflect\"\n+        \"testing\"\n+\n+        \"github.com/hashicorp/go-hclog\"\n+        \"github.com/hashicorp/go-uuid\"\n+        \"github.com/hashicorp/serf/coordinate\"\n+        \"github.com/stretchr/testify/assert\"\n+        \"github.com/stretchr/testify/require\"\n+\n+        \"github.com/hashicorp/consul/acl\"\n+        \"github.com/hashicorp/consul/agent/structs\"\n+        \"github.com/hashicorp/consul/api\"\n+        \"github.com/hashicorp/consul/types\"\n )\n \n func TestACL_filterImported_IndexedHealthChecks(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\ttype testCase struct {\n-\t\tpolicyRules string\n-\t\tlist        *structs.IndexedHealthChecks\n-\t\texpectEmpty bool\n-\t}\n+        type testCase struct {\n+                policyRules string\n+                list        *structs.IndexedHealthChecks\n+                expectEmpty bool\n+        }\n \n-\trun := func(t *testing.T, tc testCase) {\n-\t\tpolicy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n+        run := func(t *testing.T, tc testCase) {\n+                policy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tNew(authz, logger).Filter(tc.list)\n+                New(authz, logger).Filter(tc.list)\n \n-\t\tif tc.expectEmpty {\n-\t\t\trequire.Empty(t, tc.list.HealthChecks)\n-\t\t} else {\n-\t\t\trequire.Len(t, tc.list.HealthChecks, 1)\n-\t\t}\n-\t}\n+                if tc.expectEmpty {\n+                        require.Empty(t, tc.list.HealthChecks)\n+                } else {\n+                        require.Len(t, tc.list.HealthChecks, 1)\n+                }\n+        }\n \n-\ttt := map[string]testCase{\n-\t\t\"permissions for imports (Allowed)\": {\n-\t\t\tpolicyRules: `\n+        tt := map[string]testCase{\n+                \"permissions for imports (Allowed)\": {\n+                        policyRules: `\n service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedHealthChecks{\n-\t\t\t\tHealthChecks: structs.HealthChecks{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\tPeerName:    \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Can read imports with wildcard service/node reads in the importing partition.\n-\t\t\texpectEmpty: false,\n-\t\t},\n-\t\t\"permissions for local only (Deny)\": {\n-\t\t\tpolicyRules: `\n+                        list: &structs.IndexedHealthChecks{\n+                                HealthChecks: structs.HealthChecks{\n+                                        {\n+                                                Node:        \"node1\",\n+                                                CheckID:     \"check1\",\n+                                                ServiceName: \"foo\",\n+                                                PeerName:    \"some-peer\",\n+                                        },\n+                                },\n+                        },\n+                        // Can read imports with wildcard service/node reads in the importing partition.\n+                        expectEmpty: false,\n+                },\n+                \"permissions for local only (Deny)\": {\n+                        policyRules: `\n service \"foo\" { policy = \"read\" } node \"node1\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedHealthChecks{\n-\t\t\t\tHealthChecks: structs.HealthChecks{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\tPeerName:    \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Cannot read imports with rules referencing local resources with the same name\n-\t\t\t// as the imported ones.\n-\t\t\texpectEmpty: true,\n-\t\t},\n-\t}\n-\n-\tfor name, tc := range tt {\n-\t\tt.Run(name, func(t *testing.T) {\n-\t\t\trun(t, tc)\n-\t\t})\n-\t}\n+                        list: &structs.IndexedHealthChecks{\n+                                HealthChecks: structs.HealthChecks{\n+                                        {\n+                                                Node:        \"node1\",\n+                                                CheckID:     \"check1\",\n+                                                ServiceName: \"foo\",\n+                                                PeerName:    \"some-peer\",\n+                                        },\n+                                },\n+                        },\n+                        // Cannot read imports with rules referencing local resources with the same name\n+                        // as the imported ones.\n+                        expectEmpty: true,\n+                },\n+        }\n+\n+        for name, tc := range tt {\n+                t.Run(name, func(t *testing.T) {\n+                        run(t, tc)\n+                })\n+        }\n }\n \n func TestACL_filterImported_IndexedNodes(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\ttype testCase struct {\n-\t\tpolicyRules string\n-\t\tlist        *structs.IndexedNodes\n-\t\tconfigFunc  func(config *acl.Config)\n-\t\texpectEmpty bool\n-\t}\n-\n-\trun := func(t *testing.T, tc testCase) {\n-\t\tpolicy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tNew(authz, logger).Filter(tc.list)\n-\n-\t\tif tc.expectEmpty {\n-\t\t\trequire.Empty(t, tc.list.Nodes)\n-\t\t} else {\n-\t\t\trequire.Len(t, tc.list.Nodes, 1)\n-\t\t}\n-\t}\n-\n-\ttt := map[string]testCase{\n-\t\t\"permissions for imports (Allowed)\": {\n-\t\t\tpolicyRules: `\n-\t\tnode_prefix \"\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedNodes{\n-\t\t\t\tNodes: structs.Nodes{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tID:         types.NodeID(\"1\"),\n-\t\t\t\t\t\tNode:       \"foo\",\n-\t\t\t\t\t\tAddress:    \"127.0.0.1\",\n-\t\t\t\t\t\tDatacenter: \"dc1\",\n-\t\t\t\t\t\tPeerName:   \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Can read imports with wildcard service/node reads in the importing partition.\n-\t\t\texpectEmpty: false,\n-\t\t},\n-\t\t\"permissions for local only (Deny)\": {\n-\t\t\tpolicyRules: `\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        type testCase struct {\n+                policyRules string\n+                list        *structs.IndexedNodes\n+                configFunc  func(config *acl.Config)\n+                expectEmpty bool\n+        }\n+\n+        run := func(t *testing.T, tc testCase) {\n+                policy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                New(authz, logger).Filter(tc.list)\n+\n+                if tc.expectEmpty {\n+                        require.Empty(t, tc.list.Nodes)\n+                } else {\n+                        require.Len(t, tc.list.Nodes, 1)\n+                }\n+        }\n+\n+        tt := map[string]testCase{\n+                \"permissions for imports (Allowed)\": {\n+                        policyRules: `\n+                node_prefix \"\" { policy = \"read\" }`,\n+                        list: &structs.IndexedNodes{\n+                                Nodes: structs.Nodes{\n+                                        {\n+                                                ID:         types.NodeID(\"1\"),\n+                                                Node:       \"foo\",\n+                                                Address:    \"127.0.0.1\",\n+                                                Datacenter: \"dc1\",\n+                                                PeerName:   \"some-peer\",\n+                                        },\n+                                },\n+                        },\n+                        // Can read imports with wildcard service/node reads in the importing partition.\n+                        expectEmpty: false,\n+                },\n+                \"permissions for local only (Deny)\": {\n+                        policyRules: `\n node \"node1\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedNodes{\n-\t\t\t\tNodes: structs.Nodes{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tID:         types.NodeID(\"1\"),\n-\t\t\t\t\t\tNode:       \"node1\",\n-\t\t\t\t\t\tAddress:    \"127.0.0.1\",\n-\t\t\t\t\t\tDatacenter: \"dc1\",\n-\t\t\t\t\t\tPeerName:   \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Cannot read imports with rules referencing local resources with the same name\n-\t\t\t// as the imported ones.\n-\t\t\texpectEmpty: true,\n-\t\t},\n-\t}\n-\n-\tfor name, tc := range tt {\n-\t\tt.Run(name, func(t *testing.T) {\n-\t\t\trun(t, tc)\n-\t\t})\n-\t}\n+                        list: &structs.IndexedNodes{\n+                                Nodes: structs.Nodes{\n+                                        {\n+                                                ID:         types.NodeID(\"1\"),\n+                                                Node:       \"node1\",\n+                                                Address:    \"127.0.0.1\",\n+                                                Datacenter: \"dc1\",\n+                                                PeerName:   \"some-peer\",\n+                                        },\n+                                },\n+                        },\n+                        // Cannot read imports with rules referencing local resources with the same name\n+                        // as the imported ones.\n+                        expectEmpty: true,\n+                },\n+        }\n+\n+        for name, tc := range tt {\n+                t.Run(name, func(t *testing.T) {\n+                        run(t, tc)\n+                })\n+        }\n }\n \n func TestACL_filterImported_IndexedNodeServices(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\ttype testCase struct {\n-\t\tpolicyRules string\n-\t\tlist        *structs.IndexedNodeServices\n-\t\tconfigFunc  func(config *acl.Config)\n-\t\texpectEmpty bool\n-\t}\n+        type testCase struct {\n+                policyRules string\n+                list        *structs.IndexedNodeServices\n+                configFunc  func(config *acl.Config)\n+                expectEmpty bool\n+        }\n \n-\trun := func(t *testing.T, tc testCase) {\n-\t\tpolicy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n+        run := func(t *testing.T, tc testCase) {\n+                policy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tNew(authz, logger).Filter(tc.list)\n+                New(authz, logger).Filter(tc.list)\n \n-\t\tif tc.expectEmpty {\n-\t\t\trequire.Nil(t, tc.list.NodeServices)\n-\t\t} else {\n-\t\t\trequire.Len(t, tc.list.NodeServices.Services, 1)\n-\t\t}\n-\t}\n+                if tc.expectEmpty {\n+                        require.Nil(t, tc.list.NodeServices)\n+                } else {\n+                        require.Len(t, tc.list.NodeServices.Services, 1)\n+                }\n+        }\n \n-\ttt := map[string]testCase{\n-\t\t\"permissions for imports (Allowed)\": {\n-\t\t\tpolicyRules: `\n+        tt := map[string]testCase{\n+                \"permissions for imports (Allowed)\": {\n+                        policyRules: `\n service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedNodeServices{\n-\t\t\t\tNodeServices: &structs.NodeServices{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode:     \"node1\",\n-\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t\tServices: map[string]*structs.NodeService{\n-\t\t\t\t\t\t\"foo\": {\n-\t\t\t\t\t\t\tID:       \"foo\",\n-\t\t\t\t\t\t\tService:  \"foo\",\n-\t\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Can read imports with wildcard service/node reads in the importing partition.\n-\t\t\texpectEmpty: false,\n-\t\t},\n-\t\t\"permissions for local only (Deny)\": {\n-\t\t\tpolicyRules: `\n+                        list: &structs.IndexedNodeServices{\n+                                NodeServices: &structs.NodeServices{\n+                                        Node: &structs.Node{\n+                                                Node:     \"node1\",\n+                                                PeerName: \"some-peer\",\n+                                        },\n+                                        Services: map[string]*structs.NodeService{\n+                                                \"foo\": {\n+                                                        ID:       \"foo\",\n+                                                        Service:  \"foo\",\n+                                                        PeerName: \"some-peer\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                        // Can read imports with wildcard service/node reads in the importing partition.\n+                        expectEmpty: false,\n+                },\n+                \"permissions for local only (Deny)\": {\n+                        policyRules: `\n service \"foo\" { policy = \"read\" } node \"node1\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedNodeServices{\n-\t\t\t\tNodeServices: &structs.NodeServices{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode:     \"node1\",\n-\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t\tServices: map[string]*structs.NodeService{\n-\t\t\t\t\t\t\"foo\": {\n-\t\t\t\t\t\t\tID:       \"foo\",\n-\t\t\t\t\t\t\tService:  \"foo\",\n-\t\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Cannot read imports with rules referencing local resources with the same name\n-\t\t\t// as the imported ones.\n-\t\t\texpectEmpty: true,\n-\t\t},\n-\t}\n-\n-\tfor name, tc := range tt {\n-\t\tt.Run(name, func(t *testing.T) {\n-\t\t\trun(t, tc)\n-\t\t})\n-\t}\n+                        list: &structs.IndexedNodeServices{\n+                                NodeServices: &structs.NodeServices{\n+                                        Node: &structs.Node{\n+                                                Node:     \"node1\",\n+                                                PeerName: \"some-peer\",\n+                                        },\n+                                        Services: map[string]*structs.NodeService{\n+                                                \"foo\": {\n+                                                        ID:       \"foo\",\n+                                                        Service:  \"foo\",\n+                                                        PeerName: \"some-peer\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                        // Cannot read imports with rules referencing local resources with the same name\n+                        // as the imported ones.\n+                        expectEmpty: true,\n+                },\n+        }\n+\n+        for name, tc := range tt {\n+                t.Run(name, func(t *testing.T) {\n+                        run(t, tc)\n+                })\n+        }\n }\n \n func TestACL_filterImported_IndexedNodeServiceList(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\ttype testCase struct {\n-\t\tpolicyRules string\n-\t\tlist        *structs.IndexedNodeServiceList\n-\t\tconfigFunc  func(config *acl.Config)\n-\t\texpectEmpty bool\n-\t}\n+        type testCase struct {\n+                policyRules string\n+                list        *structs.IndexedNodeServiceList\n+                configFunc  func(config *acl.Config)\n+                expectEmpty bool\n+        }\n \n-\trun := func(t *testing.T, tc testCase) {\n-\t\tpolicy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n+        run := func(t *testing.T, tc testCase) {\n+                policy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tNew(authz, logger).Filter(tc.list)\n+                New(authz, logger).Filter(tc.list)\n \n-\t\tif tc.expectEmpty {\n-\t\t\trequire.Nil(t, tc.list.NodeServices.Node)\n-\t\t\trequire.Nil(t, tc.list.NodeServices.Services)\n-\t\t} else {\n-\t\t\trequire.Len(t, tc.list.NodeServices.Services, 1)\n-\t\t}\n-\t}\n+                if tc.expectEmpty {\n+                        require.Nil(t, tc.list.NodeServices.Node)\n+                        require.Nil(t, tc.list.NodeServices.Services)\n+                } else {\n+                        require.Len(t, tc.list.NodeServices.Services, 1)\n+                }\n+        }\n \n-\ttt := map[string]testCase{\n-\t\t\"permissions for imports (Allowed)\": {\n-\t\t\tpolicyRules: `\n+        tt := map[string]testCase{\n+                \"permissions for imports (Allowed)\": {\n+                        policyRules: `\n service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedNodeServiceList{\n-\t\t\t\tNodeServices: structs.NodeServiceList{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode:     \"node1\",\n-\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t\tServices: []*structs.NodeService{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tService:  \"foo\",\n-\t\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Can read imports with wildcard service/node reads in the importing partition.\n-\t\t\texpectEmpty: false,\n-\t\t},\n-\t\t\"permissions for local only (Deny)\": {\n-\t\t\tpolicyRules: `\n+                        list: &structs.IndexedNodeServiceList{\n+                                NodeServices: structs.NodeServiceList{\n+                                        Node: &structs.Node{\n+                                                Node:     \"node1\",\n+                                                PeerName: \"some-peer\",\n+                                        },\n+                                        Services: []*structs.NodeService{\n+                                                {\n+                                                        Service:  \"foo\",\n+                                                        PeerName: \"some-peer\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                        // Can read imports with wildcard service/node reads in the importing partition.\n+                        expectEmpty: false,\n+                },\n+                \"permissions for local only (Deny)\": {\n+                        policyRules: `\n service \"foo\" { policy = \"read\" } node \"node1\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedNodeServiceList{\n-\t\t\t\tNodeServices: structs.NodeServiceList{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode:     \"node1\",\n-\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t\tServices: []*structs.NodeService{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tService:  \"foo\",\n-\t\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Cannot read imports with rules referencing local resources with the same name\n-\t\t\t// as the imported ones.\n-\t\t\texpectEmpty: true,\n-\t\t},\n-\t}\n-\n-\tfor name, tc := range tt {\n-\t\tt.Run(name, func(t *testing.T) {\n-\t\t\trun(t, tc)\n-\t\t})\n-\t}\n+                        list: &structs.IndexedNodeServiceList{\n+                                NodeServices: structs.NodeServiceList{\n+                                        Node: &structs.Node{\n+                                                Node:     \"node1\",\n+                                                PeerName: \"some-peer\",\n+                                        },\n+                                        Services: []*structs.NodeService{\n+                                                {\n+                                                        Service:  \"foo\",\n+                                                        PeerName: \"some-peer\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                        // Cannot read imports with rules referencing local resources with the same name\n+                        // as the imported ones.\n+                        expectEmpty: true,\n+                },\n+        }\n+\n+        for name, tc := range tt {\n+                t.Run(name, func(t *testing.T) {\n+                        run(t, tc)\n+                })\n+        }\n }\n \n func TestACL_filterImported_IndexedServiceNodes(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\ttype testCase struct {\n-\t\tpolicyRules string\n-\t\tlist        *structs.IndexedServiceNodes\n-\t\tconfigFunc  func(config *acl.Config)\n-\t\texpectEmpty bool\n-\t}\n+        type testCase struct {\n+                policyRules string\n+                list        *structs.IndexedServiceNodes\n+                configFunc  func(config *acl.Config)\n+                expectEmpty bool\n+        }\n \n-\trun := func(t *testing.T, tc testCase) {\n-\t\tpolicy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n+        run := func(t *testing.T, tc testCase) {\n+                policy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tNew(authz, logger).Filter(tc.list)\n+                New(authz, logger).Filter(tc.list)\n \n-\t\tif tc.expectEmpty {\n-\t\t\trequire.Empty(t, tc.list.ServiceNodes)\n-\t\t} else {\n-\t\t\trequire.Len(t, tc.list.ServiceNodes, 1)\n-\t\t}\n-\t}\n+                if tc.expectEmpty {\n+                        require.Empty(t, tc.list.ServiceNodes)\n+                } else {\n+                        require.Len(t, tc.list.ServiceNodes, 1)\n+                }\n+        }\n \n-\ttt := map[string]testCase{\n-\t\t\"permissions for imports (Allowed)\": {\n-\t\t\tpolicyRules: `\n+        tt := map[string]testCase{\n+                \"permissions for imports (Allowed)\": {\n+                        policyRules: `\n service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedServiceNodes{\n-\t\t\t\tServiceNodes: structs.ServiceNodes{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\tPeerName:    \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Can read imports with wildcard service/node reads in the importing partition.\n-\t\t\texpectEmpty: false,\n-\t\t},\n-\t\t\"permissions for local only (Deny)\": {\n-\t\t\tpolicyRules: `\n+                        list: &structs.IndexedServiceNodes{\n+                                ServiceNodes: structs.ServiceNodes{\n+                                        {\n+                                                Node:        \"node1\",\n+                                                ServiceName: \"foo\",\n+                                                PeerName:    \"some-peer\",\n+                                        },\n+                                },\n+                        },\n+                        // Can read imports with wildcard service/node reads in the importing partition.\n+                        expectEmpty: false,\n+                },\n+                \"permissions for local only (Deny)\": {\n+                        policyRules: `\n service \"foo\" { policy = \"read\" } node \"node1\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.IndexedServiceNodes{\n-\t\t\t\tServiceNodes: structs.ServiceNodes{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\tPeerName:    \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Cannot read imports with rules referencing local resources with the same name\n-\t\t\t// as the imported ones.\n-\t\t\texpectEmpty: true,\n-\t\t},\n-\t}\n-\n-\tfor name, tc := range tt {\n-\t\tt.Run(name, func(t *testing.T) {\n-\t\t\trun(t, tc)\n-\t\t})\n-\t}\n+                        list: &structs.IndexedServiceNodes{\n+                                ServiceNodes: structs.ServiceNodes{\n+                                        {\n+                                                Node:        \"node1\",\n+                                                ServiceName: \"foo\",\n+                                                PeerName:    \"some-peer\",\n+                                        },\n+                                },\n+                        },\n+                        // Cannot read imports with rules referencing local resources with the same name\n+                        // as the imported ones.\n+                        expectEmpty: true,\n+                },\n+        }\n+\n+        for name, tc := range tt {\n+                t.Run(name, func(t *testing.T) {\n+                        run(t, tc)\n+                })\n+        }\n }\n \n func TestACL_filterImported_CheckServiceNode(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\ttype testCase struct {\n-\t\tpolicyRules string\n-\t\tlist        *structs.CheckServiceNodes\n-\t\tconfigFunc  func(config *acl.Config)\n-\t\texpectEmpty bool\n-\t}\n+        type testCase struct {\n+                policyRules string\n+                list        *structs.CheckServiceNodes\n+                configFunc  func(config *acl.Config)\n+                expectEmpty bool\n+        }\n \n-\trun := func(t *testing.T, tc testCase) {\n-\t\tpolicy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n+        run := func(t *testing.T, tc testCase) {\n+                policy, err := acl.NewPolicyFromSource(tc.policyRules, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tNew(authz, logger).Filter(tc.list)\n+                New(authz, logger).Filter(tc.list)\n \n-\t\tif tc.expectEmpty {\n-\t\t\trequire.Empty(t, tc.list)\n-\t\t} else {\n-\t\t\trequire.Len(t, *tc.list, 1)\n-\t\t}\n-\t}\n+                if tc.expectEmpty {\n+                        require.Empty(t, tc.list)\n+                } else {\n+                        require.Len(t, *tc.list, 1)\n+                }\n+        }\n \n-\ttt := map[string]testCase{\n-\t\t\"permissions for imports (Allowed)\": {\n-\t\t\tpolicyRules: `\n+        tt := map[string]testCase{\n+                \"permissions for imports (Allowed)\": {\n+                        policyRules: `\n service_prefix \"\" { policy = \"read\" } node_prefix \"\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.CheckServiceNodes{\n-\t\t\t\t{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode:     \"node1\",\n-\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tID:       \"foo\",\n-\t\t\t\t\t\tService:  \"foo\",\n-\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t\tPeerName:    \"some-peer\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Can read imports with wildcard service/node reads in the importing partition.\n-\t\t\texpectEmpty: false,\n-\t\t},\n-\t\t\"permissions for local only (Deny)\": {\n-\t\t\tpolicyRules: `\n+                        list: &structs.CheckServiceNodes{\n+                                {\n+                                        Node: &structs.Node{\n+                                                Node:     \"node1\",\n+                                                PeerName: \"some-peer\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                ID:       \"foo\",\n+                                                Service:  \"foo\",\n+                                                PeerName: \"some-peer\",\n+                                        },\n+                                        Checks: structs.HealthChecks{\n+                                                {\n+                                                        Node:        \"node1\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"foo\",\n+                                                        PeerName:    \"some-peer\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                        // Can read imports with wildcard service/node reads in the importing partition.\n+                        expectEmpty: false,\n+                },\n+                \"permissions for local only (Deny)\": {\n+                        policyRules: `\n service \"foo\" { policy = \"read\" } node \"node1\" { policy = \"read\" }`,\n-\t\t\tlist: &structs.CheckServiceNodes{\n-\t\t\t\t{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode:     \"node1\",\n-\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tID:       \"foo\",\n-\t\t\t\t\t\tService:  \"foo\",\n-\t\t\t\t\t\tPeerName: \"some-peer\",\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t\tPeerName:    \"some-peer\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\t// Cannot read imports with rules referencing local resources with the same name\n-\t\t\t// as the imported ones.\n-\t\t\texpectEmpty: true,\n-\t\t},\n-\t}\n-\n-\tfor name, tc := range tt {\n-\t\tt.Run(name, func(t *testing.T) {\n-\t\t\trun(t, tc)\n-\t\t})\n-\t}\n+                        list: &structs.CheckServiceNodes{\n+                                {\n+                                        Node: &structs.Node{\n+                                                Node:     \"node1\",\n+                                                PeerName: \"some-peer\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                ID:       \"foo\",\n+                                                Service:  \"foo\",\n+                                                PeerName: \"some-peer\",\n+                                        },\n+                                        Checks: structs.HealthChecks{\n+                                                {\n+                                                        Node:        \"node1\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"foo\",\n+                                                        PeerName:    \"some-peer\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                        // Cannot read imports with rules referencing local resources with the same name\n+                        // as the imported ones.\n+                        expectEmpty: true,\n+                },\n+        }\n+\n+        for name, tc := range tt {\n+                t.Run(name, func(t *testing.T) {\n+                        run(t, tc)\n+                })\n+        }\n }\n \n func TestACL_filterHealthChecks(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.IndexedHealthChecks {\n-\t\treturn &structs.IndexedHealthChecks{\n-\t\t\tHealthChecks: structs.HealthChecks{\n-\t\t\t\t{\n-\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\tt.Run(\"allowed\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.HealthChecks, 1)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.HealthChecks)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.HealthChecks)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"denied\", func(t *testing.T) {\n-\n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.HealthChecks)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.IndexedHealthChecks {\n+                return &structs.IndexedHealthChecks{\n+                        HealthChecks: structs.HealthChecks{\n+                                {\n+                                        Node:        \"node1\",\n+                                        CheckID:     \"check1\",\n+                                        ServiceName: \"foo\",\n+                                },\n+                        },\n+                }\n+        }\n+\n+        t.Run(\"allowed\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.HealthChecks, 1)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.HealthChecks)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.HealthChecks)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"denied\", func(t *testing.T) {\n+\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n+\n+                require.Empty(t, list.HealthChecks)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterIntentions(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\tmakeList := func() *structs.IndexedIntentions {\n-\t\treturn &structs.IndexedIntentions{\n-\t\t\tIntentions: structs.Intentions{\n-\t\t\t\t&structs.Intention{\n-\t\t\t\t\tID:              \"f004177f-2c28-83b7-4229-eacc25fe55d1\",\n-\t\t\t\t\tDestinationName: \"bar\",\n-\t\t\t\t},\n-\t\t\t\t&structs.Intention{\n-\t\t\t\t\tID:              \"f004177f-2c28-83b7-4229-eacc25fe55d2\",\n-\t\t\t\t\tDestinationName: \"foo\",\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n+        makeList := func() *structs.IndexedIntentions {\n+                return &structs.IndexedIntentions{\n+                        Intentions: structs.Intentions{\n+                                &structs.Intention{\n+                                        ID:              \"f004177f-2c28-83b7-4229-eacc25fe55d1\",\n+                                        DestinationName: \"bar\",\n+                                },\n+                                &structs.Intention{\n+                                        ID:              \"f004177f-2c28-83b7-4229-eacc25fe55d2\",\n+                                        DestinationName: \"foo\",\n+                                },\n+                        },\n+                }\n+        }\n \n-\tt.Run(\"allowed\", func(t *testing.T) {\n+        t.Run(\"allowed\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.AllowAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.AllowAll(), logger).Filter(list)\n \n-\t\trequire.Len(t, list.Intentions, 2)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n+                require.Len(t, list.Intentions, 2)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n \n-\tt.Run(\"allowed to read 1\", func(t *testing.T) {\n+        t.Run(\"allowed to read 1\", func(t *testing.T) {\n \n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n \n-\t\trequire.Len(t, list.Intentions, 1)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Len(t, list.Intentions, 1)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n \n-\tt.Run(\"denied\", func(t *testing.T) {\n+        t.Run(\"denied\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n \n-\t\trequire.Empty(t, list.Intentions)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Empty(t, list.Intentions)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterServices(t *testing.T) {\n-\tt.Parallel()\n-\n-\t// Create some services\n-\tservices := structs.Services{\n-\t\t\"service1\": []string{},\n-\t\t\"service2\": []string{},\n-\t\t\"consul\":   []string{},\n-\t}\n-\n-\t// Try permissive filtering.\n-\tfilt := New(acl.AllowAll(), nil)\n-\tremoved := filt.filterServices(services, nil)\n-\trequire.False(t, removed)\n-\trequire.Len(t, services, 3)\n-\n-\t// Try restrictive filtering.\n-\tfilt = New(acl.DenyAll(), nil)\n-\tremoved = filt.filterServices(services, nil)\n-\trequire.True(t, removed)\n-\trequire.Empty(t, services)\n+        t.Parallel()\n+\n+        // Create some services\n+        services := structs.Services{\n+                \"service1\": []string{},\n+                \"service2\": []string{},\n+                \"consul\":   []string{},\n+        }\n+\n+        // Try permissive filtering.\n+        filt := New(acl.AllowAll(), nil)\n+        removed := filt.filterServices(services, nil)\n+        require.False(t, removed)\n+        require.Len(t, services, 3)\n+\n+        // Try restrictive filtering.\n+        filt = New(acl.DenyAll(), nil)\n+        removed = filt.filterServices(services, nil)\n+        require.True(t, removed)\n+        require.Empty(t, services)\n }\n \n func TestACL_filterServiceNodes(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.IndexedServiceNodes {\n-\t\treturn &structs.IndexedServiceNodes{\n-\t\t\tServiceNodes: structs.ServiceNodes{\n-\t\t\t\t{\n-\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\tt.Run(\"allowed\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.ServiceNodes, 1)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.ServiceNodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"denied\", func(t *testing.T) {\n-\n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.ServiceNodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.IndexedServiceNodes {\n+                return &structs.IndexedServiceNodes{\n+                        ServiceNodes: structs.ServiceNodes{\n+                                {\n+                                        Node:        \"node1\",\n+                                        ServiceName: \"foo\",\n+                                },\n+                        },\n+                }\n+        }\n+\n+        t.Run(\"allowed\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.ServiceNodes, 1)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.ServiceNodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"denied\", func(t *testing.T) {\n+\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n+\n+                require.Empty(t, list.ServiceNodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterNodeServices(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.IndexedNodeServices {\n-\t\treturn &structs.IndexedNodeServices{\n-\t\t\tNodeServices: &structs.NodeServices{\n-\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t},\n-\t\t\t\tServices: map[string]*structs.NodeService{\n-\t\t\t\t\t\"foo\": {\n-\t\t\t\t\t\tID:      \"foo\",\n-\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\tt.Run(\"nil input\", func(t *testing.T) {\n-\n-\t\tlist := &structs.IndexedNodeServices{\n-\t\t\tNodeServices: nil,\n-\t\t}\n-\t\tNew(acl.AllowAll(), logger).Filter(list)\n-\n-\t\trequire.Nil(t, list.NodeServices)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"allowed\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.NodeServices.Services, 1)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Nil(t, list.NodeServices)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.NodeServices.Services)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"denied\", func(t *testing.T) {\n-\n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n-\n-\t\trequire.Nil(t, list.NodeServices)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.IndexedNodeServices {\n+                return &structs.IndexedNodeServices{\n+                        NodeServices: &structs.NodeServices{\n+                                Node: &structs.Node{\n+                                        Node: \"node1\",\n+                                },\n+                                Services: map[string]*structs.NodeService{\n+                                        \"foo\": {\n+                                                ID:      \"foo\",\n+                                                Service: \"foo\",\n+                                        },\n+                                },\n+                        },\n+                }\n+        }\n+\n+        t.Run(\"nil input\", func(t *testing.T) {\n+\n+                list := &structs.IndexedNodeServices{\n+                        NodeServices: nil,\n+                }\n+                New(acl.AllowAll(), logger).Filter(list)\n+\n+                require.Nil(t, list.NodeServices)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"allowed\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.NodeServices.Services, 1)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Nil(t, list.NodeServices)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.NodeServices.Services)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"denied\", func(t *testing.T) {\n+\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n+\n+                require.Nil(t, list.NodeServices)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterNodeServiceList(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\tmakeList := func() *structs.IndexedNodeServiceList {\n-\t\treturn &structs.IndexedNodeServiceList{\n-\t\t\tNodeServices: structs.NodeServiceList{\n-\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t},\n-\t\t\t\tServices: []*structs.NodeService{\n-\t\t\t\t\t{Service: \"foo\"},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n+        makeList := func() *structs.IndexedNodeServiceList {\n+                return &structs.IndexedNodeServiceList{\n+                        NodeServices: structs.NodeServiceList{\n+                                Node: &structs.Node{\n+                                        Node: \"node1\",\n+                                },\n+                                Services: []*structs.NodeService{\n+                                        {Service: \"foo\"},\n+                                },\n+                        },\n+                }\n+        }\n \n-\tt.Run(\"empty NodeServices\", func(t *testing.T) {\n+        t.Run(\"empty NodeServices\", func(t *testing.T) {\n \n-\t\tvar list structs.IndexedNodeServiceList\n-\t\tNew(acl.AllowAll(), logger).Filter(&list)\n+                var list structs.IndexedNodeServiceList\n+                New(acl.AllowAll(), logger).Filter(&list)\n \n-\t\trequire.Empty(t, list)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n+                require.Empty(t, list)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n \n-\tt.Run(\"allowed\", func(t *testing.T) {\n+        t.Run(\"allowed\", func(t *testing.T) {\n \n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n \n-\t\trequire.Len(t, list.NodeServices.Services, 1)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n+                require.Len(t, list.NodeServices.Services, 1)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n \n-\tt.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n+        t.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n \n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n \n-\t\trequire.Empty(t, list.NodeServices)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Empty(t, list.NodeServices)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n \n-\tt.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n+        t.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n \n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n \n-\t\trequire.NotEmpty(t, list.NodeServices.Node)\n-\t\trequire.Empty(t, list.NodeServices.Services)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.NotEmpty(t, list.NodeServices.Node)\n+                require.Empty(t, list.NodeServices.Services)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n \n-\tt.Run(\"denied\", func(t *testing.T) {\n+        t.Run(\"denied\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n \n-\t\trequire.Empty(t, list.NodeServices)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Empty(t, list.NodeServices)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterGatewayServices(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\tmakeList := func() *structs.IndexedGatewayServices {\n-\t\treturn &structs.IndexedGatewayServices{\n-\t\t\tServices: structs.GatewayServices{\n-\t\t\t\t{Service: structs.ServiceName{Name: \"foo\"}},\n-\t\t\t},\n-\t\t}\n-\t}\n+        makeList := func() *structs.IndexedGatewayServices {\n+                return &structs.IndexedGatewayServices{\n+                        Services: structs.GatewayServices{\n+                                {Service: structs.ServiceName{Name: \"foo\"}},\n+                        },\n+                }\n+        }\n \n-\tt.Run(\"allowed\", func(t *testing.T) {\n+        t.Run(\"allowed\", func(t *testing.T) {\n \n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n \n-\t\trequire.Len(t, list.Services, 1)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n+                require.Len(t, list.Services, 1)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n \n-\tt.Run(\"denied\", func(t *testing.T) {\n+        t.Run(\"denied\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n \n-\t\trequire.Empty(t, list.Services)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Empty(t, list.Services)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterCheckServiceNodes(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.IndexedCheckServiceNodes {\n-\t\treturn &structs.IndexedCheckServiceNodes{\n-\t\t\tNodes: structs.CheckServiceNodes{\n-\t\t\t\t{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tID:      \"foo\",\n-\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\tt.Run(\"allowed\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.Nodes, 1)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Nodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Nodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"denied\", func(t *testing.T) {\n-\n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Nodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.IndexedCheckServiceNodes {\n+                return &structs.IndexedCheckServiceNodes{\n+                        Nodes: structs.CheckServiceNodes{\n+                                {\n+                                        Node: &structs.Node{\n+                                                Node: \"node1\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                ID:      \"foo\",\n+                                                Service: \"foo\",\n+                                        },\n+                                        Checks: structs.HealthChecks{\n+                                                {\n+                                                        Node:        \"node1\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"foo\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                }\n+        }\n+\n+        t.Run(\"allowed\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.Nodes, 1)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.Nodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.Nodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"denied\", func(t *testing.T) {\n+\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n+\n+                require.Empty(t, list.Nodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterPreparedQueryExecuteResponse(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.PreparedQueryExecuteResponse {\n-\t\treturn &structs.PreparedQueryExecuteResponse{\n-\t\t\tNodes: structs.CheckServiceNodes{\n-\t\t\t\t{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tID:      \"foo\",\n-\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\tt.Run(\"allowed\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.Nodes, 1)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Nodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Nodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"denied\", func(t *testing.T) {\n-\n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Nodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.PreparedQueryExecuteResponse {\n+                return &structs.PreparedQueryExecuteResponse{\n+                        Nodes: structs.CheckServiceNodes{\n+                                {\n+                                        Node: &structs.Node{\n+                                                Node: \"node1\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                ID:      \"foo\",\n+                                                Service: \"foo\",\n+                                        },\n+                                        Checks: structs.HealthChecks{\n+                                                {\n+                                                        Node:        \"node1\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"foo\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                }\n+        }\n+\n+        t.Run(\"allowed\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.Nodes, 1)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.Nodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.Nodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"denied\", func(t *testing.T) {\n+\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n+\n+                require.Empty(t, list.Nodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterServiceTopology(t *testing.T) {\n-\tt.Parallel()\n-\t// Create some nodes.\n-\tfill := func() structs.ServiceTopology {\n-\t\treturn structs.ServiceTopology{\n-\t\t\tUpstreams: structs.CheckServiceNodes{\n-\t\t\t\tstructs.CheckServiceNode{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tID:      \"foo\",\n-\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t&structs.HealthCheck{\n-\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tDownstreams: structs.CheckServiceNodes{\n-\t\t\t\tstructs.CheckServiceNode{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode: \"node2\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tID:      \"bar\",\n-\t\t\t\t\t\tService: \"bar\",\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t&structs.HealthCheck{\n-\t\t\t\t\t\t\tNode:        \"node2\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"bar\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\toriginal := fill()\n-\n-\tt.Run(\"allow all without permissions\", func(t *testing.T) {\n-\t\ttopo := fill()\n-\t\tf := New(acl.AllowAll(), nil)\n-\n-\t\tfiltered := f.filterServiceTopology(&topo)\n-\t\tif filtered {\n-\t\t\tt.Fatalf(\"should not have been filtered\")\n-\t\t}\n-\t\tassert.Equal(t, original, topo)\n-\t})\n-\n-\tt.Run(\"deny all without permissions\", func(t *testing.T) {\n-\t\ttopo := fill()\n-\t\tf := New(acl.DenyAll(), nil)\n-\n-\t\tfiltered := f.filterServiceTopology(&topo)\n-\t\tif !filtered {\n-\t\t\tt.Fatalf(\"should have been marked as filtered\")\n-\t\t}\n-\t\tassert.Len(t, topo.Upstreams, 0)\n-\t\tassert.Len(t, topo.Upstreams, 0)\n-\t})\n-\n-\tt.Run(\"only upstream permissions\", func(t *testing.T) {\n-\t\trules := `\n+        t.Parallel()\n+        // Create some nodes.\n+        fill := func() structs.ServiceTopology {\n+                return structs.ServiceTopology{\n+                        Upstreams: structs.CheckServiceNodes{\n+                                structs.CheckServiceNode{\n+                                        Node: &structs.Node{\n+                                                Node: \"node1\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                ID:      \"foo\",\n+                                                Service: \"foo\",\n+                                        },\n+                                        Checks: structs.HealthChecks{\n+                                                &structs.HealthCheck{\n+                                                        Node:        \"node1\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"foo\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                        Downstreams: structs.CheckServiceNodes{\n+                                structs.CheckServiceNode{\n+                                        Node: &structs.Node{\n+                                                Node: \"node2\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                ID:      \"bar\",\n+                                                Service: \"bar\",\n+                                        },\n+                                        Checks: structs.HealthChecks{\n+                                                &structs.HealthCheck{\n+                                                        Node:        \"node2\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"bar\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                }\n+        }\n+        original := fill()\n+\n+        t.Run(\"allow all without permissions\", func(t *testing.T) {\n+                topo := fill()\n+                f := New(acl.AllowAll(), nil)\n+\n+                filtered := f.filterServiceTopology(&topo)\n+                if filtered {\n+                        t.Fatalf(\"should not have been filtered\")\n+                }\n+                assert.Equal(t, original, topo)\n+        })\n+\n+        t.Run(\"deny all without permissions\", func(t *testing.T) {\n+                topo := fill()\n+                f := New(acl.DenyAll(), nil)\n+\n+                filtered := f.filterServiceTopology(&topo)\n+                if !filtered {\n+                        t.Fatalf(\"should have been marked as filtered\")\n+                }\n+                assert.Len(t, topo.Upstreams, 0)\n+                assert.Len(t, topo.Upstreams, 0)\n+        })\n+\n+        t.Run(\"only upstream permissions\", func(t *testing.T) {\n+                rules := `\n node \"node1\" {\n   policy = \"read\"\n }\n service \"foo\" {\n   policy = \"read\"\n }`\n-\t\tpolicy, err := acl.NewPolicyFromSource(rules, acl.SyntaxLegacy, nil, nil)\n-\t\tif err != nil {\n-\t\t\tt.Fatalf(\"err %v\", err)\n-\t\t}\n-\t\tperms, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\tif err != nil {\n-\t\t\tt.Fatalf(\"err: %v\", err)\n-\t\t}\n-\n-\t\ttopo := fill()\n-\t\tf := New(perms, nil)\n-\n-\t\tfiltered := f.filterServiceTopology(&topo)\n-\t\tif !filtered {\n-\t\t\tt.Fatalf(\"should have been marked as filtered\")\n-\t\t}\n-\t\tassert.Equal(t, original.Upstreams, topo.Upstreams)\n-\t\tassert.Len(t, topo.Downstreams, 0)\n-\t})\n-\n-\tt.Run(\"only downstream permissions\", func(t *testing.T) {\n-\t\trules := `\n+                policy, err := acl.NewPolicyFromSource(rules, acl.SyntaxLegacy, nil, nil)\n+                if err != nil {\n+                        t.Fatalf(\"err %v\", err)\n+                }\n+                perms, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                if err != nil {\n+                        t.Fatalf(\"err: %v\", err)\n+                }\n+\n+                topo := fill()\n+                f := New(perms, nil)\n+\n+                filtered := f.filterServiceTopology(&topo)\n+                if !filtered {\n+                        t.Fatalf(\"should have been marked as filtered\")\n+                }\n+                assert.Equal(t, original.Upstreams, topo.Upstreams)\n+                assert.Len(t, topo.Downstreams, 0)\n+        })\n+\n+        t.Run(\"only downstream permissions\", func(t *testing.T) {\n+                rules := `\n node \"node2\" {\n   policy = \"read\"\n }\n service \"bar\" {\n   policy = \"read\"\n }`\n-\t\tpolicy, err := acl.NewPolicyFromSource(rules, acl.SyntaxLegacy, nil, nil)\n-\t\tif err != nil {\n-\t\t\tt.Fatalf(\"err %v\", err)\n-\t\t}\n-\t\tperms, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\tif err != nil {\n-\t\t\tt.Fatalf(\"err: %v\", err)\n-\t\t}\n-\n-\t\ttopo := fill()\n-\t\tf := New(perms, nil)\n-\n-\t\tfiltered := f.filterServiceTopology(&topo)\n-\t\tif !filtered {\n-\t\t\tt.Fatalf(\"should have been marked as filtered\")\n-\t\t}\n-\t\tassert.Equal(t, original.Downstreams, topo.Downstreams)\n-\t\tassert.Len(t, topo.Upstreams, 0)\n-\t})\n-\n-\tt.Run(\"upstream and downstream permissions\", func(t *testing.T) {\n-\t\trules := `\n+                policy, err := acl.NewPolicyFromSource(rules, acl.SyntaxLegacy, nil, nil)\n+                if err != nil {\n+                        t.Fatalf(\"err %v\", err)\n+                }\n+                perms, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                if err != nil {\n+                        t.Fatalf(\"err: %v\", err)\n+                }\n+\n+                topo := fill()\n+                f := New(perms, nil)\n+\n+                filtered := f.filterServiceTopology(&topo)\n+                if !filtered {\n+                        t.Fatalf(\"should have been marked as filtered\")\n+                }\n+                assert.Equal(t, original.Downstreams, topo.Downstreams)\n+                assert.Len(t, topo.Upstreams, 0)\n+        })\n+\n+        t.Run(\"upstream and downstream permissions\", func(t *testing.T) {\n+                rules := `\n node \"node1\" {\n   policy = \"read\"\n }\n@@ -1293,964 +1293,944 @@ node \"node2\" {\n service \"bar\" {\n   policy = \"read\"\n }`\n-\t\tpolicy, err := acl.NewPolicyFromSource(rules, acl.SyntaxLegacy, nil, nil)\n-\t\tif err != nil {\n-\t\t\tt.Fatalf(\"err %v\", err)\n-\t\t}\n-\t\tperms, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\tif err != nil {\n-\t\t\tt.Fatalf(\"err: %v\", err)\n-\t\t}\n-\n-\t\ttopo := fill()\n-\t\tf := New(perms, nil)\n-\n-\t\tfiltered := f.filterServiceTopology(&topo)\n-\t\tif filtered {\n-\t\t\tt.Fatalf(\"should not have been filtered\")\n-\t\t}\n-\n-\t\toriginal := fill()\n-\t\tassert.Equal(t, original, topo)\n-\t})\n+                policy, err := acl.NewPolicyFromSource(rules, acl.SyntaxLegacy, nil, nil)\n+                if err != nil {\n+                        t.Fatalf(\"err %v\", err)\n+                }\n+                perms, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                if err != nil {\n+                        t.Fatalf(\"err: %v\", err)\n+                }\n+\n+                topo := fill()\n+                f := New(perms, nil)\n+\n+                filtered := f.filterServiceTopology(&topo)\n+                if filtered {\n+                        t.Fatalf(\"should not have been filtered\")\n+                }\n+\n+                original := fill()\n+                assert.Equal(t, original, topo)\n+        })\n }\n \n func TestACL_filterCoordinates(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\tmakeList := func() *structs.IndexedCoordinates {\n-\t\treturn &structs.IndexedCoordinates{\n-\t\t\tCoordinates: structs.Coordinates{\n-\t\t\t\t{Node: \"node1\", Coord: coordinate.NewCoordinate(coordinate.DefaultConfig())},\n-\t\t\t\t{Node: \"node2\", Coord: coordinate.NewCoordinate(coordinate.DefaultConfig())},\n-\t\t\t},\n-\t\t}\n-\t}\n+        makeList := func() *structs.IndexedCoordinates {\n+                return &structs.IndexedCoordinates{\n+                        Coordinates: structs.Coordinates{\n+                                {Node: \"node1\", Coord: coordinate.NewCoordinate(coordinate.DefaultConfig())},\n+                                {Node: \"node2\", Coord: coordinate.NewCoordinate(coordinate.DefaultConfig())},\n+                        },\n+                }\n+        }\n \n-\tt.Run(\"allowed\", func(t *testing.T) {\n+        t.Run(\"allowed\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.AllowAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.AllowAll(), logger).Filter(list)\n \n-\t\trequire.Len(t, list.Coordinates, 2)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n+                require.Len(t, list.Coordinates, 2)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n \n-\tt.Run(\"allowed to read one node\", func(t *testing.T) {\n+        t.Run(\"allowed to read one node\", func(t *testing.T) {\n \n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n \n-\t\trequire.Len(t, list.Coordinates, 1)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Len(t, list.Coordinates, 1)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n \n-\tt.Run(\"denied\", func(t *testing.T) {\n+        t.Run(\"denied\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n \n-\t\trequire.Empty(t, list.Coordinates)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Empty(t, list.Coordinates)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterSessions(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\tmakeList := func() *structs.IndexedSessions {\n-\t\treturn &structs.IndexedSessions{\n-\t\t\tSessions: structs.Sessions{\n-\t\t\t\t{Node: \"foo\"},\n-\t\t\t\t{Node: \"bar\"},\n-\t\t\t},\n-\t\t}\n-\t}\n+        makeList := func() *structs.IndexedSessions {\n+                return &structs.IndexedSessions{\n+                        Sessions: structs.Sessions{\n+                                {Node: \"foo\"},\n+                                {Node: \"bar\"},\n+                        },\n+                }\n+        }\n \n-\tt.Run(\"all allowed\", func(t *testing.T) {\n+        t.Run(\"all allowed\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.AllowAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.AllowAll(), logger).Filter(list)\n \n-\t\trequire.Len(t, list.Sessions, 2)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n+                require.Len(t, list.Sessions, 2)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n \n-\tt.Run(\"just one node's sessions allowed\", func(t *testing.T) {\n+        t.Run(\"just one node's sessions allowed\", func(t *testing.T) {\n \n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tsession \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n+                policy, err := acl.NewPolicyFromSource(`\n+                        session \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n \n-\t\trequire.Len(t, list.Sessions, 1)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Len(t, list.Sessions, 1)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n \n-\tt.Run(\"denied\", func(t *testing.T) {\n+        t.Run(\"denied\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n \n-\t\trequire.Empty(t, list.Sessions)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Empty(t, list.Sessions)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterNodeDump(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.IndexedNodeDump {\n-\t\treturn &structs.IndexedNodeDump{\n-\t\t\tDump: structs.NodeDump{\n-\t\t\t\t{\n-\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t\tServices: []*structs.NodeService{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tID:      \"foo\",\n-\t\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: []*structs.HealthCheck{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\tt.Run(\"allowed\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.Dump, 1)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Dump)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.Dump, 1)\n-\t\trequire.Empty(t, list.Dump[0].Services)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"denied\", func(t *testing.T) {\n-\n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Dump)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.IndexedNodeDump {\n+                return &structs.IndexedNodeDump{\n+                        Dump: structs.NodeDump{\n+                                {\n+                                        Node: \"node1\",\n+                                        Services: []*structs.NodeService{\n+                                                {\n+                                                        ID:      \"foo\",\n+                                                        Service: \"foo\",\n+                                                },\n+                                        },\n+                                        Checks: []*structs.HealthCheck{\n+                                                {\n+                                                        Node:        \"node1\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"foo\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                }\n+        }\n+\n+        t.Run(\"allowed\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.Dump, 1)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.Dump)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.Dump, 1)\n+                require.Empty(t, list.Dump[0].Services)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"denied\", func(t *testing.T) {\n+\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n+\n+                require.Empty(t, list.Dump)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterNodes(t *testing.T) {\n-\tt.Parallel()\n-\n-\t// Create a nodes list.\n-\tnodes := structs.Nodes{\n-\t\t&structs.Node{\n-\t\t\tNode: \"foo\",\n-\t\t},\n-\t\t&structs.Node{\n-\t\t\tNode: \"bar\",\n-\t\t},\n-\t}\n-\n-\t// Try permissive filtering.\n-\tfilt := New(acl.AllowAll(), nil)\n-\tremoved := filt.filterNodes(&nodes)\n-\trequire.False(t, removed)\n-\trequire.Len(t, nodes, 2)\n-\n-\t// Try restrictive filtering\n-\tfilt = New(acl.DenyAll(), nil)\n-\tremoved = filt.filterNodes(&nodes)\n-\trequire.True(t, removed)\n-\trequire.Len(t, nodes, 0)\n+        t.Parallel()\n+\n+        // Create a nodes list.\n+        nodes := structs.Nodes{\n+                &structs.Node{\n+                        Node: \"foo\",\n+                },\n+                &structs.Node{\n+                        Node: \"bar\",\n+                },\n+        }\n+\n+        // Try permissive filtering.\n+        filt := New(acl.AllowAll(), nil)\n+        removed := filt.filterNodes(&nodes)\n+        require.False(t, removed)\n+        require.Len(t, nodes, 2)\n+\n+        // Try restrictive filtering\n+        filt = New(acl.DenyAll(), nil)\n+        removed = filt.filterNodes(&nodes)\n+        require.True(t, removed)\n+        require.Len(t, nodes, 0)\n }\n \n func TestACL_filterIndexedNodesWithGateways(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.IndexedNodesWithGateways {\n-\t\treturn &structs.IndexedNodesWithGateways{\n-\t\t\tNodes: structs.CheckServiceNodes{\n-\t\t\t\t{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tID:      \"foo\",\n-\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tGateways: structs.GatewayServices{\n-\t\t\t\t{Service: structs.ServiceNameFromString(\"foo\")},\n-\t\t\t\t{Service: structs.ServiceNameFromString(\"bar\")},\n-\t\t\t},\n-\t\t\tImportedNodes: structs.CheckServiceNodes{\n-\t\t\t\t{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode:     \"imported-node\",\n-\t\t\t\t\t\tPeerName: \"cluster-02\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tID:       \"zip\",\n-\t\t\t\t\t\tService:  \"zip\",\n-\t\t\t\t\t\tPeerName: \"cluster-02\",\n-\t\t\t\t\t},\n-\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t{\n-\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\tServiceName: \"zip\",\n-\t\t\t\t\t\t\tPeerName:    \"cluster-02\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\ttype testCase struct {\n-\t\tauthzFn func() acl.Authorizer\n-\t\texpect  *structs.IndexedNodesWithGateways\n-\t}\n-\n-\trun := func(t *testing.T, tc testCase) {\n-\t\tauthz := tc.authzFn()\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Equal(t, tc.expect, list)\n-\t}\n-\n-\ttt := map[string]testCase{\n-\t\t\"not filtered\": {\n-\t\t\tauthzFn: func() acl.Authorizer {\n-\t\t\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"baz\" {\n-\t\t\t\tpolicy = \"write\"\n-\t\t\t}\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice \"bar\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\t\t\trequire.NoError(t, err)\n-\n-\t\t\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\t\t\trequire.NoError(t, err)\n-\t\t\t\treturn authz\n-\t\t\t},\n-\t\t\texpect: &structs.IndexedNodesWithGateways{\n-\t\t\t\tNodes: structs.CheckServiceNodes{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\t\tID:      \"foo\",\n-\t\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t\t{\n-\t\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t\t},\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t\tGateways: structs.GatewayServices{\n-\t\t\t\t\t{Service: structs.ServiceNameFromString(\"foo\")},\n-\t\t\t\t\t{Service: structs.ServiceNameFromString(\"bar\")},\n-\t\t\t\t},\n-\t\t\t\t// Service write to \"bar\" allows reading all imported services\n-\t\t\t\tImportedNodes: structs.CheckServiceNodes{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\t\tNode:     \"imported-node\",\n-\t\t\t\t\t\t\tPeerName: \"cluster-02\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\t\tID:       \"zip\",\n-\t\t\t\t\t\t\tService:  \"zip\",\n-\t\t\t\t\t\t\tPeerName: \"cluster-02\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t\t{\n-\t\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\t\tServiceName: \"zip\",\n-\t\t\t\t\t\t\t\tPeerName:    \"cluster-02\",\n-\t\t\t\t\t\t\t},\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t\tQueryMeta: structs.QueryMeta{ResultsFilteredByACLs: false},\n-\t\t\t},\n-\t\t},\n-\t\t\"not allowed to read the node\": {\n-\t\t\tauthzFn: func() acl.Authorizer {\n-\t\t\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice \"bar\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\t\t\trequire.NoError(t, err)\n-\n-\t\t\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\t\t\trequire.NoError(t, err)\n-\t\t\t\treturn authz\n-\t\t\t},\n-\t\t\texpect: &structs.IndexedNodesWithGateways{\n-\t\t\t\tNodes: structs.CheckServiceNodes{},\n-\t\t\t\tGateways: structs.GatewayServices{\n-\t\t\t\t\t{Service: structs.ServiceNameFromString(\"foo\")},\n-\t\t\t\t\t{Service: structs.ServiceNameFromString(\"bar\")},\n-\t\t\t\t},\n-\t\t\t\tImportedNodes: structs.CheckServiceNodes{},\n-\t\t\t\tQueryMeta:     structs.QueryMeta{ResultsFilteredByACLs: true},\n-\t\t\t},\n-\t\t},\n-\t\t\"not allowed to read the service\": {\n-\t\t\tauthzFn: func() acl.Authorizer {\n-\t\t\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice \"bar\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\t\t\trequire.NoError(t, err)\n-\n-\t\t\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\t\t\trequire.NoError(t, err)\n-\t\t\t\treturn authz\n-\t\t\t},\n-\t\t\texpect: &structs.IndexedNodesWithGateways{\n-\t\t\t\tNodes: structs.CheckServiceNodes{},\n-\t\t\t\tGateways: structs.GatewayServices{\n-\t\t\t\t\t{Service: structs.ServiceNameFromString(\"bar\")},\n-\t\t\t\t},\n-\t\t\t\tImportedNodes: structs.CheckServiceNodes{},\n-\t\t\t\tQueryMeta:     structs.QueryMeta{ResultsFilteredByACLs: true},\n-\t\t\t},\n-\t\t},\n-\t\t\"not allowed to read the other gateway service\": {\n-\t\t\tauthzFn: func() acl.Authorizer {\n-\t\t\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\t\t\trequire.NoError(t, err)\n-\n-\t\t\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\t\t\trequire.NoError(t, err)\n-\t\t\t\treturn authz\n-\t\t\t},\n-\t\t\texpect: &structs.IndexedNodesWithGateways{\n-\t\t\t\tNodes: structs.CheckServiceNodes{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\t\tID:      \"foo\",\n-\t\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t\t},\n-\t\t\t\t\t\tChecks: structs.HealthChecks{\n-\t\t\t\t\t\t\t{\n-\t\t\t\t\t\t\t\tNode:        \"node1\",\n-\t\t\t\t\t\t\t\tCheckID:     \"check1\",\n-\t\t\t\t\t\t\t\tServiceName: \"foo\",\n-\t\t\t\t\t\t\t},\n-\t\t\t\t\t\t},\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t\tGateways: structs.GatewayServices{\n-\t\t\t\t\t{Service: structs.ServiceNameFromString(\"foo\")},\n-\t\t\t\t},\n-\t\t\t\tImportedNodes: structs.CheckServiceNodes{},\n-\t\t\t\tQueryMeta:     structs.QueryMeta{ResultsFilteredByACLs: true},\n-\t\t\t},\n-\t\t},\n-\t\t\"denied\": {\n-\t\t\tauthzFn: acl.DenyAll,\n-\t\t\texpect: &structs.IndexedNodesWithGateways{\n-\t\t\t\tNodes:         structs.CheckServiceNodes{},\n-\t\t\t\tGateways:      structs.GatewayServices{},\n-\t\t\t\tImportedNodes: structs.CheckServiceNodes{},\n-\t\t\t\tQueryMeta:     structs.QueryMeta{ResultsFilteredByACLs: true},\n-\t\t\t},\n-\t\t},\n-\t}\n-\n-\tfor name, tc := range tt {\n-\t\tt.Run(name, func(t *testing.T) {\n-\t\t\trun(t, tc)\n-\t\t})\n-\t}\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.IndexedNodesWithGateways {\n+                return &structs.IndexedNodesWithGateways{\n+                        Nodes: structs.CheckServiceNodes{\n+                                {\n+                                        Node: &structs.Node{\n+                                                Node: \"node1\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                ID:      \"foo\",\n+                                                Service: \"foo\",\n+                                        },\n+                                        Checks: structs.HealthChecks{\n+                                                {\n+                                                        Node:        \"node1\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"foo\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                        Gateways: structs.GatewayServices{\n+                                {Service: structs.ServiceNameFromString(\"foo\")},\n+                                {Service: structs.ServiceNameFromString(\"bar\")},\n+                        },\n+                        ImportedNodes: structs.CheckServiceNodes{\n+                                {\n+                                        Node: &structs.Node{\n+                                                Node:     \"imported-node\",\n+                                                PeerName: \"cluster-02\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                ID:       \"zip\",\n+                                                Service:  \"zip\",\n+                                                PeerName: \"cluster-02\",\n+                                        },\n+                                        Checks: structs.HealthChecks{\n+                                                {\n+                                                        Node:        \"node1\",\n+                                                        CheckID:     \"check1\",\n+                                                        ServiceName: \"zip\",\n+                                                        PeerName:    \"cluster-02\",\n+                                                },\n+                                        },\n+                                },\n+                        },\n+                }\n+        }\n+\n+        type testCase struct {\n+                authzFn func() acl.Authorizer\n+                expect  *structs.IndexedNodesWithGateways\n+        }\n+\n+        run := func(t *testing.T, tc testCase) {\n+                authz := tc.authzFn()\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Equal(t, tc.expect, list)\n+        }\n+\n+        tt := map[string]testCase{\n+                \"not filtered\": {\n+                        authzFn: func() acl.Authorizer {\n+                                policy, err := acl.NewPolicyFromSource(`\n+                        service \"baz\" {\n+                                policy = \"write\"\n+                        }\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        service \"bar\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                                require.NoError(t, err)\n+\n+                                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                                require.NoError(t, err)\n+                                return authz\n+                        },\n+                        expect: &structs.IndexedNodesWithGateways{\n+                                Nodes: structs.CheckServiceNodes{\n+                                        {\n+                                                Node: &structs.Node{\n+                                                        Node: \"node1\",\n+                                                },\n+                                                Service: &structs.NodeService{\n+                                                        ID:      \"foo\",\n+                                                        Service: \"foo\",\n+                                                },\n+                                                Checks: structs.HealthChecks{\n+                                                        {\n+                                                                Node:        \"node1\",\n+                                                                CheckID:     \"check1\",\n+                                                                ServiceName: \"foo\",\n+                                                        },\n+                                                },\n+                                        },\n+                                },\n+                                Gateways: structs.GatewayServices{\n+                                        {Service: structs.ServiceNameFromString(\"foo\")},\n+                                        {Service: structs.ServiceNameFromString(\"bar\")},\n+                                },\n+                                require.NotNil(t, actual.ImportedDump[0])\n+                                require.Nil(t, actual.ImportedDump[0].Services)\n+                                QueryMeta: structs.QueryMeta{ResultsFilteredByACLs: false},\n+                        },\n+                },\n+                \"not allowed to read the node\": {\n+                        authzFn: func() acl.Authorizer {\n+                                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        service \"bar\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                                require.NoError(t, err)\n+\n+                                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                                require.NoError(t, err)\n+                                return authz\n+                        },\n+                        expect: &structs.IndexedNodesWithGateways{\n+                                Nodes: structs.CheckServiceNodes{},\n+                                Gateways: structs.GatewayServices{\n+                                        {Service: structs.ServiceNameFromString(\"foo\")},\n+                                        {Service: structs.ServiceNameFromString(\"bar\")},\n+                                },\n+                                ImportedNodes: structs.CheckServiceNodes{},\n+                                QueryMeta:     structs.QueryMeta{ResultsFilteredByACLs: true},\n+                        },\n+                },\n+                \"not allowed to read the service\": {\n+                        authzFn: func() acl.Authorizer {\n+                                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                        service \"bar\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                                require.NoError(t, err)\n+\n+                                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                                require.NoError(t, err)\n+                                return authz\n+                        },\n+                        expect: &structs.IndexedNodesWithGateways{\n+                                Nodes: structs.CheckServiceNodes{},\n+                                Gateways: structs.GatewayServices{\n+                                        {Service: structs.ServiceNameFromString(\"bar\")},\n+                                },\n+                                ImportedNodes: structs.CheckServiceNodes{},\n+                                QueryMeta:     structs.QueryMeta{ResultsFilteredByACLs: true},\n+                        },\n+                },\n+                \"not allowed to read the other gateway service\": {\n+                        authzFn: func() acl.Authorizer {\n+                                policy, err := acl.NewPolicyFromSource(`\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                                require.NoError(t, err)\n+\n+                                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                                require.NoError(t, err)\n+                                return authz\n+                        },\n+                        expect: &structs.IndexedNodesWithGateways{\n+                                Nodes: structs.CheckServiceNodes{\n+                                        {\n+                                                Node: &structs.Node{\n+                                                        Node: \"node1\",\n+                                                },\n+                                                Service: &structs.NodeService{\n+                                                        ID:      \"foo\",\n+                                                        Service: \"foo\",\n+                                                },\n+                                                Checks: structs.HealthChecks{\n+                                                        {\n+                                                                Node:        \"node1\",\n+                                                                CheckID:     \"check1\",\n+                                                                ServiceName: \"foo\",\n+                                                        },\n+                                                },\n+                                        },\n+                                },\n+                                Gateways: structs.GatewayServices{\n+                                        {Service: structs.ServiceNameFromString(\"foo\")},\n+                                },\n+                                ImportedNodes: structs.CheckServiceNodes{},\n+                                QueryMeta:     structs.QueryMeta{ResultsFilteredByACLs: true},\n+                        },\n+                },\n+                \"denied\": {\n+                        authzFn: acl.DenyAll,\n+                        expect: &structs.IndexedNodesWithGateways{\n+                                Nodes:         structs.CheckServiceNodes{},\n+                                Gateways:      structs.GatewayServices{},\n+                                ImportedNodes: structs.CheckServiceNodes{},\n+                                QueryMeta:     structs.QueryMeta{ResultsFilteredByACLs: true},\n+                        },\n+                },\n+        }\n+\n+        for name, tc := range tt {\n+                t.Run(name, func(t *testing.T) {\n+                        run(t, tc)\n+                })\n+        }\n }\n \n func TestACL_filterIndexedServiceDump(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.IndexedServiceDump {\n-\t\treturn &structs.IndexedServiceDump{\n-\t\t\tDump: structs.ServiceDump{\n-\t\t\t\t{\n-\t\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\t\tNode: \"node1\",\n-\t\t\t\t\t},\n-\t\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\t\tService: \"foo\",\n-\t\t\t\t\t},\n-\t\t\t\t\tGatewayService: &structs.GatewayService{\n-\t\t\t\t\t\tService: structs.ServiceNameFromString(\"foo\"),\n-\t\t\t\t\t\tGateway: structs.ServiceNameFromString(\"foo-gateway\"),\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t\t// No node information.\n-\t\t\t\t{\n-\t\t\t\t\tGatewayService: &structs.GatewayService{\n-\t\t\t\t\t\tService: structs.ServiceNameFromString(\"bar\"),\n-\t\t\t\t\t\tGateway: structs.ServiceNameFromString(\"bar-gateway\"),\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\tt.Run(\"allowed\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice_prefix \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice_prefix \"bar\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.Dump, 2)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"not allowed to access node\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice_prefix \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice_prefix \"bar\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.Dump, 1)\n-\t\trequire.Equal(t, \"bar\", list.Dump[0].GatewayService.Service.Name)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"not allowed to access service\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice \"foo-gateway\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Dump)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"not allowed to access gateway\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode \"node1\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice \"foo\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.Dump)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.IndexedServiceDump {\n+                return &structs.IndexedServiceDump{\n+                        Dump: structs.ServiceDump{\n+                                {\n+                                        Node: &structs.Node{\n+                                                Node: \"node1\",\n+                                        },\n+                                        Service: &structs.NodeService{\n+                                                Service: \"foo\",\n+                                        },\n+                                        GatewayService: &structs.GatewayService{\n+                                                Service: structs.ServiceNameFromString(\"foo\"),\n+                                                Gateway: structs.ServiceNameFromString(\"foo-gateway\"),\n+                                        },\n+                                },\n+                                // No node information.\n+                                {\n+                                        GatewayService: &structs.GatewayService{\n+                                                Service: structs.ServiceNameFromString(\"bar\"),\n+                                                Gateway: structs.ServiceNameFromString(\"bar-gateway\"),\n+                                        },\n+                                },\n+                        },\n+                }\n+        }\n+\n+        t.Run(\"allowed\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                        service_prefix \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        service_prefix \"bar\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.Dump, 2)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"not allowed to access node\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service_prefix \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                        service_prefix \"bar\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.Dump, 1)\n+                require.Equal(t, \"bar\", list.Dump[0].GatewayService.Service.Name)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"not allowed to access service\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                        service \"foo-gateway\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.Dump)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"not allowed to access gateway\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node \"node1\" {\n+                          policy = \"read\"\n+                        }\n+                        service \"foo\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.Dump)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterDatacenterCheckServiceNodes(t *testing.T) {\n-\tt.Parallel()\n-\n-\tlogger := hclog.NewNullLogger()\n-\n-\tmakeList := func() *structs.DatacenterIndexedCheckServiceNodes {\n-\t\tt.Helper()\n-\n-\t\tnode := func(dc, node, ip string) structs.CheckServiceNode {\n-\t\t\tt.Helper()\n-\n-\t\t\tid, err := uuid.GenerateUUID()\n-\t\t\trequire.NoError(t, err)\n-\n-\t\t\treturn structs.CheckServiceNode{\n-\t\t\t\tNode: &structs.Node{\n-\t\t\t\t\tID:         types.NodeID(id),\n-\t\t\t\t\tNode:       node,\n-\t\t\t\t\tDatacenter: dc,\n-\t\t\t\t\tAddress:    ip,\n-\t\t\t\t},\n-\t\t\t\tService: &structs.NodeService{\n-\t\t\t\t\tID:      \"mesh-gateway\",\n-\t\t\t\t\tService: \"mesh-gateway\",\n-\t\t\t\t\tKind:    structs.ServiceKindMeshGateway,\n-\t\t\t\t\tPort:    9999,\n-\t\t\t\t\tMeta:    map[string]string{structs.MetaWANFederationKey: \"1\"},\n-\t\t\t\t},\n-\t\t\t\tChecks: []*structs.HealthCheck{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tName:      \"web connectivity\",\n-\t\t\t\t\t\tStatus:    api.HealthPassing,\n-\t\t\t\t\t\tServiceID: \"mesh-gateway\",\n-\t\t\t\t\t},\n-\t\t\t\t},\n-\t\t\t}\n-\t\t}\n-\n-\t\treturn &structs.DatacenterIndexedCheckServiceNodes{\n-\t\t\tDatacenterNodes: map[string]structs.CheckServiceNodes{\n-\t\t\t\t\"dc1\": []structs.CheckServiceNode{\n-\t\t\t\t\tnode(\"dc1\", \"gateway1a\", \"1.2.3.4\"),\n-\t\t\t\t\tnode(\"dc1\", \"gateway2a\", \"4.3.2.1\"),\n-\t\t\t\t},\n-\t\t\t\t\"dc2\": []structs.CheckServiceNode{\n-\t\t\t\t\tnode(\"dc2\", \"gateway1b\", \"5.6.7.8\"),\n-\t\t\t\t\tnode(\"dc2\", \"gateway2b\", \"8.7.6.5\"),\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n-\n-\tt.Run(\"allowed\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode_prefix \"\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t\tservice_prefix \"\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Len(t, list.DatacenterNodes[\"dc1\"], 2)\n-\t\trequire.Len(t, list.DatacenterNodes[\"dc2\"], 2)\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n-\n-\tt.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tservice_prefix \"\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.DatacenterNodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n-\n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tnode_prefix \"\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxCurrent, nil, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n-\n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.DatacenterNodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n-\n-\tt.Run(\"denied\", func(t *testing.T) {\n-\n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n-\n-\t\trequire.Empty(t, list.DatacenterNodes)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+        t.Parallel()\n+\n+        logger := hclog.NewNullLogger()\n+\n+        makeList := func() *structs.DatacenterIndexedCheckServiceNodes {\n+                t.Helper()\n+\n+                node := func(dc, node, ip string) structs.CheckServiceNode {\n+                        t.Helper()\n+\n+                        id, err := uuid.GenerateUUID()\n+                        require.NoError(t, err)\n+\n+                        return structs.CheckServiceNode{\n+                                Node: &structs.Node{\n+                                        ID:         types.NodeID(id),\n+                                        Node:       node,\n+                                        Datacenter: dc,\n+                                        Address:    ip,\n+                                },\n+                                Service: &structs.NodeService{\n+                                        ID:      \"mesh-gateway\",\n+                                        Service: \"mesh-gateway\",\n+                                        Kind:    structs.ServiceKindMeshGateway,\n+                                        Port:    9999,\n+                                        Meta:    map[string]string{structs.MetaWANFederationKey: \"1\"},\n+                                },\n+                                Checks: []*structs.HealthCheck{\n+                                        {\n+                                                Name:      \"web connectivity\",\n+                                                Status:    api.HealthPassing,\n+                                                ServiceID: \"mesh-gateway\",\n+                                        },\n+                                },\n+                        }\n+                }\n+\n+                return &structs.DatacenterIndexedCheckServiceNodes{\n+                        DatacenterNodes: map[string]structs.CheckServiceNodes{\n+                                \"dc1\": []structs.CheckServiceNode{\n+                                        node(\"dc1\", \"gateway1a\", \"1.2.3.4\"),\n+                                        node(\"dc1\", \"gateway2a\", \"4.3.2.1\"),\n+                                },\n+                                \"dc2\": []structs.CheckServiceNode{\n+                                        node(\"dc2\", \"gateway1b\", \"5.6.7.8\"),\n+                                        node(\"dc2\", \"gateway2b\", \"8.7.6.5\"),\n+                                },\n+                        },\n+                }\n+        }\n+\n+        t.Run(\"allowed\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node_prefix \"\" {\n+                          policy = \"read\"\n+                        }\n+                        service_prefix \"\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Len(t, list.DatacenterNodes[\"dc1\"], 2)\n+                require.Len(t, list.DatacenterNodes[\"dc2\"], 2)\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n+\n+        t.Run(\"allowed to read the service, but not the node\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        service_prefix \"\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.DatacenterNodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"allowed to read the node, but not the service\", func(t *testing.T) {\n+\n+                policy, err := acl.NewPolicyFromSource(`\n+                        node_prefix \"\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxCurrent, nil, nil)\n+                require.NoError(t, err)\n+\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n+\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n+\n+                require.Empty(t, list.DatacenterNodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n+\n+        t.Run(\"denied\", func(t *testing.T) {\n+\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n+\n+                require.Empty(t, list.DatacenterNodes)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_redactPreparedQueryTokens(t *testing.T) {\n-\tt.Parallel()\n-\tquery := &structs.PreparedQuery{\n-\t\tID:    \"f004177f-2c28-83b7-4229-eacc25fe55d1\",\n-\t\tToken: \"root\",\n-\t}\n-\n-\texpected := &structs.PreparedQuery{\n-\t\tID:    \"f004177f-2c28-83b7-4229-eacc25fe55d1\",\n-\t\tToken: \"root\",\n-\t}\n-\n-\t// Try permissive filtering with a management token. This will allow the\n-\t// embedded token to be seen.\n-\tfilt := New(acl.ManageAll(), nil)\n-\tfilt.redactPreparedQueryTokens(&query)\n-\tif !reflect.DeepEqual(query, expected) {\n-\t\tt.Fatalf(\"bad: %#v\", &query)\n-\t}\n-\n-\t// Hang on to the entry with a token, which needs to survive the next\n-\t// operation.\n-\toriginal := query\n-\n-\t// Now try permissive filtering with a client token, which should cause\n-\t// the embedded token to get redacted.\n-\tfilt = New(acl.AllowAll(), nil)\n-\tfilt.redactPreparedQueryTokens(&query)\n-\texpected.Token = RedactedToken\n-\tif !reflect.DeepEqual(query, expected) {\n-\t\tt.Fatalf(\"bad: %#v\", *query)\n-\t}\n-\n-\t// Make sure that the original object didn't lose its token.\n-\tif original.Token != \"root\" {\n-\t\tt.Fatalf(\"bad token: %s\", original.Token)\n-\t}\n+        t.Parallel()\n+        query := &structs.PreparedQuery{\n+                ID:    \"f004177f-2c28-83b7-4229-eacc25fe55d1\",\n+                Token: \"root\",\n+        }\n+\n+        expected := &structs.PreparedQuery{\n+                ID:    \"f004177f-2c28-83b7-4229-eacc25fe55d1\",\n+                Token: \"root\",\n+        }\n+\n+        // Try permissive filtering with a management token. This will allow the\n+        // embedded token to be seen.\n+        filt := New(acl.ManageAll(), nil)\n+        filt.redactPreparedQueryTokens(&query)\n+        if !reflect.DeepEqual(query, expected) {\n+                t.Fatalf(\"bad: %#v\", &query)\n+        }\n+\n+        // Hang on to the entry with a token, which needs to survive the next\n+        // operation.\n+        original := query\n+\n+        // Now try permissive filtering with a client token, which should cause\n+        // the embedded token to get redacted.\n+        filt = New(acl.AllowAll(), nil)\n+        filt.redactPreparedQueryTokens(&query)\n+        expected.Token = RedactedToken\n+        if !reflect.DeepEqual(query, expected) {\n+                t.Fatalf(\"bad: %#v\", *query)\n+        }\n+\n+        // Make sure that the original object didn't lose its token.\n+        if original.Token != \"root\" {\n+                t.Fatalf(\"bad token: %s\", original.Token)\n+        }\n }\n \n func TestFilterACL_redactTokenSecret(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\ttoken := &structs.ACLToken{\n-\t\tAccessorID: \"6a5e25b3-28f2-4085-9012-c3fb754314d1\",\n-\t\tSecretID:   \"6a5e25b3-28f2-4085-9012-c3fb754314d1\",\n-\t}\n+        token := &structs.ACLToken{\n+                AccessorID: \"6a5e25b3-28f2-4085-9012-c3fb754314d1\",\n+                SecretID:   \"6a5e25b3-28f2-4085-9012-c3fb754314d1\",\n+        }\n \n-\tNew(policy(t, `acl = \"write\"`), nil).Filter(&token)\n-\trequire.Equal(t, \"6a5e25b3-28f2-4085-9012-c3fb754314d1\", token.SecretID)\n+        New(policy(t, `acl = \"write\"`), nil).Filter(&token)\n+        require.Equal(t, \"6a5e25b3-28f2-4085-9012-c3fb754314d1\", token.SecretID)\n \n-\tNew(policy(t, `acl = \"read\"`), nil).Filter(&token)\n-\trequire.Equal(t, RedactedToken, token.SecretID)\n+        New(policy(t, `acl = \"read\"`), nil).Filter(&token)\n+        require.Equal(t, RedactedToken, token.SecretID)\n }\n \n func TestFilterACL_redactTokenSecrets(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\ttokens := structs.ACLTokens{\n-\t\t&structs.ACLToken{\n-\t\t\tAccessorID: \"6a5e25b3-28f2-4085-9012-c3fb754314d1\",\n-\t\t\tSecretID:   \"6a5e25b3-28f2-4085-9012-c3fb754314d1\",\n-\t\t},\n-\t}\n+        tokens := structs.ACLTokens{\n+                &structs.ACLToken{\n+                        AccessorID: \"6a5e25b3-28f2-4085-9012-c3fb754314d1\",\n+                        SecretID:   \"6a5e25b3-28f2-4085-9012-c3fb754314d1\",\n+                },\n+        }\n \n-\tNew(policy(t, `acl = \"write\"`), nil).Filter(&tokens)\n-\trequire.Equal(t, \"6a5e25b3-28f2-4085-9012-c3fb754314d1\", tokens[0].SecretID)\n+        New(policy(t, `acl = \"write\"`), nil).Filter(&tokens)\n+        require.Equal(t, \"6a5e25b3-28f2-4085-9012-c3fb754314d1\", tokens[0].SecretID)\n \n-\tNew(policy(t, `acl = \"read\"`), nil).Filter(&tokens)\n-\trequire.Equal(t, RedactedToken, tokens[0].SecretID)\n+        New(policy(t, `acl = \"read\"`), nil).Filter(&tokens)\n+        require.Equal(t, RedactedToken, tokens[0].SecretID)\n }\n \n func TestACL_filterPreparedQueries(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\tmakeList := func() *structs.IndexedPreparedQueries {\n-\t\treturn &structs.IndexedPreparedQueries{\n-\t\t\tQueries: structs.PreparedQueries{\n-\t\t\t\t{ID: \"f004177f-2c28-83b7-4229-eacc25fe55d1\"},\n-\t\t\t\t{\n-\t\t\t\t\tID:   \"f004177f-2c28-83b7-4229-eacc25fe55d2\",\n-\t\t\t\t\tName: \"query-with-no-token\",\n-\t\t\t\t},\n-\t\t\t\t{\n-\t\t\t\t\tID:    \"f004177f-2c28-83b7-4229-eacc25fe55d3\",\n-\t\t\t\t\tName:  \"query-with-a-token\",\n-\t\t\t\t\tToken: \"root\",\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}\n-\t}\n+        makeList := func() *structs.IndexedPreparedQueries {\n+                return &structs.IndexedPreparedQueries{\n+                        Queries: structs.PreparedQueries{\n+                                {ID: \"f004177f-2c28-83b7-4229-eacc25fe55d1\"},\n+                                {\n+                                        ID:   \"f004177f-2c28-83b7-4229-eacc25fe55d2\",\n+                                        Name: \"query-with-no-token\",\n+                                },\n+                                {\n+                                        ID:    \"f004177f-2c28-83b7-4229-eacc25fe55d3\",\n+                                        Name:  \"query-with-a-token\",\n+                                        Token: \"root\",\n+                                },\n+                        },\n+                }\n+        }\n \n-\tt.Run(\"management token\", func(t *testing.T) {\n+        t.Run(\"management token\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.ManageAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.ManageAll(), logger).Filter(list)\n \n-\t\t// Check we get the un-named query.\n-\t\trequire.Len(t, list.Queries, 3)\n+                // Check we get the un-named query.\n+                require.Len(t, list.Queries, 3)\n \n-\t\t// Check we get the un-redacted token.\n-\t\trequire.Equal(t, \"root\", list.Queries[2].Token)\n+                // Check we get the un-redacted token.\n+                require.Equal(t, \"root\", list.Queries[2].Token)\n \n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n \n-\tt.Run(\"permissive filtering\", func(t *testing.T) {\n+        t.Run(\"permissive filtering\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tqueryWithToken := list.Queries[2]\n+                list := makeList()\n+                queryWithToken := list.Queries[2]\n \n-\t\tNew(acl.AllowAll(), logger).Filter(list)\n+                New(acl.AllowAll(), logger).Filter(list)\n \n-\t\t// Check the un-named query is filtered out.\n-\t\trequire.Len(t, list.Queries, 2)\n+                // Check the un-named query is filtered out.\n+                require.Len(t, list.Queries, 2)\n \n-\t\t// Check the token is redacted.\n-\t\trequire.Equal(t, RedactedToken, list.Queries[1].Token)\n+                // Check the token is redacted.\n+                require.Equal(t, RedactedToken, list.Queries[1].Token)\n \n-\t\t// Check the original object is unmodified.\n-\t\trequire.Equal(t, \"root\", queryWithToken.Token)\n+                // Check the original object is unmodified.\n+                require.Equal(t, \"root\", queryWithToken.Token)\n \n-\t\t// ResultsFilteredByACLs should not include un-named queries, which are only\n-\t\t// readable by a management token.\n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t})\n+                // ResultsFilteredByACLs should not include un-named queries, which are only\n+                // readable by a management token.\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+        })\n \n-\tt.Run(\"limited access\", func(t *testing.T) {\n+        t.Run(\"limited access\", func(t *testing.T) {\n \n-\t\tpolicy, err := acl.NewPolicyFromSource(`\n-\t\t\tquery \"query-with-a-token\" {\n-\t\t\t  policy = \"read\"\n-\t\t\t}\n-\t\t`, acl.SyntaxLegacy, nil, nil)\n-\t\trequire.NoError(t, err)\n+                policy, err := acl.NewPolicyFromSource(`\n+                        query \"query-with-a-token\" {\n+                          policy = \"read\"\n+                        }\n+                `, acl.SyntaxLegacy, nil, nil)\n+                require.NoError(t, err)\n \n-\t\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\t\trequire.NoError(t, err)\n+                authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+                require.NoError(t, err)\n \n-\t\tlist := makeList()\n-\t\tNew(authz, logger).Filter(list)\n+                list := makeList()\n+                New(authz, logger).Filter(list)\n \n-\t\t// Check we only get the query we have access to.\n-\t\trequire.Len(t, list.Queries, 1)\n+                // Check we only get the query we have access to.\n+                require.Len(t, list.Queries, 1)\n \n-\t\t// Check the token is redacted.\n-\t\trequire.Equal(t, RedactedToken, list.Queries[0].Token)\n+                // Check the token is redacted.\n+                require.Equal(t, RedactedToken, list.Queries[0].Token)\n \n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n \n-\tt.Run(\"restrictive filtering\", func(t *testing.T) {\n+        t.Run(\"restrictive filtering\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n \n-\t\trequire.Empty(t, list.Queries)\n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t})\n+                require.Empty(t, list.Queries)\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+        })\n }\n \n func TestACL_filterServiceList(t *testing.T) {\n-\tlogger := hclog.NewNullLogger()\n+        logger := hclog.NewNullLogger()\n \n-\tmakeList := func() *structs.IndexedServiceList {\n-\t\treturn &structs.IndexedServiceList{\n-\t\t\tServices: structs.ServiceList{\n-\t\t\t\t{Name: \"foo\"},\n-\t\t\t\t{Name: \"bar\"},\n-\t\t\t},\n-\t\t}\n-\t}\n+        makeList := func() *structs.IndexedServiceList {\n+                return &structs.IndexedServiceList{\n+                        Services: structs.ServiceList{\n+                                {Name: \"foo\"},\n+                                {Name: \"bar\"},\n+                        },\n+                }\n+        }\n \n-\tt.Run(\"permissive filtering\", func(t *testing.T) {\n+        t.Run(\"permissive filtering\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.AllowAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.AllowAll(), logger).Filter(list)\n \n-\t\trequire.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n-\t\trequire.Len(t, list.Services, 2)\n-\t})\n+                require.False(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be false\")\n+                require.Len(t, list.Services, 2)\n+        })\n \n-\tt.Run(\"restrictive filtering\", func(t *testing.T) {\n+        t.Run(\"restrictive filtering\", func(t *testing.T) {\n \n-\t\tlist := makeList()\n-\t\tNew(acl.DenyAll(), logger).Filter(list)\n+                list := makeList()\n+                New(acl.DenyAll(), logger).Filter(list)\n \n-\t\trequire.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n-\t\trequire.Empty(t, list.Services)\n-\t})\n+                require.True(t, list.QueryMeta.ResultsFilteredByACLs, \"ResultsFilteredByACLs should be true\")\n+                require.Empty(t, list.Services)\n+        })\n }\n \n func TestACL_unhandledFilterType(t *testing.T) {\n-\tt.Parallel()\n+        t.Parallel()\n \n-\tfilter := New(acl.AllowAll(), nil)\n+        filter := New(acl.AllowAll(), nil)\n \n-\trequire.Panics(t, func() {\n-\t\tfilter.Filter(&structs.HealthCheck{})\n-\t})\n+        require.Panics(t, func() {\n+                filter.Filter(&structs.HealthCheck{})\n+        })\n }\n \n func policy(t *testing.T, hcl string) acl.Authorizer {\n-\tt.Helper()\n+        t.Helper()\n \n-\tpolicy, err := acl.NewPolicyFromSource(hcl, acl.SyntaxCurrent, nil, nil)\n-\trequire.NoError(t, err)\n+        policy, err := acl.NewPolicyFromSource(hcl, acl.SyntaxCurrent, nil, nil)\n+        require.NoError(t, err)\n \n-\tauthz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n-\trequire.NoError(t, err)\n+        authz, err := acl.NewPolicyAuthorizerWithDefaults(acl.DenyAll(), []*acl.Policy{policy}, nil)\n+        require.NoError(t, err)\n \n-\treturn authz\n+        return authz\n }\n"}
{"cve":"CVE-2024-5138:0708", "fix_patch": "diff --git a/overlord/hookstate/ctlcmd/ctlcmd.go b/overlord/hookstate/ctlcmd/ctlcmd.go\nindex b663420a3f..300d0991a4 100644\n--- a/overlord/hookstate/ctlcmd/ctlcmd.go\n+++ b/overlord/hookstate/ctlcmd/ctlcmd.go\n@@ -1,193 +1,199 @@\n-// -*- Mode: Go; indent-tabs-mode: t -*-\n-\n-/*\n- * Copyright (C) 2021 Canonical Ltd\n- *\n- * This program is free software: you can redistribute it and/or modify\n- * it under the terms of the GNU General Public License version 3 as\n- * published by the Free Software Foundation.\n- *\n- * This program is distributed in the hope that it will be useful,\n- * but WITHOUT ANY WARRANTY; without even the implied warranty of\n- * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n- * GNU General Public License for more details.\n- *\n- * You should have received a copy of the GNU General Public License\n- * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n- *\n- */\n-\n-// Package ctlcmd contains the various snapctl subcommands.\n-package ctlcmd\n-\n-import (\n-\t\"bytes\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"strconv\"\n-\n-\t\"github.com/jessevdk/go-flags\"\n-\n-\t\"github.com/snapcore/snapd/logger\"\n-\t\"github.com/snapcore/snapd/overlord/hookstate\"\n-\t\"github.com/snapcore/snapd/strutil\"\n-)\n-\n-type MissingContextError struct {\n-\tsubcommand string\n-}\n-\n-func (e *MissingContextError) Error() string {\n-\treturn fmt.Sprintf(`cannot invoke snapctl operation commands (here %q) from outside of a snap`, e.subcommand)\n-}\n-\n-type baseCommand struct {\n-\tstdout io.Writer\n-\tstderr io.Writer\n-\tc      *hookstate.Context\n-\tname   string\n-\tuid    string\n-}\n-\n-func (c *baseCommand) setName(name string) {\n-\tc.name = name\n-}\n-\n-func (c *baseCommand) setUid(uid uint32) {\n-\tc.uid = strconv.FormatUint(uint64(uid), 10)\n-}\n-\n-func (c *baseCommand) setStdout(w io.Writer) {\n-\tc.stdout = w\n-}\n-\n-func (c *baseCommand) printf(format string, a ...interface{}) {\n-\tif c.stdout != nil {\n-\t\tfmt.Fprintf(c.stdout, format, a...)\n-\t}\n-}\n-\n-func (c *baseCommand) setStderr(w io.Writer) {\n-\tc.stderr = w\n-}\n-\n-func (c *baseCommand) errorf(format string, a ...interface{}) {\n-\tif c.stderr != nil {\n-\t\tfmt.Fprintf(c.stderr, format, a...)\n-\t}\n-}\n-\n-func (c *baseCommand) setContext(context *hookstate.Context) {\n-\tc.c = context\n-}\n-\n-func (c *baseCommand) context() *hookstate.Context {\n-\treturn c.c\n-}\n-\n-func (c *baseCommand) ensureContext() (context *hookstate.Context, err error) {\n-\tif c.c == nil {\n-\t\terr = &MissingContextError{c.name}\n-\t}\n-\treturn c.c, err\n-}\n-\n-type command interface {\n-\tsetName(name string)\n-\tsetUid(uid uint32)\n-\n-\tsetStdout(w io.Writer)\n-\tsetStderr(w io.Writer)\n-\n-\tsetContext(context *hookstate.Context)\n-\tcontext() *hookstate.Context\n-\n-\tExecute(args []string) error\n-}\n-\n-type commandInfo struct {\n-\tshortHelp string\n-\tlongHelp  string\n-\tgenerator func() command\n-\thidden    bool\n-}\n-\n-var commands = make(map[string]*commandInfo)\n-\n-func addCommand(name, shortHelp, longHelp string, generator func() command) *commandInfo {\n-\tcmd := &commandInfo{\n-\t\tshortHelp: shortHelp,\n-\t\tlongHelp:  longHelp,\n-\t\tgenerator: generator,\n-\t}\n-\tcommands[name] = cmd\n-\treturn cmd\n-}\n-\n-// UnsuccessfulError carries a specific exit code to be returned to the client.\n-type UnsuccessfulError struct {\n-\tExitCode int\n-}\n-\n-func (e UnsuccessfulError) Error() string {\n-\treturn fmt.Sprintf(\"unsuccessful with exit code: %d\", e.ExitCode)\n-}\n-\n-// ForbiddenCommandError conveys that a command cannot be invoked in some context\n-type ForbiddenCommandError struct {\n-\tMessage string\n-}\n-\n-func (f ForbiddenCommandError) Error() string {\n-\treturn f.Message\n-}\n-\n-// nonRootAllowed lists the commands that can be performed even when snapctl\n-// is invoked not by root.\n-var nonRootAllowed = []string{\"get\", \"services\", \"set-health\", \"is-connected\", \"system-mode\", \"model\"}\n-\n-// Run runs the requested command.\n-func Run(context *hookstate.Context, args []string, uid uint32) (stdout, stderr []byte, err error) {\n-\tif len(args) == 0 {\n-\t\treturn nil, nil, fmt.Errorf(\"internal error: snapctl cannot run without args\")\n-\t}\n-\n-\tif !isAllowedToRun(uid, args) {\n-\t\treturn nil, nil, &ForbiddenCommandError{Message: fmt.Sprintf(\"cannot use %q with uid %d, try with sudo\", args[0], uid)}\n-\t}\n-\n-\tparser := flags.NewNamedParser(\"snapctl\", flags.PassDoubleDash|flags.HelpFlag)\n-\n-\t// Create stdout/stderr buffers, and make sure commands use them.\n-\tvar stdoutBuffer bytes.Buffer\n-\tvar stderrBuffer bytes.Buffer\n-\tfor name, cmdInfo := range commands {\n-\t\tcmd := cmdInfo.generator()\n-\t\tcmd.setName(name)\n-\t\tcmd.setUid(uid)\n-\t\tcmd.setStdout(&stdoutBuffer)\n-\t\tcmd.setStderr(&stderrBuffer)\n-\t\tcmd.setContext(context)\n-\n-\t\ttheCmd, err := parser.AddCommand(name, cmdInfo.shortHelp, cmdInfo.longHelp, cmd)\n-\t\ttheCmd.Hidden = cmdInfo.hidden\n-\t\tif err != nil {\n-\t\t\tlogger.Panicf(\"cannot add command %q: %s\", name, err)\n-\t\t}\n-\t}\n-\n-\t_, err = parser.ParseArgs(args)\n-\treturn stdoutBuffer.Bytes(), stderrBuffer.Bytes(), err\n-}\n-\n-func isAllowedToRun(uid uint32, args []string) bool {\n-\t// A command can run if any of the following are true:\n-\t//\t* It runs as root\n-\t//\t* It's contained in nonRootAllowed\n-\t//\t* It's used with the -h or --help flags\n-\t// note: commands still need valid context and snaps can only access own config.\n-\treturn uid == 0 ||\n-\t\tstrutil.ListContains(nonRootAllowed, args[0]) ||\n-\t\tstrutil.ListContains(args, \"-h\") ||\n-\t\tstrutil.ListContains(args, \"--help\")\n-}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])// -*- Mode: Go; indent-tabs-mode: t -*-\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])/*\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * Copyright (C) 2021 Canonical Ltd\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) *\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * This program is free software: you can redistribute it and/or modify\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * it under the terms of the GNU General Public License version 3 as\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * published by the Free Software Foundation.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) *\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * This program is distributed in the hope that it will be useful,\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * but WITHOUT ANY WARRANTY; without even the implied warranty of\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * GNU General Public License for more details.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) *\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * You should have received a copy of the GNU General Public License\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) * along with this program.  If not, see <http://www.gnu.org/licenses/>.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) *\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) */\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])// Package ctlcmd contains the various snapctl subcommands.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])package ctlcmd\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])import (\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        \"bytes\"\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        \"fmt\"\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        \"io\"\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        \"strconv\"\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        \"github.com/jessevdk/go-flags\"\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        \"github.com/snapcore/snapd/logger\"\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        \"github.com/snapcore/snapd/overlord/hookstate\"\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        \"github.com/snapcore/snapd/strutil\"\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]))\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])type MissingContextError struct {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        subcommand string\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (e *MissingContextError) Error() string {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        return fmt.Sprintf(`cannot invoke snapctl operation commands (here %q) from outside of a snap`, e.subcommand)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])type baseCommand struct {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        stdout io.Writer\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        stderr io.Writer\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        c      *hookstate.Context\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        name   string\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        uid    string\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) setName(name string) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        c.name = name\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) setUid(uid uint32) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        c.uid = strconv.FormatUint(uint64(uid), 10)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) setStdout(w io.Writer) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        c.stdout = w\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) printf(format string, a ...interface{}) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        if c.stdout != nil {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                fmt.Fprintf(c.stdout, format, a...)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        }\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) setStderr(w io.Writer) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        c.stderr = w\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) errorf(format string, a ...interface{}) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        if c.stderr != nil {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                fmt.Fprintf(c.stderr, format, a...)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        }\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) setContext(context *hookstate.Context) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        c.c = context\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) context() *hookstate.Context {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        return c.c\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (c *baseCommand) ensureContext() (context *hookstate.Context, err error) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        if c.c == nil {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                err = &MissingContextError{c.name}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        }\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        return c.c, err\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])type command interface {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        setName(name string)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        setUid(uid uint32)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        setStdout(w io.Writer)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        setStderr(w io.Writer)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        setContext(context *hookstate.Context)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        context() *hookstate.Context\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        Execute(args []string) error\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])type commandInfo struct {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        shortHelp string\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        longHelp  string\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        generator func() command\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        hidden    bool\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])var commands = make(map[string]*commandInfo)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func addCommand(name, shortHelp, longHelp string, generator func() command) *commandInfo {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        cmd := &commandInfo{\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                shortHelp: shortHelp,\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                longHelp:  longHelp,\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                generator: generator,\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        }\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        commands[name] = cmd\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        return cmd\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])// UnsuccessfulError carries a specific exit code to be returned to the client.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])type UnsuccessfulError struct {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        ExitCode int\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (e UnsuccessfulError) Error() string {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        return fmt.Sprintf(\"unsuccessful with exit code: %d\", e.ExitCode)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])// ForbiddenCommandError conveys that a command cannot be invoked in some context\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])type ForbiddenCommandError struct {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        Message string\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func (f ForbiddenCommandError) Error() string {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        return f.Message\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])// nonRootAllowed lists the commands that can be performed even when snapctl\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])// is invoked not by root.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])var nonRootAllowed = []string{\"get\", \"services\", \"set-health\", \"is-connected\", \"system-mode\", \"model\"}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])// Run runs the requested command.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func Run(context *hookstate.Context, args []string, uid uint32) (stdout, stderr []byte, err error) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        if len(args) == 0 {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                return nil, nil, fmt.Errorf(\"internal error: snapctl cannot run without args\")\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        }\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        if !isAllowedToRun(uid, args) {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                return nil, nil, &ForbiddenCommandError{Message: fmt.Sprintf(\"cannot use %q with uid %d, try with sudo\", args[0], uid)}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        }\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        parser := flags.NewNamedParser(\"snapctl\", flags.PassDoubleDash|flags.HelpFlag)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        // Create stdout/stderr buffers, and make sure commands use them.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        var stdoutBuffer bytes.Buffer\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        var stderrBuffer bytes.Buffer\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        for name, cmdInfo := range commands {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                cmd := cmdInfo.generator()\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                cmd.setName(name)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                cmd.setUid(uid)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                cmd.setStdout(&stdoutBuffer)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                cmd.setStderr(&stderrBuffer)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                cmd.setContext(context)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                theCmd, err := parser.AddCommand(name, cmdInfo.shortHelp, cmdInfo.longHelp, cmd)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                theCmd.Hidden = cmdInfo.hidden\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                if err != nil {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                        logger.Panicf(\"cannot add command %q: %s\", name, err)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])                }\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        }\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        _, err = parser.ParseArgs(args)\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        return stdoutBuffer.Bytes(), stderrBuffer.Bytes(), err\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])func isAllowedToRun(uid uint32, args []string) bool {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        // A command can run if any of the following are true:\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        //      * It runs as root\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        //      * It's contained in nonRootAllowed\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        //      * It's used with the -h or --help flags\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])        // note: commands still need valid context and snaps can only access own config.\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])|| strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0]) uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])|| strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\n+if strutil.ListContains(args, \"-h\") || strutil.ListContains(args, \"--help\") {\\n\\t\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])\\n\\t}\\n\\treturn uid == 0 || strutil.ListContains(nonRootAllowed, args[0])}\n"}
{"cve":"CVE-2024-27302:0708", "fix_patch": ""}
{"cve":"CVE-2021-21291:0708", "fix_patch": "diff --git a/oauthproxy.go b/oauthproxy.go\nindex 36c58c46..70ca0fe5 100644\n--- a/oauthproxy.go\n+++ b/oauthproxy.go\n@@ -1,933 +1,933 @@\n package main\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"html/template\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/justinas/alice\"\n-\tipapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/ip\"\n-\tmiddlewareapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/middleware\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/options\"\n-\tsessionsapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/authentication/basic\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/cookies\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/encryption\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/ip\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/logger\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/middleware\"\n-\trequestutil \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/requests/util\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/upstream\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/providers\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"html/template\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"regexp\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/justinas/alice\"\n+        ipapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/ip\"\n+        middlewareapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/middleware\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/options\"\n+        sessionsapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/authentication/basic\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/cookies\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/encryption\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/ip\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/logger\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/middleware\"\n+        requestutil \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/requests/util\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/upstream\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/providers\"\n )\n \n const (\n-\tschemeHTTPS     = \"https\"\n-\tapplicationJSON = \"application/json\"\n+        schemeHTTPS     = \"https\"\n+        applicationJSON = \"application/json\"\n )\n \n var (\n-\t// ErrNeedsLogin means the user should be redirected to the login page\n-\tErrNeedsLogin = errors.New(\"redirect to login page\")\n+        // ErrNeedsLogin means the user should be redirected to the login page\n+        ErrNeedsLogin = errors.New(\"redirect to login page\")\n \n-\t// ErrAccessDenied means the user should receive a 401 Unauthorized response\n-\tErrAccessDenied = errors.New(\"access denied\")\n+        // ErrAccessDenied means the user should receive a 401 Unauthorized response\n+        ErrAccessDenied = errors.New(\"access denied\")\n \n-\t// Used to check final redirects are not susceptible to open redirects.\n-\t// Matches //, /\\ and both of these with whitespace in between (eg / / or / \\).\n-\tinvalidRedirectRegex = regexp.MustCompile(`[/\\\\](?:[\\s\\v]*|\\.{1,2})[/\\\\]`)\n+        // Used to check final redirects are not susceptible to open redirects.\n+        // Matches //, /\\ and both of these with whitespace in between (eg / / or / \\).\n+        invalidRedirectRegex = regexp.MustCompile(`[/\\\\](?:[\\s\\v]*|\\.{1,2})[/\\\\]`)\n )\n \n // allowedRoute manages method + path based allowlists\n type allowedRoute struct {\n-\tmethod    string\n-\tpathRegex *regexp.Regexp\n+        method    string\n+        pathRegex *regexp.Regexp\n }\n \n // OAuthProxy is the main authentication proxy\n type OAuthProxy struct {\n-\tCookieSeed     string\n-\tCookieName     string\n-\tCSRFCookieName string\n-\tCookieDomains  []string\n-\tCookiePath     string\n-\tCookieSecure   bool\n-\tCookieHTTPOnly bool\n-\tCookieExpire   time.Duration\n-\tCookieRefresh  time.Duration\n-\tCookieSameSite string\n-\tValidator      func(string) bool\n-\n-\tRobotsPath        string\n-\tSignInPath        string\n-\tSignOutPath       string\n-\tOAuthStartPath    string\n-\tOAuthCallbackPath string\n-\tAuthOnlyPath      string\n-\tUserInfoPath      string\n-\n-\tallowedRoutes        []allowedRoute\n-\tredirectURL          *url.URL // the url to receive requests at\n-\twhitelistDomains     []string\n-\tprovider             providers.Provider\n-\tproviderNameOverride string\n-\tsessionStore         sessionsapi.SessionStore\n-\tProxyPrefix          string\n-\tSignInMessage        string\n-\tbasicAuthValidator   basic.Validator\n-\tdisplayHtpasswdForm  bool\n-\tserveMux             http.Handler\n-\tSetXAuthRequest      bool\n-\tPassBasicAuth        bool\n-\tSetBasicAuth         bool\n-\tSkipProviderButton   bool\n-\tPassUserHeaders      bool\n-\tBasicAuthPassword    string\n-\tPassAccessToken      bool\n-\tSetAuthorization     bool\n-\tPassAuthorization    bool\n-\tPreferEmailToUser    bool\n-\tskipAuthPreflight    bool\n-\tskipJwtBearerTokens  bool\n-\ttemplates            *template.Template\n-\trealClientIPParser   ipapi.RealClientIPParser\n-\ttrustedIPs           *ip.NetSet\n-\tBanner               string\n-\tFooter               string\n-\n-\tsessionChain alice.Chain\n-\theadersChain alice.Chain\n-\tpreAuthChain alice.Chain\n+        CookieSeed     string\n+        CookieName     string\n+        CSRFCookieName string\n+        CookieDomains  []string\n+        CookiePath     string\n+        CookieSecure   bool\n+        CookieHTTPOnly bool\n+        CookieExpire   time.Duration\n+        CookieRefresh  time.Duration\n+        CookieSameSite string\n+        Validator      func(string) bool\n+\n+        RobotsPath        string\n+        SignInPath        string\n+        SignOutPath       string\n+        OAuthStartPath    string\n+        OAuthCallbackPath string\n+        AuthOnlyPath      string\n+        UserInfoPath      string\n+\n+        allowedRoutes        []allowedRoute\n+        redirectURL          *url.URL // the url to receive requests at\n+        whitelistDomains     []string\n+        provider             providers.Provider\n+        providerNameOverride string\n+        sessionStore         sessionsapi.SessionStore\n+        ProxyPrefix          string\n+        SignInMessage        string\n+        basicAuthValidator   basic.Validator\n+        displayHtpasswdForm  bool\n+        serveMux             http.Handler\n+        SetXAuthRequest      bool\n+        PassBasicAuth        bool\n+        SetBasicAuth         bool\n+        SkipProviderButton   bool\n+        PassUserHeaders      bool\n+        BasicAuthPassword    string\n+        PassAccessToken      bool\n+        SetAuthorization     bool\n+        PassAuthorization    bool\n+        PreferEmailToUser    bool\n+        skipAuthPreflight    bool\n+        skipJwtBearerTokens  bool\n+        templates            *template.Template\n+        realClientIPParser   ipapi.RealClientIPParser\n+        trustedIPs           *ip.NetSet\n+        Banner               string\n+        Footer               string\n+\n+        sessionChain alice.Chain\n+        headersChain alice.Chain\n+        preAuthChain alice.Chain\n }\n \n // NewOAuthProxy creates a new instance of OAuthProxy from the options provided\n func NewOAuthProxy(opts *options.Options, validator func(string) bool) (*OAuthProxy, error) {\n-\tsessionStore, err := sessions.NewSessionStore(&opts.Session, &opts.Cookie)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error initialising session store: %v\", err)\n-\t}\n-\n-\ttemplates := loadTemplates(opts.CustomTemplatesDir)\n-\tproxyErrorHandler := upstream.NewProxyErrorHandler(templates.Lookup(\"error.html\"), opts.ProxyPrefix)\n-\tupstreamProxy, err := upstream.NewProxy(opts.UpstreamServers, opts.GetSignatureData(), proxyErrorHandler)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error initialising upstream proxy: %v\", err)\n-\t}\n-\n-\tif opts.SkipJwtBearerTokens {\n-\t\tlogger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n-\t\tfor _, issuer := range opts.ExtraJwtIssuers {\n-\t\t\tlogger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n-\t\t}\n-\t}\n-\tredirectURL := opts.GetRedirectURL()\n-\tif redirectURL.Path == \"\" {\n-\t\tredirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n-\t}\n-\n-\tlogger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.GetProvider().Data().ProviderName, opts.ClientID)\n-\trefresh := \"disabled\"\n-\tif opts.Cookie.Refresh != time.Duration(0) {\n-\t\trefresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n-\t}\n-\n-\tlogger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n-\n-\ttrustedIPs := ip.NewNetSet()\n-\tfor _, ipStr := range opts.TrustedIPs {\n-\t\tif ipNet := ip.ParseIPNet(ipStr); ipNet != nil {\n-\t\t\ttrustedIPs.AddIPNet(*ipNet)\n-\t\t} else {\n-\t\t\treturn nil, fmt.Errorf(\"could not parse IP network (%s)\", ipStr)\n-\t\t}\n-\t}\n-\n-\tvar basicAuthValidator basic.Validator\n-\tif opts.HtpasswdFile != \"\" {\n-\t\tlogger.Printf(\"using htpasswd file: %s\", opts.HtpasswdFile)\n-\t\tvar err error\n-\t\tbasicAuthValidator, err = basic.NewHTPasswdValidator(opts.HtpasswdFile)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"could not load htpasswdfile: %v\", err)\n-\t\t}\n-\t}\n-\n-\tallowedRoutes, err := buildRoutesAllowlist(opts)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tpreAuthChain, err := buildPreAuthChain(opts)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"could not build pre-auth chain: %v\", err)\n-\t}\n-\tsessionChain := buildSessionChain(opts, sessionStore, basicAuthValidator)\n-\theadersChain, err := buildHeadersChain(opts)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"could not build headers chain: %v\", err)\n-\t}\n-\n-\treturn &OAuthProxy{\n-\t\tCookieName:     opts.Cookie.Name,\n-\t\tCSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n-\t\tCookieSeed:     opts.Cookie.Secret,\n-\t\tCookieDomains:  opts.Cookie.Domains,\n-\t\tCookiePath:     opts.Cookie.Path,\n-\t\tCookieSecure:   opts.Cookie.Secure,\n-\t\tCookieHTTPOnly: opts.Cookie.HTTPOnly,\n-\t\tCookieExpire:   opts.Cookie.Expire,\n-\t\tCookieRefresh:  opts.Cookie.Refresh,\n-\t\tCookieSameSite: opts.Cookie.SameSite,\n-\t\tValidator:      validator,\n-\n-\t\tRobotsPath:        \"/robots.txt\",\n-\t\tSignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n-\t\tSignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n-\t\tOAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n-\t\tOAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n-\t\tAuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n-\t\tUserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n-\n-\t\tProxyPrefix:          opts.ProxyPrefix,\n-\t\tprovider:             opts.GetProvider(),\n-\t\tproviderNameOverride: opts.ProviderName,\n-\t\tsessionStore:         sessionStore,\n-\t\tserveMux:             upstreamProxy,\n-\t\tredirectURL:          redirectURL,\n-\t\tallowedRoutes:        allowedRoutes,\n-\t\twhitelistDomains:     opts.WhitelistDomains,\n-\t\tskipAuthPreflight:    opts.SkipAuthPreflight,\n-\t\tskipJwtBearerTokens:  opts.SkipJwtBearerTokens,\n-\t\trealClientIPParser:   opts.GetRealClientIPParser(),\n-\t\tSkipProviderButton:   opts.SkipProviderButton,\n-\t\ttemplates:            templates,\n-\t\ttrustedIPs:           trustedIPs,\n-\t\tBanner:               opts.Banner,\n-\t\tFooter:               opts.Footer,\n-\t\tSignInMessage:        buildSignInMessage(opts),\n-\n-\t\tbasicAuthValidator:  basicAuthValidator,\n-\t\tdisplayHtpasswdForm: basicAuthValidator != nil && opts.DisplayHtpasswdForm,\n-\t\tsessionChain:        sessionChain,\n-\t\theadersChain:        headersChain,\n-\t\tpreAuthChain:        preAuthChain,\n-\t}, nil\n+        sessionStore, err := sessions.NewSessionStore(&opts.Session, &opts.Cookie)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error initialising session store: %v\", err)\n+        }\n+\n+        templates := loadTemplates(opts.CustomTemplatesDir)\n+        proxyErrorHandler := upstream.NewProxyErrorHandler(templates.Lookup(\"error.html\"), opts.ProxyPrefix)\n+        upstreamProxy, err := upstream.NewProxy(opts.UpstreamServers, opts.GetSignatureData(), proxyErrorHandler)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error initialising upstream proxy: %v\", err)\n+        }\n+\n+        if opts.SkipJwtBearerTokens {\n+                logger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n+                for _, issuer := range opts.ExtraJwtIssuers {\n+                        logger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n+                }\n+        }\n+        redirectURL := opts.GetRedirectURL()\n+        if redirectURL.Path == \"\" {\n+                redirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n+        }\n+\n+        logger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.GetProvider().Data().ProviderName, opts.ClientID)\n+        refresh := \"disabled\"\n+        if opts.Cookie.Refresh != time.Duration(0) {\n+                refresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n+        }\n+\n+        logger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n+\n+        trustedIPs := ip.NewNetSet()\n+        for _, ipStr := range opts.TrustedIPs {\n+                if ipNet := ip.ParseIPNet(ipStr); ipNet != nil {\n+                        trustedIPs.AddIPNet(*ipNet)\n+                } else {\n+                        return nil, fmt.Errorf(\"could not parse IP network (%s)\", ipStr)\n+                }\n+        }\n+\n+        var basicAuthValidator basic.Validator\n+        if opts.HtpasswdFile != \"\" {\n+                logger.Printf(\"using htpasswd file: %s\", opts.HtpasswdFile)\n+                var err error\n+                basicAuthValidator, err = basic.NewHTPasswdValidator(opts.HtpasswdFile)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"could not load htpasswdfile: %v\", err)\n+                }\n+        }\n+\n+        allowedRoutes, err := buildRoutesAllowlist(opts)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        preAuthChain, err := buildPreAuthChain(opts)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"could not build pre-auth chain: %v\", err)\n+        }\n+        sessionChain := buildSessionChain(opts, sessionStore, basicAuthValidator)\n+        headersChain, err := buildHeadersChain(opts)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"could not build headers chain: %v\", err)\n+        }\n+\n+        return &OAuthProxy{\n+                CookieName:     opts.Cookie.Name,\n+                CSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n+                CookieSeed:     opts.Cookie.Secret,\n+                CookieDomains:  opts.Cookie.Domains,\n+                CookiePath:     opts.Cookie.Path,\n+                CookieSecure:   opts.Cookie.Secure,\n+                CookieHTTPOnly: opts.Cookie.HTTPOnly,\n+                CookieExpire:   opts.Cookie.Expire,\n+                CookieRefresh:  opts.Cookie.Refresh,\n+                CookieSameSite: opts.Cookie.SameSite,\n+                Validator:      validator,\n+\n+                RobotsPath:        \"/robots.txt\",\n+                SignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n+                SignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n+                OAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n+                OAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n+                AuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n+                UserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n+\n+                ProxyPrefix:          opts.ProxyPrefix,\n+                provider:             opts.GetProvider(),\n+                providerNameOverride: opts.ProviderName,\n+                sessionStore:         sessionStore,\n+                serveMux:             upstreamProxy,\n+                redirectURL:          redirectURL,\n+                allowedRoutes:        allowedRoutes,\n+                whitelistDomains:     opts.WhitelistDomains,\n+                skipAuthPreflight:    opts.SkipAuthPreflight,\n+                skipJwtBearerTokens:  opts.SkipJwtBearerTokens,\n+                realClientIPParser:   opts.GetRealClientIPParser(),\n+                SkipProviderButton:   opts.SkipProviderButton,\n+                templates:            templates,\n+                trustedIPs:           trustedIPs,\n+                Banner:               opts.Banner,\n+                Footer:               opts.Footer,\n+                SignInMessage:        buildSignInMessage(opts),\n+\n+                basicAuthValidator:  basicAuthValidator,\n+                displayHtpasswdForm: basicAuthValidator != nil && opts.DisplayHtpasswdForm,\n+                sessionChain:        sessionChain,\n+                headersChain:        headersChain,\n+                preAuthChain:        preAuthChain,\n+        }, nil\n }\n \n // buildPreAuthChain constructs a chain that should process every request before\n // the OAuth2 Proxy authentication logic kicks in.\n // For example forcing HTTPS or health checks.\n func buildPreAuthChain(opts *options.Options) (alice.Chain, error) {\n-\tchain := alice.New(middleware.NewScope(opts.ReverseProxy))\n-\n-\tif opts.ForceHTTPS {\n-\t\t_, httpsPort, err := net.SplitHostPort(opts.HTTPSAddress)\n-\t\tif err != nil {\n-\t\t\treturn alice.Chain{}, fmt.Errorf(\"invalid HTTPS address %q: %v\", opts.HTTPAddress, err)\n-\t\t}\n-\t\tchain = chain.Append(middleware.NewRedirectToHTTPS(httpsPort))\n-\t}\n-\n-\thealthCheckPaths := []string{opts.PingPath}\n-\thealthCheckUserAgents := []string{opts.PingUserAgent}\n-\tif opts.GCPHealthChecks {\n-\t\thealthCheckPaths = append(healthCheckPaths, \"/liveness_check\", \"/readiness_check\")\n-\t\thealthCheckUserAgents = append(healthCheckUserAgents, \"GoogleHC/1.0\")\n-\t}\n-\n-\t// To silence logging of health checks, register the health check handler before\n-\t// the logging handler\n-\tif opts.Logging.SilencePing {\n-\t\tchain = chain.Append(middleware.NewHealthCheck(healthCheckPaths, healthCheckUserAgents), LoggingHandler)\n-\t} else {\n-\t\tchain = chain.Append(LoggingHandler, middleware.NewHealthCheck(healthCheckPaths, healthCheckUserAgents))\n-\t}\n-\n-\treturn chain, nil\n+        chain := alice.New(middleware.NewScope(opts.ReverseProxy))\n+\n+        if opts.ForceHTTPS {\n+                _, httpsPort, err := net.SplitHostPort(opts.HTTPSAddress)\n+                if err != nil {\n+                        return alice.Chain{}, fmt.Errorf(\"invalid HTTPS address %q: %v\", opts.HTTPAddress, err)\n+                }\n+                chain = chain.Append(middleware.NewRedirectToHTTPS(httpsPort))\n+        }\n+\n+        healthCheckPaths := []string{opts.PingPath}\n+        healthCheckUserAgents := []string{opts.PingUserAgent}\n+        if opts.GCPHealthChecks {\n+                healthCheckPaths = append(healthCheckPaths, \"/liveness_check\", \"/readiness_check\")\n+                healthCheckUserAgents = append(healthCheckUserAgents, \"GoogleHC/1.0\")\n+        }\n+\n+        // To silence logging of health checks, register the health check handler before\n+        // the logging handler\n+        if opts.Logging.SilencePing {\n+                chain = chain.Append(middleware.NewHealthCheck(healthCheckPaths, healthCheckUserAgents), LoggingHandler)\n+        } else {\n+                chain = chain.Append(LoggingHandler, middleware.NewHealthCheck(healthCheckPaths, healthCheckUserAgents))\n+        }\n+\n+        return chain, nil\n }\n \n func buildSessionChain(opts *options.Options, sessionStore sessionsapi.SessionStore, validator basic.Validator) alice.Chain {\n-\tchain := alice.New()\n+        chain := alice.New()\n \n-\tif opts.SkipJwtBearerTokens {\n-\t\tsessionLoaders := []middlewareapi.TokenToSessionFunc{\n-\t\t\topts.GetProvider().CreateSessionFromToken,\n-\t\t}\n+        if opts.SkipJwtBearerTokens {\n+                sessionLoaders := []middlewareapi.TokenToSessionFunc{\n+                        opts.GetProvider().CreateSessionFromToken,\n+                }\n \n-\t\tfor _, verifier := range opts.GetJWTBearerVerifiers() {\n-\t\t\tsessionLoaders = append(sessionLoaders,\n-\t\t\t\tmiddlewareapi.CreateTokenToSessionFunc(verifier.Verify))\n-\t\t}\n+                for _, verifier := range opts.GetJWTBearerVerifiers() {\n+                        sessionLoaders = append(sessionLoaders,\n+                                middlewareapi.CreateTokenToSessionFunc(verifier.Verify))\n+                }\n \n-\t\tchain = chain.Append(middleware.NewJwtSessionLoader(sessionLoaders))\n-\t}\n+                chain = chain.Append(middleware.NewJwtSessionLoader(sessionLoaders))\n+        }\n \n-\tif validator != nil {\n-\t\tchain = chain.Append(middleware.NewBasicAuthSessionLoader(validator))\n-\t}\n+        if validator != nil {\n+                chain = chain.Append(middleware.NewBasicAuthSessionLoader(validator))\n+        }\n \n-\tchain = chain.Append(middleware.NewStoredSessionLoader(&middleware.StoredSessionLoaderOptions{\n-\t\tSessionStore:           sessionStore,\n-\t\tRefreshPeriod:          opts.Cookie.Refresh,\n-\t\tRefreshSessionIfNeeded: opts.GetProvider().RefreshSessionIfNeeded,\n-\t\tValidateSessionState:   opts.GetProvider().ValidateSession,\n-\t}))\n+        chain = chain.Append(middleware.NewStoredSessionLoader(&middleware.StoredSessionLoaderOptions{\n+                SessionStore:           sessionStore,\n+                RefreshPeriod:          opts.Cookie.Refresh,\n+                RefreshSessionIfNeeded: opts.GetProvider().RefreshSessionIfNeeded,\n+                ValidateSessionState:   opts.GetProvider().ValidateSession,\n+        }))\n \n-\treturn chain\n+        return chain\n }\n \n func buildHeadersChain(opts *options.Options) (alice.Chain, error) {\n-\trequestInjector, err := middleware.NewRequestHeaderInjector(opts.InjectRequestHeaders)\n-\tif err != nil {\n-\t\treturn alice.Chain{}, fmt.Errorf(\"error constructing request header injector: %v\", err)\n-\t}\n+        requestInjector, err := middleware.NewRequestHeaderInjector(opts.InjectRequestHeaders)\n+        if err != nil {\n+                return alice.Chain{}, fmt.Errorf(\"error constructing request header injector: %v\", err)\n+        }\n \n-\tresponseInjector, err := middleware.NewResponseHeaderInjector(opts.InjectResponseHeaders)\n-\tif err != nil {\n-\t\treturn alice.Chain{}, fmt.Errorf(\"error constructing request header injector: %v\", err)\n-\t}\n+        responseInjector, err := middleware.NewResponseHeaderInjector(opts.InjectResponseHeaders)\n+        if err != nil {\n+                return alice.Chain{}, fmt.Errorf(\"error constructing request header injector: %v\", err)\n+        }\n \n-\treturn alice.New(requestInjector, responseInjector), nil\n+        return alice.New(requestInjector, responseInjector), nil\n }\n \n func buildSignInMessage(opts *options.Options) string {\n-\tvar msg string\n-\tif len(opts.Banner) >= 1 {\n-\t\tif opts.Banner == \"-\" {\n-\t\t\tmsg = \"\"\n-\t\t} else {\n-\t\t\tmsg = opts.Banner\n-\t\t}\n-\t} else if len(opts.EmailDomains) != 0 && opts.AuthenticatedEmailsFile == \"\" {\n-\t\tif len(opts.EmailDomains) > 1 {\n-\t\t\tmsg = fmt.Sprintf(\"Authenticate using one of the following domains: %v\", strings.Join(opts.EmailDomains, \", \"))\n-\t\t} else if opts.EmailDomains[0] != \"*\" {\n-\t\t\tmsg = fmt.Sprintf(\"Authenticate using %v\", opts.EmailDomains[0])\n-\t\t}\n-\t}\n-\treturn msg\n+        var msg string\n+        if len(opts.Banner) >= 1 {\n+                if opts.Banner == \"-\" {\n+                        msg = \"\"\n+                } else {\n+                        msg = opts.Banner\n+                }\n+        } else if len(opts.EmailDomains) != 0 && opts.AuthenticatedEmailsFile == \"\" {\n+                if len(opts.EmailDomains) > 1 {\n+                        msg = fmt.Sprintf(\"Authenticate using one of the following domains: %v\", strings.Join(opts.EmailDomains, \", \"))\n+                } else if opts.EmailDomains[0] != \"*\" {\n+                        msg = fmt.Sprintf(\"Authenticate using %v\", opts.EmailDomains[0])\n+                }\n+        }\n+        return msg\n }\n \n // buildRoutesAllowlist builds an []allowedRoute  list from either the legacy\n // SkipAuthRegex option (paths only support) or newer SkipAuthRoutes option\n // (method=path support)\n func buildRoutesAllowlist(opts *options.Options) ([]allowedRoute, error) {\n-\troutes := make([]allowedRoute, 0, len(opts.SkipAuthRegex)+len(opts.SkipAuthRoutes))\n-\n-\tfor _, path := range opts.SkipAuthRegex {\n-\t\tcompiledRegex, err := regexp.Compile(path)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tlogger.Printf(\"Skipping auth - Method: ALL | Path: %s\", path)\n-\t\troutes = append(routes, allowedRoute{\n-\t\t\tmethod:    \"\",\n-\t\t\tpathRegex: compiledRegex,\n-\t\t})\n-\t}\n-\n-\tfor _, methodPath := range opts.SkipAuthRoutes {\n-\t\tvar (\n-\t\t\tmethod string\n-\t\t\tpath   string\n-\t\t)\n-\n-\t\tparts := strings.SplitN(methodPath, \"=\", 2)\n-\t\tif len(parts) == 1 {\n-\t\t\tmethod = \"\"\n-\t\t\tpath = parts[0]\n-\t\t} else {\n-\t\t\tmethod = strings.ToUpper(parts[0])\n-\t\t\tpath = parts[1]\n-\t\t}\n-\n-\t\tcompiledRegex, err := regexp.Compile(path)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tlogger.Printf(\"Skipping auth - Method: %s | Path: %s\", method, path)\n-\t\troutes = append(routes, allowedRoute{\n-\t\t\tmethod:    method,\n-\t\t\tpathRegex: compiledRegex,\n-\t\t})\n-\t}\n-\n-\treturn routes, nil\n+        routes := make([]allowedRoute, 0, len(opts.SkipAuthRegex)+len(opts.SkipAuthRoutes))\n+\n+        for _, path := range opts.SkipAuthRegex {\n+                compiledRegex, err := regexp.Compile(path)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                logger.Printf(\"Skipping auth - Method: ALL | Path: %s\", path)\n+                routes = append(routes, allowedRoute{\n+                        method:    \"\",\n+                        pathRegex: compiledRegex,\n+                })\n+        }\n+\n+        for _, methodPath := range opts.SkipAuthRoutes {\n+                var (\n+                        method string\n+                        path   string\n+                )\n+\n+                parts := strings.SplitN(methodPath, \"=\", 2)\n+                if len(parts) == 1 {\n+                        method = \"\"\n+                        path = parts[0]\n+                } else {\n+                        method = strings.ToUpper(parts[0])\n+                        path = parts[1]\n+                }\n+\n+                compiledRegex, err := regexp.Compile(path)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                logger.Printf(\"Skipping auth - Method: %s | Path: %s\", method, path)\n+                routes = append(routes, allowedRoute{\n+                        method:    method,\n+                        pathRegex: compiledRegex,\n+                })\n+        }\n+\n+        return routes, nil\n }\n \n // MakeCSRFCookie creates a cookie for CSRF\n func (p *OAuthProxy) MakeCSRFCookie(req *http.Request, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\treturn p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n+        return p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n }\n \n func (p *OAuthProxy) makeCookie(req *http.Request, name string, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\tcookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n-\n-\tif cookieDomain != \"\" {\n-\t\tdomain := requestutil.GetRequestHost(req)\n-\t\tif h, _, err := net.SplitHostPort(domain); err == nil {\n-\t\t\tdomain = h\n-\t\t}\n-\t\tif !strings.HasSuffix(domain, cookieDomain) {\n-\t\t\tlogger.Errorf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n-\t\t}\n-\t}\n-\n-\treturn &http.Cookie{\n-\t\tName:     name,\n-\t\tValue:    value,\n-\t\tPath:     p.CookiePath,\n-\t\tDomain:   cookieDomain,\n-\t\tHttpOnly: p.CookieHTTPOnly,\n-\t\tSecure:   p.CookieSecure,\n-\t\tExpires:  now.Add(expiration),\n-\t\tSameSite: cookies.ParseSameSite(p.CookieSameSite),\n-\t}\n+        cookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n+\n+        if cookieDomain != \"\" {\n+                domain := requestutil.GetRequestHost(req)\n+                if h, _, err := net.SplitHostPort(domain); err == nil {\n+                        domain = h\n+                }\n+                if !strings.HasSuffix(domain, cookieDomain) {\n+                        logger.Errorf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n+                }\n+        }\n+\n+        return &http.Cookie{\n+                Name:     name,\n+                Value:    value,\n+                Path:     p.CookiePath,\n+                Domain:   cookieDomain,\n+                HttpOnly: p.CookieHTTPOnly,\n+                Secure:   p.CookieSecure,\n+                Expires:  now.Add(expiration),\n+                SameSite: cookies.ParseSameSite(p.CookieSameSite),\n+        }\n }\n \n // ClearCSRFCookie creates a cookie to unset the CSRF cookie stored in the user's\n // session\n func (p *OAuthProxy) ClearCSRFCookie(rw http.ResponseWriter, req *http.Request) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n }\n \n // SetCSRFCookie adds a CSRF cookie to the response\n func (p *OAuthProxy) SetCSRFCookie(rw http.ResponseWriter, req *http.Request, val string) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n }\n \n // ClearSessionCookie creates a cookie to unset the user's authentication cookie\n // stored in the user's session\n func (p *OAuthProxy) ClearSessionCookie(rw http.ResponseWriter, req *http.Request) error {\n-\treturn p.sessionStore.Clear(rw, req)\n+        return p.sessionStore.Clear(rw, req)\n }\n \n // LoadCookiedSession reads the user's authentication details from the request\n func (p *OAuthProxy) LoadCookiedSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\treturn p.sessionStore.Load(req)\n+        return p.sessionStore.Load(req)\n }\n \n // SaveSession creates a new session cookie value and sets this on the response\n func (p *OAuthProxy) SaveSession(rw http.ResponseWriter, req *http.Request, s *sessionsapi.SessionState) error {\n-\treturn p.sessionStore.Save(rw, req, s)\n+        return p.sessionStore.Save(rw, req, s)\n }\n \n // IsValidRedirect checks whether the redirect URL is whitelisted\n func (p *OAuthProxy) IsValidRedirect(redirect string) bool {\n-\tswitch {\n-\tcase redirect == \"\":\n-\t\t// The user didn't specify a redirect, should fallback to `/`\n-\t\treturn false\n-\tcase strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !invalidRedirectRegex.MatchString(redirect):\n-\t\treturn true\n-\tcase strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n-\t\tredirectURL, err := url.Parse(redirect)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n-\t\t\treturn false\n-\t\t}\n-\t\tredirectHostname := redirectURL.Hostname()\n-\n-\t\tfor _, domain := range p.whitelistDomains {\n-\t\t\tdomainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n-\t\t\tif domainHostname == \"\" {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tif (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n-\t\t\t\t// the domain names match, now validate the ports\n-\t\t\t\t// if the whitelisted domain's port is '*', allow all ports\n-\t\t\t\t// if the whitelisted domain contains a specific port, only allow that port\n-\t\t\t\t// if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n-\t\t\t\tredirectPort := redirectURL.Port()\n-\t\t\t\tif (domainPort == \"*\") ||\n-\t\t\t\t\t(domainPort == redirectPort) ||\n-\t\t\t\t\t(domainPort == \"\" && redirectPort == \"\") {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n-\t\treturn false\n-\tdefault:\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n-\t\treturn false\n-\t}\n+        switch {\n+        case redirect == \"\":\n+                // The user didn't specify a redirect, should fallback to `/`\n+                return false\n+        case strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !invalidRedirectRegex.MatchString(redirect):\n+                return true\n+        case strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n+                redirectURL, err := url.Parse(redirect)\n+                if err != nil {\n+                        logger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n+                        return false\n+                }\n+                redirectHostname := redirectURL.Hostname()\n+\n+                for _, domain := range p.whitelistDomains {\n+                        domainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n+                        if domainHostname == \"\" {\n+                                continue\n+                        }\n+\n+                        if redirectHostname == domainHostname || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, \".\"+domainHostname)) {\n+                                // the domain names match, now validate the ports\n+                                // if the whitelisted domain's port is '*', allow all ports\n+                                // if the whitelisted domain contains a specific port, only allow that port\n+                                // if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n+                                redirectPort := redirectURL.Port()\n+                                if (domainPort == \"*\") ||\n+                                        (domainPort == redirectPort) ||\n+                                        (domainPort == \"\" && redirectPort == \"\") {\n+                                        return true\n+                                }\n+                        }\n+                }\n+\n+                logger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n+                return false\n+        default:\n+                logger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n+                return false\n+        }\n }\n \n func (p *OAuthProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n-\tp.preAuthChain.Then(http.HandlerFunc(p.serveHTTP)).ServeHTTP(rw, req)\n+        p.preAuthChain.Then(http.HandlerFunc(p.serveHTTP)).ServeHTTP(rw, req)\n }\n \n func (p *OAuthProxy) serveHTTP(rw http.ResponseWriter, req *http.Request) {\n-\tif req.URL.Path != p.AuthOnlyPath && strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n-\t\tprepareNoCache(rw)\n-\t}\n-\n-\tswitch path := req.URL.Path; {\n-\tcase path == p.RobotsPath:\n-\t\tp.RobotsTxt(rw)\n-\tcase p.IsAllowedRequest(req):\n-\t\tp.SkipAuthProxy(rw, req)\n-\tcase path == p.SignInPath:\n-\t\tp.SignIn(rw, req)\n-\tcase path == p.SignOutPath:\n-\t\tp.SignOut(rw, req)\n-\tcase path == p.OAuthStartPath:\n-\t\tp.OAuthStart(rw, req)\n-\tcase path == p.OAuthCallbackPath:\n-\t\tp.OAuthCallback(rw, req)\n-\tcase path == p.AuthOnlyPath:\n-\t\tp.AuthOnly(rw, req)\n-\tcase path == p.UserInfoPath:\n-\t\tp.UserInfo(rw, req)\n-\tdefault:\n-\t\tp.Proxy(rw, req)\n-\t}\n+        if req.URL.Path != p.AuthOnlyPath && strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n+                prepareNoCache(rw)\n+        }\n+\n+        switch path := req.URL.Path; {\n+        case path == p.RobotsPath:\n+                p.RobotsTxt(rw)\n+        case p.IsAllowedRequest(req):\n+                p.SkipAuthProxy(rw, req)\n+        case path == p.SignInPath:\n+                p.SignIn(rw, req)\n+        case path == p.SignOutPath:\n+                p.SignOut(rw, req)\n+        case path == p.OAuthStartPath:\n+                p.OAuthStart(rw, req)\n+        case path == p.OAuthCallbackPath:\n+                p.OAuthCallback(rw, req)\n+        case path == p.AuthOnlyPath:\n+                p.AuthOnly(rw, req)\n+        case path == p.UserInfoPath:\n+                p.UserInfo(rw, req)\n+        default:\n+                p.Proxy(rw, req)\n+        }\n }\n \n // RobotsTxt disallows scraping pages from the OAuthProxy\n func (p *OAuthProxy) RobotsTxt(rw http.ResponseWriter) {\n-\t_, err := fmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error writing robots.txt: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\trw.WriteHeader(http.StatusOK)\n+        _, err := fmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n+        if err != nil {\n+                logger.Printf(\"Error writing robots.txt: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        rw.WriteHeader(http.StatusOK)\n }\n \n // ErrorPage writes an error response\n func (p *OAuthProxy) ErrorPage(rw http.ResponseWriter, code int, title string, message string) {\n-\trw.WriteHeader(code)\n-\tt := struct {\n-\t\tTitle       string\n-\t\tMessage     string\n-\t\tProxyPrefix string\n-\t}{\n-\t\tTitle:       fmt.Sprintf(\"%d %s\", code, title),\n-\t\tMessage:     message,\n-\t\tProxyPrefix: p.ProxyPrefix,\n-\t}\n-\terr := p.templates.ExecuteTemplate(rw, \"error.html\", t)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error rendering error.html template: %v\", err)\n-\t\thttp.Error(rw, \"Internal Server Error\", http.StatusInternalServerError)\n-\t}\n+        rw.WriteHeader(code)\n+        t := struct {\n+                Title       string\n+                Message     string\n+                ProxyPrefix string\n+        }{\n+                Title:       fmt.Sprintf(\"%d %s\", code, title),\n+                Message:     message,\n+                ProxyPrefix: p.ProxyPrefix,\n+        }\n+        err := p.templates.ExecuteTemplate(rw, \"error.html\", t)\n+        if err != nil {\n+                logger.Printf(\"Error rendering error.html template: %v\", err)\n+                http.Error(rw, \"Internal Server Error\", http.StatusInternalServerError)\n+        }\n }\n \n // IsAllowedRequest is used to check if auth should be skipped for this request\n func (p *OAuthProxy) IsAllowedRequest(req *http.Request) bool {\n-\tisPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n-\treturn isPreflightRequestAllowed || p.isAllowedRoute(req) || p.isTrustedIP(req)\n+        isPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n+        return isPreflightRequestAllowed || p.isAllowedRoute(req) || p.isTrustedIP(req)\n }\n \n // IsAllowedRoute is used to check if the request method & path is allowed without auth\n func (p *OAuthProxy) isAllowedRoute(req *http.Request) bool {\n-\tfor _, route := range p.allowedRoutes {\n-\t\tif (route.method == \"\" || req.Method == route.method) && route.pathRegex.MatchString(req.URL.Path) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, route := range p.allowedRoutes {\n+                if (route.method == \"\" || req.Method == route.method) && route.pathRegex.MatchString(req.URL.Path) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // isTrustedIP is used to check if a request comes from a trusted client IP address.\n func (p *OAuthProxy) isTrustedIP(req *http.Request) bool {\n-\tif p.trustedIPs == nil {\n-\t\treturn false\n-\t}\n-\n-\tremoteAddr, err := ip.GetClientIP(p.realClientIPParser, req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining real IP for trusted IP list: %v\", err)\n-\t\t// Possibly spoofed X-Real-IP header\n-\t\treturn false\n-\t}\n-\n-\tif remoteAddr == nil {\n-\t\treturn false\n-\t}\n-\n-\treturn p.trustedIPs.Has(remoteAddr)\n+        if p.trustedIPs == nil {\n+                return false\n+        }\n+\n+        remoteAddr, err := ip.GetClientIP(p.realClientIPParser, req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining real IP for trusted IP list: %v\", err)\n+                // Possibly spoofed X-Real-IP header\n+                return false\n+        }\n+\n+        if remoteAddr == nil {\n+                return false\n+        }\n+\n+        return p.trustedIPs.Has(remoteAddr)\n }\n \n // SignInPage writes the sing in template to the response\n func (p *OAuthProxy) SignInPage(rw http.ResponseWriter, req *http.Request, code int) {\n-\tprepareNoCache(rw)\n-\terr := p.ClearSessionCookie(rw, req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error clearing session cookie: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\trw.WriteHeader(code)\n-\n-\tredirectURL, err := p.getAppRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining redirect: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tif redirectURL == p.SignInPath {\n-\t\tredirectURL = \"/\"\n-\t}\n-\n-\t// We allow unescaped template.HTML since it is user configured options\n-\t/* #nosec G203 */\n-\tt := struct {\n-\t\tProviderName  string\n-\t\tSignInMessage template.HTML\n-\t\tCustomLogin   bool\n-\t\tRedirect      string\n-\t\tVersion       string\n-\t\tProxyPrefix   string\n-\t\tFooter        template.HTML\n-\t}{\n-\t\tProviderName:  p.provider.Data().ProviderName,\n-\t\tSignInMessage: template.HTML(p.SignInMessage),\n-\t\tCustomLogin:   p.displayHtpasswdForm,\n-\t\tRedirect:      redirectURL,\n-\t\tVersion:       VERSION,\n-\t\tProxyPrefix:   p.ProxyPrefix,\n-\t\tFooter:        template.HTML(p.Footer),\n-\t}\n-\tif p.providerNameOverride != \"\" {\n-\t\tt.ProviderName = p.providerNameOverride\n-\t}\n-\terr = p.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error rendering sign_in.html template: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t}\n+        prepareNoCache(rw)\n+        err := p.ClearSessionCookie(rw, req)\n+        if err != nil {\n+                logger.Printf(\"Error clearing session cookie: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        rw.WriteHeader(code)\n+\n+        redirectURL, err := p.getAppRedirect(req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining redirect: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+\n+        if redirectURL == p.SignInPath {\n+                redirectURL = \"/\"\n+        }\n+\n+        // We allow unescaped template.HTML since it is user configured options\n+        /* #nosec G203 */\n+        t := struct {\n+                ProviderName  string\n+                SignInMessage template.HTML\n+                CustomLogin   bool\n+                Redirect      string\n+                Version       string\n+                ProxyPrefix   string\n+                Footer        template.HTML\n+        }{\n+                ProviderName:  p.provider.Data().ProviderName,\n+                SignInMessage: template.HTML(p.SignInMessage),\n+                CustomLogin:   p.displayHtpasswdForm,\n+                Redirect:      redirectURL,\n+                Version:       VERSION,\n+                ProxyPrefix:   p.ProxyPrefix,\n+                Footer:        template.HTML(p.Footer),\n+        }\n+        if p.providerNameOverride != \"\" {\n+                t.ProviderName = p.providerNameOverride\n+        }\n+        err = p.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n+        if err != nil {\n+                logger.Printf(\"Error rendering sign_in.html template: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+        }\n }\n \n // ManualSignIn handles basic auth logins to the proxy\n func (p *OAuthProxy) ManualSignIn(req *http.Request) (string, bool) {\n-\tif req.Method != \"POST\" || p.basicAuthValidator == nil {\n-\t\treturn \"\", false\n-\t}\n-\tuser := req.FormValue(\"username\")\n-\tpasswd := req.FormValue(\"password\")\n-\tif user == \"\" {\n-\t\treturn \"\", false\n-\t}\n-\t// check auth\n-\tif p.basicAuthValidator.Validate(user, passwd) {\n-\t\tlogger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n-\t\treturn user, true\n-\t}\n-\tlogger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n-\treturn \"\", false\n+        if req.Method != \"POST\" || p.basicAuthValidator == nil {\n+                return \"\", false\n+        }\n+        user := req.FormValue(\"username\")\n+        passwd := req.FormValue(\"password\")\n+        if user == \"\" {\n+                return \"\", false\n+        }\n+        // check auth\n+        if p.basicAuthValidator.Validate(user, passwd) {\n+                logger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n+                return user, true\n+        }\n+        logger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n+        return \"\", false\n }\n \n // SignIn serves a page prompting users to sign in\n func (p *OAuthProxy) SignIn(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.getAppRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining redirect: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tuser, ok := p.ManualSignIn(req)\n-\tif ok {\n-\t\tsession := &sessionsapi.SessionState{User: user}\n-\t\terr = p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error saving session: %v\", err)\n-\t\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusOK)\n-\t\t}\n-\t}\n+        redirect, err := p.getAppRedirect(req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining redirect: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+\n+        user, ok := p.ManualSignIn(req)\n+        if ok {\n+                session := &sessionsapi.SessionState{User: user}\n+                err = p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.Printf(\"Error saving session: %v\", err)\n+                        p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                        return\n+                }\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusOK)\n+                }\n+        }\n }\n \n //UserInfo endpoint outputs session email and preferred username in JSON format\n func (p *OAuthProxy) UserInfo(rw http.ResponseWriter, req *http.Request) {\n \n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\tuserInfo := struct {\n-\t\tUser              string   `json:\"user\"`\n-\t\tEmail             string   `json:\"email\"`\n-\t\tGroups            []string `json:\"groups,omitempty\"`\n-\t\tPreferredUsername string   `json:\"preferredUsername,omitempty\"`\n-\t}{\n-\t\tUser:              session.User,\n-\t\tEmail:             session.Email,\n-\t\tGroups:            session.Groups,\n-\t\tPreferredUsername: session.PreferredUsername,\n-\t}\n-\n-\trw.Header().Set(\"Content-Type\", \"application/json\")\n-\trw.WriteHeader(http.StatusOK)\n-\terr = json.NewEncoder(rw).Encode(userInfo)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error encoding user info: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t}\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        userInfo := struct {\n+                User              string   `json:\"user\"`\n+                Email             string   `json:\"email\"`\n+                Groups            []string `json:\"groups,omitempty\"`\n+                PreferredUsername string   `json:\"preferredUsername,omitempty\"`\n+        }{\n+                User:              session.User,\n+                Email:             session.Email,\n+                Groups:            session.Groups,\n+                PreferredUsername: session.PreferredUsername,\n+        }\n+\n+        rw.Header().Set(\"Content-Type\", \"application/json\")\n+        rw.WriteHeader(http.StatusOK)\n+        err = json.NewEncoder(rw).Encode(userInfo)\n+        if err != nil {\n+                logger.Printf(\"Error encoding user info: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+        }\n }\n \n // SignOut sends a response to clear the authentication cookie\n func (p *OAuthProxy) SignOut(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.getAppRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining redirect: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\terr = p.ClearSessionCookie(rw, req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error clearing session cookie: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\thttp.Redirect(rw, req, redirect, http.StatusFound)\n+        redirect, err := p.getAppRedirect(req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining redirect: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        err = p.ClearSessionCookie(rw, req)\n+        if err != nil {\n+                logger.Errorf(\"Error clearing session cookie: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        http.Redirect(rw, req, redirect, http.StatusFound)\n }\n \n // OAuthStart starts the OAuth2 authentication flow\n func (p *OAuthProxy) OAuthStart(rw http.ResponseWriter, req *http.Request) {\n-\tprepareNoCache(rw)\n-\tnonce, err := encryption.Nonce()\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining nonce: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.SetCSRFCookie(rw, req, nonce)\n-\tredirect, err := p.getAppRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining redirect: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\tredirectURI := p.getOAuthRedirectURI(req)\n-\thttp.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n+        prepareNoCache(rw)\n+        nonce, err := encryption.Nonce()\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining nonce: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        p.SetCSRFCookie(rw, req, nonce)\n+        redirect, err := p.getAppRedirect(req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining redirect: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        redirectURI := p.getOAuthRedirectURI(req)\n+        http.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n }\n \n // OAuthCallback is the OAuth2 authentication flow callback that finishes the\n // OAuth2 authentication flow\n func (p *OAuthProxy) OAuthCallback(rw http.ResponseWriter, req *http.Request) {\n-\tremoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n-\n-\t// finish the oauth cycle\n-\terr := req.ParseForm()\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error while parsing OAuth2 callback: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\terrorString := req.Form.Get(\"error\")\n-\tif errorString != \"\" {\n-\t\tlogger.Errorf(\"Error while parsing OAuth2 callback: %s\", errorString)\n-\t\tp.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", errorString)\n-\t\treturn\n-\t}\n-\n-\tsession, err := p.redeemCode(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error redeeming code during OAuth2 callback: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Internal Error\")\n-\t\treturn\n-\t}\n-\n-\terr = p.enrichSessionState(req.Context(), session)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error creating session during OAuth2 callback: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Internal Error\")\n-\t\treturn\n-\t}\n-\n-\tstate := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n-\tif len(state) != 2 {\n-\t\tlogger.Error(\"Error while parsing OAuth2 state: invalid length\")\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Invalid State\")\n-\t\treturn\n-\t}\n-\tnonce := state[0]\n-\tredirect := state[1]\n-\tc, err := req.Cookie(p.CSRFCookieName)\n-\tif err != nil {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable to obtain CSRF cookie\")\n-\t\tp.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearCSRFCookie(rw, req)\n-\tif c.Value != nonce {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: CSRF token mismatch, potential attack\")\n-\t\tp.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", \"CSRF Failed\")\n-\t\treturn\n-\t}\n-\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = \"/\"\n-\t}\n-\n-\t// set cookie, or deny\n-\tauthorized, err := p.provider.Authorize(req.Context(), session)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error with authorization: %v\", err)\n-\t}\n-\tif p.Validator(session.Email) && authorized {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n-\t\terr := p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.Errorf(\"Error saving session state for %s: %v\", remoteAddr, err)\n-\t\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n-\t\tp.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", \"Invalid Account\")\n-\t}\n+        remoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n+\n+        // finish the oauth cycle\n+        err := req.ParseForm()\n+        if err != nil {\n+                logger.Errorf(\"Error while parsing OAuth2 callback: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        errorString := req.Form.Get(\"error\")\n+        if errorString != \"\" {\n+                logger.Errorf(\"Error while parsing OAuth2 callback: %s\", errorString)\n+                p.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", errorString)\n+                return\n+        }\n+\n+        session, err := p.redeemCode(req)\n+        if err != nil {\n+                logger.Errorf(\"Error redeeming code during OAuth2 callback: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Internal Error\")\n+                return\n+        }\n+\n+        err = p.enrichSessionState(req.Context(), session)\n+        if err != nil {\n+                logger.Errorf(\"Error creating session during OAuth2 callback: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Internal Error\")\n+                return\n+        }\n+\n+        state := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n+        if len(state) != 2 {\n+                logger.Error(\"Error while parsing OAuth2 state: invalid length\")\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Invalid State\")\n+                return\n+        }\n+        nonce := state[0]\n+        redirect := state[1]\n+        c, err := req.Cookie(p.CSRFCookieName)\n+        if err != nil {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable to obtain CSRF cookie\")\n+                p.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", err.Error())\n+                return\n+        }\n+        p.ClearCSRFCookie(rw, req)\n+        if c.Value != nonce {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: CSRF token mismatch, potential attack\")\n+                p.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", \"CSRF Failed\")\n+                return\n+        }\n+\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = \"/\"\n+        }\n+\n+        // set cookie, or deny\n+        authorized, err := p.provider.Authorize(req.Context(), session)\n+        if err != nil {\n+                logger.Errorf(\"Error with authorization: %v\", err)\n+        }\n+        if p.Validator(session.Email) && authorized {\n+                logger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n+                err := p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.Errorf(\"Error saving session state for %s: %v\", remoteAddr, err)\n+                        p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                        return\n+                }\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n+                p.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", \"Invalid Account\")\n+        }\n }\n \n func (p *OAuthProxy) redeemCode(req *http.Request) (*sessionsapi.SessionState, error) {\n-\tcode := req.Form.Get(\"code\")\n-\tif code == \"\" {\n-\t\treturn nil, providers.ErrMissingCode\n-\t}\n-\n-\tredirectURI := p.getOAuthRedirectURI(req)\n-\ts, err := p.provider.Redeem(req.Context(), redirectURI, code)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn s, nil\n+        code := req.Form.Get(\"code\")\n+        if code == \"\" {\n+                return nil, providers.ErrMissingCode\n+        }\n+\n+        redirectURI := p.getOAuthRedirectURI(req)\n+        s, err := p.provider.Redeem(req.Context(), redirectURI, code)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return s, nil\n }\n \n func (p *OAuthProxy) enrichSessionState(ctx context.Context, s *sessionsapi.SessionState) error {\n-\tvar err error\n-\tif s.Email == \"\" {\n-\t\ts.Email, err = p.provider.GetEmailAddress(ctx, s)\n-\t\tif err != nil && !errors.Is(err, providers.ErrNotImplemented) {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\treturn p.provider.EnrichSession(ctx, s)\n+        var err error\n+        if s.Email == \"\" {\n+                s.Email, err = p.provider.GetEmailAddress(ctx, s)\n+                if err != nil && !errors.Is(err, providers.ErrNotImplemented) {\n+                        return err\n+                }\n+        }\n+\n+        return p.provider.EnrichSession(ctx, s)\n }\n \n // AuthOnly checks whether the user is currently logged in (both authentication\n // and optional authorization).\n func (p *OAuthProxy) AuthOnly(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\t// Unauthorized cases need to return 403 to prevent infinite redirects with\n-\t// subrequest architectures\n-\tif !authOnlyAuthorize(req, session) {\n-\t\thttp.Error(rw, http.StatusText(http.StatusForbidden), http.StatusForbidden)\n-\t\treturn\n-\t}\n-\n-\t// we are authenticated\n-\tp.addHeadersForProxying(rw, session)\n-\tp.headersChain.Then(http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) {\n-\t\trw.WriteHeader(http.StatusAccepted)\n-\t})).ServeHTTP(rw, req)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        // Unauthorized cases need to return 403 to prevent infinite redirects with\n+        // subrequest architectures\n+        if !authOnlyAuthorize(req, session) {\n+                http.Error(rw, http.StatusText(http.StatusForbidden), http.StatusForbidden)\n+                return\n+        }\n+\n+        // we are authenticated\n+        p.addHeadersForProxying(rw, session)\n+        p.headersChain.Then(http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) {\n+                rw.WriteHeader(http.StatusAccepted)\n+        })).ServeHTTP(rw, req)\n }\n \n // SkipAuthProxy proxies allowlisted requests and skips authentication\n func (p *OAuthProxy) SkipAuthProxy(rw http.ResponseWriter, req *http.Request) {\n-\tp.headersChain.Then(p.serveMux).ServeHTTP(rw, req)\n+        p.headersChain.Then(p.serveMux).ServeHTTP(rw, req)\n }\n \n // Proxy proxies the user request if the user is authenticated else it prompts\n // them to authenticate\n func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tswitch err {\n-\tcase nil:\n-\t\t// we are authenticated\n-\t\tp.addHeadersForProxying(rw, session)\n-\t\tp.headersChain.Then(p.serveMux).ServeHTTP(rw, req)\n-\tcase ErrNeedsLogin:\n-\t\t// we need to send the user to a login screen\n-\t\tif isAjax(req) {\n-\t\t\t// no point redirecting an AJAX request\n-\t\t\tp.errorJSON(rw, http.StatusUnauthorized)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusForbidden)\n-\t\t}\n-\n-\tcase ErrAccessDenied:\n-\t\tp.ErrorPage(rw, http.StatusUnauthorized, \"Permission Denied\", \"Unauthorized\")\n-\n-\tdefault:\n-\t\t// unknown error\n-\t\tlogger.Errorf(\"Unexpected internal error: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError,\n-\t\t\t\"Internal Error\", \"Internal Error\")\n-\t}\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        switch err {\n+        case nil:\n+                // we are authenticated\n+                p.addHeadersForProxying(rw, session)\n+                p.headersChain.Then(p.serveMux).ServeHTTP(rw, req)\n+        case ErrNeedsLogin:\n+                // we need to send the user to a login screen\n+                if isAjax(req) {\n+                        // no point redirecting an AJAX request\n+                        p.errorJSON(rw, http.StatusUnauthorized)\n+                        return\n+                }\n+\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusForbidden)\n+                }\n+\n+        case ErrAccessDenied:\n+                p.ErrorPage(rw, http.StatusUnauthorized, \"Permission Denied\", \"Unauthorized\")\n+\n+        default:\n+                // unknown error\n+                logger.Errorf(\"Unexpected internal error: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError,\n+                        \"Internal Error\", \"Internal Error\")\n+        }\n }\n \n // See https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching?hl=en\n var noCacheHeaders = map[string]string{\n-\t\"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n-\t\"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n-\t\"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n+        \"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n+        \"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n+        \"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n }\n \n // prepareNoCache prepares headers for preventing browser caching.\n func prepareNoCache(w http.ResponseWriter) {\n-\t// Set NoCache headers\n-\tfor k, v := range noCacheHeaders {\n-\t\tw.Header().Set(k, v)\n-\t}\n+        // Set NoCache headers\n+        for k, v := range noCacheHeaders {\n+                w.Header().Set(k, v)\n+        }\n }\n \n // getOAuthRedirectURI returns the redirectURL that the upstream OAuth Provider will\n // redirect clients to once authenticated.\n // This is usually the OAuthProxy callback URL.\n func (p *OAuthProxy) getOAuthRedirectURI(req *http.Request) string {\n-\t// if `p.redirectURL` already has a host, return it\n-\tif p.redirectURL.Host != \"\" {\n-\t\treturn p.redirectURL.String()\n-\t}\n-\n-\t// Otherwise figure out the scheme + host from the request\n-\trd := *p.redirectURL\n-\trd.Host = requestutil.GetRequestHost(req)\n-\trd.Scheme = requestutil.GetRequestProto(req)\n-\n-\t// If CookieSecure is true, return `https` no matter what\n-\t// Not all reverse proxies set X-Forwarded-Proto\n-\tif p.CookieSecure {\n-\t\trd.Scheme = schemeHTTPS\n-\t}\n-\treturn rd.String()\n+        // if `p.redirectURL` already has a host, return it\n+        if p.redirectURL.Host != \"\" {\n+                return p.redirectURL.String()\n+        }\n+\n+        // Otherwise figure out the scheme + host from the request\n+        rd := *p.redirectURL\n+        rd.Host = requestutil.GetRequestHost(req)\n+        rd.Scheme = requestutil.GetRequestProto(req)\n+\n+        // If CookieSecure is true, return `https` no matter what\n+        // Not all reverse proxies set X-Forwarded-Proto\n+        if p.CookieSecure {\n+                rd.Scheme = schemeHTTPS\n+        }\n+        return rd.String()\n }\n \n // getAppRedirect determines the full URL or URI path to redirect clients to\n@@ -941,89 +941,89 @@ func (p *OAuthProxy) getOAuthRedirectURI(req *http.Request) string {\n // - `req.URL.RequestURI` if not under the ProxyPath (i.e. /oauth2/*)\n // - `/`\n func (p *OAuthProxy) getAppRedirect(req *http.Request) (string, error) {\n-\terr := req.ParseForm()\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\t// These redirect getter functions are strategies ordered by priority\n-\t// for figuring out the redirect URL.\n-\ttype redirectGetter func(req *http.Request) string\n-\tfor _, rdGetter := range []redirectGetter{\n-\t\tp.getRdQuerystringRedirect,\n-\t\tp.getXAuthRequestRedirect,\n-\t\tp.getXForwardedHeadersRedirect,\n-\t\tp.getURIRedirect,\n-\t} {\n-\t\tredirect := rdGetter(req)\n-\t\t// Call `p.IsValidRedirect` again here a final time to be safe\n-\t\tif redirect != \"\" && p.IsValidRedirect(redirect) {\n-\t\t\treturn redirect, nil\n-\t\t}\n-\t}\n-\n-\treturn \"/\", nil\n+        err := req.ParseForm()\n+        if err != nil {\n+                return \"\", err\n+        }\n+\n+        // These redirect getter functions are strategies ordered by priority\n+        // for figuring out the redirect URL.\n+        type redirectGetter func(req *http.Request) string\n+        for _, rdGetter := range []redirectGetter{\n+                p.getRdQuerystringRedirect,\n+                p.getXAuthRequestRedirect,\n+                p.getXForwardedHeadersRedirect,\n+                p.getURIRedirect,\n+        } {\n+                redirect := rdGetter(req)\n+                // Call `p.IsValidRedirect` again here a final time to be safe\n+                if redirect != \"\" && p.IsValidRedirect(redirect) {\n+                        return redirect, nil\n+                }\n+        }\n+\n+        return \"/\", nil\n }\n \n func isForwardedRequest(req *http.Request) bool {\n-\treturn requestutil.IsProxied(req) &&\n-\t\treq.Host != requestutil.GetRequestHost(req)\n+        return requestutil.IsProxied(req) &&\n+                req.Host != requestutil.GetRequestHost(req)\n }\n \n func (p *OAuthProxy) hasProxyPrefix(path string) bool {\n-\treturn strings.HasPrefix(path, fmt.Sprintf(\"%s/\", p.ProxyPrefix))\n+        return strings.HasPrefix(path, fmt.Sprintf(\"%s/\", p.ProxyPrefix))\n }\n \n func (p *OAuthProxy) validateRedirect(redirect string, errorFormat string) string {\n-\tif p.IsValidRedirect(redirect) {\n-\t\treturn redirect\n-\t}\n-\tif redirect != \"\" {\n-\t\tlogger.Errorf(errorFormat, redirect)\n-\t}\n-\treturn \"\"\n+        if p.IsValidRedirect(redirect) {\n+                return redirect\n+        }\n+        if redirect != \"\" {\n+                logger.Errorf(errorFormat, redirect)\n+        }\n+        return \"\"\n }\n \n // getRdQuerystringRedirect handles this getAppRedirect strategy:\n // - `rd` querysting parameter\n func (p *OAuthProxy) getRdQuerystringRedirect(req *http.Request) string {\n-\treturn p.validateRedirect(\n-\t\treq.Form.Get(\"rd\"),\n-\t\t\"Invalid redirect provided in rd querystring parameter: %s\",\n-\t)\n+        return p.validateRedirect(\n+                req.Form.Get(\"rd\"),\n+                \"Invalid redirect provided in rd querystring parameter: %s\",\n+        )\n }\n \n // getXAuthRequestRedirect handles this getAppRedirect strategy:\n // - `X-Auth-Request-Redirect` Header\n func (p *OAuthProxy) getXAuthRequestRedirect(req *http.Request) string {\n-\treturn p.validateRedirect(\n-\t\treq.Header.Get(\"X-Auth-Request-Redirect\"),\n-\t\t\"Invalid redirect provided in X-Auth-Request-Redirect header: %s\",\n-\t)\n+        return p.validateRedirect(\n+                req.Header.Get(\"X-Auth-Request-Redirect\"),\n+                \"Invalid redirect provided in X-Auth-Request-Redirect header: %s\",\n+        )\n }\n \n // getXForwardedHeadersRedirect handles these getAppRedirect strategies:\n // - `X-Forwarded-(Proto|Host|Uri)` headers (when ReverseProxy mode is enabled)\n // - `X-Forwarded-(Proto|Host)` if `Uri` has the ProxyPath (i.e. /oauth2/*)\n func (p *OAuthProxy) getXForwardedHeadersRedirect(req *http.Request) string {\n-\tif !isForwardedRequest(req) {\n-\t\treturn \"\"\n-\t}\n-\n-\turi := requestutil.GetRequestURI(req)\n-\tif p.hasProxyPrefix(uri) {\n-\t\turi = \"/\"\n-\t}\n-\n-\tredirect := fmt.Sprintf(\n-\t\t\"%s://%s%s\",\n-\t\trequestutil.GetRequestProto(req),\n-\t\trequestutil.GetRequestHost(req),\n-\t\turi,\n-\t)\n-\n-\treturn p.validateRedirect(redirect,\n-\t\t\"Invalid redirect generated from X-Forwarded-* headers: %s\")\n+        if !isForwardedRequest(req) {\n+                return \"\"\n+        }\n+\n+        uri := requestutil.GetRequestURI(req)\n+        if p.hasProxyPrefix(uri) {\n+                uri = \"/\"\n+        }\n+\n+        redirect := fmt.Sprintf(\n+                \"%s://%s%s\",\n+                requestutil.GetRequestProto(req),\n+                requestutil.GetRequestHost(req),\n+                uri,\n+        )\n+\n+        return p.validateRedirect(redirect,\n+                \"Invalid redirect generated from X-Forwarded-* headers: %s\")\n }\n \n // getURIRedirect handles these getAppRedirect strategies:\n@@ -1031,18 +1031,18 @@ func (p *OAuthProxy) getXForwardedHeadersRedirect(req *http.Request) string {\n // - `req.URL.RequestURI` if not under the ProxyPath (i.e. /oauth2/*)\n // - `/`\n func (p *OAuthProxy) getURIRedirect(req *http.Request) string {\n-\tredirect := p.validateRedirect(\n-\t\trequestutil.GetRequestURI(req),\n-\t\t\"Invalid redirect generated from X-Forwarded-Uri header: %s\",\n-\t)\n-\tif redirect == \"\" {\n-\t\tredirect = req.URL.RequestURI()\n-\t}\n-\n-\tif p.hasProxyPrefix(redirect) {\n-\t\treturn \"/\"\n-\t}\n-\treturn redirect\n+        redirect := p.validateRedirect(\n+                requestutil.GetRequestURI(req),\n+                \"Invalid redirect generated from X-Forwarded-Uri header: %s\",\n+        )\n+        if redirect == \"\" {\n+                redirect = req.URL.RequestURI()\n+        }\n+\n+        if p.hasProxyPrefix(redirect) {\n+                return \"/\"\n+        }\n+        return redirect\n }\n \n // splitHostPort separates host and port. If the port is not valid, it returns\n@@ -1050,36 +1050,36 @@ func (p *OAuthProxy) getURIRedirect(req *http.Request) string {\n // Unlike net.SplitHostPort, but per RFC 3986, it requires ports to be numeric.\n // *** taken from net/url, modified validOptionalPort() to accept \":*\"\n func splitHostPort(hostport string) (host, port string) {\n-\thost = hostport\n+        host = hostport\n \n-\tcolon := strings.LastIndexByte(host, ':')\n-\tif colon != -1 && validOptionalPort(host[colon:]) {\n-\t\thost, port = host[:colon], host[colon+1:]\n-\t}\n+        colon := strings.LastIndexByte(host, ':')\n+        if colon != -1 && validOptionalPort(host[colon:]) {\n+                host, port = host[:colon], host[colon+1:]\n+        }\n \n-\tif strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n-\t\thost = host[1 : len(host)-1]\n-\t}\n+        if strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n+                host = host[1 : len(host)-1]\n+        }\n \n-\treturn\n+        return\n }\n \n // validOptionalPort reports whether port is either an empty string\n // or matches /^:\\d*$/\n // *** taken from net/url, modified to accept \":*\"\n func validOptionalPort(port string) bool {\n-\tif port == \"\" || port == \":*\" {\n-\t\treturn true\n-\t}\n-\tif port[0] != ':' {\n-\t\treturn false\n-\t}\n-\tfor _, b := range port[1:] {\n-\t\tif b < '0' || b > '9' {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\treturn true\n+        if port == \"\" || port == \":*\" {\n+                return true\n+        }\n+        if port[0] != ':' {\n+                return false\n+        }\n+        for _, b := range port[1:] {\n+                if b < '0' || b > '9' {\n+                        return false\n+                }\n+        }\n+        return true\n }\n \n // getAuthenticatedSession checks whether a user is authenticated and returns a session object and nil error if so\n@@ -1088,34 +1088,34 @@ func validOptionalPort(port string) bool {\n // - `nil, ErrAccessDenied` if the authenticated user is not authorized\n // Set-Cookie headers may be set on the response as a side-effect of calling this method.\n func (p *OAuthProxy) getAuthenticatedSession(rw http.ResponseWriter, req *http.Request) (*sessionsapi.SessionState, error) {\n-\tvar session *sessionsapi.SessionState\n-\n-\tgetSession := p.sessionChain.Then(http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) {\n-\t\tsession = middlewareapi.GetRequestScope(req).Session\n-\t}))\n-\tgetSession.ServeHTTP(rw, req)\n-\n-\tif session == nil {\n-\t\treturn nil, ErrNeedsLogin\n-\t}\n-\n-\tinvalidEmail := session.Email != \"\" && !p.Validator(session.Email)\n-\tauthorized, err := p.provider.Authorize(req.Context(), session)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error with authorization: %v\", err)\n-\t}\n-\n-\tif invalidEmail || !authorized {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authorization via session: removing session %s\", session)\n-\t\t// Invalid session, clear it\n-\t\terr := p.ClearSessionCookie(rw, req)\n-\t\tif err != nil {\n-\t\t\tlogger.Errorf(\"Error clearing session cookie: %v\", err)\n-\t\t}\n-\t\treturn nil, ErrAccessDenied\n-\t}\n-\n-\treturn session, nil\n+        var session *sessionsapi.SessionState\n+\n+        getSession := p.sessionChain.Then(http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) {\n+                session = middlewareapi.GetRequestScope(req).Session\n+        }))\n+        getSession.ServeHTTP(rw, req)\n+\n+        if session == nil {\n+                return nil, ErrNeedsLogin\n+        }\n+\n+        invalidEmail := session.Email != \"\" && !p.Validator(session.Email)\n+        authorized, err := p.provider.Authorize(req.Context(), session)\n+        if err != nil {\n+                logger.Errorf(\"Error with authorization: %v\", err)\n+        }\n+\n+        if invalidEmail || !authorized {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authorization via session: removing session %s\", session)\n+                // Invalid session, clear it\n+                err := p.ClearSessionCookie(rw, req)\n+                if err != nil {\n+                        logger.Errorf(\"Error clearing session cookie: %v\", err)\n+                }\n+                return nil, ErrAccessDenied\n+        }\n+\n+        return session, nil\n }\n \n // authOnlyAuthorize handles special authorization logic that is only done\n@@ -1126,76 +1126,76 @@ func (p *OAuthProxy) getAuthenticatedSession(rw http.ResponseWriter, req *http.R\n //\n //nolint:S1008\n func authOnlyAuthorize(req *http.Request, s *sessionsapi.SessionState) bool {\n-\t// Allow secondary group restrictions based on the `allowed_groups`\n-\t// querystring parameter\n-\tif !checkAllowedGroups(req, s) {\n-\t\treturn false\n-\t}\n+        // Allow secondary group restrictions based on the `allowed_groups`\n+        // querystring parameter\n+        if !checkAllowedGroups(req, s) {\n+                return false\n+        }\n \n-\treturn true\n+        return true\n }\n \n func checkAllowedGroups(req *http.Request, s *sessionsapi.SessionState) bool {\n-\tallowedGroups := extractAllowedGroups(req)\n-\tif len(allowedGroups) == 0 {\n-\t\treturn true\n-\t}\n-\n-\tfor _, group := range s.Groups {\n-\t\tif _, ok := allowedGroups[group]; ok {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\n-\treturn false\n+        allowedGroups := extractAllowedGroups(req)\n+        if len(allowedGroups) == 0 {\n+                return true\n+        }\n+\n+        for _, group := range s.Groups {\n+                if _, ok := allowedGroups[group]; ok {\n+                        return true\n+                }\n+        }\n+\n+        return false\n }\n \n func extractAllowedGroups(req *http.Request) map[string]struct{} {\n-\tgroups := map[string]struct{}{}\n-\n-\tquery := req.URL.Query()\n-\tfor _, allowedGroups := range query[\"allowed_groups\"] {\n-\t\tfor _, group := range strings.Split(allowedGroups, \",\") {\n-\t\t\tif group != \"\" {\n-\t\t\t\tgroups[group] = struct{}{}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn groups\n+        groups := map[string]struct{}{}\n+\n+        query := req.URL.Query()\n+        for _, allowedGroups := range query[\"allowed_groups\"] {\n+                for _, group := range strings.Split(allowedGroups, \",\") {\n+                        if group != \"\" {\n+                                groups[group] = struct{}{}\n+                        }\n+                }\n+        }\n+\n+        return groups\n }\n \n // addHeadersForProxying adds the appropriate headers the request / response for proxying\n func (p *OAuthProxy) addHeadersForProxying(rw http.ResponseWriter, session *sessionsapi.SessionState) {\n-\tif session.Email == \"\" {\n-\t\trw.Header().Set(\"GAP-Auth\", session.User)\n-\t} else {\n-\t\trw.Header().Set(\"GAP-Auth\", session.Email)\n-\t}\n+        if session.Email == \"\" {\n+                rw.Header().Set(\"GAP-Auth\", session.User)\n+        } else {\n+                rw.Header().Set(\"GAP-Auth\", session.Email)\n+        }\n }\n \n // isAjax checks if a request is an ajax request\n func isAjax(req *http.Request) bool {\n-\tacceptValues := req.Header.Values(\"Accept\")\n-\tconst ajaxReq = applicationJSON\n-\t// Iterate over multiple Accept headers, i.e.\n-\t// Accept: application/json\n-\t// Accept: text/plain\n-\tfor _, mimeTypes := range acceptValues {\n-\t\t// Iterate over multiple mimetypes in a single header, i.e.\n-\t\t// Accept: application/json, text/plain, */*\n-\t\tfor _, mimeType := range strings.Split(mimeTypes, \",\") {\n-\t\t\tmimeType = strings.TrimSpace(mimeType)\n-\t\t\tif mimeType == ajaxReq {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn false\n+        acceptValues := req.Header.Values(\"Accept\")\n+        const ajaxReq = applicationJSON\n+        // Iterate over multiple Accept headers, i.e.\n+        // Accept: application/json\n+        // Accept: text/plain\n+        for _, mimeTypes := range acceptValues {\n+                // Iterate over multiple mimetypes in a single header, i.e.\n+                // Accept: application/json, text/plain, */*\n+                for _, mimeType := range strings.Split(mimeTypes, \",\") {\n+                        mimeType = strings.TrimSpace(mimeType)\n+                        if mimeType == ajaxReq {\n+                                return true\n+                        }\n+                }\n+        }\n+        return false\n }\n \n // errorJSON returns the error code with an application/json mime type\n func (p *OAuthProxy) errorJSON(rw http.ResponseWriter, code int) {\n-\trw.Header().Set(\"Content-Type\", applicationJSON)\n-\trw.WriteHeader(code)\n+        rw.Header().Set(\"Content-Type\", applicationJSON)\n+        rw.WriteHeader(code)\n }\n"}
{"cve":"CVE-2021-43798:0708", "fix_patch": "diff --git a/pkg/api/plugins.go b/pkg/api/plugins.go\nindex 6e01a3414fe..e29587c50af 100644\n--- a/pkg/api/plugins.go\n+++ b/pkg/api/plugins.go\n@@ -1,507 +1,519 @@\n package api\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"sort\"\n-\t\"strings\"\n-\n-\t\"github.com/grafana/grafana-plugin-sdk-go/backend\"\n-\t\"github.com/grafana/grafana/pkg/api/dtos\"\n-\t\"github.com/grafana/grafana/pkg/api/response\"\n-\t\"github.com/grafana/grafana/pkg/bus\"\n-\t\"github.com/grafana/grafana/pkg/infra/fs\"\n-\t\"github.com/grafana/grafana/pkg/models\"\n-\t\"github.com/grafana/grafana/pkg/plugins\"\n-\t\"github.com/grafana/grafana/pkg/plugins/backendplugin\"\n-\t\"github.com/grafana/grafana/pkg/plugins/manager/installer\"\n-\t\"github.com/grafana/grafana/pkg/setting\"\n-\t\"github.com/grafana/grafana/pkg/web\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"net/http\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"sort\"\n+        \"strings\"\n+\n+        \"github.com/grafana/grafana-plugin-sdk-go/backend\"\n+        \"github.com/grafana/grafana/pkg/api/dtos\"\n+        \"github.com/grafana/grafana/pkg/api/response\"\n+        \"github.com/grafana/grafana/pkg/bus\"\n+        \"github.com/grafana/grafana/pkg/infra/fs\"\n+        \"github.com/grafana/grafana/pkg/models\"\n+        \"github.com/grafana/grafana/pkg/plugins\"\n+        \"github.com/grafana/grafana/pkg/plugins/backendplugin\"\n+        \"github.com/grafana/grafana/pkg/plugins/manager/installer\"\n+        \"github.com/grafana/grafana/pkg/setting\"\n+        \"github.com/grafana/grafana/pkg/web\"\n )\n \n func (hs *HTTPServer) GetPluginList(c *models.ReqContext) response.Response {\n-\ttypeFilter := c.Query(\"type\")\n-\tenabledFilter := c.Query(\"enabled\")\n-\tembeddedFilter := c.Query(\"embedded\")\n-\tcoreFilter := c.Query(\"core\")\n-\n-\t// For users with viewer role we only return core plugins\n-\tif !c.HasRole(models.ROLE_ADMIN) {\n-\t\tcoreFilter = \"1\"\n-\t}\n-\n-\tpluginSettingsMap, err := hs.pluginSettings(c.Req.Context(), c.OrgId)\n-\tif err != nil {\n-\t\treturn response.Error(500, \"Failed to get list of plugins\", err)\n-\t}\n-\n-\tresult := make(dtos.PluginList, 0)\n-\tfor _, pluginDef := range hs.pluginStore.Plugins(c.Req.Context()) {\n-\t\t// filter out app sub plugins\n-\t\tif embeddedFilter == \"0\" && pluginDef.IncludedInAppID != \"\" {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// filter out core plugins\n-\t\tif (coreFilter == \"0\" && pluginDef.IsCorePlugin()) || (coreFilter == \"1\" && !pluginDef.IsCorePlugin()) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// filter on type\n-\t\tif typeFilter != \"\" && typeFilter != string(pluginDef.Type) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif pluginDef.State == plugins.AlphaRelease && !hs.Cfg.PluginsEnableAlpha {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tlistItem := dtos.PluginListItem{\n-\t\t\tId:            pluginDef.ID,\n-\t\t\tName:          pluginDef.Name,\n-\t\t\tType:          string(pluginDef.Type),\n-\t\t\tCategory:      pluginDef.Category,\n-\t\t\tInfo:          pluginDef.Info,\n-\t\t\tDependencies:  pluginDef.Dependencies,\n-\t\t\tLatestVersion: pluginDef.GrafanaComVersion,\n-\t\t\tHasUpdate:     pluginDef.GrafanaComHasUpdate,\n-\t\t\tDefaultNavUrl: pluginDef.DefaultNavURL,\n-\t\t\tState:         pluginDef.State,\n-\t\t\tSignature:     pluginDef.Signature,\n-\t\t\tSignatureType: pluginDef.SignatureType,\n-\t\t\tSignatureOrg:  pluginDef.SignatureOrg,\n-\t\t}\n-\n-\t\tif pluginSetting, exists := pluginSettingsMap[pluginDef.ID]; exists {\n-\t\t\tlistItem.Enabled = pluginSetting.Enabled\n-\t\t\tlistItem.Pinned = pluginSetting.Pinned\n-\t\t}\n-\n-\t\tif listItem.DefaultNavUrl == \"\" || !listItem.Enabled {\n-\t\t\tlistItem.DefaultNavUrl = hs.Cfg.AppSubURL + \"/plugins/\" + listItem.Id + \"/\"\n-\t\t}\n-\n-\t\t// filter out disabled plugins\n-\t\tif enabledFilter == \"1\" && !listItem.Enabled {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// filter out built in plugins\n-\t\tif pluginDef.BuiltIn {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tresult = append(result, listItem)\n-\t}\n-\n-\tsort.Sort(result)\n-\treturn response.JSON(200, result)\n+        typeFilter := c.Query(\"type\")\n+        enabledFilter := c.Query(\"enabled\")\n+        embeddedFilter := c.Query(\"embedded\")\n+        coreFilter := c.Query(\"core\")\n+\n+        // For users with viewer role we only return core plugins\n+        if !c.HasRole(models.ROLE_ADMIN) {\n+                coreFilter = \"1\"\n+        }\n+\n+        pluginSettingsMap, err := hs.pluginSettings(c.Req.Context(), c.OrgId)\n+        if err != nil {\n+                return response.Error(500, \"Failed to get list of plugins\", err)\n+        }\n+\n+        result := make(dtos.PluginList, 0)\n+        for _, pluginDef := range hs.pluginStore.Plugins(c.Req.Context()) {\n+                // filter out app sub plugins\n+                if embeddedFilter == \"0\" && pluginDef.IncludedInAppID != \"\" {\n+                        continue\n+                }\n+\n+                // filter out core plugins\n+                if (coreFilter == \"0\" && pluginDef.IsCorePlugin()) || (coreFilter == \"1\" && !pluginDef.IsCorePlugin()) {\n+                        continue\n+                }\n+\n+                // filter on type\n+                if typeFilter != \"\" && typeFilter != string(pluginDef.Type) {\n+                        continue\n+                }\n+\n+                if pluginDef.State == plugins.AlphaRelease && !hs.Cfg.PluginsEnableAlpha {\n+                        continue\n+                }\n+\n+                listItem := dtos.PluginListItem{\n+                        Id:            pluginDef.ID,\n+                        Name:          pluginDef.Name,\n+                        Type:          string(pluginDef.Type),\n+                        Category:      pluginDef.Category,\n+                        Info:          pluginDef.Info,\n+                        Dependencies:  pluginDef.Dependencies,\n+                        LatestVersion: pluginDef.GrafanaComVersion,\n+                        HasUpdate:     pluginDef.GrafanaComHasUpdate,\n+                        DefaultNavUrl: pluginDef.DefaultNavURL,\n+                        State:         pluginDef.State,\n+                        Signature:     pluginDef.Signature,\n+                        SignatureType: pluginDef.SignatureType,\n+                        SignatureOrg:  pluginDef.SignatureOrg,\n+                }\n+\n+                if pluginSetting, exists := pluginSettingsMap[pluginDef.ID]; exists {\n+                        listItem.Enabled = pluginSetting.Enabled\n+                        listItem.Pinned = pluginSetting.Pinned\n+                }\n+\n+                if listItem.DefaultNavUrl == \"\" || !listItem.Enabled {\n+                        listItem.DefaultNavUrl = hs.Cfg.AppSubURL + \"/plugins/\" + listItem.Id + \"/\"\n+                }\n+\n+                // filter out disabled plugins\n+                if enabledFilter == \"1\" && !listItem.Enabled {\n+                        continue\n+                }\n+\n+                // filter out built in plugins\n+                if pluginDef.BuiltIn {\n+                        continue\n+                }\n+\n+                result = append(result, listItem)\n+        }\n+\n+        sort.Sort(result)\n+        return response.JSON(200, result)\n }\n \n func (hs *HTTPServer) GetPluginSettingByID(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\tplugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n-\tif !exists {\n-\t\treturn response.Error(404, \"Plugin not found, no installed plugin with that id\", nil)\n-\t}\n-\n-\tdto := &dtos.PluginSetting{\n-\t\tType:          string(plugin.Type),\n-\t\tId:            plugin.ID,\n-\t\tName:          plugin.Name,\n-\t\tInfo:          plugin.Info,\n-\t\tDependencies:  plugin.Dependencies,\n-\t\tIncludes:      plugin.Includes,\n-\t\tBaseUrl:       plugin.BaseURL,\n-\t\tModule:        plugin.Module,\n-\t\tDefaultNavUrl: plugin.DefaultNavURL,\n-\t\tLatestVersion: plugin.GrafanaComVersion,\n-\t\tHasUpdate:     plugin.GrafanaComHasUpdate,\n-\t\tState:         plugin.State,\n-\t\tSignature:     plugin.Signature,\n-\t\tSignatureType: plugin.SignatureType,\n-\t\tSignatureOrg:  plugin.SignatureOrg,\n-\t}\n-\n-\tif plugin.IsApp() {\n-\t\tdto.Enabled = plugin.AutoEnabled\n-\t\tdto.Pinned = plugin.AutoEnabled\n-\t}\n-\n-\tquery := models.GetPluginSettingByIdQuery{PluginId: pluginID, OrgId: c.OrgId}\n-\tif err := bus.DispatchCtx(c.Req.Context(), &query); err != nil {\n-\t\tif !errors.Is(err, models.ErrPluginSettingNotFound) {\n-\t\t\treturn response.Error(500, \"Failed to get login settings\", nil)\n-\t\t}\n-\t} else {\n-\t\tdto.Enabled = query.Result.Enabled\n-\t\tdto.Pinned = query.Result.Pinned\n-\t\tdto.JsonData = query.Result.JsonData\n-\t}\n-\n-\treturn response.JSON(200, dto)\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        plugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n+        if !exists {\n+                return response.Error(404, \"Plugin not found, no installed plugin with that id\", nil)\n+        }\n+\n+        dto := &dtos.PluginSetting{\n+                Type:          string(plugin.Type),\n+                Id:            plugin.ID,\n+                Name:          plugin.Name,\n+                Info:          plugin.Info,\n+                Dependencies:  plugin.Dependencies,\n+                Includes:      plugin.Includes,\n+                BaseUrl:       plugin.BaseURL,\n+                Module:        plugin.Module,\n+                DefaultNavUrl: plugin.DefaultNavURL,\n+                LatestVersion: plugin.GrafanaComVersion,\n+                HasUpdate:     plugin.GrafanaComHasUpdate,\n+                State:         plugin.State,\n+                Signature:     plugin.Signature,\n+                SignatureType: plugin.SignatureType,\n+                SignatureOrg:  plugin.SignatureOrg,\n+        }\n+\n+        if plugin.IsApp() {\n+                dto.Enabled = plugin.AutoEnabled\n+                dto.Pinned = plugin.AutoEnabled\n+        }\n+\n+        query := models.GetPluginSettingByIdQuery{PluginId: pluginID, OrgId: c.OrgId}\n+        if err := bus.DispatchCtx(c.Req.Context(), &query); err != nil {\n+                if !errors.Is(err, models.ErrPluginSettingNotFound) {\n+                        return response.Error(500, \"Failed to get login settings\", nil)\n+                }\n+        } else {\n+                dto.Enabled = query.Result.Enabled\n+                dto.Pinned = query.Result.Pinned\n+                dto.JsonData = query.Result.JsonData\n+        }\n+\n+        return response.JSON(200, dto)\n }\n \n func (hs *HTTPServer) UpdatePluginSetting(c *models.ReqContext) response.Response {\n-\tcmd := models.UpdatePluginSettingCmd{}\n-\tif err := web.Bind(c.Req, &cmd); err != nil {\n-\t\treturn response.Error(http.StatusBadRequest, \"bad request data\", err)\n-\t}\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\tif _, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID); !exists {\n-\t\treturn response.Error(404, \"Plugin not installed\", nil)\n-\t}\n-\n-\tcmd.OrgId = c.OrgId\n-\tcmd.PluginId = pluginID\n-\tif err := bus.DispatchCtx(c.Req.Context(), &cmd); err != nil {\n-\t\treturn response.Error(500, \"Failed to update plugin setting\", err)\n-\t}\n-\n-\treturn response.Success(\"Plugin settings updated\")\n+        cmd := models.UpdatePluginSettingCmd{}\n+        if err := web.Bind(c.Req, &cmd); err != nil {\n+                return response.Error(http.StatusBadRequest, \"bad request data\", err)\n+        }\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        if _, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID); !exists {\n+                return response.Error(404, \"Plugin not installed\", nil)\n+        }\n+\n+        cmd.OrgId = c.OrgId\n+        cmd.PluginId = pluginID\n+        if err := bus.DispatchCtx(c.Req.Context(), &cmd); err != nil {\n+                return response.Error(500, \"Failed to update plugin setting\", err)\n+        }\n+\n+        return response.Success(\"Plugin settings updated\")\n }\n \n func (hs *HTTPServer) GetPluginDashboards(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n \n-\tlist, err := hs.pluginDashboardManager.GetPluginDashboards(c.Req.Context(), c.OrgId, pluginID)\n-\tif err != nil {\n-\t\tvar notFound plugins.NotFoundError\n-\t\tif errors.As(err, &notFound) {\n-\t\t\treturn response.Error(404, notFound.Error(), nil)\n-\t\t}\n+        list, err := hs.pluginDashboardManager.GetPluginDashboards(c.Req.Context(), c.OrgId, pluginID)\n+        if err != nil {\n+                var notFound plugins.NotFoundError\n+                if errors.As(err, &notFound) {\n+                        return response.Error(404, notFound.Error(), nil)\n+                }\n \n-\t\treturn response.Error(500, \"Failed to get plugin dashboards\", err)\n-\t}\n+                return response.Error(500, \"Failed to get plugin dashboards\", err)\n+        }\n \n-\treturn response.JSON(200, list)\n+        return response.JSON(200, list)\n }\n \n func (hs *HTTPServer) GetPluginMarkdown(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\tname := web.Params(c.Req)[\":name\"]\n-\n-\tcontent, err := hs.pluginMarkdown(c.Req.Context(), pluginID, name)\n-\tif err != nil {\n-\t\tvar notFound plugins.NotFoundError\n-\t\tif errors.As(err, &notFound) {\n-\t\t\treturn response.Error(404, notFound.Error(), nil)\n-\t\t}\n-\n-\t\treturn response.Error(500, \"Could not get markdown file\", err)\n-\t}\n-\n-\t// fallback try readme\n-\tif len(content) == 0 {\n-\t\tcontent, err = hs.pluginMarkdown(c.Req.Context(), pluginID, \"readme\")\n-\t\tif err != nil {\n-\t\t\treturn response.Error(501, \"Could not get markdown file\", err)\n-\t\t}\n-\t}\n-\n-\tresp := response.Respond(200, content)\n-\tresp.SetHeader(\"Content-Type\", \"text/plain; charset=utf-8\")\n-\treturn resp\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+        name := web.Params(c.Req)[\":name\"]\n+\n+        content, err := hs.pluginMarkdown(c.Req.Context(), pluginID, name)\n+        if err != nil {\n+                var notFound plugins.NotFoundError\n+                if errors.As(err, &notFound) {\n+                        return response.Error(404, notFound.Error(), nil)\n+                }\n+\n+                return response.Error(500, \"Could not get markdown file\", err)\n+        }\n+\n+        // fallback try readme\n+        if len(content) == 0 {\n+                content, err = hs.pluginMarkdown(c.Req.Context(), pluginID, \"readme\")\n+                if err != nil {\n+                        return response.Error(501, \"Could not get markdown file\", err)\n+                }\n+        }\n+\n+        resp := response.Respond(200, content)\n+        resp.SetHeader(\"Content-Type\", \"text/plain; charset=utf-8\")\n+        return resp\n }\n \n func (hs *HTTPServer) ImportDashboard(c *models.ReqContext) response.Response {\n-\tapiCmd := dtos.ImportDashboardCommand{}\n-\tif err := web.Bind(c.Req, &apiCmd); err != nil {\n-\t\treturn response.Error(http.StatusBadRequest, \"bad request data\", err)\n-\t}\n-\tvar err error\n-\tif apiCmd.PluginId == \"\" && apiCmd.Dashboard == nil {\n-\t\treturn response.Error(422, \"Dashboard must be set\", nil)\n-\t}\n-\n-\tlimitReached, err := hs.QuotaService.QuotaReached(c, \"dashboard\")\n-\tif err != nil {\n-\t\treturn response.Error(500, \"failed to get quota\", err)\n-\t}\n-\tif limitReached {\n-\t\treturn response.Error(403, \"Quota reached\", nil)\n-\t}\n-\n-\ttrimDefaults := c.QueryBoolWithDefault(\"trimdefaults\", true)\n-\tif trimDefaults && !hs.LoadSchemaService.IsDisabled() {\n-\t\tapiCmd.Dashboard, err = hs.LoadSchemaService.DashboardApplyDefaults(apiCmd.Dashboard)\n-\t\tif err != nil {\n-\t\t\treturn response.Error(500, \"Error while applying default value to the dashboard json\", err)\n-\t\t}\n-\t}\n-\n-\tdashInfo, dash, err := hs.pluginDashboardManager.ImportDashboard(c.Req.Context(), apiCmd.PluginId, apiCmd.Path, c.OrgId, apiCmd.FolderId,\n-\t\tapiCmd.Dashboard, apiCmd.Overwrite, apiCmd.Inputs, c.SignedInUser)\n-\tif err != nil {\n-\t\treturn hs.dashboardSaveErrorToApiResponse(c.Req.Context(), err)\n-\t}\n-\n-\terr = hs.LibraryPanelService.ImportLibraryPanelsForDashboard(c.Req.Context(), c.SignedInUser, dash, apiCmd.FolderId)\n-\tif err != nil {\n-\t\treturn response.Error(500, \"Error while importing library panels\", err)\n-\t}\n-\n-\terr = hs.LibraryPanelService.ConnectLibraryPanelsForDashboard(c.Req.Context(), c.SignedInUser, dash)\n-\tif err != nil {\n-\t\treturn response.Error(500, \"Error while connecting library panels\", err)\n-\t}\n-\n-\treturn response.JSON(200, dashInfo)\n+        apiCmd := dtos.ImportDashboardCommand{}\n+        if err := web.Bind(c.Req, &apiCmd); err != nil {\n+                return response.Error(http.StatusBadRequest, \"bad request data\", err)\n+        }\n+        var err error\n+        if apiCmd.PluginId == \"\" && apiCmd.Dashboard == nil {\n+                return response.Error(422, \"Dashboard must be set\", nil)\n+        }\n+\n+        limitReached, err := hs.QuotaService.QuotaReached(c, \"dashboard\")\n+        if err != nil {\n+                return response.Error(500, \"failed to get quota\", err)\n+        }\n+        if limitReached {\n+                return response.Error(403, \"Quota reached\", nil)\n+        }\n+\n+        trimDefaults := c.QueryBoolWithDefault(\"trimdefaults\", true)\n+        if trimDefaults && !hs.LoadSchemaService.IsDisabled() {\n+                apiCmd.Dashboard, err = hs.LoadSchemaService.DashboardApplyDefaults(apiCmd.Dashboard)\n+                if err != nil {\n+                        return response.Error(500, \"Error while applying default value to the dashboard json\", err)\n+                }\n+        }\n+\n+        dashInfo, dash, err := hs.pluginDashboardManager.ImportDashboard(c.Req.Context(), apiCmd.PluginId, apiCmd.Path, c.OrgId, apiCmd.FolderId,\n+                apiCmd.Dashboard, apiCmd.Overwrite, apiCmd.Inputs, c.SignedInUser)\n+        if err != nil {\n+                return hs.dashboardSaveErrorToApiResponse(c.Req.Context(), err)\n+        }\n+\n+        err = hs.LibraryPanelService.ImportLibraryPanelsForDashboard(c.Req.Context(), c.SignedInUser, dash, apiCmd.FolderId)\n+        if err != nil {\n+                return response.Error(500, \"Error while importing library panels\", err)\n+        }\n+\n+        err = hs.LibraryPanelService.ConnectLibraryPanelsForDashboard(c.Req.Context(), c.SignedInUser, dash)\n+        if err != nil {\n+                return response.Error(500, \"Error while connecting library panels\", err)\n+        }\n+\n+        return response.JSON(200, dashInfo)\n }\n \n // CollectPluginMetrics collect metrics from a plugin.\n //\n // /api/plugins/:pluginId/metrics\n func (hs *HTTPServer) CollectPluginMetrics(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\tplugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n-\tif !exists {\n-\t\treturn response.Error(404, \"Plugin not found\", nil)\n-\t}\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+        plugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n+        if !exists {\n+                return response.Error(404, \"Plugin not found\", nil)\n+        }\n \n-\tresp, err := hs.pluginClient.CollectMetrics(c.Req.Context(), plugin.ID)\n-\tif err != nil {\n-\t\treturn translatePluginRequestErrorToAPIError(err)\n-\t}\n+        resp, err := hs.pluginClient.CollectMetrics(c.Req.Context(), plugin.ID)\n+        if err != nil {\n+                return translatePluginRequestErrorToAPIError(err)\n+        }\n \n-\theaders := make(http.Header)\n-\theaders.Set(\"Content-Type\", \"text/plain\")\n+        headers := make(http.Header)\n+        headers.Set(\"Content-Type\", \"text/plain\")\n \n-\treturn response.CreateNormalResponse(headers, resp.PrometheusMetrics, http.StatusOK)\n+        return response.CreateNormalResponse(headers, resp.PrometheusMetrics, http.StatusOK)\n }\n \n // getPluginAssets returns public plugin assets (images, JS, etc.)\n //\n // /public/plugins/:pluginId/*\n func (hs *HTTPServer) getPluginAssets(c *models.ReqContext) {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\tplugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n-\tif !exists {\n-\t\tc.JsonApiErr(404, \"Plugin not found\", nil)\n-\t\treturn\n-\t}\n-\n-\trequestedFile := filepath.Clean(web.Params(c.Req)[\"*\"])\n-\tpluginFilePath := filepath.Join(plugin.PluginDir, requestedFile)\n-\n-\tif !plugin.IncludedInSignature(requestedFile) {\n-\t\ths.log.Warn(\"Access to requested plugin file will be forbidden in upcoming Grafana versions as the file \"+\n-\t\t\t\"is not included in the plugin signature\", \"file\", requestedFile)\n-\t}\n-\n-\t// It's safe to ignore gosec warning G304 since we already clean the requested file path and subsequently\n-\t// use this with a prefix of the plugin's directory, which is set during plugin loading\n-\t// nolint:gosec\n-\tf, err := os.Open(pluginFilePath)\n-\tif err != nil {\n-\t\tif os.IsNotExist(err) {\n-\t\t\tc.JsonApiErr(404, \"Plugin file not found\", err)\n-\t\t\treturn\n-\t\t}\n-\t\tc.JsonApiErr(500, \"Could not open plugin file\", err)\n-\t\treturn\n-\t}\n-\tdefer func() {\n-\t\tif err := f.Close(); err != nil {\n-\t\t\ths.log.Error(\"Failed to close file\", \"err\", err)\n-\t\t}\n-\t}()\n-\n-\tfi, err := f.Stat()\n-\tif err != nil {\n-\t\tc.JsonApiErr(500, \"Plugin file exists but could not open\", err)\n-\t\treturn\n-\t}\n-\n-\tif hs.Cfg.Env == setting.Dev {\n-\t\tc.Resp.Header().Set(\"Cache-Control\", \"max-age=0, must-revalidate, no-cache\")\n-\t} else {\n-\t\tc.Resp.Header().Set(\"Cache-Control\", \"public, max-age=3600\")\n-\t}\n-\n-\thttp.ServeContent(c.Resp, c.Req, pluginFilePath, fi.ModTime(), f)\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+        plugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n+        if !exists {\n+                c.JsonApiErr(404, \"Plugin not found\", nil)\n+                return\n+        }\n+\n+        \n+requestedFile := web.Params(c.Req)[\"*\"]\n+\n+absPluginDir, err := filepath.Abs(plugin.PluginDir)\n+if err != nil {\n+ApiErr(500, \"Failed to get absolute path for plugin directory\", err)\n+\n+}\n+\n+pluginFilePath := filepath.Join(absPluginDir, requestedFile)\n+if !strings.HasPrefix(filepath.Clean(pluginFilePath), absPluginDir) {\n+ApiErr(403, \"Access to this file is forbidden\", nil)\n+\n+}\n+\n+        if !plugin.IncludedInSignature(requestedFile) {\n+                hs.log.Warn(\"Access to requested plugin file will be forbidden in upcoming Grafana versions as the file \"+\n+                        \"is not included in the plugin signature\", \"file\", requestedFile)\n+        }\n+\n+        // It's safe to ignore gosec warning G304 since we already clean the requested file path and subsequently\n+        // use this with a prefix of the plugin's directory, which is set during plugin loading\n+        // nolint:gosec\n+        f, err := os.Open(pluginFilePath)\n+        if err != nil {\n+                if os.IsNotExist(err) {\n+                        c.JsonApiErr(404, \"Plugin file not found\", err)\n+                        return\n+                }\n+                c.JsonApiErr(500, \"Could not open plugin file\", err)\n+                return\n+        }\n+        defer func() {\n+                if err := f.Close(); err != nil {\n+                        hs.log.Error(\"Failed to close file\", \"err\", err)\n+                }\n+        }()\n+\n+        fi, err := f.Stat()\n+        if err != nil {\n+                c.JsonApiErr(500, \"Plugin file exists but could not open\", err)\n+                return\n+        }\n+\n+        if hs.Cfg.Env == setting.Dev {\n+                c.Resp.Header().Set(\"Cache-Control\", \"max-age=0, must-revalidate, no-cache\")\n+        } else {\n+                c.Resp.Header().Set(\"Cache-Control\", \"public, max-age=3600\")\n+        }\n+\n+        http.ServeContent(c.Resp, c.Req, pluginFilePath, fi.ModTime(), f)\n }\n \n // CheckHealth returns the health of a plugin.\n // /api/plugins/:pluginId/health\n func (hs *HTTPServer) CheckHealth(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\tpCtx, found, err := hs.PluginContextProvider.Get(c.Req.Context(), pluginID, \"\", c.SignedInUser, false)\n-\tif err != nil {\n-\t\treturn response.Error(500, \"Failed to get plugin settings\", err)\n-\t}\n-\tif !found {\n-\t\treturn response.Error(404, \"Plugin not found\", nil)\n-\t}\n-\n-\tresp, err := hs.pluginClient.CheckHealth(c.Req.Context(), &backend.CheckHealthRequest{\n-\t\tPluginContext: pCtx,\n-\t})\n-\tif err != nil {\n-\t\treturn translatePluginRequestErrorToAPIError(err)\n-\t}\n-\n-\tpayload := map[string]interface{}{\n-\t\t\"status\":  resp.Status.String(),\n-\t\t\"message\": resp.Message,\n-\t}\n-\n-\t// Unmarshal JSONDetails if it's not empty.\n-\tif len(resp.JSONDetails) > 0 {\n-\t\tvar jsonDetails map[string]interface{}\n-\t\terr = json.Unmarshal(resp.JSONDetails, &jsonDetails)\n-\t\tif err != nil {\n-\t\t\treturn response.Error(500, \"Failed to unmarshal detailed response from backend plugin\", err)\n-\t\t}\n-\n-\t\tpayload[\"details\"] = jsonDetails\n-\t}\n-\n-\tif resp.Status != backend.HealthStatusOk {\n-\t\treturn response.JSON(503, payload)\n-\t}\n-\n-\treturn response.JSON(200, payload)\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        pCtx, found, err := hs.PluginContextProvider.Get(c.Req.Context(), pluginID, \"\", c.SignedInUser, false)\n+        if err != nil {\n+                return response.Error(500, \"Failed to get plugin settings\", err)\n+        }\n+        if !found {\n+                return response.Error(404, \"Plugin not found\", nil)\n+        }\n+\n+        resp, err := hs.pluginClient.CheckHealth(c.Req.Context(), &backend.CheckHealthRequest{\n+                PluginContext: pCtx,\n+        })\n+        if err != nil {\n+                return translatePluginRequestErrorToAPIError(err)\n+        }\n+\n+        payload := map[string]interface{}{\n+                \"status\":  resp.Status.String(),\n+                \"message\": resp.Message,\n+        }\n+\n+        // Unmarshal JSONDetails if it's not empty.\n+        if len(resp.JSONDetails) > 0 {\n+                var jsonDetails map[string]interface{}\n+                err = json.Unmarshal(resp.JSONDetails, &jsonDetails)\n+                if err != nil {\n+                        return response.Error(500, \"Failed to unmarshal detailed response from backend plugin\", err)\n+                }\n+\n+                payload[\"details\"] = jsonDetails\n+        }\n+\n+        if resp.Status != backend.HealthStatusOk {\n+                return response.JSON(503, payload)\n+        }\n+\n+        return response.JSON(200, payload)\n }\n \n // CallResource passes a resource call from a plugin to the backend plugin.\n //\n // /api/plugins/:pluginId/resources/*\n func (hs *HTTPServer) CallResource(c *models.ReqContext) {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\tpCtx, found, err := hs.PluginContextProvider.Get(c.Req.Context(), pluginID, \"\", c.SignedInUser, false)\n-\tif err != nil {\n-\t\tc.JsonApiErr(500, \"Failed to get plugin settings\", err)\n-\t\treturn\n-\t}\n-\tif !found {\n-\t\tc.JsonApiErr(404, \"Plugin not found\", nil)\n-\t\treturn\n-\t}\n-\ths.pluginClient.CallResource(pCtx, c, web.Params(c.Req)[\"*\"])\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        pCtx, found, err := hs.PluginContextProvider.Get(c.Req.Context(), pluginID, \"\", c.SignedInUser, false)\n+        if err != nil {\n+                c.JsonApiErr(500, \"Failed to get plugin settings\", err)\n+                return\n+        }\n+        if !found {\n+                c.JsonApiErr(404, \"Plugin not found\", nil)\n+                return\n+        }\n+        hs.pluginClient.CallResource(pCtx, c, web.Params(c.Req)[\"*\"])\n }\n \n func (hs *HTTPServer) GetPluginErrorsList(_ *models.ReqContext) response.Response {\n-\treturn response.JSON(200, hs.pluginErrorResolver.PluginErrors())\n+        return response.JSON(200, hs.pluginErrorResolver.PluginErrors())\n }\n \n func (hs *HTTPServer) InstallPlugin(c *models.ReqContext) response.Response {\n-\tdto := dtos.InstallPluginCommand{}\n-\tif err := web.Bind(c.Req, &dto); err != nil {\n-\t\treturn response.Error(http.StatusBadRequest, \"bad request data\", err)\n-\t}\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\terr := hs.pluginStore.Add(c.Req.Context(), pluginID, dto.Version, plugins.AddOpts{})\n-\tif err != nil {\n-\t\tvar dupeErr plugins.DuplicateError\n-\t\tif errors.As(err, &dupeErr) {\n-\t\t\treturn response.Error(http.StatusConflict, \"Plugin already installed\", err)\n-\t\t}\n-\t\tvar versionUnsupportedErr installer.ErrVersionUnsupported\n-\t\tif errors.As(err, &versionUnsupportedErr) {\n-\t\t\treturn response.Error(http.StatusConflict, \"Plugin version not supported\", err)\n-\t\t}\n-\t\tvar versionNotFoundErr installer.ErrVersionNotFound\n-\t\tif errors.As(err, &versionNotFoundErr) {\n-\t\t\treturn response.Error(http.StatusNotFound, \"Plugin version not found\", err)\n-\t\t}\n-\t\tvar clientError installer.Response4xxError\n-\t\tif errors.As(err, &clientError) {\n-\t\t\treturn response.Error(clientError.StatusCode, clientError.Message, err)\n-\t\t}\n-\t\tif errors.Is(err, plugins.ErrInstallCorePlugin) {\n-\t\t\treturn response.Error(http.StatusForbidden, \"Cannot install or change a Core plugin\", err)\n-\t\t}\n-\n-\t\treturn response.Error(http.StatusInternalServerError, \"Failed to install plugin\", err)\n-\t}\n-\n-\treturn response.JSON(http.StatusOK, []byte{})\n+        dto := dtos.InstallPluginCommand{}\n+        if err := web.Bind(c.Req, &dto); err != nil {\n+                return response.Error(http.StatusBadRequest, \"bad request data\", err)\n+        }\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        err := hs.pluginStore.Add(c.Req.Context(), pluginID, dto.Version, plugins.AddOpts{})\n+        if err != nil {\n+                var dupeErr plugins.DuplicateError\n+                if errors.As(err, &dupeErr) {\n+                        return response.Error(http.StatusConflict, \"Plugin already installed\", err)\n+                }\n+                var versionUnsupportedErr installer.ErrVersionUnsupported\n+                if errors.As(err, &versionUnsupportedErr) {\n+                        return response.Error(http.StatusConflict, \"Plugin version not supported\", err)\n+                }\n+                var versionNotFoundErr installer.ErrVersionNotFound\n+                if errors.As(err, &versionNotFoundErr) {\n+                        return response.Error(http.StatusNotFound, \"Plugin version not found\", err)\n+                }\n+                var clientError installer.Response4xxError\n+                if errors.As(err, &clientError) {\n+                        return response.Error(clientError.StatusCode, clientError.Message, err)\n+                }\n+                if errors.Is(err, plugins.ErrInstallCorePlugin) {\n+                        return response.Error(http.StatusForbidden, \"Cannot install or change a Core plugin\", err)\n+                }\n+\n+                return response.Error(http.StatusInternalServerError, \"Failed to install plugin\", err)\n+        }\n+\n+        return response.JSON(http.StatusOK, []byte{})\n }\n \n func (hs *HTTPServer) UninstallPlugin(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\terr := hs.pluginStore.Remove(c.Req.Context(), pluginID)\n-\tif err != nil {\n-\t\tif errors.Is(err, plugins.ErrPluginNotInstalled) {\n-\t\t\treturn response.Error(http.StatusNotFound, \"Plugin not installed\", err)\n-\t\t}\n-\t\tif errors.Is(err, plugins.ErrUninstallCorePlugin) {\n-\t\t\treturn response.Error(http.StatusForbidden, \"Cannot uninstall a Core plugin\", err)\n-\t\t}\n-\t\tif errors.Is(err, plugins.ErrUninstallOutsideOfPluginDir) {\n-\t\t\treturn response.Error(http.StatusForbidden, \"Cannot uninstall a plugin outside of the plugins directory\", err)\n-\t\t}\n-\n-\t\treturn response.Error(http.StatusInternalServerError, \"Failed to uninstall plugin\", err)\n-\t}\n-\treturn response.JSON(http.StatusOK, []byte{})\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        err := hs.pluginStore.Remove(c.Req.Context(), pluginID)\n+        if err != nil {\n+                if errors.Is(err, plugins.ErrPluginNotInstalled) {\n+                        return response.Error(http.StatusNotFound, \"Plugin not installed\", err)\n+                }\n+                if errors.Is(err, plugins.ErrUninstallCorePlugin) {\n+                        return response.Error(http.StatusForbidden, \"Cannot uninstall a Core plugin\", err)\n+                }\n+                if errors.Is(err, plugins.ErrUninstallOutsideOfPluginDir) {\n+                        return response.Error(http.StatusForbidden, \"Cannot uninstall a plugin outside of the plugins directory\", err)\n+                }\n+\n+                return response.Error(http.StatusInternalServerError, \"Failed to uninstall plugin\", err)\n+        }\n+        return response.JSON(http.StatusOK, []byte{})\n }\n \n func translatePluginRequestErrorToAPIError(err error) response.Response {\n-\tif errors.Is(err, backendplugin.ErrPluginNotRegistered) {\n-\t\treturn response.Error(404, \"Plugin not found\", err)\n-\t}\n+        if errors.Is(err, backendplugin.ErrPluginNotRegistered) {\n+                return response.Error(404, \"Plugin not found\", err)\n+        }\n \n-\tif errors.Is(err, backendplugin.ErrMethodNotImplemented) {\n-\t\treturn response.Error(404, \"Not found\", err)\n-\t}\n+        if errors.Is(err, backendplugin.ErrMethodNotImplemented) {\n+                return response.Error(404, \"Not found\", err)\n+        }\n \n-\tif errors.Is(err, backendplugin.ErrHealthCheckFailed) {\n-\t\treturn response.Error(500, \"Plugin health check failed\", err)\n-\t}\n+        if errors.Is(err, backendplugin.ErrHealthCheckFailed) {\n+                return response.Error(500, \"Plugin health check failed\", err)\n+        }\n \n-\tif errors.Is(err, backendplugin.ErrPluginUnavailable) {\n-\t\treturn response.Error(503, \"Plugin unavailable\", err)\n-\t}\n+        if errors.Is(err, backendplugin.ErrPluginUnavailable) {\n+                return response.Error(503, \"Plugin unavailable\", err)\n+        }\n \n-\treturn response.Error(500, \"Plugin request failed\", err)\n+        return response.Error(500, \"Plugin request failed\", err)\n }\n \n func (hs *HTTPServer) pluginMarkdown(ctx context.Context, pluginId string, name string) ([]byte, error) {\n-\tplugin, exists := hs.pluginStore.Plugin(ctx, pluginId)\n-\tif !exists {\n-\t\treturn nil, plugins.NotFoundError{PluginID: pluginId}\n-\t}\n-\n-\t// nolint:gosec\n-\t// We can ignore the gosec G304 warning on this one because `plugin.PluginDir` is based\n-\t// on plugin the folder structure on disk and not user input.\n-\tpath := filepath.Join(plugin.PluginDir, fmt.Sprintf(\"%s.md\", strings.ToUpper(name)))\n-\texists, err := fs.Exists(path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif !exists {\n-\t\tpath = filepath.Join(plugin.PluginDir, fmt.Sprintf(\"%s.md\", strings.ToLower(name)))\n-\t}\n-\n-\texists, err = fs.Exists(path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif !exists {\n-\t\treturn make([]byte, 0), nil\n-\t}\n-\n-\t// nolint:gosec\n-\t// We can ignore the gosec G304 warning on this one because `plugin.PluginDir` is based\n-\t// on plugin the folder structure on disk and not user input.\n-\tdata, err := ioutil.ReadFile(path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn data, nil\n+        plugin, exists := hs.pluginStore.Plugin(ctx, pluginId)\n+        if !exists {\n+                return nil, plugins.NotFoundError{PluginID: pluginId}\n+        }\n+\n+        // nolint:gosec\n+        // We can ignore the gosec G304 warning on this one because `plugin.PluginDir` is based\n+        // on plugin the folder structure on disk and not user input.\n+        path := filepath.Join(plugin.PluginDir, fmt.Sprintf(\"%s.md\", strings.ToUpper(name)))\n+        exists, err := fs.Exists(path)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if !exists {\n+                path = filepath.Join(plugin.PluginDir, fmt.Sprintf(\"%s.md\", strings.ToLower(name)))\n+        }\n+\n+        exists, err = fs.Exists(path)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if !exists {\n+                return make([]byte, 0), nil\n+        }\n+\n+        // nolint:gosec\n+        // We can ignore the gosec G304 warning on this one because `plugin.PluginDir` is based\n+        // on plugin the folder structure on disk and not user input.\n+        data, err := ioutil.ReadFile(path)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return data, nil\n }\n"}
{"cve":"CVE-2020-11053:0708", "fix_patch": "diff --git a/oauthproxy.go b/oauthproxy.go\nindex 587215fd..413f4a58 100644\n--- a/oauthproxy.go\n+++ b/oauthproxy.go\n@@ -1,541 +1,541 @@\n package main\n \n import (\n-\t\"context\"\n-\t\"crypto/tls\"\n-\tb64 \"encoding/base64\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"html/template\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/http/httputil\"\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/coreos/go-oidc\"\n-\t\"github.com/mbland/hmacauth\"\n-\tsessionsapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/cookies\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/encryption\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/logger\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/providers\"\n-\t\"github.com/yhat/wsutil\"\n+        \"context\"\n+        \"crypto/tls\"\n+        b64 \"encoding/base64\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"html/template\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/http/httputil\"\n+        \"net/url\"\n+        \"regexp\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/coreos/go-oidc\"\n+        \"github.com/mbland/hmacauth\"\n+        sessionsapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/cookies\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/encryption\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/logger\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/providers\"\n+        \"github.com/yhat/wsutil\"\n )\n \n const (\n-\t// SignatureHeader is the name of the request header containing the GAP Signature\n-\t// Part of hmacauth\n-\tSignatureHeader = \"GAP-Signature\"\n+        // SignatureHeader is the name of the request header containing the GAP Signature\n+        // Part of hmacauth\n+        SignatureHeader = \"GAP-Signature\"\n \n-\thttpScheme  = \"http\"\n-\thttpsScheme = \"https\"\n+        httpScheme  = \"http\"\n+        httpsScheme = \"https\"\n \n-\tapplicationJSON = \"application/json\"\n+        applicationJSON = \"application/json\"\n )\n \n // SignatureHeaders contains the headers to be signed by the hmac algorithm\n // Part of hmacauth\n var SignatureHeaders = []string{\n-\t\"Content-Length\",\n-\t\"Content-Md5\",\n-\t\"Content-Type\",\n-\t\"Date\",\n-\t\"Authorization\",\n-\t\"X-Forwarded-User\",\n-\t\"X-Forwarded-Email\",\n-\t\"X-Forwarded-Preferred-User\",\n-\t\"X-Forwarded-Access-Token\",\n-\t\"Cookie\",\n-\t\"Gap-Auth\",\n+        \"Content-Length\",\n+        \"Content-Md5\",\n+        \"Content-Type\",\n+        \"Date\",\n+        \"Authorization\",\n+        \"X-Forwarded-User\",\n+        \"X-Forwarded-Email\",\n+        \"X-Forwarded-Preferred-User\",\n+        \"X-Forwarded-Access-Token\",\n+        \"Cookie\",\n+        \"Gap-Auth\",\n }\n \n var (\n-\t// ErrNeedsLogin means the user should be redirected to the login page\n-\tErrNeedsLogin = errors.New(\"redirect to login page\")\n+        // ErrNeedsLogin means the user should be redirected to the login page\n+        ErrNeedsLogin = errors.New(\"redirect to login page\")\n )\n \n // OAuthProxy is the main authentication proxy\n type OAuthProxy struct {\n-\tCookieSeed     string\n-\tCookieName     string\n-\tCSRFCookieName string\n-\tCookieDomains  []string\n-\tCookiePath     string\n-\tCookieSecure   bool\n-\tCookieHTTPOnly bool\n-\tCookieExpire   time.Duration\n-\tCookieRefresh  time.Duration\n-\tCookieSameSite string\n-\tValidator      func(string) bool\n-\n-\tRobotsPath        string\n-\tPingPath          string\n-\tSignInPath        string\n-\tSignOutPath       string\n-\tOAuthStartPath    string\n-\tOAuthCallbackPath string\n-\tAuthOnlyPath      string\n-\tUserInfoPath      string\n-\n-\tredirectURL          *url.URL // the url to receive requests at\n-\twhitelistDomains     []string\n-\tprovider             providers.Provider\n-\tproviderNameOverride string\n-\tsessionStore         sessionsapi.SessionStore\n-\tProxyPrefix          string\n-\tSignInMessage        string\n-\tHtpasswdFile         *HtpasswdFile\n-\tDisplayHtpasswdForm  bool\n-\tserveMux             http.Handler\n-\tSetXAuthRequest      bool\n-\tPassBasicAuth        bool\n-\tSetBasicAuth         bool\n-\tSkipProviderButton   bool\n-\tPassUserHeaders      bool\n-\tBasicAuthPassword    string\n-\tPassAccessToken      bool\n-\tSetAuthorization     bool\n-\tPassAuthorization    bool\n-\tPreferEmailToUser    bool\n-\tskipAuthRegex        []string\n-\tskipAuthPreflight    bool\n-\tskipJwtBearerTokens  bool\n-\tjwtBearerVerifiers   []*oidc.IDTokenVerifier\n-\tcompiledRegex        []*regexp.Regexp\n-\ttemplates            *template.Template\n-\tBanner               string\n-\tFooter               string\n+        CookieSeed     string\n+        CookieName     string\n+        CSRFCookieName string\n+        CookieDomains  []string\n+        CookiePath     string\n+        CookieSecure   bool\n+        CookieHTTPOnly bool\n+        CookieExpire   time.Duration\n+        CookieRefresh  time.Duration\n+        CookieSameSite string\n+        Validator      func(string) bool\n+\n+        RobotsPath        string\n+        PingPath          string\n+        SignInPath        string\n+        SignOutPath       string\n+        OAuthStartPath    string\n+        OAuthCallbackPath string\n+        AuthOnlyPath      string\n+        UserInfoPath      string\n+\n+        redirectURL          *url.URL // the url to receive requests at\n+        whitelistDomains     []string\n+        provider             providers.Provider\n+        providerNameOverride string\n+        sessionStore         sessionsapi.SessionStore\n+        ProxyPrefix          string\n+        SignInMessage        string\n+        HtpasswdFile         *HtpasswdFile\n+        DisplayHtpasswdForm  bool\n+        serveMux             http.Handler\n+        SetXAuthRequest      bool\n+        PassBasicAuth        bool\n+        SetBasicAuth         bool\n+        SkipProviderButton   bool\n+        PassUserHeaders      bool\n+        BasicAuthPassword    string\n+        PassAccessToken      bool\n+        SetAuthorization     bool\n+        PassAuthorization    bool\n+        PreferEmailToUser    bool\n+        skipAuthRegex        []string\n+        skipAuthPreflight    bool\n+        skipJwtBearerTokens  bool\n+        jwtBearerVerifiers   []*oidc.IDTokenVerifier\n+        compiledRegex        []*regexp.Regexp\n+        templates            *template.Template\n+        Banner               string\n+        Footer               string\n }\n \n // UpstreamProxy represents an upstream server to proxy to\n type UpstreamProxy struct {\n-\tupstream  string\n-\thandler   http.Handler\n-\twsHandler http.Handler\n-\tauth      hmacauth.HmacAuth\n+        upstream  string\n+        handler   http.Handler\n+        wsHandler http.Handler\n+        auth      hmacauth.HmacAuth\n }\n \n // ServeHTTP proxies requests to the upstream provider while signing the\n // request headers\n func (u *UpstreamProxy) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tw.Header().Set(\"GAP-Upstream-Address\", u.upstream)\n-\tif u.auth != nil {\n-\t\tr.Header.Set(\"GAP-Auth\", w.Header().Get(\"GAP-Auth\"))\n-\t\tu.auth.SignRequest(r)\n-\t}\n-\tif u.wsHandler != nil && strings.EqualFold(r.Header.Get(\"Connection\"), \"upgrade\") && r.Header.Get(\"Upgrade\") == \"websocket\" {\n-\t\tu.wsHandler.ServeHTTP(w, r)\n-\t} else {\n-\t\tu.handler.ServeHTTP(w, r)\n-\t}\n+        w.Header().Set(\"GAP-Upstream-Address\", u.upstream)\n+        if u.auth != nil {\n+                r.Header.Set(\"GAP-Auth\", w.Header().Get(\"GAP-Auth\"))\n+                u.auth.SignRequest(r)\n+        }\n+        if u.wsHandler != nil && strings.EqualFold(r.Header.Get(\"Connection\"), \"upgrade\") && r.Header.Get(\"Upgrade\") == \"websocket\" {\n+                u.wsHandler.ServeHTTP(w, r)\n+        } else {\n+                u.handler.ServeHTTP(w, r)\n+        }\n \n }\n \n // NewReverseProxy creates a new reverse proxy for proxying requests to upstream\n // servers\n func NewReverseProxy(target *url.URL, opts *Options) (proxy *httputil.ReverseProxy) {\n-\tproxy = httputil.NewSingleHostReverseProxy(target)\n-\tproxy.FlushInterval = opts.FlushInterval\n-\tif opts.SSLUpstreamInsecureSkipVerify {\n-\t\tproxy.Transport = &http.Transport{\n-\t\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n-\t\t}\n-\t}\n-\treturn proxy\n+        proxy = httputil.NewSingleHostReverseProxy(target)\n+        proxy.FlushInterval = opts.FlushInterval\n+        if opts.SSLUpstreamInsecureSkipVerify {\n+                proxy.Transport = &http.Transport{\n+                        TLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n+                }\n+        }\n+        return proxy\n }\n \n func setProxyUpstreamHostHeader(proxy *httputil.ReverseProxy, target *url.URL) {\n-\tdirector := proxy.Director\n-\tproxy.Director = func(req *http.Request) {\n-\t\tdirector(req)\n-\t\t// use RequestURI so that we aren't unescaping encoded slashes in the request path\n-\t\treq.Host = target.Host\n-\t\treq.URL.Opaque = req.RequestURI\n-\t\treq.URL.RawQuery = \"\"\n-\t}\n+        director := proxy.Director\n+        proxy.Director = func(req *http.Request) {\n+                director(req)\n+                // use RequestURI so that we aren't unescaping encoded slashes in the request path\n+                req.Host = target.Host\n+                req.URL.Opaque = req.RequestURI\n+                req.URL.RawQuery = \"\"\n+        }\n }\n \n func setProxyDirector(proxy *httputil.ReverseProxy) {\n-\tdirector := proxy.Director\n-\tproxy.Director = func(req *http.Request) {\n-\t\tdirector(req)\n-\t\t// use RequestURI so that we aren't unescaping encoded slashes in the request path\n-\t\treq.URL.Opaque = req.RequestURI\n-\t\treq.URL.RawQuery = \"\"\n-\t}\n+        director := proxy.Director\n+        proxy.Director = func(req *http.Request) {\n+                director(req)\n+                // use RequestURI so that we aren't unescaping encoded slashes in the request path\n+                req.URL.Opaque = req.RequestURI\n+                req.URL.RawQuery = \"\"\n+        }\n }\n \n // NewFileServer creates a http.Handler to serve files from the filesystem\n func NewFileServer(path string, filesystemPath string) (proxy http.Handler) {\n-\treturn http.StripPrefix(path, http.FileServer(http.Dir(filesystemPath)))\n+        return http.StripPrefix(path, http.FileServer(http.Dir(filesystemPath)))\n }\n \n // NewWebSocketOrRestReverseProxy creates a reverse proxy for REST or websocket based on url\n func NewWebSocketOrRestReverseProxy(u *url.URL, opts *Options, auth hmacauth.HmacAuth) http.Handler {\n-\tu.Path = \"\"\n-\tproxy := NewReverseProxy(u, opts)\n-\tif !opts.PassHostHeader {\n-\t\tsetProxyUpstreamHostHeader(proxy, u)\n-\t} else {\n-\t\tsetProxyDirector(proxy)\n-\t}\n-\n-\t// this should give us a wss:// scheme if the url is https:// based.\n-\tvar wsProxy *wsutil.ReverseProxy\n-\tif opts.ProxyWebSockets {\n-\t\twsScheme := \"ws\" + strings.TrimPrefix(u.Scheme, \"http\")\n-\t\twsURL := &url.URL{Scheme: wsScheme, Host: u.Host}\n-\t\twsProxy = wsutil.NewSingleHostReverseProxy(wsURL)\n-\t\tif opts.SSLUpstreamInsecureSkipVerify {\n-\t\t\twsProxy.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n-\t\t}\n-\t}\n-\treturn &UpstreamProxy{\n-\t\tupstream:  u.Host,\n-\t\thandler:   proxy,\n-\t\twsHandler: wsProxy,\n-\t\tauth:      auth,\n-\t}\n+        u.Path = \"\"\n+        proxy := NewReverseProxy(u, opts)\n+        if !opts.PassHostHeader {\n+                setProxyUpstreamHostHeader(proxy, u)\n+        } else {\n+                setProxyDirector(proxy)\n+        }\n+\n+        // this should give us a wss:// scheme if the url is https:// based.\n+        var wsProxy *wsutil.ReverseProxy\n+        if opts.ProxyWebSockets {\n+                wsScheme := \"ws\" + strings.TrimPrefix(u.Scheme, \"http\")\n+                wsURL := &url.URL{Scheme: wsScheme, Host: u.Host}\n+                wsProxy = wsutil.NewSingleHostReverseProxy(wsURL)\n+                if opts.SSLUpstreamInsecureSkipVerify {\n+                        wsProxy.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n+                }\n+        }\n+        return &UpstreamProxy{\n+                upstream:  u.Host,\n+                handler:   proxy,\n+                wsHandler: wsProxy,\n+                auth:      auth,\n+        }\n }\n \n // NewOAuthProxy creates a new instance of OAuthProxy from the options provided\n func NewOAuthProxy(opts *Options, validator func(string) bool) *OAuthProxy {\n-\tserveMux := http.NewServeMux()\n-\tvar auth hmacauth.HmacAuth\n-\tif sigData := opts.signatureData; sigData != nil {\n-\t\tauth = hmacauth.NewHmacAuth(sigData.hash, []byte(sigData.key),\n-\t\t\tSignatureHeader, SignatureHeaders)\n-\t}\n-\tfor _, u := range opts.proxyURLs {\n-\t\tpath := u.Path\n-\t\thost := u.Host\n-\t\tswitch u.Scheme {\n-\t\tcase httpScheme, httpsScheme:\n-\t\t\tlogger.Printf(\"mapping path %q => upstream %q\", path, u)\n-\t\t\tproxy := NewWebSocketOrRestReverseProxy(u, opts, auth)\n-\t\t\tserveMux.Handle(path, proxy)\n-\t\tcase \"static\":\n-\t\t\tresponseCode, err := strconv.Atoi(host)\n-\t\t\tif err != nil {\n-\t\t\t\tlogger.Printf(\"unable to convert %q to int, use default \\\"200\\\"\", host)\n-\t\t\t\tresponseCode = 200\n-\t\t\t}\n-\n-\t\t\tserveMux.HandleFunc(path, func(rw http.ResponseWriter, req *http.Request) {\n-\t\t\t\trw.WriteHeader(responseCode)\n-\t\t\t\tfmt.Fprintf(rw, \"Authenticated\")\n-\t\t\t})\n-\t\tcase \"file\":\n-\t\t\tif u.Fragment != \"\" {\n-\t\t\t\tpath = u.Fragment\n-\t\t\t}\n-\t\t\tlogger.Printf(\"mapping path %q => file system %q\", path, u.Path)\n-\t\t\tproxy := NewFileServer(path, u.Path)\n-\t\t\tuProxy := UpstreamProxy{\n-\t\t\t\tupstream:  path,\n-\t\t\t\thandler:   proxy,\n-\t\t\t\twsHandler: nil,\n-\t\t\t\tauth:      nil,\n-\t\t\t}\n-\t\t\tserveMux.Handle(path, &uProxy)\n-\t\tdefault:\n-\t\t\tpanic(fmt.Sprintf(\"unknown upstream protocol %s\", u.Scheme))\n-\t\t}\n-\t}\n-\tfor _, u := range opts.compiledRegex {\n-\t\tlogger.Printf(\"compiled skip-auth-regex => %q\", u)\n-\t}\n-\n-\tif opts.SkipJwtBearerTokens {\n-\t\tlogger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n-\t\tfor _, issuer := range opts.ExtraJwtIssuers {\n-\t\t\tlogger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n-\t\t}\n-\t}\n-\tredirectURL := opts.redirectURL\n-\tif redirectURL.Path == \"\" {\n-\t\tredirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n-\t}\n-\n-\tlogger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.provider.Data().ProviderName, opts.ClientID)\n-\trefresh := \"disabled\"\n-\tif opts.Cookie.Refresh != time.Duration(0) {\n-\t\trefresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n-\t}\n-\n-\tlogger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n-\n-\treturn &OAuthProxy{\n-\t\tCookieName:     opts.Cookie.Name,\n-\t\tCSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n-\t\tCookieSeed:     opts.Cookie.Secret,\n-\t\tCookieDomains:  opts.Cookie.Domains,\n-\t\tCookiePath:     opts.Cookie.Path,\n-\t\tCookieSecure:   opts.Cookie.Secure,\n-\t\tCookieHTTPOnly: opts.Cookie.HTTPOnly,\n-\t\tCookieExpire:   opts.Cookie.Expire,\n-\t\tCookieRefresh:  opts.Cookie.Refresh,\n-\t\tCookieSameSite: opts.Cookie.SameSite,\n-\t\tValidator:      validator,\n-\n-\t\tRobotsPath:        \"/robots.txt\",\n-\t\tPingPath:          opts.PingPath,\n-\t\tSignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n-\t\tSignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n-\t\tOAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n-\t\tOAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n-\t\tAuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n-\t\tUserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n-\n-\t\tProxyPrefix:          opts.ProxyPrefix,\n-\t\tprovider:             opts.provider,\n-\t\tproviderNameOverride: opts.ProviderName,\n-\t\tsessionStore:         opts.sessionStore,\n-\t\tserveMux:             serveMux,\n-\t\tredirectURL:          redirectURL,\n-\t\twhitelistDomains:     opts.WhitelistDomains,\n-\t\tskipAuthRegex:        opts.SkipAuthRegex,\n-\t\tskipAuthPreflight:    opts.SkipAuthPreflight,\n-\t\tskipJwtBearerTokens:  opts.SkipJwtBearerTokens,\n-\t\tjwtBearerVerifiers:   opts.jwtBearerVerifiers,\n-\t\tcompiledRegex:        opts.compiledRegex,\n-\t\tSetXAuthRequest:      opts.SetXAuthRequest,\n-\t\tPassBasicAuth:        opts.PassBasicAuth,\n-\t\tSetBasicAuth:         opts.SetBasicAuth,\n-\t\tPassUserHeaders:      opts.PassUserHeaders,\n-\t\tBasicAuthPassword:    opts.BasicAuthPassword,\n-\t\tPassAccessToken:      opts.PassAccessToken,\n-\t\tSetAuthorization:     opts.SetAuthorization,\n-\t\tPassAuthorization:    opts.PassAuthorization,\n-\t\tPreferEmailToUser:    opts.PreferEmailToUser,\n-\t\tSkipProviderButton:   opts.SkipProviderButton,\n-\t\ttemplates:            loadTemplates(opts.CustomTemplatesDir),\n-\t\tBanner:               opts.Banner,\n-\t\tFooter:               opts.Footer,\n-\t}\n+        serveMux := http.NewServeMux()\n+        var auth hmacauth.HmacAuth\n+        if sigData := opts.signatureData; sigData != nil {\n+                auth = hmacauth.NewHmacAuth(sigData.hash, []byte(sigData.key),\n+                        SignatureHeader, SignatureHeaders)\n+        }\n+        for _, u := range opts.proxyURLs {\n+                path := u.Path\n+                host := u.Host\n+                switch u.Scheme {\n+                case httpScheme, httpsScheme:\n+                        logger.Printf(\"mapping path %q => upstream %q\", path, u)\n+                        proxy := NewWebSocketOrRestReverseProxy(u, opts, auth)\n+                        serveMux.Handle(path, proxy)\n+                case \"static\":\n+                        responseCode, err := strconv.Atoi(host)\n+                        if err != nil {\n+                                logger.Printf(\"unable to convert %q to int, use default \\\"200\\\"\", host)\n+                                responseCode = 200\n+                        }\n+\n+                        serveMux.HandleFunc(path, func(rw http.ResponseWriter, req *http.Request) {\n+                                rw.WriteHeader(responseCode)\n+                                fmt.Fprintf(rw, \"Authenticated\")\n+                        })\n+                case \"file\":\n+                        if u.Fragment != \"\" {\n+                                path = u.Fragment\n+                        }\n+                        logger.Printf(\"mapping path %q => file system %q\", path, u.Path)\n+                        proxy := NewFileServer(path, u.Path)\n+                        uProxy := UpstreamProxy{\n+                                upstream:  path,\n+                                handler:   proxy,\n+                                wsHandler: nil,\n+                                auth:      nil,\n+                        }\n+                        serveMux.Handle(path, &uProxy)\n+                default:\n+                        panic(fmt.Sprintf(\"unknown upstream protocol %s\", u.Scheme))\n+                }\n+        }\n+        for _, u := range opts.compiledRegex {\n+                logger.Printf(\"compiled skip-auth-regex => %q\", u)\n+        }\n+\n+        if opts.SkipJwtBearerTokens {\n+                logger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n+                for _, issuer := range opts.ExtraJwtIssuers {\n+                        logger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n+                }\n+        }\n+        redirectURL := opts.redirectURL\n+        if redirectURL.Path == \"\" {\n+                redirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n+        }\n+\n+        logger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.provider.Data().ProviderName, opts.ClientID)\n+        refresh := \"disabled\"\n+        if opts.Cookie.Refresh != time.Duration(0) {\n+                refresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n+        }\n+\n+        logger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n+\n+        return &OAuthProxy{\n+                CookieName:     opts.Cookie.Name,\n+                CSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n+                CookieSeed:     opts.Cookie.Secret,\n+                CookieDomains:  opts.Cookie.Domains,\n+                CookiePath:     opts.Cookie.Path,\n+                CookieSecure:   opts.Cookie.Secure,\n+                CookieHTTPOnly: opts.Cookie.HTTPOnly,\n+                CookieExpire:   opts.Cookie.Expire,\n+                CookieRefresh:  opts.Cookie.Refresh,\n+                CookieSameSite: opts.Cookie.SameSite,\n+                Validator:      validator,\n+\n+                RobotsPath:        \"/robots.txt\",\n+                PingPath:          opts.PingPath,\n+                SignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n+                SignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n+                OAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n+                OAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n+                AuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n+                UserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n+\n+                ProxyPrefix:          opts.ProxyPrefix,\n+                provider:             opts.provider,\n+                providerNameOverride: opts.ProviderName,\n+                sessionStore:         opts.sessionStore,\n+                serveMux:             serveMux,\n+                redirectURL:          redirectURL,\n+                whitelistDomains:     opts.WhitelistDomains,\n+                skipAuthRegex:        opts.SkipAuthRegex,\n+                skipAuthPreflight:    opts.SkipAuthPreflight,\n+                skipJwtBearerTokens:  opts.SkipJwtBearerTokens,\n+                jwtBearerVerifiers:   opts.jwtBearerVerifiers,\n+                compiledRegex:        opts.compiledRegex,\n+                SetXAuthRequest:      opts.SetXAuthRequest,\n+                PassBasicAuth:        opts.PassBasicAuth,\n+                SetBasicAuth:         opts.SetBasicAuth,\n+                PassUserHeaders:      opts.PassUserHeaders,\n+                BasicAuthPassword:    opts.BasicAuthPassword,\n+                PassAccessToken:      opts.PassAccessToken,\n+                SetAuthorization:     opts.SetAuthorization,\n+                PassAuthorization:    opts.PassAuthorization,\n+                PreferEmailToUser:    opts.PreferEmailToUser,\n+                SkipProviderButton:   opts.SkipProviderButton,\n+                templates:            loadTemplates(opts.CustomTemplatesDir),\n+                Banner:               opts.Banner,\n+                Footer:               opts.Footer,\n+        }\n }\n \n // GetRedirectURI returns the redirectURL that the upstream OAuth Provider will\n // redirect clients to once authenticated\n func (p *OAuthProxy) GetRedirectURI(host string) string {\n-\t// default to the request Host if not set\n-\tif p.redirectURL.Host != \"\" {\n-\t\treturn p.redirectURL.String()\n-\t}\n-\tu := *p.redirectURL\n-\tif u.Scheme == \"\" {\n-\t\tif p.CookieSecure {\n-\t\t\tu.Scheme = httpsScheme\n-\t\t} else {\n-\t\t\tu.Scheme = httpScheme\n-\t\t}\n-\t}\n-\tu.Host = host\n-\treturn u.String()\n+        // default to the request Host if not set\n+        if p.redirectURL.Host != \"\" {\n+                return p.redirectURL.String()\n+        }\n+        u := *p.redirectURL\n+        if u.Scheme == \"\" {\n+                if p.CookieSecure {\n+                        u.Scheme = httpsScheme\n+                } else {\n+                        u.Scheme = httpScheme\n+                }\n+        }\n+        u.Host = host\n+        return u.String()\n }\n \n func (p *OAuthProxy) displayCustomLoginForm() bool {\n-\treturn p.HtpasswdFile != nil && p.DisplayHtpasswdForm\n+        return p.HtpasswdFile != nil && p.DisplayHtpasswdForm\n }\n \n func (p *OAuthProxy) redeemCode(host, code string) (s *sessionsapi.SessionState, err error) {\n-\tif code == \"\" {\n-\t\treturn nil, errors.New(\"missing code\")\n-\t}\n-\tredirectURI := p.GetRedirectURI(host)\n-\ts, err = p.provider.Redeem(redirectURI, code)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tif s.Email == \"\" {\n-\t\ts.Email, err = p.provider.GetEmailAddress(s)\n-\t}\n-\n-\tif s.PreferredUsername == \"\" {\n-\t\ts.PreferredUsername, err = p.provider.GetPreferredUsername(s)\n-\t\tif err != nil && err.Error() == \"not implemented\" {\n-\t\t\terr = nil\n-\t\t}\n-\t}\n-\n-\tif s.User == \"\" {\n-\t\ts.User, err = p.provider.GetUserName(s)\n-\t\tif err != nil && err.Error() == \"not implemented\" {\n-\t\t\terr = nil\n-\t\t}\n-\t}\n-\treturn\n+        if code == \"\" {\n+                return nil, errors.New(\"missing code\")\n+        }\n+        redirectURI := p.GetRedirectURI(host)\n+        s, err = p.provider.Redeem(redirectURI, code)\n+        if err != nil {\n+                return\n+        }\n+\n+        if s.Email == \"\" {\n+                s.Email, err = p.provider.GetEmailAddress(s)\n+        }\n+\n+        if s.PreferredUsername == \"\" {\n+                s.PreferredUsername, err = p.provider.GetPreferredUsername(s)\n+                if err != nil && err.Error() == \"not implemented\" {\n+                        err = nil\n+                }\n+        }\n+\n+        if s.User == \"\" {\n+                s.User, err = p.provider.GetUserName(s)\n+                if err != nil && err.Error() == \"not implemented\" {\n+                        err = nil\n+                }\n+        }\n+        return\n }\n \n // MakeCSRFCookie creates a cookie for CSRF\n func (p *OAuthProxy) MakeCSRFCookie(req *http.Request, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\treturn p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n+        return p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n }\n \n func (p *OAuthProxy) makeCookie(req *http.Request, name string, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\tcookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n-\n-\tif cookieDomain != \"\" {\n-\t\tdomain := cookies.GetRequestHost(req)\n-\t\tif h, _, err := net.SplitHostPort(domain); err == nil {\n-\t\t\tdomain = h\n-\t\t}\n-\t\tif !strings.HasSuffix(domain, cookieDomain) {\n-\t\t\tlogger.Printf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n-\t\t}\n-\t}\n-\n-\treturn &http.Cookie{\n-\t\tName:     name,\n-\t\tValue:    value,\n-\t\tPath:     p.CookiePath,\n-\t\tDomain:   cookieDomain,\n-\t\tHttpOnly: p.CookieHTTPOnly,\n-\t\tSecure:   p.CookieSecure,\n-\t\tExpires:  now.Add(expiration),\n-\t\tSameSite: cookies.ParseSameSite(p.CookieSameSite),\n-\t}\n+        cookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n+\n+        if cookieDomain != \"\" {\n+                domain := cookies.GetRequestHost(req)\n+                if h, _, err := net.SplitHostPort(domain); err == nil {\n+                        domain = h\n+                }\n+                if !strings.HasSuffix(domain, cookieDomain) {\n+                        logger.Printf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n+                }\n+        }\n+\n+        return &http.Cookie{\n+                Name:     name,\n+                Value:    value,\n+                Path:     p.CookiePath,\n+                Domain:   cookieDomain,\n+                HttpOnly: p.CookieHTTPOnly,\n+                Secure:   p.CookieSecure,\n+                Expires:  now.Add(expiration),\n+                SameSite: cookies.ParseSameSite(p.CookieSameSite),\n+        }\n }\n \n // ClearCSRFCookie creates a cookie to unset the CSRF cookie stored in the user's\n // session\n func (p *OAuthProxy) ClearCSRFCookie(rw http.ResponseWriter, req *http.Request) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n }\n \n // SetCSRFCookie adds a CSRF cookie to the response\n func (p *OAuthProxy) SetCSRFCookie(rw http.ResponseWriter, req *http.Request, val string) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n }\n \n // ClearSessionCookie creates a cookie to unset the user's authentication cookie\n // stored in the user's session\n func (p *OAuthProxy) ClearSessionCookie(rw http.ResponseWriter, req *http.Request) error {\n-\treturn p.sessionStore.Clear(rw, req)\n+        return p.sessionStore.Clear(rw, req)\n }\n \n // LoadCookiedSession reads the user's authentication details from the request\n func (p *OAuthProxy) LoadCookiedSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\treturn p.sessionStore.Load(req)\n+        return p.sessionStore.Load(req)\n }\n \n // SaveSession creates a new session cookie value and sets this on the response\n func (p *OAuthProxy) SaveSession(rw http.ResponseWriter, req *http.Request, s *sessionsapi.SessionState) error {\n-\treturn p.sessionStore.Save(rw, req, s)\n+        return p.sessionStore.Save(rw, req, s)\n }\n \n // RobotsTxt disallows scraping pages from the OAuthProxy\n func (p *OAuthProxy) RobotsTxt(rw http.ResponseWriter) {\n-\trw.WriteHeader(http.StatusOK)\n-\tfmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n+        rw.WriteHeader(http.StatusOK)\n+        fmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n }\n \n // PingPage responds 200 OK to requests\n func (p *OAuthProxy) PingPage(rw http.ResponseWriter) {\n-\trw.WriteHeader(http.StatusOK)\n-\tfmt.Fprintf(rw, \"OK\")\n+        rw.WriteHeader(http.StatusOK)\n+        fmt.Fprintf(rw, \"OK\")\n }\n \n // ErrorPage writes an error response\n func (p *OAuthProxy) ErrorPage(rw http.ResponseWriter, code int, title string, message string) {\n-\trw.WriteHeader(code)\n-\tt := struct {\n-\t\tTitle       string\n-\t\tMessage     string\n-\t\tProxyPrefix string\n-\t}{\n-\t\tTitle:       fmt.Sprintf(\"%d %s\", code, title),\n-\t\tMessage:     message,\n-\t\tProxyPrefix: p.ProxyPrefix,\n-\t}\n-\tp.templates.ExecuteTemplate(rw, \"error.html\", t)\n+        rw.WriteHeader(code)\n+        t := struct {\n+                Title       string\n+                Message     string\n+                ProxyPrefix string\n+        }{\n+                Title:       fmt.Sprintf(\"%d %s\", code, title),\n+                Message:     message,\n+                ProxyPrefix: p.ProxyPrefix,\n+        }\n+        p.templates.ExecuteTemplate(rw, \"error.html\", t)\n }\n \n // SignInPage writes the sing in template to the response\n func (p *OAuthProxy) SignInPage(rw http.ResponseWriter, req *http.Request, code int) {\n-\tprepareNoCache(rw)\n-\tp.ClearSessionCookie(rw, req)\n-\trw.WriteHeader(code)\n-\n-\tredirectURL, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tif redirectURL == p.SignInPath {\n-\t\tredirectURL = \"/\"\n-\t}\n-\n-\tt := struct {\n-\t\tProviderName  string\n-\t\tSignInMessage template.HTML\n-\t\tCustomLogin   bool\n-\t\tRedirect      string\n-\t\tVersion       string\n-\t\tProxyPrefix   string\n-\t\tFooter        template.HTML\n-\t}{\n-\t\tProviderName:  p.provider.Data().ProviderName,\n-\t\tSignInMessage: template.HTML(p.SignInMessage),\n-\t\tCustomLogin:   p.displayCustomLoginForm(),\n-\t\tRedirect:      redirectURL,\n-\t\tVersion:       VERSION,\n-\t\tProxyPrefix:   p.ProxyPrefix,\n-\t\tFooter:        template.HTML(p.Footer),\n-\t}\n-\tif p.providerNameOverride != \"\" {\n-\t\tt.ProviderName = p.providerNameOverride\n-\t}\n-\tp.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n+        prepareNoCache(rw)\n+        p.ClearSessionCookie(rw, req)\n+        rw.WriteHeader(code)\n+\n+        redirectURL, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+\n+        if redirectURL == p.SignInPath {\n+                redirectURL = \"/\"\n+        }\n+\n+        t := struct {\n+                ProviderName  string\n+                SignInMessage template.HTML\n+                CustomLogin   bool\n+                Redirect      string\n+                Version       string\n+                ProxyPrefix   string\n+                Footer        template.HTML\n+        }{\n+                ProviderName:  p.provider.Data().ProviderName,\n+                SignInMessage: template.HTML(p.SignInMessage),\n+                CustomLogin:   p.displayCustomLoginForm(),\n+                Redirect:      redirectURL,\n+                Version:       VERSION,\n+                ProxyPrefix:   p.ProxyPrefix,\n+                Footer:        template.HTML(p.Footer),\n+        }\n+        if p.providerNameOverride != \"\" {\n+                t.ProviderName = p.providerNameOverride\n+        }\n+        p.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n }\n \n // ManualSignIn handles basic auth logins to the proxy\n func (p *OAuthProxy) ManualSignIn(rw http.ResponseWriter, req *http.Request) (string, bool) {\n-\tif req.Method != \"POST\" || p.HtpasswdFile == nil {\n-\t\treturn \"\", false\n-\t}\n-\tuser := req.FormValue(\"username\")\n-\tpasswd := req.FormValue(\"password\")\n-\tif user == \"\" {\n-\t\treturn \"\", false\n-\t}\n-\t// check auth\n-\tif p.HtpasswdFile.Validate(user, passwd) {\n-\t\tlogger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n-\t\treturn user, true\n-\t}\n-\tlogger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n-\treturn \"\", false\n+        if req.Method != \"POST\" || p.HtpasswdFile == nil {\n+                return \"\", false\n+        }\n+        user := req.FormValue(\"username\")\n+        passwd := req.FormValue(\"password\")\n+        if user == \"\" {\n+                return \"\", false\n+        }\n+        // check auth\n+        if p.HtpasswdFile.Validate(user, passwd) {\n+                logger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n+                return user, true\n+        }\n+        logger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n+        return \"\", false\n }\n \n // GetRedirect reads the query parameter to get the URL to redirect clients to\n // once authenticated with the OAuthProxy\n func (p *OAuthProxy) GetRedirect(req *http.Request) (redirect string, err error) {\n-\terr = req.ParseForm()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tredirect = req.Header.Get(\"X-Auth-Request-Redirect\")\n-\tif req.Form.Get(\"rd\") != \"\" {\n-\t\tredirect = req.Form.Get(\"rd\")\n-\t}\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = req.URL.Path\n-\t\tif strings.HasPrefix(redirect, p.ProxyPrefix) {\n-\t\t\tredirect = \"/\"\n-\t\t}\n-\t}\n-\n-\treturn\n+        err = req.ParseForm()\n+        if err != nil {\n+                return\n+        }\n+\n+        redirect = req.Header.Get(\"X-Auth-Request-Redirect\")\n+        if req.Form.Get(\"rd\") != \"\" {\n+                redirect = req.Form.Get(\"rd\")\n+        }\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = req.URL.Path\n+                if strings.HasPrefix(redirect, p.ProxyPrefix) {\n+                        redirect = \"/\"\n+                }\n+        }\n+\n+        return\n }\n \n // splitHostPort separates host and port. If the port is not valid, it returns\n@@ -543,332 +543,338 @@ func (p *OAuthProxy) GetRedirect(req *http.Request) (redirect string, err error)\n // Unlike net.SplitHostPort, but per RFC 3986, it requires ports to be numeric.\n // *** taken from net/url, modified validOptionalPort() to accept \":*\"\n func splitHostPort(hostport string) (host, port string) {\n-\thost = hostport\n+        host = hostport\n \n-\tcolon := strings.LastIndexByte(host, ':')\n-\tif colon != -1 && validOptionalPort(host[colon:]) {\n-\t\thost, port = host[:colon], host[colon+1:]\n-\t}\n+        colon := strings.LastIndexByte(host, ':')\n+        if colon != -1 && validOptionalPort(host[colon:]) {\n+                host, port = host[:colon], host[colon+1:]\n+        }\n \n-\tif strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n-\t\thost = host[1 : len(host)-1]\n-\t}\n+        if strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n+                host = host[1 : len(host)-1]\n+        }\n \n-\treturn\n+        return\n }\n \n // validOptionalPort reports whether port is either an empty string\n // or matches /^:\\d*$/\n // *** taken from net/url, modified to accept \":*\"\n func validOptionalPort(port string) bool {\n-\tif port == \"\" || port == \":*\" {\n-\t\treturn true\n-\t}\n-\tif port[0] != ':' {\n-\t\treturn false\n-\t}\n-\tfor _, b := range port[1:] {\n-\t\tif b < '0' || b > '9' {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\treturn true\n+        if port == \"\" || port == \":*\" {\n+                return true\n+        }\n+        if port[0] != ':' {\n+                return false\n+        }\n+        for _, b := range port[1:] {\n+                if b < '0' || b > '9' {\n+                        return false\n+                }\n+        }\n+        return true\n }\n \n // IsValidRedirect checks whether the redirect URL is whitelisted\n func (p *OAuthProxy) IsValidRedirect(redirect string) bool {\n-\tswitch {\n-\tcase strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !strings.HasPrefix(redirect, \"/\\\\\"):\n-\t\treturn true\n-\tcase strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n-\t\tredirectURL, err := url.Parse(redirect)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n-\t\t\treturn false\n-\t\t}\n-\t\tredirectHostname := redirectURL.Hostname()\n-\n-\t\tfor _, domain := range p.whitelistDomains {\n-\t\t\tdomainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n-\t\t\tif domainHostname == \"\" {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tif (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n-\t\t\t\t// the domain names match, now validate the ports\n-\t\t\t\t// if the whitelisted domain's port is '*', allow all ports\n-\t\t\t\t// if the whitelisted domain contains a specific port, only allow that port\n-\t\t\t\t// if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n-\t\t\t\tredirectPort := redirectURL.Port()\n-\t\t\t\tif (domainPort == \"*\") ||\n-\t\t\t\t\t(domainPort == redirectPort) ||\n-\t\t\t\t\t(domainPort == \"\" && redirectPort == \"\") {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n-\t\treturn false\n-\tdefault:\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n-\t\treturn false\n-\t}\n+        switch {\n+        case strings.HasPrefix(redirect, \"/\"):\n+                if len(redirect) > 1 && (redirect[1] == '/' || redirect[1] == '\\\\') {\n+                        return false\n+                }\n+                if strings.ContainsAny(redirect, \"\\t \\r\\n\") || strings.Contains(redirect, \"\\v\") {\n+                        return false\n+                }\n+                return true\n+        case strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n+                redirectURL, err := url.Parse(redirect)\n+                if err != nil {\n+                        logger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n+                        return false\n+                }\n+                redirectHostname := redirectURL.Hostname()\n+\n+                for _, domain := range p.whitelistDomains {\n+                        domainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n+                        if domainHostname == \"\" {\n+                                continue\n+                        }\n+\n+                        if (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n+                                // the domain names match, now validate the ports\n+                                // if the whitelisted domain's port is '*', allow all ports\n+                                // if the whitelisted domain contains a specific port, only allow that port\n+                                // if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n+                                redirectPort := redirectURL.Port()\n+                                if (domainPort == \"*\") ||\n+                                        (domainPort == redirectPort) ||\n+                                        (domainPort == \"\" && redirectPort == \"\") {\n+                                        return true\n+                                }\n+                        }\n+                }\n+\n+                logger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n+                return false\n+        default:\n+                logger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n+                return false\n+        }\n }\n \n // IsWhitelistedRequest is used to check if auth should be skipped for this request\n func (p *OAuthProxy) IsWhitelistedRequest(req *http.Request) bool {\n-\tisPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n-\treturn isPreflightRequestAllowed || p.IsWhitelistedPath(req.URL.Path)\n+        isPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n+        return isPreflightRequestAllowed || p.IsWhitelistedPath(req.URL.Path)\n }\n \n // IsWhitelistedPath is used to check if the request path is allowed without auth\n func (p *OAuthProxy) IsWhitelistedPath(path string) bool {\n-\tfor _, u := range p.compiledRegex {\n-\t\tif u.MatchString(path) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, u := range p.compiledRegex {\n+                if u.MatchString(path) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func getRemoteAddr(req *http.Request) (s string) {\n-\ts = req.RemoteAddr\n-\tif req.Header.Get(\"X-Real-IP\") != \"\" {\n-\t\ts += fmt.Sprintf(\" (%q)\", req.Header.Get(\"X-Real-IP\"))\n-\t}\n-\treturn\n+        s = req.RemoteAddr\n+        if req.Header.Get(\"X-Real-IP\") != \"\" {\n+                s += fmt.Sprintf(\" (%q)\", req.Header.Get(\"X-Real-IP\"))\n+        }\n+        return\n }\n \n // See https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching?hl=en\n var noCacheHeaders = map[string]string{\n-\t\"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n-\t\"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n-\t\"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n+        \"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n+        \"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n+        \"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n }\n \n // prepareNoCache prepares headers for preventing browser caching.\n func prepareNoCache(w http.ResponseWriter) {\n-\t// Set NoCache headers\n-\tfor k, v := range noCacheHeaders {\n-\t\tw.Header().Set(k, v)\n-\t}\n+        // Set NoCache headers\n+        for k, v := range noCacheHeaders {\n+                w.Header().Set(k, v)\n+        }\n }\n \n func (p *OAuthProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n-\tif strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n-\t\tprepareNoCache(rw)\n-\t}\n-\n-\tswitch path := req.URL.Path; {\n-\tcase path == p.RobotsPath:\n-\t\tp.RobotsTxt(rw)\n-\tcase path == p.PingPath:\n-\t\tp.PingPage(rw)\n-\tcase p.IsWhitelistedRequest(req):\n-\t\tp.serveMux.ServeHTTP(rw, req)\n-\tcase path == p.SignInPath:\n-\t\tp.SignIn(rw, req)\n-\tcase path == p.SignOutPath:\n-\t\tp.SignOut(rw, req)\n-\tcase path == p.OAuthStartPath:\n-\t\tp.OAuthStart(rw, req)\n-\tcase path == p.OAuthCallbackPath:\n-\t\tp.OAuthCallback(rw, req)\n-\tcase path == p.AuthOnlyPath:\n-\t\tp.AuthenticateOnly(rw, req)\n-\tcase path == p.UserInfoPath:\n-\t\tp.UserInfo(rw, req)\n-\tdefault:\n-\t\tp.Proxy(rw, req)\n-\t}\n+        if strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n+                prepareNoCache(rw)\n+        }\n+\n+        switch path := req.URL.Path; {\n+        case path == p.RobotsPath:\n+                p.RobotsTxt(rw)\n+        case path == p.PingPath:\n+                p.PingPage(rw)\n+        case p.IsWhitelistedRequest(req):\n+                p.serveMux.ServeHTTP(rw, req)\n+        case path == p.SignInPath:\n+                p.SignIn(rw, req)\n+        case path == p.SignOutPath:\n+                p.SignOut(rw, req)\n+        case path == p.OAuthStartPath:\n+                p.OAuthStart(rw, req)\n+        case path == p.OAuthCallbackPath:\n+                p.OAuthCallback(rw, req)\n+        case path == p.AuthOnlyPath:\n+                p.AuthenticateOnly(rw, req)\n+        case path == p.UserInfoPath:\n+                p.UserInfo(rw, req)\n+        default:\n+                p.Proxy(rw, req)\n+        }\n }\n \n // SignIn serves a page prompting users to sign in\n func (p *OAuthProxy) SignIn(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tuser, ok := p.ManualSignIn(rw, req)\n-\tif ok {\n-\t\tsession := &sessionsapi.SessionState{User: user}\n-\t\tp.SaveSession(rw, req, session)\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusOK)\n-\t\t}\n-\t}\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+\n+        user, ok := p.ManualSignIn(rw, req)\n+        if ok {\n+                session := &sessionsapi.SessionState{User: user}\n+                p.SaveSession(rw, req, session)\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusOK)\n+                }\n+        }\n }\n \n //UserInfo endpoint outputs session email and preferred username in JSON format\n func (p *OAuthProxy) UserInfo(rw http.ResponseWriter, req *http.Request) {\n \n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\tuserInfo := struct {\n-\t\tEmail             string `json:\"email\"`\n-\t\tPreferredUsername string `json:\"preferredUsername,omitempty\"`\n-\t}{\n-\t\tEmail:             session.Email,\n-\t\tPreferredUsername: session.PreferredUsername,\n-\t}\n-\trw.Header().Set(\"Content-Type\", \"application/json\")\n-\trw.WriteHeader(http.StatusOK)\n-\tjson.NewEncoder(rw).Encode(userInfo)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+                return\n+        }\n+        userInfo := struct {\n+                Email             string `json:\"email\"`\n+                PreferredUsername string `json:\"preferredUsername,omitempty\"`\n+        }{\n+                Email:             session.Email,\n+                PreferredUsername: session.PreferredUsername,\n+        }\n+        rw.Header().Set(\"Content-Type\", \"application/json\")\n+        rw.WriteHeader(http.StatusOK)\n+        json.NewEncoder(rw).Encode(userInfo)\n }\n \n // SignOut sends a response to clear the authentication cookie\n func (p *OAuthProxy) SignOut(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearSessionCookie(rw, req)\n-\thttp.Redirect(rw, req, redirect, http.StatusFound)\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        p.ClearSessionCookie(rw, req)\n+        http.Redirect(rw, req, redirect, http.StatusFound)\n }\n \n // OAuthStart starts the OAuth2 authentication flow\n func (p *OAuthProxy) OAuthStart(rw http.ResponseWriter, req *http.Request) {\n-\tprepareNoCache(rw)\n-\tnonce, err := encryption.Nonce()\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining nonce: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.SetCSRFCookie(rw, req, nonce)\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tredirectURI := p.GetRedirectURI(req.Host)\n-\thttp.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n+        prepareNoCache(rw)\n+        nonce, err := encryption.Nonce()\n+        if err != nil {\n+                logger.Printf(\"Error obtaining nonce: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        p.SetCSRFCookie(rw, req, nonce)\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        redirectURI := p.GetRedirectURI(req.Host)\n+        http.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n }\n \n // OAuthCallback is the OAuth2 authentication flow callback that finishes the\n // OAuth2 authentication flow\n func (p *OAuthProxy) OAuthCallback(rw http.ResponseWriter, req *http.Request) {\n-\tremoteAddr := getRemoteAddr(req)\n-\n-\t// finish the oauth cycle\n-\terr := req.ParseForm()\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 callback: %s\" + err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\terrorString := req.Form.Get(\"error\")\n-\tif errorString != \"\" {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 callback: %s \", errorString)\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", errorString)\n-\t\treturn\n-\t}\n-\n-\tsession, err := p.redeemCode(req.Host, req.Form.Get(\"code\"))\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error redeeming code during OAuth2 callback: %s \", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n-\t\treturn\n-\t}\n-\n-\ts := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n-\tif len(s) != 2 {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 state: invalid length\")\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Invalid State\")\n-\t\treturn\n-\t}\n-\tnonce := s[0]\n-\tredirect := s[1]\n-\tc, err := req.Cookie(p.CSRFCookieName)\n-\tif err != nil {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable too obtain CSRF cookie\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearCSRFCookie(rw, req)\n-\tif c.Value != nonce {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: csrf token mismatch, potential attack\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", \"csrf failed\")\n-\t\treturn\n-\t}\n-\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = \"/\"\n-\t}\n-\n-\t// set cookie, or deny\n-\tif p.Validator(session.Email) && p.provider.ValidateGroup(session.Email) {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n-\t\terr := p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"%s %s\", remoteAddr, err)\n-\t\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n-\t\t\treturn\n-\t\t}\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", \"Invalid Account\")\n-\t}\n+        remoteAddr := getRemoteAddr(req)\n+\n+        // finish the oauth cycle\n+        err := req.ParseForm()\n+        if err != nil {\n+                logger.Printf(\"Error while parsing OAuth2 callback: %s\" + err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        errorString := req.Form.Get(\"error\")\n+        if errorString != \"\" {\n+                logger.Printf(\"Error while parsing OAuth2 callback: %s \", errorString)\n+                p.ErrorPage(rw, 403, \"Permission Denied\", errorString)\n+                return\n+        }\n+\n+        session, err := p.redeemCode(req.Host, req.Form.Get(\"code\"))\n+        if err != nil {\n+                logger.Printf(\"Error redeeming code during OAuth2 callback: %s \", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n+                return\n+        }\n+\n+        s := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n+        if len(s) != 2 {\n+                logger.Printf(\"Error while parsing OAuth2 state: invalid length\")\n+                p.ErrorPage(rw, 500, \"Internal Error\", \"Invalid State\")\n+                return\n+        }\n+        nonce := s[0]\n+        redirect := s[1]\n+        c, err := req.Cookie(p.CSRFCookieName)\n+        if err != nil {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable too obtain CSRF cookie\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", err.Error())\n+                return\n+        }\n+        p.ClearCSRFCookie(rw, req)\n+        if c.Value != nonce {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: csrf token mismatch, potential attack\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", \"csrf failed\")\n+                return\n+        }\n+\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = \"/\"\n+        }\n+\n+        // set cookie, or deny\n+        if p.Validator(session.Email) && p.provider.ValidateGroup(session.Email) {\n+                logger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n+                err := p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.Printf(\"%s %s\", remoteAddr, err)\n+                        p.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n+                        return\n+                }\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", \"Invalid Account\")\n+        }\n }\n \n // AuthenticateOnly checks whether the user is currently logged in\n func (p *OAuthProxy) AuthenticateOnly(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, \"unauthorized request\", http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\t// we are authenticated\n-\tp.addHeadersForProxying(rw, req, session)\n-\trw.WriteHeader(http.StatusAccepted)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, \"unauthorized request\", http.StatusUnauthorized)\n+                return\n+        }\n+\n+        // we are authenticated\n+        p.addHeadersForProxying(rw, req, session)\n+        rw.WriteHeader(http.StatusAccepted)\n }\n \n // Proxy proxies the user request if the user is authenticated else it prompts\n // them to authenticate\n func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tswitch err {\n-\tcase nil:\n-\t\t// we are authenticated\n-\t\tp.addHeadersForProxying(rw, req, session)\n-\t\tp.serveMux.ServeHTTP(rw, req)\n-\n-\tcase ErrNeedsLogin:\n-\t\t// we need to send the user to a login screen\n-\t\tif isAjax(req) {\n-\t\t\t// no point redirecting an AJAX request\n-\t\t\tp.ErrorJSON(rw, http.StatusUnauthorized)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusForbidden)\n-\t\t}\n-\n-\tdefault:\n-\t\t// unknown error\n-\t\tlogger.Printf(\"Unexpected internal error: %s\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError,\n-\t\t\t\"Internal Error\", \"Internal Error\")\n-\t}\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        switch err {\n+        case nil:\n+                // we are authenticated\n+                p.addHeadersForProxying(rw, req, session)\n+                p.serveMux.ServeHTTP(rw, req)\n+\n+        case ErrNeedsLogin:\n+                // we need to send the user to a login screen\n+                if isAjax(req) {\n+                        // no point redirecting an AJAX request\n+                        p.ErrorJSON(rw, http.StatusUnauthorized)\n+                        return\n+                }\n+\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusForbidden)\n+                }\n+\n+        default:\n+                // unknown error\n+                logger.Printf(\"Unexpected internal error: %s\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError,\n+                        \"Internal Error\", \"Internal Error\")\n+        }\n \n }\n \n@@ -876,303 +882,303 @@ func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n // Returns nil, ErrNeedsLogin if user needs to login.\n // Set-Cookie headers may be set on the response as a side-effect of calling this method.\n func (p *OAuthProxy) getAuthenticatedSession(rw http.ResponseWriter, req *http.Request) (*sessionsapi.SessionState, error) {\n-\tvar session *sessionsapi.SessionState\n-\tvar err error\n-\tvar saveSession, clearSession, revalidated bool\n-\n-\tif p.skipJwtBearerTokens && req.Header.Get(\"Authorization\") != \"\" {\n-\t\tsession, err = p.GetJwtSession(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error retrieving session from token in Authorization header: %s\", err)\n-\t\t}\n-\t\tif session != nil {\n-\t\t\tsaveSession = false\n-\t\t}\n-\t}\n-\n-\tremoteAddr := getRemoteAddr(req)\n-\tif session == nil {\n-\t\tsession, err = p.LoadCookiedSession(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error loading cookied session: %s\", err)\n-\t\t}\n-\n-\t\tif session != nil {\n-\t\t\tif session.Age() > p.CookieRefresh && p.CookieRefresh != time.Duration(0) {\n-\t\t\t\tlogger.Printf(\"Refreshing %s old session cookie for %s (refresh after %s)\", session.Age(), session, p.CookieRefresh)\n-\t\t\t\tsaveSession = true\n-\t\t\t}\n-\n-\t\t\tif ok, err := p.provider.RefreshSessionIfNeeded(session); err != nil {\n-\t\t\t\tlogger.Printf(\"%s removing session. error refreshing access token %s %s\", remoteAddr, err, session)\n-\t\t\t\tclearSession = true\n-\t\t\t\tsession = nil\n-\t\t\t} else if ok {\n-\t\t\t\tsaveSession = true\n-\t\t\t\trevalidated = true\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif session != nil && session.IsExpired() {\n-\t\tlogger.Printf(\"Removing session: token expired %s\", session)\n-\t\tsession = nil\n-\t\tsaveSession = false\n-\t\tclearSession = true\n-\t}\n-\n-\tif saveSession && !revalidated && session != nil && session.AccessToken != \"\" {\n-\t\tif !p.provider.ValidateSessionState(session) {\n-\t\t\tlogger.Printf(\"Removing session: error validating %s\", session)\n-\t\t\tsaveSession = false\n-\t\t\tsession = nil\n-\t\t\tclearSession = true\n-\t\t}\n-\t}\n-\n-\tif session != nil && session.Email != \"\" && !p.Validator(session.Email) {\n-\t\tlogger.Printf(session.Email, req, logger.AuthFailure, \"Invalid authentication via session: removing session %s\", session)\n-\t\tsession = nil\n-\t\tsaveSession = false\n-\t\tclearSession = true\n-\t}\n-\n-\tif saveSession && session != nil {\n-\t\terr = p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.PrintAuthf(session.Email, req, logger.AuthError, \"Save session error %s\", err)\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\tif clearSession {\n-\t\tp.ClearSessionCookie(rw, req)\n-\t}\n-\n-\tif session == nil {\n-\t\tsession, err = p.CheckBasicAuth(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error during basic auth validation: %s\", err)\n-\t\t}\n-\t}\n-\n-\tif session == nil {\n-\t\treturn nil, ErrNeedsLogin\n-\t}\n-\n-\treturn session, nil\n+        var session *sessionsapi.SessionState\n+        var err error\n+        var saveSession, clearSession, revalidated bool\n+\n+        if p.skipJwtBearerTokens && req.Header.Get(\"Authorization\") != \"\" {\n+                session, err = p.GetJwtSession(req)\n+                if err != nil {\n+                        logger.Printf(\"Error retrieving session from token in Authorization header: %s\", err)\n+                }\n+                if session != nil {\n+                        saveSession = false\n+                }\n+        }\n+\n+        remoteAddr := getRemoteAddr(req)\n+        if session == nil {\n+                session, err = p.LoadCookiedSession(req)\n+                if err != nil {\n+                        logger.Printf(\"Error loading cookied session: %s\", err)\n+                }\n+\n+                if session != nil {\n+                        if session.Age() > p.CookieRefresh && p.CookieRefresh != time.Duration(0) {\n+                                logger.Printf(\"Refreshing %s old session cookie for %s (refresh after %s)\", session.Age(), session, p.CookieRefresh)\n+                                saveSession = true\n+                        }\n+\n+                        if ok, err := p.provider.RefreshSessionIfNeeded(session); err != nil {\n+                                logger.Printf(\"%s removing session. error refreshing access token %s %s\", remoteAddr, err, session)\n+                                clearSession = true\n+                                session = nil\n+                        } else if ok {\n+                                saveSession = true\n+                                revalidated = true\n+                        }\n+                }\n+        }\n+\n+        if session != nil && session.IsExpired() {\n+                logger.Printf(\"Removing session: token expired %s\", session)\n+                session = nil\n+                saveSession = false\n+                clearSession = true\n+        }\n+\n+        if saveSession && !revalidated && session != nil && session.AccessToken != \"\" {\n+                if !p.provider.ValidateSessionState(session) {\n+                        logger.Printf(\"Removing session: error validating %s\", session)\n+                        saveSession = false\n+                        session = nil\n+                        clearSession = true\n+                }\n+        }\n+\n+        if session != nil && session.Email != \"\" && !p.Validator(session.Email) {\n+                logger.Printf(session.Email, req, logger.AuthFailure, \"Invalid authentication via session: removing session %s\", session)\n+                session = nil\n+                saveSession = false\n+                clearSession = true\n+        }\n+\n+        if saveSession && session != nil {\n+                err = p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.PrintAuthf(session.Email, req, logger.AuthError, \"Save session error %s\", err)\n+                        return nil, err\n+                }\n+        }\n+\n+        if clearSession {\n+                p.ClearSessionCookie(rw, req)\n+        }\n+\n+        if session == nil {\n+                session, err = p.CheckBasicAuth(req)\n+                if err != nil {\n+                        logger.Printf(\"Error during basic auth validation: %s\", err)\n+                }\n+        }\n+\n+        if session == nil {\n+                return nil, ErrNeedsLogin\n+        }\n+\n+        return session, nil\n }\n \n // addHeadersForProxying adds the appropriate headers the request / response for proxying\n func (p *OAuthProxy) addHeadersForProxying(rw http.ResponseWriter, req *http.Request, session *sessionsapi.SessionState) {\n-\tif p.PassBasicAuth {\n-\t\tif p.PreferEmailToUser && session.Email != \"\" {\n-\t\t\treq.SetBasicAuth(session.Email, p.BasicAuthPassword)\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.Email}\n-\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t} else {\n-\t\t\treq.SetBasicAuth(session.User, p.BasicAuthPassword)\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.User}\n-\t\t\tif session.Email != \"\" {\n-\t\t\t\treq.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n-\t\t\t} else {\n-\t\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t\t}\n-\t\t}\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Preferred-Username\")\n-\t\t}\n-\t}\n-\n-\tif p.PassUserHeaders {\n-\t\tif p.PreferEmailToUser && session.Email != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.Email}\n-\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t} else {\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.User}\n-\t\t\tif session.Email != \"\" {\n-\t\t\t\treq.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n-\t\t\t} else {\n-\t\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t\t}\n-\t\t}\n-\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Preferred-Username\")\n-\t\t}\n-\t}\n-\n-\tif p.SetXAuthRequest {\n-\t\trw.Header().Set(\"X-Auth-Request-User\", session.User)\n-\t\tif session.Email != \"\" {\n-\t\t\trw.Header().Set(\"X-Auth-Request-Email\", session.Email)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"X-Auth-Request-Email\")\n-\t\t}\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\trw.Header().Set(\"X-Auth-Request-Preferred-Username\", session.PreferredUsername)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"X-Auth-Request-Preferred-Username\")\n-\t\t}\n-\n-\t\tif p.PassAccessToken {\n-\t\t\tif session.AccessToken != \"\" {\n-\t\t\t\trw.Header().Set(\"X-Auth-Request-Access-Token\", session.AccessToken)\n-\t\t\t} else {\n-\t\t\t\trw.Header().Del(\"X-Auth-Request-Access-Token\")\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif p.PassAccessToken {\n-\t\tif session.AccessToken != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Access-Token\"] = []string{session.AccessToken}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Access-Token\")\n-\t\t}\n-\t}\n-\n-\tif p.PassAuthorization {\n-\t\tif session.IDToken != \"\" {\n-\t\t\treq.Header[\"Authorization\"] = []string{fmt.Sprintf(\"Bearer %s\", session.IDToken)}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"Authorization\")\n-\t\t}\n-\t}\n-\tif p.SetBasicAuth {\n-\t\tif session.User != \"\" {\n-\t\t\tauthVal := b64.StdEncoding.EncodeToString([]byte(session.User + \":\" + p.BasicAuthPassword))\n-\t\t\trw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"Authorization\")\n-\t\t}\n-\t}\n-\tif p.SetAuthorization {\n-\t\tif session.IDToken != \"\" {\n-\t\t\trw.Header().Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", session.IDToken))\n-\t\t} else {\n-\t\t\trw.Header().Del(\"Authorization\")\n-\t\t}\n-\t}\n-\n-\tif session.Email == \"\" {\n-\t\trw.Header().Set(\"GAP-Auth\", session.User)\n-\t} else {\n-\t\trw.Header().Set(\"GAP-Auth\", session.Email)\n-\t}\n+        if p.PassBasicAuth {\n+                if p.PreferEmailToUser && session.Email != \"\" {\n+                        req.SetBasicAuth(session.Email, p.BasicAuthPassword)\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.Email}\n+                        req.Header.Del(\"X-Forwarded-Email\")\n+                } else {\n+                        req.SetBasicAuth(session.User, p.BasicAuthPassword)\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.User}\n+                        if session.Email != \"\" {\n+                                req.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n+                        } else {\n+                                req.Header.Del(\"X-Forwarded-Email\")\n+                        }\n+                }\n+                if session.PreferredUsername != \"\" {\n+                        req.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Preferred-Username\")\n+                }\n+        }\n+\n+        if p.PassUserHeaders {\n+                if p.PreferEmailToUser && session.Email != \"\" {\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.Email}\n+                        req.Header.Del(\"X-Forwarded-Email\")\n+                } else {\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.User}\n+                        if session.Email != \"\" {\n+                                req.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n+                        } else {\n+                                req.Header.Del(\"X-Forwarded-Email\")\n+                        }\n+                }\n+\n+                if session.PreferredUsername != \"\" {\n+                        req.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Preferred-Username\")\n+                }\n+        }\n+\n+        if p.SetXAuthRequest {\n+                rw.Header().Set(\"X-Auth-Request-User\", session.User)\n+                if session.Email != \"\" {\n+                        rw.Header().Set(\"X-Auth-Request-Email\", session.Email)\n+                } else {\n+                        rw.Header().Del(\"X-Auth-Request-Email\")\n+                }\n+                if session.PreferredUsername != \"\" {\n+                        rw.Header().Set(\"X-Auth-Request-Preferred-Username\", session.PreferredUsername)\n+                } else {\n+                        rw.Header().Del(\"X-Auth-Request-Preferred-Username\")\n+                }\n+\n+                if p.PassAccessToken {\n+                        if session.AccessToken != \"\" {\n+                                rw.Header().Set(\"X-Auth-Request-Access-Token\", session.AccessToken)\n+                        } else {\n+                                rw.Header().Del(\"X-Auth-Request-Access-Token\")\n+                        }\n+                }\n+        }\n+\n+        if p.PassAccessToken {\n+                if session.AccessToken != \"\" {\n+                        req.Header[\"X-Forwarded-Access-Token\"] = []string{session.AccessToken}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Access-Token\")\n+                }\n+        }\n+\n+        if p.PassAuthorization {\n+                if session.IDToken != \"\" {\n+                        req.Header[\"Authorization\"] = []string{fmt.Sprintf(\"Bearer %s\", session.IDToken)}\n+                } else {\n+                        req.Header.Del(\"Authorization\")\n+                }\n+        }\n+        if p.SetBasicAuth {\n+                if session.User != \"\" {\n+                        authVal := b64.StdEncoding.EncodeToString([]byte(session.User + \":\" + p.BasicAuthPassword))\n+                        rw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n+                } else {\n+                        rw.Header().Del(\"Authorization\")\n+                }\n+        }\n+        if p.SetAuthorization {\n+                if session.IDToken != \"\" {\n+                        rw.Header().Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", session.IDToken))\n+                } else {\n+                        rw.Header().Del(\"Authorization\")\n+                }\n+        }\n+\n+        if session.Email == \"\" {\n+                rw.Header().Set(\"GAP-Auth\", session.User)\n+        } else {\n+                rw.Header().Set(\"GAP-Auth\", session.Email)\n+        }\n }\n \n // CheckBasicAuth checks the requests Authorization header for basic auth\n // credentials and authenticates these against the proxies HtpasswdFile\n func (p *OAuthProxy) CheckBasicAuth(req *http.Request) (*sessionsapi.SessionState, error) {\n-\tif p.HtpasswdFile == nil {\n-\t\treturn nil, nil\n-\t}\n-\tauth := req.Header.Get(\"Authorization\")\n-\tif auth == \"\" {\n-\t\treturn nil, nil\n-\t}\n-\ts := strings.SplitN(auth, \" \", 2)\n-\tif len(s) != 2 || s[0] != \"Basic\" {\n-\t\treturn nil, fmt.Errorf(\"invalid Authorization header %s\", req.Header.Get(\"Authorization\"))\n-\t}\n-\tb, err := b64.StdEncoding.DecodeString(s[1])\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tpair := strings.SplitN(string(b), \":\", 2)\n-\tif len(pair) != 2 {\n-\t\treturn nil, fmt.Errorf(\"invalid format %s\", b)\n-\t}\n-\tif p.HtpasswdFile.Validate(pair[0], pair[1]) {\n-\t\tlogger.PrintAuthf(pair[0], req, logger.AuthSuccess, \"Authenticated via basic auth and HTpasswd File\")\n-\t\treturn &sessionsapi.SessionState{User: pair[0]}, nil\n-\t}\n-\tlogger.PrintAuthf(pair[0], req, logger.AuthFailure, \"Invalid authentication via basic auth: not in Htpasswd File\")\n-\treturn nil, nil\n+        if p.HtpasswdFile == nil {\n+                return nil, nil\n+        }\n+        auth := req.Header.Get(\"Authorization\")\n+        if auth == \"\" {\n+                return nil, nil\n+        }\n+        s := strings.SplitN(auth, \" \", 2)\n+        if len(s) != 2 || s[0] != \"Basic\" {\n+                return nil, fmt.Errorf(\"invalid Authorization header %s\", req.Header.Get(\"Authorization\"))\n+        }\n+        b, err := b64.StdEncoding.DecodeString(s[1])\n+        if err != nil {\n+                return nil, err\n+        }\n+        pair := strings.SplitN(string(b), \":\", 2)\n+        if len(pair) != 2 {\n+                return nil, fmt.Errorf(\"invalid format %s\", b)\n+        }\n+        if p.HtpasswdFile.Validate(pair[0], pair[1]) {\n+                logger.PrintAuthf(pair[0], req, logger.AuthSuccess, \"Authenticated via basic auth and HTpasswd File\")\n+                return &sessionsapi.SessionState{User: pair[0]}, nil\n+        }\n+        logger.PrintAuthf(pair[0], req, logger.AuthFailure, \"Invalid authentication via basic auth: not in Htpasswd File\")\n+        return nil, nil\n }\n \n // isAjax checks if a request is an ajax request\n func isAjax(req *http.Request) bool {\n-\tacceptValues := req.Header.Values(\"Accept\")\n-\tconst ajaxReq = applicationJSON\n-\tfor _, v := range acceptValues {\n-\t\tif v == ajaxReq {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        acceptValues := req.Header.Values(\"Accept\")\n+        const ajaxReq = applicationJSON\n+        for _, v := range acceptValues {\n+                if v == ajaxReq {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // ErrorJSON returns the error code with an application/json mime type\n func (p *OAuthProxy) ErrorJSON(rw http.ResponseWriter, code int) {\n-\trw.Header().Set(\"Content-Type\", applicationJSON)\n-\trw.WriteHeader(code)\n+        rw.Header().Set(\"Content-Type\", applicationJSON)\n+        rw.WriteHeader(code)\n }\n \n // GetJwtSession loads a session based on a JWT token in the authorization header.\n // (see the config options skip-jwt-bearer-tokens and extra-jwt-issuers)\n func (p *OAuthProxy) GetJwtSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\trawBearerToken, err := p.findBearerToken(req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tctx := context.Background()\n-\tfor _, verifier := range p.jwtBearerVerifiers {\n-\t\tbearerToken, err := verifier.Verify(ctx, rawBearerToken)\n-\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"failed to verify bearer token: %v\", err)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\treturn p.provider.CreateSessionStateFromBearerToken(rawBearerToken, bearerToken)\n-\t}\n-\treturn nil, fmt.Errorf(\"unable to verify jwt token %s\", req.Header.Get(\"Authorization\"))\n+        rawBearerToken, err := p.findBearerToken(req)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        ctx := context.Background()\n+        for _, verifier := range p.jwtBearerVerifiers {\n+                bearerToken, err := verifier.Verify(ctx, rawBearerToken)\n+\n+                if err != nil {\n+                        logger.Printf(\"failed to verify bearer token: %v\", err)\n+                        continue\n+                }\n+\n+                return p.provider.CreateSessionStateFromBearerToken(rawBearerToken, bearerToken)\n+        }\n+        return nil, fmt.Errorf(\"unable to verify jwt token %s\", req.Header.Get(\"Authorization\"))\n }\n \n // findBearerToken finds a valid JWT token from the Authorization header of a given request.\n func (p *OAuthProxy) findBearerToken(req *http.Request) (string, error) {\n-\tauth := req.Header.Get(\"Authorization\")\n-\ts := strings.SplitN(auth, \" \", 2)\n-\tif len(s) != 2 {\n-\t\treturn \"\", fmt.Errorf(\"invalid authorization header %s\", auth)\n-\t}\n-\tjwtRegex := regexp.MustCompile(`^eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]+$`)\n-\tvar rawBearerToken string\n-\tif s[0] == \"Bearer\" && jwtRegex.MatchString(s[1]) {\n-\t\trawBearerToken = s[1]\n-\t} else if s[0] == \"Basic\" {\n-\t\t// Check if we have a Bearer token masquerading in Basic\n-\t\tb, err := b64.StdEncoding.DecodeString(s[1])\n-\t\tif err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\tpair := strings.SplitN(string(b), \":\", 2)\n-\t\tif len(pair) != 2 {\n-\t\t\treturn \"\", fmt.Errorf(\"invalid format %s\", b)\n-\t\t}\n-\t\tuser, password := pair[0], pair[1]\n-\n-\t\t// check user, user+password, or just password for a token\n-\t\tif jwtRegex.MatchString(user) {\n-\t\t\t// Support blank passwords or magic `x-oauth-basic` passwords - nothing else\n-\t\t\tif password == \"\" || password == \"x-oauth-basic\" {\n-\t\t\t\trawBearerToken = user\n-\t\t\t}\n-\t\t} else if jwtRegex.MatchString(password) {\n-\t\t\t// support passwords and ignore user\n-\t\t\trawBearerToken = password\n-\t\t}\n-\t}\n-\tif rawBearerToken == \"\" {\n-\t\treturn \"\", fmt.Errorf(\"no valid bearer token found in authorization header\")\n-\t}\n-\n-\treturn rawBearerToken, nil\n+        auth := req.Header.Get(\"Authorization\")\n+        s := strings.SplitN(auth, \" \", 2)\n+        if len(s) != 2 {\n+                return \"\", fmt.Errorf(\"invalid authorization header %s\", auth)\n+        }\n+        jwtRegex := regexp.MustCompile(`^eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]+$`)\n+        var rawBearerToken string\n+        if s[0] == \"Bearer\" && jwtRegex.MatchString(s[1]) {\n+                rawBearerToken = s[1]\n+        } else if s[0] == \"Basic\" {\n+                // Check if we have a Bearer token masquerading in Basic\n+                b, err := b64.StdEncoding.DecodeString(s[1])\n+                if err != nil {\n+                        return \"\", err\n+                }\n+                pair := strings.SplitN(string(b), \":\", 2)\n+                if len(pair) != 2 {\n+                        return \"\", fmt.Errorf(\"invalid format %s\", b)\n+                }\n+                user, password := pair[0], pair[1]\n+\n+                // check user, user+password, or just password for a token\n+                if jwtRegex.MatchString(user) {\n+                        // Support blank passwords or magic `x-oauth-basic` passwords - nothing else\n+                        if password == \"\" || password == \"x-oauth-basic\" {\n+                                rawBearerToken = user\n+                        }\n+                } else if jwtRegex.MatchString(password) {\n+                        // support passwords and ignore user\n+                        rawBearerToken = password\n+                }\n+        }\n+        if rawBearerToken == \"\" {\n+                return \"\", fmt.Errorf(\"no valid bearer token found in authorization header\")\n+        }\n+\n+        return rawBearerToken, nil\n }\n"}
{"cve":"CVE-2020-4037:0708", "fix_patch": "diff --git a/oauthproxy.go b/oauthproxy.go\nindex 8c303df8..df444aa8 100644\n--- a/oauthproxy.go\n+++ b/oauthproxy.go\n@@ -1,563 +1,563 @@\n package main\n \n import (\n-\t\"context\"\n-\t\"crypto/tls\"\n-\tb64 \"encoding/base64\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"html/template\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/http/httputil\"\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/coreos/go-oidc\"\n-\t\"github.com/mbland/hmacauth\"\n-\tipapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/ip\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/options\"\n-\tsessionsapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/cookies\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/encryption\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/ip\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/logger\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/providers\"\n-\t\"github.com/yhat/wsutil\"\n+        \"context\"\n+        \"crypto/tls\"\n+        b64 \"encoding/base64\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"html/template\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/http/httputil\"\n+        \"net/url\"\n+        \"regexp\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/coreos/go-oidc\"\n+        \"github.com/mbland/hmacauth\"\n+        ipapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/ip\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/options\"\n+        sessionsapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/cookies\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/encryption\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/ip\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/logger\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/providers\"\n+        \"github.com/yhat/wsutil\"\n )\n \n const (\n-\t// SignatureHeader is the name of the request header containing the GAP Signature\n-\t// Part of hmacauth\n-\tSignatureHeader = \"GAP-Signature\"\n+        // SignatureHeader is the name of the request header containing the GAP Signature\n+        // Part of hmacauth\n+        SignatureHeader = \"GAP-Signature\"\n \n-\thttpScheme  = \"http\"\n-\thttpsScheme = \"https\"\n+        httpScheme  = \"http\"\n+        httpsScheme = \"https\"\n \n-\tapplicationJSON = \"application/json\"\n+        applicationJSON = \"application/json\"\n )\n \n // SignatureHeaders contains the headers to be signed by the hmac algorithm\n // Part of hmacauth\n var SignatureHeaders = []string{\n-\t\"Content-Length\",\n-\t\"Content-Md5\",\n-\t\"Content-Type\",\n-\t\"Date\",\n-\t\"Authorization\",\n-\t\"X-Forwarded-User\",\n-\t\"X-Forwarded-Email\",\n-\t\"X-Forwarded-Preferred-User\",\n-\t\"X-Forwarded-Access-Token\",\n-\t\"Cookie\",\n-\t\"Gap-Auth\",\n+        \"Content-Length\",\n+        \"Content-Md5\",\n+        \"Content-Type\",\n+        \"Date\",\n+        \"Authorization\",\n+        \"X-Forwarded-User\",\n+        \"X-Forwarded-Email\",\n+        \"X-Forwarded-Preferred-User\",\n+        \"X-Forwarded-Access-Token\",\n+        \"Cookie\",\n+        \"Gap-Auth\",\n }\n \n var (\n-\t// ErrNeedsLogin means the user should be redirected to the login page\n-\tErrNeedsLogin = errors.New(\"redirect to login page\")\n+        // ErrNeedsLogin means the user should be redirected to the login page\n+        ErrNeedsLogin = errors.New(\"redirect to login page\")\n \n-\t// Used to check final redirects are not susceptible to open redirects.\n-\t// Matches //, /\\ and both of these with whitespace in between (eg / / or / \\).\n-\tinvalidRedirectRegex = regexp.MustCompile(`^/(\\s|\\v)?(/|\\\\)`)\n+        // Used to check final redirects are not susceptible to open redirects.\n+        // Matches //, /\\ and both of these with whitespace in between (eg / / or / \\).\n+        invalidRedirectRegex = regexp.MustCompile(`^/([\\s/.\\\\]|\\v)+`)\n )\n \n // OAuthProxy is the main authentication proxy\n type OAuthProxy struct {\n-\tCookieSeed     string\n-\tCookieName     string\n-\tCSRFCookieName string\n-\tCookieDomains  []string\n-\tCookiePath     string\n-\tCookieSecure   bool\n-\tCookieHTTPOnly bool\n-\tCookieExpire   time.Duration\n-\tCookieRefresh  time.Duration\n-\tCookieSameSite string\n-\tValidator      func(string) bool\n-\n-\tRobotsPath        string\n-\tSignInPath        string\n-\tSignOutPath       string\n-\tOAuthStartPath    string\n-\tOAuthCallbackPath string\n-\tAuthOnlyPath      string\n-\tUserInfoPath      string\n-\n-\tredirectURL             *url.URL // the url to receive requests at\n-\twhitelistDomains        []string\n-\tprovider                providers.Provider\n-\tproviderNameOverride    string\n-\tsessionStore            sessionsapi.SessionStore\n-\tProxyPrefix             string\n-\tSignInMessage           string\n-\tHtpasswdFile            *HtpasswdFile\n-\tDisplayHtpasswdForm     bool\n-\tserveMux                http.Handler\n-\tSetXAuthRequest         bool\n-\tPassBasicAuth           bool\n-\tSetBasicAuth            bool\n-\tSkipProviderButton      bool\n-\tPassUserHeaders         bool\n-\tBasicAuthPassword       string\n-\tPassAccessToken         bool\n-\tSetAuthorization        bool\n-\tPassAuthorization       bool\n-\tPreferEmailToUser       bool\n-\tskipAuthRegex           []string\n-\tskipAuthPreflight       bool\n-\tskipJwtBearerTokens     bool\n-\tmainJwtBearerVerifier   *oidc.IDTokenVerifier\n-\textraJwtBearerVerifiers []*oidc.IDTokenVerifier\n-\tcompiledRegex           []*regexp.Regexp\n-\ttemplates               *template.Template\n-\trealClientIPParser      ipapi.RealClientIPParser\n-\tBanner                  string\n-\tFooter                  string\n+        CookieSeed     string\n+        CookieName     string\n+        CSRFCookieName string\n+        CookieDomains  []string\n+        CookiePath     string\n+        CookieSecure   bool\n+        CookieHTTPOnly bool\n+        CookieExpire   time.Duration\n+        CookieRefresh  time.Duration\n+        CookieSameSite string\n+        Validator      func(string) bool\n+\n+        RobotsPath        string\n+        SignInPath        string\n+        SignOutPath       string\n+        OAuthStartPath    string\n+        OAuthCallbackPath string\n+        AuthOnlyPath      string\n+        UserInfoPath      string\n+\n+        redirectURL             *url.URL // the url to receive requests at\n+        whitelistDomains        []string\n+        provider                providers.Provider\n+        providerNameOverride    string\n+        sessionStore            sessionsapi.SessionStore\n+        ProxyPrefix             string\n+        SignInMessage           string\n+        HtpasswdFile            *HtpasswdFile\n+        DisplayHtpasswdForm     bool\n+        serveMux                http.Handler\n+        SetXAuthRequest         bool\n+        PassBasicAuth           bool\n+        SetBasicAuth            bool\n+        SkipProviderButton      bool\n+        PassUserHeaders         bool\n+        BasicAuthPassword       string\n+        PassAccessToken         bool\n+        SetAuthorization        bool\n+        PassAuthorization       bool\n+        PreferEmailToUser       bool\n+        skipAuthRegex           []string\n+        skipAuthPreflight       bool\n+        skipJwtBearerTokens     bool\n+        mainJwtBearerVerifier   *oidc.IDTokenVerifier\n+        extraJwtBearerVerifiers []*oidc.IDTokenVerifier\n+        compiledRegex           []*regexp.Regexp\n+        templates               *template.Template\n+        realClientIPParser      ipapi.RealClientIPParser\n+        Banner                  string\n+        Footer                  string\n }\n \n // UpstreamProxy represents an upstream server to proxy to\n type UpstreamProxy struct {\n-\tupstream  string\n-\thandler   http.Handler\n-\twsHandler http.Handler\n-\tauth      hmacauth.HmacAuth\n+        upstream  string\n+        handler   http.Handler\n+        wsHandler http.Handler\n+        auth      hmacauth.HmacAuth\n }\n \n // ServeHTTP proxies requests to the upstream provider while signing the\n // request headers\n func (u *UpstreamProxy) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tw.Header().Set(\"GAP-Upstream-Address\", u.upstream)\n-\tif u.auth != nil {\n-\t\tr.Header.Set(\"GAP-Auth\", w.Header().Get(\"GAP-Auth\"))\n-\t\tu.auth.SignRequest(r)\n-\t}\n-\tif u.wsHandler != nil && strings.EqualFold(r.Header.Get(\"Connection\"), \"upgrade\") && r.Header.Get(\"Upgrade\") == \"websocket\" {\n-\t\tu.wsHandler.ServeHTTP(w, r)\n-\t} else {\n-\t\tu.handler.ServeHTTP(w, r)\n-\t}\n+        w.Header().Set(\"GAP-Upstream-Address\", u.upstream)\n+        if u.auth != nil {\n+                r.Header.Set(\"GAP-Auth\", w.Header().Get(\"GAP-Auth\"))\n+                u.auth.SignRequest(r)\n+        }\n+        if u.wsHandler != nil && strings.EqualFold(r.Header.Get(\"Connection\"), \"upgrade\") && r.Header.Get(\"Upgrade\") == \"websocket\" {\n+                u.wsHandler.ServeHTTP(w, r)\n+        } else {\n+                u.handler.ServeHTTP(w, r)\n+        }\n \n }\n \n // NewReverseProxy creates a new reverse proxy for proxying requests to upstream\n // servers\n func NewReverseProxy(target *url.URL, opts *options.Options) (proxy *httputil.ReverseProxy) {\n-\tproxy = httputil.NewSingleHostReverseProxy(target)\n-\tproxy.FlushInterval = opts.FlushInterval\n-\tif opts.SSLUpstreamInsecureSkipVerify {\n-\t\tproxy.Transport = &http.Transport{\n-\t\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n-\t\t}\n-\t}\n-\tsetProxyErrorHandler(proxy, opts)\n-\treturn proxy\n+        proxy = httputil.NewSingleHostReverseProxy(target)\n+        proxy.FlushInterval = opts.FlushInterval\n+        if opts.SSLUpstreamInsecureSkipVerify {\n+                proxy.Transport = &http.Transport{\n+                        TLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n+                }\n+        }\n+        setProxyErrorHandler(proxy, opts)\n+        return proxy\n }\n \n func setProxyErrorHandler(proxy *httputil.ReverseProxy, opts *options.Options) {\n-\ttemplates := loadTemplates(opts.CustomTemplatesDir)\n-\tproxy.ErrorHandler = func(w http.ResponseWriter, r *http.Request, proxyErr error) {\n-\t\tlogger.Printf(\"Error proxying to upstream server: %v\", proxyErr)\n-\t\tw.WriteHeader(http.StatusBadGateway)\n-\t\tdata := struct {\n-\t\t\tTitle       string\n-\t\t\tMessage     string\n-\t\t\tProxyPrefix string\n-\t\t}{\n-\t\t\tTitle:       \"Bad Gateway\",\n-\t\t\tMessage:     \"Error proxying to upstream server\",\n-\t\t\tProxyPrefix: opts.ProxyPrefix,\n-\t\t}\n-\t\ttemplates.ExecuteTemplate(w, \"error.html\", data)\n-\t}\n+        templates := loadTemplates(opts.CustomTemplatesDir)\n+        proxy.ErrorHandler = func(w http.ResponseWriter, r *http.Request, proxyErr error) {\n+                logger.Printf(\"Error proxying to upstream server: %v\", proxyErr)\n+                w.WriteHeader(http.StatusBadGateway)\n+                data := struct {\n+                        Title       string\n+                        Message     string\n+                        ProxyPrefix string\n+                }{\n+                        Title:       \"Bad Gateway\",\n+                        Message:     \"Error proxying to upstream server\",\n+                        ProxyPrefix: opts.ProxyPrefix,\n+                }\n+                templates.ExecuteTemplate(w, \"error.html\", data)\n+        }\n }\n \n func setProxyUpstreamHostHeader(proxy *httputil.ReverseProxy, target *url.URL) {\n-\tdirector := proxy.Director\n-\tproxy.Director = func(req *http.Request) {\n-\t\tdirector(req)\n-\t\t// use RequestURI so that we aren't unescaping encoded slashes in the request path\n-\t\treq.Host = target.Host\n-\t\treq.URL.Opaque = req.RequestURI\n-\t\treq.URL.RawQuery = \"\"\n-\t}\n+        director := proxy.Director\n+        proxy.Director = func(req *http.Request) {\n+                director(req)\n+                // use RequestURI so that we aren't unescaping encoded slashes in the request path\n+                req.Host = target.Host\n+                req.URL.Opaque = req.RequestURI\n+                req.URL.RawQuery = \"\"\n+        }\n }\n \n func setProxyDirector(proxy *httputil.ReverseProxy) {\n-\tdirector := proxy.Director\n-\tproxy.Director = func(req *http.Request) {\n-\t\tdirector(req)\n-\t\t// use RequestURI so that we aren't unescaping encoded slashes in the request path\n-\t\treq.URL.Opaque = req.RequestURI\n-\t\treq.URL.RawQuery = \"\"\n-\t}\n+        director := proxy.Director\n+        proxy.Director = func(req *http.Request) {\n+                director(req)\n+                // use RequestURI so that we aren't unescaping encoded slashes in the request path\n+                req.URL.Opaque = req.RequestURI\n+                req.URL.RawQuery = \"\"\n+        }\n }\n \n // NewFileServer creates a http.Handler to serve files from the filesystem\n func NewFileServer(path string, filesystemPath string) (proxy http.Handler) {\n-\treturn http.StripPrefix(path, http.FileServer(http.Dir(filesystemPath)))\n+        return http.StripPrefix(path, http.FileServer(http.Dir(filesystemPath)))\n }\n \n // NewWebSocketOrRestReverseProxy creates a reverse proxy for REST or websocket based on url\n func NewWebSocketOrRestReverseProxy(u *url.URL, opts *options.Options, auth hmacauth.HmacAuth) http.Handler {\n-\tu.Path = \"\"\n-\tproxy := NewReverseProxy(u, opts)\n-\tif !opts.PassHostHeader {\n-\t\tsetProxyUpstreamHostHeader(proxy, u)\n-\t} else {\n-\t\tsetProxyDirector(proxy)\n-\t}\n-\n-\t// this should give us a wss:// scheme if the url is https:// based.\n-\tvar wsProxy *wsutil.ReverseProxy\n-\tif opts.ProxyWebSockets {\n-\t\twsScheme := \"ws\" + strings.TrimPrefix(u.Scheme, \"http\")\n-\t\twsURL := &url.URL{Scheme: wsScheme, Host: u.Host}\n-\t\twsProxy = wsutil.NewSingleHostReverseProxy(wsURL)\n-\t\tif opts.SSLUpstreamInsecureSkipVerify {\n-\t\t\twsProxy.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n-\t\t}\n-\t}\n-\treturn &UpstreamProxy{\n-\t\tupstream:  u.Host,\n-\t\thandler:   proxy,\n-\t\twsHandler: wsProxy,\n-\t\tauth:      auth,\n-\t}\n+        u.Path = \"\"\n+        proxy := NewReverseProxy(u, opts)\n+        if !opts.PassHostHeader {\n+                setProxyUpstreamHostHeader(proxy, u)\n+        } else {\n+                setProxyDirector(proxy)\n+        }\n+\n+        // this should give us a wss:// scheme if the url is https:// based.\n+        var wsProxy *wsutil.ReverseProxy\n+        if opts.ProxyWebSockets {\n+                wsScheme := \"ws\" + strings.TrimPrefix(u.Scheme, \"http\")\n+                wsURL := &url.URL{Scheme: wsScheme, Host: u.Host}\n+                wsProxy = wsutil.NewSingleHostReverseProxy(wsURL)\n+                if opts.SSLUpstreamInsecureSkipVerify {\n+                        wsProxy.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n+                }\n+        }\n+        return &UpstreamProxy{\n+                upstream:  u.Host,\n+                handler:   proxy,\n+                wsHandler: wsProxy,\n+                auth:      auth,\n+        }\n }\n \n // NewOAuthProxy creates a new instance of OAuthProxy from the options provided\n func NewOAuthProxy(opts *options.Options, validator func(string) bool) *OAuthProxy {\n-\tserveMux := http.NewServeMux()\n-\tvar auth hmacauth.HmacAuth\n-\tif sigData := opts.GetSignatureData(); sigData != nil {\n-\t\tauth = hmacauth.NewHmacAuth(sigData.Hash, []byte(sigData.Key),\n-\t\t\tSignatureHeader, SignatureHeaders)\n-\t}\n-\tfor _, u := range opts.GetProxyURLs() {\n-\t\tpath := u.Path\n-\t\thost := u.Host\n-\t\tswitch u.Scheme {\n-\t\tcase httpScheme, httpsScheme:\n-\t\t\tlogger.Printf(\"mapping path %q => upstream %q\", path, u)\n-\t\t\tproxy := NewWebSocketOrRestReverseProxy(u, opts, auth)\n-\t\t\tserveMux.Handle(path, proxy)\n-\t\tcase \"static\":\n-\t\t\tresponseCode, err := strconv.Atoi(host)\n-\t\t\tif err != nil {\n-\t\t\t\tlogger.Printf(\"unable to convert %q to int, use default \\\"200\\\"\", host)\n-\t\t\t\tresponseCode = 200\n-\t\t\t}\n-\n-\t\t\tserveMux.HandleFunc(path, func(rw http.ResponseWriter, req *http.Request) {\n-\t\t\t\trw.WriteHeader(responseCode)\n-\t\t\t\tfmt.Fprintf(rw, \"Authenticated\")\n-\t\t\t})\n-\t\tcase \"file\":\n-\t\t\tif u.Fragment != \"\" {\n-\t\t\t\tpath = u.Fragment\n-\t\t\t}\n-\t\t\tlogger.Printf(\"mapping path %q => file system %q\", path, u.Path)\n-\t\t\tproxy := NewFileServer(path, u.Path)\n-\t\t\tuProxy := UpstreamProxy{\n-\t\t\t\tupstream:  path,\n-\t\t\t\thandler:   proxy,\n-\t\t\t\twsHandler: nil,\n-\t\t\t\tauth:      nil,\n-\t\t\t}\n-\t\t\tserveMux.Handle(path, &uProxy)\n-\t\tdefault:\n-\t\t\tpanic(fmt.Sprintf(\"unknown upstream protocol %s\", u.Scheme))\n-\t\t}\n-\t}\n-\tfor _, u := range opts.GetCompiledRegex() {\n-\t\tlogger.Printf(\"compiled skip-auth-regex => %q\", u)\n-\t}\n-\n-\tif opts.SkipJwtBearerTokens {\n-\t\tlogger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n-\t\tfor _, issuer := range opts.ExtraJwtIssuers {\n-\t\t\tlogger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n-\t\t}\n-\t}\n-\tredirectURL := opts.GetRedirectURL()\n-\tif redirectURL.Path == \"\" {\n-\t\tredirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n-\t}\n-\n-\tlogger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.GetProvider().Data().ProviderName, opts.ClientID)\n-\trefresh := \"disabled\"\n-\tif opts.Cookie.Refresh != time.Duration(0) {\n-\t\trefresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n-\t}\n-\n-\tlogger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n-\n-\treturn &OAuthProxy{\n-\t\tCookieName:     opts.Cookie.Name,\n-\t\tCSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n-\t\tCookieSeed:     opts.Cookie.Secret,\n-\t\tCookieDomains:  opts.Cookie.Domains,\n-\t\tCookiePath:     opts.Cookie.Path,\n-\t\tCookieSecure:   opts.Cookie.Secure,\n-\t\tCookieHTTPOnly: opts.Cookie.HTTPOnly,\n-\t\tCookieExpire:   opts.Cookie.Expire,\n-\t\tCookieRefresh:  opts.Cookie.Refresh,\n-\t\tCookieSameSite: opts.Cookie.SameSite,\n-\t\tValidator:      validator,\n-\n-\t\tRobotsPath:        \"/robots.txt\",\n-\t\tSignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n-\t\tSignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n-\t\tOAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n-\t\tOAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n-\t\tAuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n-\t\tUserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n-\n-\t\tProxyPrefix:             opts.ProxyPrefix,\n-\t\tprovider:                opts.GetProvider(),\n-\t\tproviderNameOverride:    opts.ProviderName,\n-\t\tsessionStore:            opts.GetSessionStore(),\n-\t\tserveMux:                serveMux,\n-\t\tredirectURL:             redirectURL,\n-\t\twhitelistDomains:        opts.WhitelistDomains,\n-\t\tskipAuthRegex:           opts.SkipAuthRegex,\n-\t\tskipAuthPreflight:       opts.SkipAuthPreflight,\n-\t\tskipJwtBearerTokens:     opts.SkipJwtBearerTokens,\n-\t\tmainJwtBearerVerifier:   opts.GetOIDCVerifier(),\n-\t\textraJwtBearerVerifiers: opts.GetJWTBearerVerifiers(),\n-\t\tcompiledRegex:           opts.GetCompiledRegex(),\n-\t\trealClientIPParser:      opts.GetRealClientIPParser(),\n-\t\tSetXAuthRequest:         opts.SetXAuthRequest,\n-\t\tPassBasicAuth:           opts.PassBasicAuth,\n-\t\tSetBasicAuth:            opts.SetBasicAuth,\n-\t\tPassUserHeaders:         opts.PassUserHeaders,\n-\t\tBasicAuthPassword:       opts.BasicAuthPassword,\n-\t\tPassAccessToken:         opts.PassAccessToken,\n-\t\tSetAuthorization:        opts.SetAuthorization,\n-\t\tPassAuthorization:       opts.PassAuthorization,\n-\t\tPreferEmailToUser:       opts.PreferEmailToUser,\n-\t\tSkipProviderButton:      opts.SkipProviderButton,\n-\t\ttemplates:               loadTemplates(opts.CustomTemplatesDir),\n-\t\tBanner:                  opts.Banner,\n-\t\tFooter:                  opts.Footer,\n-\t}\n+        serveMux := http.NewServeMux()\n+        var auth hmacauth.HmacAuth\n+        if sigData := opts.GetSignatureData(); sigData != nil {\n+                auth = hmacauth.NewHmacAuth(sigData.Hash, []byte(sigData.Key),\n+                        SignatureHeader, SignatureHeaders)\n+        }\n+        for _, u := range opts.GetProxyURLs() {\n+                path := u.Path\n+                host := u.Host\n+                switch u.Scheme {\n+                case httpScheme, httpsScheme:\n+                        logger.Printf(\"mapping path %q => upstream %q\", path, u)\n+                        proxy := NewWebSocketOrRestReverseProxy(u, opts, auth)\n+                        serveMux.Handle(path, proxy)\n+                case \"static\":\n+                        responseCode, err := strconv.Atoi(host)\n+                        if err != nil {\n+                                logger.Printf(\"unable to convert %q to int, use default \\\"200\\\"\", host)\n+                                responseCode = 200\n+                        }\n+\n+                        serveMux.HandleFunc(path, func(rw http.ResponseWriter, req *http.Request) {\n+                                rw.WriteHeader(responseCode)\n+                                fmt.Fprintf(rw, \"Authenticated\")\n+                        })\n+                case \"file\":\n+                        if u.Fragment != \"\" {\n+                                path = u.Fragment\n+                        }\n+                        logger.Printf(\"mapping path %q => file system %q\", path, u.Path)\n+                        proxy := NewFileServer(path, u.Path)\n+                        uProxy := UpstreamProxy{\n+                                upstream:  path,\n+                                handler:   proxy,\n+                                wsHandler: nil,\n+                                auth:      nil,\n+                        }\n+                        serveMux.Handle(path, &uProxy)\n+                default:\n+                        panic(fmt.Sprintf(\"unknown upstream protocol %s\", u.Scheme))\n+                }\n+        }\n+        for _, u := range opts.GetCompiledRegex() {\n+                logger.Printf(\"compiled skip-auth-regex => %q\", u)\n+        }\n+\n+        if opts.SkipJwtBearerTokens {\n+                logger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n+                for _, issuer := range opts.ExtraJwtIssuers {\n+                        logger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n+                }\n+        }\n+        redirectURL := opts.GetRedirectURL()\n+        if redirectURL.Path == \"\" {\n+                redirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n+        }\n+\n+        logger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.GetProvider().Data().ProviderName, opts.ClientID)\n+        refresh := \"disabled\"\n+        if opts.Cookie.Refresh != time.Duration(0) {\n+                refresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n+        }\n+\n+        logger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n+\n+        return &OAuthProxy{\n+                CookieName:     opts.Cookie.Name,\n+                CSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n+                CookieSeed:     opts.Cookie.Secret,\n+                CookieDomains:  opts.Cookie.Domains,\n+                CookiePath:     opts.Cookie.Path,\n+                CookieSecure:   opts.Cookie.Secure,\n+                CookieHTTPOnly: opts.Cookie.HTTPOnly,\n+                CookieExpire:   opts.Cookie.Expire,\n+                CookieRefresh:  opts.Cookie.Refresh,\n+                CookieSameSite: opts.Cookie.SameSite,\n+                Validator:      validator,\n+\n+                RobotsPath:        \"/robots.txt\",\n+                SignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n+                SignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n+                OAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n+                OAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n+                AuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n+                UserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n+\n+                ProxyPrefix:             opts.ProxyPrefix,\n+                provider:                opts.GetProvider(),\n+                providerNameOverride:    opts.ProviderName,\n+                sessionStore:            opts.GetSessionStore(),\n+                serveMux:                serveMux,\n+                redirectURL:             redirectURL,\n+                whitelistDomains:        opts.WhitelistDomains,\n+                skipAuthRegex:           opts.SkipAuthRegex,\n+                skipAuthPreflight:       opts.SkipAuthPreflight,\n+                skipJwtBearerTokens:     opts.SkipJwtBearerTokens,\n+                mainJwtBearerVerifier:   opts.GetOIDCVerifier(),\n+                extraJwtBearerVerifiers: opts.GetJWTBearerVerifiers(),\n+                compiledRegex:           opts.GetCompiledRegex(),\n+                realClientIPParser:      opts.GetRealClientIPParser(),\n+                SetXAuthRequest:         opts.SetXAuthRequest,\n+                PassBasicAuth:           opts.PassBasicAuth,\n+                SetBasicAuth:            opts.SetBasicAuth,\n+                PassUserHeaders:         opts.PassUserHeaders,\n+                BasicAuthPassword:       opts.BasicAuthPassword,\n+                PassAccessToken:         opts.PassAccessToken,\n+                SetAuthorization:        opts.SetAuthorization,\n+                PassAuthorization:       opts.PassAuthorization,\n+                PreferEmailToUser:       opts.PreferEmailToUser,\n+                SkipProviderButton:      opts.SkipProviderButton,\n+                templates:               loadTemplates(opts.CustomTemplatesDir),\n+                Banner:                  opts.Banner,\n+                Footer:                  opts.Footer,\n+        }\n }\n \n // GetRedirectURI returns the redirectURL that the upstream OAuth Provider will\n // redirect clients to once authenticated\n func (p *OAuthProxy) GetRedirectURI(host string) string {\n-\t// default to the request Host if not set\n-\tif p.redirectURL.Host != \"\" {\n-\t\treturn p.redirectURL.String()\n-\t}\n-\tu := *p.redirectURL\n-\tif u.Scheme == \"\" {\n-\t\tif p.CookieSecure {\n-\t\t\tu.Scheme = httpsScheme\n-\t\t} else {\n-\t\t\tu.Scheme = httpScheme\n-\t\t}\n-\t}\n-\tu.Host = host\n-\treturn u.String()\n+        // default to the request Host if not set\n+        if p.redirectURL.Host != \"\" {\n+                return p.redirectURL.String()\n+        }\n+        u := *p.redirectURL\n+        if u.Scheme == \"\" {\n+                if p.CookieSecure {\n+                        u.Scheme = httpsScheme\n+                } else {\n+                        u.Scheme = httpScheme\n+                }\n+        }\n+        u.Host = host\n+        return u.String()\n }\n \n func (p *OAuthProxy) displayCustomLoginForm() bool {\n-\treturn p.HtpasswdFile != nil && p.DisplayHtpasswdForm\n+        return p.HtpasswdFile != nil && p.DisplayHtpasswdForm\n }\n \n func (p *OAuthProxy) redeemCode(ctx context.Context, host, code string) (s *sessionsapi.SessionState, err error) {\n-\tif code == \"\" {\n-\t\treturn nil, errors.New(\"missing code\")\n-\t}\n-\tredirectURI := p.GetRedirectURI(host)\n-\ts, err = p.provider.Redeem(ctx, redirectURI, code)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tif s.Email == \"\" {\n-\t\ts.Email, err = p.provider.GetEmailAddress(ctx, s)\n-\t}\n-\n-\tif s.PreferredUsername == \"\" {\n-\t\ts.PreferredUsername, err = p.provider.GetPreferredUsername(ctx, s)\n-\t\tif err != nil && err.Error() == \"not implemented\" {\n-\t\t\terr = nil\n-\t\t}\n-\t}\n-\n-\tif s.User == \"\" {\n-\t\ts.User, err = p.provider.GetUserName(ctx, s)\n-\t\tif err != nil && err.Error() == \"not implemented\" {\n-\t\t\terr = nil\n-\t\t}\n-\t}\n-\treturn\n+        if code == \"\" {\n+                return nil, errors.New(\"missing code\")\n+        }\n+        redirectURI := p.GetRedirectURI(host)\n+        s, err = p.provider.Redeem(ctx, redirectURI, code)\n+        if err != nil {\n+                return\n+        }\n+\n+        if s.Email == \"\" {\n+                s.Email, err = p.provider.GetEmailAddress(ctx, s)\n+        }\n+\n+        if s.PreferredUsername == \"\" {\n+                s.PreferredUsername, err = p.provider.GetPreferredUsername(ctx, s)\n+                if err != nil && err.Error() == \"not implemented\" {\n+                        err = nil\n+                }\n+        }\n+\n+        if s.User == \"\" {\n+                s.User, err = p.provider.GetUserName(ctx, s)\n+                if err != nil && err.Error() == \"not implemented\" {\n+                        err = nil\n+                }\n+        }\n+        return\n }\n \n // MakeCSRFCookie creates a cookie for CSRF\n func (p *OAuthProxy) MakeCSRFCookie(req *http.Request, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\treturn p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n+        return p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n }\n \n func (p *OAuthProxy) makeCookie(req *http.Request, name string, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\tcookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n-\n-\tif cookieDomain != \"\" {\n-\t\tdomain := cookies.GetRequestHost(req)\n-\t\tif h, _, err := net.SplitHostPort(domain); err == nil {\n-\t\t\tdomain = h\n-\t\t}\n-\t\tif !strings.HasSuffix(domain, cookieDomain) {\n-\t\t\tlogger.Printf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n-\t\t}\n-\t}\n-\n-\treturn &http.Cookie{\n-\t\tName:     name,\n-\t\tValue:    value,\n-\t\tPath:     p.CookiePath,\n-\t\tDomain:   cookieDomain,\n-\t\tHttpOnly: p.CookieHTTPOnly,\n-\t\tSecure:   p.CookieSecure,\n-\t\tExpires:  now.Add(expiration),\n-\t\tSameSite: cookies.ParseSameSite(p.CookieSameSite),\n-\t}\n+        cookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n+\n+        if cookieDomain != \"\" {\n+                domain := cookies.GetRequestHost(req)\n+                if h, _, err := net.SplitHostPort(domain); err == nil {\n+                        domain = h\n+                }\n+                if !strings.HasSuffix(domain, cookieDomain) {\n+                        logger.Printf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n+                }\n+        }\n+\n+        return &http.Cookie{\n+                Name:     name,\n+                Value:    value,\n+                Path:     p.CookiePath,\n+                Domain:   cookieDomain,\n+                HttpOnly: p.CookieHTTPOnly,\n+                Secure:   p.CookieSecure,\n+                Expires:  now.Add(expiration),\n+                SameSite: cookies.ParseSameSite(p.CookieSameSite),\n+        }\n }\n \n // ClearCSRFCookie creates a cookie to unset the CSRF cookie stored in the user's\n // session\n func (p *OAuthProxy) ClearCSRFCookie(rw http.ResponseWriter, req *http.Request) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n }\n \n // SetCSRFCookie adds a CSRF cookie to the response\n func (p *OAuthProxy) SetCSRFCookie(rw http.ResponseWriter, req *http.Request, val string) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n }\n \n // ClearSessionCookie creates a cookie to unset the user's authentication cookie\n // stored in the user's session\n func (p *OAuthProxy) ClearSessionCookie(rw http.ResponseWriter, req *http.Request) error {\n-\treturn p.sessionStore.Clear(rw, req)\n+        return p.sessionStore.Clear(rw, req)\n }\n \n // LoadCookiedSession reads the user's authentication details from the request\n func (p *OAuthProxy) LoadCookiedSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\treturn p.sessionStore.Load(req)\n+        return p.sessionStore.Load(req)\n }\n \n // SaveSession creates a new session cookie value and sets this on the response\n func (p *OAuthProxy) SaveSession(rw http.ResponseWriter, req *http.Request, s *sessionsapi.SessionState) error {\n-\treturn p.sessionStore.Save(rw, req, s)\n+        return p.sessionStore.Save(rw, req, s)\n }\n \n // RobotsTxt disallows scraping pages from the OAuthProxy\n func (p *OAuthProxy) RobotsTxt(rw http.ResponseWriter) {\n-\trw.WriteHeader(http.StatusOK)\n-\tfmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n+        rw.WriteHeader(http.StatusOK)\n+        fmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n }\n \n // ErrorPage writes an error response\n func (p *OAuthProxy) ErrorPage(rw http.ResponseWriter, code int, title string, message string) {\n-\trw.WriteHeader(code)\n-\tt := struct {\n-\t\tTitle       string\n-\t\tMessage     string\n-\t\tProxyPrefix string\n-\t}{\n-\t\tTitle:       fmt.Sprintf(\"%d %s\", code, title),\n-\t\tMessage:     message,\n-\t\tProxyPrefix: p.ProxyPrefix,\n-\t}\n-\tp.templates.ExecuteTemplate(rw, \"error.html\", t)\n+        rw.WriteHeader(code)\n+        t := struct {\n+                Title       string\n+                Message     string\n+                ProxyPrefix string\n+        }{\n+                Title:       fmt.Sprintf(\"%d %s\", code, title),\n+                Message:     message,\n+                ProxyPrefix: p.ProxyPrefix,\n+        }\n+        p.templates.ExecuteTemplate(rw, \"error.html\", t)\n }\n \n // SignInPage writes the sing in template to the response\n func (p *OAuthProxy) SignInPage(rw http.ResponseWriter, req *http.Request, code int) {\n-\tprepareNoCache(rw)\n-\tp.ClearSessionCookie(rw, req)\n-\trw.WriteHeader(code)\n-\n-\tredirectURL, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tif redirectURL == p.SignInPath {\n-\t\tredirectURL = \"/\"\n-\t}\n-\n-\tt := struct {\n-\t\tProviderName  string\n-\t\tSignInMessage template.HTML\n-\t\tCustomLogin   bool\n-\t\tRedirect      string\n-\t\tVersion       string\n-\t\tProxyPrefix   string\n-\t\tFooter        template.HTML\n-\t}{\n-\t\tProviderName:  p.provider.Data().ProviderName,\n-\t\tSignInMessage: template.HTML(p.SignInMessage),\n-\t\tCustomLogin:   p.displayCustomLoginForm(),\n-\t\tRedirect:      redirectURL,\n-\t\tVersion:       VERSION,\n-\t\tProxyPrefix:   p.ProxyPrefix,\n-\t\tFooter:        template.HTML(p.Footer),\n-\t}\n-\tif p.providerNameOverride != \"\" {\n-\t\tt.ProviderName = p.providerNameOverride\n-\t}\n-\tp.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n+        prepareNoCache(rw)\n+        p.ClearSessionCookie(rw, req)\n+        rw.WriteHeader(code)\n+\n+        redirectURL, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+\n+        if redirectURL == p.SignInPath {\n+                redirectURL = \"/\"\n+        }\n+\n+        t := struct {\n+                ProviderName  string\n+                SignInMessage template.HTML\n+                CustomLogin   bool\n+                Redirect      string\n+                Version       string\n+                ProxyPrefix   string\n+                Footer        template.HTML\n+        }{\n+                ProviderName:  p.provider.Data().ProviderName,\n+                SignInMessage: template.HTML(p.SignInMessage),\n+                CustomLogin:   p.displayCustomLoginForm(),\n+                Redirect:      redirectURL,\n+                Version:       VERSION,\n+                ProxyPrefix:   p.ProxyPrefix,\n+                Footer:        template.HTML(p.Footer),\n+        }\n+        if p.providerNameOverride != \"\" {\n+                t.ProviderName = p.providerNameOverride\n+        }\n+        p.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n }\n \n // ManualSignIn handles basic auth logins to the proxy\n func (p *OAuthProxy) ManualSignIn(rw http.ResponseWriter, req *http.Request) (string, bool) {\n-\tif req.Method != \"POST\" || p.HtpasswdFile == nil {\n-\t\treturn \"\", false\n-\t}\n-\tuser := req.FormValue(\"username\")\n-\tpasswd := req.FormValue(\"password\")\n-\tif user == \"\" {\n-\t\treturn \"\", false\n-\t}\n-\t// check auth\n-\tif p.HtpasswdFile.Validate(user, passwd) {\n-\t\tlogger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n-\t\treturn user, true\n-\t}\n-\tlogger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n-\treturn \"\", false\n+        if req.Method != \"POST\" || p.HtpasswdFile == nil {\n+                return \"\", false\n+        }\n+        user := req.FormValue(\"username\")\n+        passwd := req.FormValue(\"password\")\n+        if user == \"\" {\n+                return \"\", false\n+        }\n+        // check auth\n+        if p.HtpasswdFile.Validate(user, passwd) {\n+                logger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n+                return user, true\n+        }\n+        logger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n+        return \"\", false\n }\n \n // GetRedirect reads the query parameter to get the URL to redirect clients to\n // once authenticated with the OAuthProxy\n func (p *OAuthProxy) GetRedirect(req *http.Request) (redirect string, err error) {\n-\terr = req.ParseForm()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tredirect = req.Header.Get(\"X-Auth-Request-Redirect\")\n-\tif req.Form.Get(\"rd\") != \"\" {\n-\t\tredirect = req.Form.Get(\"rd\")\n-\t}\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = req.URL.Path\n-\t\tif strings.HasPrefix(redirect, p.ProxyPrefix) {\n-\t\t\tredirect = \"/\"\n-\t\t}\n-\t}\n-\n-\treturn\n+        err = req.ParseForm()\n+        if err != nil {\n+                return\n+        }\n+\n+        redirect = req.Header.Get(\"X-Auth-Request-Redirect\")\n+        if req.Form.Get(\"rd\") != \"\" {\n+                redirect = req.Form.Get(\"rd\")\n+        }\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = req.URL.Path\n+                if strings.HasPrefix(redirect, p.ProxyPrefix) {\n+                        redirect = \"/\"\n+                }\n+        }\n+\n+        return\n }\n \n // splitHostPort separates host and port. If the port is not valid, it returns\n@@ -565,325 +565,325 @@ func (p *OAuthProxy) GetRedirect(req *http.Request) (redirect string, err error)\n // Unlike net.SplitHostPort, but per RFC 3986, it requires ports to be numeric.\n // *** taken from net/url, modified validOptionalPort() to accept \":*\"\n func splitHostPort(hostport string) (host, port string) {\n-\thost = hostport\n+        host = hostport\n \n-\tcolon := strings.LastIndexByte(host, ':')\n-\tif colon != -1 && validOptionalPort(host[colon:]) {\n-\t\thost, port = host[:colon], host[colon+1:]\n-\t}\n+        colon := strings.LastIndexByte(host, ':')\n+        if colon != -1 && validOptionalPort(host[colon:]) {\n+                host, port = host[:colon], host[colon+1:]\n+        }\n \n-\tif strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n-\t\thost = host[1 : len(host)-1]\n-\t}\n+        if strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n+                host = host[1 : len(host)-1]\n+        }\n \n-\treturn\n+        return\n }\n \n // validOptionalPort reports whether port is either an empty string\n // or matches /^:\\d*$/\n // *** taken from net/url, modified to accept \":*\"\n func validOptionalPort(port string) bool {\n-\tif port == \"\" || port == \":*\" {\n-\t\treturn true\n-\t}\n-\tif port[0] != ':' {\n-\t\treturn false\n-\t}\n-\tfor _, b := range port[1:] {\n-\t\tif b < '0' || b > '9' {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\treturn true\n+        if port == \"\" || port == \":*\" {\n+                return true\n+        }\n+        if port[0] != ':' {\n+                return false\n+        }\n+        for _, b := range port[1:] {\n+                if b < '0' || b > '9' {\n+                        return false\n+                }\n+        }\n+        return true\n }\n \n // IsValidRedirect checks whether the redirect URL is whitelisted\n func (p *OAuthProxy) IsValidRedirect(redirect string) bool {\n-\tswitch {\n-\tcase redirect == \"\":\n-\t\t// The user didn't specify a redirect, should fallback to `/`\n-\t\treturn false\n-\tcase strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !invalidRedirectRegex.MatchString(redirect):\n-\t\treturn true\n-\tcase strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n-\t\tredirectURL, err := url.Parse(redirect)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n-\t\t\treturn false\n-\t\t}\n-\t\tredirectHostname := redirectURL.Hostname()\n-\n-\t\tfor _, domain := range p.whitelistDomains {\n-\t\t\tdomainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n-\t\t\tif domainHostname == \"\" {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tif (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n-\t\t\t\t// the domain names match, now validate the ports\n-\t\t\t\t// if the whitelisted domain's port is '*', allow all ports\n-\t\t\t\t// if the whitelisted domain contains a specific port, only allow that port\n-\t\t\t\t// if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n-\t\t\t\tredirectPort := redirectURL.Port()\n-\t\t\t\tif (domainPort == \"*\") ||\n-\t\t\t\t\t(domainPort == redirectPort) ||\n-\t\t\t\t\t(domainPort == \"\" && redirectPort == \"\") {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n-\t\treturn false\n-\tdefault:\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n-\t\treturn false\n-\t}\n+        switch {\n+        case redirect == \"\":\n+                // The user didn't specify a redirect, should fallback to `/`\n+                return false\n+        case strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !invalidRedirectRegex.MatchString(redirect):\n+                return true\n+        case strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n+                redirectURL, err := url.Parse(redirect)\n+                if err != nil {\n+                        logger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n+                        return false\n+                }\n+                redirectHostname := redirectURL.Hostname()\n+\n+                for _, domain := range p.whitelistDomains {\n+                        domainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n+                        if domainHostname == \"\" {\n+                                continue\n+                        }\n+\n+                        if (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n+                                // the domain names match, now validate the ports\n+                                // if the whitelisted domain's port is '*', allow all ports\n+                                // if the whitelisted domain contains a specific port, only allow that port\n+                                // if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n+                                redirectPort := redirectURL.Port()\n+                                if (domainPort == \"*\") ||\n+                                        (domainPort == redirectPort) ||\n+                                        (domainPort == \"\" && redirectPort == \"\") {\n+                                        return true\n+                                }\n+                        }\n+                }\n+\n+                logger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n+                return false\n+        default:\n+                logger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n+                return false\n+        }\n }\n \n // IsWhitelistedRequest is used to check if auth should be skipped for this request\n func (p *OAuthProxy) IsWhitelistedRequest(req *http.Request) bool {\n-\tisPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n-\treturn isPreflightRequestAllowed || p.IsWhitelistedPath(req.URL.Path)\n+        isPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n+        return isPreflightRequestAllowed || p.IsWhitelistedPath(req.URL.Path)\n }\n \n // IsWhitelistedPath is used to check if the request path is allowed without auth\n func (p *OAuthProxy) IsWhitelistedPath(path string) bool {\n-\tfor _, u := range p.compiledRegex {\n-\t\tif u.MatchString(path) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, u := range p.compiledRegex {\n+                if u.MatchString(path) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // See https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching?hl=en\n var noCacheHeaders = map[string]string{\n-\t\"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n-\t\"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n-\t\"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n+        \"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n+        \"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n+        \"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n }\n \n // prepareNoCache prepares headers for preventing browser caching.\n func prepareNoCache(w http.ResponseWriter) {\n-\t// Set NoCache headers\n-\tfor k, v := range noCacheHeaders {\n-\t\tw.Header().Set(k, v)\n-\t}\n+        // Set NoCache headers\n+        for k, v := range noCacheHeaders {\n+                w.Header().Set(k, v)\n+        }\n }\n \n func (p *OAuthProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n-\tif strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n-\t\tprepareNoCache(rw)\n-\t}\n-\n-\tswitch path := req.URL.Path; {\n-\tcase path == p.RobotsPath:\n-\t\tp.RobotsTxt(rw)\n-\tcase p.IsWhitelistedRequest(req):\n-\t\tp.serveMux.ServeHTTP(rw, req)\n-\tcase path == p.SignInPath:\n-\t\tp.SignIn(rw, req)\n-\tcase path == p.SignOutPath:\n-\t\tp.SignOut(rw, req)\n-\tcase path == p.OAuthStartPath:\n-\t\tp.OAuthStart(rw, req)\n-\tcase path == p.OAuthCallbackPath:\n-\t\tp.OAuthCallback(rw, req)\n-\tcase path == p.AuthOnlyPath:\n-\t\tp.AuthenticateOnly(rw, req)\n-\tcase path == p.UserInfoPath:\n-\t\tp.UserInfo(rw, req)\n-\tdefault:\n-\t\tp.Proxy(rw, req)\n-\t}\n+        if strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n+                prepareNoCache(rw)\n+        }\n+\n+        switch path := req.URL.Path; {\n+        case path == p.RobotsPath:\n+                p.RobotsTxt(rw)\n+        case p.IsWhitelistedRequest(req):\n+                p.serveMux.ServeHTTP(rw, req)\n+        case path == p.SignInPath:\n+                p.SignIn(rw, req)\n+        case path == p.SignOutPath:\n+                p.SignOut(rw, req)\n+        case path == p.OAuthStartPath:\n+                p.OAuthStart(rw, req)\n+        case path == p.OAuthCallbackPath:\n+                p.OAuthCallback(rw, req)\n+        case path == p.AuthOnlyPath:\n+                p.AuthenticateOnly(rw, req)\n+        case path == p.UserInfoPath:\n+                p.UserInfo(rw, req)\n+        default:\n+                p.Proxy(rw, req)\n+        }\n }\n \n // SignIn serves a page prompting users to sign in\n func (p *OAuthProxy) SignIn(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tuser, ok := p.ManualSignIn(rw, req)\n-\tif ok {\n-\t\tsession := &sessionsapi.SessionState{User: user}\n-\t\tp.SaveSession(rw, req, session)\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusOK)\n-\t\t}\n-\t}\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+\n+        user, ok := p.ManualSignIn(rw, req)\n+        if ok {\n+                session := &sessionsapi.SessionState{User: user}\n+                p.SaveSession(rw, req, session)\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusOK)\n+                }\n+        }\n }\n \n //UserInfo endpoint outputs session email and preferred username in JSON format\n func (p *OAuthProxy) UserInfo(rw http.ResponseWriter, req *http.Request) {\n \n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\tuserInfo := struct {\n-\t\tEmail             string `json:\"email\"`\n-\t\tPreferredUsername string `json:\"preferredUsername,omitempty\"`\n-\t}{\n-\t\tEmail:             session.Email,\n-\t\tPreferredUsername: session.PreferredUsername,\n-\t}\n-\trw.Header().Set(\"Content-Type\", \"application/json\")\n-\trw.WriteHeader(http.StatusOK)\n-\tjson.NewEncoder(rw).Encode(userInfo)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+                return\n+        }\n+        userInfo := struct {\n+                Email             string `json:\"email\"`\n+                PreferredUsername string `json:\"preferredUsername,omitempty\"`\n+        }{\n+                Email:             session.Email,\n+                PreferredUsername: session.PreferredUsername,\n+        }\n+        rw.Header().Set(\"Content-Type\", \"application/json\")\n+        rw.WriteHeader(http.StatusOK)\n+        json.NewEncoder(rw).Encode(userInfo)\n }\n \n // SignOut sends a response to clear the authentication cookie\n func (p *OAuthProxy) SignOut(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearSessionCookie(rw, req)\n-\thttp.Redirect(rw, req, redirect, http.StatusFound)\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        p.ClearSessionCookie(rw, req)\n+        http.Redirect(rw, req, redirect, http.StatusFound)\n }\n \n // OAuthStart starts the OAuth2 authentication flow\n func (p *OAuthProxy) OAuthStart(rw http.ResponseWriter, req *http.Request) {\n-\tprepareNoCache(rw)\n-\tnonce, err := encryption.Nonce()\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining nonce: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.SetCSRFCookie(rw, req, nonce)\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tredirectURI := p.GetRedirectURI(req.Host)\n-\thttp.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n+        prepareNoCache(rw)\n+        nonce, err := encryption.Nonce()\n+        if err != nil {\n+                logger.Printf(\"Error obtaining nonce: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        p.SetCSRFCookie(rw, req, nonce)\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        redirectURI := p.GetRedirectURI(req.Host)\n+        http.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n }\n \n // OAuthCallback is the OAuth2 authentication flow callback that finishes the\n // OAuth2 authentication flow\n func (p *OAuthProxy) OAuthCallback(rw http.ResponseWriter, req *http.Request) {\n-\tremoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n-\n-\t// finish the oauth cycle\n-\terr := req.ParseForm()\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 callback: %s\" + err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\terrorString := req.Form.Get(\"error\")\n-\tif errorString != \"\" {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 callback: %s \", errorString)\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", errorString)\n-\t\treturn\n-\t}\n-\n-\tsession, err := p.redeemCode(req.Context(), req.Host, req.Form.Get(\"code\"))\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error redeeming code during OAuth2 callback: %s \", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n-\t\treturn\n-\t}\n-\n-\ts := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n-\tif len(s) != 2 {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 state: invalid length\")\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Invalid State\")\n-\t\treturn\n-\t}\n-\tnonce := s[0]\n-\tredirect := s[1]\n-\tc, err := req.Cookie(p.CSRFCookieName)\n-\tif err != nil {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable too obtain CSRF cookie\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearCSRFCookie(rw, req)\n-\tif c.Value != nonce {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: csrf token mismatch, potential attack\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", \"csrf failed\")\n-\t\treturn\n-\t}\n-\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = \"/\"\n-\t}\n-\n-\t// set cookie, or deny\n-\tif p.Validator(session.Email) && p.provider.ValidateGroup(session.Email) {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n-\t\terr := p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"%s %s\", remoteAddr, err)\n-\t\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n-\t\t\treturn\n-\t\t}\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", \"Invalid Account\")\n-\t}\n+        remoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n+\n+        // finish the oauth cycle\n+        err := req.ParseForm()\n+        if err != nil {\n+                logger.Printf(\"Error while parsing OAuth2 callback: %s\" + err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        errorString := req.Form.Get(\"error\")\n+        if errorString != \"\" {\n+                logger.Printf(\"Error while parsing OAuth2 callback: %s \", errorString)\n+                p.ErrorPage(rw, 403, \"Permission Denied\", errorString)\n+                return\n+        }\n+\n+        session, err := p.redeemCode(req.Context(), req.Host, req.Form.Get(\"code\"))\n+        if err != nil {\n+                logger.Printf(\"Error redeeming code during OAuth2 callback: %s \", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n+                return\n+        }\n+\n+        s := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n+        if len(s) != 2 {\n+                logger.Printf(\"Error while parsing OAuth2 state: invalid length\")\n+                p.ErrorPage(rw, 500, \"Internal Error\", \"Invalid State\")\n+                return\n+        }\n+        nonce := s[0]\n+        redirect := s[1]\n+        c, err := req.Cookie(p.CSRFCookieName)\n+        if err != nil {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable too obtain CSRF cookie\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", err.Error())\n+                return\n+        }\n+        p.ClearCSRFCookie(rw, req)\n+        if c.Value != nonce {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: csrf token mismatch, potential attack\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", \"csrf failed\")\n+                return\n+        }\n+\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = \"/\"\n+        }\n+\n+        // set cookie, or deny\n+        if p.Validator(session.Email) && p.provider.ValidateGroup(session.Email) {\n+                logger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n+                err := p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.Printf(\"%s %s\", remoteAddr, err)\n+                        p.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n+                        return\n+                }\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", \"Invalid Account\")\n+        }\n }\n \n // AuthenticateOnly checks whether the user is currently logged in\n func (p *OAuthProxy) AuthenticateOnly(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, \"unauthorized request\", http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\t// we are authenticated\n-\tp.addHeadersForProxying(rw, req, session)\n-\trw.WriteHeader(http.StatusAccepted)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, \"unauthorized request\", http.StatusUnauthorized)\n+                return\n+        }\n+\n+        // we are authenticated\n+        p.addHeadersForProxying(rw, req, session)\n+        rw.WriteHeader(http.StatusAccepted)\n }\n \n // Proxy proxies the user request if the user is authenticated else it prompts\n // them to authenticate\n func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tswitch err {\n-\tcase nil:\n-\t\t// we are authenticated\n-\t\tp.addHeadersForProxying(rw, req, session)\n-\t\tp.serveMux.ServeHTTP(rw, req)\n-\n-\tcase ErrNeedsLogin:\n-\t\t// we need to send the user to a login screen\n-\t\tif isAjax(req) {\n-\t\t\t// no point redirecting an AJAX request\n-\t\t\tp.ErrorJSON(rw, http.StatusUnauthorized)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusForbidden)\n-\t\t}\n-\n-\tdefault:\n-\t\t// unknown error\n-\t\tlogger.Printf(\"Unexpected internal error: %s\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError,\n-\t\t\t\"Internal Error\", \"Internal Error\")\n-\t}\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        switch err {\n+        case nil:\n+                // we are authenticated\n+                p.addHeadersForProxying(rw, req, session)\n+                p.serveMux.ServeHTTP(rw, req)\n+\n+        case ErrNeedsLogin:\n+                // we need to send the user to a login screen\n+                if isAjax(req) {\n+                        // no point redirecting an AJAX request\n+                        p.ErrorJSON(rw, http.StatusUnauthorized)\n+                        return\n+                }\n+\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusForbidden)\n+                }\n+\n+        default:\n+                // unknown error\n+                logger.Printf(\"Unexpected internal error: %s\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError,\n+                        \"Internal Error\", \"Internal Error\")\n+        }\n \n }\n \n@@ -891,315 +891,315 @@ func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n // Returns nil, ErrNeedsLogin if user needs to login.\n // Set-Cookie headers may be set on the response as a side-effect of calling this method.\n func (p *OAuthProxy) getAuthenticatedSession(rw http.ResponseWriter, req *http.Request) (*sessionsapi.SessionState, error) {\n-\tvar session *sessionsapi.SessionState\n-\tvar err error\n-\tvar saveSession, clearSession, revalidated bool\n-\n-\tif p.skipJwtBearerTokens && req.Header.Get(\"Authorization\") != \"\" {\n-\t\tsession, err = p.GetJwtSession(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error retrieving session from token in Authorization header: %s\", err)\n-\t\t}\n-\t\tif session != nil {\n-\t\t\tsaveSession = false\n-\t\t}\n-\t}\n-\n-\tremoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n-\tif session == nil {\n-\t\tsession, err = p.LoadCookiedSession(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error loading cookied session: %s\", err)\n-\t\t}\n-\n-\t\tif session != nil {\n-\t\t\tif session.Age() > p.CookieRefresh && p.CookieRefresh != time.Duration(0) {\n-\t\t\t\tlogger.Printf(\"Refreshing %s old session cookie for %s (refresh after %s)\", session.Age(), session, p.CookieRefresh)\n-\t\t\t\tsaveSession = true\n-\t\t\t}\n-\n-\t\t\tif ok, err := p.provider.RefreshSessionIfNeeded(req.Context(), session); err != nil {\n-\t\t\t\tlogger.Printf(\"%s removing session. error refreshing access token %s %s\", remoteAddr, err, session)\n-\t\t\t\tclearSession = true\n-\t\t\t\tsession = nil\n-\t\t\t} else if ok {\n-\t\t\t\tsaveSession = true\n-\t\t\t\trevalidated = true\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif session != nil && session.IsExpired() {\n-\t\tlogger.Printf(\"Removing session: token expired %s\", session)\n-\t\tsession = nil\n-\t\tsaveSession = false\n-\t\tclearSession = true\n-\t}\n-\n-\tif saveSession && !revalidated && session != nil && session.AccessToken != \"\" {\n-\t\tif !p.provider.ValidateSessionState(req.Context(), session) {\n-\t\t\tlogger.Printf(\"Removing session: error validating %s\", session)\n-\t\t\tsaveSession = false\n-\t\t\tsession = nil\n-\t\t\tclearSession = true\n-\t\t}\n-\t}\n-\n-\tif session != nil && session.Email != \"\" && !p.Validator(session.Email) {\n-\t\tlogger.Printf(session.Email, req, logger.AuthFailure, \"Invalid authentication via session: removing session %s\", session)\n-\t\tsession = nil\n-\t\tsaveSession = false\n-\t\tclearSession = true\n-\t}\n-\n-\tif saveSession && session != nil {\n-\t\terr = p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.PrintAuthf(session.Email, req, logger.AuthError, \"Save session error %s\", err)\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\tif clearSession {\n-\t\tp.ClearSessionCookie(rw, req)\n-\t}\n-\n-\tif session == nil {\n-\t\tsession, err = p.CheckBasicAuth(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error during basic auth validation: %s\", err)\n-\t\t}\n-\t}\n-\n-\tif session == nil {\n-\t\treturn nil, ErrNeedsLogin\n-\t}\n-\n-\treturn session, nil\n+        var session *sessionsapi.SessionState\n+        var err error\n+        var saveSession, clearSession, revalidated bool\n+\n+        if p.skipJwtBearerTokens && req.Header.Get(\"Authorization\") != \"\" {\n+                session, err = p.GetJwtSession(req)\n+                if err != nil {\n+                        logger.Printf(\"Error retrieving session from token in Authorization header: %s\", err)\n+                }\n+                if session != nil {\n+                        saveSession = false\n+                }\n+        }\n+\n+        remoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n+        if session == nil {\n+                session, err = p.LoadCookiedSession(req)\n+                if err != nil {\n+                        logger.Printf(\"Error loading cookied session: %s\", err)\n+                }\n+\n+                if session != nil {\n+                        if session.Age() > p.CookieRefresh && p.CookieRefresh != time.Duration(0) {\n+                                logger.Printf(\"Refreshing %s old session cookie for %s (refresh after %s)\", session.Age(), session, p.CookieRefresh)\n+                                saveSession = true\n+                        }\n+\n+                        if ok, err := p.provider.RefreshSessionIfNeeded(req.Context(), session); err != nil {\n+                                logger.Printf(\"%s removing session. error refreshing access token %s %s\", remoteAddr, err, session)\n+                                clearSession = true\n+                                session = nil\n+                        } else if ok {\n+                                saveSession = true\n+                                revalidated = true\n+                        }\n+                }\n+        }\n+\n+        if session != nil && session.IsExpired() {\n+                logger.Printf(\"Removing session: token expired %s\", session)\n+                session = nil\n+                saveSession = false\n+                clearSession = true\n+        }\n+\n+        if saveSession && !revalidated && session != nil && session.AccessToken != \"\" {\n+                if !p.provider.ValidateSessionState(req.Context(), session) {\n+                        logger.Printf(\"Removing session: error validating %s\", session)\n+                        saveSession = false\n+                        session = nil\n+                        clearSession = true\n+                }\n+        }\n+\n+        if session != nil && session.Email != \"\" && !p.Validator(session.Email) {\n+                logger.Printf(session.Email, req, logger.AuthFailure, \"Invalid authentication via session: removing session %s\", session)\n+                session = nil\n+                saveSession = false\n+                clearSession = true\n+        }\n+\n+        if saveSession && session != nil {\n+                err = p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.PrintAuthf(session.Email, req, logger.AuthError, \"Save session error %s\", err)\n+                        return nil, err\n+                }\n+        }\n+\n+        if clearSession {\n+                p.ClearSessionCookie(rw, req)\n+        }\n+\n+        if session == nil {\n+                session, err = p.CheckBasicAuth(req)\n+                if err != nil {\n+                        logger.Printf(\"Error during basic auth validation: %s\", err)\n+                }\n+        }\n+\n+        if session == nil {\n+                return nil, ErrNeedsLogin\n+        }\n+\n+        return session, nil\n }\n \n // addHeadersForProxying adds the appropriate headers the request / response for proxying\n func (p *OAuthProxy) addHeadersForProxying(rw http.ResponseWriter, req *http.Request, session *sessionsapi.SessionState) {\n-\tif p.PassBasicAuth {\n-\t\tif p.PreferEmailToUser && session.Email != \"\" {\n-\t\t\treq.SetBasicAuth(session.Email, p.BasicAuthPassword)\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.Email}\n-\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t} else {\n-\t\t\treq.SetBasicAuth(session.User, p.BasicAuthPassword)\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.User}\n-\t\t\tif session.Email != \"\" {\n-\t\t\t\treq.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n-\t\t\t} else {\n-\t\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t\t}\n-\t\t}\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Preferred-Username\")\n-\t\t}\n-\t}\n-\n-\tif p.PassUserHeaders {\n-\t\tif p.PreferEmailToUser && session.Email != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.Email}\n-\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t} else {\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.User}\n-\t\t\tif session.Email != \"\" {\n-\t\t\t\treq.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n-\t\t\t} else {\n-\t\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t\t}\n-\t\t}\n-\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Preferred-Username\")\n-\t\t}\n-\t}\n-\n-\tif p.SetXAuthRequest {\n-\t\trw.Header().Set(\"X-Auth-Request-User\", session.User)\n-\t\tif session.Email != \"\" {\n-\t\t\trw.Header().Set(\"X-Auth-Request-Email\", session.Email)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"X-Auth-Request-Email\")\n-\t\t}\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\trw.Header().Set(\"X-Auth-Request-Preferred-Username\", session.PreferredUsername)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"X-Auth-Request-Preferred-Username\")\n-\t\t}\n-\n-\t\tif p.PassAccessToken {\n-\t\t\tif session.AccessToken != \"\" {\n-\t\t\t\trw.Header().Set(\"X-Auth-Request-Access-Token\", session.AccessToken)\n-\t\t\t} else {\n-\t\t\t\trw.Header().Del(\"X-Auth-Request-Access-Token\")\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif p.PassAccessToken {\n-\t\tif session.AccessToken != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Access-Token\"] = []string{session.AccessToken}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Access-Token\")\n-\t\t}\n-\t}\n-\n-\tif p.PassAuthorization {\n-\t\tif session.IDToken != \"\" {\n-\t\t\treq.Header[\"Authorization\"] = []string{fmt.Sprintf(\"Bearer %s\", session.IDToken)}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"Authorization\")\n-\t\t}\n-\t}\n-\tif p.SetBasicAuth {\n-\t\tswitch {\n-\t\tcase p.PreferEmailToUser && session.Email != \"\":\n-\t\t\tauthVal := b64.StdEncoding.EncodeToString([]byte(session.Email + \":\" + p.BasicAuthPassword))\n-\t\t\trw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n-\t\tcase session.User != \"\":\n-\t\t\tauthVal := b64.StdEncoding.EncodeToString([]byte(session.User + \":\" + p.BasicAuthPassword))\n-\t\t\trw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n-\t\tdefault:\n-\t\t\trw.Header().Del(\"Authorization\")\n-\t\t}\n-\t}\n-\tif p.SetAuthorization {\n-\t\tif session.IDToken != \"\" {\n-\t\t\trw.Header().Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", session.IDToken))\n-\t\t} else {\n-\t\t\trw.Header().Del(\"Authorization\")\n-\t\t}\n-\t}\n-\n-\tif session.Email == \"\" {\n-\t\trw.Header().Set(\"GAP-Auth\", session.User)\n-\t} else {\n-\t\trw.Header().Set(\"GAP-Auth\", session.Email)\n-\t}\n+        if p.PassBasicAuth {\n+                if p.PreferEmailToUser && session.Email != \"\" {\n+                        req.SetBasicAuth(session.Email, p.BasicAuthPassword)\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.Email}\n+                        req.Header.Del(\"X-Forwarded-Email\")\n+                } else {\n+                        req.SetBasicAuth(session.User, p.BasicAuthPassword)\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.User}\n+                        if session.Email != \"\" {\n+                                req.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n+                        } else {\n+                                req.Header.Del(\"X-Forwarded-Email\")\n+                        }\n+                }\n+                if session.PreferredUsername != \"\" {\n+                        req.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Preferred-Username\")\n+                }\n+        }\n+\n+        if p.PassUserHeaders {\n+                if p.PreferEmailToUser && session.Email != \"\" {\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.Email}\n+                        req.Header.Del(\"X-Forwarded-Email\")\n+                } else {\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.User}\n+                        if session.Email != \"\" {\n+                                req.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n+                        } else {\n+                                req.Header.Del(\"X-Forwarded-Email\")\n+                        }\n+                }\n+\n+                if session.PreferredUsername != \"\" {\n+                        req.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Preferred-Username\")\n+                }\n+        }\n+\n+        if p.SetXAuthRequest {\n+                rw.Header().Set(\"X-Auth-Request-User\", session.User)\n+                if session.Email != \"\" {\n+                        rw.Header().Set(\"X-Auth-Request-Email\", session.Email)\n+                } else {\n+                        rw.Header().Del(\"X-Auth-Request-Email\")\n+                }\n+                if session.PreferredUsername != \"\" {\n+                        rw.Header().Set(\"X-Auth-Request-Preferred-Username\", session.PreferredUsername)\n+                } else {\n+                        rw.Header().Del(\"X-Auth-Request-Preferred-Username\")\n+                }\n+\n+                if p.PassAccessToken {\n+                        if session.AccessToken != \"\" {\n+                                rw.Header().Set(\"X-Auth-Request-Access-Token\", session.AccessToken)\n+                        } else {\n+                                rw.Header().Del(\"X-Auth-Request-Access-Token\")\n+                        }\n+                }\n+        }\n+\n+        if p.PassAccessToken {\n+                if session.AccessToken != \"\" {\n+                        req.Header[\"X-Forwarded-Access-Token\"] = []string{session.AccessToken}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Access-Token\")\n+                }\n+        }\n+\n+        if p.PassAuthorization {\n+                if session.IDToken != \"\" {\n+                        req.Header[\"Authorization\"] = []string{fmt.Sprintf(\"Bearer %s\", session.IDToken)}\n+                } else {\n+                        req.Header.Del(\"Authorization\")\n+                }\n+        }\n+        if p.SetBasicAuth {\n+                switch {\n+                case p.PreferEmailToUser && session.Email != \"\":\n+                        authVal := b64.StdEncoding.EncodeToString([]byte(session.Email + \":\" + p.BasicAuthPassword))\n+                        rw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n+                case session.User != \"\":\n+                        authVal := b64.StdEncoding.EncodeToString([]byte(session.User + \":\" + p.BasicAuthPassword))\n+                        rw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n+                default:\n+                        rw.Header().Del(\"Authorization\")\n+                }\n+        }\n+        if p.SetAuthorization {\n+                if session.IDToken != \"\" {\n+                        rw.Header().Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", session.IDToken))\n+                } else {\n+                        rw.Header().Del(\"Authorization\")\n+                }\n+        }\n+\n+        if session.Email == \"\" {\n+                rw.Header().Set(\"GAP-Auth\", session.User)\n+        } else {\n+                rw.Header().Set(\"GAP-Auth\", session.Email)\n+        }\n }\n \n // CheckBasicAuth checks the requests Authorization header for basic auth\n // credentials and authenticates these against the proxies HtpasswdFile\n func (p *OAuthProxy) CheckBasicAuth(req *http.Request) (*sessionsapi.SessionState, error) {\n-\tif p.HtpasswdFile == nil {\n-\t\treturn nil, nil\n-\t}\n-\tauth := req.Header.Get(\"Authorization\")\n-\tif auth == \"\" {\n-\t\treturn nil, nil\n-\t}\n-\ts := strings.SplitN(auth, \" \", 2)\n-\tif len(s) != 2 || s[0] != \"Basic\" {\n-\t\treturn nil, fmt.Errorf(\"invalid Authorization header %s\", req.Header.Get(\"Authorization\"))\n-\t}\n-\tb, err := b64.StdEncoding.DecodeString(s[1])\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tpair := strings.SplitN(string(b), \":\", 2)\n-\tif len(pair) != 2 {\n-\t\treturn nil, fmt.Errorf(\"invalid format %s\", b)\n-\t}\n-\tif p.HtpasswdFile.Validate(pair[0], pair[1]) {\n-\t\tlogger.PrintAuthf(pair[0], req, logger.AuthSuccess, \"Authenticated via basic auth and HTpasswd File\")\n-\t\treturn &sessionsapi.SessionState{User: pair[0]}, nil\n-\t}\n-\tlogger.PrintAuthf(pair[0], req, logger.AuthFailure, \"Invalid authentication via basic auth: not in Htpasswd File\")\n-\treturn nil, nil\n+        if p.HtpasswdFile == nil {\n+                return nil, nil\n+        }\n+        auth := req.Header.Get(\"Authorization\")\n+        if auth == \"\" {\n+                return nil, nil\n+        }\n+        s := strings.SplitN(auth, \" \", 2)\n+        if len(s) != 2 || s[0] != \"Basic\" {\n+                return nil, fmt.Errorf(\"invalid Authorization header %s\", req.Header.Get(\"Authorization\"))\n+        }\n+        b, err := b64.StdEncoding.DecodeString(s[1])\n+        if err != nil {\n+                return nil, err\n+        }\n+        pair := strings.SplitN(string(b), \":\", 2)\n+        if len(pair) != 2 {\n+                return nil, fmt.Errorf(\"invalid format %s\", b)\n+        }\n+        if p.HtpasswdFile.Validate(pair[0], pair[1]) {\n+                logger.PrintAuthf(pair[0], req, logger.AuthSuccess, \"Authenticated via basic auth and HTpasswd File\")\n+                return &sessionsapi.SessionState{User: pair[0]}, nil\n+        }\n+        logger.PrintAuthf(pair[0], req, logger.AuthFailure, \"Invalid authentication via basic auth: not in Htpasswd File\")\n+        return nil, nil\n }\n \n // isAjax checks if a request is an ajax request\n func isAjax(req *http.Request) bool {\n-\tacceptValues := req.Header.Values(\"Accept\")\n-\tconst ajaxReq = applicationJSON\n-\tfor _, v := range acceptValues {\n-\t\tif v == ajaxReq {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        acceptValues := req.Header.Values(\"Accept\")\n+        const ajaxReq = applicationJSON\n+        for _, v := range acceptValues {\n+                if v == ajaxReq {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // ErrorJSON returns the error code with an application/json mime type\n func (p *OAuthProxy) ErrorJSON(rw http.ResponseWriter, code int) {\n-\trw.Header().Set(\"Content-Type\", applicationJSON)\n-\trw.WriteHeader(code)\n+        rw.Header().Set(\"Content-Type\", applicationJSON)\n+        rw.WriteHeader(code)\n }\n \n // GetJwtSession loads a session based on a JWT token in the authorization header.\n // (see the config options skip-jwt-bearer-tokens and extra-jwt-issuers)\n func (p *OAuthProxy) GetJwtSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\trawBearerToken, err := p.findBearerToken(req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// If we are using an oidc provider, go ahead and try that provider first with its Verifier\n-\t// and Bearer Token -> Session converter\n-\tif p.mainJwtBearerVerifier != nil {\n-\t\tbearerToken, err := p.mainJwtBearerVerifier.Verify(req.Context(), rawBearerToken)\n-\t\tif err == nil {\n-\t\t\treturn p.provider.CreateSessionStateFromBearerToken(req.Context(), rawBearerToken, bearerToken)\n-\t\t}\n-\t}\n-\n-\t// Otherwise, attempt to verify against the extra JWT issuers and use a more generic\n-\t// Bearer Token -> Session converter\n-\tfor _, verifier := range p.extraJwtBearerVerifiers {\n-\t\tbearerToken, err := verifier.Verify(req.Context(), rawBearerToken)\n-\t\tif err != nil {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\treturn (*providers.ProviderData)(nil).CreateSessionStateFromBearerToken(req.Context(), rawBearerToken, bearerToken)\n-\t}\n-\treturn nil, fmt.Errorf(\"unable to verify jwt token %s\", req.Header.Get(\"Authorization\"))\n+        rawBearerToken, err := p.findBearerToken(req)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // If we are using an oidc provider, go ahead and try that provider first with its Verifier\n+        // and Bearer Token -> Session converter\n+        if p.mainJwtBearerVerifier != nil {\n+                bearerToken, err := p.mainJwtBearerVerifier.Verify(req.Context(), rawBearerToken)\n+                if err == nil {\n+                        return p.provider.CreateSessionStateFromBearerToken(req.Context(), rawBearerToken, bearerToken)\n+                }\n+        }\n+\n+        // Otherwise, attempt to verify against the extra JWT issuers and use a more generic\n+        // Bearer Token -> Session converter\n+        for _, verifier := range p.extraJwtBearerVerifiers {\n+                bearerToken, err := verifier.Verify(req.Context(), rawBearerToken)\n+                if err != nil {\n+                        continue\n+                }\n+\n+                return (*providers.ProviderData)(nil).CreateSessionStateFromBearerToken(req.Context(), rawBearerToken, bearerToken)\n+        }\n+        return nil, fmt.Errorf(\"unable to verify jwt token %s\", req.Header.Get(\"Authorization\"))\n }\n \n // findBearerToken finds a valid JWT token from the Authorization header of a given request.\n func (p *OAuthProxy) findBearerToken(req *http.Request) (string, error) {\n-\tauth := req.Header.Get(\"Authorization\")\n-\ts := strings.SplitN(auth, \" \", 2)\n-\tif len(s) != 2 {\n-\t\treturn \"\", fmt.Errorf(\"invalid authorization header %s\", auth)\n-\t}\n-\tjwtRegex := regexp.MustCompile(`^eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]+$`)\n-\tvar rawBearerToken string\n-\tif s[0] == \"Bearer\" && jwtRegex.MatchString(s[1]) {\n-\t\trawBearerToken = s[1]\n-\t} else if s[0] == \"Basic\" {\n-\t\t// Check if we have a Bearer token masquerading in Basic\n-\t\tb, err := b64.StdEncoding.DecodeString(s[1])\n-\t\tif err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\tpair := strings.SplitN(string(b), \":\", 2)\n-\t\tif len(pair) != 2 {\n-\t\t\treturn \"\", fmt.Errorf(\"invalid format %s\", b)\n-\t\t}\n-\t\tuser, password := pair[0], pair[1]\n-\n-\t\t// check user, user+password, or just password for a token\n-\t\tif jwtRegex.MatchString(user) {\n-\t\t\t// Support blank passwords or magic `x-oauth-basic` passwords - nothing else\n-\t\t\tif password == \"\" || password == \"x-oauth-basic\" {\n-\t\t\t\trawBearerToken = user\n-\t\t\t}\n-\t\t} else if jwtRegex.MatchString(password) {\n-\t\t\t// support passwords and ignore user\n-\t\t\trawBearerToken = password\n-\t\t}\n-\t}\n-\tif rawBearerToken == \"\" {\n-\t\treturn \"\", fmt.Errorf(\"no valid bearer token found in authorization header\")\n-\t}\n-\n-\treturn rawBearerToken, nil\n+        auth := req.Header.Get(\"Authorization\")\n+        s := strings.SplitN(auth, \" \", 2)\n+        if len(s) != 2 {\n+                return \"\", fmt.Errorf(\"invalid authorization header %s\", auth)\n+        }\n+        jwtRegex := regexp.MustCompile(`^eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]+$`)\n+        var rawBearerToken string\n+        if s[0] == \"Bearer\" && jwtRegex.MatchString(s[1]) {\n+                rawBearerToken = s[1]\n+        } else if s[0] == \"Basic\" {\n+                // Check if we have a Bearer token masquerading in Basic\n+                b, err := b64.StdEncoding.DecodeString(s[1])\n+                if err != nil {\n+                        return \"\", err\n+                }\n+                pair := strings.SplitN(string(b), \":\", 2)\n+                if len(pair) != 2 {\n+                        return \"\", fmt.Errorf(\"invalid format %s\", b)\n+                }\n+                user, password := pair[0], pair[1]\n+\n+                // check user, user+password, or just password for a token\n+                if jwtRegex.MatchString(user) {\n+                        // Support blank passwords or magic `x-oauth-basic` passwords - nothing else\n+                        if password == \"\" || password == \"x-oauth-basic\" {\n+                                rawBearerToken = user\n+                        }\n+                } else if jwtRegex.MatchString(password) {\n+                        // support passwords and ignore user\n+                        rawBearerToken = password\n+                }\n+        }\n+        if rawBearerToken == \"\" {\n+                return \"\", fmt.Errorf(\"no valid bearer token found in authorization header\")\n+        }\n+\n+        return rawBearerToken, nil\n }\n"}
{"cve":"CVE-2024-25620:0708", "fix_patch": "diff --git a/pkg/chart/metadata.go b/pkg/chart/metadata.go\nindex 97bfc2c0c..88fae11d2 100644\n--- a/pkg/chart/metadata.go\n+++ b/pkg/chart/metadata.go\n@@ -3,6 +3,9 @@ Copyright The Helm Authors.\n Licensed under the Apache License, Version 2.0 (the \"License\");\n you may not use this file except in compliance with the License.\n You may obtain a copy of the License at\n+import (\n+\"strings\"\n+)\n \n http://www.apache.org/licenses/LICENSE-2.0\n \n@@ -16,157 +19,170 @@ limitations under the License.\n package chart\n \n import (\n-\t\"strings\"\n-\t\"unicode\"\n+        \"strings\"\n+        \"unicode\"\n \n-\t\"github.com/Masterminds/semver/v3\"\n+        \"github.com/Masterminds/semver/v3\"\n )\n \n // Maintainer describes a Chart maintainer.\n type Maintainer struct {\n-\t// Name is a user name or organization name\n-\tName string `json:\"name,omitempty\"`\n-\t// Email is an optional email address to contact the named maintainer\n-\tEmail string `json:\"email,omitempty\"`\n-\t// URL is an optional URL to an address for the named maintainer\n-\tURL string `json:\"url,omitempty\"`\n+        // Name is a user name or organization name\n+        Name string `json:\"name,omitempty\"`\n+        // Email is an optional email address to contact the named maintainer\n+        Email string `json:\"email,omitempty\"`\n+        // URL is an optional URL to an address for the named maintainer\n+        URL string `json:\"url,omitempty\"`\n }\n \n // Validate checks valid data and sanitizes string characters.\n func (m *Maintainer) Validate() error {\n-\tif m == nil {\n-\t\treturn ValidationError(\"maintainers must not contain empty or null nodes\")\n-\t}\n-\tm.Name = sanitizeString(m.Name)\n-\tm.Email = sanitizeString(m.Email)\n-\tm.URL = sanitizeString(m.URL)\n-\treturn nil\n+        if m == nil {\n+                return ValidationError(\"maintainers must not contain empty or null nodes\")\n+        }\n+        m.Name = sanitizeString(m.Name)\n+        m.Email = sanitizeString(m.Email)\n+        m.URL = sanitizeString(m.URL)\n+        return nil\n }\n \n // Metadata for a Chart file. This models the structure of a Chart.yaml file.\n type Metadata struct {\n-\t// The name of the chart. Required.\n-\tName string `json:\"name,omitempty\"`\n-\t// The URL to a relevant project page, git repo, or contact person\n-\tHome string `json:\"home,omitempty\"`\n-\t// Source is the URL to the source code of this chart\n-\tSources []string `json:\"sources,omitempty\"`\n-\t// A SemVer 2 conformant version string of the chart. Required.\n-\tVersion string `json:\"version,omitempty\"`\n-\t// A one-sentence description of the chart\n-\tDescription string `json:\"description,omitempty\"`\n-\t// A list of string keywords\n-\tKeywords []string `json:\"keywords,omitempty\"`\n-\t// A list of name and URL/email address combinations for the maintainer(s)\n-\tMaintainers []*Maintainer `json:\"maintainers,omitempty\"`\n-\t// The URL to an icon file.\n-\tIcon string `json:\"icon,omitempty\"`\n-\t// The API Version of this chart. Required.\n-\tAPIVersion string `json:\"apiVersion,omitempty\"`\n-\t// The condition to check to enable chart\n-\tCondition string `json:\"condition,omitempty\"`\n-\t// The tags to check to enable chart\n-\tTags string `json:\"tags,omitempty\"`\n-\t// The version of the application enclosed inside of this chart.\n-\tAppVersion string `json:\"appVersion,omitempty\"`\n-\t// Whether or not this chart is deprecated\n-\tDeprecated bool `json:\"deprecated,omitempty\"`\n-\t// Annotations are additional mappings uninterpreted by Helm,\n-\t// made available for inspection by other applications.\n-\tAnnotations map[string]string `json:\"annotations,omitempty\"`\n-\t// KubeVersion is a SemVer constraint specifying the version of Kubernetes required.\n-\tKubeVersion string `json:\"kubeVersion,omitempty\"`\n-\t// Dependencies are a list of dependencies for a chart.\n-\tDependencies []*Dependency `json:\"dependencies,omitempty\"`\n-\t// Specifies the chart type: application or library\n-\tType string `json:\"type,omitempty\"`\n+        // The name of the chart. Required.\n+        Name string `json:\"name,omitempty\"`\n+        // The URL to a relevant project page, git repo, or contact person\n+        Home string `json:\"home,omitempty\"`\n+        // Source is the URL to the source code of this chart\n+        Sources []string `json:\"sources,omitempty\"`\n+        // A SemVer 2 conformant version string of the chart. Required.\n+        Version string `json:\"version,omitempty\"`\n+        // A one-sentence description of the chart\n+        Description string `json:\"description,omitempty\"`\n+        // A list of string keywords\n+        Keywords []string `json:\"keywords,omitempty\"`\n+        // A list of name and URL/email address combinations for the maintainer(s)\n+        Maintainers []*Maintainer `json:\"maintainers,omitempty\"`\n+        // The URL to an icon file.\n+        Icon string `json:\"icon,omitempty\"`\n+        // The API Version of this chart. Required.\n+        APIVersion string `json:\"apiVersion,omitempty\"`\n+        // The condition to check to enable chart\n+        Condition string `json:\"condition,omitempty\"`\n+        // The tags to check to enable chart\n+        Tags string `json:\"tags,omitempty\"`\n+        // The version of the application enclosed inside of this chart.\n+        AppVersion string `json:\"appVersion,omitempty\"`\n+        // Whether or not this chart is deprecated\n+        Deprecated bool `json:\"deprecated,omitempty\"`\n+        // Annotations are additional mappings uninterpreted by Helm,\n+        // made available for inspection by other applications.\n+        Annotations map[string]string `json:\"annotations,omitempty\"`\n+        // KubeVersion is a SemVer constraint specifying the version of Kubernetes required.\n+        KubeVersion string `json:\"kubeVersion,omitempty\"`\n+        // Dependencies are a list of dependencies for a chart.\n+        Dependencies []*Dependency `json:\"dependencies,omitempty\"`\n+        // Specifies the chart type: application or library\n+        Type string `json:\"type,omitempty\"`\n }\n \n // Validate checks the metadata for known issues and sanitizes string\n // characters.\n func (md *Metadata) Validate() error {\n-\tif md == nil {\n-\t\treturn ValidationError(\"chart.metadata is required\")\n-\t}\n-\n-\tmd.Name = sanitizeString(md.Name)\n-\tmd.Description = sanitizeString(md.Description)\n-\tmd.Home = sanitizeString(md.Home)\n-\tmd.Icon = sanitizeString(md.Icon)\n-\tmd.Condition = sanitizeString(md.Condition)\n-\tmd.Tags = sanitizeString(md.Tags)\n-\tmd.AppVersion = sanitizeString(md.AppVersion)\n-\tmd.KubeVersion = sanitizeString(md.KubeVersion)\n-\tfor i := range md.Sources {\n-\t\tmd.Sources[i] = sanitizeString(md.Sources[i])\n-\t}\n-\tfor i := range md.Keywords {\n-\t\tmd.Keywords[i] = sanitizeString(md.Keywords[i])\n-\t}\n-\n-\tif md.APIVersion == \"\" {\n-\t\treturn ValidationError(\"chart.metadata.apiVersion is required\")\n-\t}\n-\tif md.Name == \"\" {\n-\t\treturn ValidationError(\"chart.metadata.name is required\")\n-\t}\n-\tif md.Version == \"\" {\n-\t\treturn ValidationError(\"chart.metadata.version is required\")\n-\t}\n-\tif !isValidSemver(md.Version) {\n-\t\treturn ValidationErrorf(\"chart.metadata.version %q is invalid\", md.Version)\n-\t}\n-\tif !isValidChartType(md.Type) {\n-\t\treturn ValidationError(\"chart.metadata.type must be application or library\")\n-\t}\n-\n-\tfor _, m := range md.Maintainers {\n-\t\tif err := m.Validate(); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\t// Aliases need to be validated here to make sure that the alias name does\n-\t// not contain any illegal characters.\n-\tdependencies := map[string]*Dependency{}\n-\tfor _, dependency := range md.Dependencies {\n-\t\tif err := dependency.Validate(); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tkey := dependency.Name\n-\t\tif dependency.Alias != \"\" {\n-\t\t\tkey = dependency.Alias\n-\t\t}\n-\t\tif dependencies[key] != nil {\n-\t\t\treturn ValidationErrorf(\"more than one dependency with name or alias %q\", key)\n-\t\t}\n-\t\tdependencies[key] = dependency\n-\t}\n-\treturn nil\n+        if md == nil {\n+                return ValidationError(\"chart.metadata is required\")\n+        }\n+\n+        md.Name = sanitizeString(md.Name)\n+        md.Description = sanitizeString(md.Description)\n+        md.Home = sanitizeString(md.Home)\n+        md.Icon = sanitizeString(md.Icon)\n+        md.Condition = sanitizeString(md.Condition)\n+        md.Tags = sanitizeString(md.Tags)\n+        md.AppVersion = sanitizeString(md.AppVersion)\n+        md.KubeVersion = sanitizeString(md.KubeVersion)\n+        for i := range md.Sources {\n+                md.Sources[i] = sanitizeString(md.Sources[i])\n+        }\n+        for i := range md.Keywords {\n+                md.Keywords[i] = sanitizeString(md.Keywords[i])\n+        }\n+\n+        if md.APIVersion == \"\" {\n+                return ValidationError(\"chart.metadata.apiVersion is required\")\n+        }\n+\n+        if md.Name == \"\" {\n+                return ValidationError(\"chart.metadata.name is required\")\n+        }\n+                return ValidationErrorf(\"chart.metadata.name %q is invalid\", md.Name)\n+        }\n+        if md.Name == \".\" || md.Name == \"..\" {\n+                return ValidationErrorf(\"chart.metadata.name %q is invalid\", md.Name)\n+        }\n+        }\n+        if strings.Contains(md.Name, \"/\") || strings.Contains(md.Name, \"\\\\\") {\n+                return ValidationErrorf(\"chart.metadata.name %q is invalid\", md.Name)\n+        }\n+        if md.Name == \".\" || md.Name == \"..\" {\n+                return ValidationErrorf(\"chart.metadata.name %q is invalid\", md.Name)\n+        }\n+        if md.Version == \"\" {\n+                return ValidationError(\"chart.metadata.version is required\")\n+        }\n+        if !isValidSemver(md.Version) {\n+                return ValidationErrorf(\"chart.metadata.version %q is invalid\", md.Version)\n+        }\n+        if !isValidChartType(md.Type) {\n+                return ValidationError(\"chart.metadata.type must be application or library\")\n+        }\n+\n+        for _, m := range md.Maintainers {\n+                if err := m.Validate(); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // Aliases need to be validated here to make sure that the alias name does\n+        // not contain any illegal characters.\n+        dependencies := map[string]*Dependency{}\n+        for _, dependency := range md.Dependencies {\n+                if err := dependency.Validate(); err != nil {\n+                        return err\n+                }\n+                key := dependency.Name\n+                if dependency.Alias != \"\" {\n+                        key = dependency.Alias\n+                }\n+                if dependencies[key] != nil {\n+                        return ValidationErrorf(\"more than one dependency with name or alias %q\", key)\n+                }\n+                dependencies[key] = dependency\n+        }\n+        return nil\n }\n \n func isValidChartType(in string) bool {\n-\tswitch in {\n-\tcase \"\", \"application\", \"library\":\n-\t\treturn true\n-\t}\n-\treturn false\n+        switch in {\n+        case \"\", \"application\", \"library\":\n+                return true\n+        }\n+        return false\n }\n \n func isValidSemver(v string) bool {\n-\t_, err := semver.NewVersion(v)\n-\treturn err == nil\n+        _, err := semver.NewVersion(v)\n+        return err == nil\n }\n \n // sanitizeString normalize spaces and removes non-printable characters.\n func sanitizeString(str string) string {\n-\treturn strings.Map(func(r rune) rune {\n-\t\tif unicode.IsSpace(r) {\n-\t\t\treturn ' '\n-\t\t}\n-\t\tif unicode.IsPrint(r) {\n-\t\t\treturn r\n-\t\t}\n-\t\treturn -1\n-\t}, str)\n+        return strings.Map(func(r rune) rune {\n+                if unicode.IsSpace(r) {\n+                        return ' '\n+                }\n+                if unicode.IsPrint(r) {\n+                        return r\n+                }\n+                return -1\n+        }, str)\n }\n"}
{"cve":"CVE-2022-2024:0708", "fix_patch": "diff --git a/internal/db/repo_editor.go b/internal/db/repo_editor.go\nindex 3edb16e2..749f6dc1 100644\n--- a/internal/db/repo_editor.go\n+++ b/internal/db/repo_editor.go\n@@ -5,65 +5,65 @@\n package db\n \n import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"mime/multipart\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/pkg/errors\"\n-\tgouuid \"github.com/satori/go.uuid\"\n-\t\"github.com/unknwon/com\"\n-\n-\t\"github.com/gogs/git-module\"\n-\n-\t\"gogs.io/gogs/internal/conf\"\n-\t\"gogs.io/gogs/internal/cryptoutil\"\n-\tdberrors \"gogs.io/gogs/internal/db/errors\"\n-\t\"gogs.io/gogs/internal/gitutil\"\n-\t\"gogs.io/gogs/internal/osutil\"\n-\t\"gogs.io/gogs/internal/pathutil\"\n-\t\"gogs.io/gogs/internal/process\"\n-\t\"gogs.io/gogs/internal/tool\"\n+        \"fmt\"\n+        \"io\"\n+        \"mime/multipart\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path\"\n+        \"path/filepath\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/pkg/errors\"\n+        gouuid \"github.com/satori/go.uuid\"\n+        \"github.com/unknwon/com\"\n+\n+        \"github.com/gogs/git-module\"\n+\n+        \"gogs.io/gogs/internal/conf\"\n+        \"gogs.io/gogs/internal/cryptoutil\"\n+        dberrors \"gogs.io/gogs/internal/db/errors\"\n+        \"gogs.io/gogs/internal/gitutil\"\n+        \"gogs.io/gogs/internal/osutil\"\n+        \"gogs.io/gogs/internal/pathutil\"\n+        \"gogs.io/gogs/internal/process\"\n+        \"gogs.io/gogs/internal/tool\"\n )\n \n const (\n-\tENV_AUTH_USER_ID           = \"GOGS_AUTH_USER_ID\"\n-\tENV_AUTH_USER_NAME         = \"GOGS_AUTH_USER_NAME\"\n-\tENV_AUTH_USER_EMAIL        = \"GOGS_AUTH_USER_EMAIL\"\n-\tENV_REPO_OWNER_NAME        = \"GOGS_REPO_OWNER_NAME\"\n-\tENV_REPO_OWNER_SALT_MD5    = \"GOGS_REPO_OWNER_SALT_MD5\"\n-\tENV_REPO_ID                = \"GOGS_REPO_ID\"\n-\tENV_REPO_NAME              = \"GOGS_REPO_NAME\"\n-\tENV_REPO_CUSTOM_HOOKS_PATH = \"GOGS_REPO_CUSTOM_HOOKS_PATH\"\n+        ENV_AUTH_USER_ID           = \"GOGS_AUTH_USER_ID\"\n+        ENV_AUTH_USER_NAME         = \"GOGS_AUTH_USER_NAME\"\n+        ENV_AUTH_USER_EMAIL        = \"GOGS_AUTH_USER_EMAIL\"\n+        ENV_REPO_OWNER_NAME        = \"GOGS_REPO_OWNER_NAME\"\n+        ENV_REPO_OWNER_SALT_MD5    = \"GOGS_REPO_OWNER_SALT_MD5\"\n+        ENV_REPO_ID                = \"GOGS_REPO_ID\"\n+        ENV_REPO_NAME              = \"GOGS_REPO_NAME\"\n+        ENV_REPO_CUSTOM_HOOKS_PATH = \"GOGS_REPO_CUSTOM_HOOKS_PATH\"\n )\n \n type ComposeHookEnvsOptions struct {\n-\tAuthUser  *User\n-\tOwnerName string\n-\tOwnerSalt string\n-\tRepoID    int64\n-\tRepoName  string\n-\tRepoPath  string\n+        AuthUser  *User\n+        OwnerName string\n+        OwnerSalt string\n+        RepoID    int64\n+        RepoName  string\n+        RepoPath  string\n }\n \n func ComposeHookEnvs(opts ComposeHookEnvsOptions) []string {\n-\tenvs := []string{\n-\t\t\"SSH_ORIGINAL_COMMAND=1\",\n-\t\tENV_AUTH_USER_ID + \"=\" + com.ToStr(opts.AuthUser.ID),\n-\t\tENV_AUTH_USER_NAME + \"=\" + opts.AuthUser.Name,\n-\t\tENV_AUTH_USER_EMAIL + \"=\" + opts.AuthUser.Email,\n-\t\tENV_REPO_OWNER_NAME + \"=\" + opts.OwnerName,\n-\t\tENV_REPO_OWNER_SALT_MD5 + \"=\" + cryptoutil.MD5(opts.OwnerSalt),\n-\t\tENV_REPO_ID + \"=\" + com.ToStr(opts.RepoID),\n-\t\tENV_REPO_NAME + \"=\" + opts.RepoName,\n-\t\tENV_REPO_CUSTOM_HOOKS_PATH + \"=\" + filepath.Join(opts.RepoPath, \"custom_hooks\"),\n-\t}\n-\treturn envs\n+        envs := []string{\n+                \"SSH_ORIGINAL_COMMAND=1\",\n+                ENV_AUTH_USER_ID + \"=\" + com.ToStr(opts.AuthUser.ID),\n+                ENV_AUTH_USER_NAME + \"=\" + opts.AuthUser.Name,\n+                ENV_AUTH_USER_EMAIL + \"=\" + opts.AuthUser.Email,\n+                ENV_REPO_OWNER_NAME + \"=\" + opts.OwnerName,\n+                ENV_REPO_OWNER_SALT_MD5 + \"=\" + cryptoutil.MD5(opts.OwnerSalt),\n+                ENV_REPO_ID + \"=\" + com.ToStr(opts.RepoID),\n+                ENV_REPO_NAME + \"=\" + opts.RepoName,\n+                ENV_REPO_CUSTOM_HOOKS_PATH + \"=\" + filepath.Join(opts.RepoPath, \"custom_hooks\"),\n+        }\n+        return envs\n }\n \n // ___________    .___.__  __    ___________.__.__\n@@ -76,194 +76,194 @@ func ComposeHookEnvs(opts ComposeHookEnvsOptions) []string {\n // discardLocalRepoBranchChanges discards local commits/changes of\n // given branch to make sure it is even to remote branch.\n func discardLocalRepoBranchChanges(localPath, branch string) error {\n-\tif !com.IsExist(localPath) {\n-\t\treturn nil\n-\t}\n-\n-\t// No need to check if nothing in the repository.\n-\tif !git.RepoHasBranch(localPath, branch) {\n-\t\treturn nil\n-\t}\n-\n-\trev := \"origin/\" + branch\n-\tif err := git.Reset(localPath, rev, git.ResetOptions{Hard: true}); err != nil {\n-\t\treturn fmt.Errorf(\"reset [revision: %s]: %v\", rev, err)\n-\t}\n-\treturn nil\n+        if !com.IsExist(localPath) {\n+                return nil\n+        }\n+\n+        // No need to check if nothing in the repository.\n+        if !git.RepoHasBranch(localPath, branch) {\n+                return nil\n+        }\n+\n+        rev := \"origin/\" + branch\n+        if err := git.Reset(localPath, rev, git.ResetOptions{Hard: true}); err != nil {\n+                return fmt.Errorf(\"reset [revision: %s]: %v\", rev, err)\n+        }\n+        return nil\n }\n \n func (repo *Repository) DiscardLocalRepoBranchChanges(branch string) error {\n-\treturn discardLocalRepoBranchChanges(repo.LocalCopyPath(), branch)\n+        return discardLocalRepoBranchChanges(repo.LocalCopyPath(), branch)\n }\n \n // CheckoutNewBranch checks out to a new branch from the a branch name.\n func (repo *Repository) CheckoutNewBranch(oldBranch, newBranch string) error {\n-\tif err := git.Checkout(repo.LocalCopyPath(), newBranch, git.CheckoutOptions{\n-\t\tBaseBranch: oldBranch,\n-\t\tTimeout:    time.Duration(conf.Git.Timeout.Pull) * time.Second,\n-\t}); err != nil {\n-\t\treturn fmt.Errorf(\"checkout [base: %s, new: %s]: %v\", oldBranch, newBranch, err)\n-\t}\n-\treturn nil\n+        if err := git.Checkout(repo.LocalCopyPath(), newBranch, git.CheckoutOptions{\n+                BaseBranch: oldBranch,\n+                Timeout:    time.Duration(conf.Git.Timeout.Pull) * time.Second,\n+        }); err != nil {\n+                return fmt.Errorf(\"checkout [base: %s, new: %s]: %v\", oldBranch, newBranch, err)\n+        }\n+        return nil\n }\n \n type UpdateRepoFileOptions struct {\n-\tOldBranch   string\n-\tNewBranch   string\n-\tOldTreeName string\n-\tNewTreeName string\n-\tMessage     string\n-\tContent     string\n-\tIsNewFile   bool\n+        OldBranch   string\n+        NewBranch   string\n+        OldTreeName string\n+        NewTreeName string\n+        Message     string\n+        Content     string\n+        IsNewFile   bool\n }\n \n // UpdateRepoFile adds or updates a file in repository.\n func (repo *Repository) UpdateRepoFile(doer *User, opts UpdateRepoFileOptions) (err error) {\n-\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\tif isRepositoryGitPath(opts.NewTreeName) {\n-\t\treturn errors.Errorf(\"bad tree path %q\", opts.NewTreeName)\n-\t}\n-\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\trepoPath := repo.RepoPath()\n-\tlocalPath := repo.LocalCopyPath()\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\t// Directly return error if new branch already exists in the server\n-\t\tif git.RepoHasBranch(repoPath, opts.NewBranch) {\n-\t\t\treturn dberrors.BranchAlreadyExists{Name: opts.NewBranch}\n-\t\t}\n-\n-\t\t// Otherwise, delete branch from local copy in case out of sync\n-\t\tif git.RepoHasBranch(localPath, opts.NewBranch) {\n-\t\t\tif err = git.DeleteBranch(localPath, opts.NewBranch, git.DeleteBranchOptions{\n-\t\t\t\tForce: true,\n-\t\t\t}); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"delete branch %q: %v\", opts.NewBranch, err)\n-\t\t\t}\n-\t\t}\n-\n-\t\tif err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\toldFilePath := path.Join(localPath, opts.OldTreeName)\n-\tfilePath := path.Join(localPath, opts.NewTreeName)\n-\tif err = os.MkdirAll(path.Dir(filePath), os.ModePerm); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// If it's meant to be a new file, make sure it doesn't exist.\n-\tif opts.IsNewFile {\n-\t\tif com.IsExist(filePath) {\n-\t\t\treturn ErrRepoFileAlreadyExist{filePath}\n-\t\t}\n-\t}\n-\n-\t// Ignore move step if it's a new file under a directory.\n-\t// Otherwise, move the file when name changed.\n-\tif osutil.IsFile(oldFilePath) && opts.OldTreeName != opts.NewTreeName {\n-\t\tif err = git.Move(localPath, opts.OldTreeName, opts.NewTreeName); err != nil {\n-\t\t\treturn fmt.Errorf(\"git mv %q %q: %v\", opts.OldTreeName, opts.NewTreeName, err)\n-\t\t}\n-\t}\n-\n-\tif err = os.WriteFile(filePath, []byte(opts.Content), 0600); err != nil {\n-\t\treturn fmt.Errorf(\"write file: %v\", err)\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t}\n-\n-\terr = git.CreateCommit(\n-\t\tlocalPath,\n-\t\t&git.Signature{\n-\t\t\tName:  doer.DisplayName(),\n-\t\t\tEmail: doer.Email,\n-\t\t\tWhen:  time.Now(),\n-\t\t},\n-\t\topts.Message,\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\treturn nil\n+        // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+        if isRepositoryGitPath(opts.NewTreeName) {\n+                return errors.Errorf(\"bad tree path %q\", opts.NewTreeName)\n+        }\n+\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        repoPath := repo.RepoPath()\n+        localPath := repo.LocalCopyPath()\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                // Directly return error if new branch already exists in the server\n+                if git.RepoHasBranch(repoPath, opts.NewBranch) {\n+                        return dberrors.BranchAlreadyExists{Name: opts.NewBranch}\n+                }\n+\n+                // Otherwise, delete branch from local copy in case out of sync\n+                if git.RepoHasBranch(localPath, opts.NewBranch) {\n+                        if err = git.DeleteBranch(localPath, opts.NewBranch, git.DeleteBranchOptions{\n+                                Force: true,\n+                        }); err != nil {\n+                                return fmt.Errorf(\"delete branch %q: %v\", opts.NewBranch, err)\n+                        }\n+                }\n+\n+                if err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        oldFilePath := path.Join(localPath, opts.OldTreeName)\n+        filePath := path.Join(localPath, opts.NewTreeName)\n+        if err = os.MkdirAll(path.Dir(filePath), os.ModePerm); err != nil {\n+                return err\n+        }\n+\n+        // If it's meant to be a new file, make sure it doesn't exist.\n+        if opts.IsNewFile {\n+                if com.IsExist(filePath) {\n+                        return ErrRepoFileAlreadyExist{filePath}\n+                }\n+        }\n+\n+        // Ignore move step if it's a new file under a directory.\n+        // Otherwise, move the file when name changed.\n+        if osutil.IsFile(oldFilePath) && opts.OldTreeName != opts.NewTreeName {\n+                if err = git.Move(localPath, opts.OldTreeName, opts.NewTreeName); err != nil {\n+                        return fmt.Errorf(\"git mv %q %q: %v\", opts.OldTreeName, opts.NewTreeName, err)\n+                }\n+        }\n+\n+        if err = os.WriteFile(filePath, []byte(opts.Content), 0600); err != nil {\n+                return fmt.Errorf(\"write file: %v\", err)\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        }\n+\n+        err = git.CreateCommit(\n+                localPath,\n+                &git.Signature{\n+                        Name:  doer.DisplayName(),\n+                        Email: doer.Email,\n+                        When:  time.Now(),\n+                },\n+                opts.Message,\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+        return nil\n }\n \n // GetDiffPreview produces and returns diff result of a file which is not yet committed.\n func (repo *Repository) GetDiffPreview(branch, treePath, content string) (diff *gitutil.Diff, err error) {\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(branch); err != nil {\n-\t\treturn nil, fmt.Errorf(\"discard local repo branch[%s] changes: %v\", branch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(branch); err != nil {\n-\t\treturn nil, fmt.Errorf(\"update local copy branch[%s]: %v\", branch, err)\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tfilePath := path.Join(localPath, treePath)\n-\tif err = os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif err = os.WriteFile(filePath, []byte(content), 0600); err != nil {\n-\t\treturn nil, fmt.Errorf(\"write file: %v\", err)\n-\t}\n-\n-\tcmd := exec.Command(\"git\", \"diff\", treePath)\n-\tcmd.Dir = localPath\n-\tcmd.Stderr = os.Stderr\n-\n-\tstdout, err := cmd.StdoutPipe()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"get stdout pipe: %v\", err)\n-\t}\n-\n-\tif err = cmd.Start(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"start: %v\", err)\n-\t}\n-\n-\tpid := process.Add(fmt.Sprintf(\"GetDiffPreview [repo_path: %s]\", repo.RepoPath()), cmd)\n-\tdefer process.Remove(pid)\n-\n-\tdiff, err = gitutil.ParseDiff(stdout, conf.Git.MaxDiffFiles, conf.Git.MaxDiffLines, conf.Git.MaxDiffLineChars)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"parse diff: %v\", err)\n-\t}\n-\n-\tif err = cmd.Wait(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"wait: %v\", err)\n-\t}\n-\n-\treturn diff, nil\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(branch); err != nil {\n+                return nil, fmt.Errorf(\"discard local repo branch[%s] changes: %v\", branch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(branch); err != nil {\n+                return nil, fmt.Errorf(\"update local copy branch[%s]: %v\", branch, err)\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        filePath := path.Join(localPath, treePath)\n+        if err = os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {\n+                return nil, err\n+        }\n+        if err = os.WriteFile(filePath, []byte(content), 0600); err != nil {\n+                return nil, fmt.Errorf(\"write file: %v\", err)\n+        }\n+\n+        cmd := exec.Command(\"git\", \"diff\", treePath)\n+        cmd.Dir = localPath\n+        cmd.Stderr = os.Stderr\n+\n+        stdout, err := cmd.StdoutPipe()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"get stdout pipe: %v\", err)\n+        }\n+\n+        if err = cmd.Start(); err != nil {\n+                return nil, fmt.Errorf(\"start: %v\", err)\n+        }\n+\n+        pid := process.Add(fmt.Sprintf(\"GetDiffPreview [repo_path: %s]\", repo.RepoPath()), cmd)\n+        defer process.Remove(pid)\n+\n+        diff, err = gitutil.ParseDiff(stdout, conf.Git.MaxDiffFiles, conf.Git.MaxDiffLines, conf.Git.MaxDiffLineChars)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"parse diff: %v\", err)\n+        }\n+\n+        if err = cmd.Wait(); err != nil {\n+                return nil, fmt.Errorf(\"wait: %v\", err)\n+        }\n+\n+        return diff, nil\n }\n \n // ________         .__          __           ___________.__.__\n@@ -275,69 +275,69 @@ func (repo *Repository) GetDiffPreview(branch, treePath, content string) (diff *\n //\n \n type DeleteRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tTreePath     string\n-\tMessage      string\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        TreePath     string\n+        Message      string\n }\n \n func (repo *Repository) DeleteRepoFile(doer *User, opts DeleteRepoFileOptions) (err error) {\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\tif err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tif err = os.Remove(path.Join(localPath, opts.TreePath)); err != nil {\n-\t\treturn fmt.Errorf(\"remove file %q: %v\", opts.TreePath, err)\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t}\n-\n-\terr = git.CreateCommit(\n-\t\tlocalPath,\n-\t\t&git.Signature{\n-\t\t\tName:  doer.DisplayName(),\n-\t\t\tEmail: doer.Email,\n-\t\t\tWhen:  time.Now(),\n-\t\t},\n-\t\topts.Message,\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"commit changes to %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\treturn nil\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                if err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        if err = os.Remove(path.Join(localPath, opts.TreePath)); err != nil {\n+                return fmt.Errorf(\"remove file %q: %v\", opts.TreePath, err)\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        }\n+\n+        err = git.CreateCommit(\n+                localPath,\n+                &git.Signature{\n+                        Name:  doer.DisplayName(),\n+                        Email: doer.Email,\n+                        When:  time.Now(),\n+                },\n+                opts.Message,\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"commit changes to %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+        return nil\n }\n \n //  ____ ___        .__                    .___ ___________.___.__\n@@ -350,241 +350,246 @@ func (repo *Repository) DeleteRepoFile(doer *User, opts DeleteRepoFileOptions) (\n \n // Upload represent a uploaded file to a repo to be deleted when moved\n type Upload struct {\n-\tID   int64\n-\tUUID string `xorm:\"uuid UNIQUE\"`\n-\tName string\n+        ID   int64\n+        UUID string `xorm:\"uuid UNIQUE\"`\n+        Name string\n }\n \n // UploadLocalPath returns where uploads is stored in local file system based on given UUID.\n func UploadLocalPath(uuid string) string {\n-\treturn path.Join(conf.Repository.Upload.TempPath, uuid[0:1], uuid[1:2], uuid)\n+        return path.Join(conf.Repository.Upload.TempPath, uuid[0:1], uuid[1:2], uuid)\n }\n \n // LocalPath returns where uploads are temporarily stored in local file system.\n func (upload *Upload) LocalPath() string {\n-\treturn UploadLocalPath(upload.UUID)\n+        return UploadLocalPath(upload.UUID)\n }\n \n // NewUpload creates a new upload object.\n func NewUpload(name string, buf []byte, file multipart.File) (_ *Upload, err error) {\n-\tif tool.IsMaliciousPath(name) {\n-\t\treturn nil, fmt.Errorf(\"malicious path detected: %s\", name)\n-\t}\n-\n-\tupload := &Upload{\n-\t\tUUID: gouuid.NewV4().String(),\n-\t\tName: name,\n-\t}\n-\n-\tlocalPath := upload.LocalPath()\n-\tif err = os.MkdirAll(path.Dir(localPath), os.ModePerm); err != nil {\n-\t\treturn nil, fmt.Errorf(\"mkdir all: %v\", err)\n-\t}\n-\n-\tfw, err := os.Create(localPath)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"create: %v\", err)\n-\t}\n-\tdefer func() { _ = fw.Close() }()\n-\n-\tif _, err = fw.Write(buf); err != nil {\n-\t\treturn nil, fmt.Errorf(\"write: %v\", err)\n-\t} else if _, err = io.Copy(fw, file); err != nil {\n-\t\treturn nil, fmt.Errorf(\"copy: %v\", err)\n-\t}\n-\n-\tif _, err := x.Insert(upload); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn upload, nil\n+        if tool.IsMaliciousPath(name) {\n+                return nil, fmt.Errorf(\"malicious path detected: %s\", name)\n+        }\n+\n+        upload := &Upload{\n+                UUID: gouuid.NewV4().String(),\n+                Name: name,\n+        }\n+\n+        localPath := upload.LocalPath()\n+        if err = os.MkdirAll(path.Dir(localPath), os.ModePerm); err != nil {\n+                return nil, fmt.Errorf(\"mkdir all: %v\", err)\n+        }\n+\n+        fw, err := os.Create(localPath)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"create: %v\", err)\n+        }\n+        defer func() { _ = fw.Close() }()\n+\n+        if _, err = fw.Write(buf); err != nil {\n+                return nil, fmt.Errorf(\"write: %v\", err)\n+        } else if _, err = io.Copy(fw, file); err != nil {\n+                return nil, fmt.Errorf(\"copy: %v\", err)\n+        }\n+\n+        if _, err := x.Insert(upload); err != nil {\n+                return nil, err\n+        }\n+\n+        return upload, nil\n }\n \n func GetUploadByUUID(uuid string) (*Upload, error) {\n-\tupload := &Upload{UUID: uuid}\n-\thas, err := x.Get(upload)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t} else if !has {\n-\t\treturn nil, ErrUploadNotExist{0, uuid}\n-\t}\n-\treturn upload, nil\n+        upload := &Upload{UUID: uuid}\n+        has, err := x.Get(upload)\n+        if err != nil {\n+                return nil, err\n+        } else if !has {\n+                return nil, ErrUploadNotExist{0, uuid}\n+        }\n+        return upload, nil\n }\n \n func GetUploadsByUUIDs(uuids []string) ([]*Upload, error) {\n-\tif len(uuids) == 0 {\n-\t\treturn []*Upload{}, nil\n-\t}\n+        if len(uuids) == 0 {\n+                return []*Upload{}, nil\n+        }\n \n-\t// Silently drop invalid uuids.\n-\tuploads := make([]*Upload, 0, len(uuids))\n-\treturn uploads, x.In(\"uuid\", uuids).Find(&uploads)\n+        // Silently drop invalid uuids.\n+        uploads := make([]*Upload, 0, len(uuids))\n+        return uploads, x.In(\"uuid\", uuids).Find(&uploads)\n }\n \n func DeleteUploads(uploads ...*Upload) (err error) {\n-\tif len(uploads) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tsess := x.NewSession()\n-\tdefer sess.Close()\n-\tif err = sess.Begin(); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tids := make([]int64, len(uploads))\n-\tfor i := 0; i < len(uploads); i++ {\n-\t\tids[i] = uploads[i].ID\n-\t}\n-\tif _, err = sess.In(\"id\", ids).Delete(new(Upload)); err != nil {\n-\t\treturn fmt.Errorf(\"delete uploads: %v\", err)\n-\t}\n-\n-\tfor _, upload := range uploads {\n-\t\tlocalPath := upload.LocalPath()\n-\t\tif !osutil.IsFile(localPath) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif err := os.Remove(localPath); err != nil {\n-\t\t\treturn fmt.Errorf(\"remove upload: %v\", err)\n-\t\t}\n-\t}\n-\n-\treturn sess.Commit()\n+        if len(uploads) == 0 {\n+                return nil\n+        }\n+\n+        sess := x.NewSession()\n+        defer sess.Close()\n+        if err = sess.Begin(); err != nil {\n+                return err\n+        }\n+\n+        ids := make([]int64, len(uploads))\n+        for i := 0; i < len(uploads); i++ {\n+                ids[i] = uploads[i].ID\n+        }\n+        if _, err = sess.In(\"id\", ids).Delete(new(Upload)); err != nil {\n+                return fmt.Errorf(\"delete uploads: %v\", err)\n+        }\n+\n+        for _, upload := range uploads {\n+                localPath := upload.LocalPath()\n+                if !osutil.IsFile(localPath) {\n+                        continue\n+                }\n+\n+                if err := os.Remove(localPath); err != nil {\n+                        return fmt.Errorf(\"remove upload: %v\", err)\n+                }\n+        }\n+\n+        return sess.Commit()\n }\n \n func DeleteUpload(u *Upload) error {\n-\treturn DeleteUploads(u)\n+        return DeleteUploads(u)\n }\n \n func DeleteUploadByUUID(uuid string) error {\n-\tupload, err := GetUploadByUUID(uuid)\n-\tif err != nil {\n-\t\tif IsErrUploadNotExist(err) {\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn fmt.Errorf(\"get upload by UUID[%s]: %v\", uuid, err)\n-\t}\n-\n-\tif err := DeleteUpload(upload); err != nil {\n-\t\treturn fmt.Errorf(\"delete upload: %v\", err)\n-\t}\n-\n-\treturn nil\n+        upload, err := GetUploadByUUID(uuid)\n+        if err != nil {\n+                if IsErrUploadNotExist(err) {\n+                        return nil\n+                }\n+                return fmt.Errorf(\"get upload by UUID[%s]: %v\", uuid, err)\n+        }\n+\n+        if err := DeleteUpload(upload); err != nil {\n+                return fmt.Errorf(\"delete upload: %v\", err)\n+        }\n+\n+        return nil\n }\n \n type UploadRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tTreePath     string\n-\tMessage      string\n-\tFiles        []string // In UUID format\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        TreePath     string\n+        Message      string\n+        Files        []string // In UUID format\n }\n \n // isRepositoryGitPath returns true if given path is or resides inside \".git\"\n // path of the repository.\n func isRepositoryGitPath(path string) bool {\n-\treturn strings.HasSuffix(path, \".git\") ||\n-\t\tstrings.Contains(path, \".git/\") ||\n-\t\tstrings.Contains(path, `.git\\`) ||\n-\t\t// Windows treats \".git.\" the same as \".git\"\n-\t\tstrings.HasSuffix(path, \".git.\") ||\n-\t\tstrings.Contains(path, \".git./\") ||\n-\t\tstrings.Contains(path, `.git.\\`)\n+\n+        path = strings.TrimSpace(path)\n+        path = strings.ToLower(path)\n+        path = strings.TrimSpace(path)\n+        path = strings.ToLower(path)\n+        return strings.HasSuffix(path, \".git\") ||\n+                strings.Contains(path, \".git/\") ||\n+                strings.Contains(path, `.git\\`) ||\n+                // Windows treats \".git.\" the same as \".git\"\n+                strings.HasSuffix(path, \".git.\") ||\n+                strings.Contains(path, \".git./\") ||\n+                strings.Contains(path, `.git.\\`)\n }\n \n func (repo *Repository) UploadRepoFiles(doer *User, opts UploadRepoFileOptions) error {\n-\tif len(opts.Files) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\tif isRepositoryGitPath(opts.TreePath) {\n-\t\treturn errors.Errorf(\"bad tree path %q\", opts.TreePath)\n-\t}\n-\n-\tuploads, err := GetUploadsByUUIDs(opts.Files)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"get uploads by UUIDs[%v]: %v\", opts.Files, err)\n-\t}\n-\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\tif err = repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tdirPath := path.Join(localPath, opts.TreePath)\n-\tif err = os.MkdirAll(dirPath, os.ModePerm); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Copy uploaded files into repository\n-\tfor _, upload := range uploads {\n-\t\ttmpPath := upload.LocalPath()\n-\t\tif !osutil.IsFile(tmpPath) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tupload.Name = pathutil.Clean(upload.Name)\n-\n-\t\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\t\tif isRepositoryGitPath(upload.Name) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\ttargetPath := path.Join(dirPath, upload.Name)\n-\t\tif err = com.Copy(tmpPath, targetPath); err != nil {\n-\t\t\treturn fmt.Errorf(\"copy: %v\", err)\n-\t\t}\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t}\n-\n-\terr = git.CreateCommit(\n-\t\tlocalPath,\n-\t\t&git.Signature{\n-\t\t\tName:  doer.DisplayName(),\n-\t\t\tEmail: doer.Email,\n-\t\t\tWhen:  time.Now(),\n-\t\t},\n-\t\topts.Message,\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\n-\treturn DeleteUploads(uploads...)\n+        if len(opts.Files) == 0 {\n+                return nil\n+        }\n+\n+        // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+        if isRepositoryGitPath(opts.TreePath) {\n+                return errors.Errorf(\"bad tree path %q\", opts.TreePath)\n+        }\n+\n+        uploads, err := GetUploadsByUUIDs(opts.Files)\n+        if err != nil {\n+                return fmt.Errorf(\"get uploads by UUIDs[%v]: %v\", opts.Files, err)\n+        }\n+\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                if err = repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        dirPath := path.Join(localPath, opts.TreePath)\n+        if err = os.MkdirAll(dirPath, os.ModePerm); err != nil {\n+                return err\n+        }\n+\n+        // Copy uploaded files into repository\n+        for _, upload := range uploads {\n+                tmpPath := upload.LocalPath()\n+                if !osutil.IsFile(tmpPath) {\n+                        continue\n+                }\n+\n+                upload.Name = pathutil.Clean(upload.Name)\n+\n+                // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+                if isRepositoryGitPath(upload.Name) {\n+                        continue\n+                }\n+\n+                targetPath := path.Join(dirPath, upload.Name)\n+                if err = com.Copy(tmpPath, targetPath); err != nil {\n+                        return fmt.Errorf(\"copy: %v\", err)\n+                }\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        }\n+\n+        err = git.CreateCommit(\n+                localPath,\n+                &git.Signature{\n+                        Name:  doer.DisplayName(),\n+                        Email: doer.Email,\n+                        When:  time.Now(),\n+                },\n+                opts.Message,\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+\n+        return DeleteUploads(uploads...)\n }\n"}
{"cve":"CVE-2022-39340:0708", "fix_patch": "diff --git a/pkg/cmd/service/service.go b/pkg/cmd/service/service.go\nindex b99ad681..3bd4acbd 100644\n--- a/pkg/cmd/service/service.go\n+++ b/pkg/cmd/service/service.go\n@@ -1,219 +1,219 @@\n package service\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"net\"\n-\t\"net/netip\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/go-errors/errors\"\n-\tgrpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\"\n-\t\"github.com/openfga/openfga/pkg/encoder\"\n-\t\"github.com/openfga/openfga/pkg/encrypter\"\n-\t\"github.com/openfga/openfga/pkg/logger\"\n-\t\"github.com/openfga/openfga/pkg/telemetry\"\n-\t\"github.com/openfga/openfga/server\"\n-\t\"github.com/openfga/openfga/server/authn\"\n-\t\"github.com/openfga/openfga/server/authn/oidc\"\n-\t\"github.com/openfga/openfga/server/authn/presharedkey\"\n-\t\"github.com/openfga/openfga/server/middleware\"\n-\t\"github.com/openfga/openfga/storage\"\n-\t\"github.com/openfga/openfga/storage/caching\"\n-\t\"github.com/openfga/openfga/storage/memory\"\n-\t\"github.com/openfga/openfga/storage/mysql\"\n-\t\"github.com/openfga/openfga/storage/postgres\"\n-\t\"github.com/spf13/viper\"\n-\t\"google.golang.org/grpc\"\n+        \"context\"\n+        \"fmt\"\n+        \"net\"\n+        \"net/netip\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/go-errors/errors\"\n+        grpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\"\n+        \"github.com/openfga/openfga/pkg/encoder\"\n+        \"github.com/openfga/openfga/pkg/encrypter\"\n+        \"github.com/openfga/openfga/pkg/logger\"\n+        \"github.com/openfga/openfga/pkg/telemetry\"\n+        \"github.com/openfga/openfga/server\"\n+        \"github.com/openfga/openfga/server/authn\"\n+        \"github.com/openfga/openfga/server/authn/oidc\"\n+        \"github.com/openfga/openfga/server/authn/presharedkey\"\n+        \"github.com/openfga/openfga/server/middleware\"\n+        \"github.com/openfga/openfga/storage\"\n+        \"github.com/openfga/openfga/storage/caching\"\n+        \"github.com/openfga/openfga/storage/memory\"\n+        \"github.com/openfga/openfga/storage/mysql\"\n+        \"github.com/openfga/openfga/storage/postgres\"\n+        \"github.com/spf13/viper\"\n+        \"google.golang.org/grpc\"\n )\n \n var (\n-\tErrInvalidGRPCTLSConfig = errors.New(\"'grpc.tls.cert' and 'grpc.tls.key' configs must be set\")\n-\tErrInvalidHTTPTLSConfig = errors.New(\"'http.tls.cert' and 'http.tls.key' configs must be set\")\n+        ErrInvalidGRPCTLSConfig = errors.New(\"'grpc.tls.cert' and 'grpc.tls.key' configs must be set\")\n+        ErrInvalidHTTPTLSConfig = errors.New(\"'http.tls.cert' and 'http.tls.key' configs must be set\")\n )\n \n // DatastoreConfig defines OpenFGA server configurations for datastore specific settings.\n type DatastoreConfig struct {\n \n-\t// Engine is the datastore engine to use (e.g. 'memory', 'postgres', 'mysql')\n-\tEngine string\n-\tURI    string\n+        // Engine is the datastore engine to use (e.g. 'memory', 'postgres', 'mysql')\n+        Engine string\n+        URI    string\n \n-\t// MaxCacheSize is the maximum number of cache keys that the storage cache can store before evicting\n-\t// old keys. The storage cache is used to cache query results for various static resources\n-\t// such as type definitions.\n-\tMaxCacheSize int\n+        // MaxCacheSize is the maximum number of cache keys that the storage cache can store before evicting\n+        // old keys. The storage cache is used to cache query results for various static resources\n+        // such as type definitions.\n+        MaxCacheSize int\n }\n \n // GRPCConfig defines OpenFGA server configurations for grpc server specific settings.\n type GRPCConfig struct {\n-\tAddr string\n-\tTLS  TLSConfig\n+        Addr string\n+        TLS  TLSConfig\n }\n \n // HTTPConfig defines OpenFGA server configurations for HTTP server specific settings.\n type HTTPConfig struct {\n-\tEnabled bool\n-\tAddr    string\n-\tTLS     TLSConfig\n+        Enabled bool\n+        Addr    string\n+        TLS     TLSConfig\n \n-\t// UpstreamTimeout is the timeout duration for proxying HTTP requests upstream\n-\t// to the grpc endpoint.\n-\tUpstreamTimeout time.Duration\n+        // UpstreamTimeout is the timeout duration for proxying HTTP requests upstream\n+        // to the grpc endpoint.\n+        UpstreamTimeout time.Duration\n \n-\tCORSAllowedOrigins []string `default:\"*\" split_words:\"true\"`\n-\tCORSAllowedHeaders []string `default:\"*\" split_words:\"true\"`\n+        CORSAllowedOrigins []string `default:\"*\" split_words:\"true\"`\n+        CORSAllowedHeaders []string `default:\"*\" split_words:\"true\"`\n }\n \n // TLSConfig defines configuration specific to Transport Layer Security (TLS) settings.\n type TLSConfig struct {\n-\tEnabled  bool\n-\tCertPath string `mapstructure:\"cert\"`\n-\tKeyPath  string `mapstructure:\"key\"`\n+        Enabled  bool\n+        CertPath string `mapstructure:\"cert\"`\n+        KeyPath  string `mapstructure:\"key\"`\n }\n \n // AuthnConfig defines OpenFGA server configurations for authentication specific settings.\n type AuthnConfig struct {\n \n-\t// Method is the authentication method that should be enforced (e.g. 'none', 'preshared', 'oidc')\n-\tMethod                   string\n-\t*AuthnOIDCConfig         `mapstructure:\"oidc\"`\n-\t*AuthnPresharedKeyConfig `mapstructure:\"preshared\"`\n+        // Method is the authentication method that should be enforced (e.g. 'none', 'preshared', 'oidc')\n+        Method                   string\n+        *AuthnOIDCConfig         `mapstructure:\"oidc\"`\n+        *AuthnPresharedKeyConfig `mapstructure:\"preshared\"`\n }\n \n // AuthnOIDCConfig defines configurations for the 'oidc' method of authentication.\n type AuthnOIDCConfig struct {\n-\tIssuer   string\n-\tAudience string\n+        Issuer   string\n+        Audience string\n }\n \n // AuthnPresharedKeyConfig defines configurations for the 'preshared' method of authentication.\n type AuthnPresharedKeyConfig struct {\n \n-\t// Keys define the preshared keys to verify authn tokens against.\n-\tKeys []string\n+        // Keys define the preshared keys to verify authn tokens against.\n+        Keys []string\n }\n \n // LogConfig defines OpenFGA server configurations for log specific settings. For production we\n // recommend using the 'json' log format.\n type LogConfig struct {\n \n-\t// Format is the log format to use in the log output (e.g. 'text' or 'json')\n-\tFormat string\n+        // Format is the log format to use in the log output (e.g. 'text' or 'json')\n+        Format string\n }\n \n // PlaygroundConfig defines OpenFGA server configurations for the Playground specific settings.\n type PlaygroundConfig struct {\n-\tEnabled bool\n-\tPort    int\n+        Enabled bool\n+        Port    int\n }\n \n // ProfilerConfig defines server configurations specific to pprof profiling.\n type ProfilerConfig struct {\n-\tEnabled bool\n-\tAddr    string\n+        Enabled bool\n+        Addr    string\n }\n \n type Config struct {\n-\t// If you change any of these settings, please update the documentation at https://github.com/openfga/openfga.dev/blob/main/docs/content/intro/setup-openfga.mdx\n+        // If you change any of these settings, please update the documentation at https://github.com/openfga/openfga.dev/blob/main/docs/content/intro/setup-openfga.mdx\n \n-\t// ListObjectsDeadline defines the maximum amount of time to accumulate ListObjects results\n-\t// before the server will respond. This is to protect the server from misuse of the\n-\t// ListObjects endpoints.\n-\tListObjectsDeadline time.Duration\n+        // ListObjectsDeadline defines the maximum amount of time to accumulate ListObjects results\n+        // before the server will respond. This is to protect the server from misuse of the\n+        // ListObjects endpoints.\n+        ListObjectsDeadline time.Duration\n \n-\t// ListObjectsMaxResults defines the maximum number of ListObjects results to accumulate\n-\t// before the server will respond. This is to protect the server from misuse of the\n-\t// ListObjects endpoints.\n-\tListObjectsMaxResults uint32\n+        // ListObjectsMaxResults defines the maximum number of ListObjects results to accumulate\n+        // before the server will respond. This is to protect the server from misuse of the\n+        // ListObjects endpoints.\n+        ListObjectsMaxResults uint32\n \n-\t// MaxTuplesPerWrite defines the maximum number of tuples per Write endpoint.\n-\tMaxTuplesPerWrite int\n+        // MaxTuplesPerWrite defines the maximum number of tuples per Write endpoint.\n+        MaxTuplesPerWrite int\n \n-\t// MaxTypesPerAuthorizationModel defines the maximum number of type definitions per authorization model for the WriteAuthorizationModel endpoint.\n-\tMaxTypesPerAuthorizationModel int\n+        // MaxTypesPerAuthorizationModel defines the maximum number of type definitions per authorization model for the WriteAuthorizationModel endpoint.\n+        MaxTypesPerAuthorizationModel int\n \n-\t// ChangelogHorizonOffset is an offset in minutes from the current time. Changes that occur after this offset will not be included in the response of ReadChanges.\n-\tChangelogHorizonOffset int\n+        // ChangelogHorizonOffset is an offset in minutes from the current time. Changes that occur after this offset will not be included in the response of ReadChanges.\n+        ChangelogHorizonOffset int\n \n-\t// ResolveNodeLimit indicates how deeply nested an authorization model can be.\n-\tResolveNodeLimit uint32\n+        // ResolveNodeLimit indicates how deeply nested an authorization model can be.\n+        ResolveNodeLimit uint32\n \n-\tDatastore  DatastoreConfig\n-\tGRPC       GRPCConfig\n-\tHTTP       HTTPConfig\n-\tAuthn      AuthnConfig\n-\tLog        LogConfig\n-\tPlayground PlaygroundConfig\n-\tProfiler   ProfilerConfig\n+        Datastore  DatastoreConfig\n+        GRPC       GRPCConfig\n+        HTTP       HTTPConfig\n+        Authn      AuthnConfig\n+        Log        LogConfig\n+        Playground PlaygroundConfig\n+        Profiler   ProfilerConfig\n }\n \n func DefaultConfigWithRandomPorts() (*Config, error) {\n-\tconfig := DefaultConfig()\n-\n-\tl, err := net.Listen(\"tcp\", \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tdefer l.Close()\n-\thttpPort := l.Addr().(*net.TCPAddr).Port\n-\n-\tl, err = net.Listen(\"tcp\", \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tdefer l.Close()\n-\tgrpcPort := l.Addr().(*net.TCPAddr).Port\n-\n-\tconfig.GRPC.Addr = fmt.Sprintf(\"0.0.0.0:%d\", grpcPort)\n-\tconfig.HTTP.Addr = fmt.Sprintf(\"0.0.0.0:%d\", httpPort)\n-\n-\treturn config, nil\n+        config := DefaultConfig()\n+\n+        l, err := net.Listen(\"tcp\", \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+        defer l.Close()\n+        httpPort := l.Addr().(*net.TCPAddr).Port\n+\n+        l, err = net.Listen(\"tcp\", \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+        defer l.Close()\n+        grpcPort := l.Addr().(*net.TCPAddr).Port\n+\n+        config.GRPC.Addr = fmt.Sprintf(\"0.0.0.0:%d\", grpcPort)\n+        config.HTTP.Addr = fmt.Sprintf(\"0.0.0.0:%d\", httpPort)\n+\n+        return config, nil\n }\n \n // DefaultConfig returns the OpenFGA server default configurations.\n func DefaultConfig() *Config {\n-\treturn &Config{\n-\t\tMaxTuplesPerWrite:             100,\n-\t\tMaxTypesPerAuthorizationModel: 100,\n-\t\tChangelogHorizonOffset:        0,\n-\t\tResolveNodeLimit:              25,\n-\t\tListObjectsDeadline:           3 * time.Second, // there is a 3-second timeout elsewhere\n-\t\tListObjectsMaxResults:         1000,\n-\t\tDatastore: DatastoreConfig{\n-\t\t\tEngine:       \"memory\",\n-\t\t\tMaxCacheSize: 100000,\n-\t\t},\n-\t\tGRPC: GRPCConfig{\n-\t\t\tAddr: \"0.0.0.0:8081\",\n-\t\t\tTLS:  TLSConfig{Enabled: false},\n-\t\t},\n-\t\tHTTP: HTTPConfig{\n-\t\t\tEnabled:            true,\n-\t\t\tAddr:               \"0.0.0.0:8080\",\n-\t\t\tTLS:                TLSConfig{Enabled: false},\n-\t\t\tUpstreamTimeout:    5 * time.Second,\n-\t\t\tCORSAllowedOrigins: []string{\"*\"},\n-\t\t\tCORSAllowedHeaders: []string{\"*\"},\n-\t\t},\n-\t\tAuthn: AuthnConfig{\n-\t\t\tMethod:                  \"none\",\n-\t\t\tAuthnPresharedKeyConfig: &AuthnPresharedKeyConfig{},\n-\t\t\tAuthnOIDCConfig:         &AuthnOIDCConfig{},\n-\t\t},\n-\t\tLog: LogConfig{\n-\t\t\tFormat: \"text\",\n-\t\t},\n-\t\tPlayground: PlaygroundConfig{\n-\t\t\tEnabled: true,\n-\t\t\tPort:    3000,\n-\t\t},\n-\t\tProfiler: ProfilerConfig{\n-\t\t\tEnabled: false,\n-\t\t\tAddr:    \":3001\",\n-\t\t},\n-\t}\n+        return &Config{\n+                MaxTuplesPerWrite:             100,\n+                MaxTypesPerAuthorizationModel: 100,\n+                ChangelogHorizonOffset:        0,\n+                ResolveNodeLimit:              25,\n+                ListObjectsDeadline:           3 * time.Second, // there is a 3-second timeout elsewhere\n+                ListObjectsMaxResults:         1000,\n+                Datastore: DatastoreConfig{\n+                        Engine:       \"memory\",\n+                        MaxCacheSize: 100000,\n+                },\n+                GRPC: GRPCConfig{\n+                        Addr: \"0.0.0.0:8081\",\n+                        TLS:  TLSConfig{Enabled: false},\n+                },\n+                HTTP: HTTPConfig{\n+                        Enabled:            true,\n+                        Addr:               \"0.0.0.0:8080\",\n+                        TLS:                TLSConfig{Enabled: false},\n+                        UpstreamTimeout:    5 * time.Second,\n+                        CORSAllowedOrigins: []string{\"*\"},\n+                        CORSAllowedHeaders: []string{\"*\"},\n+                },\n+                Authn: AuthnConfig{\n+                        Method:                  \"none\",\n+                        AuthnPresharedKeyConfig: &AuthnPresharedKeyConfig{},\n+                        AuthnOIDCConfig:         &AuthnOIDCConfig{},\n+                },\n+                Log: LogConfig{\n+                        Format: \"text\",\n+                },\n+                Playground: PlaygroundConfig{\n+                        Enabled: true,\n+                        Port:    3000,\n+                },\n+                Profiler: ProfilerConfig{\n+                        Enabled: false,\n+                        Addr:    \":3001\",\n+                },\n+        }\n }\n \n // GetServiceConfig returns the OpenFGA server configuration based on the values provided in the server's 'config.yaml' file.\n@@ -221,212 +221,215 @@ func DefaultConfig() *Config {\n // file is present, the default values are returned.\n func GetServiceConfig() (*Config, error) {\n \n-\tconfig := DefaultConfig()\n+        config := DefaultConfig()\n \n-\tviper.SetConfigName(\"config\")\n-\tviper.SetConfigType(\"yaml\")\n+        viper.SetConfigName(\"config\")\n+        viper.SetConfigType(\"yaml\")\n \n-\tconfigPaths := []string{\"/etc/openfga\", \"$HOME/.openfga\", \".\"}\n-\tfor _, path := range configPaths {\n-\t\tviper.AddConfigPath(path)\n-\t}\n-\tviper.SetEnvPrefix(\"OPENFGA\")\n-\tviper.SetEnvKeyReplacer(strings.NewReplacer(\".\", \"_\"))\n-\tviper.AutomaticEnv()\n+        configPaths := []string{\"/etc/openfga\", \"$HOME/.openfga\", \".\"}\n+        for _, path := range configPaths {\n+                viper.AddConfigPath(path)\n+        }\n+        viper.SetEnvPrefix(\"OPENFGA\")\n+        viper.SetEnvKeyReplacer(strings.NewReplacer(\".\", \"_\"))\n+        viper.AutomaticEnv()\n \n-\terr := viper.ReadInConfig()\n-\tif err != nil {\n-\t\t_, ok := err.(viper.ConfigFileNotFoundError)\n-\t\tif !ok {\n-\t\t\treturn nil, errors.Errorf(\"failed to load server config: %w\", err)\n-\t\t}\n-\t}\n+        err := viper.ReadInConfig()\n+        if err != nil {\n+                _, ok := err.(viper.ConfigFileNotFoundError)\n+                if !ok {\n+                        return nil, errors.Errorf(\"failed to load server config: %w\", err)\n+                }\n+        }\n \n-\tif err := viper.Unmarshal(config); err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to unmarshal server config: %w\", err)\n-\t}\n+        if err := viper.Unmarshal(config); err != nil {\n+                return nil, errors.Errorf(\"failed to unmarshal server config: %w\", err)\n+        }\n \n-\treturn config, nil\n+        return config, nil\n }\n \n type service struct {\n-\tserver        *server.Server\n-\tgrpcAddr      netip.AddrPort\n-\thttpAddr      netip.AddrPort\n-\tdatastore     storage.OpenFGADatastore\n-\tauthenticator authn.Authenticator\n+        server        *server.Server\n+        grpcAddr      netip.AddrPort\n+        httpAddr      netip.AddrPort\n+        datastore     storage.OpenFGADatastore\n+        authenticator authn.Authenticator\n }\n \n func (s *service) Close(ctx context.Context) error {\n-\ts.authenticator.Close()\n+        s.authenticator.Close()\n \n-\treturn s.datastore.Close(ctx)\n+        return s.datastore.Close(ctx)\n }\n \n func (s *service) Run(ctx context.Context) error {\n-\treturn s.server.Run(ctx)\n+        return s.server.Run(ctx)\n }\n \n // GetHTTPAddrPort returns the configured or auto-assigned port that the underlying HTTP service is running on.\n func (s *service) GetHTTPAddrPort() netip.AddrPort {\n-\treturn s.httpAddr\n+        return s.httpAddr\n }\n \n // GetGRPCAddrPort returns the configured or auto-assigned port that the underlying grpc service is running on.\n func (s *service) GetGRPCAddrPort() netip.AddrPort {\n-\treturn s.grpcAddr\n+        return s.grpcAddr\n }\n \n func BuildService(config *Config, logger logger.Logger) (*service, error) {\n-\ttracer := telemetry.NewNoopTracer()\n-\tmeter := telemetry.NewNoopMeter()\n-\ttokenEncoder := encoder.NewTokenEncoder(encrypter.NewNoopEncrypter(), encoder.NewBase64Encoder())\n-\n-\tvar datastore storage.OpenFGADatastore\n-\tvar err error\n-\tswitch config.Datastore.Engine {\n-\tcase \"memory\":\n-\t\tdatastore = memory.New(tracer, config.MaxTuplesPerWrite, config.MaxTypesPerAuthorizationModel)\n-\tcase \"mysql\":\n-\t\topts := []mysql.MySQLOption{\n-\t\t\tmysql.WithLogger(logger),\n-\t\t\tmysql.WithTracer(tracer),\n-\t\t}\n-\n-\t\tdatastore, err = mysql.NewMySQLDatastore(config.Datastore.URI, opts...)\n-\t\tif err != nil {\n-\t\t\treturn nil, errors.Errorf(\"failed to initialize mysql datastore: %v\", err)\n-\t\t}\n-\tcase \"postgres\":\n-\t\topts := []postgres.PostgresOption{\n-\t\t\tpostgres.WithLogger(logger),\n-\t\t\tpostgres.WithTracer(tracer),\n-\t\t}\n-\n-\t\tdatastore, err = postgres.NewPostgresDatastore(config.Datastore.URI, opts...)\n-\t\tif err != nil {\n-\t\t\treturn nil, errors.Errorf(\"failed to initialize postgres datastore: %v\", err)\n-\t\t}\n-\tdefault:\n-\t\treturn nil, errors.Errorf(\"storage engine '%s' is unsupported\", config.Datastore.Engine)\n-\t}\n-\n-\tlogger.Info(fmt.Sprintf(\"using '%v' storage engine\", config.Datastore.Engine))\n-\n-\tvar grpcTLSConfig *server.TLSConfig\n-\tif config.GRPC.TLS.Enabled {\n-\t\tif config.GRPC.TLS.CertPath == \"\" || config.GRPC.TLS.KeyPath == \"\" {\n-\t\t\treturn nil, ErrInvalidGRPCTLSConfig\n-\t\t}\n-\t\tgrpcTLSConfig = &server.TLSConfig{\n-\t\t\tCertPath: config.GRPC.TLS.CertPath,\n-\t\t\tKeyPath:  config.GRPC.TLS.KeyPath,\n-\t\t}\n-\t\tlogger.Info(\"grpc TLS is enabled, serving connections using the provided certificate\")\n-\t} else {\n-\t\tlogger.Warn(\"grpc TLS is disabled, serving connections using insecure plaintext\")\n-\t}\n-\n-\tvar httpTLSConfig *server.TLSConfig\n-\tif config.HTTP.TLS.Enabled {\n-\t\tif config.HTTP.TLS.CertPath == \"\" || config.HTTP.TLS.KeyPath == \"\" {\n-\t\t\treturn nil, ErrInvalidHTTPTLSConfig\n-\t\t}\n-\t\thttpTLSConfig = &server.TLSConfig{\n-\t\t\tCertPath: config.HTTP.TLS.CertPath,\n-\t\t\tKeyPath:  config.HTTP.TLS.KeyPath,\n-\t\t}\n-\t\tlogger.Info(\"HTTP TLS is enabled, serving HTTP connections using the provided certificate\")\n-\t} else {\n-\t\tlogger.Warn(\"HTTP TLS is disabled, serving connections using insecure plaintext\")\n-\t}\n-\n-\tvar authenticator authn.Authenticator\n-\tswitch config.Authn.Method {\n-\tcase \"none\":\n-\t\tlogger.Warn(\"authentication is disabled\")\n-\t\tauthenticator = authn.NoopAuthenticator{}\n-\tcase \"preshared\":\n-\t\tlogger.Info(\"using 'preshared' authentication\")\n-\t\tauthenticator, err = presharedkey.NewPresharedKeyAuthenticator(config.Authn.Keys)\n-\tcase \"oidc\":\n-\t\tlogger.Info(\"using 'oidc' authentication\")\n-\t\tauthenticator, err = oidc.NewRemoteOidcAuthenticator(config.Authn.Issuer, config.Authn.Audience)\n-\tdefault:\n-\t\treturn nil, errors.Errorf(\"unsupported authentication method '%v'\", config.Authn.Method)\n-\t}\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to initialize authenticator: %v\", err)\n-\t}\n-\n-\tinterceptors := []grpc.UnaryServerInterceptor{\n-\t\tgrpc_auth.UnaryServerInterceptor(middleware.AuthFunc(authenticator)),\n-\t\tmiddleware.NewErrorLoggingInterceptor(logger),\n-\t}\n-\n-\tgrpcHostAddr, grpcHostPort, err := net.SplitHostPort(config.GRPC.Addr)\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"`grpc.addr` config must be in the form [host]:port\")\n-\t}\n-\n-\tif grpcHostAddr == \"\" {\n-\t\tgrpcHostAddr = \"0.0.0.0\"\n-\t}\n-\n-\tgrpcAddr, err := netip.ParseAddrPort(fmt.Sprintf(\"%s:%s\", grpcHostAddr, grpcHostPort))\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to parse the 'grpc.addr' config: %v\", err)\n-\t}\n-\n-\thttpHostAddr, httpHostPort, err := net.SplitHostPort(config.HTTP.Addr)\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"`http.addr` config must be in the form [host]:port\")\n-\t}\n-\n-\tif httpHostAddr == \"\" {\n-\t\thttpHostAddr = \"0.0.0.0\"\n-\t}\n-\n-\thttpAddr, err := netip.ParseAddrPort(fmt.Sprintf(\"%s:%s\", httpHostAddr, httpHostPort))\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to parse the 'http.addr' config: %v\", err)\n-\t}\n-\n-\topenFgaServer, err := server.New(&server.Dependencies{\n-\t\tDatastore:    caching.NewCachedOpenFGADatastore(datastore, config.Datastore.MaxCacheSize),\n-\t\tTracer:       tracer,\n-\t\tLogger:       logger,\n-\t\tMeter:        meter,\n-\t\tTokenEncoder: tokenEncoder,\n-\t}, &server.Config{\n-\t\tGRPCServer: server.GRPCServerConfig{\n-\t\t\tAddr:      grpcAddr,\n-\t\t\tTLSConfig: grpcTLSConfig,\n-\t\t},\n-\t\tHTTPServer: server.HTTPServerConfig{\n-\t\t\tEnabled:            config.HTTP.Enabled,\n-\t\t\tAddr:               httpAddr,\n-\t\t\tTLSConfig:          httpTLSConfig,\n-\t\t\tUpstreamTimeout:    config.HTTP.UpstreamTimeout,\n-\t\t\tCORSAllowedOrigins: config.HTTP.CORSAllowedOrigins,\n-\t\t\tCORSAllowedHeaders: config.HTTP.CORSAllowedHeaders,\n-\t\t},\n-\t\tResolveNodeLimit:       config.ResolveNodeLimit,\n-\t\tChangelogHorizonOffset: config.ChangelogHorizonOffset,\n-\t\tListObjectsDeadline:    config.ListObjectsDeadline,\n-\t\tListObjectsMaxResults:  config.ListObjectsMaxResults,\n-\t\tUnaryInterceptors:      interceptors,\n-\t\tMuxOptions:             nil,\n-\t})\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to initialize openfga server: %v\", err)\n-\t}\n-\n-\treturn &service{\n-\t\tserver:        openFgaServer,\n-\t\tgrpcAddr:      grpcAddr,\n-\t\thttpAddr:      httpAddr,\n-\t\tdatastore:     datastore,\n-\t\tauthenticator: authenticator,\n-\t}, nil\n+        tracer := telemetry.NewNoopTracer()\n+        meter := telemetry.NewNoopMeter()\n+        tokenEncoder := encoder.NewTokenEncoder(encrypter.NewNoopEncrypter(), encoder.NewBase64Encoder())\n+\n+        var datastore storage.OpenFGADatastore\n+        var err error\n+        switch config.Datastore.Engine {\n+        case \"memory\":\n+                datastore = memory.New(tracer, config.MaxTuplesPerWrite, config.MaxTypesPerAuthorizationModel)\n+        case \"mysql\":\n+                opts := []mysql.MySQLOption{\n+                        mysql.WithLogger(logger),\n+                        mysql.WithTracer(tracer),\n+                }\n+\n+                datastore, err = mysql.NewMySQLDatastore(config.Datastore.URI, opts...)\n+                if err != nil {\n+                        return nil, errors.Errorf(\"failed to initialize mysql datastore: %v\", err)\n+                }\n+        case \"postgres\":\n+                opts := []postgres.PostgresOption{\n+                        postgres.WithLogger(logger),\n+                        postgres.WithTracer(tracer),\n+                }\n+\n+                datastore, err = postgres.NewPostgresDatastore(config.Datastore.URI, opts...)\n+                if err != nil {\n+                        return nil, errors.Errorf(\"failed to initialize postgres datastore: %v\", err)\n+                }\n+        default:\n+                return nil, errors.Errorf(\"storage engine '%s' is unsupported\", config.Datastore.Engine)\n+        }\n+\n+        logger.Info(fmt.Sprintf(\"using '%v' storage engine\", config.Datastore.Engine))\n+\n+        var grpcTLSConfig *server.TLSConfig\n+        if config.GRPC.TLS.Enabled {\n+                if config.GRPC.TLS.CertPath == \"\" || config.GRPC.TLS.KeyPath == \"\" {\n+                        return nil, ErrInvalidGRPCTLSConfig\n+                }\n+                grpcTLSConfig = &server.TLSConfig{\n+                        CertPath: config.GRPC.TLS.CertPath,\n+                        KeyPath:  config.GRPC.TLS.KeyPath,\n+                }\n+                logger.Info(\"grpc TLS is enabled, serving connections using the provided certificate\")\n+        } else {\n+                logger.Warn(\"grpc TLS is disabled, serving connections using insecure plaintext\")\n+        }\n+\n+        var httpTLSConfig *server.TLSConfig\n+        if config.HTTP.TLS.Enabled {\n+                if config.HTTP.TLS.CertPath == \"\" || config.HTTP.TLS.KeyPath == \"\" {\n+                        return nil, ErrInvalidHTTPTLSConfig\n+                }\n+                httpTLSConfig = &server.TLSConfig{\n+                        CertPath: config.HTTP.TLS.CertPath,\n+                        KeyPath:  config.HTTP.TLS.KeyPath,\n+                }\n+                logger.Info(\"HTTP TLS is enabled, serving HTTP connections using the provided certificate\")\n+        } else {\n+                logger.Warn(\"HTTP TLS is disabled, serving connections using insecure plaintext\")\n+        }\n+\n+        var authenticator authn.Authenticator\n+        switch config.Authn.Method {\n+        case \"none\":\n+                logger.Warn(\"authentication is disabled\")\n+                authenticator = authn.NoopAuthenticator{}\n+        case \"preshared\":\n+                logger.Info(\"using 'preshared' authentication\")\n+                authenticator, err = presharedkey.NewPresharedKeyAuthenticator(config.Authn.Keys)\n+        case \"oidc\":\n+                logger.Info(\"using 'oidc' authentication\")\n+                authenticator, err = oidc.NewRemoteOidcAuthenticator(config.Authn.Issuer, config.Authn.Audience)\n+        default:\n+                return nil, errors.Errorf(\"unsupported authentication method '%v'\", config.Authn.Method)\n+        }\n+        if err != nil {\n+                return nil, errors.Errorf(\"failed to initialize authenticator: %v\", err)\n+        }\n+\n+        interceptors := []grpc.UnaryServerInterceptor{\n+                grpc_auth.UnaryServerInterceptor(middleware.AuthFunc(authenticator)),\n+                middleware.NewErrorLoggingInterceptor(logger),\n+        }\n+\n+        grpcHostAddr, grpcHostPort, err := net.SplitHostPort(config.GRPC.Addr)\n+        if err != nil {\n+                return nil, errors.Errorf(\"`grpc.addr` config must be in the form [host]:port\")\n+        }\n+\n+        if grpcHostAddr == \"\" {\n+                grpcHostAddr = \"0.0.0.0\"\n+        }\n+\n+        grpcAddr, err := netip.ParseAddrPort(fmt.Sprintf(\"%s:%s\", grpcHostAddr, grpcHostPort))\n+        if err != nil {\n+                return nil, errors.Errorf(\"failed to parse the 'grpc.addr' config: %v\", err)\n+        }\n+\n+        httpHostAddr, httpHostPort, err := net.SplitHostPort(config.HTTP.Addr)\n+        if err != nil {\n+                return nil, errors.Errorf(\"`http.addr` config must be in the form [host]:port\")\n+        }\n+\n+        if httpHostAddr == \"\" {\n+                httpHostAddr = \"0.0.0.0\"\n+        }\n+\n+        httpAddr, err := netip.ParseAddrPort(fmt.Sprintf(\"%s:%s\", httpHostAddr, httpHostPort))\n+        if err != nil {\n+                return nil, errors.Errorf(\"failed to parse the 'http.addr' config: %v\", err)\n+        }\n+\n+        openFgaServer, err := server.New(&server.Dependencies{\n+                Datastore:    caching.NewCachedOpenFGADatastore(datastore, config.Datastore.MaxCacheSize),\n+                Tracer:       tracer,\n+                Logger:       logger,\n+                Meter:        meter,\n+                TokenEncoder: tokenEncoder,\n+        }, &server.Config{\n+                GRPCServer: server.GRPCServerConfig{\n+                        Addr:      grpcAddr,\n+                        TLSConfig: grpcTLSConfig,\n+                },\n+                HTTPServer: server.HTTPServerConfig{\n+                        Enabled:            config.HTTP.Enabled,\n+                        Addr:               httpAddr,\n+                        TLSConfig:          httpTLSConfig,\n+                        UpstreamTimeout:    config.HTTP.UpstreamTimeout,\n+                        CORSAllowedOrigins: config.HTTP.CORSAllowedOrigins,\n+                        CORSAllowedHeaders: config.HTTP.CORSAllowedHeaders,\n+                },\n+                ResolveNodeLimit:       config.ResolveNodeLimit,\n+                ChangelogHorizonOffset: config.ChangelogHorizonOffset,\n+                ListObjectsDeadline:    config.ListObjectsDeadline,\n+                ListObjectsMaxResults:  config.ListObjectsMaxResults,\n+                UnaryInterceptors:      interceptors,\n+                StreamInterceptors: []grpc.StreamServerInterceptor{\n+grpc_auth.StreamServerInterceptor(middleware.AuthFunc(authenticator)),\n+},\n+                MuxOptions:             nil,\n+        })\n+        if err != nil {\n+                return nil, errors.Errorf(\"failed to initialize openfga server: %v\", err)\n+        }\n+\n+        return &service{\n+                server:        openFgaServer,\n+                grpcAddr:      grpcAddr,\n+                httpAddr:      httpAddr,\n+                datastore:     datastore,\n+                authenticator: authenticator,\n+        }, nil\n }\ndiff --git a/server/server.go b/server/server.go\nindex 6b77d920..1ee5abe0 100644\n--- a/server/server.go\n+++ b/server/server.go\n@@ -1,612 +1,614 @@\n package server\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/netip\"\n-\t\"reflect\"\n-\t\"strconv\"\n-\t\"time\"\n-\n-\t\"github.com/go-errors/errors\"\n-\tgrpc_validator \"github.com/grpc-ecosystem/go-grpc-middleware/validator\"\n-\t\"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n-\thttpmiddleware \"github.com/openfga/openfga/internal/middleware/http\"\n-\t\"github.com/openfga/openfga/pkg/encoder\"\n-\t\"github.com/openfga/openfga/pkg/id\"\n-\t\"github.com/openfga/openfga/pkg/logger\"\n-\t\"github.com/openfga/openfga/server/commands\"\n-\tserverErrors \"github.com/openfga/openfga/server/errors\"\n-\t\"github.com/openfga/openfga/server/gateway\"\n-\t\"github.com/openfga/openfga/server/health\"\n-\t\"github.com/openfga/openfga/storage\"\n-\t\"github.com/rs/cors\"\n-\topenfgapb \"go.buf.build/openfga/go/openfga/api/openfga/v1\"\n-\t\"go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc\"\n-\t\"go.opentelemetry.io/otel/attribute\"\n-\t\"go.opentelemetry.io/otel/metric\"\n-\t\"go.opentelemetry.io/otel/trace\"\n-\t\"google.golang.org/grpc\"\n-\t\"google.golang.org/grpc/credentials\"\n-\t\"google.golang.org/grpc/credentials/insecure\"\n-\thealthv1pb \"google.golang.org/grpc/health/grpc_health_v1\"\n-\t\"google.golang.org/grpc/reflection\"\n-\t\"google.golang.org/grpc/status\"\n+        \"context\"\n+        \"fmt\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/netip\"\n+        \"reflect\"\n+        \"strconv\"\n+        \"time\"\n+\n+        \"github.com/go-errors/errors\"\n+        grpc_validator \"github.com/grpc-ecosystem/go-grpc-middleware/validator\"\n+        \"github.com/grpc-ecosystem/grpc-gateway/v2/runtime\"\n+        httpmiddleware \"github.com/openfga/openfga/internal/middleware/http\"\n+        \"github.com/openfga/openfga/pkg/encoder\"\n+        \"github.com/openfga/openfga/pkg/id\"\n+        \"github.com/openfga/openfga/pkg/logger\"\n+        \"github.com/openfga/openfga/server/commands\"\n+        serverErrors \"github.com/openfga/openfga/server/errors\"\n+        \"github.com/openfga/openfga/server/gateway\"\n+        \"github.com/openfga/openfga/server/health\"\n+        \"github.com/openfga/openfga/storage\"\n+        \"github.com/rs/cors\"\n+        openfgapb \"go.buf.build/openfga/go/openfga/api/openfga/v1\"\n+        \"go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc\"\n+        \"go.opentelemetry.io/otel/attribute\"\n+        \"go.opentelemetry.io/otel/metric\"\n+        \"go.opentelemetry.io/otel/trace\"\n+        \"google.golang.org/grpc\"\n+        \"google.golang.org/grpc/credentials\"\n+        \"google.golang.org/grpc/credentials/insecure\"\n+        healthv1pb \"google.golang.org/grpc/health/grpc_health_v1\"\n+        \"google.golang.org/grpc/reflection\"\n+        \"google.golang.org/grpc/status\"\n )\n \n const (\n-\tAuthorizationModelIDHeader = \"openfga-authorization-model-id\"\n+        AuthorizationModelIDHeader = \"openfga-authorization-model-id\"\n )\n \n var (\n-\tErrNilTokenEncoder = errors.Errorf(\"tokenEncoder must be a non-nil interface value\")\n-\tErrNilTransport    = errors.Errorf(\"transport must be a non-nil interface value\")\n+        ErrNilTokenEncoder = errors.Errorf(\"tokenEncoder must be a non-nil interface value\")\n+        ErrNilTransport    = errors.Errorf(\"transport must be a non-nil interface value\")\n )\n \n // A Server implements the OpenFGA service backend as both\n // a GRPC and HTTP server.\n type Server struct {\n-\topenfgapb.UnimplementedOpenFGAServiceServer\n+        openfgapb.UnimplementedOpenFGAServiceServer\n \n-\ttracer    trace.Tracer\n-\tmeter     metric.Meter\n-\tlogger    logger.Logger\n-\tdatastore storage.OpenFGADatastore\n-\tencoder   encoder.Encoder\n-\tconfig    *Config\n-\ttransport gateway.Transport\n+        tracer    trace.Tracer\n+        meter     metric.Meter\n+        logger    logger.Logger\n+        datastore storage.OpenFGADatastore\n+        encoder   encoder.Encoder\n+        config    *Config\n+        transport gateway.Transport\n \n-\tdefaultServeMuxOpts []runtime.ServeMuxOption\n+        defaultServeMuxOpts []runtime.ServeMuxOption\n }\n \n type Dependencies struct {\n-\tDatastore storage.OpenFGADatastore\n-\tTracer    trace.Tracer\n-\tMeter     metric.Meter\n-\tLogger    logger.Logger\n-\tTransport gateway.Transport\n-\n-\t// TokenEncoder is the encoder used to encode continuation tokens for paginated views.\n-\t// Defaults to a base64 encoder if none is provided.\n-\tTokenEncoder encoder.Encoder\n+        Datastore storage.OpenFGADatastore\n+        Tracer    trace.Tracer\n+        Meter     metric.Meter\n+        Logger    logger.Logger\n+        Transport gateway.Transport\n+\n+        // TokenEncoder is the encoder used to encode continuation tokens for paginated views.\n+        // Defaults to a base64 encoder if none is provided.\n+        TokenEncoder encoder.Encoder\n }\n \n type Config struct {\n-\tGRPCServer             GRPCServerConfig\n-\tHTTPServer             HTTPServerConfig\n-\tResolveNodeLimit       uint32\n-\tChangelogHorizonOffset int\n-\tListObjectsDeadline    time.Duration\n-\tListObjectsMaxResults  uint32\n-\tUnaryInterceptors      []grpc.UnaryServerInterceptor\n-\tMuxOptions             []runtime.ServeMuxOption\n+        GRPCServer             GRPCServerConfig\n+        HTTPServer             HTTPServerConfig\n+        ResolveNodeLimit       uint32\n+        ChangelogHorizonOffset int\n+        ListObjectsDeadline    time.Duration\n+        ListObjectsMaxResults  uint32\n+        UnaryInterceptors      []grpc.UnaryServerInterceptor\n+        StreamInterceptors     []grpc.StreamServerInterceptor\n+        MuxOptions             []runtime.ServeMuxOption\n }\n \n type GRPCServerConfig struct {\n-\tAddr      netip.AddrPort\n-\tTLSConfig *TLSConfig\n+        Addr      netip.AddrPort\n+        TLSConfig *TLSConfig\n }\n \n type HTTPServerConfig struct {\n-\tEnabled            bool\n-\tAddr               netip.AddrPort\n-\tUpstreamTimeout    time.Duration\n-\tTLSConfig          *TLSConfig\n-\tCORSAllowedOrigins []string\n-\tCORSAllowedHeaders []string\n+        Enabled            bool\n+        Addr               netip.AddrPort\n+        UpstreamTimeout    time.Duration\n+        TLSConfig          *TLSConfig\n+        CORSAllowedOrigins []string\n+        CORSAllowedHeaders []string\n }\n \n type TLSConfig struct {\n-\tCertPath string\n-\tKeyPath  string\n+        CertPath string\n+        KeyPath  string\n }\n \n // New creates a new Server which uses the supplied backends\n // for managing data.\n func New(dependencies *Dependencies, config *Config) (*Server, error) {\n-\ttokenEncoder := dependencies.TokenEncoder\n-\tif tokenEncoder == nil {\n-\t\ttokenEncoder = encoder.NewBase64Encoder()\n-\t} else {\n-\t\tt := reflect.TypeOf(tokenEncoder)\n-\t\tif reflect.ValueOf(tokenEncoder) == reflect.Zero(t) {\n-\t\t\treturn nil, ErrNilTokenEncoder\n-\t\t}\n-\t}\n-\n-\ttransport := dependencies.Transport\n-\tif transport == nil {\n-\t\ttransport = gateway.NewRPCTransport(dependencies.Logger)\n-\t} else {\n-\t\tt := reflect.TypeOf(transport)\n-\t\tif reflect.ValueOf(transport) == reflect.Zero(t) {\n-\t\t\treturn nil, ErrNilTransport\n-\t\t}\n-\t}\n-\n-\tserver := &Server{\n-\t\ttracer:    dependencies.Tracer,\n-\t\tmeter:     dependencies.Meter,\n-\t\tlogger:    dependencies.Logger,\n-\t\tdatastore: dependencies.Datastore,\n-\t\tencoder:   tokenEncoder,\n-\t\ttransport: transport,\n-\t\tconfig:    config,\n-\t\tdefaultServeMuxOpts: []runtime.ServeMuxOption{\n-\t\t\truntime.WithForwardResponseOption(httpmiddleware.HTTPResponseModifier),\n-\t\t\truntime.WithErrorHandler(func(c context.Context, sr *runtime.ServeMux, mm runtime.Marshaler, w http.ResponseWriter, r *http.Request, e error) {\n-\t\t\t\tintCode := serverErrors.ConvertToEncodedErrorCode(status.Convert(e))\n-\t\t\t\thttpmiddleware.CustomHTTPErrorHandler(c, w, r, serverErrors.NewEncodedError(intCode, e.Error()))\n-\t\t\t}),\n-\t\t\truntime.WithStreamErrorHandler(func(ctx context.Context, e error) *status.Status {\n-\t\t\t\tintCode := serverErrors.ConvertToEncodedErrorCode(status.Convert(e))\n-\t\t\t\tencodedErr := serverErrors.NewEncodedError(intCode, e.Error())\n-\t\t\t\treturn status.Convert(&encodedErr)\n-\t\t\t}),\n-\t\t},\n-\t}\n-\n-\terrors.MaxStackDepth = logger.MaxDepthBacktraceStack\n-\n-\treturn server, nil\n+        tokenEncoder := dependencies.TokenEncoder\n+        if tokenEncoder == nil {\n+                tokenEncoder = encoder.NewBase64Encoder()\n+        } else {\n+                t := reflect.TypeOf(tokenEncoder)\n+                if reflect.ValueOf(tokenEncoder) == reflect.Zero(t) {\n+                        return nil, ErrNilTokenEncoder\n+                }\n+        }\n+\n+        transport := dependencies.Transport\n+        if transport == nil {\n+                transport = gateway.NewRPCTransport(dependencies.Logger)\n+        } else {\n+                t := reflect.TypeOf(transport)\n+                if reflect.ValueOf(transport) == reflect.Zero(t) {\n+                        return nil, ErrNilTransport\n+                }\n+        }\n+\n+        server := &Server{\n+                tracer:    dependencies.Tracer,\n+                meter:     dependencies.Meter,\n+                logger:    dependencies.Logger,\n+                datastore: dependencies.Datastore,\n+                encoder:   tokenEncoder,\n+                transport: transport,\n+                config:    config,\n+                defaultServeMuxOpts: []runtime.ServeMuxOption{\n+                        runtime.WithForwardResponseOption(httpmiddleware.HTTPResponseModifier),\n+                        runtime.WithErrorHandler(func(c context.Context, sr *runtime.ServeMux, mm runtime.Marshaler, w http.ResponseWriter, r *http.Request, e error) {\n+                                intCode := serverErrors.ConvertToEncodedErrorCode(status.Convert(e))\n+                                httpmiddleware.CustomHTTPErrorHandler(c, w, r, serverErrors.NewEncodedError(intCode, e.Error()))\n+                        }),\n+                        runtime.WithStreamErrorHandler(func(ctx context.Context, e error) *status.Status {\n+                                intCode := serverErrors.ConvertToEncodedErrorCode(status.Convert(e))\n+                                encodedErr := serverErrors.NewEncodedError(intCode, e.Error())\n+                                return status.Convert(&encodedErr)\n+                        }),\n+                },\n+        }\n+\n+        errors.MaxStackDepth = logger.MaxDepthBacktraceStack\n+\n+        return server, nil\n }\n \n func (s *Server) ListObjects(ctx context.Context, req *openfgapb.ListObjectsRequest) (*openfgapb.ListObjectsResponse, error) {\n-\tstoreID := req.GetStoreId()\n-\ttargetObjectType := req.GetType()\n-\n-\tctx, span := s.tracer.Start(ctx, \"listObjects\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t\tattribute.KeyValue{Key: \"objectType\", Value: attribute.StringValue(targetObjectType)},\n-\t))\n-\tdefer span.End()\n-\n-\tmodelID, err := s.resolveAuthorizationModelID(ctx, storeID, req.GetAuthorizationModelId())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tq := &commands.ListObjectsQuery{\n-\t\tDatastore:             s.datastore,\n-\t\tLogger:                s.logger,\n-\t\tTracer:                s.tracer,\n-\t\tMeter:                 s.meter,\n-\t\tListObjectsDeadline:   s.config.ListObjectsDeadline,\n-\t\tListObjectsMaxResults: s.config.ListObjectsMaxResults,\n-\t\tResolveNodeLimit:      s.config.ResolveNodeLimit,\n-\t}\n-\n-\treturn q.Execute(ctx, &openfgapb.ListObjectsRequest{\n-\t\tStoreId:              storeID,\n-\t\tContextualTuples:     req.GetContextualTuples(),\n-\t\tAuthorizationModelId: modelID,\n-\t\tType:                 targetObjectType,\n-\t\tRelation:             req.Relation,\n-\t\tUser:                 req.User,\n-\t})\n+        storeID := req.GetStoreId()\n+        targetObjectType := req.GetType()\n+\n+        ctx, span := s.tracer.Start(ctx, \"listObjects\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+                attribute.KeyValue{Key: \"objectType\", Value: attribute.StringValue(targetObjectType)},\n+        ))\n+        defer span.End()\n+\n+        modelID, err := s.resolveAuthorizationModelID(ctx, storeID, req.GetAuthorizationModelId())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        q := &commands.ListObjectsQuery{\n+                Datastore:             s.datastore,\n+                Logger:                s.logger,\n+                Tracer:                s.tracer,\n+                Meter:                 s.meter,\n+                ListObjectsDeadline:   s.config.ListObjectsDeadline,\n+                ListObjectsMaxResults: s.config.ListObjectsMaxResults,\n+                ResolveNodeLimit:      s.config.ResolveNodeLimit,\n+        }\n+\n+        return q.Execute(ctx, &openfgapb.ListObjectsRequest{\n+                StoreId:              storeID,\n+                ContextualTuples:     req.GetContextualTuples(),\n+                AuthorizationModelId: modelID,\n+                Type:                 targetObjectType,\n+                Relation:             req.Relation,\n+                User:                 req.User,\n+        })\n }\n \n func (s *Server) StreamedListObjects(req *openfgapb.StreamedListObjectsRequest, srv openfgapb.OpenFGAService_StreamedListObjectsServer) error {\n-\tstoreID := req.GetStoreId()\n-\tctx := context.Background()\n-\tctx, span := s.tracer.Start(ctx, \"streamedListObjects\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t\tattribute.KeyValue{Key: \"objectType\", Value: attribute.StringValue(req.GetType())},\n-\t))\n-\tdefer span.End()\n-\n-\tmodelID, err := s.resolveAuthorizationModelID(ctx, storeID, req.GetAuthorizationModelId())\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tq := &commands.ListObjectsQuery{\n-\t\tDatastore:             s.datastore,\n-\t\tLogger:                s.logger,\n-\t\tTracer:                s.tracer,\n-\t\tMeter:                 s.meter,\n-\t\tListObjectsDeadline:   s.config.ListObjectsDeadline,\n-\t\tListObjectsMaxResults: s.config.ListObjectsMaxResults,\n-\t\tResolveNodeLimit:      s.config.ResolveNodeLimit,\n-\t}\n-\n-\treq.AuthorizationModelId = modelID\n-\treturn q.ExecuteStreamed(ctx, req, srv)\n+        storeID := req.GetStoreId()\n+        ctx := context.Background()\n+        ctx, span := s.tracer.Start(ctx, \"streamedListObjects\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+                attribute.KeyValue{Key: \"objectType\", Value: attribute.StringValue(req.GetType())},\n+        ))\n+        defer span.End()\n+\n+        modelID, err := s.resolveAuthorizationModelID(ctx, storeID, req.GetAuthorizationModelId())\n+        if err != nil {\n+                return err\n+        }\n+        q := &commands.ListObjectsQuery{\n+                Datastore:             s.datastore,\n+                Logger:                s.logger,\n+                Tracer:                s.tracer,\n+                Meter:                 s.meter,\n+                ListObjectsDeadline:   s.config.ListObjectsDeadline,\n+                ListObjectsMaxResults: s.config.ListObjectsMaxResults,\n+                ResolveNodeLimit:      s.config.ResolveNodeLimit,\n+        }\n+\n+        req.AuthorizationModelId = modelID\n+        return q.ExecuteStreamed(ctx, req, srv)\n }\n \n func (s *Server) Read(ctx context.Context, req *openfgapb.ReadRequest) (*openfgapb.ReadResponse, error) {\n-\tstore := req.GetStoreId()\n-\ttk := req.GetTupleKey()\n-\tctx, span := s.tracer.Start(ctx, \"read\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(store)},\n-\t\tattribute.KeyValue{Key: \"object\", Value: attribute.StringValue(tk.GetObject())},\n-\t\tattribute.KeyValue{Key: \"relation\", Value: attribute.StringValue(tk.GetRelation())},\n-\t\tattribute.KeyValue{Key: \"user\", Value: attribute.StringValue(tk.GetUser())},\n-\t))\n-\tdefer span.End()\n-\n-\tmodelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tspan.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n-\n-\tq := commands.NewReadQuery(s.datastore, s.tracer, s.logger, s.encoder)\n-\treturn q.Execute(ctx, &openfgapb.ReadRequest{\n-\t\tStoreId:              store,\n-\t\tTupleKey:             tk,\n-\t\tAuthorizationModelId: modelID,\n-\t\tPageSize:             req.GetPageSize(),\n-\t\tContinuationToken:    req.GetContinuationToken(),\n-\t})\n+        store := req.GetStoreId()\n+        tk := req.GetTupleKey()\n+        ctx, span := s.tracer.Start(ctx, \"read\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(store)},\n+                attribute.KeyValue{Key: \"object\", Value: attribute.StringValue(tk.GetObject())},\n+                attribute.KeyValue{Key: \"relation\", Value: attribute.StringValue(tk.GetRelation())},\n+                attribute.KeyValue{Key: \"user\", Value: attribute.StringValue(tk.GetUser())},\n+        ))\n+        defer span.End()\n+\n+        modelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n+        if err != nil {\n+                return nil, err\n+        }\n+        span.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n+\n+        q := commands.NewReadQuery(s.datastore, s.tracer, s.logger, s.encoder)\n+        return q.Execute(ctx, &openfgapb.ReadRequest{\n+                StoreId:              store,\n+                TupleKey:             tk,\n+                AuthorizationModelId: modelID,\n+                PageSize:             req.GetPageSize(),\n+                ContinuationToken:    req.GetContinuationToken(),\n+        })\n }\n \n func (s *Server) ReadTuples(ctx context.Context, req *openfgapb.ReadTuplesRequest) (*openfgapb.ReadTuplesResponse, error) {\n \n-\tctx, span := s.tracer.Start(ctx, \"readTuples\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t))\n-\tdefer span.End()\n+        ctx, span := s.tracer.Start(ctx, \"readTuples\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+        ))\n+        defer span.End()\n \n-\tq := commands.NewReadTuplesQuery(s.datastore, s.logger, s.encoder)\n-\treturn q.Execute(ctx, req)\n+        q := commands.NewReadTuplesQuery(s.datastore, s.logger, s.encoder)\n+        return q.Execute(ctx, req)\n }\n \n func (s *Server) Write(ctx context.Context, req *openfgapb.WriteRequest) (*openfgapb.WriteResponse, error) {\n-\tstore := req.GetStoreId()\n-\tctx, span := s.tracer.Start(ctx, \"write\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(store)},\n-\t))\n-\tdefer span.End()\n-\n-\tmodelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tcmd := commands.NewWriteCommand(s.datastore, s.tracer, s.logger)\n-\treturn cmd.Execute(ctx, &openfgapb.WriteRequest{\n-\t\tStoreId:              store,\n-\t\tAuthorizationModelId: modelID,\n-\t\tWrites:               req.GetWrites(),\n-\t\tDeletes:              req.GetDeletes(),\n-\t})\n+        store := req.GetStoreId()\n+        ctx, span := s.tracer.Start(ctx, \"write\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(store)},\n+        ))\n+        defer span.End()\n+\n+        modelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        cmd := commands.NewWriteCommand(s.datastore, s.tracer, s.logger)\n+        return cmd.Execute(ctx, &openfgapb.WriteRequest{\n+                StoreId:              store,\n+                AuthorizationModelId: modelID,\n+                Writes:               req.GetWrites(),\n+                Deletes:              req.GetDeletes(),\n+        })\n }\n \n func (s *Server) Check(ctx context.Context, req *openfgapb.CheckRequest) (*openfgapb.CheckResponse, error) {\n-\tstore := req.GetStoreId()\n-\ttk := req.GetTupleKey()\n-\tctx, span := s.tracer.Start(ctx, \"check\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t\tattribute.KeyValue{Key: \"object\", Value: attribute.StringValue(tk.GetObject())},\n-\t\tattribute.KeyValue{Key: \"relation\", Value: attribute.StringValue(tk.GetRelation())},\n-\t\tattribute.KeyValue{Key: \"user\", Value: attribute.StringValue(tk.GetUser())},\n-\t))\n-\tdefer span.End()\n-\n-\tmodelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tspan.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n-\n-\tq := commands.NewCheckQuery(s.datastore, s.tracer, s.meter, s.logger, s.config.ResolveNodeLimit)\n-\n-\tres, err := q.Execute(ctx, &openfgapb.CheckRequest{\n-\t\tStoreId:              store,\n-\t\tTupleKey:             tk,\n-\t\tContextualTuples:     req.GetContextualTuples(),\n-\t\tAuthorizationModelId: modelID,\n-\t\tTrace:                req.GetTrace(),\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tspan.SetAttributes(attribute.KeyValue{Key: \"allowed\", Value: attribute.BoolValue(res.GetAllowed())})\n-\treturn res, nil\n+        store := req.GetStoreId()\n+        tk := req.GetTupleKey()\n+        ctx, span := s.tracer.Start(ctx, \"check\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+                attribute.KeyValue{Key: \"object\", Value: attribute.StringValue(tk.GetObject())},\n+                attribute.KeyValue{Key: \"relation\", Value: attribute.StringValue(tk.GetRelation())},\n+                attribute.KeyValue{Key: \"user\", Value: attribute.StringValue(tk.GetUser())},\n+        ))\n+        defer span.End()\n+\n+        modelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n+        if err != nil {\n+                return nil, err\n+        }\n+        span.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n+\n+        q := commands.NewCheckQuery(s.datastore, s.tracer, s.meter, s.logger, s.config.ResolveNodeLimit)\n+\n+        res, err := q.Execute(ctx, &openfgapb.CheckRequest{\n+                StoreId:              store,\n+                TupleKey:             tk,\n+                ContextualTuples:     req.GetContextualTuples(),\n+                AuthorizationModelId: modelID,\n+                Trace:                req.GetTrace(),\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        span.SetAttributes(attribute.KeyValue{Key: \"allowed\", Value: attribute.BoolValue(res.GetAllowed())})\n+        return res, nil\n }\n \n func (s *Server) Expand(ctx context.Context, req *openfgapb.ExpandRequest) (*openfgapb.ExpandResponse, error) {\n-\tstore := req.GetStoreId()\n-\ttk := req.GetTupleKey()\n-\tctx, span := s.tracer.Start(ctx, \"expand\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(store)},\n-\t\tattribute.KeyValue{Key: \"object\", Value: attribute.StringValue(tk.GetObject())},\n-\t\tattribute.KeyValue{Key: \"relation\", Value: attribute.StringValue(tk.GetRelation())},\n-\t\tattribute.KeyValue{Key: \"user\", Value: attribute.StringValue(tk.GetUser())},\n-\t))\n-\tdefer span.End()\n-\n-\tmodelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tspan.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n-\n-\tq := commands.NewExpandQuery(s.datastore, s.tracer, s.logger)\n-\treturn q.Execute(ctx, &openfgapb.ExpandRequest{\n-\t\tStoreId:              store,\n-\t\tAuthorizationModelId: modelID,\n-\t\tTupleKey:             tk,\n-\t})\n+        store := req.GetStoreId()\n+        tk := req.GetTupleKey()\n+        ctx, span := s.tracer.Start(ctx, \"expand\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(store)},\n+                attribute.KeyValue{Key: \"object\", Value: attribute.StringValue(tk.GetObject())},\n+                attribute.KeyValue{Key: \"relation\", Value: attribute.StringValue(tk.GetRelation())},\n+                attribute.KeyValue{Key: \"user\", Value: attribute.StringValue(tk.GetUser())},\n+        ))\n+        defer span.End()\n+\n+        modelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n+        if err != nil {\n+                return nil, err\n+        }\n+        span.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n+\n+        q := commands.NewExpandQuery(s.datastore, s.tracer, s.logger)\n+        return q.Execute(ctx, &openfgapb.ExpandRequest{\n+                StoreId:              store,\n+                AuthorizationModelId: modelID,\n+                TupleKey:             tk,\n+        })\n }\n \n func (s *Server) ReadAuthorizationModel(ctx context.Context, req *openfgapb.ReadAuthorizationModelRequest) (*openfgapb.ReadAuthorizationModelResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"readAuthorizationModel\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t\tattribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(req.GetId())},\n-\t))\n-\tdefer span.End()\n-\n-\tq := commands.NewReadAuthorizationModelQuery(s.datastore, s.logger)\n-\treturn q.Execute(ctx, req)\n+        ctx, span := s.tracer.Start(ctx, \"readAuthorizationModel\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+                attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(req.GetId())},\n+        ))\n+        defer span.End()\n+\n+        q := commands.NewReadAuthorizationModelQuery(s.datastore, s.logger)\n+        return q.Execute(ctx, req)\n }\n \n func (s *Server) WriteAuthorizationModel(ctx context.Context, req *openfgapb.WriteAuthorizationModelRequest) (*openfgapb.WriteAuthorizationModelResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"writeAuthorizationModel\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t))\n-\tdefer span.End()\n+        ctx, span := s.tracer.Start(ctx, \"writeAuthorizationModel\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+        ))\n+        defer span.End()\n \n-\tc := commands.NewWriteAuthorizationModelCommand(s.datastore, s.logger)\n-\tres, err := c.Execute(ctx, req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        c := commands.NewWriteAuthorizationModelCommand(s.datastore, s.logger)\n+        res, err := c.Execute(ctx, req)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\ts.transport.SetHeader(ctx, httpmiddleware.XHttpCode, strconv.Itoa(http.StatusCreated))\n+        s.transport.SetHeader(ctx, httpmiddleware.XHttpCode, strconv.Itoa(http.StatusCreated))\n \n-\treturn res, nil\n+        return res, nil\n }\n \n func (s *Server) ReadAuthorizationModels(ctx context.Context, req *openfgapb.ReadAuthorizationModelsRequest) (*openfgapb.ReadAuthorizationModelsResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"readAuthorizationModels\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t))\n-\tdefer span.End()\n+        ctx, span := s.tracer.Start(ctx, \"readAuthorizationModels\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+        ))\n+        defer span.End()\n \n-\tc := commands.NewReadAuthorizationModelsQuery(s.datastore, s.logger, s.encoder)\n-\treturn c.Execute(ctx, req)\n+        c := commands.NewReadAuthorizationModelsQuery(s.datastore, s.logger, s.encoder)\n+        return c.Execute(ctx, req)\n }\n \n func (s *Server) WriteAssertions(ctx context.Context, req *openfgapb.WriteAssertionsRequest) (*openfgapb.WriteAssertionsResponse, error) {\n-\tstore := req.GetStoreId()\n-\tctx, span := s.tracer.Start(ctx, \"writeAssertions\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(store)},\n-\t))\n-\tdefer span.End()\n-\n-\tmodelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tspan.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n-\n-\tc := commands.NewWriteAssertionsCommand(s.datastore, s.logger)\n-\tres, err := c.Execute(ctx, &openfgapb.WriteAssertionsRequest{\n-\t\tStoreId:              store,\n-\t\tAuthorizationModelId: modelID,\n-\t\tAssertions:           req.GetAssertions(),\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.transport.SetHeader(ctx, httpmiddleware.XHttpCode, strconv.Itoa(http.StatusNoContent))\n-\n-\treturn res, nil\n+        store := req.GetStoreId()\n+        ctx, span := s.tracer.Start(ctx, \"writeAssertions\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(store)},\n+        ))\n+        defer span.End()\n+\n+        modelID, err := s.resolveAuthorizationModelID(ctx, store, req.GetAuthorizationModelId())\n+        if err != nil {\n+                return nil, err\n+        }\n+        span.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n+\n+        c := commands.NewWriteAssertionsCommand(s.datastore, s.logger)\n+        res, err := c.Execute(ctx, &openfgapb.WriteAssertionsRequest{\n+                StoreId:              store,\n+                AuthorizationModelId: modelID,\n+                Assertions:           req.GetAssertions(),\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        s.transport.SetHeader(ctx, httpmiddleware.XHttpCode, strconv.Itoa(http.StatusNoContent))\n+\n+        return res, nil\n }\n \n func (s *Server) ReadAssertions(ctx context.Context, req *openfgapb.ReadAssertionsRequest) (*openfgapb.ReadAssertionsResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"readAssertions\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t))\n-\tdefer span.End()\n-\tmodelID, err := s.resolveAuthorizationModelID(ctx, req.GetStoreId(), req.GetAuthorizationModelId())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tspan.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n-\tq := commands.NewReadAssertionsQuery(s.datastore, s.logger)\n-\treturn q.Execute(ctx, req.GetStoreId(), req.GetAuthorizationModelId())\n+        ctx, span := s.tracer.Start(ctx, \"readAssertions\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+        ))\n+        defer span.End()\n+        modelID, err := s.resolveAuthorizationModelID(ctx, req.GetStoreId(), req.GetAuthorizationModelId())\n+        if err != nil {\n+                return nil, err\n+        }\n+        span.SetAttributes(attribute.KeyValue{Key: \"authorization-model-id\", Value: attribute.StringValue(modelID)})\n+        q := commands.NewReadAssertionsQuery(s.datastore, s.logger)\n+        return q.Execute(ctx, req.GetStoreId(), req.GetAuthorizationModelId())\n }\n \n func (s *Server) ReadChanges(ctx context.Context, req *openfgapb.ReadChangesRequest) (*openfgapb.ReadChangesResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"ReadChangesQuery\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t\tattribute.KeyValue{Key: \"type\", Value: attribute.StringValue(req.GetType())},\n-\t))\n-\tdefer span.End()\n-\n-\tq := commands.NewReadChangesQuery(s.datastore, s.tracer, s.logger, s.encoder, s.config.ChangelogHorizonOffset)\n-\treturn q.Execute(ctx, req)\n+        ctx, span := s.tracer.Start(ctx, \"ReadChangesQuery\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+                attribute.KeyValue{Key: \"type\", Value: attribute.StringValue(req.GetType())},\n+        ))\n+        defer span.End()\n+\n+        q := commands.NewReadChangesQuery(s.datastore, s.tracer, s.logger, s.encoder, s.config.ChangelogHorizonOffset)\n+        return q.Execute(ctx, req)\n }\n \n func (s *Server) CreateStore(ctx context.Context, req *openfgapb.CreateStoreRequest) (*openfgapb.CreateStoreResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"createStore\")\n-\tdefer span.End()\n+        ctx, span := s.tracer.Start(ctx, \"createStore\")\n+        defer span.End()\n \n-\tc := commands.NewCreateStoreCommand(s.datastore, s.logger)\n-\tres, err := c.Execute(ctx, req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        c := commands.NewCreateStoreCommand(s.datastore, s.logger)\n+        res, err := c.Execute(ctx, req)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\ts.transport.SetHeader(ctx, httpmiddleware.XHttpCode, strconv.Itoa(http.StatusCreated))\n+        s.transport.SetHeader(ctx, httpmiddleware.XHttpCode, strconv.Itoa(http.StatusCreated))\n \n-\treturn res, nil\n+        return res, nil\n }\n \n func (s *Server) DeleteStore(ctx context.Context, req *openfgapb.DeleteStoreRequest) (*openfgapb.DeleteStoreResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"deleteStore\")\n-\tdefer span.End()\n+        ctx, span := s.tracer.Start(ctx, \"deleteStore\")\n+        defer span.End()\n \n-\tcmd := commands.NewDeleteStoreCommand(s.datastore, s.logger)\n-\tres, err := cmd.Execute(ctx, req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        cmd := commands.NewDeleteStoreCommand(s.datastore, s.logger)\n+        res, err := cmd.Execute(ctx, req)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\ts.transport.SetHeader(ctx, httpmiddleware.XHttpCode, strconv.Itoa(http.StatusNoContent))\n+        s.transport.SetHeader(ctx, httpmiddleware.XHttpCode, strconv.Itoa(http.StatusNoContent))\n \n-\treturn res, nil\n+        return res, nil\n }\n \n func (s *Server) GetStore(ctx context.Context, req *openfgapb.GetStoreRequest) (*openfgapb.GetStoreResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"getStore\", trace.WithAttributes(\n-\t\tattribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n-\t))\n-\tdefer span.End()\n+        ctx, span := s.tracer.Start(ctx, \"getStore\", trace.WithAttributes(\n+                attribute.KeyValue{Key: \"store\", Value: attribute.StringValue(req.GetStoreId())},\n+        ))\n+        defer span.End()\n \n-\tq := commands.NewGetStoreQuery(s.datastore, s.logger)\n-\treturn q.Execute(ctx, req)\n+        q := commands.NewGetStoreQuery(s.datastore, s.logger)\n+        return q.Execute(ctx, req)\n }\n \n func (s *Server) ListStores(ctx context.Context, req *openfgapb.ListStoresRequest) (*openfgapb.ListStoresResponse, error) {\n-\tctx, span := s.tracer.Start(ctx, \"listStores\")\n-\tdefer span.End()\n+        ctx, span := s.tracer.Start(ctx, \"listStores\")\n+        defer span.End()\n \n-\tq := commands.NewListStoresQuery(s.datastore, s.logger, s.encoder)\n-\treturn q.Execute(ctx, req)\n+        q := commands.NewListStoresQuery(s.datastore, s.logger, s.encoder)\n+        return q.Execute(ctx, req)\n }\n \n // IsReady reports whether this OpenFGA server instance is ready to accept\n // traffic.\n func (s *Server) IsReady(ctx context.Context) (bool, error) {\n \n-\t// for now we only depend on the datastore being ready, but in the future\n-\t// server readiness may also depend on other criteria in addition to the\n-\t// datastore being ready.\n-\treturn s.datastore.IsReady(ctx)\n+        // for now we only depend on the datastore being ready, but in the future\n+        // server readiness may also depend on other criteria in addition to the\n+        // datastore being ready.\n+        return s.datastore.IsReady(ctx)\n }\n \n // Run starts server execution, and blocks until complete, returning any server errors. To close the\n // server cancel the provided ctx.\n func (s *Server) Run(ctx context.Context) error {\n \n-\tinterceptors := []grpc.UnaryServerInterceptor{\n-\t\tgrpc_validator.UnaryServerInterceptor(),\n-\t}\n-\tinterceptors = append(interceptors, s.config.UnaryInterceptors...)\n-\n-\topts := []grpc.ServerOption{\n-\t\tgrpc.ChainUnaryInterceptor(interceptors...),\n-\t}\n-\n-\tif s.config.GRPCServer.TLSConfig != nil {\n-\t\tcreds, err := credentials.NewServerTLSFromFile(s.config.GRPCServer.TLSConfig.CertPath, s.config.GRPCServer.TLSConfig.KeyPath)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\topts = append(opts, grpc.Creds(creds))\n-\t}\n-\t// nosemgrep: grpc-server-insecure-connection\n-\tgrpcServer := grpc.NewServer(opts...)\n-\topenfgapb.RegisterOpenFGAServiceServer(grpcServer, s)\n-\thealthServer := &health.Checker{TargetService: s, TargetServiceName: openfgapb.OpenFGAService_ServiceDesc.ServiceName}\n-\thealthv1pb.RegisterHealthServer(grpcServer, healthServer)\n-\treflection.Register(grpcServer)\n-\n-\trpcAddr := s.config.GRPCServer.Addr\n-\tlis, err := net.Listen(\"tcp\", rpcAddr.String())\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tgo func() {\n-\t\tif err := grpcServer.Serve(lis); err != nil {\n-\t\t\ts.logger.Error(\"failed to start grpc server\", logger.Error(err))\n-\t\t}\n-\t}()\n-\n-\ts.logger.Info(fmt.Sprintf(\"grpc server listening on '%s'...\", rpcAddr))\n-\n-\tvar httpServer *http.Server\n-\tif s.config.HTTPServer.Enabled {\n-\t\t// Set a request timeout.\n-\t\truntime.DefaultContextTimeout = s.config.HTTPServer.UpstreamTimeout\n-\n-\t\tdialOpts := []grpc.DialOption{\n-\t\t\tgrpc.WithBlock(),\n-\t\t\tgrpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor()),\n-\t\t}\n-\t\tif s.config.GRPCServer.TLSConfig != nil {\n-\t\t\tcreds, err := credentials.NewClientTLSFromFile(s.config.GRPCServer.TLSConfig.CertPath, \"\")\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tdialOpts = append(dialOpts, grpc.WithTransportCredentials(creds))\n-\t\t} else {\n-\t\t\tdialOpts = append(dialOpts, grpc.WithTransportCredentials(insecure.NewCredentials()))\n-\t\t}\n-\n-\t\ttimeoutCtx, cancel := context.WithTimeout(ctx, 3*time.Second)\n-\t\tdefer cancel()\n-\n-\t\tconn, err := grpc.DialContext(timeoutCtx, rpcAddr.String(), dialOpts...)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tdefer conn.Close()\n-\n-\t\thealthClient := healthv1pb.NewHealthClient(conn)\n-\n-\t\tmuxOpts := []runtime.ServeMuxOption{\n-\t\t\truntime.WithHealthzEndpoint(healthClient),\n-\t\t}\n-\t\tmuxOpts = append(muxOpts, s.defaultServeMuxOpts...) // register the defaults first\n-\t\tmuxOpts = append(muxOpts, s.config.MuxOptions...)   // any provided options override defaults if they are duplicates\n-\n-\t\tmux := runtime.NewServeMux(muxOpts...)\n-\n-\t\tif err := openfgapb.RegisterOpenFGAServiceHandler(ctx, mux, conn); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\thttpServer = &http.Server{\n-\t\t\tAddr: s.config.HTTPServer.Addr.String(),\n-\t\t\tHandler: cors.New(cors.Options{\n-\t\t\t\tAllowedOrigins:   s.config.HTTPServer.CORSAllowedOrigins,\n-\t\t\t\tAllowCredentials: true,\n-\t\t\t\tAllowedHeaders:   s.config.HTTPServer.CORSAllowedHeaders,\n-\t\t\t\tAllowedMethods: []string{http.MethodGet, http.MethodPost,\n-\t\t\t\t\thttp.MethodHead, http.MethodPatch, http.MethodDelete, http.MethodPut},\n-\t\t\t}).Handler(mux),\n-\t\t}\n-\n-\t\tgo func() {\n-\t\t\ts.logger.Info(fmt.Sprintf(\"HTTP server listening on '%s'...\", httpServer.Addr))\n-\n-\t\t\tvar err error\n-\t\t\tif s.config.HTTPServer.TLSConfig != nil {\n-\t\t\t\terr = httpServer.ListenAndServeTLS(s.config.HTTPServer.TLSConfig.CertPath, s.config.HTTPServer.TLSConfig.KeyPath)\n-\t\t\t} else {\n-\t\t\t\terr = httpServer.ListenAndServe()\n-\t\t\t}\n-\t\t\tif err != http.ErrServerClosed {\n-\t\t\t\ts.logger.ErrorWithContext(ctx, \"HTTP server closed with unexpected error\", logger.Error(err))\n-\t\t\t}\n-\t\t}()\n-\t}\n-\n-\t<-ctx.Done()\n-\ts.logger.InfoWithContext(ctx, \"Termination signal received! Gracefully shutting down\")\n-\n-\tctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n-\tdefer cancel()\n-\n-\tif httpServer != nil {\n-\t\tif err := httpServer.Shutdown(ctx); err != nil {\n-\t\t\ts.logger.ErrorWithContext(ctx, \"HTTP server shutdown failed\", logger.Error(err))\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\tgrpcServer.GracefulStop()\n-\n-\treturn nil\n+        interceptors := []grpc.UnaryServerInterceptor{\n+                grpc_validator.UnaryServerInterceptor(),\n+        }\n+        interceptors = append(interceptors, s.config.UnaryInterceptors...)\n+\n+        opts := []grpc.ServerOption{\n+                grpc.ChainUnaryInterceptor(interceptors...),\n+grpc.ChainStreamInterceptor(s.config.StreamInterceptors...),\n+        }\n+\n+        if s.config.GRPCServer.TLSConfig != nil {\n+                creds, err := credentials.NewServerTLSFromFile(s.config.GRPCServer.TLSConfig.CertPath, s.config.GRPCServer.TLSConfig.KeyPath)\n+                if err != nil {\n+                        return err\n+                }\n+                opts = append(opts, grpc.Creds(creds))\n+        }\n+        // nosemgrep: grpc-server-insecure-connection\n+        grpcServer := grpc.NewServer(opts...)\n+        openfgapb.RegisterOpenFGAServiceServer(grpcServer, s)\n+        healthServer := &health.Checker{TargetService: s, TargetServiceName: openfgapb.OpenFGAService_ServiceDesc.ServiceName}\n+        healthv1pb.RegisterHealthServer(grpcServer, healthServer)\n+        reflection.Register(grpcServer)\n+\n+        rpcAddr := s.config.GRPCServer.Addr\n+        lis, err := net.Listen(\"tcp\", rpcAddr.String())\n+        if err != nil {\n+                return err\n+        }\n+\n+        go func() {\n+                if err := grpcServer.Serve(lis); err != nil {\n+                        s.logger.Error(\"failed to start grpc server\", logger.Error(err))\n+                }\n+        }()\n+\n+        s.logger.Info(fmt.Sprintf(\"grpc server listening on '%s'...\", rpcAddr))\n+\n+        var httpServer *http.Server\n+        if s.config.HTTPServer.Enabled {\n+                // Set a request timeout.\n+                runtime.DefaultContextTimeout = s.config.HTTPServer.UpstreamTimeout\n+\n+                dialOpts := []grpc.DialOption{\n+                        grpc.WithBlock(),\n+                        grpc.WithUnaryInterceptor(otelgrpc.UnaryClientInterceptor()),\n+                }\n+                if s.config.GRPCServer.TLSConfig != nil {\n+                        creds, err := credentials.NewClientTLSFromFile(s.config.GRPCServer.TLSConfig.CertPath, \"\")\n+                        if err != nil {\n+                                return err\n+                        }\n+                        dialOpts = append(dialOpts, grpc.WithTransportCredentials(creds))\n+                } else {\n+                        dialOpts = append(dialOpts, grpc.WithTransportCredentials(insecure.NewCredentials()))\n+                }\n+\n+                timeoutCtx, cancel := context.WithTimeout(ctx, 3*time.Second)\n+                defer cancel()\n+\n+                conn, err := grpc.DialContext(timeoutCtx, rpcAddr.String(), dialOpts...)\n+                if err != nil {\n+                        return err\n+                }\n+                defer conn.Close()\n+\n+                healthClient := healthv1pb.NewHealthClient(conn)\n+\n+                muxOpts := []runtime.ServeMuxOption{\n+                        runtime.WithHealthzEndpoint(healthClient),\n+                }\n+                muxOpts = append(muxOpts, s.defaultServeMuxOpts...) // register the defaults first\n+                muxOpts = append(muxOpts, s.config.MuxOptions...)   // any provided options override defaults if they are duplicates\n+\n+                mux := runtime.NewServeMux(muxOpts...)\n+\n+                if err := openfgapb.RegisterOpenFGAServiceHandler(ctx, mux, conn); err != nil {\n+                        return err\n+                }\n+\n+                httpServer = &http.Server{\n+                        Addr: s.config.HTTPServer.Addr.String(),\n+                        Handler: cors.New(cors.Options{\n+                                AllowedOrigins:   s.config.HTTPServer.CORSAllowedOrigins,\n+                                AllowCredentials: true,\n+                                AllowedHeaders:   s.config.HTTPServer.CORSAllowedHeaders,\n+                                AllowedMethods: []string{http.MethodGet, http.MethodPost,\n+                                        http.MethodHead, http.MethodPatch, http.MethodDelete, http.MethodPut},\n+                        }).Handler(mux),\n+                }\n+\n+                go func() {\n+                        s.logger.Info(fmt.Sprintf(\"HTTP server listening on '%s'...\", httpServer.Addr))\n+\n+                        var err error\n+                        if s.config.HTTPServer.TLSConfig != nil {\n+                                err = httpServer.ListenAndServeTLS(s.config.HTTPServer.TLSConfig.CertPath, s.config.HTTPServer.TLSConfig.KeyPath)\n+                        } else {\n+                                err = httpServer.ListenAndServe()\n+                        }\n+                        if err != http.ErrServerClosed {\n+                                s.logger.ErrorWithContext(ctx, \"HTTP server closed with unexpected error\", logger.Error(err))\n+                        }\n+                }()\n+        }\n+\n+        <-ctx.Done()\n+        s.logger.InfoWithContext(ctx, \"Termination signal received! Gracefully shutting down\")\n+\n+        ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)\n+        defer cancel()\n+\n+        if httpServer != nil {\n+                if err := httpServer.Shutdown(ctx); err != nil {\n+                        s.logger.ErrorWithContext(ctx, \"HTTP server shutdown failed\", logger.Error(err))\n+                        return err\n+                }\n+        }\n+\n+        grpcServer.GracefulStop()\n+\n+        return nil\n }\n \n // Util to find the latest authorization model ID to be used through all the request lifecycle.\n@@ -614,24 +616,24 @@ func (s *Server) Run(ctx context.Context) error {\n // provide this field (which should be rate limited more aggressively) the in-flight requests won't be\n // affected and newer calls will use the updated authorization model.\n func (s *Server) resolveAuthorizationModelID(ctx context.Context, store, modelID string) (string, error) {\n-\tctx, span := s.tracer.Start(ctx, \"resolveAuthorizationModelID\")\n-\tdefer span.End()\n-\n-\tvar err error\n-\tif modelID != \"\" {\n-\t\tif !id.IsValid(modelID) {\n-\t\t\treturn \"\", serverErrors.AuthorizationModelNotFound(modelID)\n-\t\t}\n-\t} else {\n-\t\tif modelID, err = s.datastore.FindLatestAuthorizationModelID(ctx, store); err != nil {\n-\t\t\tif errors.Is(err, storage.ErrNotFound) {\n-\t\t\t\treturn \"\", serverErrors.LatestAuthorizationModelNotFound(store)\n-\t\t\t}\n-\t\t\treturn \"\", serverErrors.HandleError(\"\", err)\n-\t\t}\n-\t}\n-\n-\ts.transport.SetHeader(ctx, AuthorizationModelIDHeader, modelID)\n-\n-\treturn modelID, nil\n+        ctx, span := s.tracer.Start(ctx, \"resolveAuthorizationModelID\")\n+        defer span.End()\n+\n+        var err error\n+        if modelID != \"\" {\n+                if !id.IsValid(modelID) {\n+                        return \"\", serverErrors.AuthorizationModelNotFound(modelID)\n+                }\n+        } else {\n+                if modelID, err = s.datastore.FindLatestAuthorizationModelID(ctx, store); err != nil {\n+                        if errors.Is(err, storage.ErrNotFound) {\n+                                return \"\", serverErrors.LatestAuthorizationModelNotFound(store)\n+                        }\n+                        return \"\", serverErrors.HandleError(\"\", err)\n+                }\n+        }\n+\n+        s.transport.SetHeader(ctx, AuthorizationModelIDHeader, modelID)\n+\n+        return modelID, nil\n }\n"}
{"cve":"CVE-2018-12976:0708", "fix_patch": "diff --git a/gosrc/gosrc.go b/gosrc/gosrc.go\nindex 9a9e44b..b2d7f77 100644\n--- a/gosrc/gosrc.go\n+++ b/gosrc/gosrc.go\n@@ -8,526 +8,532 @@\n package gosrc\n \n import (\n-\t\"context\"\n-\t\"encoding/xml\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"path\"\n-\t\"regexp\"\n-\t\"strings\"\n-\t\"time\"\n+        \"context\"\n+        \"encoding/xml\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"path\"\n+        \"regexp\"\n+        \"strings\"\n+        \"time\"\n )\n \n const ExpiresAfter = 2 * 365 * 24 * time.Hour // Package with no commits and imports expires.\n \n // File represents a file.\n type File struct {\n-\t// File name with no directory.\n-\tName string\n+        // File name with no directory.\n+        Name string\n \n-\t// Contents of the file.\n-\tData []byte\n+        // Contents of the file.\n+        Data []byte\n \n-\t// Location of file on version control service website.\n-\tBrowseURL string\n+        // Location of file on version control service website.\n+        BrowseURL string\n }\n \n type DirectoryStatus int\n \n const (\n-\tActive          DirectoryStatus = iota\n-\tDeadEndFork                     // Forks with no commits\n-\tQuickFork                       // Forks with less than 3 commits, all within a week from creation\n-\tNoRecentCommits                 // No commits for ExpiresAfter\n-\n-\t// No commits for ExpiresAfter and no imports.\n-\t// This is a status derived from NoRecentCommits and the imports count information in the db.\n-\tInactive\n+        Active          DirectoryStatus = iota\n+        DeadEndFork                     // Forks with no commits\n+        QuickFork                       // Forks with less than 3 commits, all within a week from creation\n+        NoRecentCommits                 // No commits for ExpiresAfter\n+\n+        // No commits for ExpiresAfter and no imports.\n+        // This is a status derived from NoRecentCommits and the imports count information in the db.\n+        Inactive\n )\n \n // Directory describes a directory on a version control service.\n type Directory struct {\n-\t// The import path for this package.\n-\tImportPath string\n+        // The import path for this package.\n+        ImportPath string\n \n-\t// Import path of package after resolving go-import meta tags, if any.\n-\tResolvedPath string\n+        // Import path of package after resolving go-import meta tags, if any.\n+        ResolvedPath string\n \n-\t// Import path prefix for all packages in the project.\n-\tProjectRoot string\n+        // Import path prefix for all packages in the project.\n+        ProjectRoot string\n \n-\t// Name of the project.\n-\tProjectName string\n+        // Name of the project.\n+        ProjectName string\n \n-\t// Project home page.\n-\tProjectURL string\n+        // Project home page.\n+        ProjectURL string\n \n-\t// Version control system: git, hg, bzr, ...\n-\tVCS string\n+        // Version control system: git, hg, bzr, ...\n+        VCS string\n \n-\t// Version control: active or should be suppressed.\n-\tStatus DirectoryStatus\n+        // Version control: active or should be suppressed.\n+        Status DirectoryStatus\n \n-\t// Cache validation tag. This tag is not necessarily an HTTP entity tag.\n-\t// The tag is \"\" if there is no meaningful cache validation for the VCS.\n-\tEtag string\n+        // Cache validation tag. This tag is not necessarily an HTTP entity tag.\n+        // The tag is \"\" if there is no meaningful cache validation for the VCS.\n+        Etag string\n \n-\t// Files.\n-\tFiles []*File\n+        // Files.\n+        Files []*File\n \n-\t// Subdirectories, not guaranteed to contain Go code.\n-\tSubdirectories []string\n+        // Subdirectories, not guaranteed to contain Go code.\n+        Subdirectories []string\n \n-\t// Location of directory on version control service website.\n-\tBrowseURL string\n+        // Location of directory on version control service website.\n+        BrowseURL string\n \n-\t// Format specifier for link to source line. It must contain one %s (file URL)\n-\t// followed by one %d (source line number), or be empty string if not available.\n-\t// Example: \"%s#L%d\".\n-\tLineFmt string\n+        // Format specifier for link to source line. It must contain one %s (file URL)\n+        // followed by one %d (source line number), or be empty string if not available.\n+        // Example: \"%s#L%d\".\n+        LineFmt string\n \n-\t// Whether the repository of this directory is a fork of another one.\n-\tFork bool\n+        // Whether the repository of this directory is a fork of another one.\n+        Fork bool\n \n-\t// How many stars (for a GitHub project) or followers (for a BitBucket\n-\t// project) the repository of this directory has.\n-\tStars int\n+        // How many stars (for a GitHub project) or followers (for a BitBucket\n+        // project) the repository of this directory has.\n+        Stars int\n }\n \n // Project represents a repository.\n type Project struct {\n-\tDescription string\n+        Description string\n }\n \n // NotFoundError indicates that the directory or presentation was not found.\n type NotFoundError struct {\n-\t// Diagnostic message describing why the directory was not found.\n-\tMessage string\n+        // Diagnostic message describing why the directory was not found.\n+        Message string\n \n-\t// Redirect specifies the path where package can be found.\n-\tRedirect string\n+        // Redirect specifies the path where package can be found.\n+        Redirect string\n }\n \n func (e NotFoundError) Error() string {\n-\treturn e.Message\n+        return e.Message\n }\n \n // IsNotFound returns true if err is of type NotFoundError.\n func IsNotFound(err error) bool {\n-\t_, ok := err.(NotFoundError)\n-\treturn ok\n+        _, ok := err.(NotFoundError)\n+        return ok\n }\n \n type RemoteError struct {\n-\tHost string\n-\terr  error\n+        Host string\n+        err  error\n }\n \n func (e *RemoteError) Error() string {\n-\treturn e.err.Error()\n+        return e.err.Error()\n }\n \n type NotModifiedError struct {\n-\tSince  time.Time\n-\tStatus DirectoryStatus\n+        Since  time.Time\n+        Status DirectoryStatus\n }\n \n func (e NotModifiedError) Error() string {\n-\tmsg := \"package not modified\"\n-\tif !e.Since.IsZero() {\n-\t\tmsg += fmt.Sprintf(\" since %s\", e.Since.Format(time.RFC1123))\n-\t}\n-\tif e.Status == QuickFork {\n-\t\tmsg += \" (package is a quick fork)\"\n-\t}\n-\treturn msg\n+        msg := \"package not modified\"\n+        if !e.Since.IsZero() {\n+                msg += fmt.Sprintf(\" since %s\", e.Since.Format(time.RFC1123))\n+        }\n+        if e.Status == QuickFork {\n+                msg += \" (package is a quick fork)\"\n+        }\n+        return msg\n }\n \n var errNoMatch = errors.New(\"no match\")\n \n // service represents a source code control service.\n type service struct {\n-\tpattern         *regexp.Regexp\n-\tprefix          string\n-\tget             func(context.Context, *http.Client, map[string]string, string) (*Directory, error)\n-\tgetPresentation func(context.Context, *http.Client, map[string]string) (*Presentation, error)\n-\tgetProject      func(context.Context, *http.Client, map[string]string) (*Project, error)\n+        pattern         *regexp.Regexp\n+        prefix          string\n+        get             func(context.Context, *http.Client, map[string]string, string) (*Directory, error)\n+        getPresentation func(context.Context, *http.Client, map[string]string) (*Presentation, error)\n+        getProject      func(context.Context, *http.Client, map[string]string) (*Project, error)\n }\n \n var services []*service\n \n func addService(s *service) {\n-\tif s.prefix == \"\" {\n-\t\tservices = append(services, s)\n-\t} else {\n-\t\tservices = append([]*service{s}, services...)\n-\t}\n+        if s.prefix == \"\" {\n+                services = append(services, s)\n+        } else {\n+                services = append([]*service{s}, services...)\n+        }\n }\n \n func (s *service) match(importPath string) (map[string]string, error) {\n-\tif !strings.HasPrefix(importPath, s.prefix) {\n-\t\treturn nil, nil\n-\t}\n-\tm := s.pattern.FindStringSubmatch(importPath)\n-\tif m == nil {\n-\t\tif s.prefix != \"\" {\n-\t\t\treturn nil, NotFoundError{Message: \"Import path prefix matches known service, but regexp does not.\"}\n-\t\t}\n-\t\treturn nil, nil\n-\t}\n-\tmatch := map[string]string{\"importPath\": importPath}\n-\tfor i, n := range s.pattern.SubexpNames() {\n-\t\tif n != \"\" {\n-\t\t\tmatch[n] = m[i]\n-\t\t}\n-\t}\n-\treturn match, nil\n+        if !strings.HasPrefix(importPath, s.prefix) {\n+                return nil, nil\n+        }\n+        m := s.pattern.FindStringSubmatch(importPath)\n+        if m == nil {\n+                if s.prefix != \"\" {\n+                        return nil, NotFoundError{Message: \"Import path prefix matches known service, but regexp does not.\"}\n+                }\n+                return nil, nil\n+        }\n+        match := map[string]string{\"importPath\": importPath}\n+        for i, n := range s.pattern.SubexpNames() {\n+                if n != \"\" {\n+                        match[n] = m[i]\n+                }\n+        }\n+        return match, nil\n }\n \n // importMeta represents the values in a go-import meta tag.\n type importMeta struct {\n-\tprojectRoot string\n-\tvcs         string\n-\trepo        string\n+        projectRoot string\n+        vcs         string\n+        repo        string\n }\n \n // sourceMeta represents the values in a go-source meta tag.\n type sourceMeta struct {\n-\tprojectRoot  string\n-\tprojectURL   string\n-\tdirTemplate  string\n-\tfileTemplate string\n+        projectRoot  string\n+        projectURL   string\n+        dirTemplate  string\n+        fileTemplate string\n }\n \n func isHTTPURL(s string) bool {\n-\treturn strings.HasPrefix(s, \"https://\") || strings.HasPrefix(s, \"http://\")\n+        return strings.HasPrefix(s, \"https://\") || strings.HasPrefix(s, \"http://\")\n }\n \n func replaceDir(s string, dir string) string {\n-\tslashDir := \"\"\n-\tdir = strings.Trim(dir, \"/\")\n-\tif dir != \"\" {\n-\t\tslashDir = \"/\" + dir\n-\t}\n-\ts = strings.Replace(s, \"{dir}\", dir, -1)\n-\ts = strings.Replace(s, \"{/dir}\", slashDir, -1)\n-\treturn s\n+        slashDir := \"\"\n+        dir = strings.Trim(dir, \"/\")\n+        if dir != \"\" {\n+                slashDir = \"/\" + dir\n+        }\n+        s = strings.Replace(s, \"{dir}\", dir, -1)\n+        s = strings.Replace(s, \"{/dir}\", slashDir, -1)\n+        return s\n }\n \n func attrValue(attrs []xml.Attr, name string) string {\n-\tfor _, a := range attrs {\n-\t\tif strings.EqualFold(a.Name.Local, name) {\n-\t\t\treturn a.Value\n-\t\t}\n-\t}\n-\treturn \"\"\n+        for _, a := range attrs {\n+                if strings.EqualFold(a.Name.Local, name) {\n+                        return a.Value\n+                }\n+        }\n+        return \"\"\n }\n \n func fetchMeta(ctx context.Context, client *http.Client, importPath string) (scheme string, im *importMeta, sm *sourceMeta, redir bool, err error) {\n-\turi := importPath\n-\tif !strings.Contains(uri, \"/\") {\n-\t\t// Add slash for root of domain.\n-\t\turi = uri + \"/\"\n-\t}\n-\turi = uri + \"?go-get=1\"\n-\n-\tc := httpClient{client: client}\n-\tscheme = \"https\"\n-\tresp, err := c.get(ctx, scheme+\"://\"+uri)\n-\tif err != nil || resp.StatusCode != 200 {\n-\t\tif err == nil {\n-\t\t\tresp.Body.Close()\n-\t\t}\n-\t\tscheme = \"http\"\n-\t\tresp, err = c.get(ctx, scheme+\"://\"+uri)\n-\t\tif err != nil {\n-\t\t\treturn scheme, nil, nil, false, err\n-\t\t}\n-\t}\n-\tdefer resp.Body.Close()\n-\tim, sm, redir, err = parseMeta(scheme, importPath, resp.Body)\n-\treturn scheme, im, sm, redir, err\n+        uri := importPath\n+        if !strings.Contains(uri, \"/\") {\n+                // Add slash for root of domain.\n+                uri = uri + \"/\"\n+        }\n+        uri = uri + \"?go-get=1\"\n+\n+        c := httpClient{client: client}\n+        scheme = \"https\"\n+        resp, err := c.get(ctx, scheme+\"://\"+uri)\n+        if err != nil || resp.StatusCode != 200 {\n+                if err == nil {\n+                        resp.Body.Close()\n+                }\n+                scheme = \"http\"\n+                resp, err = c.get(ctx, scheme+\"://\"+uri)\n+                if err != nil {\n+                        return scheme, nil, nil, false, err\n+                }\n+        }\n+        defer resp.Body.Close()\n+        im, sm, redir, err = parseMeta(scheme, importPath, resp.Body)\n+        return scheme, im, sm, redir, err\n }\n \n var refreshToGodocPat = regexp.MustCompile(`(?i)^\\d+; url=https?://godoc\\.org/`)\n \n func parseMeta(scheme, importPath string, r io.Reader) (im *importMeta, sm *sourceMeta, redir bool, err error) {\n-\terrorMessage := \"go-import meta tag not found\"\n+        errorMessage := \"go-import meta tag not found\"\n \n-\td := xml.NewDecoder(r)\n-\td.Strict = false\n+        d := xml.NewDecoder(r)\n+        d.Strict = false\n metaScan:\n-\tfor {\n-\t\tt, tokenErr := d.Token()\n-\t\tif tokenErr != nil {\n-\t\t\tbreak metaScan\n-\t\t}\n-\t\tswitch t := t.(type) {\n-\t\tcase xml.EndElement:\n-\t\t\tif strings.EqualFold(t.Name.Local, \"head\") {\n-\t\t\t\tbreak metaScan\n-\t\t\t}\n-\t\tcase xml.StartElement:\n-\t\t\tif strings.EqualFold(t.Name.Local, \"body\") {\n-\t\t\t\tbreak metaScan\n-\t\t\t}\n-\t\t\tif !strings.EqualFold(t.Name.Local, \"meta\") {\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tif strings.EqualFold(attrValue(t.Attr, \"http-equiv\"), \"refresh\") {\n-\t\t\t\t// Check for http-equiv refresh back to godoc.org.\n-\t\t\t\tredir = refreshToGodocPat.MatchString(attrValue(t.Attr, \"content\"))\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tnameAttr := attrValue(t.Attr, \"name\")\n-\t\t\tif nameAttr != \"go-import\" && nameAttr != \"go-source\" {\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tfields := strings.Fields(attrValue(t.Attr, \"content\"))\n-\t\t\tif len(fields) < 1 {\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tprojectRoot := fields[0]\n-\t\t\tif !strings.HasPrefix(importPath, projectRoot) ||\n-\t\t\t\t!(len(importPath) == len(projectRoot) || importPath[len(projectRoot)] == '/') {\n-\t\t\t\t// Ignore if root is not a prefix of the  path. This allows a\n-\t\t\t\t// site to use a single error page for multiple repositories.\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tswitch nameAttr {\n-\t\t\tcase \"go-import\":\n-\t\t\t\tif len(fields) != 3 {\n-\t\t\t\t\terrorMessage = \"go-import meta tag content attribute does not have three fields\"\n-\t\t\t\t\tcontinue metaScan\n-\t\t\t\t}\n-\t\t\t\tif fields[1] == \"mod\" {\n-\t\t\t\t\t// vgo adds a special mod vcs type; we can skip this\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tif im != nil {\n-\t\t\t\t\tim = nil\n-\t\t\t\t\terrorMessage = \"more than one go-import meta tag found\"\n-\t\t\t\t\tbreak metaScan\n-\t\t\t\t}\n-\t\t\t\tim = &importMeta{\n-\t\t\t\t\tprojectRoot: projectRoot,\n-\t\t\t\t\tvcs:         fields[1],\n-\t\t\t\t\trepo:        fields[2],\n-\t\t\t\t}\n-\t\t\tcase \"go-source\":\n-\t\t\t\tif sm != nil {\n-\t\t\t\t\t// Ignore extra go-source meta tags.\n-\t\t\t\t\tcontinue metaScan\n-\t\t\t\t}\n-\t\t\t\tif len(fields) != 4 {\n-\t\t\t\t\tcontinue metaScan\n-\t\t\t\t}\n-\t\t\t\tsm = &sourceMeta{\n-\t\t\t\t\tprojectRoot:  projectRoot,\n-\t\t\t\t\tprojectURL:   fields[1],\n-\t\t\t\t\tdirTemplate:  fields[2],\n-\t\t\t\t\tfileTemplate: fields[3],\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif im == nil {\n-\t\treturn nil, nil, redir, NotFoundError{Message: fmt.Sprintf(\"%s at %s://%s\", errorMessage, scheme, importPath)}\n-\t}\n-\tif sm != nil && sm.projectRoot != im.projectRoot {\n-\t\tsm = nil\n-\t}\n-\treturn im, sm, redir, nil\n+        for {\n+                t, tokenErr := d.Token()\n+                if tokenErr != nil {\n+                        break metaScan\n+                }\n+                switch t := t.(type) {\n+                case xml.EndElement:\n+                        if strings.EqualFold(t.Name.Local, \"head\") {\n+                                break metaScan\n+                        }\n+                case xml.StartElement:\n+                        if strings.EqualFold(t.Name.Local, \"body\") {\n+                                break metaScan\n+                        }\n+                        if !strings.EqualFold(t.Name.Local, \"meta\") {\n+                                continue metaScan\n+                        }\n+                        if strings.EqualFold(attrValue(t.Attr, \"http-equiv\"), \"refresh\") {\n+                                // Check for http-equiv refresh back to godoc.org.\n+                                redir = refreshToGodocPat.MatchString(attrValue(t.Attr, \"content\"))\n+                                continue metaScan\n+                        }\n+                        nameAttr := attrValue(t.Attr, \"name\")\n+                        if nameAttr != \"go-import\" && nameAttr != \"go-source\" {\n+                                continue metaScan\n+                        }\n+                        fields := strings.Fields(attrValue(t.Attr, \"content\"))\n+                        if len(fields) < 1 {\n+                                continue metaScan\n+                        }\n+                        projectRoot := fields[0]\n+                        if !strings.HasPrefix(importPath, projectRoot) ||\n+                                !(len(importPath) == len(projectRoot) || importPath[len(projectRoot)] == '/') {\n+                                // Ignore if root is not a prefix of the  path. This allows a\n+                                // site to use a single error page for multiple repositories.\n+                                continue metaScan\n+                        }\n+                        switch nameAttr {\n+                        case \"go-import\":\n+                                if len(fields) != 3 {\n+                                        errorMessage = \"go-import meta tag content attribute does not have three fields\"\n+                                        continue metaScan\n+                                }\n+                                if fields[1] == \"mod\" {\n+                                        // vgo adds a special mod vcs type; we can skip this\n+                                        continue\n+                                }\n+                                if im != nil {\n+                                        im = nil\n+                                        errorMessage = \"more than one go-import meta tag found\"\n+                                        break metaScan\n+                                }\n+                                im = &importMeta{\n+                                        projectRoot: projectRoot,\n+                                        vcs:         fields[1],\n+                                        repo:        fields[2],\n+                                }\n+                        case \"go-source\":\n+                                if sm != nil {\n+                                        // Ignore extra go-source meta tags.\n+                                        continue metaScan\n+                                }\n+                                if len(fields) != 4 {\n+                                        continue metaScan\n+                                }\n+                                sm = &sourceMeta{\n+                                        projectRoot:  projectRoot,\n+                                        projectURL:   fields[1],\n+                                        dirTemplate:  fields[2],\n+                                        fileTemplate: fields[3],\n+                                }\n+                        }\n+                }\n+        }\n+        if im == nil {\n+                return nil, nil, redir, NotFoundError{Message: fmt.Sprintf(\"%s at %s://%s\", errorMessage, scheme, importPath)}\n+        }\n+        if sm != nil && sm.projectRoot != im.projectRoot {\n+                sm = nil\n+        }\n+        return im, sm, redir, nil\n }\n \n // getVCSDirFn is called by getDynamic to fetch source using VCS commands. The\n // default value here does nothing. If the code is not built for App Engine,\n // then getVCSDirFn is set getVCSDir, the function that actually does the work.\n var getVCSDirFn = func(ctx context.Context, client *http.Client, m map[string]string, etag string) (*Directory, error) {\n-\treturn nil, errNoMatch\n+        return nil, errNoMatch\n }\n \n // getDynamic gets a directory from a service that is not statically known.\n func getDynamic(ctx context.Context, client *http.Client, importPath, etag string) (*Directory, error) {\n-\tmetaProto, im, sm, redir, err := fetchMeta(ctx, client, importPath)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif im.projectRoot != importPath {\n-\t\tvar imRoot *importMeta\n-\t\tmetaProto, imRoot, _, redir, err = fetchMeta(ctx, client, im.projectRoot)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif *imRoot != *im {\n-\t\t\treturn nil, NotFoundError{Message: \"project root mismatch.\"}\n-\t\t}\n-\t}\n-\n-\t// clonePath is the repo URL from import meta tag, with the \"scheme://\" prefix removed.\n-\t// It should be used for cloning repositories.\n-\t// repo is the repo URL from import meta tag, with the \"scheme://\" prefix removed, and\n-\t// a possible \".vcs\" suffix trimmed.\n-\ti := strings.Index(im.repo, \"://\")\n-\tif i < 0 {\n-\t\treturn nil, NotFoundError{Message: \"bad repo URL: \" + im.repo}\n-\t}\n-\tproto := im.repo[:i]\n-\tclonePath := im.repo[i+len(\"://\"):]\n-\trepo := strings.TrimSuffix(clonePath, \".\"+im.vcs)\n-\tdirName := importPath[len(im.projectRoot):]\n-\n-\tresolvedPath := repo + dirName\n-\tdir, err := getStatic(ctx, client, resolvedPath, etag)\n-\tif err == errNoMatch {\n-\t\tresolvedPath = repo + \".\" + im.vcs + dirName\n-\t\tmatch := map[string]string{\n-\t\t\t\"dir\":        dirName,\n-\t\t\t\"importPath\": importPath,\n-\t\t\t\"clonePath\":  clonePath,\n-\t\t\t\"repo\":       repo,\n-\t\t\t\"scheme\":     proto,\n-\t\t\t\"vcs\":        im.vcs,\n-\t\t}\n-\t\tdir, err = getVCSDirFn(ctx, client, match, etag)\n-\t}\n-\tif err != nil || dir == nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tdir.ImportPath = importPath\n-\tdir.ProjectRoot = im.projectRoot\n-\tdir.ResolvedPath = resolvedPath\n-\tdir.ProjectName = path.Base(im.projectRoot)\n-\tif !redir {\n-\t\tdir.ProjectURL = metaProto + \"://\" + im.projectRoot\n-\t}\n-\n-\tif sm == nil {\n-\t\treturn dir, nil\n-\t}\n-\n-\tif isHTTPURL(sm.projectURL) {\n-\t\tdir.ProjectURL = sm.projectURL\n-\t}\n-\n-\tif isHTTPURL(sm.dirTemplate) {\n-\t\tdir.BrowseURL = replaceDir(sm.dirTemplate, dirName)\n-\t}\n-\n-\t// TODO: Refactor this to be simpler, implement the go-source meta tag spec fully.\n-\tif isHTTPURL(sm.fileTemplate) {\n-\t\tfileTemplate := replaceDir(sm.fileTemplate, dirName)\n-\t\tif strings.Contains(fileTemplate, \"{file}\") {\n-\t\t\tcut := strings.LastIndex(fileTemplate, \"{file}\") + len(\"{file}\") // Cut point is right after last {file} section.\n-\t\t\tswitch hash := strings.Index(fileTemplate, \"#\"); {\n-\t\t\tcase hash == -1: // If there's no '#', place cut at the end.\n-\t\t\t\tcut = len(fileTemplate)\n-\t\t\tcase hash > cut: // If a '#' comes after last {file}, use it as cut point.\n-\t\t\t\tcut = hash\n-\t\t\t}\n-\t\t\thead, tail := fileTemplate[:cut], fileTemplate[cut:]\n-\t\t\tfor _, f := range dir.Files {\n-\t\t\t\tf.BrowseURL = strings.Replace(head, \"{file}\", f.Name, -1)\n-\t\t\t}\n-\n-\t\t\tif strings.Contains(tail, \"{line}\") {\n-\t\t\t\ts := strings.Replace(tail, \"%\", \"%%\", -1)\n-\t\t\t\ts = strings.Replace(s, \"{line}\", \"%d\", 1)\n-\t\t\t\tdir.LineFmt = \"%s\" + s\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn dir, nil\n+        metaProto, im, sm, redir, err := fetchMeta(ctx, client, importPath)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if im.projectRoot != importPath {\n+                var imRoot *importMeta\n+                metaProto, imRoot, _, redir, err = fetchMeta(ctx, client, im.projectRoot)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                if *imRoot != *im {\n+                        return nil, NotFoundError{Message: \"project root mismatch.\"}\n+                }\n+        }\n+\n+        // clonePath is the repo URL from import meta tag, with the \"scheme://\" prefix removed.\n+        // It should be used for cloning repositories.\n+        // repo is the repo URL from import meta tag, with the \"scheme://\" prefix removed, and\n+        // a possible \".vcs\" suffix trimmed.\n+        i := strings.Index(im.repo, \"://\")\n+        if i < 0 {\n+                return nil, NotFoundError{Message: \"bad repo URL: \" + im.repo}\n+        }\n+        proto := im.repo[:i]\n+        clonePath := im.repo[i+len(\"://\"):]\n+        repo := strings.TrimSuffix(clonePath, \".\"+im.vcs)\n+        dirName := path.Clean(importPath[len(im.projectRoot):])\n+if strings.HasPrefix(dirName, \"../\") || dirName == \"..\" {\n+return nil, NotFoundError{Message: \"invalid import path\"}\n+} else if dirName == \".\" {\n+dirName = \"\"\n+}\n+\n+        resolvedPath := repo + dirName\n+\n+        dir, err := getStatic(ctx, client, resolvedPath, etag)\n+        if err == errNoMatch {\n+                resolvedPath = repo + \".\" + im.vcs + dirName\n+                match := map[string]string{\n+                        \"dir\":        dirName,\n+                        \"importPath\": importPath,\n+                        \"clonePath\":  clonePath,\n+                        \"repo\":       repo,\n+                        \"scheme\":     proto,\n+                        \"vcs\":        im.vcs,\n+                }\n+                dir, err = getVCSDirFn(ctx, client, match, etag)\n+        }\n+        if err != nil || dir == nil {\n+                return nil, err\n+        }\n+\n+        dir.ImportPath = importPath\n+        dir.ProjectRoot = im.projectRoot\n+        dir.ResolvedPath = resolvedPath\n+        dir.ProjectName = path.Base(im.projectRoot)\n+        if !redir {\n+                dir.ProjectURL = metaProto + \"://\" + im.projectRoot\n+        }\n+\n+        if sm == nil {\n+                return dir, nil\n+        }\n+\n+        if isHTTPURL(sm.projectURL) {\n+                dir.ProjectURL = sm.projectURL\n+        }\n+\n+        if isHTTPURL(sm.dirTemplate) {\n+                dir.BrowseURL = replaceDir(sm.dirTemplate, dirName)\n+        }\n+\n+        // TODO: Refactor this to be simpler, implement the go-source meta tag spec fully.\n+        if isHTTPURL(sm.fileTemplate) {\n+                fileTemplate := replaceDir(sm.fileTemplate, dirName)\n+                if strings.Contains(fileTemplate, \"{file}\") {\n+                        cut := strings.LastIndex(fileTemplate, \"{file}\") + len(\"{file}\") // Cut point is right after last {file} section.\n+                        switch hash := strings.Index(fileTemplate, \"#\"); {\n+                        case hash == -1: // If there's no '#', place cut at the end.\n+                                cut = len(fileTemplate)\n+                        case hash > cut: // If a '#' comes after last {file}, use it as cut point.\n+                                cut = hash\n+                        }\n+                        head, tail := fileTemplate[:cut], fileTemplate[cut:]\n+                        for _, f := range dir.Files {\n+                                f.BrowseURL = strings.Replace(head, \"{file}\", f.Name, -1)\n+                        }\n+\n+                        if strings.Contains(tail, \"{line}\") {\n+                                s := strings.Replace(tail, \"%\", \"%%\", -1)\n+                                s = strings.Replace(s, \"{line}\", \"%d\", 1)\n+                                dir.LineFmt = \"%s\" + s\n+                        }\n+                }\n+        }\n+\n+        return dir, nil\n }\n \n // getStatic gets a diretory from a statically known service. getStatic\n // returns errNoMatch if the import path is not recognized.\n func getStatic(ctx context.Context, client *http.Client, importPath, etag string) (*Directory, error) {\n-\tfor _, s := range services {\n-\t\tif s.get == nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tmatch, err := s.match(importPath)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif match != nil {\n-\t\t\tdir, err := s.get(ctx, client, match, etag)\n-\t\t\tif dir != nil {\n-\t\t\t\tdir.ImportPath = importPath\n-\t\t\t\tdir.ResolvedPath = importPath\n-\t\t\t}\n-\t\t\treturn dir, err\n-\t\t}\n-\t}\n-\treturn nil, errNoMatch\n+        for _, s := range services {\n+                if s.get == nil {\n+                        continue\n+                }\n+                match, err := s.match(importPath)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                if match != nil {\n+                        dir, err := s.get(ctx, client, match, etag)\n+                        if dir != nil {\n+                                dir.ImportPath = importPath\n+                                dir.ResolvedPath = importPath\n+                        }\n+                        return dir, err\n+                }\n+        }\n+        return nil, errNoMatch\n }\n \n func Get(ctx context.Context, client *http.Client, importPath string, etag string) (dir *Directory, err error) {\n-\tswitch {\n-\tcase localPath != \"\":\n-\t\tdir, err = getLocal(importPath)\n-\tcase IsGoRepoPath(importPath):\n-\t\tdir, err = getStandardDir(ctx, client, importPath, etag)\n-\tcase IsValidRemotePath(importPath):\n-\t\tdir, err = getStatic(ctx, client, importPath, etag)\n-\t\tif err == errNoMatch {\n-\t\t\tdir, err = getDynamic(ctx, client, importPath, etag)\n-\t\t}\n-\tdefault:\n-\t\terr = errNoMatch\n-\t}\n-\n-\tif err == errNoMatch {\n-\t\terr = NotFoundError{Message: \"Import path not valid:\"}\n-\t}\n-\n-\treturn dir, err\n+        switch {\n+        case localPath != \"\":\n+                dir, err = getLocal(importPath)\n+        case IsGoRepoPath(importPath):\n+                dir, err = getStandardDir(ctx, client, importPath, etag)\n+        case IsValidRemotePath(importPath):\n+                dir, err = getStatic(ctx, client, importPath, etag)\n+                if err == errNoMatch {\n+                        dir, err = getDynamic(ctx, client, importPath, etag)\n+                }\n+        default:\n+                err = errNoMatch\n+        }\n+\n+        if err == errNoMatch {\n+                err = NotFoundError{Message: \"Import path not valid:\"}\n+        }\n+\n+        return dir, err\n }\n \n // GetPresentation gets a presentation from the the given path.\n func GetPresentation(ctx context.Context, client *http.Client, importPath string) (*Presentation, error) {\n-\text := path.Ext(importPath)\n-\tif ext != \".slide\" && ext != \".article\" {\n-\t\treturn nil, NotFoundError{Message: \"unknown file extension.\"}\n-\t}\n-\n-\timportPath, file := path.Split(importPath)\n-\timportPath = strings.TrimSuffix(importPath, \"/\")\n-\tfor _, s := range services {\n-\t\tif s.getPresentation == nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tmatch, err := s.match(importPath)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif match != nil {\n-\t\t\tmatch[\"file\"] = file\n-\t\t\treturn s.getPresentation(ctx, client, match)\n-\t\t}\n-\t}\n-\treturn nil, NotFoundError{Message: \"path does not match registered service\"}\n+        ext := path.Ext(importPath)\n+        if ext != \".slide\" && ext != \".article\" {\n+                return nil, NotFoundError{Message: \"unknown file extension.\"}\n+        }\n+\n+        importPath, file := path.Split(importPath)\n+        importPath = strings.TrimSuffix(importPath, \"/\")\n+        for _, s := range services {\n+                if s.getPresentation == nil {\n+                        continue\n+                }\n+                match, err := s.match(importPath)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                if match != nil {\n+                        match[\"file\"] = file\n+                        return s.getPresentation(ctx, client, match)\n+                }\n+        }\n+        return nil, NotFoundError{Message: \"path does not match registered service\"}\n }\n \n // GetProject gets information about a repository.\n func GetProject(ctx context.Context, client *http.Client, importPath string) (*Project, error) {\n-\tfor _, s := range services {\n-\t\tif s.getProject == nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tmatch, err := s.match(importPath)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif match != nil {\n-\t\t\treturn s.getProject(ctx, client, match)\n-\t\t}\n-\t}\n-\treturn nil, NotFoundError{Message: \"path does not match registered service\"}\n+        for _, s := range services {\n+                if s.getProject == nil {\n+                        continue\n+                }\n+                match, err := s.match(importPath)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                if match != nil {\n+                        return s.getProject(ctx, client, match)\n+                }\n+        }\n+        return nil, NotFoundError{Message: \"path does not match registered service\"}\n }\n"}
{"cve":"CVE-2024-54132:0708", "fix_patch": "diff --git a/pkg/cmd/run/download/download.go b/pkg/cmd/run/download/download.go\nindex 99ec45bbe..3f0edb871 100644\n--- a/pkg/cmd/run/download/download.go\n+++ b/pkg/cmd/run/download/download.go\n@@ -1,202 +1,217 @@\n package download\n \n import (\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"path/filepath\"\n-\n-\t\"github.com/MakeNowJust/heredoc\"\n-\t\"github.com/cli/cli/v2/pkg/cmd/run/shared\"\n-\t\"github.com/cli/cli/v2/pkg/cmdutil\"\n-\t\"github.com/cli/cli/v2/pkg/iostreams\"\n-\t\"github.com/cli/cli/v2/pkg/set\"\n-\t\"github.com/spf13/cobra\"\n+        \"errors\"\n+        \"fmt\"\n+        \"path/filepath\"\n+\"strings\"\n+\n+\n+        \"github.com/MakeNowJust/heredoc\"\n+        \"github.com/cli/cli/v2/pkg/cmd/run/shared\"\n+        \"github.com/cli/cli/v2/pkg/cmdutil\"\n+        \"github.com/cli/cli/v2/pkg/iostreams\"\n+        \"github.com/cli/cli/v2/pkg/set\"\n+        \"github.com/spf13/cobra\"\n )\n \n type DownloadOptions struct {\n-\tIO       *iostreams.IOStreams\n-\tPlatform platform\n-\tPrompter iprompter\n-\n-\tDoPrompt       bool\n-\tRunID          string\n-\tDestinationDir string\n-\tNames          []string\n-\tFilePatterns   []string\n+        IO       *iostreams.IOStreams\n+        Platform platform\n+        Prompter iprompter\n+\n+        DoPrompt       bool\n+        RunID          string\n+        DestinationDir string\n+        Names          []string\n+        FilePatterns   []string\n }\n \n type platform interface {\n-\tList(runID string) ([]shared.Artifact, error)\n-\tDownload(url string, dir string) error\n+        List(runID string) ([]shared.Artifact, error)\n+        Download(url string, dir string) error\n }\n type iprompter interface {\n-\tMultiSelect(string, []string, []string) ([]int, error)\n+        MultiSelect(string, []string, []string) ([]int, error)\n }\n \n func NewCmdDownload(f *cmdutil.Factory, runF func(*DownloadOptions) error) *cobra.Command {\n-\topts := &DownloadOptions{\n-\t\tIO:       f.IOStreams,\n-\t\tPrompter: f.Prompter,\n-\t}\n-\n-\tcmd := &cobra.Command{\n-\t\tUse:   \"download [<run-id>]\",\n-\t\tShort: \"Download artifacts generated by a workflow run\",\n-\t\tLong: heredoc.Docf(`\n-\t\t\tDownload artifacts generated by a GitHub Actions workflow run.\n-\n-\t\t\tThe contents of each artifact will be extracted under separate directories based on\n-\t\t\tthe artifact name. If only a single artifact is specified, it will be extracted into\n-\t\t\tthe current directory.\n-\n-\t\t\tBy default, this command downloads the latest artifact created and uploaded through\n-\t\t\tGitHub Actions. Because workflows can delete or overwrite artifacts, %[1]s<run-id>%[1]s\n-\t\t\tmust be used to select an artifact from a specific workflow run.\n-\t\t`, \"`\"),\n-\t\tArgs: cobra.MaximumNArgs(1),\n-\t\tExample: heredoc.Doc(`\n-\t\t\t# Download all artifacts generated by a workflow run\n-\t\t\t$ gh run download <run-id>\n-\n-\t\t\t# Download a specific artifact within a run\n-\t\t\t$ gh run download <run-id> -n <name>\n-\n-\t\t\t# Download specific artifacts across all runs in a repository\n-\t\t\t$ gh run download -n <name1> -n <name2>\n-\n-\t\t\t# Select artifacts to download interactively\n-\t\t\t$ gh run download\n-\t\t`),\n-\t\tRunE: func(cmd *cobra.Command, args []string) error {\n-\t\t\tif len(args) > 0 {\n-\t\t\t\topts.RunID = args[0]\n-\t\t\t} else if len(opts.Names) == 0 &&\n-\t\t\t\tlen(opts.FilePatterns) == 0 &&\n-\t\t\t\topts.IO.CanPrompt() {\n-\t\t\t\topts.DoPrompt = true\n-\t\t\t}\n-\t\t\t// support `-R, --repo` override\n-\t\t\tbaseRepo, err := f.BaseRepo()\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\thttpClient, err := f.HttpClient()\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\topts.Platform = &apiPlatform{\n-\t\t\t\tclient: httpClient,\n-\t\t\t\trepo:   baseRepo,\n-\t\t\t}\n-\n-\t\t\tif runF != nil {\n-\t\t\t\treturn runF(opts)\n-\t\t\t}\n-\t\t\treturn runDownload(opts)\n-\t\t},\n-\t}\n-\n-\tcmd.Flags().StringVarP(&opts.DestinationDir, \"dir\", \"D\", \".\", \"The directory to download artifacts into\")\n-\tcmd.Flags().StringArrayVarP(&opts.Names, \"name\", \"n\", nil, \"Download artifacts that match any of the given names\")\n-\tcmd.Flags().StringArrayVarP(&opts.FilePatterns, \"pattern\", \"p\", nil, \"Download artifacts that match a glob pattern\")\n-\n-\treturn cmd\n+        opts := &DownloadOptions{\n+                IO:       f.IOStreams,\n+                Prompter: f.Prompter,\n+        }\n+\n+        cmd := &cobra.Command{\n+                Use:   \"download [<run-id>]\",\n+                Short: \"Download artifacts generated by a workflow run\",\n+                Long: heredoc.Docf(`\n+                        Download artifacts generated by a GitHub Actions workflow run.\n+\n+                        The contents of each artifact will be extracted under separate directories based on\n+                        the artifact name. If only a single artifact is specified, it will be extracted into\n+                        the current directory.\n+\n+                        By default, this command downloads the latest artifact created and uploaded through\n+                        GitHub Actions. Because workflows can delete or overwrite artifacts, %[1]s<run-id>%[1]s\n+                        must be used to select an artifact from a specific workflow run.\n+                `, \"`\"),\n+                Args: cobra.MaximumNArgs(1),\n+                Example: heredoc.Doc(`\n+                        # Download all artifacts generated by a workflow run\n+                        $ gh run download <run-id>\n+\n+                        # Download a specific artifact within a run\n+                        $ gh run download <run-id> -n <name>\n+\n+                        # Download specific artifacts across all runs in a repository\n+                        $ gh run download -n <name1> -n <name2>\n+\n+                        # Select artifacts to download interactively\n+                        $ gh run download\n+                `),\n+                RunE: func(cmd *cobra.Command, args []string) error {\n+                        if len(args) > 0 {\n+                                opts.RunID = args[0]\n+                        } else if len(opts.Names) == 0 &&\n+                                len(opts.FilePatterns) == 0 &&\n+                                opts.IO.CanPrompt() {\n+                                opts.DoPrompt = true\n+                        }\n+                        // support `-R, --repo` override\n+                        baseRepo, err := f.BaseRepo()\n+                        if err != nil {\n+                                return err\n+                        }\n+                        httpClient, err := f.HttpClient()\n+                        if err != nil {\n+                                return err\n+                        }\n+                        opts.Platform = &apiPlatform{\n+                                client: httpClient,\n+                                repo:   baseRepo,\n+                        }\n+\n+                        if runF != nil {\n+                                return runF(opts)\n+                        }\n+                        return runDownload(opts)\n+                },\n+        }\n+\n+        cmd.Flags().StringVarP(&opts.DestinationDir, \"dir\", \"D\", \".\", \"The directory to download artifacts into\")\n+        cmd.Flags().StringArrayVarP(&opts.Names, \"name\", \"n\", nil, \"Download artifacts that match any of the given names\")\n+        cmd.Flags().StringArrayVarP(&opts.FilePatterns, \"pattern\", \"p\", nil, \"Download artifacts that match a glob pattern\")\n+\n+        return cmd\n }\n \n func runDownload(opts *DownloadOptions) error {\n-\topts.IO.StartProgressIndicator()\n-\tartifacts, err := opts.Platform.List(opts.RunID)\n-\topts.IO.StopProgressIndicator()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error fetching artifacts: %w\", err)\n-\t}\n-\n-\tnumValidArtifacts := 0\n-\tfor _, a := range artifacts {\n-\t\tif a.Expired {\n-\t\t\tcontinue\n-\t\t}\n-\t\tnumValidArtifacts++\n-\t}\n-\tif numValidArtifacts == 0 {\n-\t\treturn errors.New(\"no valid artifacts found to download\")\n-\t}\n-\n-\twantPatterns := opts.FilePatterns\n-\twantNames := opts.Names\n-\tif opts.DoPrompt {\n-\t\tartifactNames := set.NewStringSet()\n-\t\tfor _, a := range artifacts {\n-\t\t\tif !a.Expired {\n-\t\t\t\tartifactNames.Add(a.Name)\n-\t\t\t}\n-\t\t}\n-\t\toptions := artifactNames.ToSlice()\n-\t\tif len(options) > 10 {\n-\t\t\toptions = options[:10]\n-\t\t}\n-\t\tvar selected []int\n-\t\tif selected, err = opts.Prompter.MultiSelect(\"Select artifacts to download:\", nil, options); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\twantNames = []string{}\n-\t\tfor _, x := range selected {\n-\t\t\twantNames = append(wantNames, options[x])\n-\t\t}\n-\t\tif len(wantNames) == 0 {\n-\t\t\treturn errors.New(\"no artifacts selected\")\n-\t\t}\n-\t}\n-\n-\topts.IO.StartProgressIndicator()\n-\tdefer opts.IO.StopProgressIndicator()\n-\n-\t// track downloaded artifacts and avoid re-downloading any of the same name\n-\tdownloaded := set.NewStringSet()\n-\tfor _, a := range artifacts {\n-\t\tif a.Expired {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif downloaded.Contains(a.Name) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif len(wantNames) > 0 || len(wantPatterns) > 0 {\n-\t\t\tif !matchAnyName(wantNames, a.Name) && !matchAnyPattern(wantPatterns, a.Name) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\tdestDir := opts.DestinationDir\n-\t\tif len(wantPatterns) != 0 || len(wantNames) != 1 {\n-\t\t\tdestDir = filepath.Join(destDir, a.Name)\n-\t\t}\n-\t\terr := opts.Platform.Download(a.DownloadURL, destDir)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error downloading %s: %w\", a.Name, err)\n-\t\t}\n-\t\tdownloaded.Add(a.Name)\n-\t}\n-\n-\tif downloaded.Len() == 0 {\n-\t\treturn errors.New(\"no artifact matches any of the names or patterns provided\")\n-\t}\n-\n-\treturn nil\n+        opts.IO.StartProgressIndicator()\n+        artifacts, err := opts.Platform.List(opts.RunID)\n+        opts.IO.StopProgressIndicator()\n+        if err != nil {\n+                return fmt.Errorf(\"error fetching artifacts: %w\", err)\n+        }\n+\n+        numValidArtifacts := 0\n+        for _, a := range artifacts {\n+                if a.Expired {\n+                        continue\n+                }\n+                numValidArtifacts++\n+        }\n+        if numValidArtifacts == 0 {\n+                return errors.New(\"no valid artifacts found to download\")\n+        }\n+\n+        wantPatterns := opts.FilePatterns\n+        wantNames := opts.Names\n+        if opts.DoPrompt {\n+                artifactNames := set.NewStringSet()\n+                for _, a := range artifacts {\n+                        if !a.Expired {\n+                                artifactNames.Add(a.Name)\n+                        }\n+                }\n+                options := artifactNames.ToSlice()\n+                if len(options) > 10 {\n+                        options = options[:10]\n+                }\n+                var selected []int\n+                if selected, err = opts.Prompter.MultiSelect(\"Select artifacts to download:\", nil, options); err != nil {\n+                        return err\n+                }\n+                wantNames = []string{}\n+                for _, x := range selected {\n+                        wantNames = append(wantNames, options[x])\n+                }\n+                if len(wantNames) == 0 {\n+                        return errors.New(\"no artifacts selected\")\n+                }\n+        }\n+\n+        opts.IO.StartProgressIndicator()\n+        defer opts.IO.StopProgressIndicator()\n+\n+        // track downloaded artifacts and avoid re-downloading any of the same name\n+        downloaded := set.NewStringSet()\n+        for _, a := range artifacts {\n+                if a.Expired {\n+                        continue\n+                }\n+                if downloaded.Contains(a.Name) {\n+                        continue\n+                }\n+                if len(wantNames) > 0 || len(wantPatterns) > 0 {\n+                        if !matchAnyName(wantNames, a.Name) && !matchAnyPattern(wantPatterns, a.Name) {\n+                                continue\n+                        }\n+                }\n+                \n+\n+if isPathTraversal(a.Name) {\n+return fmt.Errorf(\"error downloading %s: would result in path traversal\", a.Name)\n+}\n+\n+destDir := opts.DestinationDir\n+if len(wantPatterns) != 0 || len(wantNames) != 1 {\n+destDir = filepath.Join(destDir, a.Name)\n+}\n+                err := opts.Platform.Download(a.DownloadURL, destDir)\n+                if err != nil {\n+                        return fmt.Errorf(\"error downloading %s: %w\", a.Name, err)\n+                }\n+                downloaded.Add(a.Name)\n+        }\n+\n+        if downloaded.Len() == 0 {\n+                return errors.New(\"no artifact matches any of the names or patterns provided\")\n+        }\n+\n+        return nil\n }\n \n func matchAnyName(names []string, name string) bool {\n-\tfor _, n := range names {\n-\t\tif name == n {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, n := range names {\n+                if name == n {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func matchAnyPattern(patterns []string, name string) bool {\n-\tfor _, p := range patterns {\n-\t\tif isMatch, err := filepath.Match(p, name); err == nil && isMatch {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, p := range patterns {\n+                if isMatch, err := filepath.Match(p, name); err == nil && isMatch {\n+                        return true\n+                }\n+        }\n+        return false\n+}\n+\n+func isPathTraversal(name string) bool {\n+return name == \"..\" || strings.Contains(name, \"../\")\n+\n }\n+\n+\n"}
{"cve":"CVE-2024-6257:0708", "fix_patch": ""}
{"cve":"CVE-2020-15233:0708", "fix_patch": "diff --git a/authorize_helper.go b/authorize_helper.go\nindex ec286f5..79f8bcf 100644\n--- a/authorize_helper.go\n+++ b/authorize_helper.go\n@@ -1,181 +1,47 @@\n-/*\n- * Copyright \u00a9 2015-2018 Aeneas Rekkas <aeneas+oss@aeneas.io>\n- *\n- * Licensed under the Apache License, Version 2.0 (the \"License\");\n- * you may not use this file except in compliance with the License.\n- * You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- *\n- * @author\t\tAeneas Rekkas <aeneas+oss@aeneas.io>\n- * @copyright \t2015-2018 Aeneas Rekkas <aeneas+oss@aeneas.io>\n- * @license \tApache-2.0\n- *\n- */\n-\n package fosite\n \n import (\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"strings\"\n-\n-\t\"github.com/asaskevich/govalidator\"\n-\t\"github.com/pkg/errors\"\n+\"net\"\n+\"net/url\"\n+\"strings\"\n )\n \n-// GetRedirectURIFromRequestValues extracts the redirect_uri from values but does not do any sort of validation.\n-//\n-// Considered specifications\n-// * https://tools.ietf.org/html/rfc6749#section-3.1.2\n-//   The endpoint URI MAY include an\n-//   \"application/x-www-form-urlencoded\" formatted (per Appendix B) query\n-//   component ([RFC3986] Section 3.4), which MUST be retained when adding\n-//   additional query parameters.\n-func GetRedirectURIFromRequestValues(values url.Values) (string, error) {\n-\t// rfc6749 3.1.   Authorization Endpoint\n-\t// The endpoint URI MAY include an \"application/x-www-form-urlencoded\" formatted (per Appendix B) query component\n-\tredirectURI, err := url.QueryUnescape(values.Get(\"redirect_uri\"))\n-\tif err != nil {\n-\t\treturn \"\", errors.WithStack(ErrInvalidRequest.WithHint(`The \"redirect_uri\" parameter is malformed or missing.`).WithCause(err).WithDebug(err.Error()))\n-\t}\n-\treturn redirectURI, nil\n-}\n-\n-// MatchRedirectURIWithClientRedirectURIs if the given uri is a registered redirect uri. Does not perform\n-// uri validation.\n-//\n-// Considered specifications\n-// * https://tools.ietf.org/html/rfc6749#section-3.1.2.3\n-//   If multiple redirection URIs have been registered, if only part of\n-//   the redirection URI has been registered, or if no redirection URI has\n-//   been registered, the client MUST include a redirection URI with the\n-//   authorization request using the \"redirect_uri\" request parameter.\n-//\n-//   When a redirection URI is included in an authorization request, the\n-//   authorization server MUST compare and match the value received\n-//   against at least one of the registered redirection URIs (or URI\n-//   components) as defined in [RFC3986] Section 6, if any redirection\n-//   URIs were registered.  If the client registration included the full\n-//   redirection URI, the authorization server MUST compare the two URIs\n-//   using simple string comparison as defined in [RFC3986] Section 6.2.1.\n-//\n-// * https://tools.ietf.org/html/rfc6819#section-4.4.1.7\n-//   * The authorization server may also enforce the usage and validation\n-//     of pre-registered redirect URIs (see Section 5.2.3.5).  This will\n-//     allow for early recognition of authorization \"code\" disclosure to\n-//     counterfeit clients.\n-//   * The attacker will need to use another redirect URI for its\n-//     authorization process rather than the target web site because it\n-//     needs to intercept the flow.  So, if the authorization server\n-//     associates the authorization \"code\" with the redirect URI of a\n-//     particular end-user authorization and validates this redirect URI\n-//     with the redirect URI passed to the token's endpoint, such an\n-//     attack is detected (see Section 5.2.4.5).\n-func MatchRedirectURIWithClientRedirectURIs(rawurl string, client Client) (*url.URL, error) {\n-\tif rawurl == \"\" && len(client.GetRedirectURIs()) == 1 {\n-\t\tif redirectURIFromClient, err := url.Parse(client.GetRedirectURIs()[0]); err == nil && IsValidRedirectURI(redirectURIFromClient) {\n-\t\t\t// If no redirect_uri was given and the client has exactly one valid redirect_uri registered, use that instead\n-\t\t\treturn redirectURIFromClient, nil\n-\t\t}\n-\t} else if rawurl != \"\" && isMatchingRedirectURI(rawurl, client.GetRedirectURIs()) {\n-\t\t// If a redirect_uri was given and the clients knows it (simple string comparison!)\n-\t\t// return it.\n-\t\tif parsed, err := url.Parse(rawurl); err == nil && IsValidRedirectURI(parsed) {\n-\t\t\t// If no redirect_uri was given and the client has exactly one valid redirect_uri registered, use that instead\n-\t\t\treturn parsed, nil\n-\t\t}\n-\t}\n-\n-\treturn nil, errors.WithStack(ErrInvalidRequest.WithHint(`The \"redirect_uri\" parameter does not match any of the OAuth 2.0 Client's pre-registered redirect urls.`))\n-}\n-\n-// Match a requested  redirect URI against a pool of registered client URIs\n-//\n-// Test a given redirect URI against a pool of URIs provided by a registered client.\n-// If the OAuth 2.0 Client has loopback URIs registered either an IPv4 URI http://127.0.0.1 or\n-// an IPv6 URI http://[::1] a client is allowed to request a dynamic port and the server MUST accept\n-// it as a valid redirection uri.\n-//\n-// https://tools.ietf.org/html/rfc8252#section-7.3\n-// Native apps that are able to open a port on the loopback network\n-// interface without needing special permissions (typically, those on\n-// desktop operating systems) can use the loopback interface to receive\n-// the OAuth redirect.\n-//\n-// Loopback redirect URIs use the \"http\" scheme and are constructed with\n-// the loopback IP literal and whatever port the client is listening on.\n+// isMatchingRedirectURI checks if a redirect URI is whitelisted.\n func isMatchingRedirectURI(uri string, haystack []string) bool {\n-\trequested, err := url.Parse(uri)\n-\tif err != nil {\n-\t\treturn false\n-\t}\n-\n-\tfor _, b := range haystack {\n-\t\tif strings.ToLower(b) == strings.ToLower(uri) || isLoopbackURI(requested, b) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+for _, b := range haystack {\n+gs.ToLower(b) == strings.ToLower(uri) || IsLoopbackURI(uri, b) {\n+ true\n+ false\n }\n \n-func isLoopbackURI(requested *url.URL, registeredURI string) bool {\n-\tregistered, err := url.Parse(registeredURI)\n-\tif err != nil {\n-\t\treturn false\n-\t}\n-\n-\tif registered.Scheme != \"http\" || !isLoopbackAddress(registered.Host) {\n-\t\treturn false\n-\t}\n-\n-\tif requested.Scheme == \"http\" && isLoopbackAddress(requested.Host) && registered.Path == requested.Path {\n-\t\treturn true\n-\t}\n-\n-\treturn false\n+// IsLoopbackURI checks if a redirect URI is a loopback URI.\n+func IsLoopbackURI(request, registered string) bool {\n+requestURL, err := url.Parse(request)\n+if err != nil {\n+ false\n }\n \n-// Check if address is either an IPv4 loopback or an IPv6 loopback-\n-// An optional port is ignored\n-func isLoopbackAddress(address string) bool {\n-\tmatch, _ := regexp.MatchString(\"^(127.0.0.1|\\\\[::1\\\\])(:?)(\\\\d*)$\", address)\n-\treturn match\n+registeredURL, err := url.Parse(registered)\n+if err != nil {\n+ false\n }\n \n-// IsValidRedirectURI validates a redirect_uri as specified in:\n-//\n-// * https://tools.ietf.org/html/rfc6749#section-3.1.2\n-//   * The redirection endpoint URI MUST be an absolute URI as defined by [RFC3986] Section 4.3.\n-//   * The endpoint URI MUST NOT include a fragment component.\n-// * https://tools.ietf.org/html/rfc3986#section-4.3\n-//   absolute-URI  = scheme \":\" hier-part [ \"?\" query ]\n-// * https://tools.ietf.org/html/rfc6819#section-5.1.1\n-func IsValidRedirectURI(redirectURI *url.URL) bool {\n-\t// We need to explicitly check for a scheme\n-\tif !govalidator.IsRequestURL(redirectURI.String()) {\n-\t\treturn false\n-\t}\n-\n-\tif redirectURI.Fragment != \"\" {\n-\t\t// \"The endpoint URI MUST NOT include a fragment component.\"\n-\t\treturn false\n-\t}\n+if registeredURL.Scheme != \"http\" || !isLoopbackAddress(registeredURL.Hostname()) {\n+ false\n+}\n \n-\treturn true\n+if requestURL.Scheme == \"http\" &&\n+uestURL.Hostname()) &&\n+uestURL.Path == registeredURL.Path {\n+ true\n }\n \n-func IsRedirectURISecure(redirectURI *url.URL) bool {\n-\treturn !(redirectURI.Scheme == \"http\" && !IsLocalhost(redirectURI))\n+return false\n }\n \n-func IsLocalhost(redirectURI *url.URL) bool {\n-\thn := redirectURI.Hostname()\n-\treturn strings.HasSuffix(hn, \".localhost\") || hn == \"127.0.0.1\" || hn == \"localhost\"\n+// isLoopbackAddress checks if an address is a loopback address.\n+func isLoopbackAddress(address string) bool {\n+host, _, err := net.SplitHostPort(address)\n+if err != nil {\n+ host == \"127.0.0.1\" || host == \"::1\"\n }\n"}
{"cve":"CVE-2021-36157:0708", "fix_patch": "diff --git a/pkg/tenant/errors.go b/pkg/tenant/errors.go\nnew file mode 100644\nindex 000000000..c1ca2b660\n--- /dev/null\n+++ b/pkg/tenant/errors.go\n@@ -0,0 +1,5 @@\n+package tenant\n+\n+import \"errors\"\n+\n+var errInvalidTenantID = errors.New(\"invalid tenant ID\")\n\\ No newline at end of file\ndiff --git a/pkg/tenant/resolver.go b/pkg/tenant/resolver.go\nindex e5fbea252..057c62ba6 100644\n--- a/pkg/tenant/resolver.go\n+++ b/pkg/tenant/resolver.go\n@@ -1,18 +1,18 @@\n package tenant\n \n import (\n-\t\"context\"\n-\t\"net/http\"\n-\t\"strings\"\n+        \"context\"\n+        \"net/http\"\n+        \"strings\"\n \n-\t\"github.com/weaveworks/common/user\"\n+        \"github.com/weaveworks/common/user\"\n )\n \n var defaultResolver Resolver = NewSingleResolver()\n \n // WithDefaultResolver updates the resolver used for the package methods.\n func WithDefaultResolver(r Resolver) {\n-\tdefaultResolver = r\n+        defaultResolver = r\n }\n \n // TenantID returns exactly a single tenant ID from the context. It should be\n@@ -23,7 +23,7 @@ func WithDefaultResolver(r Resolver) {\n // ignore stutter warning\n //nolint:golint\n func TenantID(ctx context.Context) (string, error) {\n-\treturn defaultResolver.TenantID(ctx)\n+        return defaultResolver.TenantID(ctx)\n }\n \n // TenantIDs returns all tenant IDs from the context. It should return\n@@ -33,44 +33,60 @@ func TenantID(ctx context.Context) (string, error) {\n // ignore stutter warning\n //nolint:golint\n func TenantIDs(ctx context.Context) ([]string, error) {\n-\treturn defaultResolver.TenantIDs(ctx)\n+        return defaultResolver.TenantIDs(ctx)\n }\n \n type Resolver interface {\n-\t// TenantID returns exactly a single tenant ID from the context. It should be\n-\t// used when a certain endpoint should only support exactly a single\n-\t// tenant ID. It returns an error user.ErrNoOrgID if there is no tenant ID\n-\t// supplied or user.ErrTooManyOrgIDs if there are multiple tenant IDs present.\n-\tTenantID(context.Context) (string, error)\n-\n-\t// TenantIDs returns all tenant IDs from the context. It should return\n-\t// normalized list of ordered and distinct tenant IDs (as produced by\n-\t// NormalizeTenantIDs).\n-\tTenantIDs(context.Context) ([]string, error)\n+        // TenantID returns exactly a single tenant ID from the context. It should be\n+        // used when a certain endpoint should only support exactly a single\n+        // tenant ID. It returns an error user.ErrNoOrgID if there is no tenant ID\n+        // supplied or user.ErrTooManyOrgIDs if there are multiple tenant IDs present.\n+        TenantID(context.Context) (string, error)\n+\n+        // TenantIDs returns all tenant IDs from the context. It should return\n+        // normalized list of ordered and distinct tenant IDs (as produced by\n+        // NormalizeTenantIDs).\n+        TenantIDs(context.Context) ([]string, error)\n }\n \n // NewSingleResolver creates a tenant resolver, which restricts all requests to\n // be using a single tenant only. This allows a wider set of characters to be\n // used within the tenant ID and should not impose a breaking change.\n func NewSingleResolver() *SingleResolver {\n-\treturn &SingleResolver{}\n+        return &SingleResolver{}\n }\n \n type SingleResolver struct {\n }\n \n func (t *SingleResolver) TenantID(ctx context.Context) (string, error) {\n-\t//lint:ignore faillint wrapper around upstream method\n-\treturn user.ExtractOrgID(ctx)\n+        //lint:ignore faillint wrapper around upstream method\n+        tenantID, err := user.ExtractOrgID(ctx)\n+if err != nil {\n+return \"\", err\n+}\n+\n+return SanitizeTenantID(tenantID)\n }\n \n func (t *SingleResolver) TenantIDs(ctx context.Context) ([]string, error) {\n-\t//lint:ignore faillint wrapper around upstream method\n-\torgID, err := user.ExtractOrgID(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn []string{orgID}, err\n+        //lint:ignore faillint wrapper around upstream method\n+orgID, err := user.ExtractOrgID(ctx)\n+if err != nil {\n+return nil, err\n+}\n+\n+if _, err := SanitizeTenantID(orgID); err != nil {\n+return nil, err\n+}\n+\n+return []string{orgID}, nil\n+}\n+func SanitizeTenantID(id string) (string, error) {\n+if strings.Contains(id, \"/\") || strings.Contains(id, \"..\") {\n+return \"\", errInvalidTenantID\n+}\n+return id, nil\n }\n \n type MultiResolver struct {\n@@ -81,52 +97,52 @@ type MultiResolver struct {\n // further limits on the character set allowed within tenants as detailed here:\n // https://cortexmetrics.io/docs/guides/limitations/#tenant-id-naming)\n func NewMultiResolver() *MultiResolver {\n-\treturn &MultiResolver{}\n+        return &MultiResolver{}\n }\n \n func (t *MultiResolver) TenantID(ctx context.Context) (string, error) {\n-\torgIDs, err := t.TenantIDs(ctx)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n+        orgIDs, err := t.TenantIDs(ctx)\n+        if err != nil {\n+                return \"\", err\n+        }\n \n-\tif len(orgIDs) > 1 {\n-\t\treturn \"\", user.ErrTooManyOrgIDs\n-\t}\n+        if len(orgIDs) > 1 {\n+                return \"\", user.ErrTooManyOrgIDs\n+        }\n \n-\treturn orgIDs[0], nil\n+        return orgIDs[0], nil\n }\n \n func (t *MultiResolver) TenantIDs(ctx context.Context) ([]string, error) {\n-\t//lint:ignore faillint wrapper around upstream method\n-\torgID, err := user.ExtractOrgID(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\torgIDs := strings.Split(orgID, tenantIDsLabelSeparator)\n-\tfor _, orgID := range orgIDs {\n-\t\tif err := ValidTenantID(orgID); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn NormalizeTenantIDs(orgIDs), nil\n+        //lint:ignore faillint wrapper around upstream method\n+        orgID, err := user.ExtractOrgID(ctx)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        orgIDs := strings.Split(orgID, tenantIDsLabelSeparator)\n+        for _, orgID := range orgIDs {\n+                if err := ValidTenantID(orgID); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return NormalizeTenantIDs(orgIDs), nil\n }\n \n // ExtractTenantIDFromHTTPRequest extracts a single TenantID through a given\n // resolver directly from a HTTP request.\n func ExtractTenantIDFromHTTPRequest(req *http.Request) (string, context.Context, error) {\n-\t//lint:ignore faillint wrapper around upstream method\n-\t_, ctx, err := user.ExtractOrgIDFromHTTPRequest(req)\n-\tif err != nil {\n-\t\treturn \"\", nil, err\n-\t}\n-\n-\ttenantID, err := defaultResolver.TenantID(ctx)\n-\tif err != nil {\n-\t\treturn \"\", nil, err\n-\t}\n-\n-\treturn tenantID, ctx, nil\n+        //lint:ignore faillint wrapper around upstream method\n+        _, ctx, err := user.ExtractOrgIDFromHTTPRequest(req)\n+        if err != nil {\n+                return \"\", nil, err\n+        }\n+\n+        tenantID, err := defaultResolver.TenantID(ctx)\n+        if err != nil {\n+                return \"\", nil, err\n+        }\n+\n+        return tenantID, ctx, nil\n }\n"}
{"cve":"CVE-2020-29529:0708", "fix_patch": "diff --git a/slug.go b/slug.go\nindex 0404173..7cc6627 100644\n--- a/slug.go\n+++ b/slug.go\n@@ -1,22 +1,22 @@\n package slug\n \n import (\n-\t\"archive/tar\"\n-\t\"compress/gzip\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"strings\"\n+        \"archive/tar\"\n+        \"compress/gzip\"\n+        \"fmt\"\n+        \"io\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"strings\"\n )\n \n // Meta provides detailed information about a slug.\n type Meta struct {\n-\t// The list of files contained in the slug.\n-\tFiles []string\n+        // The list of files contained in the slug.\n+        Files []string\n \n-\t// Total size of the slug in bytes.\n-\tSize int64\n+        // Total size of the slug in bytes.\n+        Size int64\n }\n \n // Pack creates a slug from a src directory, and writes the new slug\n@@ -27,294 +27,355 @@ type Meta struct {\n // false symlinks with a target outside the src directory are omitted\n // from the slug.\n func Pack(src string, w io.Writer, dereference bool) (*Meta, error) {\n-\t// Gzip compress all the output data.\n-\tgzipW := gzip.NewWriter(w)\n+        // Gzip compress all the output data.\n+        gzipW := gzip.NewWriter(w)\n \n-\t// Tar the file contents.\n-\ttarW := tar.NewWriter(gzipW)\n+        // Tar the file contents.\n+        tarW := tar.NewWriter(gzipW)\n \n-\t// Load the ignore rule configuration, which will use\n-\t// defaults if no .terraformignore is configured\n-\tignoreRules := parseIgnoreFile(src)\n+        // Load the ignore rule configuration, which will use\n+        // defaults if no .terraformignore is configured\n+        ignoreRules := parseIgnoreFile(src)\n \n-\t// Track the metadata details as we go.\n-\tmeta := &Meta{}\n+        // Track the metadata details as we go.\n+        meta := &Meta{}\n \n-\t// Walk the tree of files.\n-\terr := filepath.Walk(src, packWalkFn(src, src, src, tarW, meta, dereference, ignoreRules))\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        // Walk the tree of files.\n+        err := filepath.Walk(src, packWalkFn(src, src, src, tarW, meta, dereference, ignoreRules))\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\t// Flush the tar writer.\n-\tif err := tarW.Close(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"Failed to close the tar archive: %v\", err)\n-\t}\n+        // Flush the tar writer.\n+        if err := tarW.Close(); err != nil {\n+                return nil, fmt.Errorf(\"Failed to close the tar archive: %v\", err)\n+        }\n \n-\t// Flush the gzip writer.\n-\tif err := gzipW.Close(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"Failed to close the gzip writer: %v\", err)\n-\t}\n+        // Flush the gzip writer.\n+        if err := gzipW.Close(); err != nil {\n+                return nil, fmt.Errorf(\"Failed to close the gzip writer: %v\", err)\n+        }\n \n-\treturn meta, nil\n+        return meta, nil\n }\n \n func packWalkFn(root, src, dst string, tarW *tar.Writer, meta *Meta, dereference bool, ignoreRules []rule) filepath.WalkFunc {\n-\treturn func(path string, info os.FileInfo, err error) error {\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\t// Get the relative path from the current src directory.\n-\t\tsubpath, err := filepath.Rel(src, path)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to get relative path for file %q: %v\", path, err)\n-\t\t}\n-\t\tif subpath == \".\" {\n-\t\t\treturn nil\n-\t\t}\n-\n-\t\tif m := matchIgnoreRule(subpath, ignoreRules); m {\n-\t\t\treturn nil\n-\t\t}\n-\n-\t\t// Catch directories so we don't end up with empty directories,\n-\t\t// the files are ignored correctly\n-\t\tif info.IsDir() {\n-\t\t\tif m := matchIgnoreRule(subpath+string(os.PathSeparator), ignoreRules); m {\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Get the relative path from the initial root directory.\n-\t\tsubpath, err = filepath.Rel(root, strings.Replace(path, src, dst, 1))\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to get relative path for file %q: %v\", path, err)\n-\t\t}\n-\t\tif subpath == \".\" {\n-\t\t\treturn nil\n-\t\t}\n-\n-\t\t// Check the file type and if we need to write the body.\n-\t\tkeepFile, writeBody := checkFileMode(info.Mode())\n-\t\tif !keepFile {\n-\t\t\treturn nil\n-\t\t}\n-\n-\t\tfm := info.Mode()\n-\t\theader := &tar.Header{\n-\t\t\tName:    filepath.ToSlash(subpath),\n-\t\t\tModTime: info.ModTime(),\n-\t\t\tMode:    int64(fm.Perm()),\n-\t\t}\n-\n-\t\tswitch {\n-\t\tcase info.IsDir():\n-\t\t\theader.Typeflag = tar.TypeDir\n-\t\t\theader.Name += \"/\"\n-\n-\t\tcase fm.IsRegular():\n-\t\t\theader.Typeflag = tar.TypeReg\n-\t\t\theader.Size = info.Size()\n-\n-\t\tcase fm&os.ModeSymlink != 0:\n-\t\t\ttarget, err := filepath.EvalSymlinks(path)\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"Failed to get symbolic link destination for %q: %v\", path, err)\n-\t\t\t}\n-\n-\t\t\t// If the target is within the current source, we\n-\t\t\t// create the symlink using a relative path.\n-\t\t\tif strings.HasPrefix(target, src) {\n-\t\t\t\tlink, err := filepath.Rel(filepath.Dir(path), target)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn fmt.Errorf(\"Failed to get relative path for symlink destination %q: %v\", target, err)\n-\t\t\t\t}\n-\n-\t\t\t\theader.Typeflag = tar.TypeSymlink\n-\t\t\t\theader.Linkname = filepath.ToSlash(link)\n-\n-\t\t\t\t// Break out of the case as a symlink\n-\t\t\t\t// doesn't need any additional config.\n-\t\t\t\tbreak\n-\t\t\t}\n-\n-\t\t\tif !dereference {\n-\t\t\t\t// Return early as the symlink has a target outside of the\n-\t\t\t\t// src directory and we don't want to dereference symlinks.\n-\t\t\t\treturn nil\n-\t\t\t}\n-\n-\t\t\t// Get the file info for the target.\n-\t\t\tinfo, err = os.Lstat(target)\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"Failed to get file info from file %q: %v\", target, err)\n-\t\t\t}\n-\n-\t\t\t// If the target is a directory we can recurse into the target\n-\t\t\t// directory by calling the packWalkFn with updated arguments.\n-\t\t\tif info.IsDir() {\n-\t\t\t\treturn filepath.Walk(target, packWalkFn(root, target, path, tarW, meta, dereference, ignoreRules))\n-\t\t\t}\n-\n-\t\t\t// Dereference this symlink by updating the header with the target file\n-\t\t\t// details and set writeBody to true so the body will be written.\n-\t\t\theader.Typeflag = tar.TypeReg\n-\t\t\theader.ModTime = info.ModTime()\n-\t\t\theader.Mode = int64(info.Mode().Perm())\n-\t\t\theader.Size = info.Size()\n-\t\t\twriteBody = true\n-\n-\t\tdefault:\n-\t\t\treturn fmt.Errorf(\"Unexpected file mode %v\", fm)\n-\t\t}\n-\n-\t\t// Write the header first to the archive.\n-\t\tif err := tarW.WriteHeader(header); err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed writing archive header for file %q: %v\", path, err)\n-\t\t}\n-\n-\t\t// Account for the file in the list.\n-\t\tmeta.Files = append(meta.Files, header.Name)\n-\n-\t\t// Skip writing file data for certain file types (above).\n-\t\tif !writeBody {\n-\t\t\treturn nil\n-\t\t}\n-\n-\t\tf, err := os.Open(path)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed opening file %q for archiving: %v\", path, err)\n-\t\t}\n-\t\tdefer f.Close()\n-\n-\t\tsize, err := io.Copy(tarW, f)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed copying file %q to archive: %v\", path, err)\n-\t\t}\n-\n-\t\t// Add the size we copied to the body.\n-\t\tmeta.Size += size\n-\n-\t\treturn nil\n-\t}\n+        return func(path string, info os.FileInfo, err error) error {\n+                if err != nil {\n+                        return err\n+                }\n+\n+                // Get the relative path from the current src directory.\n+                subpath, err := filepath.Rel(src, path)\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed to get relative path for file %q: %v\", path, err)\n+                }\n+                if subpath == \".\" {\n+                        return nil\n+                }\n+\n+                if m := matchIgnoreRule(subpath, ignoreRules); m {\n+                        return nil\n+                }\n+\n+                // Catch directories so we don't end up with empty directories,\n+                // the files are ignored correctly\n+                if info.IsDir() {\n+                        if m := matchIgnoreRule(subpath+string(os.PathSeparator), ignoreRules); m {\n+                                return nil\n+                        }\n+                }\n+\n+                // Get the relative path from the initial root directory.\n+                subpath, err = filepath.Rel(root, strings.Replace(path, src, dst, 1))\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed to get relative path for file %q: %v\", path, err)\n+                }\n+                if subpath == \".\" {\n+                        return nil\n+                }\n+\n+                // Check the file type and if we need to write the body.\n+                keepFile, writeBody := checkFileMode(info.Mode())\n+                if !keepFile {\n+                        return nil\n+                }\n+\n+                fm := info.Mode()\n+                header := &tar.Header{\n+                        Name:    filepath.ToSlash(subpath),\n+                        ModTime: info.ModTime(),\n+                        Mode:    int64(fm.Perm()),\n+                }\n+\n+                switch {\n+                case info.IsDir():\n+                        header.Typeflag = tar.TypeDir\n+                        header.Name += \"/\"\n+\n+                case fm.IsRegular():\n+                        header.Typeflag = tar.TypeReg\n+                        header.Size = info.Size()\n+\n+                case fm&os.ModeSymlink != 0:\n+                        target, err := filepath.EvalSymlinks(path)\n+                        if err != nil {\n+                                return fmt.Errorf(\"Failed to get symbolic link destination for %q: %v\", path, err)\n+                        }\n+\n+                        // If the target is within the current source, we\n+                        // create the symlink using a relative path.\n+                        if strings.HasPrefix(target, src) {\n+                                link, err := filepath.Rel(filepath.Dir(path), target)\n+                                if err != nil {\n+                                        return fmt.Errorf(\"Failed to get relative path for symlink destination %q: %v\", target, err)\n+                                }\n+\n+                                header.Typeflag = tar.TypeSymlink\n+                                header.Linkname = filepath.ToSlash(link)\n+\n+                                // Break out of the case as a symlink\n+                                // doesn't need any additional config.\n+                                break\n+                        }\n+\n+                        if !dereference {\n+                                // Return early as the symlink has a target outside of the\n+                                // src directory and we don't want to dereference symlinks.\n+                                return nil\n+                        }\n+\n+                        // Get the file info for the target.\n+                        info, err = os.Lstat(target)\n+                        if err != nil {\n+                                return fmt.Errorf(\"Failed to get file info from file %q: %v\", target, err)\n+                        }\n+\n+                        // If the target is a directory we can recurse into the target\n+                        // directory by calling the packWalkFn with updated arguments.\n+                        if info.IsDir() {\n+                                return filepath.Walk(target, packWalkFn(root, target, path, tarW, meta, dereference, ignoreRules))\n+                        }\n+\n+                        // Dereference this symlink by updating the header with the target file\n+                        // details and set writeBody to true so the body will be written.\n+                        header.Typeflag = tar.TypeReg\n+                        header.ModTime = info.ModTime()\n+                        header.Mode = int64(info.Mode().Perm())\n+                        header.Size = info.Size()\n+                        writeBody = true\n+\n+                default:\n+                        return fmt.Errorf(\"Unexpected file mode %v\", fm)\n+                }\n+\n+                // Write the header first to the archive.\n+                if err := tarW.WriteHeader(header); err != nil {\n+                        return fmt.Errorf(\"Failed writing archive header for file %q: %v\", path, err)\n+                }\n+\n+                // Account for the file in the list.\n+                meta.Files = append(meta.Files, header.Name)\n+\n+                // Skip writing file data for certain file types (above).\n+                if !writeBody {\n+                        return nil\n+                }\n+\n+                f, err := os.Open(path)\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed opening file %q for archiving: %v\", path, err)\n+                }\n+                defer f.Close()\n+\n+                size, err := io.Copy(tarW, f)\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed copying file %q to archive: %v\", path, err)\n+                }\n+\n+                // Add the size we copied to the body.\n+                meta.Size += size\n+\n+                return nil\n+        }\n }\n \n // Unpack is used to read and extract the contents of a slug to the dst\n // directory. Symlinks within the slug are supported, provided their targets\n // are relative and point to paths within the destination directory.\n func Unpack(r io.Reader, dst string) error {\n-\t// Decompress as we read.\n-\tuncompressed, err := gzip.NewReader(r)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"Failed to uncompress slug: %v\", err)\n-\t}\n-\n-\t// Untar as we read.\n-\tuntar := tar.NewReader(uncompressed)\n-\n-\t// Unpackage all the contents into the directory.\n-\tfor {\n-\t\theader, err := untar.Next()\n-\t\tif err == io.EOF {\n-\t\t\tbreak\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to untar slug: %v\", err)\n-\t\t}\n-\n-\t\t// Get rid of absolute paths.\n-\t\tpath := header.Name\n-\t\tif path[0] == '/' {\n-\t\t\tpath = path[1:]\n-\t\t}\n-\t\tpath = filepath.Join(dst, path)\n-\n-\t\t// Make the directories to the path.\n-\t\tdir := filepath.Dir(path)\n-\t\tif err := os.MkdirAll(dir, 0755); err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to create directory %q: %v\", dir, err)\n-\t\t}\n-\n-\t\t// Handle symlinks.\n-\t\tif header.Typeflag == tar.TypeSymlink {\n-\t\t\t// Disallow absolute targets.\n-\t\t\tif filepath.IsAbs(header.Linkname) {\n-\t\t\t\treturn fmt.Errorf(\"Invalid symlink (%q -> %q) has absolute target\",\n-\t\t\t\t\theader.Name, header.Linkname)\n-\t\t\t}\n-\n-\t\t\t// Ensure the link target is within the destination directory. This\n-\t\t\t// disallows providing symlinks to external files and directories.\n-\t\t\ttarget := filepath.Join(dir, header.Linkname)\n-\t\t\tif !strings.HasPrefix(target, dst) {\n-\t\t\t\treturn fmt.Errorf(\"Invalid symlink (%q -> %q) has external target\",\n-\t\t\t\t\theader.Name, header.Linkname)\n-\t\t\t}\n-\n-\t\t\t// Create the symlink.\n-\t\t\tif err := os.Symlink(header.Linkname, path); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"Failed creating symlink (%q -> %q): %v\",\n-\t\t\t\t\theader.Name, header.Linkname, err)\n-\t\t\t}\n-\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Only unpack regular files from this point on.\n-\t\tif header.Typeflag == tar.TypeDir {\n-\t\t\tcontinue\n-\t\t} else if header.Typeflag != tar.TypeReg && header.Typeflag != tar.TypeRegA {\n-\t\t\treturn fmt.Errorf(\"Failed creating %q: unsupported type %c\", path,\n-\t\t\t\theader.Typeflag)\n-\t\t}\n-\n-\t\t// Open a handle to the destination.\n-\t\tfh, err := os.Create(path)\n-\t\tif err != nil {\n-\t\t\t// This mimics tar's behavior wrt the tar file containing duplicate files\n-\t\t\t// and it allowing later ones to clobber earlier ones even if the file\n-\t\t\t// has perms that don't allow overwriting.\n-\t\t\tif os.IsPermission(err) {\n-\t\t\t\tos.Chmod(path, 0600)\n-\t\t\t\tfh, err = os.Create(path)\n-\t\t\t}\n-\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"Failed creating file %q: %v\", path, err)\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Copy the contents.\n-\t\t_, err = io.Copy(fh, untar)\n-\t\tfh.Close()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to copy slug file %q: %v\", path, err)\n-\t\t}\n-\n-\t\t// Restore the file mode. We have to do this after writing the file,\n-\t\t// since it is possible we have a read-only mode.\n-\t\tmode := header.FileInfo().Mode()\n-\t\tif err := os.Chmod(path, mode); err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed setting permissions on %q: %v\", path, err)\n-\t\t}\n-\t}\n-\treturn nil\n+        // Decompress as we read.\n+        uncompressed, err := gzip.NewReader(r)\n+        if err != nil {\n+                return fmt.Errorf(\"Failed to uncompress slug: %v\", err)\n+        }\n+\n+        // Untar as we read.\n+        untar := tar.NewReader(uncompressed)\n+\n+        // Unpackage all the contents into the directory.\n+\n+cleanDst, err := filepath.Abs(dst)\n+if err != nil {\n+return fmt.Errorf(\"cannot get absolute destination %q: %v\", dst, err)\n+}\n+\n+// Handle symlinks.\n+if header.Typeflag == tar.TypeSymlink {\n+if filepath.IsAbs(header.Linkname) {\n+return fmt.Errorf(\"archive contains symlink with absolute target %q\", header.Linkname)\n+}\n+\n+target := filepath.Join(dir, header.Linkname)\n+absTarget, err := filepath.Abs(target)\n+if err != nil {\n+return fmt.Errorf(\"cannot get absolute target for %q: %v\", target, err)\n+}\n+\n+if !strings.HasPrefix(absTarget, cleanDst) {\n+return fmt.Errorf(\"archive contains symlink with external target %q\", target)\n+}\n+\n+// Create the symlink.\n+if err := os.Symlink(header.Linkname, path); err != nil {\n+return fmt.Errorf(\"failed creating symlink (%q -> %q): %v\",\n+header.Name, header.Linkname, err)\n+}\n+\n+continue\n+}\n+\n+\n+\n+\n+\n+\n+        for {\n+                header, err := untar.Next()\n+                if err == io.EOF {\n+                        break\n+                }\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed to untar slug: %v\", err)\n+                }\n+\n+                // Get rid of absolute paths.\n+                path := header.Name\n+                if path[0] == '/' {\n+                        path = path[1:]\n+                }\n+                finalDest := dst\n+if f, err := os.Lstat(dst); err == nil && f.Mode()&os.ModeSymlink != 0 {\n+finalDest, err = filepath.EvalSymlinks(dst)\n+if err != nil {\n+return fmt.Errorf(\"failed to evaluate symlink %s: %w\", dst, err)\n+}\n+}\n+\n+path = filepath.Join(finalDest, path)\n+\n+if !strings.HasPrefix(path, finalDest) {\n+return fmt.Errorf(\"archive contains invalid path %s\", header.Name)\n+}\n+\n+                // Make the directories to the path.\n+                dir := filepath.Dir(path)\n+                if err := os.MkdirAll(dir, 0755); err != nil {\n+                        return fmt.Errorf(\"Failed to create directory %q: %v\", dir, err)\n+                }\n+\n+\n+\n+                \n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+\n+                        // Create the symlink.\n+                        if err := os.Symlink(header.Linkname, path); err != nil {\n+                                return fmt.Errorf(\"Failed creating symlink (%q -> %q): %v\",\n+                                        header.Name, header.Linkname, err)\n+                        }\n+\n+                        continue\n+                }\n+\n+                // Only unpack regular files from this point on.\n+                if header.Typeflag == tar.TypeDir {\n+                        continue\n+                } else if header.Typeflag != tar.TypeReg && header.Typeflag != tar.TypeRegA {\n+                        return fmt.Errorf(\"Failed creating %q: unsupported type %c\", path,\n+                                header.Typeflag)\n+                }\n+\n+                // Open a handle to the destination.\n+                fh, err := os.Create(path)\n+                if err != nil {\n+                        // This mimics tar's behavior wrt the tar file containing duplicate files\n+                        // and it allowing later ones to clobber earlier ones even if the file\n+                        // has perms that don't allow overwriting.\n+                        if os.IsPermission(err) {\n+                                os.Chmod(path, 0600)\n+                                fh, err = os.Create(path)\n+                        }\n+\n+                        if err != nil {\n+                                return fmt.Errorf(\"Failed creating file %q: %v\", path, err)\n+                        }\n+                }\n+\n+                // Copy the contents.\n+                _, err = io.Copy(fh, untar)\n+                fh.Close()\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed to copy slug file %q: %v\", path, err)\n+                }\n+\n+                // Restore the file mode. We have to do this after writing the file,\n+                // since it is possible we have a read-only mode.\n+                mode := header.FileInfo().Mode()\n+                if err := os.Chmod(path, mode); err != nil {\n+                        return fmt.Errorf(\"Failed setting permissions on %q: %v\", path, err)\n+                }\n+        }\n+        return nil\n }\n \n+\n+\n+\n+\n+\n+\n+\n+\n+\n // checkFileMode is used to examine an os.FileMode and determine if it should\n+\n+\n+\n+\n+\n // be included in the archive, and if it has a data body which needs writing.\n func checkFileMode(m os.FileMode) (keep, body bool) {\n-\tswitch {\n-\tcase m.IsDir():\n-\t\treturn true, false\n+        switch {\n+        case m.IsDir():\n+                return true, false\n \n-\tcase m.IsRegular():\n-\t\treturn true, true\n+        case m.IsRegular():\n+                return true, true\n \n-\tcase m&os.ModeSymlink != 0:\n-\t\treturn true, false\n-\t}\n+        case m&os.ModeSymlink != 0:\n+                return true, false\n+        }\n \n-\treturn false, false\n+        return false, false\n }\n"}
{"cve":"CVE-2024-43405:0708", "fix_patch": "diff --git a/pkg/templates/signer/tmpl_signer.go b/pkg/templates/signer/tmpl_signer.go\nindex c4378fd4..5ec55976 100644\n--- a/pkg/templates/signer/tmpl_signer.go\n+++ b/pkg/templates/signer/tmpl_signer.go\n@@ -1,234 +1,244 @@\n package signer\n \n import (\n-\t\"bytes\"\n-\t\"crypto/ecdsa\"\n-\t\"crypto/md5\"\n-\t\"crypto/rand\"\n-\t\"crypto/sha256\"\n-\t\"encoding/gob\"\n-\t\"encoding/hex\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"os\"\n-\t\"regexp\"\n-\t\"strings\"\n-\t\"sync\"\n-\n-\t\"github.com/projectdiscovery/gologger\"\n-\t\"github.com/projectdiscovery/nuclei/v3/pkg/catalog/config\"\n-\terrorutil \"github.com/projectdiscovery/utils/errors\"\n+        \"bytes\"\n+        \"crypto/ecdsa\"\n+        \"crypto/md5\"\n+        \"crypto/rand\"\n+        \"crypto/sha256\"\n+        \"encoding/gob\"\n+        \"encoding/hex\"\n+        \"errors\"\n+        \"fmt\"\n+        \"os\"\n+        \"regexp\"\n+        \"strings\"\n+        \"sync\"\n+\n+        \"github.com/projectdiscovery/gologger\"\n+        \"github.com/projectdiscovery/nuclei/v3/pkg/catalog/config\"\n+        errorutil \"github.com/projectdiscovery/utils/errors\"\n )\n \n var (\n-\tReDigest            = regexp.MustCompile(`(?m)^#\\sdigest:\\s.+$`)\n-\tErrUnknownAlgorithm = errors.New(\"unknown algorithm\")\n-\tSignaturePattern    = \"# digest: \"\n-\tSignatureFmt        = SignaturePattern + \"%x\" + \":%v\" // `#digest: <signature>:<fragment>`\n+        ReDigest            = regexp.MustCompile(`(?m)^#\\sdigest:\\s.+$`)\n+        ErrUnknownAlgorithm = errors.New(\"unknown algorithm\")\n+        SignaturePattern    = \"# digest: \"\n+        SignatureFmt        = SignaturePattern + \"%x\" + \":%v\" // `#digest: <signature>:<fragment>`\n )\n \n func RemoveSignatureFromData(data []byte) []byte {\n-\treturn bytes.Trim(ReDigest.ReplaceAll(data, []byte(\"\")), \"\\n\")\n+        return bytes.Trim(ReDigest.ReplaceAll(data, []byte(\"\")), \"\\n\")\n }\n \n func GetSignatureFromData(data []byte) []byte {\n-\treturn ReDigest.Find(data)\n+        return ReDigest.Find(data)\n }\n \n // SignableTemplate is a template that can be signed\n type SignableTemplate interface {\n-\t// GetFileImports returns a list of files that are imported by the template\n-\tGetFileImports() []string\n-\t// HasCodeProtocol returns true if the template has a code protocol section\n-\tHasCodeProtocol() bool\n+        // GetFileImports returns a list of files that are imported by the template\n+        GetFileImports() []string\n+        // HasCodeProtocol returns true if the template has a code protocol section\n+        HasCodeProtocol() bool\n }\n \n type TemplateSigner struct {\n-\tsync.Once\n-\thandler  *KeyHandler\n-\tfragment string\n+        sync.Once\n+        handler  *KeyHandler\n+        fragment string\n }\n \n // Identifier returns the identifier for the template signer\n func (t *TemplateSigner) Identifier() string {\n-\treturn t.handler.cert.Subject.CommonName\n+        return t.handler.cert.Subject.CommonName\n }\n \n // fragment is optional part of signature that is used to identify the user\n // who signed the template via md5 hash of public key\n func (t *TemplateSigner) GetUserFragment() string {\n-\t// wrap with sync.Once to reduce unnecessary md5 hashing\n-\tt.Do(func() {\n-\t\tif t.handler.ecdsaPubKey != nil {\n-\t\t\thashed := md5.Sum(t.handler.ecdsaPubKey.X.Bytes())\n-\t\t\tt.fragment = fmt.Sprintf(\"%x\", hashed)\n-\t\t}\n-\t})\n-\treturn t.fragment\n+        // wrap with sync.Once to reduce unnecessary md5 hashing\n+        t.Do(func() {\n+                if t.handler.ecdsaPubKey != nil {\n+                        hashed := md5.Sum(t.handler.ecdsaPubKey.X.Bytes())\n+                        t.fragment = fmt.Sprintf(\"%x\", hashed)\n+                }\n+        })\n+        return t.fragment\n }\n \n // Sign signs the given template with the template signer and returns the signature\n func (t *TemplateSigner) Sign(data []byte, tmpl SignableTemplate) (string, error) {\n-\t// while re-signing template check if it has a code protocol\n-\t// if it does then verify that it is signed by current signer\n-\t// if not then return error\n-\tif tmpl.HasCodeProtocol() {\n-\t\tsig := GetSignatureFromData(data)\n-\t\tarr := strings.SplitN(string(sig), \":\", 3)\n-\t\tif len(arr) == 2 {\n-\t\t\t// signature has no fragment\n-\t\t\treturn \"\", errorutil.NewWithTag(\"signer\", \"re-signing code templates are not allowed for security reasons.\")\n-\t\t}\n-\t\tif len(arr) == 3 {\n-\t\t\t// signature has fragment verify if it is equal to current fragment\n-\t\t\tfragment := t.GetUserFragment()\n-\t\t\tif fragment != arr[2] {\n-\t\t\t\treturn \"\", errorutil.NewWithTag(\"signer\", \"re-signing code templates are not allowed for security reasons.\")\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tbuff := bytes.NewBuffer(RemoveSignatureFromData(data))\n-\t// if file has any imports process them\n-\tfor _, file := range tmpl.GetFileImports() {\n-\t\tbin, err := os.ReadFile(file)\n-\t\tif err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\tbuff.WriteRune('\\n')\n-\t\tbuff.Write(bin)\n-\t}\n-\tsignatureData, err := t.sign(buff.Bytes())\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\treturn signatureData, nil\n+\n+// Use canonicalized line endings\n+data = bytes.ReplaceAll(data, []byte(\"\\r\\n\"), []byte(\"\\n\"))\n+        // while re-signing template check if it has a code protocol\n+        // if it does then verify that it is signed by current signer\n+        // if not then return error\n+        if tmpl.HasCodeProtocol() {\n+                sig := GetSignatureFromData(data)\n+                arr := strings.SplitN(string(sig), \":\", 3)\n+                if len(arr) == 2 {\n+                        // signature has no fragment\n+                        return \"\", errorutil.NewWithTag(\"signer\", \"re-signing code templates are not allowed for security reasons.\")\n+                }\n+                if len(arr) == 3 {\n+                        // signature has fragment verify if it is equal to current fragment\n+                        fragment := t.GetUserFragment()\n+                        if fragment != arr[2] {\n+                                return \"\", errorutil.NewWithTag(\"signer\", \"re-signing code templates are not allowed for security reasons.\")\n+                        }\n+                }\n+        }\n+\n+        buff := bytes.NewBuffer(RemoveSignatureFromData(data))\n+        // if file has any imports process them\n+        for _, file := range tmpl.GetFileImports() {\n+                bin, err := os.ReadFile(file)\n+                if err != nil {\n+                        return \"\", err\n+                }\n+                buff.WriteRune('\\n')\n+                buff.Write(bin)\n+        }\n+        signatureData, err := t.sign(buff.Bytes())\n+        if err != nil {\n+                return \"\", err\n+        }\n+        return signatureData, nil\n }\n \n // Signs given data with the template signer\n // Note: this should not be used for signing templates as file references\n // in templates are not processed use template.SignTemplate() instead\n func (t *TemplateSigner) sign(data []byte) (string, error) {\n-\tdataHash := sha256.Sum256(data)\n-\tecdsaSignature, err := ecdsa.SignASN1(rand.Reader, t.handler.ecdsaKey, dataHash[:])\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tvar signatureData bytes.Buffer\n-\tif err := gob.NewEncoder(&signatureData).Encode(ecdsaSignature); err != nil {\n-\t\treturn \"\", err\n-\t}\n-\treturn fmt.Sprintf(SignatureFmt, signatureData.Bytes(), t.GetUserFragment()), nil\n+        dataHash := sha256.Sum256(data)\n+        ecdsaSignature, err := ecdsa.SignASN1(rand.Reader, t.handler.ecdsaKey, dataHash[:])\n+        if err != nil {\n+                return \"\", err\n+        }\n+        var signatureData bytes.Buffer\n+        if err := gob.NewEncoder(&signatureData).Encode(ecdsaSignature); err != nil {\n+                return \"\", err\n+        }\n+        return fmt.Sprintf(SignatureFmt, signatureData.Bytes(), t.GetUserFragment()), nil\n }\n \n // Verify verifies the given template with the template signer\n func (t *TemplateSigner) Verify(data []byte, tmpl SignableTemplate) (bool, error) {\n-\tdigestData := ReDigest.Find(data)\n-\tif len(digestData) == 0 {\n-\t\treturn false, errors.New(\"digest not found\")\n-\t}\n-\n-\tdigestData = bytes.TrimSpace(bytes.TrimPrefix(digestData, []byte(SignaturePattern)))\n-\t// remove fragment from digest as it is used for re-signing purposes only\n-\tdigestString := strings.TrimSuffix(string(digestData), \":\"+t.GetUserFragment())\n-\tdigest, err := hex.DecodeString(digestString)\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n-\n-\tbuff := bytes.NewBuffer(RemoveSignatureFromData(data))\n-\t// if file has any imports process them\n-\tfor _, file := range tmpl.GetFileImports() {\n-\t\tbin, err := os.ReadFile(file)\n-\t\tif err != nil {\n-\t\t\treturn false, err\n-\t\t}\n-\t\tbuff.WriteRune('\\n')\n-\t\tbuff.Write(bin)\n-\t}\n-\n-\treturn t.verify(buff.Bytes(), digest)\n+\n+allSignatures := ReDigest.FindAll(data, -1)\n+if len(allSignatures) > 1 {\n+return false, errors.New(\"multiple signatures are not allowed\")\n+}\n+// Use canonicalized line endings\n+data = bytes.ReplaceAll(data, []byte(\"\\r\\n\"), []byte(\"\\n\"))\n+        digestData := ReDigest.Find(data)\n+        if len(digestData) == 0 {\n+                return false, errors.New(\"digest not found\")\n+        }\n+\n+        digestData = bytes.TrimSpace(bytes.TrimPrefix(digestData, []byte(SignaturePattern)))\n+        // remove fragment from digest as it is used for re-signing purposes only\n+        digestString := strings.TrimSuffix(string(digestData), \":\"+t.GetUserFragment())\n+        digest, err := hex.DecodeString(digestString)\n+        if err != nil {\n+                return false, err\n+        }\n+\n+        buff := bytes.NewBuffer(RemoveSignatureFromData(data))\n+        // if file has any imports process them\n+        for _, file := range tmpl.GetFileImports() {\n+                bin, err := os.ReadFile(file)\n+                if err != nil {\n+                        return false, err\n+                }\n+                buff.WriteRune('\\n')\n+                buff.Write(bin)\n+        }\n+\n+        return t.verify(buff.Bytes(), digest)\n }\n \n // Verify verifies the given data with the template signer\n // Note: this should not be used for verifying templates as file references\n // in templates are not processed\n func (t *TemplateSigner) verify(data, signatureData []byte) (bool, error) {\n-\tdataHash := sha256.Sum256(data)\n+        dataHash := sha256.Sum256(data)\n \n-\tvar signature []byte\n-\tif err := gob.NewDecoder(bytes.NewReader(signatureData)).Decode(&signature); err != nil {\n-\t\treturn false, err\n-\t}\n-\treturn ecdsa.VerifyASN1(t.handler.ecdsaPubKey, dataHash[:], signature), nil\n+        var signature []byte\n+        if err := gob.NewDecoder(bytes.NewReader(signatureData)).Decode(&signature); err != nil {\n+                return false, err\n+        }\n+        return ecdsa.VerifyASN1(t.handler.ecdsaPubKey, dataHash[:], signature), nil\n }\n \n // NewTemplateSigner creates a new signer for signing templates\n func NewTemplateSigner(cert, privateKey []byte) (*TemplateSigner, error) {\n-\thandler := &KeyHandler{}\n-\tvar err error\n-\tif cert != nil || privateKey != nil {\n-\t\thandler.UserCert = cert\n-\t\thandler.PrivateKey = privateKey\n-\t} else {\n-\t\terr = handler.ReadCert(CertEnvVarName, config.DefaultConfig.GetKeysDir())\n-\t\tif err == nil {\n-\t\t\terr = handler.ReadPrivateKey(PrivateKeyEnvName, config.DefaultConfig.GetKeysDir())\n-\t\t}\n-\t}\n-\tif err != nil && !SkipGeneratingKeys {\n-\t\tif err != ErrNoCertificate && err != ErrNoPrivateKey {\n-\t\t\tgologger.Info().Msgf(\"Invalid user cert found : %s\\n\", err)\n-\t\t}\n-\t\t// generating new keys\n-\t\thandler.GenerateKeyPair()\n-\t\tif err := handler.SaveToDisk(config.DefaultConfig.GetKeysDir()); err != nil {\n-\t\t\tgologger.Fatal().Msgf(\"could not save generated keys to disk: %s\\n\", err)\n-\t\t}\n-\t\t// do not continue further let user re-run the command\n-\t\tos.Exit(0)\n-\t} else if err != nil && SkipGeneratingKeys {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err := handler.ParseUserCert(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif err := handler.ParsePrivateKey(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn &TemplateSigner{\n-\t\thandler: handler,\n-\t}, nil\n+        handler := &KeyHandler{}\n+        var err error\n+        if cert != nil || privateKey != nil {\n+                handler.UserCert = cert\n+                handler.PrivateKey = privateKey\n+        } else {\n+                err = handler.ReadCert(CertEnvVarName, config.DefaultConfig.GetKeysDir())\n+                if err == nil {\n+                        err = handler.ReadPrivateKey(PrivateKeyEnvName, config.DefaultConfig.GetKeysDir())\n+                }\n+        }\n+        if err != nil && !SkipGeneratingKeys {\n+                if err != ErrNoCertificate && err != ErrNoPrivateKey {\n+                        gologger.Info().Msgf(\"Invalid user cert found : %s\\n\", err)\n+                }\n+                // generating new keys\n+                handler.GenerateKeyPair()\n+                if err := handler.SaveToDisk(config.DefaultConfig.GetKeysDir()); err != nil {\n+                        gologger.Fatal().Msgf(\"could not save generated keys to disk: %s\\n\", err)\n+                }\n+                // do not continue further let user re-run the command\n+                os.Exit(0)\n+        } else if err != nil && SkipGeneratingKeys {\n+                return nil, err\n+        }\n+\n+        if err := handler.ParseUserCert(); err != nil {\n+                return nil, err\n+        }\n+        if err := handler.ParsePrivateKey(); err != nil {\n+                return nil, err\n+        }\n+        return &TemplateSigner{\n+                handler: handler,\n+        }, nil\n }\n \n // NewTemplateSignerFromFiles creates a new signer for signing templates\n func NewTemplateSignerFromFiles(cert, privKey string) (*TemplateSigner, error) {\n-\tcertData, err := os.ReadFile(cert)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tprivKeyData, err := os.ReadFile(privKey)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn NewTemplateSigner(certData, privKeyData)\n+        certData, err := os.ReadFile(cert)\n+        if err != nil {\n+                return nil, err\n+        }\n+        privKeyData, err := os.ReadFile(privKey)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return NewTemplateSigner(certData, privKeyData)\n }\n \n // NewTemplateSigVerifier creates a new signer for verifying templates\n func NewTemplateSigVerifier(cert []byte) (*TemplateSigner, error) {\n-\thandler := &KeyHandler{}\n-\tif cert != nil {\n-\t\thandler.UserCert = cert\n-\t} else {\n-\t\tif err := handler.ReadCert(CertEnvVarName, config.DefaultConfig.GetKeysDir()); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\tif err := handler.ParseUserCert(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn &TemplateSigner{\n-\t\thandler: handler,\n-\t}, nil\n+        handler := &KeyHandler{}\n+        if cert != nil {\n+                handler.UserCert = cert\n+        } else {\n+                if err := handler.ReadCert(CertEnvVarName, config.DefaultConfig.GetKeysDir()); err != nil {\n+                        return nil, err\n+                }\n+        }\n+        if err := handler.ParseUserCert(); err != nil {\n+                return nil, err\n+        }\n+        return &TemplateSigner{\n+                handler: handler,\n+        }, nil\n }\n"}
{"cve":"CVE-2023-33967:0708", "fix_patch": "diff --git a/probe/client/mysql/mysql.go b/probe/client/mysql/mysql.go\nindex 572a330..62bc1c1 100644\n--- a/probe/client/mysql/mysql.go\n+++ b/probe/client/mysql/mysql.go\n@@ -19,17 +19,17 @@\n package mysql\n \n import (\n-\t\"crypto/tls\"\n-\t\"database/sql\"\n-\t\"fmt\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/go-sql-driver/mysql\"\n-\t\"github.com/megaease/easeprobe/global\"\n-\t\"github.com/megaease/easeprobe/probe/client/conf\"\n-\tlog \"github.com/sirupsen/logrus\"\n+        \"crypto/tls\"\n+        \"database/sql\"\n+        \"fmt\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/go-sql-driver/mysql\"\n+        \"github.com/megaease/easeprobe/global\"\n+        \"github.com/megaease/easeprobe/probe/client/conf\"\n+        log \"github.com/sirupsen/logrus\"\n )\n \n // Kind is the type of driver\n@@ -37,116 +37,116 @@ const Kind string = \"MySQL\"\n \n // MySQL is the MySQL client\n type MySQL struct {\n-\tconf.Options `yaml:\",inline\"`\n-\ttls          *tls.Config `yaml:\"-\" json:\"-\"`\n-\tConnStr      string      `yaml:\"conn_str,omitempty\" json:\"conn_str,omitempty\"`\n+        conf.Options `yaml:\",inline\"`\n+        tls          *tls.Config `yaml:\"-\" json:\"-\"`\n+        ConnStr      string      `yaml:\"conn_str,omitempty\" json:\"conn_str,omitempty\"`\n }\n \n // New create a Mysql client\n func New(opt conf.Options) (*MySQL, error) {\n \n-\tvar conn string\n-\tif len(opt.Password) > 0 {\n-\t\tconn = fmt.Sprintf(\"%s:%s@tcp(%s)/?timeout=%s\",\n-\t\t\topt.Username, opt.Password, opt.Host, opt.Timeout().Round(time.Second))\n-\t} else {\n-\t\tconn = fmt.Sprintf(\"%s@tcp(%s)/?timeout=%s\",\n-\t\t\topt.Username, opt.Host, opt.Timeout().Round(time.Second))\n-\t}\n-\n-\ttls, err := opt.TLS.Config()\n-\tif err != nil {\n-\t\tlog.Errorf(\"[%s / %s / %s] - TLS Config Error - %v\", opt.ProbeKind, opt.ProbeName, opt.ProbeTag, err)\n-\t\treturn nil, fmt.Errorf(\"TLS Config Error - %v\", err)\n-\t} else if tls != nil {\n-\t\tconn += \"&tls=\" + global.DefaultProg\n-\t}\n-\n-\tm := &MySQL{\n-\t\tOptions: opt,\n-\t\ttls:     tls,\n-\t\tConnStr: conn,\n-\t}\n-\n-\tif err := m.checkData(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn m, nil\n+        var conn string\n+        if len(opt.Password) > 0 {\n+                conn = fmt.Sprintf(\"%s:%s@tcp(%s)/?timeout=%s\",\n+                        opt.Username, opt.Password, opt.Host, opt.Timeout().Round(time.Second))\n+        } else {\n+                conn = fmt.Sprintf(\"%s@tcp(%s)/?timeout=%s\",\n+                        opt.Username, opt.Host, opt.Timeout().Round(time.Second))\n+        }\n+\n+        tls, err := opt.TLS.Config()\n+        if err != nil {\n+                log.Errorf(\"[%s / %s / %s] - TLS Config Error - %v\", opt.ProbeKind, opt.ProbeName, opt.ProbeTag, err)\n+                return nil, fmt.Errorf(\"TLS Config Error - %v\", err)\n+        } else if tls != nil {\n+                conn += \"&tls=\" + global.DefaultProg\n+        }\n+\n+        m := &MySQL{\n+                Options: opt,\n+                tls:     tls,\n+                ConnStr: conn,\n+        }\n+\n+        if err := m.checkData(); err != nil {\n+                return nil, err\n+        }\n+        return m, nil\n }\n \n // Kind return the name of client\n func (r *MySQL) Kind() string {\n-\treturn Kind\n+        return Kind\n }\n \n // checkData do the data checking\n func (r *MySQL) checkData() error {\n \n-\tfor k := range r.Data {\n-\t\tif _, err := r.getSQL(k); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n+        for k := range r.Data {\n+                if _, err := r.getSQL(k); err != nil {\n+                        return err\n+                }\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n // Probe do the health check\n func (r *MySQL) Probe() (bool, string) {\n \n-\tif r.tls != nil {\n-\t\tmysql.RegisterTLSConfig(global.DefaultProg, r.tls)\n-\t}\n-\n-\tdb, err := sql.Open(\"mysql\", r.ConnStr)\n-\tif err != nil {\n-\t\treturn false, err.Error()\n-\t}\n-\tdefer db.Close()\n-\n-\t// Check if we need to query specific data\n-\tif len(r.Data) > 0 {\n-\t\tfor k, v := range r.Data {\n-\t\t\tlog.Debugf(\"[%s / %s / %s] - Verifying Data - [%s] : [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, k, v)\n-\t\t\tsql, err := r.getSQL(k)\n-\t\t\tif err != nil {\n-\t\t\t\treturn false, err.Error()\n-\t\t\t}\n-\t\t\tlog.Debugf(\"[%s / %s / %s] - SQL - [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, sql)\n-\t\t\trows, err := db.Query(sql)\n-\t\t\tif err != nil {\n-\t\t\t\treturn false, err.Error()\n-\t\t\t}\n-\t\t\tif !rows.Next() {\n-\t\t\t\trows.Close()\n-\t\t\t\treturn false, fmt.Sprintf(\"No data found for [%s]\", k)\n-\t\t\t}\n-\t\t\t//check the value is equal to the value in data\n-\t\t\tvar value string\n-\t\t\tif err := rows.Scan(&value); err != nil {\n-\t\t\t\trows.Close()\n-\t\t\t\treturn false, err.Error()\n-\t\t\t}\n-\t\t\tif value != v {\n-\t\t\t\trows.Close()\n-\t\t\t\treturn false, fmt.Sprintf(\"Value not match for [%s] expected [%s] got [%s] \", k, v, value)\n-\t\t\t}\n-\t\t\trows.Close()\n-\t\t\tlog.Debugf(\"[%s / %s / %s] - Data Verified Successfully! - [%s] : [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, k, v)\n-\t\t}\n-\t} else {\n-\t\terr = db.Ping()\n-\t\tif err != nil {\n-\t\t\treturn false, err.Error()\n-\t\t}\n-\t\trow, err := db.Query(\"show status like \\\"uptime\\\"\") // run a SQL to test\n-\t\tif err != nil {\n-\t\t\treturn false, err.Error()\n-\t\t}\n-\t\tdefer row.Close()\n-\t}\n-\n-\treturn true, \"Check MySQL Server Successfully!\"\n+        if r.tls != nil {\n+                mysql.RegisterTLSConfig(global.DefaultProg, r.tls)\n+        }\n+\n+        db, err := sql.Open(\"mysql\", r.ConnStr)\n+        if err != nil {\n+                return false, err.Error()\n+        }\n+        defer db.Close()\n+\n+        // Check if we need to query specific data\n+        if len(r.Data) > 0 {\n+                for k, v := range r.Data {\n+                        log.Debugf(\"[%s / %s / %s] - Verifying Data - [%s] : [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, k, v)\n+                        sql, err := r.getSQL(k)\n+                        if err != nil {\n+                                return false, err.Error()\n+                        }\n+                        log.Debugf(\"[%s / %s / %s] - SQL - [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, sql)\n+                        rows, err := db.Query(sql)\n+                        if err != nil {\n+                                return false, err.Error()\n+                        }\n+                        if !rows.Next() {\n+                                rows.Close()\n+                                return false, fmt.Sprintf(\"No data found for [%s]\", k)\n+                        }\n+                        //check the value is equal to the value in data\n+                        var value string\n+                        if err := rows.Scan(&value); err != nil {\n+                                rows.Close()\n+                                return false, err.Error()\n+                        }\n+                        if value != v {\n+                                rows.Close()\n+                                return false, fmt.Sprintf(\"Value not match for [%s] expected [%s] got [%s] \", k, v, value)\n+                        }\n+                        rows.Close()\n+                        log.Debugf(\"[%s / %s / %s] - Data Verified Successfully! - [%s] : [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, k, v)\n+                }\n+        } else {\n+                err = db.Ping()\n+                if err != nil {\n+                        return false, err.Error()\n+                }\n+                row, err := db.Query(\"show status like \\\"uptime\\\"\") // run a SQL to test\n+                if err != nil {\n+                        return false, err.Error()\n+                }\n+                defer row.Close()\n+        }\n+\n+        return true, \"Check MySQL Server Successfully!\"\n \n }\n \n@@ -154,23 +154,22 @@ func (r *MySQL) Probe() (bool, string) {\n // input: database:table:column:key:value\n // output: SELECT column FROM database.table WHERE key = value\n func (r *MySQL) getSQL(str string) (string, error) {\n-\tif len(strings.TrimSpace(str)) == 0 {\n-\t\treturn \"\", fmt.Errorf(\"Empty SQL data\")\n-\t}\n-\tfields := strings.Split(str, \":\")\n-\tif len(fields) != 5 {\n-\t\treturn \"\", fmt.Errorf(\"Invalid SQL data - [%s]. (syntax: database:table:field:key:value)\", str)\n-\t}\n-\tdb := fields[0]\n-\ttable := fields[1]\n-\tfield := fields[2]\n-\tkey := fields[3]\n-\tvalue := fields[4]\n-\t//check value is int or not\n-\tif _, err := strconv.Atoi(value); err != nil {\n-\t\treturn \"\", fmt.Errorf(\"Invalid SQL data - [%s], the value must be int\", str)\n-\t}\n-\n-\tsql := fmt.Sprintf(\"SELECT %s FROM %s.%s WHERE %s = %s\", field, db, table, key, value)\n-\treturn sql, nil\n+        if len(strings.TrimSpace(str)) == 0 {\n+                return \"\", fmt.Errorf(\"Empty SQL data\")\n+        }\n+        fields := strings.Split(str, \":\")\n+        if len(fields) != 5 {\n+                return \"\", fmt.Errorf(\"Invalid SQL data - [%s]. (syntax: database:table:field:key:value)\", str)\n+        }\n+        db := strings.Replace(fields[0], \"`\", \"``\", -1)\n+        table := strings.Replace(fields[1], \"`\", \"``\", -1)\n+        field := strings.Replace(fields[2], \"`\", \"``\", -1)\n+        key := strings.Replace(fields[3], \"`\", \"``\", -1)\n+        value := fields[4]\n+        i, err := strconv.Atoi(value)\n+        if err != nil {\n+                return \"\", fmt.Errorf(\"Invalid SQL data - [%s], the value must be int\", str)\n+        }\n+        sql := fmt.Sprintf(\"SELECT `%s` FROM `%s`.`%s` WHERE `%s` = %d\", field, db, table, key, i)\n+        return sql, nil\n }\n"}
{"cve":"CVE-2024-27289:0708", "fix_patch": ""}
{"cve":"CVE-2023-30625:0708", "fix_patch": "diff --git a/router/failed-events-manager.go b/router/failed-events-manager.go\nindex b76908fb..246cdc56 100644\n--- a/router/failed-events-manager.go\n+++ b/router/failed-events-manager.go\n@@ -1,190 +1,190 @@\n package router\n \n import (\n-\t\"context\"\n-\t\"database/sql\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/rudderlabs/rudder-server/utils/misc\"\n+        \"context\"\n+        \"database/sql\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/rudderlabs/rudder-server/utils/misc\"\n )\n \n var failedEventsManager FailedEventsManagerI\n \n type FailedEventRowT struct {\n-\tDestinationID string          `json:\"destination_id\"`\n-\tRecordID      json.RawMessage `json:\"record_id\"`\n+        DestinationID string          `json:\"destination_id\"`\n+        RecordID      json.RawMessage `json:\"record_id\"`\n }\n \n var (\n-\tfailedKeysTablePrefix  = \"failed_keys\"\n-\tfailedKeysExpire       time.Duration\n-\tfailedKeysCleanUpSleep time.Duration\n-\tfailedKeysEnabled      bool\n+        failedKeysTablePrefix  = \"failed_keys\"\n+        failedKeysExpire       time.Duration\n+        failedKeysCleanUpSleep time.Duration\n+        failedKeysEnabled      bool\n )\n \n type FailedEventsManagerI interface {\n-\tSaveFailedRecordIDs(map[string][]*FailedEventRowT, *sql.Tx)\n-\tDropFailedRecordIDs(jobRunID string)\n-\tFetchFailedRecordIDs(jobRunID string) []*FailedEventRowT\n-\tGetDBHandle() *sql.DB\n+        SaveFailedRecordIDs(map[string][]*FailedEventRowT, *sql.Tx)\n+        DropFailedRecordIDs(jobRunID string)\n+        FetchFailedRecordIDs(jobRunID string) []*FailedEventRowT\n+        GetDBHandle() *sql.DB\n }\n \n type FailedEventsManagerT struct {\n-\tdbHandle *sql.DB\n+        dbHandle *sql.DB\n }\n \n func GetFailedEventsManager() FailedEventsManagerI {\n-\tif failedEventsManager == nil {\n-\t\tfem := new(FailedEventsManagerT)\n-\t\tdbHandle, err := sql.Open(\"postgres\", misc.GetConnectionString())\n-\t\tif err != nil {\n-\t\t\tpanic(err)\n-\t\t}\n-\t\tfem.dbHandle = dbHandle\n-\t\tfailedEventsManager = fem\n-\t}\n-\n-\treturn failedEventsManager\n+        if failedEventsManager == nil {\n+                fem := new(FailedEventsManagerT)\n+                dbHandle, err := sql.Open(\"postgres\", misc.GetConnectionString())\n+                if err != nil {\n+                        panic(err)\n+                }\n+                fem.dbHandle = dbHandle\n+                failedEventsManager = fem\n+        }\n+\n+        return failedEventsManager\n }\n \n func (*FailedEventsManagerT) SaveFailedRecordIDs(taskRunIDFailedEventsMap map[string][]*FailedEventRowT, txn *sql.Tx) {\n-\tif !failedKeysEnabled {\n-\t\treturn\n-\t}\n-\n-\tfor taskRunID, failedEvents := range taskRunIDFailedEventsMap {\n-\t\ttable := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n-\t\tsqlStatement := fmt.Sprintf(`CREATE TABLE IF NOT EXISTS %s (\n-\t\tdestination_id TEXT NOT NULL,\n-\t\trecord_id JSONB NOT NULL,\n-\t\tcreated_at TIMESTAMP NOT NULL);`, table)\n-\t\t_, err := txn.Exec(sqlStatement)\n-\t\tif err != nil {\n-\t\t\t_ = txn.Rollback()\n-\t\t\tpanic(err)\n-\t\t}\n-\t\tinsertQuery := fmt.Sprintf(`INSERT INTO %s VALUES($1, $2, $3);`, table)\n-\t\tstmt, err := txn.Prepare(insertQuery)\n-\t\tif err != nil {\n-\t\t\t_ = txn.Rollback()\n-\t\t\tpanic(err)\n-\t\t}\n-\t\tcreatedAt := time.Now()\n-\t\tfor _, failedEvent := range failedEvents {\n-\t\t\tif len(failedEvent.RecordID) == 0 || !json.Valid(failedEvent.RecordID) {\n-\t\t\t\tpkgLogger.Infof(\"skipped adding invalid recordId: %s, to failed keys table: %s\", failedEvent.RecordID, table)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\t_, err = stmt.Exec(failedEvent.DestinationID, failedEvent.RecordID, createdAt)\n-\t\t\tif err != nil {\n-\t\t\t\tpanic(err)\n-\t\t\t}\n-\t\t}\n-\n-\t\tstmt.Close()\n-\t}\n+        if !failedKeysEnabled {\n+                return\n+        }\n+\n+        for taskRunID, failedEvents := range taskRunIDFailedEventsMap {\n+                table := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n+                sqlStatement := fmt.Sprintf(`CREATE TABLE IF NOT EXISTS %s (\n+                destination_id TEXT NOT NULL,\n+                record_id JSONB NOT NULL,\n+                created_at TIMESTAMP NOT NULL);`, table)\n+                _, err := txn.Exec(sqlStatement)\n+                if err != nil {\n+                        _ = txn.Rollback()\n+                        panic(err)\n+                }\n+                insertQuery := fmt.Sprintf(`INSERT INTO %s VALUES($1, $2, $3);`, table)\n+                stmt, err := txn.Prepare(insertQuery)\n+                if err != nil {\n+                        _ = txn.Rollback()\n+                        panic(err)\n+                }\n+                createdAt := time.Now()\n+                for _, failedEvent := range failedEvents {\n+                        if len(failedEvent.RecordID) == 0 || !json.Valid(failedEvent.RecordID) {\n+                                pkgLogger.Infof(\"skipped adding invalid recordId: %s, to failed keys table: %s\", failedEvent.RecordID, table)\n+                                continue\n+                        }\n+                        _, err = stmt.Exec(failedEvent.DestinationID, failedEvent.RecordID, createdAt)\n+                        if err != nil {\n+                                panic(err)\n+                        }\n+                }\n+\n+                stmt.Close()\n+        }\n }\n \n func (fem *FailedEventsManagerT) DropFailedRecordIDs(taskRunID string) {\n-\tif !failedKeysEnabled {\n-\t\treturn\n-\t}\n-\n-\t// Drop table\n-\ttable := fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID)\n-\tsqlStatement := fmt.Sprintf(`DROP TABLE IF EXISTS %s`, table)\n-\t_, err := fem.dbHandle.Exec(sqlStatement)\n-\tif err != nil {\n-\t\tpkgLogger.Errorf(\"Failed to drop table %s with error: %v\", taskRunID, err)\n-\t}\n+        if !failedKeysEnabled {\n+                return\n+        }\n+\n+        // Drop table\n+        table := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n+        sqlStatement := fmt.Sprintf(`DROP TABLE IF EXISTS %s`, table)\n+        _, err := fem.dbHandle.Exec(sqlStatement)\n+        if err != nil {\n+                pkgLogger.Errorf(\"Failed to drop table %s with error: %v\", taskRunID, err)\n+        }\n }\n \n func (fem *FailedEventsManagerT) FetchFailedRecordIDs(taskRunID string) []*FailedEventRowT {\n-\tif !failedKeysEnabled {\n-\t\treturn []*FailedEventRowT{}\n-\t}\n+        if !failedKeysEnabled {\n+                return []*FailedEventRowT{}\n+        }\n \n-\tfailedEvents := make([]*FailedEventRowT, 0)\n+        failedEvents := make([]*FailedEventRowT, 0)\n \n-\tvar rows *sql.Rows\n-\tvar err error\n-\ttable := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n-\tsqlStatement := fmt.Sprintf(`SELECT %[1]s.destination_id, %[1]s.record_id\n+        var rows *sql.Rows\n+        var err error\n+        table := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n+        sqlStatement := fmt.Sprintf(`SELECT %[1]s.destination_id, %[1]s.record_id\n                                              FROM %[1]s `, table)\n-\trows, err = fem.dbHandle.Query(sqlStatement)\n-\tif err != nil {\n-\t\tpkgLogger.Errorf(\"Failed to fetch from table %s with error: %v\", taskRunID, err)\n-\t\treturn failedEvents\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar failedEvent FailedEventRowT\n-\t\terr := rows.Scan(&failedEvent.DestinationID, &failedEvent.RecordID)\n-\t\tif err != nil {\n-\t\t\tpanic(err)\n-\t\t}\n-\t\tfailedEvents = append(failedEvents, &failedEvent)\n-\t}\n-\n-\treturn failedEvents\n+        rows, err = fem.dbHandle.Query(sqlStatement)\n+        if err != nil {\n+                pkgLogger.Errorf(\"Failed to fetch from table %s with error: %v\", taskRunID, err)\n+                return failedEvents\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var failedEvent FailedEventRowT\n+                err := rows.Scan(&failedEvent.DestinationID, &failedEvent.RecordID)\n+                if err != nil {\n+                        panic(err)\n+                }\n+                failedEvents = append(failedEvents, &failedEvent)\n+        }\n+\n+        return failedEvents\n }\n \n func CleanFailedRecordsTableProcess(ctx context.Context) {\n-\tif !failedKeysEnabled {\n-\t\treturn\n-\t}\n-\n-\tfor {\n-\t\tselect {\n-\t\tcase <-ctx.Done():\n-\t\t\treturn\n-\t\tcase <-time.After(failedKeysCleanUpSleep):\n-\t\t\tdbHandle, err := sql.Open(\"postgres\", misc.GetConnectionString())\n-\t\t\tif err != nil {\n-\t\t\t\tpanic(err)\n-\t\t\t}\n-\t\t\tfailedKeysLike := failedKeysTablePrefix + \"%\"\n-\t\t\tfailedKeysTableQuery := fmt.Sprintf(`SELECT table_name\n-\t\t\t\t\t\t\t\t\t\t\t\t\tFROM information_schema.tables\n-\t\t\t\t\t\t\t\t\t\t\t\t\tWHERE table_schema='public' AND table_type='BASE TABLE' AND table_name ilike '%s'`, failedKeysLike)\n-\t\t\trows, err := dbHandle.Query(failedKeysTableQuery)\n-\t\t\tif err != nil {\n-\t\t\t\tpanic(err)\n-\t\t\t}\n-\t\t\tfor rows.Next() {\n-\t\t\t\tvar table string\n-\t\t\t\terr = rows.Scan(&table)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tpkgLogger.Errorf(\"Failed to scan failed keys table %s with error: %v\", table, err)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tlatestCreatedAtQuery := fmt.Sprintf(`SELECT created_at from %s order by created_at desc limit 1`, table)\n-\t\t\t\trow := dbHandle.QueryRow(latestCreatedAtQuery)\n-\t\t\t\tvar latestCreatedAt time.Time\n-\t\t\t\terr = row.Scan(&latestCreatedAt)\n-\t\t\t\tif err != nil && err != sql.ErrNoRows {\n-\t\t\t\t\tpkgLogger.Errorf(\"Failed to fetch records from failed keys table %s with error: %v\", table, err)\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tcurrentTime := time.Now()\n-\t\t\t\tdiff := currentTime.Sub(latestCreatedAt)\n-\t\t\t\tif diff > failedKeysExpire {\n-\t\t\t\t\tdropQuery := fmt.Sprintf(`DROP TABLE IF EXISTS %s`, table)\n-\t\t\t\t\trows, err = dbHandle.Query(dropQuery)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\tpkgLogger.Errorf(\"Failed to drop table %s with error: %v\", table, err)\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdbHandle.Close()\n-\t\t}\n-\t}\n+        if !failedKeysEnabled {\n+                return\n+        }\n+\n+        for {\n+                select {\n+                case <-ctx.Done():\n+                        return\n+                case <-time.After(failedKeysCleanUpSleep):\n+                        dbHandle, err := sql.Open(\"postgres\", misc.GetConnectionString())\n+                        if err != nil {\n+                                panic(err)\n+                        }\n+                        failedKeysLike := failedKeysTablePrefix + \"%\"\n+                        failedKeysTableQuery := fmt.Sprintf(`SELECT table_name\n+                                                                                                        FROM information_schema.tables\n+                                                                                                        WHERE table_schema='public' AND table_type='BASE TABLE' AND table_name ilike '%s'`, failedKeysLike)\n+                        rows, err := dbHandle.Query(failedKeysTableQuery)\n+                        if err != nil {\n+                                panic(err)\n+                        }\n+                        for rows.Next() {\n+                                var table string\n+                                err = rows.Scan(&table)\n+                                if err != nil {\n+                                        pkgLogger.Errorf(\"Failed to scan failed keys table %s with error: %v\", table, err)\n+                                        return\n+                                }\n+                                latestCreatedAtQuery := fmt.Sprintf(`SELECT created_at from %s order by created_at desc limit 1`, table)\n+                                row := dbHandle.QueryRow(latestCreatedAtQuery)\n+                                var latestCreatedAt time.Time\n+                                err = row.Scan(&latestCreatedAt)\n+                                if err != nil && err != sql.ErrNoRows {\n+                                        pkgLogger.Errorf(\"Failed to fetch records from failed keys table %s with error: %v\", table, err)\n+                                        continue\n+                                }\n+                                currentTime := time.Now()\n+                                diff := currentTime.Sub(latestCreatedAt)\n+                                if diff > failedKeysExpire {\n+                                        dropQuery := fmt.Sprintf(`DROP TABLE IF EXISTS %s`, table)\n+                                        rows, err = dbHandle.Query(dropQuery)\n+                                        if err != nil {\n+                                                pkgLogger.Errorf(\"Failed to drop table %s with error: %v\", table, err)\n+                                        }\n+                                }\n+                        }\n+                        dbHandle.Close()\n+                }\n+        }\n }\n \n func (fem *FailedEventsManagerT) GetDBHandle() *sql.DB {\n-\treturn fem.dbHandle\n+        return fem.dbHandle\n }\n"}
{"cve":"CVE-2022-23857:0708", "fix_patch": "diff --git a/model/criteria/criteria.go b/model/criteria/criteria.go\nindex 0d3c7295..4d6d2878 100644\n--- a/model/criteria/criteria.go\n+++ b/model/criteria/criteria.go\n@@ -2,98 +2,103 @@\n package criteria\n \n import (\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"strings\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"strings\"\n \n-\t\"github.com/navidrome/navidrome/log\"\n+        \"github.com/navidrome/navidrome/log\"\n \n-\t\"github.com/Masterminds/squirrel\"\n+        \"github.com/Masterminds/squirrel\"\n )\n \n type Expression = squirrel.Sqlizer\n \n type Criteria struct {\n-\tExpression\n-\tSort   string\n-\tOrder  string\n-\tLimit  int\n-\tOffset int\n+        Expression\n+        Sort   string\n+        Order  string\n+        Limit  int\n+        Offset int\n }\n \n func (c Criteria) OrderBy() string {\n-\tif c.Sort == \"\" {\n-\t\tc.Sort = \"title\"\n-\t}\n-\tf := fieldMap[strings.ToLower(c.Sort)]\n-\tvar mapped string\n-\tif f == nil {\n-\t\tlog.Error(\"Invalid field in 'sort' field\", \"field\", c.Sort)\n-\t\tmapped = c.Sort\n-\t} else {\n-\t\tif f.order == \"\" {\n-\t\t\tmapped = f.field\n-\t\t} else {\n-\t\t\tmapped = f.order\n-\t\t}\n-\t}\n-\tif c.Order != \"\" {\n-\t\tmapped = mapped + \" \" + c.Order\n-\t}\n-\treturn mapped\n+        if c.Sort == \"\" {\n+                c.Sort = \"title\"\n+        }\n+        f := fieldMap[strings.ToLower(c.Sort)]\n+        var mapped string\n+if f == nil {\n+log.Error(\"Invalid field in 'sort' field\", \"field\", c.Sort)\n+c.Sort = \"title\"\n+f = fieldMap[c.Sort]\n+}\n+if f.order == \"\" {\n+mapped = f.field\n+} else {\n+mapped = f.order\n+}\n+\n+if c.Order != \"\" {\n+order := strings.ToLower(c.Order)\n+if order == \"asc\" || order == \"desc\" {\n+mapped = mapped + \" \" + order\n+}\n+}\n+\n+return mapped\n }\n \n func (c Criteria) ToSql() (sql string, args []interface{}, err error) {\n-\treturn c.Expression.ToSql()\n+        return c.Expression.ToSql()\n }\n \n func (c Criteria) MarshalJSON() ([]byte, error) {\n-\taux := struct {\n-\t\tAll    []Expression `json:\"all,omitempty\"`\n-\t\tAny    []Expression `json:\"any,omitempty\"`\n-\t\tSort   string       `json:\"sort,omitempty\"`\n-\t\tOrder  string       `json:\"order,omitempty\"`\n-\t\tLimit  int          `json:\"limit,omitempty\"`\n-\t\tOffset int          `json:\"offset,omitempty\"`\n-\t}{\n-\t\tSort:   c.Sort,\n-\t\tOrder:  c.Order,\n-\t\tLimit:  c.Limit,\n-\t\tOffset: c.Offset,\n-\t}\n-\tswitch rules := c.Expression.(type) {\n-\tcase Any:\n-\t\taux.Any = rules\n-\tcase All:\n-\t\taux.All = rules\n-\tdefault:\n-\t\taux.All = All{rules}\n-\t}\n-\treturn json.Marshal(aux)\n+        aux := struct {\n+                All    []Expression `json:\"all,omitempty\"`\n+                Any    []Expression `json:\"any,omitempty\"`\n+                Sort   string       `json:\"sort,omitempty\"`\n+                Order  string       `json:\"order,omitempty\"`\n+                Limit  int          `json:\"limit,omitempty\"`\n+                Offset int          `json:\"offset,omitempty\"`\n+        }{\n+                Sort:   c.Sort,\n+                Order:  c.Order,\n+                Limit:  c.Limit,\n+                Offset: c.Offset,\n+        }\n+        switch rules := c.Expression.(type) {\n+        case Any:\n+                aux.Any = rules\n+        case All:\n+                aux.All = rules\n+        default:\n+                aux.All = All{rules}\n+        }\n+        return json.Marshal(aux)\n }\n \n func (c *Criteria) UnmarshalJSON(data []byte) error {\n-\tvar aux struct {\n-\t\tAll    unmarshalConjunctionType `json:\"all\"`\n-\t\tAny    unmarshalConjunctionType `json:\"any\"`\n-\t\tSort   string                   `json:\"sort\"`\n-\t\tOrder  string                   `json:\"order\"`\n-\t\tLimit  int                      `json:\"limit\"`\n-\t\tOffset int                      `json:\"offset\"`\n-\t}\n-\tif err := json.Unmarshal(data, &aux); err != nil {\n-\t\treturn err\n-\t}\n-\tif len(aux.Any) > 0 {\n-\t\tc.Expression = Any(aux.Any)\n-\t} else if len(aux.All) > 0 {\n-\t\tc.Expression = All(aux.All)\n-\t} else {\n-\t\treturn errors.New(\"invalid criteria json. missing rules (key 'all' or 'any')\")\n-\t}\n-\tc.Sort = aux.Sort\n-\tc.Order = aux.Order\n-\tc.Limit = aux.Limit\n-\tc.Offset = aux.Offset\n-\treturn nil\n+        var aux struct {\n+                All    unmarshalConjunctionType `json:\"all\"`\n+                Any    unmarshalConjunctionType `json:\"any\"`\n+                Sort   string                   `json:\"sort\"`\n+                Order  string                   `json:\"order\"`\n+                Limit  int                      `json:\"limit\"`\n+                Offset int                      `json:\"offset\"`\n+        }\n+        if err := json.Unmarshal(data, &aux); err != nil {\n+                return err\n+        }\n+        if len(aux.Any) > 0 {\n+                c.Expression = Any(aux.Any)\n+        } else if len(aux.All) > 0 {\n+                c.Expression = All(aux.All)\n+        } else {\n+                return errors.New(\"invalid criteria json. missing rules (key 'all' or 'any')\")\n+        }\n+        c.Sort = aux.Sort\n+        c.Order = aux.Order\n+        c.Limit = aux.Limit\n+        c.Offset = aux.Offset\n+        return nil\n }\n"}
{"cve":"CVE-2019-19499:0708", "fix_patch": "diff --git a/pkg/tsdb/mysql/mysql.go b/pkg/tsdb/mysql/mysql.go\nindex fe41a9f38b5..29406afc952 100644\n--- a/pkg/tsdb/mysql/mysql.go\n+++ b/pkg/tsdb/mysql/mysql.go\n@@ -1,148 +1,170 @@\n package mysql\n \n import (\n-\t\"database/sql\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"reflect\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\t\"github.com/VividCortex/mysqlerr\"\n-\n-\t\"github.com/grafana/grafana/pkg/setting\"\n-\n-\t\"github.com/go-sql-driver/mysql\"\n-\t\"github.com/go-xorm/core\"\n-\t\"github.com/grafana/grafana/pkg/infra/log\"\n-\t\"github.com/grafana/grafana/pkg/models\"\n-\t\"github.com/grafana/grafana/pkg/tsdb\"\n-\t\"github.com/grafana/grafana/pkg/tsdb/sqleng\"\n+        \"database/sql\"\n+        \"errors\"\n+        \"fmt\"\n+        \"reflect\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        \"github.com/VividCortex/mysqlerr\"\n+\n+        \"github.com/grafana/grafana/pkg/setting\"\n+\n+        \"github.com/go-sql-driver/mysql\"\n+        \"github.com/go-xorm/core\"\n+        \"github.com/grafana/grafana/pkg/infra/log\"\n+        \"github.com/grafana/grafana/pkg/models\"\n+        \"github.com/grafana/grafana/pkg/tsdb\"\n+        \"github.com/grafana/grafana/pkg/tsdb/sqleng\"\n )\n \n func init() {\n-\ttsdb.RegisterTsdbQueryEndpoint(\"mysql\", newMysqlQueryEndpoint)\n+        tsdb.RegisterTsdbQueryEndpoint(\"mysql\", newMysqlQueryEndpoint)\n }\n \n func newMysqlQueryEndpoint(datasource *models.DataSource) (tsdb.TsdbQueryEndpoint, error) {\n-\tlogger := log.New(\"tsdb.mysql\")\n-\n-\tprotocol := \"tcp\"\n-\tif strings.HasPrefix(datasource.Url, \"/\") {\n-\t\tprotocol = \"unix\"\n-\t}\n-\tcnnstr := fmt.Sprintf(\"%s:%s@%s(%s)/%s?collation=utf8mb4_unicode_ci&parseTime=true&loc=UTC&allowNativePasswords=true\",\n-\t\tdatasource.User,\n-\t\tdatasource.DecryptedPassword(),\n-\t\tprotocol,\n-\t\tdatasource.Url,\n-\t\tdatasource.Database,\n-\t)\n-\n-\ttlsConfig, err := datasource.GetTLSConfig()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif tlsConfig.RootCAs != nil || len(tlsConfig.Certificates) > 0 {\n-\t\ttlsConfigString := fmt.Sprintf(\"ds%d\", datasource.Id)\n-\t\tif err := mysql.RegisterTLSConfig(tlsConfigString, tlsConfig); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tcnnstr += \"&tls=\" + tlsConfigString\n-\t}\n-\n-\tif setting.Env == setting.DEV {\n-\t\tlogger.Debug(\"getEngine\", \"connection\", cnnstr)\n-\t}\n-\n-\tconfig := sqleng.SqlQueryEndpointConfiguration{\n-\t\tDriverName:        \"mysql\",\n-\t\tConnectionString:  cnnstr,\n-\t\tDatasource:        datasource,\n-\t\tTimeColumnNames:   []string{\"time\", \"time_sec\"},\n-\t\tMetricColumnTypes: []string{\"CHAR\", \"VARCHAR\", \"TINYTEXT\", \"TEXT\", \"MEDIUMTEXT\", \"LONGTEXT\"},\n-\t}\n-\n-\trowTransformer := mysqlQueryResultTransformer{\n-\t\tlog: logger,\n-\t}\n-\n-\treturn sqleng.NewSqlQueryEndpoint(&config, &rowTransformer, newMysqlMacroEngine(logger), logger)\n+        logger := log.New(\"tsdb.mysql\")\n+\n+        protocol := \"tcp\"\n+        if strings.HasPrefix(datasource.Url, \"/\") {\n+                protocol = \"unix\"\n+        }\n+config, err := mysql.ParseDSN(datasource.Url)\n+if err == nil {\n+If the environment variable is not set, default to the user and password from the datasource\n+config.User == \"\" {\n+fig.User = datasource.User\n+config.Passwd == \"\" {\n+fig.Passwd = datasource.DecryptedPassword()\n+\n+if the database is not set, default to the one from the datasource\n+config.DBName == \"\" {\n+fig.DBName = datasource.Database\n+else {\n+fig = mysql.NewConfig()\n+fig.User = datasource.User\n+fig.Passwd = datasource.DecryptedPassword()\n+fig.DBName = datasource.Database\n+fig.Net = protocol\n+fig.Addr = datasource.Url\n+}\n+\n+config.AllowNativePasswords = true\n+\n+if config.Params == nil {\n+fig.Params = make(map[string]string)\n+}\n+config.Params[\"collation\"] = \"utf8mb4_unicode_ci\"\n+config.Params[\"parseTime\"] = \"true\"\n+config.Params[\"loc\"] = \"UTC\"\n+\n+tlsConfig, err := datasource.GetTLSConfig()\n+if err != nil {\n+ nil, err\n+}\n+\n+if tlsConfig.RootCAs != nil || len(tlsConfig.Certificates) > 0 {\n+figString := fmt.Sprintf(\"ds%d\", datasource.Id)\n+err := mysql.RegisterTLSConfig(tlsConfigString, tlsConfig); err != nil {\n+ nil, err\n+fig.TLS = &tls.Config{ServerName: tlsConfigString}\n+}\n+\n+cnnstr := config.FormatDSN()\n+\n+        if setting.Env == setting.DEV {\n+                logger.Debug(\"getEngine\", \"connection\", cnnstr)\n+        }\n+\n+        config := sqleng.SqlQueryEndpointConfiguration{\n+                DriverName:        \"mysql\",\n+                ConnectionString:  cnnstr,\n+                Datasource:        datasource,\n+                TimeColumnNames:   []string{\"time\", \"time_sec\"},\n+                MetricColumnTypes: []string{\"CHAR\", \"VARCHAR\", \"TINYTEXT\", \"TEXT\", \"MEDIUMTEXT\", \"LONGTEXT\"},\n+        }\n+\n+        rowTransformer := mysqlQueryResultTransformer{\n+                log: logger,\n+        }\n+\n+        return sqleng.NewSqlQueryEndpoint(&config, &rowTransformer, newMysqlMacroEngine(logger), logger)\n }\n \n type mysqlQueryResultTransformer struct {\n-\tlog log.Logger\n+        log log.Logger\n }\n \n func (t *mysqlQueryResultTransformer) TransformQueryResult(columnTypes []*sql.ColumnType, rows *core.Rows) (tsdb.RowValues, error) {\n-\tvalues := make([]interface{}, len(columnTypes))\n-\n-\tfor i := range values {\n-\t\tscanType := columnTypes[i].ScanType()\n-\t\tvalues[i] = reflect.New(scanType).Interface()\n-\n-\t\tif columnTypes[i].DatabaseTypeName() == \"BIT\" {\n-\t\t\tvalues[i] = new([]byte)\n-\t\t}\n-\t}\n-\n-\tif err := rows.Scan(values...); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor i := 0; i < len(columnTypes); i++ {\n-\t\ttypeName := reflect.ValueOf(values[i]).Type().String()\n-\n-\t\tswitch typeName {\n-\t\tcase \"*sql.RawBytes\":\n-\t\t\tvalues[i] = string(*values[i].(*sql.RawBytes))\n-\t\tcase \"*mysql.NullTime\":\n-\t\t\tsqlTime := (*values[i].(*mysql.NullTime))\n-\t\t\tif sqlTime.Valid {\n-\t\t\t\tvalues[i] = sqlTime.Time\n-\t\t\t} else {\n-\t\t\t\tvalues[i] = nil\n-\t\t\t}\n-\t\tcase \"*sql.NullInt64\":\n-\t\t\tnullInt64 := (*values[i].(*sql.NullInt64))\n-\t\t\tif nullInt64.Valid {\n-\t\t\t\tvalues[i] = nullInt64.Int64\n-\t\t\t} else {\n-\t\t\t\tvalues[i] = nil\n-\t\t\t}\n-\t\tcase \"*sql.NullFloat64\":\n-\t\t\tnullFloat64 := (*values[i].(*sql.NullFloat64))\n-\t\t\tif nullFloat64.Valid {\n-\t\t\t\tvalues[i] = nullFloat64.Float64\n-\t\t\t} else {\n-\t\t\t\tvalues[i] = nil\n-\t\t\t}\n-\t\t}\n-\n-\t\tif columnTypes[i].DatabaseTypeName() == \"DECIMAL\" {\n-\t\t\tf, err := strconv.ParseFloat(values[i].(string), 64)\n-\n-\t\t\tif err == nil {\n-\t\t\t\tvalues[i] = f\n-\t\t\t} else {\n-\t\t\t\tvalues[i] = nil\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn values, nil\n+        values := make([]interface{}, len(columnTypes))\n+\n+        for i := range values {\n+                scanType := columnTypes[i].ScanType()\n+                values[i] = reflect.New(scanType).Interface()\n+\n+                if columnTypes[i].DatabaseTypeName() == \"BIT\" {\n+                        values[i] = new([]byte)\n+                }\n+        }\n+\n+        if err := rows.Scan(values...); err != nil {\n+                return nil, err\n+        }\n+\n+        for i := 0; i < len(columnTypes); i++ {\n+                typeName := reflect.ValueOf(values[i]).Type().String()\n+\n+                switch typeName {\n+                case \"*sql.RawBytes\":\n+                        values[i] = string(*values[i].(*sql.RawBytes))\n+                case \"*mysql.NullTime\":\n+                        sqlTime := (*values[i].(*mysql.NullTime))\n+                        if sqlTime.Valid {\n+                                values[i] = sqlTime.Time\n+                        } else {\n+                                values[i] = nil\n+                        }\n+                case \"*sql.NullInt64\":\n+                        nullInt64 := (*values[i].(*sql.NullInt64))\n+                        if nullInt64.Valid {\n+                                values[i] = nullInt64.Int64\n+                        } else {\n+                                values[i] = nil\n+                        }\n+                case \"*sql.NullFloat64\":\n+                        nullFloat64 := (*values[i].(*sql.NullFloat64))\n+                        if nullFloat64.Valid {\n+                                values[i] = nullFloat64.Float64\n+                        } else {\n+                                values[i] = nil\n+                        }\n+                }\n+\n+                if columnTypes[i].DatabaseTypeName() == \"DECIMAL\" {\n+                        f, err := strconv.ParseFloat(values[i].(string), 64)\n+\n+                        if err == nil {\n+                                values[i] = f\n+                        } else {\n+                                values[i] = nil\n+                        }\n+                }\n+        }\n+\n+        return values, nil\n }\n \n func (t *mysqlQueryResultTransformer) TransformQueryError(err error) error {\n-\tif driverErr, ok := err.(*mysql.MySQLError); ok {\n-\t\tif driverErr.Number != mysqlerr.ER_PARSE_ERROR && driverErr.Number != mysqlerr.ER_BAD_FIELD_ERROR && driverErr.Number != mysqlerr.ER_NO_SUCH_TABLE {\n-\t\t\tt.log.Error(\"query error\", \"err\", err)\n-\t\t\treturn errQueryFailed\n-\t\t}\n-\t}\n-\n-\treturn err\n+        if driverErr, ok := err.(*mysql.MySQLError); ok {\n+                if driverErr.Number != mysqlerr.ER_PARSE_ERROR && driverErr.Number != mysqlerr.ER_BAD_FIELD_ERROR && driverErr.Number != mysqlerr.ER_NO_SUCH_TABLE {\n+                        t.log.Error(\"query error\", \"err\", err)\n+                        return errQueryFailed\n+                }\n+        }\n+\n+        return err\n }\n \n var errQueryFailed = errors.New(\"Query failed. Please inspect Grafana server log for details\")\n"}
{"cve":"CVE-2022-1883:0708", "fix_patch": "diff --git a/db/db.go b/db/db.go\nindex 88e5390..fccf22f 100644\n--- a/db/db.go\n+++ b/db/db.go\n@@ -1,432 +1,434 @@\n package db\n \n import (\n-\t\"database/sql\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"net/url\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\n-\t\"github.com/camptocamp/terraboard/config\"\n-\t\"github.com/camptocamp/terraboard/internal/terraform/addrs\"\n-\t\"github.com/camptocamp/terraboard/internal/terraform/states\"\n-\t\"github.com/camptocamp/terraboard/internal/terraform/states/statefile\"\n-\t\"github.com/camptocamp/terraboard/state\"\n-\t\"github.com/camptocamp/terraboard/types\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\n-\tctyJson \"github.com/zclconf/go-cty/cty/json\"\n-\t\"gorm.io/driver/postgres\"\n-\t\"gorm.io/gorm\"\n-\t\"gorm.io/gorm/logger\"\n+        \"database/sql\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"net/url\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+\n+        \"github.com/camptocamp/terraboard/config\"\n+        \"github.com/camptocamp/terraboard/internal/terraform/addrs\"\n+        \"github.com/camptocamp/terraboard/internal/terraform/states\"\n+        \"github.com/camptocamp/terraboard/internal/terraform/states/statefile\"\n+        \"github.com/camptocamp/terraboard/state\"\n+        \"github.com/camptocamp/terraboard/types\"\n+        log \"github.com/sirupsen/logrus\"\n+\n+        ctyJson \"github.com/zclconf/go-cty/cty/json\"\n+        \"gorm.io/driver/postgres\"\n+        \"gorm.io/gorm\"\n+        \"gorm.io/gorm/logger\"\n )\n \n // Database is a wrapping structure to *gorm.DB\n type Database struct {\n-\t*gorm.DB\n-\tlock sync.Mutex\n+        *gorm.DB\n+        lock sync.Mutex\n }\n \n var pageSize = 20\n \n // Init setups up the Database and a pointer to it\n func Init(config config.DBConfig, debug bool) *Database {\n-\tvar err error\n-\tconnString := fmt.Sprintf(\n-\t\t\"host=%s port=%d user=%s dbname=%s sslmode=%s password=%s\",\n-\t\tconfig.Host,\n-\t\tconfig.Port,\n-\t\tconfig.User,\n-\t\tconfig.Name,\n-\t\tconfig.SSLMode,\n-\t\tconfig.Password,\n-\t)\n-\tdb, err := gorm.Open(postgres.Open(connString), &gorm.Config{\n-\t\tLogger: &LogrusGormLogger,\n-\t})\n-\tif err != nil {\n-\t\tlog.Fatal(err)\n-\t}\n-\n-\tlog.Infof(\"Automigrate\")\n-\terr = db.AutoMigrate(\n-\t\t&types.Lineage{},\n-\t\t&types.Version{},\n-\t\t&types.State{},\n-\t\t&types.Module{},\n-\t\t&types.Resource{},\n-\t\t&types.Attribute{},\n-\t\t&types.OutputValue{},\n-\t\t&types.Plan{},\n-\t\t&types.PlanModel{},\n-\t\t&types.PlanModelVariable{},\n-\t\t&types.PlanOutput{},\n-\t\t&types.PlanResourceChange{},\n-\t\t&types.PlanState{},\n-\t\t&types.PlanStateModule{},\n-\t\t&types.PlanStateOutput{},\n-\t\t&types.PlanStateResource{},\n-\t\t&types.PlanStateResourceAttribute{},\n-\t\t&types.PlanStateValue{},\n-\t\t&types.Change{},\n-\t)\n-\tif err != nil {\n-\t\tlog.Fatalf(\"Migration failed: %v\\n\", err)\n-\t}\n-\n-\tif debug {\n-\t\tdb.Config.Logger.LogMode(logger.Info)\n-\t}\n-\n-\td := &Database{DB: db}\n-\tif err = d.MigrateLineage(); err != nil {\n-\t\tlog.Fatalf(\"Lineage migration failed: %v\\n\", err)\n-\t}\n-\n-\treturn d\n+        var err error\n+        connString := fmt.Sprintf(\n+                \"host=%s port=%d user=%s dbname=%s sslmode=%s password=%s\",\n+                config.Host,\n+                config.Port,\n+                config.User,\n+                config.Name,\n+                config.SSLMode,\n+                config.Password,\n+        )\n+        db, err := gorm.Open(postgres.Open(connString), &gorm.Config{\n+                Logger: &LogrusGormLogger,\n+        })\n+        if err != nil {\n+                log.Fatal(err)\n+        }\n+\n+        log.Infof(\"Automigrate\")\n+        err = db.AutoMigrate(\n+                &types.Lineage{},\n+                &types.Version{},\n+                &types.State{},\n+                &types.Module{},\n+                &types.Resource{},\n+                &types.Attribute{},\n+                &types.OutputValue{},\n+                &types.Plan{},\n+                &types.PlanModel{},\n+                &types.PlanModelVariable{},\n+                &types.PlanOutput{},\n+                &types.PlanResourceChange{},\n+                &types.PlanState{},\n+                &types.PlanStateModule{},\n+                &types.PlanStateOutput{},\n+                &types.PlanStateResource{},\n+                &types.PlanStateResourceAttribute{},\n+                &types.PlanStateValue{},\n+                &types.Change{},\n+        )\n+        if err != nil {\n+                log.Fatalf(\"Migration failed: %v\\n\", err)\n+        }\n+\n+        if debug {\n+                db.Config.Logger.LogMode(logger.Info)\n+        }\n+\n+        d := &Database{DB: db}\n+        if err = d.MigrateLineage(); err != nil {\n+                log.Fatalf(\"Lineage migration failed: %v\\n\", err)\n+        }\n+\n+        return d\n }\n \n // MigrateLineage is a migration function to update db and its data to the\n // new lineage db scheme. It will update State table data, delete \"lineage\" column\n // and add corresponding Lineage entries\n func (db *Database) MigrateLineage() error {\n-\tif db.Migrator().HasColumn(&types.State{}, \"lineage\") {\n-\t\tvar states []types.State\n-\t\tif err := db.Find(&states).Error; err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tfor _, st := range states {\n-\t\t\tif err := db.UpdateState(st); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"Failed to update %s state during lineage migration: %v\", st.Path, err)\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Custom migration rules\n-\t\tif err := db.Migrator().DropColumn(&types.State{}, \"lineage\"); err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to drop lineage column during migration: %v\", err)\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        if db.Migrator().HasColumn(&types.State{}, \"lineage\") {\n+                var states []types.State\n+                if err := db.Find(&states).Error; err != nil {\n+                        return err\n+                }\n+\n+                for _, st := range states {\n+                        if err := db.UpdateState(st); err != nil {\n+                                return fmt.Errorf(\"Failed to update %s state during lineage migration: %v\", st.Path, err)\n+                        }\n+                }\n+\n+                // Custom migration rules\n+                if err := db.Migrator().DropColumn(&types.State{}, \"lineage\"); err != nil {\n+                        return fmt.Errorf(\"Failed to drop lineage column during migration: %v\", err)\n+                }\n+        }\n+\n+        return nil\n }\n \n type attributeValues map[string]interface{}\n \n func (db *Database) stateS3toDB(sf *statefile.File, path string, versionID string) (st types.State, err error) {\n-\tvar version types.Version\n-\tdb.First(&version, types.Version{VersionID: versionID})\n-\n-\t// Check if the associated lineage is already present in lineages table\n-\t// If so, it recovers its ID otherwise it inserts it at the same time as the state\n-\tvar lineage types.Lineage\n-\tdb.lock.Lock()\n-\terr = db.FirstOrCreate(&lineage, types.Lineage{Value: sf.Lineage}).Error\n-\tif err != nil || lineage.ID == 0 {\n-\t\tlog.WithField(\"error\", err).\n-\t\t\tError(\"Unknown error in stateS3toDB during lineage finding\")\n-\t\treturn types.State{}, err\n-\t}\n-\tdb.lock.Unlock()\n-\n-\tst = types.State{\n-\t\tPath:      path,\n-\t\tVersion:   version,\n-\t\tTFVersion: sf.TerraformVersion.String(),\n-\t\tSerial:    int64(sf.Serial),\n-\t\tLineageID: sql.NullInt64{Int64: int64(lineage.ID), Valid: true},\n-\t}\n-\n-\tfor _, m := range sf.State.Modules {\n-\t\tmod := types.Module{\n-\t\t\tPath: m.Addr.String(),\n-\t\t}\n-\t\tfor _, r := range m.Resources {\n-\t\t\tfor index, i := range r.Instances {\n-\t\t\t\tres := types.Resource{\n-\t\t\t\t\tType:       r.Addr.Resource.Type,\n-\t\t\t\t\tName:       r.Addr.Resource.Name,\n-\t\t\t\t\tIndex:      getResourceIndex(index),\n-\t\t\t\t\tAttributes: marshalAttributeValues(i.Current),\n-\t\t\t\t}\n-\t\t\t\tmod.Resources = append(mod.Resources, res)\n-\t\t\t}\n-\t\t}\n-\n-\t\tfor n, r := range m.OutputValues {\n-\t\t\tjsonVal, err := ctyJson.Marshal(r.Value, r.Value.Type())\n-\t\t\tif err != nil {\n-\t\t\t\tlog.WithError(err).Errorf(\"failed to load output for %s\", r.Addr.String())\n-\t\t\t}\n-\t\t\tout := types.OutputValue{\n-\t\t\t\tSensitive: r.Sensitive,\n-\t\t\t\tName:      n,\n-\t\t\t\tValue:     string(jsonVal),\n-\t\t\t}\n-\n-\t\t\tmod.OutputValues = append(mod.OutputValues, out)\n-\t\t}\n-\n-\t\tst.Modules = append(st.Modules, mod)\n-\t}\n-\treturn\n+        var version types.Version\n+        db.First(&version, types.Version{VersionID: versionID})\n+\n+        // Check if the associated lineage is already present in lineages table\n+        // If so, it recovers its ID otherwise it inserts it at the same time as the state\n+        var lineage types.Lineage\n+        db.lock.Lock()\n+        err = db.FirstOrCreate(&lineage, types.Lineage{Value: sf.Lineage}).Error\n+        if err != nil || lineage.ID == 0 {\n+                log.WithField(\"error\", err).\n+                        Error(\"Unknown error in stateS3toDB during lineage finding\")\n+                return types.State{}, err\n+        }\n+        db.lock.Unlock()\n+\n+        st = types.State{\n+                Path:      path,\n+                Version:   version,\n+                TFVersion: sf.TerraformVersion.String(),\n+                Serial:    int64(sf.Serial),\n+                LineageID: sql.NullInt64{Int64: int64(lineage.ID), Valid: true},\n+        }\n+\n+        for _, m := range sf.State.Modules {\n+                mod := types.Module{\n+                        Path: m.Addr.String(),\n+                }\n+                for _, r := range m.Resources {\n+                        for index, i := range r.Instances {\n+                                res := types.Resource{\n+                                        Type:       r.Addr.Resource.Type,\n+                                        Name:       r.Addr.Resource.Name,\n+                                        Index:      getResourceIndex(index),\n+                                        Attributes: marshalAttributeValues(i.Current),\n+                                }\n+                                mod.Resources = append(mod.Resources, res)\n+                        }\n+                }\n+\n+                for n, r := range m.OutputValues {\n+                        jsonVal, err := ctyJson.Marshal(r.Value, r.Value.Type())\n+                        if err != nil {\n+                                log.WithError(err).Errorf(\"failed to load output for %s\", r.Addr.String())\n+                        }\n+                        out := types.OutputValue{\n+                                Sensitive: r.Sensitive,\n+                                Name:      n,\n+                                Value:     string(jsonVal),\n+                        }\n+\n+                        mod.OutputValues = append(mod.OutputValues, out)\n+                }\n+\n+                st.Modules = append(st.Modules, mod)\n+        }\n+        return\n }\n \n // getResourceIndex transforms an addrs.InstanceKey instance into a string representation\n func getResourceIndex(index addrs.InstanceKey) string {\n-\tswitch index.(type) {\n-\tcase addrs.IntKey, addrs.StringKey:\n-\t\treturn index.String()\n-\t}\n-\treturn \"\"\n+        switch index.(type) {\n+        case addrs.IntKey, addrs.StringKey:\n+                return index.String()\n+        }\n+        return \"\"\n }\n \n func marshalAttributeValues(src *states.ResourceInstanceObjectSrc) (attrs []types.Attribute) {\n-\tvals := make(attributeValues)\n-\tif src == nil {\n-\t\treturn\n-\t}\n-\tif src.AttrsFlat != nil {\n-\t\tfor k, v := range src.AttrsFlat {\n-\t\t\tvals[k] = v\n-\t\t}\n-\t} else if err := json.Unmarshal(src.AttrsJSON, &vals); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\tlog.Debug(vals)\n-\n-\tfor k, v := range vals {\n-\t\tvJSON, _ := json.Marshal(v)\n-\t\tattr := types.Attribute{\n-\t\t\tKey:   k,\n-\t\t\tValue: string(vJSON),\n-\t\t}\n-\t\tlog.Debug(attrs)\n-\t\tattrs = append(attrs, attr)\n-\t}\n-\treturn attrs\n+        vals := make(attributeValues)\n+        if src == nil {\n+                return\n+        }\n+        if src.AttrsFlat != nil {\n+                for k, v := range src.AttrsFlat {\n+                        vals[k] = v\n+                }\n+        } else if err := json.Unmarshal(src.AttrsJSON, &vals); err != nil {\n+                log.Error(err.Error())\n+        }\n+        log.Debug(vals)\n+\n+        for k, v := range vals {\n+                vJSON, _ := json.Marshal(v)\n+                attr := types.Attribute{\n+                        Key:   k,\n+                        Value: string(vJSON),\n+                }\n+                log.Debug(attrs)\n+                attrs = append(attrs, attr)\n+        }\n+        return attrs\n }\n \n // InsertState inserts a Terraform State in the Database\n func (db *Database) InsertState(path string, versionID string, sf *statefile.File) error {\n-\tst, err := db.stateS3toDB(sf, path, versionID)\n-\tif err == nil {\n-\t\tdb.Create(&st)\n-\t}\n-\treturn nil\n+        st, err := db.stateS3toDB(sf, path, versionID)\n+        if err == nil {\n+                db.Create(&st)\n+        }\n+        return nil\n }\n \n // UpdateState update a Terraform State in the Database with Lineage foreign constraint\n // It will also insert Lineage entry in the db if needed.\n // This method is only use during the Lineage migration since States are immutable\n func (db *Database) UpdateState(st types.State) error {\n-\t// Get lineage from old column\n-\tvar lineageValue sql.NullString\n-\tif err := db.Raw(\"SELECT lineage FROM states WHERE id = ?\", st.ID).Scan(&lineageValue).Error; err != nil {\n-\t\treturn fmt.Errorf(\"Error on %s lineage recovering during migration: %v\", st.Path, err)\n-\t}\n-\tif lineageValue.String == \"\" || !lineageValue.Valid {\n-\t\tlog.Warnf(\"Missing lineage for '%s' state, attempt to recover lineage from other states...\", st.Path)\n-\t\tvar lineages []string\n-\t\tdb.Table(\"states\").\n-\t\t\tDistinct(\"lineage\").\n-\t\t\tOrder(\"lineage desc\").\n-\t\t\tWhere(\"path = ?\", st.Path).\n-\t\t\tScan(&lineages)\n-\n-\t\tfor _, l := range lineages {\n-\t\t\tif l != \"\" {\n-\t\t\t\tlineageValue.String = l\n-\t\t\t\tlineageValue.Valid = true\n-\t\t\t\tlog.Infof(\"Missing lineage for '%s' state solved!\", st.Path)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\n-\t\tif lineageValue.String == \"\" || !lineageValue.Valid {\n-\t\t\tlog.Warnf(\"Failed to recover '%s' lineage from others states. Orphan state\", st.Path)\n-\t\t\treturn nil\n-\t\t}\n-\t}\n-\n-\t// Create Lineage entry if not exist (value column is unique)\n-\tlineage := types.Lineage{\n-\t\tValue: lineageValue.String,\n-\t}\n-\ttx := db.FirstOrCreate(&lineage, lineage)\n-\tif tx.Error != nil || lineage.ID == 0 {\n-\t\treturn tx.Error\n-\t}\n-\n-\t// Get Lineage ID for foreign constraint\n-\tst.LineageID = sql.NullInt64{Int64: int64(lineage.ID), Valid: true}\n-\n-\treturn db.Save(&st).Error\n+        // Get lineage from old column\n+        var lineageValue sql.NullString\n+        if err := db.Raw(\"SELECT lineage FROM states WHERE id = ?\", st.ID).Scan(&lineageValue).Error; err != nil {\n+                return fmt.Errorf(\"Error on %s lineage recovering during migration: %v\", st.Path, err)\n+        }\n+        if lineageValue.String == \"\" || !lineageValue.Valid {\n+                log.Warnf(\"Missing lineage for '%s' state, attempt to recover lineage from other states...\", st.Path)\n+                var lineages []string\n+                db.Table(\"states\").\n+                        Distinct(\"lineage\").\n+                        Order(\"lineage desc\").\n+                        Where(\"path = ?\", st.Path).\n+                        Scan(&lineages)\n+\n+                for _, l := range lineages {\n+                        if l != \"\" {\n+                                lineageValue.String = l\n+                                lineageValue.Valid = true\n+                                log.Infof(\"Missing lineage for '%s' state solved!\", st.Path)\n+                                break\n+                        }\n+                }\n+\n+                if lineageValue.String == \"\" || !lineageValue.Valid {\n+                        log.Warnf(\"Failed to recover '%s' lineage from others states. Orphan state\", st.Path)\n+                        return nil\n+                }\n+        }\n+\n+        // Create Lineage entry if not exist (value column is unique)\n+        lineage := types.Lineage{\n+                Value: lineageValue.String,\n+        }\n+        tx := db.FirstOrCreate(&lineage, lineage)\n+        if tx.Error != nil || lineage.ID == 0 {\n+                return tx.Error\n+        }\n+\n+        // Get Lineage ID for foreign constraint\n+        st.LineageID = sql.NullInt64{Int64: int64(lineage.ID), Valid: true}\n+\n+        return db.Save(&st).Error\n }\n \n // InsertVersion inserts an AWS S3 Version in the Database\n func (db *Database) InsertVersion(version *state.Version) error {\n-\tvar v types.Version\n-\tdb.lock.Lock()\n-\tdb.FirstOrCreate(&v, types.Version{\n-\t\tVersionID:    version.ID,\n-\t\tLastModified: version.LastModified,\n-\t})\n-\tdb.lock.Unlock()\n-\treturn nil\n+        var v types.Version\n+        db.lock.Lock()\n+        db.FirstOrCreate(&v, types.Version{\n+                VersionID:    version.ID,\n+                LastModified: version.LastModified,\n+        })\n+        db.lock.Unlock()\n+        return nil\n }\n \n // GetState retrieves a State from the database by its path and versionID\n func (db *Database) GetState(lineage, versionID string) (state types.State) {\n-\tdb.Joins(\"JOIN lineages on states.lineage_id=lineages.id\").\n-\t\tJoins(\"JOIN versions on states.version_id=versions.id\").\n-\t\tPreload(\"Version\").Preload(\"Modules\").Preload(\"Modules.Resources\").Preload(\"Modules.Resources.Attributes\").\n-\t\tPreload(\"Modules.OutputValues\").\n-\t\tFind(&state, \"lineages.value = ? AND versions.version_id = ?\", lineage, versionID)\n-\treturn\n+        db.Joins(\"JOIN lineages on states.lineage_id=lineages.id\").\n+                Joins(\"JOIN versions on states.version_id=versions.id\").\n+                Preload(\"Version\").Preload(\"Modules\").Preload(\"Modules.Resources\").Preload(\"Modules.Resources.Attributes\").\n+                Preload(\"Modules.OutputValues\").\n+                Find(&state, \"lineages.value = ? AND versions.version_id = ?\", lineage, versionID)\n+        return\n }\n \n // GetLineageActivity returns a slice of StateStat from the Database\n // for a given lineage representing the State activity over time (Versions)\n func (db *Database) GetLineageActivity(lineage string) (states []types.StateStat) {\n-\tsql := \"SELECT t.path, t.serial, t.tf_version, t.version_id, t.last_modified, count(resources.*) as resource_count\" +\n-\t\t\" FROM (SELECT states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified FROM states JOIN lineages ON lineages.id = states.lineage_id JOIN versions ON versions.id = states.version_id WHERE lineages.value = ? ORDER BY states.path, versions.last_modified ASC) t\" +\n-\t\t\" JOIN modules ON modules.state_id = t.id\" +\n-\t\t\" JOIN resources ON resources.module_id = modules.id\" +\n-\t\t\" GROUP BY t.path, t.serial, t.tf_version, t.version_id, t.last_modified\" +\n-\t\t\" ORDER BY last_modified ASC\"\n-\n-\tdb.Raw(sql, lineage).Find(&states)\n-\treturn\n+        sql := \"SELECT t.path, t.serial, t.tf_version, t.version_id, t.last_modified, count(resources.*) as resource_count\" +\n+                \" FROM (SELECT states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified FROM states JOIN lineages ON lineages.id = states.lineage_id JOIN versions ON versions.id = states.version_id WHERE lineages.value = ? ORDER BY states.path, versions.last_modified ASC) t\" +\n+                \" JOIN modules ON modules.state_id = t.id\" +\n+                \" JOIN resources ON resources.module_id = modules.id\" +\n+                \" GROUP BY t.path, t.serial, t.tf_version, t.version_id, t.last_modified\" +\n+                \" ORDER BY last_modified ASC\"\n+\n+        db.Raw(sql, lineage).Find(&states)\n+        return\n }\n \n // KnownVersions returns a slice of all known Versions in the Database\n func (db *Database) KnownVersions() (versions []string) {\n-\t// TODO: err\n-\trows, _ := db.Table(\"versions\").Select(\"DISTINCT version_id\").Rows()\n-\tdefer rows.Close()\n-\tfor rows.Next() {\n-\t\tvar version string\n-\t\tif err := rows.Scan(&version); err != nil {\n-\t\t\tlog.Error(err.Error())\n-\t\t}\n-\t\tversions = append(versions, version)\n-\t}\n-\treturn\n+        // TODO: err\n+        rows, _ := db.Table(\"versions\").Select(\"DISTINCT version_id\").Rows()\n+        defer rows.Close()\n+        for rows.Next() {\n+                var version string\n+                if err := rows.Scan(&version); err != nil {\n+                        log.Error(err.Error())\n+                }\n+                versions = append(versions, version)\n+        }\n+        return\n }\n \n // SearchAttribute returns a slice of SearchResult given a query\n // The query might contain parameters 'type', 'name', 'key', 'value' and 'tf_version'\n // SearchAttribute also returns paging information: the page number and the total results\n func (db *Database) SearchAttribute(query url.Values) (results []types.SearchResult, page int, total int) {\n-\tlog.WithFields(log.Fields{\n-\t\t\"query\": query,\n-\t}).Info(\"Searching for attribute with query\")\n-\n-\ttargetVersion := string(query.Get(\"versionid\"))\n-\n-\tsqlQuery := \"\"\n-\tif targetVersion == \"\" {\n-\t\tsqlQuery += \" FROM (SELECT states.path, max(states.serial) as mx FROM states GROUP BY states.path) t\" +\n-\t\t\t\" JOIN states ON t.path = states.path AND t.mx = states.serial\"\n-\t} else {\n-\t\tsqlQuery += \" FROM states\"\n-\t}\n-\n-\tsqlQuery += \" JOIN modules ON states.id = modules.state_id\" +\n-\t\t\" JOIN resources ON modules.id = resources.module_id\" +\n-\t\t\" JOIN attributes ON resources.id = attributes.resource_id\" +\n-\t\t\" JOIN lineages ON lineages.id = states.lineage_id\" +\n-\t\t\" JOIN versions ON states.version_id = versions.id\"\n-\n-\tvar where []string\n-\tvar params []interface{}\n-\tif targetVersion != \"\" && targetVersion != \"*\" {\n-\t\t// filter by version unless we want all (*) or most recent (\"\")\n-\t\twhere = append(where, \"states.version_id = ?\")\n-\t\tparams = append(params, targetVersion)\n-\t}\n-\n-\tif v := string(query.Get(\"type\")); v != \"\" {\n-\t\twhere = append(where, \"resources.type LIKE ?\")\n-\t\tparams = append(params, fmt.Sprintf(\"%%%s%%\", v))\n-\t}\n-\n-\tif v := string(query.Get(\"name\")); v != \"\" {\n-\t\twhere = append(where, \"resources.name LIKE ?\")\n-\t\tparams = append(params, fmt.Sprintf(\"%%%s%%\", v))\n-\t}\n-\n-\tif v := string(query.Get(\"key\")); v != \"\" {\n-\t\twhere = append(where, \"attributes.key LIKE ?\")\n-\t\tparams = append(params, fmt.Sprintf(\"%%%s%%\", v))\n-\t}\n-\n-\tif v := string(query.Get(\"value\")); v != \"\" {\n-\t\twhere = append(where, \"attributes.value LIKE ?\")\n-\t\tparams = append(params, fmt.Sprintf(\"%%%s%%\", v))\n-\t}\n-\n-\tif v := query.Get(\"tf_version\"); string(v) != \"\" {\n-\t\twhere = append(where, fmt.Sprintf(\"states.tf_version LIKE '%s'\", fmt.Sprintf(\"%%%s%%\", v)))\n-\t}\n-\n-\tif v := query.Get(\"lineage_value\"); string(v) != \"\" {\n-\t\twhere = append(where, fmt.Sprintf(\"lineages.value LIKE '%s'\", fmt.Sprintf(\"%%%s%%\", v)))\n-\t}\n-\n-\tif len(where) > 0 {\n-\t\tsqlQuery += \" WHERE \" + strings.Join(where, \" AND \")\n-\t}\n-\n-\t// Count everything\n-\trow := db.Raw(\"SELECT count(*)\"+sqlQuery, params...).Row()\n-\tif err := row.Scan(&total); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\n-\t// Now get results\n-\t// gorm doesn't support subqueries...\n-\tsql := \"SELECT states.path, versions.version_id, states.tf_version, states.serial, lineages.value as lineage_value, modules.path as module_path, resources.type, resources.name, resources.index, attributes.key, attributes.value\" +\n-\t\tsqlQuery +\n-\t\t\" ORDER BY states.path, states.serial, lineage_value, modules.path, resources.type, resources.name, resources.index, attributes.key\" +\n-\t\t\" LIMIT ?\"\n-\n-\tparams = append(params, pageSize)\n-\n-\tif v := string(query.Get(\"page\")); v != \"\" {\n-\t\tpage, _ = strconv.Atoi(v) // TODO: err\n-\t\to := (page - 1) * pageSize\n-\t\tsql += \" OFFSET ?\"\n-\t\tparams = append(params, o)\n-\t} else {\n-\t\tpage = 1\n-\t}\n-\n-\tdb.Raw(sql, params...).Find(&results)\n-\n-\treturn\n+        log.WithFields(log.Fields{\n+                \"query\": query,\n+        }).Info(\"Searching for attribute with query\")\n+\n+        targetVersion := string(query.Get(\"versionid\"))\n+\n+        sqlQuery := \"\"\n+        if targetVersion == \"\" {\n+                sqlQuery += \" FROM (SELECT states.path, max(states.serial) as mx FROM states GROUP BY states.path) t\" +\n+                        \" JOIN states ON t.path = states.path AND t.mx = states.serial\"\n+        } else {\n+                sqlQuery += \" FROM states\"\n+        }\n+\n+        sqlQuery += \" JOIN modules ON states.id = modules.state_id\" +\n+                \" JOIN resources ON modules.id = resources.module_id\" +\n+                \" JOIN attributes ON resources.id = attributes.resource_id\" +\n+                \" JOIN lineages ON lineages.id = states.lineage_id\" +\n+                \" JOIN versions ON states.version_id = versions.id\"\n+\n+        var where []string\n+        var params []interface{}\n+        if targetVersion != \"\" && targetVersion != \"*\" {\n+                // filter by version unless we want all (*) or most recent (\"\")\n+                where = append(where, \"states.version_id = ?\")\n+                params = append(params, targetVersion)\n+        }\n+\n+        if v := string(query.Get(\"type\")); v != \"\" {\n+                where = append(where, \"resources.type LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := string(query.Get(\"name\")); v != \"\" {\n+                where = append(where, \"resources.name LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := string(query.Get(\"key\")); v != \"\" {\n+                where = append(where, \"attributes.key LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := string(query.Get(\"value\")); v != \"\" {\n+                where = append(where, \"attributes.value LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := query.Get(\"tf_version\"); string(v) != \"\" {\n+                where = append(where, \"states.tf_version LIKE ?\")\n+params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := query.Get(\"lineage_value\"); string(v) != \"\" {\n+                where = append(where, \"lineages.value LIKE ?\")\n+params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if len(where) > 0 {\n+                sqlQuery += \" WHERE \" + strings.Join(where, \" AND \")\n+        }\n+\n+        // Count everything\n+        row := db.Raw(\"SELECT count(*)\"+sqlQuery, params...).Row()\n+        if err := row.Scan(&total); err != nil {\n+                log.Error(err.Error())\n+        }\n+\n+        // Now get results\n+        // gorm doesn't support subqueries...\n+        sql := \"SELECT states.path, versions.version_id, states.tf_version, states.serial, lineages.value as lineage_value, modules.path as module_path, resources.type, resources.name, resources.index, attributes.key, attributes.value\" +\n+                sqlQuery +\n+                \" ORDER BY states.path, states.serial, lineage_value, modules.path, resources.type, resources.name, resources.index, attributes.key\" +\n+                \" LIMIT ?\"\n+\n+        params = append(params, pageSize)\n+\n+        if v := string(query.Get(\"page\")); v != \"\" {\n+                page, _ = strconv.Atoi(v) // TODO: err\n+                o := (page - 1) * pageSize\n+                sql += \" OFFSET ?\"\n+                params = append(params, o)\n+        } else {\n+                page = 1\n+        }\n+\n+        db.Raw(sql, params...).Find(&results)\n+\n+        return\n }\n \n // ListStatesVersions returns a map of Version IDs to a slice of State paths\n // from the Database\n func (db *Database) ListStatesVersions() (statesVersions map[string][]string) {\n-\trows, _ := db.Table(\"states\").\n-\t\tJoins(\"JOIN versions ON versions.id = states.version_id\").\n-\t\tSelect(\"states.path, versions.version_id\").Rows()\n-\tdefer rows.Close()\n-\tstatesVersions = make(map[string][]string)\n-\tfor rows.Next() {\n-\t\tvar path string\n-\t\tvar versionID string\n-\t\tif err := rows.Scan(&path, &versionID); err != nil {\n-\t\t\tlog.Error(err.Error())\n-\t\t}\n-\t\tstatesVersions[versionID] = append(statesVersions[versionID], path)\n-\t}\n-\treturn\n+        rows, _ := db.Table(\"states\").\n+                Joins(\"JOIN versions ON versions.id = states.version_id\").\n+                Select(\"states.path, versions.version_id\").Rows()\n+        defer rows.Close()\n+        statesVersions = make(map[string][]string)\n+        for rows.Next() {\n+                var path string\n+                var versionID string\n+                if err := rows.Scan(&path, &versionID); err != nil {\n+                        log.Error(err.Error())\n+                }\n+                statesVersions[versionID] = append(statesVersions[versionID], path)\n+        }\n+        return\n }\n \n // ListTerraformVersionsWithCount returns a slice of maps of Terraform versions\n@@ -434,376 +436,376 @@ func (db *Database) ListStatesVersions() (statesVersions map[string][]string) {\n // ListTerraformVersionsWithCount also takes a query with possible parameter 'orderBy'\n // to sort results. Default sorting is by descending version number.\n func (db *Database) ListTerraformVersionsWithCount(query url.Values) (results []map[string]string, err error) {\n-\torderBy := string(query.Get(\"orderBy\"))\n-\tsql := \"SELECT t.tf_version, COUNT(*)\" +\n-\t\t\" FROM (SELECT DISTINCT ON(states.path) states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified\" +\n-\t\t\" FROM states JOIN versions ON versions.id = states.version_id ORDER BY states.path, versions.last_modified DESC) t\" +\n-\t\t\" GROUP BY t.tf_version ORDER BY \"\n-\n-\tif orderBy == \"version\" {\n-\t\tsql += \"string_to_array(t.tf_version, '.')::int[] DESC\"\n-\t} else {\n-\t\tsql += \"count DESC\"\n-\t}\n-\n-\trows, err := db.Raw(sql).Rows()\n-\tif err != nil {\n-\t\treturn results, err\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar name string\n-\t\tvar count string\n-\t\tr := make(map[string]string)\n-\t\tif err = rows.Scan(&name, &count); err != nil {\n-\t\t\treturn\n-\t\t}\n-\t\tr[\"name\"] = name\n-\t\tr[\"count\"] = count\n-\t\tresults = append(results, r)\n-\t}\n-\treturn\n+        orderBy := string(query.Get(\"orderBy\"))\n+        sql := \"SELECT t.tf_version, COUNT(*)\" +\n+                \" FROM (SELECT DISTINCT ON(states.path) states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified\" +\n+                \" FROM states JOIN versions ON versions.id = states.version_id ORDER BY states.path, versions.last_modified DESC) t\" +\n+                \" GROUP BY t.tf_version ORDER BY \"\n+\n+        if orderBy == \"version\" {\n+                sql += \"string_to_array(t.tf_version, '.')::int[] DESC\"\n+        } else {\n+                sql += \"count DESC\"\n+        }\n+\n+        rows, err := db.Raw(sql).Rows()\n+        if err != nil {\n+                return results, err\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var name string\n+                var count string\n+                r := make(map[string]string)\n+                if err = rows.Scan(&name, &count); err != nil {\n+                        return\n+                }\n+                r[\"name\"] = name\n+                r[\"count\"] = count\n+                results = append(results, r)\n+        }\n+        return\n }\n \n // ListStateStats returns a slice of StateStat, along with paging information\n func (db *Database) ListStateStats(query url.Values) (states []types.StateStat, page int, total int) {\n-\trow := db.Raw(\"SELECT count(*) FROM (SELECT DISTINCT lineage_id FROM states) AS t\").Row()\n-\tif err := row.Scan(&total); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\n-\tvar paginationQuery string\n-\tvar params []interface{}\n-\tpage = 1\n-\tif v := string(query.Get(\"page\")); v != \"\" {\n-\t\tpage, _ = strconv.Atoi(v) // TODO: err\n-\t\toffset := (page - 1) * pageSize\n-\t\tparams = append(params, offset)\n-\t\tpaginationQuery = \" LIMIT 20 OFFSET ?\"\n-\t} else {\n-\t\tpage = -1\n-\t}\n-\n-\tsql := \"SELECT t.path, lineages.value as lineage_value, t.serial, t.tf_version, t.version_id, t.last_modified, count(resources.*) as resource_count\" +\n-\t\t\" FROM (SELECT DISTINCT ON(states.lineage_id) states.id, states.lineage_id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified FROM states JOIN versions ON versions.id = states.version_id ORDER BY states.lineage_id, versions.last_modified DESC) t\" +\n-\t\t\" JOIN modules ON modules.state_id = t.id\" +\n-\t\t\" JOIN resources ON resources.module_id = modules.id\" +\n-\t\t\" JOIN lineages ON lineages.id = t.lineage_id\" +\n-\t\t\" GROUP BY t.path, lineages.value, t.serial, t.tf_version, t.version_id, t.last_modified\" +\n-\t\t\" ORDER BY last_modified DESC\" +\n-\t\tpaginationQuery\n-\n-\tdb.Raw(sql, params...).Find(&states)\n-\treturn\n+        row := db.Raw(\"SELECT count(*) FROM (SELECT DISTINCT lineage_id FROM states) AS t\").Row()\n+        if err := row.Scan(&total); err != nil {\n+                log.Error(err.Error())\n+        }\n+\n+        var paginationQuery string\n+        var params []interface{}\n+        page = 1\n+        if v := string(query.Get(\"page\")); v != \"\" {\n+                page, _ = strconv.Atoi(v) // TODO: err\n+                offset := (page - 1) * pageSize\n+                params = append(params, offset)\n+                paginationQuery = \" LIMIT 20 OFFSET ?\"\n+        } else {\n+                page = -1\n+        }\n+\n+        sql := \"SELECT t.path, lineages.value as lineage_value, t.serial, t.tf_version, t.version_id, t.last_modified, count(resources.*) as resource_count\" +\n+                \" FROM (SELECT DISTINCT ON(states.lineage_id) states.id, states.lineage_id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified FROM states JOIN versions ON versions.id = states.version_id ORDER BY states.lineage_id, versions.last_modified DESC) t\" +\n+                \" JOIN modules ON modules.state_id = t.id\" +\n+                \" JOIN resources ON resources.module_id = modules.id\" +\n+                \" JOIN lineages ON lineages.id = t.lineage_id\" +\n+                \" GROUP BY t.path, lineages.value, t.serial, t.tf_version, t.version_id, t.last_modified\" +\n+                \" ORDER BY last_modified DESC\" +\n+                paginationQuery\n+\n+        db.Raw(sql, params...).Find(&states)\n+        return\n }\n \n // listField is a wrapper utility method to list distinct values in Database tables.\n func (db *Database) listField(table, field string) (results []string, err error) {\n-\trows, err := db.Table(table).Select(fmt.Sprintf(\"DISTINCT %s\", field)).Rows()\n-\tif err != nil {\n-\t\treturn results, err\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar t string\n-\t\tif err = rows.Scan(&t); err != nil {\n-\t\t\treturn\n-\t\t}\n-\t\tresults = append(results, t)\n-\t}\n-\n-\treturn\n+        rows, err := db.Table(table).Select(fmt.Sprintf(\"DISTINCT %s\", field)).Rows()\n+        if err != nil {\n+                return results, err\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var t string\n+                if err = rows.Scan(&t); err != nil {\n+                        return\n+                }\n+                results = append(results, t)\n+        }\n+\n+        return\n }\n \n // ListResourceTypes lists all Resource types from the Database\n func (db *Database) ListResourceTypes() ([]string, error) {\n-\treturn db.listField(\"resources\", \"type\")\n+        return db.listField(\"resources\", \"type\")\n }\n \n // ListResourceTypesWithCount returns a list of Resource types with associated counts\n // from the Database\n func (db *Database) ListResourceTypesWithCount() (results []map[string]string, err error) {\n-\tsql := \"SELECT resources.type, COUNT(*)\" +\n-\t\t\" FROM (SELECT DISTINCT ON(states.path) states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified\" +\n-\t\t\" FROM states\" +\n-\t\t\" JOIN versions ON versions.id = states.version_id\" +\n-\t\t\" ORDER BY states.path, versions.last_modified DESC) t\" +\n-\t\t\" JOIN modules ON modules.state_id = t.id\" +\n-\t\t\" JOIN resources ON resources.module_id = modules.id\" +\n-\t\t\" GROUP BY resources.type\" +\n-\t\t\" ORDER BY count DESC\"\n-\n-\trows, err := db.Raw(sql).Rows()\n-\tif err != nil {\n-\t\treturn results, err\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar name string\n-\t\tvar count string\n-\t\tr := make(map[string]string)\n-\t\tif err = rows.Scan(&name, &count); err != nil {\n-\t\t\treturn\n-\t\t}\n-\t\tr[\"name\"] = name\n-\t\tr[\"count\"] = count\n-\t\tresults = append(results, r)\n-\t}\n-\treturn\n+        sql := \"SELECT resources.type, COUNT(*)\" +\n+                \" FROM (SELECT DISTINCT ON(states.path) states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified\" +\n+                \" FROM states\" +\n+                \" JOIN versions ON versions.id = states.version_id\" +\n+                \" ORDER BY states.path, versions.last_modified DESC) t\" +\n+                \" JOIN modules ON modules.state_id = t.id\" +\n+                \" JOIN resources ON resources.module_id = modules.id\" +\n+                \" GROUP BY resources.type\" +\n+                \" ORDER BY count DESC\"\n+\n+        rows, err := db.Raw(sql).Rows()\n+        if err != nil {\n+                return results, err\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var name string\n+                var count string\n+                r := make(map[string]string)\n+                if err = rows.Scan(&name, &count); err != nil {\n+                        return\n+                }\n+                r[\"name\"] = name\n+                r[\"count\"] = count\n+                results = append(results, r)\n+        }\n+        return\n }\n \n // ListResourceNames lists all Resource names from the Database\n func (db *Database) ListResourceNames() ([]string, error) {\n-\treturn db.listField(\"resources\", \"name\")\n+        return db.listField(\"resources\", \"name\")\n }\n \n // ListTfVersions lists all Terraform versions from the Database\n func (db *Database) ListTfVersions() ([]string, error) {\n-\treturn db.listField(\"states\", \"tf_version\")\n+        return db.listField(\"states\", \"tf_version\")\n }\n \n // ListAttributeKeys lists all Resource Attribute keys for a given Resource type\n // from the Database\n func (db *Database) ListAttributeKeys(resourceType string) (results []string, err error) {\n-\tquery := db.Table(\"attributes\").\n-\t\tSelect(\"DISTINCT key\").\n-\t\tJoins(\"JOIN resources ON attributes.resource_id = resources.id\")\n-\n-\tif resourceType != \"\" {\n-\t\tquery = query.Where(\"resources.type = ?\", resourceType)\n-\t}\n-\n-\trows, err := query.Rows()\n-\tif err != nil {\n-\t\treturn results, err\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar t string\n-\t\tif err = rows.Scan(&t); err != nil {\n-\t\t\treturn\n-\t\t}\n-\t\tresults = append(results, t)\n-\t}\n-\n-\treturn\n+        query := db.Table(\"attributes\").\n+                Select(\"DISTINCT key\").\n+                Joins(\"JOIN resources ON attributes.resource_id = resources.id\")\n+\n+        if resourceType != \"\" {\n+                query = query.Where(\"resources.type = ?\", resourceType)\n+        }\n+\n+        rows, err := query.Rows()\n+        if err != nil {\n+                return results, err\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var t string\n+                if err = rows.Scan(&t); err != nil {\n+                        return\n+                }\n+                results = append(results, t)\n+        }\n+\n+        return\n }\n \n // InsertPlan inserts a Terraform plan with associated information in the Database\n func (db *Database) InsertPlan(plan []byte) error {\n-\tvar lineage types.Lineage\n-\tif err := json.Unmarshal(plan, &lineage); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Recover lineage from db if it's already exists or insert it\n-\tres := db.FirstOrCreate(&lineage, lineage)\n-\tif res.Error != nil {\n-\t\treturn fmt.Errorf(\"Error on lineage retrival during plan insertion: %v\", res.Error)\n-\t}\n-\n-\tvar p types.Plan\n-\tif err := json.Unmarshal(plan, &p); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := json.Unmarshal(p.PlanJSON, &p.ParsedPlan); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tp.LineageID = lineage.ID\n-\treturn db.Create(&p).Error\n+        var lineage types.Lineage\n+        if err := json.Unmarshal(plan, &lineage); err != nil {\n+                return err\n+        }\n+\n+        // Recover lineage from db if it's already exists or insert it\n+        res := db.FirstOrCreate(&lineage, lineage)\n+        if res.Error != nil {\n+                return fmt.Errorf(\"Error on lineage retrival during plan insertion: %v\", res.Error)\n+        }\n+\n+        var p types.Plan\n+        if err := json.Unmarshal(plan, &p); err != nil {\n+                return err\n+        }\n+        if err := json.Unmarshal(p.PlanJSON, &p.ParsedPlan); err != nil {\n+                return err\n+        }\n+\n+        p.LineageID = lineage.ID\n+        return db.Create(&p).Error\n }\n \n // GetPlansSummary retrieves a summary of all Plans of a lineage from the database\n func (db *Database) GetPlansSummary(lineage, limitStr, pageStr string) (plans []types.Plan, page int, total int) {\n-\tvar whereClause []interface{}\n-\tvar whereClauseTotal string\n-\tif lineage != \"\" {\n-\t\twhereClause = append(whereClause, `\"Lineage\".\"value\" = ?`, lineage)\n-\t\twhereClauseTotal = ` JOIN lineages on lineages.id=t.lineage_id WHERE lineages.value = ?`\n-\t}\n-\n-\trow := db.Raw(\"SELECT count(*) FROM plans AS t\"+whereClauseTotal, lineage).Row()\n-\tif err := row.Scan(&total); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\n-\tvar limit int\n-\tif limitStr == \"\" {\n-\t\tlimit = -1\n-\t} else {\n-\t\tvar err error\n-\t\tlimit, err = strconv.Atoi(limitStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetPlans limit ignored: %v\", err)\n-\t\t\tlimit = -1\n-\t\t}\n-\t}\n-\n-\tvar offset int\n-\tif pageStr == \"\" {\n-\t\toffset = -1\n-\t} else {\n-\t\tvar err error\n-\t\tpage, err = strconv.Atoi(pageStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetPlans offset ignored: %v\", err)\n-\t\t} else {\n-\t\t\toffset = (page - 1) * pageSize\n-\t\t}\n-\t}\n-\n-\tdb.Select(`\"plans\".\"id\"`, `\"plans\".\"created_at\"`, `\"plans\".\"updated_at\"`, `\"plans\".\"tf_version\"`,\n-\t\t`\"plans\".\"git_remote\"`, `\"plans\".\"git_commit\"`, `\"plans\".\"ci_url\"`, `\"plans\".\"source\"`, `\"plans\".\"exit_code\"`).\n-\t\tJoins(\"Lineage\").\n-\t\tOrder(\"created_at desc\").\n-\t\tLimit(limit).\n-\t\tOffset(offset).\n-\t\tFind(&plans, whereClause...)\n-\n-\treturn\n+        var whereClause []interface{}\n+        var whereClauseTotal string\n+        if lineage != \"\" {\n+                whereClause = append(whereClause, `\"Lineage\".\"value\" = ?`, lineage)\n+                whereClauseTotal = ` JOIN lineages on lineages.id=t.lineage_id WHERE lineages.value = ?`\n+        }\n+\n+        row := db.Raw(\"SELECT count(*) FROM plans AS t\"+whereClauseTotal, lineage).Row()\n+        if err := row.Scan(&total); err != nil {\n+                log.Error(err.Error())\n+        }\n+\n+        var limit int\n+        if limitStr == \"\" {\n+                limit = -1\n+        } else {\n+                var err error\n+                limit, err = strconv.Atoi(limitStr)\n+                if err != nil {\n+                        log.Warnf(\"GetPlans limit ignored: %v\", err)\n+                        limit = -1\n+                }\n+        }\n+\n+        var offset int\n+        if pageStr == \"\" {\n+                offset = -1\n+        } else {\n+                var err error\n+                page, err = strconv.Atoi(pageStr)\n+                if err != nil {\n+                        log.Warnf(\"GetPlans offset ignored: %v\", err)\n+                } else {\n+                        offset = (page - 1) * pageSize\n+                }\n+        }\n+\n+        db.Select(`\"plans\".\"id\"`, `\"plans\".\"created_at\"`, `\"plans\".\"updated_at\"`, `\"plans\".\"tf_version\"`,\n+                `\"plans\".\"git_remote\"`, `\"plans\".\"git_commit\"`, `\"plans\".\"ci_url\"`, `\"plans\".\"source\"`, `\"plans\".\"exit_code\"`).\n+                Joins(\"Lineage\").\n+                Order(\"created_at desc\").\n+                Limit(limit).\n+                Offset(offset).\n+                Find(&plans, whereClause...)\n+\n+        return\n }\n \n // GetPlan retrieves a specific Plan by his ID from the database\n func (db *Database) GetPlan(id string) (plans types.Plan) {\n-\tdb.Joins(\"Lineage\").\n-\t\tPreload(\"ParsedPlan\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateModules\").\n-\t\tPreload(\"ParsedPlan.Variables\").\n-\t\tPreload(\"ParsedPlan.PlanResourceChanges\").\n-\t\tPreload(\"ParsedPlan.PlanResourceChanges.Change\").\n-\t\tPreload(\"ParsedPlan.PlanOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanOutputs.Change\").\n-\t\tPreload(\"ParsedPlan.PlanState\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateModules\").\n-\t\tFind(&plans, `\"plans\".\"id\" = ?`, id)\n-\n-\treturn\n+        db.Joins(\"Lineage\").\n+                Preload(\"ParsedPlan\").\n+                Preload(\"ParsedPlan.PlanStateValue\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateOutputs\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateModules\").\n+                Preload(\"ParsedPlan.Variables\").\n+                Preload(\"ParsedPlan.PlanResourceChanges\").\n+                Preload(\"ParsedPlan.PlanResourceChanges.Change\").\n+                Preload(\"ParsedPlan.PlanOutputs\").\n+                Preload(\"ParsedPlan.PlanOutputs.Change\").\n+                Preload(\"ParsedPlan.PlanState\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateOutputs\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateModules\").\n+                Find(&plans, `\"plans\".\"id\" = ?`, id)\n+\n+        return\n }\n \n // GetPlans retrieves all Plan of a lineage from the database\n func (db *Database) GetPlans(lineage, limitStr, pageStr string) (plans []types.Plan, page int, total int) {\n-\tvar whereClause []interface{}\n-\tvar whereClauseTotal string\n-\tif lineage != \"\" {\n-\t\twhereClause = append(whereClause, `\"Lineage\".\"value\" = ?`, lineage)\n-\t\twhereClauseTotal = ` JOIN lineages on lineages.id=t.lineage_id WHERE lineages.value = ?`\n-\t}\n-\n-\trow := db.Raw(\"SELECT count(*) FROM plans AS t\"+whereClauseTotal, lineage).Row()\n-\tif err := row.Scan(&total); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\n-\tvar limit int\n-\tif limitStr == \"\" {\n-\t\tlimit = -1\n-\t} else {\n-\t\tvar err error\n-\t\tlimit, err = strconv.Atoi(limitStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetPlans limit ignored: %v\", err)\n-\t\t\tlimit = -1\n-\t\t}\n-\t}\n-\n-\tvar offset int\n-\tif pageStr == \"\" {\n-\t\toffset = -1\n-\t} else {\n-\t\tvar err error\n-\t\tpage, err = strconv.Atoi(pageStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetPlans offset ignored: %v\", err)\n-\t\t} else {\n-\t\t\toffset = (page - 1) * pageSize\n-\t\t}\n-\t}\n-\n-\tdb.Joins(\"Lineage\").\n-\t\tPreload(\"ParsedPlan\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateModules\").\n-\t\tPreload(\"ParsedPlan.Variables\").\n-\t\tPreload(\"ParsedPlan.PlanResourceChanges\").\n-\t\tPreload(\"ParsedPlan.PlanResourceChanges.Change\").\n-\t\tPreload(\"ParsedPlan.PlanOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanOutputs.Change\").\n-\t\tPreload(\"ParsedPlan.PlanState\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateModules\").\n-\t\tOrder(\"created_at desc\").\n-\t\tLimit(limit).\n-\t\tOffset(offset).\n-\t\tFind(&plans, whereClause...)\n-\n-\treturn\n+        var whereClause []interface{}\n+        var whereClauseTotal string\n+        if lineage != \"\" {\n+                whereClause = append(whereClause, `\"Lineage\".\"value\" = ?`, lineage)\n+                whereClauseTotal = ` JOIN lineages on lineages.id=t.lineage_id WHERE lineages.value = ?`\n+        }\n+\n+        row := db.Raw(\"SELECT count(*) FROM plans AS t\"+whereClauseTotal, lineage).Row()\n+        if err := row.Scan(&total); err != nil {\n+                log.Error(err.Error())\n+        }\n+\n+        var limit int\n+        if limitStr == \"\" {\n+                limit = -1\n+        } else {\n+                var err error\n+                limit, err = strconv.Atoi(limitStr)\n+                if err != nil {\n+                        log.Warnf(\"GetPlans limit ignored: %v\", err)\n+                        limit = -1\n+                }\n+        }\n+\n+        var offset int\n+        if pageStr == \"\" {\n+                offset = -1\n+        } else {\n+                var err error\n+                page, err = strconv.Atoi(pageStr)\n+                if err != nil {\n+                        log.Warnf(\"GetPlans offset ignored: %v\", err)\n+                } else {\n+                        offset = (page - 1) * pageSize\n+                }\n+        }\n+\n+        db.Joins(\"Lineage\").\n+                Preload(\"ParsedPlan\").\n+                Preload(\"ParsedPlan.PlanStateValue\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateOutputs\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateModules\").\n+                Preload(\"ParsedPlan.Variables\").\n+                Preload(\"ParsedPlan.PlanResourceChanges\").\n+                Preload(\"ParsedPlan.PlanResourceChanges.Change\").\n+                Preload(\"ParsedPlan.PlanOutputs\").\n+                Preload(\"ParsedPlan.PlanOutputs.Change\").\n+                Preload(\"ParsedPlan.PlanState\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateOutputs\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateModules\").\n+                Order(\"created_at desc\").\n+                Limit(limit).\n+                Offset(offset).\n+                Find(&plans, whereClause...)\n+\n+        return\n }\n \n // GetLineages retrieves all Lineage from the database\n func (db *Database) GetLineages(limitStr string) (lineages []types.Lineage) {\n-\tvar limit int\n-\tif limitStr == \"\" {\n-\t\tlimit = -1\n-\t} else {\n-\t\tvar err error\n-\t\tlimit, err = strconv.Atoi(limitStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetLineages limit ignored: %v\", err)\n-\t\t\tlimit = -1\n-\t\t}\n-\t}\n-\n-\tdb.Order(\"created_at desc\").\n-\t\tLimit(limit).\n-\t\tFind(&lineages)\n-\treturn\n+        var limit int\n+        if limitStr == \"\" {\n+                limit = -1\n+        } else {\n+                var err error\n+                limit, err = strconv.Atoi(limitStr)\n+                if err != nil {\n+                        log.Warnf(\"GetLineages limit ignored: %v\", err)\n+                        limit = -1\n+                }\n+        }\n+\n+        db.Order(\"created_at desc\").\n+                Limit(limit).\n+                Find(&lineages)\n+        return\n }\n \n // DefaultVersion returns the default VersionID for a given Lineage\n // Copied and adapted from github.com/hashicorp/terraform/command/jsonstate/state.go\n func (db *Database) DefaultVersion(lineage string) (version string, err error) {\n-\tsqlQuery := \"SELECT versions.version_id FROM\" +\n-\t\t\" (SELECT states.path, max(states.serial) as mx FROM states GROUP BY states.path) t\" +\n-\t\t\" JOIN states ON t.path = states.path AND t.mx = states.serial\" +\n-\t\t\" JOIN versions on states.version_id=versions.id\" +\n-\t\t\" JOIN lineages on lineages.id=states.lineage_id\" +\n-\t\t\" WHERE lineages.value = ?\" +\n-\t\t\" ORDER BY versions.last_modified DESC\"\n-\n-\trow := db.Raw(sqlQuery, lineage).Row()\n-\terr = row.Scan(&version)\n-\treturn\n+        sqlQuery := \"SELECT versions.version_id FROM\" +\n+                \" (SELECT states.path, max(states.serial) as mx FROM states GROUP BY states.path) t\" +\n+                \" JOIN states ON t.path = states.path AND t.mx = states.serial\" +\n+                \" JOIN versions on states.version_id=versions.id\" +\n+                \" JOIN lineages on lineages.id=states.lineage_id\" +\n+                \" WHERE lineages.value = ?\" +\n+                \" ORDER BY versions.last_modified DESC\"\n+\n+        row := db.Raw(sqlQuery, lineage).Row()\n+        err = row.Scan(&version)\n+        return\n }\n \n // Close get generic database interface *sql.DB from the current *gorm.DB\n // and close it\n func (db *Database) Close() {\n-\tsqlDb, err := db.DB.DB()\n-\tif err != nil {\n-\t\tlog.Fatalf(\"Unable to terminate db instance: %v\\n\", err)\n-\t}\n-\tsqlDb.Close()\n+        sqlDb, err := db.DB.DB()\n+        if err != nil {\n+                log.Fatalf(\"Unable to terminate db instance: %v\\n\", err)\n+        }\n+        sqlDb.Close()\n }\n"}
{"cve":"CVE-2024-52010:0708", "fix_patch": "diff --git a/src/mod/sshprox/sshprox.go b/src/mod/sshprox/sshprox.go\nindex ed1b92c..9e1eed7 100644\n--- a/src/mod/sshprox/sshprox.go\n+++ b/src/mod/sshprox/sshprox.go\n@@ -1,215 +1,232 @@\n package sshprox\n \n import (\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"log\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path/filepath\"\n-\t\"runtime\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\t\"github.com/google/uuid\"\n-\t\"imuslab.com/zoraxy/mod/reverseproxy\"\n-\t\"imuslab.com/zoraxy/mod/utils\"\n-\t\"imuslab.com/zoraxy/mod/websocketproxy\"\n+\"unicode\"\n+        \"errors\"\n+        \"fmt\"\n+        \"log\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path/filepath\"\n+        \"runtime\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        \"github.com/google/uuid\"\n+        \"imuslab.com/zoraxy/mod/reverseproxy\"\n+        \"imuslab.com/zoraxy/mod/utils\"\n+        \"imuslab.com/zoraxy/mod/websocketproxy\"\n )\n \n /*\n-\tSSH Proxy\n+        SSH Proxy\n \n-\tThis is a tool to bind gotty into Zoraxy\n-\tso that you can do something similar to\n-\tonline ssh terminal\n+        This is a tool to bind gotty into Zoraxy\n+        so that you can do something similar to\n+        online ssh terminal\n */\n \n type Manager struct {\n-\tStartingPort int\n-\tInstances    []*Instance\n+        StartingPort int\n+        Instances    []*Instance\n }\n \n type Instance struct {\n-\tUUID         string\n-\tExecPath     string\n-\tRemoteAddr   string\n-\tRemotePort   int\n-\tAssignedPort int\n-\tconn         *reverseproxy.ReverseProxy //HTTP proxy\n-\ttty          *exec.Cmd                  //SSH connection ported to web interface\n-\tParent       *Manager\n+        UUID         string\n+        ExecPath     string\n+        RemoteAddr   string\n+        RemotePort   int\n+        AssignedPort int\n+        conn         *reverseproxy.ReverseProxy //HTTP proxy\n+        tty          *exec.Cmd                  //SSH connection ported to web interface\n+        Parent       *Manager\n }\n \n func NewSSHProxyManager() *Manager {\n-\treturn &Manager{\n-\t\tStartingPort: 14810,\n-\t\tInstances:    []*Instance{},\n-\t}\n+        return &Manager{\n+                StartingPort: 14810,\n+                Instances:    []*Instance{},\n+        }\n }\n \n // Get the next free port in the list\n func (m *Manager) GetNextPort() int {\n-\tnextPort := m.StartingPort\n-\toccupiedPort := make(map[int]bool)\n-\tfor _, instance := range m.Instances {\n-\t\toccupiedPort[instance.AssignedPort] = true\n-\t}\n-\tfor {\n-\t\tif !occupiedPort[nextPort] {\n-\t\t\treturn nextPort\n-\t\t}\n-\t\tnextPort++\n-\t}\n+        nextPort := m.StartingPort\n+        occupiedPort := make(map[int]bool)\n+        for _, instance := range m.Instances {\n+                occupiedPort[instance.AssignedPort] = true\n+        }\n+        for {\n+                if !occupiedPort[nextPort] {\n+                        return nextPort\n+                }\n+                nextPort++\n+        }\n }\n \n func (m *Manager) HandleHttpByInstanceId(instanceId string, w http.ResponseWriter, r *http.Request) {\n-\ttargetInstance, err := m.GetInstanceById(instanceId)\n-\tif err != nil {\n-\t\thttp.Error(w, err.Error(), http.StatusNotFound)\n-\t\treturn\n-\t}\n-\n-\tif targetInstance.tty == nil {\n-\t\t//Server side already closed\n-\t\thttp.Error(w, \"Connection already closed\", http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\tr.Header.Set(\"X-Forwarded-Host\", r.Host)\n-\trequestURL := r.URL.String()\n-\tif r.Header[\"Upgrade\"] != nil && strings.ToLower(r.Header[\"Upgrade\"][0]) == \"websocket\" {\n-\t\t//Handle WebSocket request. Forward the custom Upgrade header and rewrite origin\n-\t\tr.Header.Set(\"Zr-Origin-Upgrade\", \"websocket\")\n-\t\trequestURL = strings.TrimPrefix(requestURL, \"/\")\n-\t\tu, _ := url.Parse(\"ws://127.0.0.1:\" + strconv.Itoa(targetInstance.AssignedPort) + \"/\" + requestURL)\n-\t\twspHandler := websocketproxy.NewProxy(u, websocketproxy.Options{\n-\t\t\tSkipTLSValidation: false,\n-\t\t\tSkipOriginCheck:   false,\n-\t\t\tLogger:            nil,\n-\t\t})\n-\t\twspHandler.ServeHTTP(w, r)\n-\t\treturn\n-\t}\n-\n-\ttargetInstance.conn.ProxyHTTP(w, r)\n+        targetInstance, err := m.GetInstanceById(instanceId)\n+        if err != nil {\n+                http.Error(w, err.Error(), http.StatusNotFound)\n+                return\n+        }\n+\n+        if targetInstance.tty == nil {\n+                //Server side already closed\n+                http.Error(w, \"Connection already closed\", http.StatusInternalServerError)\n+                return\n+        }\n+\n+        r.Header.Set(\"X-Forwarded-Host\", r.Host)\n+        requestURL := r.URL.String()\n+        if r.Header[\"Upgrade\"] != nil && strings.ToLower(r.Header[\"Upgrade\"][0]) == \"websocket\" {\n+                //Handle WebSocket request. Forward the custom Upgrade header and rewrite origin\n+                r.Header.Set(\"Zr-Origin-Upgrade\", \"websocket\")\n+                requestURL = strings.TrimPrefix(requestURL, \"/\")\n+                u, _ := url.Parse(\"ws://127.0.0.1:\" + strconv.Itoa(targetInstance.AssignedPort) + \"/\" + requestURL)\n+                wspHandler := websocketproxy.NewProxy(u, websocketproxy.Options{\n+                        SkipTLSValidation: false,\n+                        SkipOriginCheck:   false,\n+                        Logger:            nil,\n+                })\n+                wspHandler.ServeHTTP(w, r)\n+                return\n+        }\n+\n+        targetInstance.conn.ProxyHTTP(w, r)\n }\n \n func (m *Manager) GetInstanceById(instanceId string) (*Instance, error) {\n-\tfor _, instance := range m.Instances {\n-\t\tif instance.UUID == instanceId {\n-\t\t\treturn instance, nil\n-\t\t}\n-\t}\n-\treturn nil, fmt.Errorf(\"instance not found: %s\", instanceId)\n+        for _, instance := range m.Instances {\n+                if instance.UUID == instanceId {\n+                        return instance, nil\n+                }\n+        }\n+        return nil, fmt.Errorf(\"instance not found: %s\", instanceId)\n }\n func (m *Manager) NewSSHProxy(binaryRoot string) (*Instance, error) {\n-\t//Check if the binary exists in system/gotty/\n-\tbinary := \"gotty_\" + runtime.GOOS + \"_\" + runtime.GOARCH\n-\n-\tif runtime.GOOS == \"windows\" {\n-\t\tbinary = binary + \".exe\"\n-\t}\n-\n-\t//Extract it from embedfs if not exists locally\n-\texecPath := filepath.Join(binaryRoot, binary)\n-\n-\t//Create the storage folder structure\n-\tos.MkdirAll(filepath.Dir(execPath), 0775)\n-\n-\t//Create config file if not exists\n-\tif !utils.FileExists(filepath.Join(filepath.Dir(execPath), \".gotty\")) {\n-\t\tconfigFile, _ := gotty.ReadFile(\"gotty/.gotty\")\n-\t\tos.WriteFile(filepath.Join(filepath.Dir(execPath), \".gotty\"), configFile, 0775)\n-\t}\n-\n-\t//Create web.ssh binary if not exists\n-\tif !utils.FileExists(execPath) {\n-\t\t//Try to extract it from embedded fs\n-\t\texecutable, err := gotty.ReadFile(\"gotty/\" + binary)\n-\t\tif err != nil {\n-\t\t\t//Binary not found in embedded\n-\t\t\treturn nil, errors.New(\"platform not supported\")\n-\t\t}\n-\n-\t\t//Extract to target location\n-\t\terr = os.WriteFile(execPath, executable, 0777)\n-\t\tif err != nil {\n-\t\t\t//Binary not found in embedded\n-\t\t\tlog.Println(\"Extract web.ssh failed: \" + err.Error())\n-\t\t\treturn nil, errors.New(\"web.ssh sub-program extract failed\")\n-\t\t}\n-\t}\n-\n-\t//Convert the binary path to realpath\n-\trealpath, err := filepath.Abs(execPath)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tthisInstance := Instance{\n-\t\tUUID:         uuid.New().String(),\n-\t\tExecPath:     realpath,\n-\t\tAssignedPort: -1,\n-\t\tParent:       m,\n-\t}\n-\n-\tm.Instances = append(m.Instances, &thisInstance)\n-\n-\treturn &thisInstance, nil\n+        //Check if the binary exists in system/gotty/\n+        binary := \"gotty_\" + runtime.GOOS + \"_\" + runtime.GOARCH\n+\n+        if runtime.GOOS == \"windows\" {\n+                binary = binary + \".exe\"\n+        }\n+\n+        //Extract it from embedfs if not exists locally\n+        execPath := filepath.Join(binaryRoot, binary)\n+\n+        //Create the storage folder structure\n+        os.MkdirAll(filepath.Dir(execPath), 0775)\n+\n+        //Create config file if not exists\n+        if !utils.FileExists(filepath.Join(filepath.Dir(execPath), \".gotty\")) {\n+                configFile, _ := gotty.ReadFile(\"gotty/.gotty\")\n+                os.WriteFile(filepath.Join(filepath.Dir(execPath), \".gotty\"), configFile, 0775)\n+        }\n+\n+        //Create web.ssh binary if not exists\n+        if !utils.FileExists(execPath) {\n+                //Try to extract it from embedded fs\n+                executable, err := gotty.ReadFile(\"gotty/\" + binary)\n+                if err != nil {\n+                        //Binary not found in embedded\n+                        return nil, errors.New(\"platform not supported\")\n+                }\n+\n+                //Extract to target location\n+                err = os.WriteFile(execPath, executable, 0777)\n+                if err != nil {\n+                        //Binary not found in embedded\n+                        log.Println(\"Extract web.ssh failed: \" + err.Error())\n+                        return nil, errors.New(\"web.ssh sub-program extract failed\")\n+                }\n+        }\n+\n+        //Convert the binary path to realpath\n+        realpath, err := filepath.Abs(execPath)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        thisInstance := Instance{\n+                UUID:         uuid.New().String(),\n+                ExecPath:     realpath,\n+                AssignedPort: -1,\n+                Parent:       m,\n+        }\n+\n+        m.Instances = append(m.Instances, &thisInstance)\n+\n+        return &thisInstance, nil\n }\n \n // Create a new Connection to target address\n func (i *Instance) CreateNewConnection(listenPort int, username string, remoteIpAddr string, remotePort int) error {\n-\t//Create a gotty instance\n-\tconnAddr := remoteIpAddr\n-\tif username != \"\" {\n-\t\tconnAddr = username + \"@\" + remoteIpAddr\n-\t}\n-\tconfigPath := filepath.Join(filepath.Dir(i.ExecPath), \".gotty\")\n-\ttitle := username + \"@\" + remoteIpAddr\n-\tif remotePort != 22 {\n-\t\ttitle = title + \":\" + strconv.Itoa(remotePort)\n-\t}\n-\n-\tsshCommand := []string{\"ssh\", \"-t\", connAddr, \"-p\", strconv.Itoa(remotePort)}\n-\tcmd := exec.Command(i.ExecPath, \"-w\", \"-p\", strconv.Itoa(listenPort), \"--once\", \"--config\", configPath, \"--title-format\", title, \"bash\", \"-c\", strings.Join(sshCommand, \" \"))\n-\tcmd.Dir = filepath.Dir(i.ExecPath)\n-\tcmd.Env = append(os.Environ(), \"TERM=xterm\")\n-\tcmd.Stdout = os.Stdout\n-\tcmd.Stderr = os.Stderr\n-\tgo func() {\n-\t\tcmd.Run()\n-\t\ti.Destroy()\n-\t}()\n-\ti.tty = cmd\n-\ti.AssignedPort = listenPort\n-\ti.RemoteAddr = remoteIpAddr\n-\ti.RemotePort = remotePort\n-\n-\t//Create a new proxy agent for this root\n-\tpath, err := url.Parse(\"http://127.0.0.1:\" + strconv.Itoa(listenPort))\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t//Create new proxy objects to the proxy\n-\tproxy := reverseproxy.NewReverseProxy(path)\n-\n-\ti.conn = proxy\n-\treturn nil\n+        //Create a gotty instance\n+        connAddr := remoteIpAddr\n+        if username != \"\" {\n+if !isValidUsername(username) {\n+return errors.New(\"invalid username\")\n }\n+                connAddr = username + \"@\" + remoteIpAddr\n+        }\n+        configPath := filepath.Join(filepath.Dir(i.ExecPath), \".gotty\")\n+        title := username + \"@\" + remoteIpAddr\n+        if remotePort != 22 {\n+                title = title + \":\" + strconv.Itoa(remotePort)\n+        }\n+\n+        sshCommand := []string{\"ssh\", \"-t\", connAddr, \"-p\", strconv.Itoa(remotePort)}\n+        cmd := exec.Command(i.ExecPath, \"-w\", \"-p\", strconv.Itoa(listenPort), \"--once\", \"--config\", configPath, \"--title-format\", title, \"bash\", \"-c\", strings.Join(sshCommand, \" \"))\n+        cmd.Dir = filepath.Dir(i.ExecPath)\n+        cmd.Env = append(os.Environ(), \"TERM=xterm\")\n+        cmd.Stdout = os.Stdout\n+        cmd.Stderr = os.Stderr\n+        go func() {\n+                cmd.Run()\n+                i.Destroy()\n+        }()\n+        i.tty = cmd\n+        i.AssignedPort = listenPort\n+        i.RemoteAddr = remoteIpAddr\n+        i.RemotePort = remotePort\n+\n+        //Create a new proxy agent for this root\n+        path, err := url.Parse(\"http://127.0.0.1:\" + strconv.Itoa(listenPort))\n+        if err != nil {\n+                return err\n+        }\n+\n+        //Create new proxy objects to the proxy\n+        proxy := reverseproxy.NewReverseProxy(path)\n+\n+        i.conn = proxy\n+        return nil\n+}\n+\n+// isValidUsername checks if the username is valid\n+func isValidUsername(username string) bool {\n+// Allow alphanumeric characters, underscores, and hyphens\n+// This is a basic check, you might want to adjust it based on your needs\n+for _, r := range username {\n+if !unicode.IsLetter(r) && !unicode.IsNumber(r) && r != '_' && r != '-' {\n+return false\n+}\n+}\n+return true\n+}\n+\n \n func (i *Instance) Destroy() {\n-\t// Remove the instance from the Manager's Instances list\n-\tfor idx, inst := range i.Parent.Instances {\n-\t\tif inst == i {\n-\t\t\t// Remove the instance from the slice by swapping it with the last instance and slicing the slice\n-\t\t\ti.Parent.Instances[len(i.Parent.Instances)-1], i.Parent.Instances[idx] = i.Parent.Instances[idx], i.Parent.Instances[len(i.Parent.Instances)-1]\n-\t\t\ti.Parent.Instances = i.Parent.Instances[:len(i.Parent.Instances)-1]\n-\t\t\tbreak\n-\t\t}\n-\t}\n+        // Remove the instance from the Manager's Instances list\n+        for idx, inst := range i.Parent.Instances {\n+                if inst == i {\n+                        // Remove the instance from the slice by swapping it with the last instance and slicing the slice\n+                        i.Parent.Instances[len(i.Parent.Instances)-1], i.Parent.Instances[idx] = i.Parent.Instances[idx], i.Parent.Instances[len(i.Parent.Instances)-1]\n+                        i.Parent.Instances = i.Parent.Instances[:len(i.Parent.Instances)-1]\n+                        break\n+                }\n+        }\n }\n"}
{"cve":"CVE-2023-41891:0708", "fix_patch": "diff --git a/pkg/common/sorting.go b/pkg/common/sorting.go\nindex c4922d0b..7b19d8a8 100644\n--- a/pkg/common/sorting.go\n+++ b/pkg/common/sorting.go\n@@ -1,39 +1,59 @@\n package common\n \n import (\n-\t\"fmt\"\n+\"fmt\"\n \n-\t\"github.com/flyteorg/flyteadmin/pkg/errors\"\n-\t\"github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/admin\"\n-\t\"google.golang.org/grpc/codes\"\n+\"github.com/flyteorg/flyteadmin/pkg/errors\"\n+\"github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/admin\"\n+\"google.golang.org/grpc/codes\"\n )\n \n const gormDescending = \"%s desc\"\n const gormAscending = \"%s asc\"\n \n type SortParameter interface {\n-\tGetGormOrderExpr() string\n+GetGormOrderExpr() string\n }\n \n type sortParamImpl struct {\n-\tgormOrderExpression string\n+gormOrderExpression string\n }\n \n func (s *sortParamImpl) GetGormOrderExpr() string {\n-\treturn s.gormOrderExpression\n+return s.gormOrderExpression\n }\n \n func NewSortParameter(sort admin.Sort) (SortParameter, error) {\n-\tvar gormOrderExpression string\n-\tswitch sort.Direction {\n-\tcase admin.Sort_DESCENDING:\n-\t\tgormOrderExpression = fmt.Sprintf(gormDescending, sort.Key)\n-\tcase admin.Sort_ASCENDING:\n-\t\tgormOrderExpression = fmt.Sprintf(gormAscending, sort.Key)\n-\tdefault:\n-\t\treturn nil, errors.NewFlyteAdminErrorf(codes.InvalidArgument, \"invalid sort order specified: %v\", sort)\n-\t}\n-\treturn &sortParamImpl{\n-\t\tgormOrderExpression: gormOrderExpression,\n-\t}, nil\n+if !validateSortKey(sort.Key) {\n+return nil, errors.NewFlyteAdminErrorf(codes.InvalidArgument, \"invalid sort key specified: %v\", sort.Key)\n+}\n+var gormOrderExpression string\n+switch sort.Direction {\n+case admin.Sort_DESCENDING:\n+gormOrderExpression = fmt.Sprintf(gormDescending, sort.Key)\n+case admin.Sort_ASCENDING:\n+gormOrderExpression = fmt.Sprintf(gormAscending, sort.Key)\n+default:\n+return nil, errors.NewFlyteAdminErrorf(codes.InvalidArgument, \"invalid sort order specified: %v\", sort)\n+}\n+return &sortParamImpl{\n+gormOrderExpression: gormOrderExpression,\n+}, nil\n+}\n+\n+var allowedSortKeys = map[string]bool{\n+\"name\": true,\n+\"created_at\": true,\n+\"updated_at\": true,\n+\"execution_created_at\": true,\n+\"execution_updated_at\": true,\n+\"schedule_time\": true,\n+\"version\": true,\n+\"launch_plan_name\": true,\n+\"launch_plan_version\": true,\n+}\n+\n+func validateSortKey(key string) bool {\n+_, ok := allowedSortKeys[key]\n+return ok\n }\n"}
{"cve":"CVE-2022-23536:0708", "fix_patch": ""}
{"cve":"CVE-2024-24579:0708", "fix_patch": "diff --git a/pkg/file/tarutil.go b/pkg/file/tarutil.go\nindex 374c60f..dd5d655 100644\n--- a/pkg/file/tarutil.go\n+++ b/pkg/file/tarutil.go\n@@ -1,15 +1,16 @@\n package file\n \n import (\n-\t\"archive/tar\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path/filepath\"\n+\"archive/tar\"\n+\"fmt\"\n+\"io\"\n+\"os\"\n+\"path/filepath\"\n+\"strings\"\n \n-\t\"github.com/pkg/errors\"\n+\"github.com/pkg/errors\"\n \n-\t\"github.com/anchore/stereoscope/internal/log\"\n+\"github.com/anchore/stereoscope/internal/log\"\n )\n \n const perFileReadLimit = 2 * GB\n@@ -18,15 +19,15 @@ var ErrTarStopIteration = fmt.Errorf(\"halt iterating tar\")\n \n // tarFile is a ReadCloser of a tar file on disk.\n type tarFile struct {\n-\tio.Reader\n-\tio.Closer\n+        io.Reader\n+        io.Closer\n }\n \n // TarFileEntry represents the header, contents, and list position of an entry within a tar file.\n type TarFileEntry struct {\n-\tSequence int64\n-\tHeader   tar.Header\n-\tReader   io.Reader\n+        Sequence int64\n+        Header   tar.Header\n+        Reader   io.Reader\n }\n \n // TarFileVisitor is a visitor function meant to be used in conjunction with the IterateTar.\n@@ -34,130 +35,138 @@ type TarFileVisitor func(TarFileEntry) error\n \n // ErrFileNotFound returned from ReaderFromTar if a file is not found in the given archive.\n type ErrFileNotFound struct {\n-\tPath string\n+        Path string\n }\n \n func (e *ErrFileNotFound) Error() string {\n-\treturn fmt.Sprintf(\"file not found (path=%s)\", e.Path)\n+        return fmt.Sprintf(\"file not found (path=%s)\", e.Path)\n }\n \n // IterateTar is a function that reads across a tar and invokes a visitor function for each entry discovered. The iterator\n // stops when there are no more entries to read, if there is an error in the underlying reader or visitor function,\n // or if the visitor function returns a ErrTarStopIteration sentinel error.\n func IterateTar(reader io.Reader, visitor TarFileVisitor) error {\n-\ttarReader := tar.NewReader(reader)\n-\tvar sequence int64 = -1\n-\tfor {\n-\t\tsequence++\n-\n-\t\thdr, err := tarReader.Next()\n-\t\tif errors.Is(err, io.EOF) {\n-\t\t\tbreak\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tif hdr == nil {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif err := visitor(TarFileEntry{\n-\t\t\tSequence: sequence,\n-\t\t\tHeader:   *hdr,\n-\t\t\tReader:   tarReader,\n-\t\t}); err != nil {\n-\t\t\tif errors.Is(err, ErrTarStopIteration) {\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\treturn fmt.Errorf(\"failed to visit tar entry=%q : %w\", hdr.Name, err)\n-\t\t}\n-\t}\n-\treturn nil\n+        tarReader := tar.NewReader(reader)\n+        var sequence int64 = -1\n+        for {\n+                sequence++\n+\n+                hdr, err := tarReader.Next()\n+                if errors.Is(err, io.EOF) {\n+                        break\n+                }\n+                if err != nil {\n+                        return err\n+                }\n+                if hdr == nil {\n+                        continue\n+                }\n+\n+                if err := visitor(TarFileEntry{\n+                        Sequence: sequence,\n+                        Header:   *hdr,\n+                        Reader:   tarReader,\n+                }); err != nil {\n+                        if errors.Is(err, ErrTarStopIteration) {\n+                                return nil\n+                        }\n+                        return fmt.Errorf(\"failed to visit tar entry=%q : %w\", hdr.Name, err)\n+                }\n+        }\n+        return nil\n }\n \n // ReaderFromTar returns a io.ReadCloser for the Path within a tar file.\n func ReaderFromTar(reader io.ReadCloser, tarPath string) (io.ReadCloser, error) {\n-\tvar result io.ReadCloser\n-\n-\tvisitor := func(entry TarFileEntry) error {\n-\t\tif entry.Header.Name == tarPath {\n-\t\t\tresult = &tarFile{\n-\t\t\t\tReader: entry.Reader,\n-\t\t\t\tCloser: reader,\n-\t\t\t}\n-\t\t\treturn ErrTarStopIteration\n-\t\t}\n-\t\treturn nil\n-\t}\n-\tif err := IterateTar(reader, visitor); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif result == nil {\n-\t\treturn nil, &ErrFileNotFound{tarPath}\n-\t}\n-\n-\treturn result, nil\n+        var result io.ReadCloser\n+\n+        visitor := func(entry TarFileEntry) error {\n+                if entry.Header.Name == tarPath {\n+                        result = &tarFile{\n+                                Reader: entry.Reader,\n+                                Closer: reader,\n+                        }\n+                        return ErrTarStopIteration\n+                }\n+                return nil\n+        }\n+        if err := IterateTar(reader, visitor); err != nil {\n+                return nil, err\n+        }\n+\n+        if result == nil {\n+                return nil, &ErrFileNotFound{tarPath}\n+        }\n+\n+        return result, nil\n }\n \n // MetadataFromTar returns the tar metadata from the header info.\n func MetadataFromTar(reader io.ReadCloser, tarPath string) (Metadata, error) {\n-\tvar metadata *Metadata\n-\tvisitor := func(entry TarFileEntry) error {\n-\t\tif entry.Header.Name == tarPath {\n-\t\t\tvar content io.Reader\n-\t\t\tif entry.Header.Size > 0 {\n-\t\t\t\tcontent = reader\n-\t\t\t}\n-\t\t\tm := NewMetadata(entry.Header, content)\n-\t\t\tmetadata = &m\n-\t\t\treturn ErrTarStopIteration\n-\t\t}\n-\t\treturn nil\n-\t}\n-\tif err := IterateTar(reader, visitor); err != nil {\n-\t\treturn Metadata{}, err\n-\t}\n-\tif metadata == nil {\n-\t\treturn Metadata{}, &ErrFileNotFound{tarPath}\n-\t}\n-\treturn *metadata, nil\n+        var metadata *Metadata\n+        visitor := func(entry TarFileEntry) error {\n+                if entry.Header.Name == tarPath {\n+                        var content io.Reader\n+                        if entry.Header.Size > 0 {\n+                                content = reader\n+                        }\n+                        m := NewMetadata(entry.Header, content)\n+                        metadata = &m\n+                        return ErrTarStopIteration\n+                }\n+                return nil\n+        }\n+        if err := IterateTar(reader, visitor); err != nil {\n+                return Metadata{}, err\n+        }\n+        if metadata == nil {\n+                return Metadata{}, &ErrFileNotFound{tarPath}\n+        }\n+        return *metadata, nil\n }\n \n // UntarToDirectory writes the contents of the given tar reader to the given destination\n func UntarToDirectory(reader io.Reader, dst string) error {\n-\tvisitor := func(entry TarFileEntry) error {\n-\t\ttarget := filepath.Join(dst, entry.Header.Name)\n-\n-\t\tswitch entry.Header.Typeflag {\n-\t\tcase tar.TypeDir:\n-\t\t\tif _, err := os.Stat(target); err != nil {\n-\t\t\t\tif err := os.MkdirAll(target, 0755); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\tcase tar.TypeReg:\n-\t\t\tf, err := os.OpenFile(target, os.O_CREATE|os.O_RDWR, os.FileMode(entry.Header.Mode))\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\t// limit the reader on each file read to prevent decompression bomb attacks\n-\t\t\tnumBytes, err := io.Copy(f, io.LimitReader(entry.Reader, perFileReadLimit))\n-\t\t\tif numBytes >= perFileReadLimit || errors.Is(err, io.EOF) {\n-\t\t\t\treturn fmt.Errorf(\"zip read limit hit (potential decompression bomb attack)\")\n-\t\t\t}\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"unable to copy file: %w\", err)\n-\t\t\t}\n-\n-\t\t\tif err = f.Close(); err != nil {\n-\t\t\t\tlog.Errorf(\"failed to close file during untar of path=%q: %w\", f.Name(), err)\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n-\n-\treturn IterateTar(reader, visitor)\n+        visitor := func(entry TarFileEntry) error {\n+                // path sanitization: this happens automatically with filepath.Join, but we can be explicit about it\n+                // for security purposes.\n+                // we need to be careful to not introduce an issue where a malicious tar file can overwrite any file on the\n+                // host machine.\n+                //  \u00fanicamente el path sanitizado se utiliza para escribir en el sistema de ficheros\n+                destPath := filepath.Join(dst, entry.Header.Name)\n+                if !strings.HasPrefix(destPath, dst) {\n+                        return fmt.Errorf(\"tar entry %q is trying to write outside of %q\", entry.Header.Name, dst)\n+                }\n+\n+                switch entry.Header.Typeflag {\n+                case tar.TypeDir:\n+                        if _, err := os.Stat(destPath); err != nil {\n+                                if err := os.MkdirAll(destPath, 0755); err != nil {\n+                                        return err\n+                                }\n+                        }\n+\n+                case tar.TypeReg:\n+                        f, err := os.OpenFile(destPath, os.O_CREATE|os.O_RDWR, os.FileMode(entry.Header.Mode))\n+                        if err != nil {\n+                                return err\n+                        }\n+\n+                        // limit the reader on each file read to prevent decompression bomb attacks\n+                        numBytes, err := io.Copy(f, io.LimitReader(entry.Reader, perFileReadLimit))\n+                        if numBytes >= perFileReadLimit || errors.Is(err, io.EOF) {\n+                                return fmt.Errorf(\"zip read limit hit (potential decompression bomb attack)\")\n+                        }\n+                        if err != nil {\n+                                return fmt.Errorf(\"unable to copy file: %w\", err)\n+                        }\n+\n+                        if err = f.Close(); err != nil {\n+                                log.Errorf(\"failed to close file during untar of path=%q: %w\", f.Name(), err)\n+                        }\n+                }\n+                return nil\n+        }\n+\n+        return IterateTar(reader, visitor)\n }\n"}
{"cve":"CVE-2020-4053:0708", "fix_patch": "diff --git a/pkg/plugin/installer/http_installer.go b/pkg/plugin/installer/http_installer.go\nindex c07cad80a..d2339fac8 100644\n--- a/pkg/plugin/installer/http_installer.go\n+++ b/pkg/plugin/installer/http_installer.go\n@@ -16,31 +16,40 @@ limitations under the License.\n package installer // import \"helm.sh/helm/v3/pkg/plugin/installer\"\n \n import (\n-\t\"archive/tar\"\n-\t\"bytes\"\n-\t\"compress/gzip\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"strings\"\n-\n-\t\"github.com/pkg/errors\"\n-\n-\t\"helm.sh/helm/v3/internal/third_party/dep/fs\"\n-\t\"helm.sh/helm/v3/pkg/cli\"\n-\t\"helm.sh/helm/v3/pkg/getter\"\n-\t\"helm.sh/helm/v3/pkg/helmpath\"\n-\t\"helm.sh/helm/v3/pkg/plugin/cache\"\n+        \"archive/tar\"\n+        \"bytes\"\n+        \"compress/gzip\"\n+        \"io\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"strings\"\n+\n+        \"github.com/pkg/errors\"\n+\n+        \"helm.sh/helm/v3/internal/third_party/dep/fs\"\n+        \"helm.sh/helm/v3/pkg/cli\"\n+        \"helm.sh/helm/v3/pkg/getter\"\n+        \"helm.sh/helm/v3/pkg/helmpath\"\n+        \"helm.sh/helm/v3/pkg/plugin/cache\"\n )\n \n+\n+// isPathInside returns true if the given path is inside the given directory.\n+func isPathInside(path, dir string) (bool, error) {\n+rel, err := filepath.Rel(dir, path)\n+if err != nil {\n+return false, err\n+}\n+return rel == \".\" || (!strings.HasPrefix(rel, \"..\") && rel != \"..\"), nil\n+}\n // HTTPInstaller installs plugins from an archive served by a web server.\n type HTTPInstaller struct {\n-\tCacheDir   string\n-\tPluginName string\n-\tbase\n-\textractor Extractor\n-\tgetter    getter.Getter\n+        CacheDir   string\n+        PluginName string\n+        base\n+        extractor Extractor\n+        getter    getter.Getter\n }\n \n // TarGzExtractor extracts gzip compressed tar archives\n@@ -48,63 +57,63 @@ type TarGzExtractor struct{}\n \n // Extractor provides an interface for extracting archives\n type Extractor interface {\n-\tExtract(buffer *bytes.Buffer, targetDir string) error\n+        Extract(buffer *bytes.Buffer, targetDir string) error\n }\n \n // Extractors contains a map of suffixes and matching implementations of extractor to return\n var Extractors = map[string]Extractor{\n-\t\".tar.gz\": &TarGzExtractor{},\n-\t\".tgz\":    &TarGzExtractor{},\n+        \".tar.gz\": &TarGzExtractor{},\n+        \".tgz\":    &TarGzExtractor{},\n }\n \n // NewExtractor creates a new extractor matching the source file name\n func NewExtractor(source string) (Extractor, error) {\n-\tfor suffix, extractor := range Extractors {\n-\t\tif strings.HasSuffix(source, suffix) {\n-\t\t\treturn extractor, nil\n-\t\t}\n-\t}\n-\treturn nil, errors.Errorf(\"no extractor implemented yet for %s\", source)\n+        for suffix, extractor := range Extractors {\n+                if strings.HasSuffix(source, suffix) {\n+                        return extractor, nil\n+                }\n+        }\n+        return nil, errors.Errorf(\"no extractor implemented yet for %s\", source)\n }\n \n // NewHTTPInstaller creates a new HttpInstaller.\n func NewHTTPInstaller(source string) (*HTTPInstaller, error) {\n-\tkey, err := cache.Key(source)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\textractor, err := NewExtractor(source)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tget, err := getter.All(new(cli.EnvSettings)).ByScheme(\"http\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ti := &HTTPInstaller{\n-\t\tCacheDir:   helmpath.CachePath(\"plugins\", key),\n-\t\tPluginName: stripPluginName(filepath.Base(source)),\n-\t\tbase:       newBase(source),\n-\t\textractor:  extractor,\n-\t\tgetter:     get,\n-\t}\n-\treturn i, nil\n+        key, err := cache.Key(source)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        extractor, err := NewExtractor(source)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        get, err := getter.All(new(cli.EnvSettings)).ByScheme(\"http\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        i := &HTTPInstaller{\n+                CacheDir:   helmpath.CachePath(\"plugins\", key),\n+                PluginName: stripPluginName(filepath.Base(source)),\n+                base:       newBase(source),\n+                extractor:  extractor,\n+                getter:     get,\n+        }\n+        return i, nil\n }\n \n // helper that relies on some sort of convention for plugin name (plugin-name-<version>)\n func stripPluginName(name string) string {\n-\tvar strippedName string\n-\tfor suffix := range Extractors {\n-\t\tif strings.HasSuffix(name, suffix) {\n-\t\t\tstrippedName = strings.TrimSuffix(name, suffix)\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\tre := regexp.MustCompile(`(.*)-[0-9]+\\..*`)\n-\treturn re.ReplaceAllString(strippedName, `$1`)\n+        var strippedName string\n+        for suffix := range Extractors {\n+                if strings.HasSuffix(name, suffix) {\n+                        strippedName = strings.TrimSuffix(name, suffix)\n+                        break\n+                }\n+        }\n+        re := regexp.MustCompile(`(.*)-[0-9]+\\..*`)\n+        return re.ReplaceAllString(strippedName, `$1`)\n }\n \n // Install downloads and extracts the tarball into the cache directory\n@@ -112,88 +121,93 @@ func stripPluginName(name string) string {\n //\n // Implements Installer.\n func (i *HTTPInstaller) Install() error {\n-\tpluginData, err := i.getter.Get(i.Source)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif err := i.extractor.Extract(pluginData, i.CacheDir); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif !isPlugin(i.CacheDir) {\n-\t\treturn ErrMissingMetadata\n-\t}\n-\n-\tsrc, err := filepath.Abs(i.CacheDir)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tdebug(\"copying %s to %s\", src, i.Path())\n-\treturn fs.CopyDir(src, i.Path())\n+        pluginData, err := i.getter.Get(i.Source)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if err := i.extractor.Extract(pluginData, i.CacheDir); err != nil {\n+                return err\n+        }\n+\n+        if !isPlugin(i.CacheDir) {\n+                return ErrMissingMetadata\n+        }\n+\n+        src, err := filepath.Abs(i.CacheDir)\n+        if err != nil {\n+                return err\n+        }\n+\n+        debug(\"copying %s to %s\", src, i.Path())\n+        return fs.CopyDir(src, i.Path())\n }\n \n // Update updates a local repository\n // Not implemented for now since tarball most likely will be packaged by version\n func (i *HTTPInstaller) Update() error {\n-\treturn errors.Errorf(\"method Update() not implemented for HttpInstaller\")\n+        return errors.Errorf(\"method Update() not implemented for HttpInstaller\")\n }\n \n // Path is overridden because we want to join on the plugin name not the file name\n func (i HTTPInstaller) Path() string {\n-\tif i.base.Source == \"\" {\n-\t\treturn \"\"\n-\t}\n-\treturn helmpath.DataPath(\"plugins\", i.PluginName)\n+        if i.base.Source == \"\" {\n+                return \"\"\n+        }\n+        return helmpath.DataPath(\"plugins\", i.PluginName)\n }\n \n // Extract extracts compressed archives\n //\n // Implements Extractor.\n func (g *TarGzExtractor) Extract(buffer *bytes.Buffer, targetDir string) error {\n-\tuncompressedStream, err := gzip.NewReader(buffer)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif err := os.MkdirAll(targetDir, 0755); err != nil {\n-\t\treturn err\n-\t}\n-\n-\ttarReader := tar.NewReader(uncompressedStream)\n-\tfor {\n-\t\theader, err := tarReader.Next()\n-\t\tif err == io.EOF {\n-\t\t\tbreak\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tpath := filepath.Join(targetDir, header.Name)\n-\n-\t\tswitch header.Typeflag {\n-\t\tcase tar.TypeDir:\n-\t\t\tif err := os.Mkdir(path, 0755); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\tcase tar.TypeReg:\n-\t\t\toutFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR, os.FileMode(header.Mode))\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tif _, err := io.Copy(outFile, tarReader); err != nil {\n-\t\t\t\toutFile.Close()\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\toutFile.Close()\n-\t\t// We don't want to process these extension header files.\n-\t\tcase tar.TypeXGlobalHeader, tar.TypeXHeader:\n-\t\t\tcontinue\n-\t\tdefault:\n-\t\t\treturn errors.Errorf(\"unknown type: %b in %s\", header.Typeflag, header.Name)\n-\t\t}\n-\t}\n-\treturn nil\n+        uncompressedStream, err := gzip.NewReader(buffer)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if err := os.MkdirAll(targetDir, 0755); err != nil {\n+                return err\n+        }\n+\n+        tarReader := tar.NewReader(uncompressedStream)\n+        for {\n+                header, err := tarReader.Next()\n+                if err == io.EOF {\n+                        break\n+                }\n+                if err != nil {\n+                        return err\n+                }\n+\n+                path := filepath.Join(targetDir, header.Name)\n+if ok, err := isPathInside(path, targetDir); err != nil {\n+return err\n+} else if !ok {\n+return errors.Errorf(\"path '%s' is outside of plugin directory\", path)\n+}\n+\n+                switch header.Typeflag {\n+                case tar.TypeDir:\n+                        if err := os.Mkdir(path, 0755); err != nil {\n+                                return err\n+                        }\n+                case tar.TypeReg:\n+                        outFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR, os.FileMode(header.Mode))\n+                        if err != nil {\n+                                return err\n+                        }\n+                        if _, err := io.Copy(outFile, tarReader); err != nil {\n+                                outFile.Close()\n+                                return err\n+                        }\n+                        outFile.Close()\n+                // We don't want to process these extension header files.\n+                case tar.TypeXGlobalHeader, tar.TypeXHeader:\n+                        continue\n+                default:\n+                        return errors.Errorf(\"unknown type: %b in %s\", header.Typeflag, header.Name)\n+                }\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2025-24366:0708", "fix_patch": "diff --git a/internal/sftpd/ssh_cmd.go b/internal/sftpd/ssh_cmd.go\nindex 8a342e99..13ef114a 100644\n--- a/internal/sftpd/ssh_cmd.go\n+++ b/internal/sftpd/ssh_cmd.go\n@@ -15,598 +15,715 @@\n package sftpd\n \n import (\n-\t\"crypto/md5\"\n-\t\"crypto/sha1\"\n-\t\"crypto/sha256\"\n-\t\"crypto/sha512\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"hash\"\n-\t\"io\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path\"\n-\t\"runtime/debug\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\t\"github.com/google/shlex\"\n-\t\"github.com/sftpgo/sdk\"\n-\t\"golang.org/x/crypto/ssh\"\n-\n-\t\"github.com/drakkan/sftpgo/v2/internal/common\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/logger\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/metric\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/util\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/vfs\"\n+        \"crypto/md5\"\n+        \"crypto/sha1\"\n+        \"crypto/sha256\"\n+        \"crypto/sha512\"\n+        \"errors\"\n+        \"fmt\"\n+        \"hash\"\n+        \"io\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path\"\n+        \"runtime/debug\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        \"github.com/google/shlex\"\n+        \"github.com/sftpgo/sdk\"\n+\"fmt\"\n+\n+        \"golang.org/x/crypto/ssh\"\n+\n+        \"github.com/drakkan/sftpgo/v2/internal/common\"\n+        \"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n+        \"github.com/drakkan/sftpgo/v2/internal/logger\"\n+        \"github.com/drakkan/sftpgo/v2/internal/metric\"\n+        \"github.com/drakkan/sftpgo/v2/internal/util\"\n+        \"github.com/drakkan/sftpgo/v2/internal/vfs\"\n )\n \n const (\n-\tscpCmdName          = \"scp\"\n-\tsshCommandLogSender = \"SSHCommand\"\n+        scpCmdName          = \"scp\"\n+        sshCommandLogSender = \"SSHCommand\"\n )\n \n var (\n-\terrUnsupportedConfig = errors.New(\"command unsupported for this configuration\")\n+        errUnsupportedConfig = errors.New(\"command unsupported for this configuration\")\n )\n \n type sshCommand struct {\n-\tcommand    string\n-\targs       []string\n-\tconnection *Connection\n-\tstartTime  time.Time\n+        command    string\n+        args       []string\n+        connection *Connection\n+        startTime  time.Time\n }\n \n type systemCommand struct {\n-\tcmd            *exec.Cmd\n-\tfsPath         string\n-\tquotaCheckPath string\n-\tfs             vfs.Fs\n+        cmd            *exec.Cmd\n+        fsPath         string\n+        quotaCheckPath string\n+        fs             vfs.Fs\n }\n \n func (c *systemCommand) GetSTDs() (io.WriteCloser, io.ReadCloser, io.ReadCloser, error) {\n-\tstdin, err := c.cmd.StdinPipe()\n-\tif err != nil {\n-\t\treturn nil, nil, nil, err\n-\t}\n-\tstdout, err := c.cmd.StdoutPipe()\n-\tif err != nil {\n-\t\tstdin.Close()\n-\t\treturn nil, nil, nil, err\n-\t}\n-\tstderr, err := c.cmd.StderrPipe()\n-\tif err != nil {\n-\t\tstdin.Close()\n-\t\tstdout.Close()\n-\t\treturn nil, nil, nil, err\n-\t}\n-\treturn stdin, stdout, stderr, nil\n+        stdin, err := c.cmd.StdinPipe()\n+        if err != nil {\n+                return nil, nil, nil, err\n+        }\n+        stdout, err := c.cmd.StdoutPipe()\n+        if err != nil {\n+                stdin.Close()\n+                return nil, nil, nil, err\n+        }\n+        stderr, err := c.cmd.StderrPipe()\n+        if err != nil {\n+                stdin.Close()\n+                stdout.Close()\n+                return nil, nil, nil, err\n+        }\n+        return stdin, stdout, stderr, nil\n }\n \n func processSSHCommand(payload []byte, connection *Connection, enabledSSHCommands []string) bool {\n-\tvar msg sshSubsystemExecMsg\n-\tif err := ssh.Unmarshal(payload, &msg); err == nil {\n-\t\tname, args, err := parseCommandPayload(msg.Command)\n-\t\tconnection.Log(logger.LevelDebug, \"new ssh command: %q args: %v num args: %d user: %s, error: %v\",\n-\t\t\tname, args, len(args), connection.User.Username, err)\n-\t\tif err == nil && util.Contains(enabledSSHCommands, name) {\n-\t\t\tconnection.command = msg.Command\n-\t\t\tif name == scpCmdName && len(args) >= 2 {\n-\t\t\t\tconnection.SetProtocol(common.ProtocolSCP)\n-\t\t\t\tscpCommand := scpCommand{\n-\t\t\t\t\tsshCommand: sshCommand{\n-\t\t\t\t\t\tcommand:    name,\n-\t\t\t\t\t\tconnection: connection,\n-\t\t\t\t\t\tstartTime:  time.Now(),\n-\t\t\t\t\t\targs:       args},\n-\t\t\t\t}\n-\t\t\t\tgo scpCommand.handle() //nolint:errcheck\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t\tif name != scpCmdName {\n-\t\t\t\tconnection.SetProtocol(common.ProtocolSSH)\n-\t\t\t\tsshCommand := sshCommand{\n-\t\t\t\t\tcommand:    name,\n-\t\t\t\t\tconnection: connection,\n-\t\t\t\t\tstartTime:  time.Now(),\n-\t\t\t\t\targs:       args,\n-\t\t\t\t}\n-\t\t\t\tgo sshCommand.handle() //nolint:errcheck\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t} else {\n-\t\t\tconnection.Log(logger.LevelInfo, \"ssh command not enabled/supported: %q\", name)\n-\t\t}\n-\t}\n-\terr := connection.CloseFS()\n-\tconnection.Log(logger.LevelError, \"unable to unmarshal ssh command, close fs, err: %v\", err)\n-\treturn false\n+        var msg sshSubsystemExecMsg\n+        if err := ssh.Unmarshal(payload, &msg); err == nil {\n+                name, args, err := parseCommandPayload(msg.Command)\n+                connection.Log(logger.LevelDebug, \"new ssh command: %q args: %v num args: %d user: %s, error: %v\",\n+                        name, args, len(args), connection.User.Username, err)\n+                if err == nil && util.Contains(enabledSSHCommands, name) {\n+                        connection.command = msg.Command\n+                        if name == scpCmdName && len(args) >= 2 {\n+                                connection.SetProtocol(common.ProtocolSCP)\n+                                scpCommand := scpCommand{\n+                                        sshCommand: sshCommand{\n+                                                command:    name,\n+                                                connection: connection,\n+                                                startTime:  time.Now(),\n+                                                args:       args},\n+                                }\n+                                go scpCommand.handle() //nolint:errcheck\n+                                return true\n+                        }\n+                        if name != scpCmdName {\n+                                connection.SetProtocol(common.ProtocolSSH)\n+                                sshCommand := sshCommand{\n+                                        command:    name,\n+                                        connection: connection,\n+                                        startTime:  time.Now(),\n+                                        args:       args,\n+                                }\n+                                go sshCommand.handle() //nolint:errcheck\n+                                return true\n+                        }\n+                } else {\n+                        connection.Log(logger.LevelInfo, \"ssh command not enabled/supported: %q\", name)\n+                }\n+        }\n+        err := connection.CloseFS()\n+        connection.Log(logger.LevelError, \"unable to unmarshal ssh command, close fs, err: %v\", err)\n+        return false\n }\n \n func (c *sshCommand) handle() (err error) {\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlogger.Error(logSender, \"\", \"panic in handle ssh command: %q stack trace: %v\", r, string(debug.Stack()))\n-\t\t\terr = common.ErrGenericFailure\n-\t\t}\n-\t}()\n-\tif err := common.Connections.Add(c.connection); err != nil {\n-\t\tlogger.Info(logSender, \"\", \"unable to add SSH command connection: %v\", err)\n-\t\treturn err\n-\t}\n-\tdefer common.Connections.Remove(c.connection.GetID())\n-\n-\tc.connection.UpdateLastActivity()\n-\tif util.Contains(sshHashCommands, c.command) {\n-\t\treturn c.handleHashCommands()\n-\t} else if util.Contains(systemCommands, c.command) {\n-\t\tcommand, err := c.getSystemCommand()\n-\t\tif err != nil {\n-\t\t\treturn c.sendErrorResponse(err)\n-\t\t}\n-\t\treturn c.executeSystemCommand(command)\n-\t} else if c.command == \"cd\" {\n-\t\tc.sendExitStatus(nil)\n-\t} else if c.command == \"pwd\" {\n-\t\t// hard coded response to the start directory\n-\t\tc.connection.channel.Write([]byte(util.CleanPath(c.connection.User.Filters.StartDirectory) + \"\\n\")) //nolint:errcheck\n-\t\tc.sendExitStatus(nil)\n-\t} else if c.command == \"sftpgo-copy\" {\n-\t\treturn c.handleSFTPGoCopy()\n-\t} else if c.command == \"sftpgo-remove\" {\n-\t\treturn c.handleSFTPGoRemove()\n-\t}\n-\treturn\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        logger.Error(logSender, \"\", \"panic in handle ssh command: %q stack trace: %v\", r, string(debug.Stack()))\n+                        err = common.ErrGenericFailure\n+                }\n+        }()\n+        if err := common.Connections.Add(c.connection); err != nil {\n+                logger.Info(logSender, \"\", \"unable to add SSH command connection: %v\", err)\n+                return err\n+        }\n+        defer common.Connections.Remove(c.connection.GetID())\n+\n+        c.connection.UpdateLastActivity()\n+        if util.Contains(sshHashCommands, c.command) {\n+                return c.handleHashCommands()\n+        } else if util.Contains(systemCommands, c.command) {\n+                command, err := c.getSystemCommand()\n+                if err != nil {\n+                        return c.sendErrorResponse(err)\n+                }\n+                return c.executeSystemCommand(command)\n+        } else if c.command == \"cd\" {\n+                c.sendExitStatus(nil)\n+        } else if c.command == \"pwd\" {\n+                // hard coded response to the start directory\n+                c.connection.channel.Write([]byte(util.CleanPath(c.connection.User.Filters.StartDirectory) + \"\\n\")) //nolint:errcheck\n+                c.sendExitStatus(nil)\n+        } else if c.command == \"sftpgo-copy\" {\n+                return c.handleSFTPGoCopy()\n+        } else if c.command == \"sftpgo-remove\" {\n+                return c.handleSFTPGoRemove()\n+        }\n+        return\n }\n \n func (c *sshCommand) handleSFTPGoCopy() error {\n-\tsshSourcePath := c.getSourcePath()\n-\tsshDestPath := c.getDestPath()\n-\tif sshSourcePath == \"\" || sshDestPath == \"\" || len(c.args) != 2 {\n-\t\treturn c.sendErrorResponse(errors.New(\"usage sftpgo-copy <source dir path> <destination dir path>\"))\n-\t}\n-\tc.connection.Log(logger.LevelDebug, \"requested copy %q -> %q\", sshSourcePath, sshDestPath)\n-\tif err := c.connection.Copy(sshSourcePath, sshDestPath); err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\tc.connection.channel.Write([]byte(\"OK\\n\")) //nolint:errcheck\n-\tc.sendExitStatus(nil)\n-\treturn nil\n+        sshSourcePath := c.getSourcePath()\n+        sshDestPath := c.getDestPath()\n+        if sshSourcePath == \"\" || sshDestPath == \"\" || len(c.args) != 2 {\n+                return c.sendErrorResponse(errors.New(\"usage sftpgo-copy <source dir path> <destination dir path>\"))\n+        }\n+        c.connection.Log(logger.LevelDebug, \"requested copy %q -> %q\", sshSourcePath, sshDestPath)\n+        if err := c.connection.Copy(sshSourcePath, sshDestPath); err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+        c.connection.channel.Write([]byte(\"OK\\n\")) //nolint:errcheck\n+        c.sendExitStatus(nil)\n+        return nil\n }\n \n func (c *sshCommand) handleSFTPGoRemove() error {\n-\tsshDestPath, err := c.getRemovePath()\n-\tif err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\tif err := c.connection.RemoveAll(sshDestPath); err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\tc.connection.channel.Write([]byte(\"OK\\n\")) //nolint:errcheck\n-\tc.sendExitStatus(nil)\n-\treturn nil\n+        sshDestPath, err := c.getRemovePath()\n+        if err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+        if err := c.connection.RemoveAll(sshDestPath); err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+        c.connection.channel.Write([]byte(\"OK\\n\")) //nolint:errcheck\n+        c.sendExitStatus(nil)\n+        return nil\n }\n \n func (c *sshCommand) updateQuota(sshDestPath string, filesNum int, filesSize int64) {\n-\tvfolder, err := c.connection.User.GetVirtualFolderForPath(sshDestPath)\n-\tif err == nil {\n-\t\tdataprovider.UpdateVirtualFolderQuota(&vfolder.BaseVirtualFolder, filesNum, filesSize, false) //nolint:errcheck\n-\t\tif vfolder.IsIncludedInUserQuota() {\n-\t\t\tdataprovider.UpdateUserQuota(&c.connection.User, filesNum, filesSize, false) //nolint:errcheck\n-\t\t}\n-\t} else {\n-\t\tdataprovider.UpdateUserQuota(&c.connection.User, filesNum, filesSize, false) //nolint:errcheck\n-\t}\n+        vfolder, err := c.connection.User.GetVirtualFolderForPath(sshDestPath)\n+        if err == nil {\n+                dataprovider.UpdateVirtualFolderQuota(&vfolder.BaseVirtualFolder, filesNum, filesSize, false) //nolint:errcheck\n+                if vfolder.IsIncludedInUserQuota() {\n+                        dataprovider.UpdateUserQuota(&c.connection.User, filesNum, filesSize, false) //nolint:errcheck\n+                }\n+        } else {\n+                dataprovider.UpdateUserQuota(&c.connection.User, filesNum, filesSize, false) //nolint:errcheck\n+        }\n }\n \n func (c *sshCommand) handleHashCommands() error {\n-\tvar h hash.Hash\n-\tif c.command == \"md5sum\" {\n-\t\th = md5.New()\n-\t} else if c.command == \"sha1sum\" {\n-\t\th = sha1.New()\n-\t} else if c.command == \"sha256sum\" {\n-\t\th = sha256.New()\n-\t} else if c.command == \"sha384sum\" {\n-\t\th = sha512.New384()\n-\t} else {\n-\t\th = sha512.New()\n-\t}\n-\tvar response string\n-\tif len(c.args) == 0 {\n-\t\t// without args we need to read the string to hash from stdin\n-\t\tbuf := make([]byte, 4096)\n-\t\tn, err := c.connection.channel.Read(buf)\n-\t\tif err != nil && err != io.EOF {\n-\t\t\treturn c.sendErrorResponse(err)\n-\t\t}\n-\t\th.Write(buf[:n]) //nolint:errcheck\n-\t\tresponse = fmt.Sprintf(\"%x  -\\n\", h.Sum(nil))\n-\t} else {\n-\t\tsshPath := c.getDestPath()\n-\t\tif ok, policy := c.connection.User.IsFileAllowed(sshPath); !ok {\n-\t\t\tc.connection.Log(logger.LevelInfo, \"hash not allowed for file %q\", sshPath)\n-\t\t\treturn c.sendErrorResponse(c.connection.GetErrorForDeniedFile(policy))\n-\t\t}\n-\t\tfs, fsPath, err := c.connection.GetFsAndResolvedPath(sshPath)\n-\t\tif err != nil {\n-\t\t\treturn c.sendErrorResponse(err)\n-\t\t}\n-\t\tif !c.connection.User.HasPerm(dataprovider.PermListItems, sshPath) {\n-\t\t\treturn c.sendErrorResponse(c.connection.GetPermissionDeniedError())\n-\t\t}\n-\t\thash, err := c.computeHashForFile(fs, h, fsPath)\n-\t\tif err != nil {\n-\t\t\treturn c.sendErrorResponse(c.connection.GetFsError(fs, err))\n-\t\t}\n-\t\tresponse = fmt.Sprintf(\"%v  %v\\n\", hash, sshPath)\n-\t}\n-\tc.connection.channel.Write([]byte(response)) //nolint:errcheck\n-\tc.sendExitStatus(nil)\n-\treturn nil\n+        var h hash.Hash\n+        if c.command == \"md5sum\" {\n+                h = md5.New()\n+        } else if c.command == \"sha1sum\" {\n+                h = sha1.New()\n+        } else if c.command == \"sha256sum\" {\n+                h = sha256.New()\n+        } else if c.command == \"sha384sum\" {\n+                h = sha512.New384()\n+        } else {\n+                h = sha512.New()\n+        }\n+        var response string\n+        if len(c.args) == 0 {\n+                // without args we need to read the string to hash from stdin\n+                buf := make([]byte, 4096)\n+                n, err := c.connection.channel.Read(buf)\n+                if err != nil && err != io.EOF {\n+                        return c.sendErrorResponse(err)\n+                }\n+                h.Write(buf[:n]) //nolint:errcheck\n+                response = fmt.Sprintf(\"%x  -\\n\", h.Sum(nil))\n+        } else {\n+                sshPath := c.getDestPath()\n+                if ok, policy := c.connection.User.IsFileAllowed(sshPath); !ok {\n+                        c.connection.Log(logger.LevelInfo, \"hash not allowed for file %q\", sshPath)\n+                        return c.sendErrorResponse(c.connection.GetErrorForDeniedFile(policy))\n+                }\n+                fs, fsPath, err := c.connection.GetFsAndResolvedPath(sshPath)\n+                if err != nil {\n+                        return c.sendErrorResponse(err)\n+                }\n+                if !c.connection.User.HasPerm(dataprovider.PermListItems, sshPath) {\n+                        return c.sendErrorResponse(c.connection.GetPermissionDeniedError())\n+                }\n+                hash, err := c.computeHashForFile(fs, h, fsPath)\n+                if err != nil {\n+                        return c.sendErrorResponse(c.connection.GetFsError(fs, err))\n+                }\n+                response = fmt.Sprintf(\"%v  %v\\n\", hash, sshPath)\n+        }\n+        c.connection.channel.Write([]byte(response)) //nolint:errcheck\n+        c.sendExitStatus(nil)\n+        return nil\n }\n \n func (c *sshCommand) executeSystemCommand(command systemCommand) error {\n-\tsshDestPath := c.getDestPath()\n-\tif !c.isLocalPath(sshDestPath) {\n-\t\treturn c.sendErrorResponse(errUnsupportedConfig)\n-\t}\n-\tdiskQuota, transferQuota := c.connection.HasSpace(true, false, command.quotaCheckPath)\n-\tif !diskQuota.HasSpace || !transferQuota.HasUploadSpace() || !transferQuota.HasDownloadSpace() {\n-\t\treturn c.sendErrorResponse(common.ErrQuotaExceeded)\n-\t}\n-\tperms := []string{dataprovider.PermDownload, dataprovider.PermUpload, dataprovider.PermCreateDirs, dataprovider.PermListItems,\n-\t\tdataprovider.PermOverwrite, dataprovider.PermDelete}\n-\tif !c.connection.User.HasPerms(perms, sshDestPath) {\n-\t\treturn c.sendErrorResponse(c.connection.GetPermissionDeniedError())\n-\t}\n-\n-\tinitialFiles, initialSize, err := c.getSizeForPath(command.fs, command.fsPath)\n-\tif err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\n-\tstdin, stdout, stderr, err := command.GetSTDs()\n-\tif err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\terr = command.cmd.Start()\n-\tif err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\n-\tcloseCmdOnError := func() {\n-\t\tc.connection.Log(logger.LevelDebug, \"kill cmd: %q and close ssh channel after read or write error\",\n-\t\t\tc.connection.command)\n-\t\tkillerr := command.cmd.Process.Kill()\n-\t\tcloserr := c.connection.channel.Close()\n-\t\tc.connection.Log(logger.LevelDebug, \"kill cmd error: %v close channel error: %v\", killerr, closerr)\n-\t}\n-\tvar once sync.Once\n-\tcommandResponse := make(chan bool)\n-\n-\tremainingQuotaSize := diskQuota.GetRemainingSize()\n-\n-\tgo func() {\n-\t\tdefer stdin.Close()\n-\t\tbaseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n-\t\t\tcommon.TransferUpload, 0, 0, remainingQuotaSize, 0, false, command.fs, transferQuota)\n-\t\ttransfer := newTransfer(baseTransfer, nil, nil, nil)\n-\n-\t\tw, e := transfer.copyFromReaderToWriter(stdin, c.connection.channel)\n-\t\tc.connection.Log(logger.LevelDebug, \"command: %q, copy from remote command to sdtin ended, written: %v, \"+\n-\t\t\t\"initial remaining quota: %v, err: %v\", c.connection.command, w, remainingQuotaSize, e)\n-\t\tif e != nil {\n-\t\t\tonce.Do(closeCmdOnError)\n-\t\t}\n-\t}()\n-\n-\tgo func() {\n-\t\tbaseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n-\t\t\tcommon.TransferDownload, 0, 0, 0, 0, false, command.fs, transferQuota)\n-\t\ttransfer := newTransfer(baseTransfer, nil, nil, nil)\n-\n-\t\tw, e := transfer.copyFromReaderToWriter(c.connection.channel, stdout)\n-\t\tc.connection.Log(logger.LevelDebug, \"command: %q, copy from sdtout to remote command ended, written: %v err: %v\",\n-\t\t\tc.connection.command, w, e)\n-\t\tif e != nil {\n-\t\t\tonce.Do(closeCmdOnError)\n-\t\t}\n-\t\tcommandResponse <- true\n-\t}()\n-\n-\tgo func() {\n-\t\tbaseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n-\t\t\tcommon.TransferDownload, 0, 0, 0, 0, false, command.fs, transferQuota)\n-\t\ttransfer := newTransfer(baseTransfer, nil, nil, nil)\n-\n-\t\tw, e := transfer.copyFromReaderToWriter(c.connection.channel.(ssh.Channel).Stderr(), stderr)\n-\t\tc.connection.Log(logger.LevelDebug, \"command: %q, copy from sdterr to remote command ended, written: %v err: %v\",\n-\t\t\tc.connection.command, w, e)\n-\t\t// os.ErrClosed means that the command is finished so we don't need to do anything\n-\t\tif (e != nil && !errors.Is(e, os.ErrClosed)) || w > 0 {\n-\t\t\tonce.Do(closeCmdOnError)\n-\t\t}\n-\t}()\n-\n-\t<-commandResponse\n-\terr = command.cmd.Wait()\n-\tc.sendExitStatus(err)\n-\n-\tnumFiles, dirSize, errSize := c.getSizeForPath(command.fs, command.fsPath)\n-\tif errSize == nil {\n-\t\tc.updateQuota(sshDestPath, numFiles-initialFiles, dirSize-initialSize)\n-\t}\n-\tc.connection.Log(logger.LevelDebug, \"command %q finished for path %q, initial files %v initial size %v \"+\n-\t\t\"current files %v current size %v size err: %v\", c.connection.command, command.fsPath, initialFiles, initialSize,\n-\t\tnumFiles, dirSize, errSize)\n-\treturn c.connection.GetFsError(command.fs, err)\n+        sshDestPath := c.getDestPath()\n+        if !c.isLocalPath(sshDestPath) {\n+                return c.sendErrorResponse(errUnsupportedConfig)\n+        }\n+        diskQuota, transferQuota := c.connection.HasSpace(true, false, command.quotaCheckPath)\n+        if !diskQuota.HasSpace || !transferQuota.HasUploadSpace() || !transferQuota.HasDownloadSpace() {\n+                return c.sendErrorResponse(common.ErrQuotaExceeded)\n+        }\n+        perms := []string{dataprovider.PermDownload, dataprovider.PermUpload, dataprovider.PermCreateDirs, dataprovider.PermListItems,\n+                dataprovider.PermOverwrite, dataprovider.PermDelete}\n+        if !c.connection.User.HasPerms(perms, sshDestPath) {\n+                return c.sendErrorResponse(c.connection.GetPermissionDeniedError())\n+        }\n+\n+        initialFiles, initialSize, err := c.getSizeForPath(command.fs, command.fsPath)\n+        if err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+\n+        stdin, stdout, stderr, err := command.GetSTDs()\n+        if err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+        err = command.cmd.Start()\n+        if err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+\n+        closeCmdOnError := func() {\n+                c.connection.Log(logger.LevelDebug, \"kill cmd: %q and close ssh channel after read or write error\",\n+                        c.connection.command)\n+                killerr := command.cmd.Process.Kill()\n+                closerr := c.connection.channel.Close()\n+                c.connection.Log(logger.LevelDebug, \"kill cmd error: %v close channel error: %v\", killerr, closerr)\n+        }\n+        var once sync.Once\n+        commandResponse := make(chan bool)\n+\n+        remainingQuotaSize := diskQuota.GetRemainingSize()\n+\n+        go func() {\n+                defer stdin.Close()\n+                baseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n+                        common.TransferUpload, 0, 0, remainingQuotaSize, 0, false, command.fs, transferQuota)\n+                transfer := newTransfer(baseTransfer, nil, nil, nil)\n+\n+                w, e := transfer.copyFromReaderToWriter(stdin, c.connection.channel)\n+                c.connection.Log(logger.LevelDebug, \"command: %q, copy from remote command to sdtin ended, written: %v, \"+\n+                        \"initial remaining quota: %v, err: %v\", c.connection.command, w, remainingQuotaSize, e)\n+                if e != nil {\n+                        once.Do(closeCmdOnError)\n+                }\n+        }()\n+\n+        go func() {\n+                baseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n+                        common.TransferDownload, 0, 0, 0, 0, false, command.fs, transferQuota)\n+                transfer := newTransfer(baseTransfer, nil, nil, nil)\n+\n+                w, e := transfer.copyFromReaderToWriter(c.connection.channel, stdout)\n+                c.connection.Log(logger.LevelDebug, \"command: %q, copy from sdtout to remote command ended, written: %v err: %v\",\n+                        c.connection.command, w, e)\n+                if e != nil {\n+                        once.Do(closeCmdOnError)\n+                }\n+                commandResponse <- true\n+        }()\n+\n+        go func() {\n+                baseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n+                        common.TransferDownload, 0, 0, 0, 0, false, command.fs, transferQuota)\n+                transfer := newTransfer(baseTransfer, nil, nil, nil)\n+\n+                w, e := transfer.copyFromReaderToWriter(c.connection.channel.(ssh.Channel).Stderr(), stderr)\n+                c.connection.Log(logger.LevelDebug, \"command: %q, copy from sdterr to remote command ended, written: %v err: %v\",\n+                        c.connection.command, w, e)\n+                // os.ErrClosed means that the command is finished so we don't need to do anything\n+                if (e != nil && !errors.Is(e, os.ErrClosed)) || w > 0 {\n+                        once.Do(closeCmdOnError)\n+                }\n+        }()\n+\n+        <-commandResponse\n+        err = command.cmd.Wait()\n+        c.sendExitStatus(err)\n+\n+        numFiles, dirSize, errSize := c.getSizeForPath(command.fs, command.fsPath)\n+        if errSize == nil {\n+                c.updateQuota(sshDestPath, numFiles-initialFiles, dirSize-initialSize)\n+        }\n+        c.connection.Log(logger.LevelDebug, \"command %q finished for path %q, initial files %v initial size %v \"+\n+                \"current files %v current size %v size err: %v\", c.connection.command, command.fsPath, initialFiles, initialSize,\n+                numFiles, dirSize, errSize)\n+        return c.connection.GetFsError(command.fs, err)\n }\n \n func (c *sshCommand) isSystemCommandAllowed() error {\n-\tsshDestPath := c.getDestPath()\n-\tif c.connection.User.IsVirtualFolder(sshDestPath) {\n-\t\t// overlapped virtual path are not allowed\n-\t\treturn nil\n-\t}\n-\tif c.connection.User.HasVirtualFoldersInside(sshDestPath) {\n-\t\tc.connection.Log(logger.LevelDebug, \"command %q is not allowed, path %q has virtual folders inside it, user %q\",\n-\t\t\tc.command, sshDestPath, c.connection.User.Username)\n-\t\treturn errUnsupportedConfig\n-\t}\n-\tfor _, f := range c.connection.User.Filters.FilePatterns {\n-\t\tif f.Path == sshDestPath {\n-\t\t\tc.connection.Log(logger.LevelDebug,\n-\t\t\t\t\"command %q is not allowed inside folders with file patterns filters %q user %q\",\n-\t\t\t\tc.command, sshDestPath, c.connection.User.Username)\n-\t\t\treturn errUnsupportedConfig\n-\t\t}\n-\t\tif len(sshDestPath) > len(f.Path) {\n-\t\t\tif strings.HasPrefix(sshDestPath, f.Path+\"/\") || f.Path == \"/\" {\n-\t\t\t\tc.connection.Log(logger.LevelDebug,\n-\t\t\t\t\t\"command %q is not allowed it includes folders with file patterns filters %q user %q\",\n-\t\t\t\t\tc.command, sshDestPath, c.connection.User.Username)\n-\t\t\t\treturn errUnsupportedConfig\n-\t\t\t}\n-\t\t}\n-\t\tif len(sshDestPath) < len(f.Path) {\n-\t\t\tif strings.HasPrefix(sshDestPath+\"/\", f.Path) || sshDestPath == \"/\" {\n-\t\t\t\tc.connection.Log(logger.LevelDebug,\n-\t\t\t\t\t\"command %q is not allowed inside folder with file patterns filters %q user %q\",\n-\t\t\t\t\tc.command, sshDestPath, c.connection.User.Username)\n-\t\t\t\treturn errUnsupportedConfig\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        sshDestPath := c.getDestPath()\n+        if c.connection.User.IsVirtualFolder(sshDestPath) {\n+                // overlapped virtual path are not allowed\n+                return nil\n+        }\n+        if c.connection.User.HasVirtualFoldersInside(sshDestPath) {\n+                c.connection.Log(logger.LevelDebug, \"command %q is not allowed, path %q has virtual folders inside it, user %q\",\n+                        c.command, sshDestPath, c.connection.User.Username)\n+                return errUnsupportedConfig\n+        }\n+        for _, f := range c.connection.User.Filters.FilePatterns {\n+                if f.Path == sshDestPath {\n+                        c.connection.Log(logger.LevelDebug,\n+                                \"command %q is not allowed inside folders with file patterns filters %q user %q\",\n+                                c.command, sshDestPath, c.connection.User.Username)\n+                        return errUnsupportedConfig\n+                }\n+                if len(sshDestPath) > len(f.Path) {\n+                        if strings.HasPrefix(sshDestPath, f.Path+\"/\") || f.Path == \"/\" {\n+                                c.connection.Log(logger.LevelDebug,\n+                                        \"command %q is not allowed it includes folders with file patterns filters %q user %q\",\n+                                        c.command, sshDestPath, c.connection.User.Username)\n+                                return errUnsupportedConfig\n+                        }\n+                }\n+                if len(sshDestPath) < len(f.Path) {\n+                        if strings.HasPrefix(sshDestPath+\"/\", f.Path) || sshDestPath == \"/\" {\n+                                c.connection.Log(logger.LevelDebug,\n+                                        \"command %q is not allowed inside folder with file patterns filters %q user %q\",\n+                                        c.command, sshDestPath, c.connection.User.Username)\n+                                return errUnsupportedConfig\n+                        }\n+                }\n+        }\n+        return nil\n+}\n+\n+var allowedRsyncArgs = []string{\n+\"-v\", \"--verbose\",\n+\"-q\", \"--quiet\",\n+\"-c\", \"--checksum\",\n+\"-a\", \"--archive\",\n+\"-r\", \"--recursive\",\n+\"-R\", \"--relative\",\n+\"-u\", \"--update\",\n+\"-d\", \"--dirs\",\n+\"-t\", \"--times\",\n+\"-O\", \"--omit-dir-times\",\n+\"-g\", \"--group\",\n+\"--devices\",\n+\"--specials\",\n+\"-D\",\n+\"-l\", \"--links\",\n+\"-L\", \"--copy-links\",\n+\"--copy-unsafe-links\",\n+\"--safe-links\",\n+\"-k\", \"--copy-dirlinks\",\n+\"-H\", \"--hard-links\",\n+\"-p\", \"--perms\",\n+\"-E\", \"--executability\",\n+\"--chmod\",\n+\"-o\", \"--owner\",\n+\"-S\", \"--sparse\",\n+\"--inplace\",\n+\"--append\",\n+\"--append-verify\",\n+\"--existing\",\n+\"--ignore-existing\",\n+\"--remove-source-files\",\n+\"--delete\",\n+\"--delete-before\",\n+\"--delete-during\",\n+\"--delete-delay\",\n+\"--delete-after\",\n+\"--delete-excluded\",\n+\"--force\",\n+\"--ignore-errors\",\n+\"--ignore-times\",\n+\"--size-only\",\n+\"-I\",\n+\"-z\", \"--compress\",\n+\"--compress-level\",\n+\"--exclude\",\n+\"--exclude-from\",\n+\"--include\",\n+\"--include-from\",\n+\"--cvs-exclude\",\n+\"-C\",\n+\"--progress\",\n+\"-P\",\n+\"--stats\",\n+\"-h\", \"--human-readable\",\n+\"--version\",\n+\"--munge-links\",\n+\".\",\n+\"--server\",\n+\"-e\",\n+\"-i\",\n+\"-s\",\n+\"-f\",\n+\"-x\",\n }\n \n func (c *sshCommand) getSystemCommand() (systemCommand, error) {\n-\tcommand := systemCommand{\n-\t\tcmd:            nil,\n-\t\tfs:             nil,\n-\t\tfsPath:         \"\",\n-\t\tquotaCheckPath: \"\",\n-\t}\n-\tif err := common.CheckClosing(); err != nil {\n-\t\treturn command, err\n-\t}\n-\targs := make([]string, len(c.args))\n-\tcopy(args, c.args)\n-\tvar fsPath, quotaPath string\n-\tsshPath := c.getDestPath()\n-\tfs, err := c.connection.User.GetFilesystemForPath(sshPath, c.connection.ID)\n-\tif err != nil {\n-\t\treturn command, err\n-\t}\n-\tif len(c.args) > 0 {\n-\t\tvar err error\n-\t\tfsPath, err = fs.ResolvePath(sshPath)\n-\t\tif err != nil {\n-\t\t\treturn command, c.connection.GetFsError(fs, err)\n-\t\t}\n-\t\tquotaPath = sshPath\n-\t\tfi, err := fs.Stat(fsPath)\n-\t\tif err == nil && fi.IsDir() {\n-\t\t\t// if the target is an existing dir the command will write inside this dir\n-\t\t\t// so we need to check the quota for this directory and not its parent dir\n-\t\t\tquotaPath = path.Join(sshPath, \"fakecontent\")\n-\t\t}\n-\t\tif strings.HasSuffix(sshPath, \"/\") && !strings.HasSuffix(fsPath, string(os.PathSeparator)) {\n-\t\t\tfsPath += string(os.PathSeparator)\n-\t\t\tc.connection.Log(logger.LevelDebug, \"path separator added to fsPath %q\", fsPath)\n-\t\t}\n-\t\targs = args[:len(args)-1]\n-\t\targs = append(args, fsPath)\n-\t}\n-\tif err := c.isSystemCommandAllowed(); err != nil {\n-\t\treturn command, errUnsupportedConfig\n-\t}\n-\tif c.command == \"rsync\" {\n-\t\t// we cannot avoid that rsync creates symlinks so if the user has the permission\n-\t\t// to create symlinks we add the option --safe-links to the received rsync command if\n-\t\t// it is not already set. This should prevent to create symlinks that point outside\n-\t\t// the home dir.\n-\t\t// If the user cannot create symlinks we add the option --munge-links, if it is not\n-\t\t// already set. This should make symlinks unusable (but manually recoverable)\n-\t\tif c.connection.User.HasPerm(dataprovider.PermCreateSymlinks, c.getDestPath()) {\n-\t\t\tif !util.Contains(args, \"--safe-links\") {\n-\t\t\t\targs = append([]string{\"--safe-links\"}, args...)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif !util.Contains(args, \"--munge-links\") {\n-\t\t\t\targs = append([]string{\"--munge-links\"}, args...)\n-\t\t\t}\n-\t\t}\n-\t}\n-\tc.connection.Log(logger.LevelDebug, \"new system command %q, with args: %+v fs path %q quota check path %q\",\n-\t\tc.command, args, fsPath, quotaPath)\n-\tcmd := exec.Command(c.command, args...)\n-\tuid := c.connection.User.GetUID()\n-\tgid := c.connection.User.GetGID()\n-\tcmd = wrapCmd(cmd, uid, gid)\n-\tcommand.cmd = cmd\n-\tcommand.fsPath = fsPath\n-\tcommand.quotaCheckPath = quotaPath\n-\tcommand.fs = fs\n-\treturn command, nil\n+        command := systemCommand{\n+                cmd:            nil,\n+                fs:             nil,\n+                fsPath:         \"\",\n+                quotaCheckPath: \"\",\n+        }\n+        if err := common.CheckClosing(); err != nil {\n+                return command, err\n+        }\n+        args := make([]string, len(c.args))\n+        copy(args, c.args)\n+        var fsPath, quotaPath string\n+        sshPath := c.getDestPath()\n+        fs, err := c.connection.User.GetFilesystemForPath(sshPath, c.connection.ID)\n+        if err != nil {\n+                return command, err\n+        }\n+        if len(c.args) > 0 {\n+                var err error\n+                fsPath, err = fs.ResolvePath(sshPath)\n+                if err != nil {\n+                        return command, c.connection.GetFsError(fs, err)\n+                }\n+                quotaPath = sshPath\n+                fi, err := fs.Stat(fsPath)\n+                if err == nil && fi.IsDir() {\n+                        // if the target is an existing dir the command will write inside this dir\n+                        // so we need to check the quota for this directory and not its parent dir\n+                        quotaPath = path.Join(sshPath, \"fakecontent\")\n+                }\n+                if strings.HasSuffix(sshPath, \"/\") && !strings.HasSuffix(fsPath, string(os.PathSeparator)) {\n+                        fsPath += string(os.PathSeparator)\n+                        c.connection.Log(logger.LevelDebug, \"path separator added to fsPath %q\", fsPath)\n+                }\n+                args = args[:len(args)-1]\n+                args = append(args, fsPath)\n+        }\n+        if err := c.isSystemCommandAllowed(); err != nil {\n+                return command, errUnsupportedConfig\n+        }\n+        if c.command == \"rsync\" {\n+if !canAcceptRsyncArgs(c.args) {\n+return command, errors.New(\"unsupported rsync arguments\")\n+}\n+                // we cannot avoid that rsync creates symlinks so if the user has the permission\n+                // to create symlinks we add the option --safe-links to the received rsync command if\n+                // it is not already set. This should prevent to create symlinks that point outside\n+                // the home dir.\n+                // If the user cannot create symlinks we add the option --munge-links, if it is not\n+                // already set. This should make symlinks unusable (but manually recoverable)\n+                if c.connection.User.HasPerm(dataprovider.PermCreateSymlinks, c.getDestPath()) {\n+                        if !util.Contains(args, \"--safe-links\") {\n+                                args = append([]string{\"--safe-links\"}, args...)\n+                        }\n+                } else {\n+                        if !util.Contains(args, \"--munge-links\") {\n+                                args = append([]string{\"--munge-links\"}, args...)\n+                        }\n+                }\n+        }\n+        c.connection.Log(logger.LevelDebug, \"new system command %q, with args: %+v fs path %q quota check path %q\",\n+                c.command, args, fsPath, quotaPath)\n+        cmd := exec.Command(c.command, args...)\n+        uid := c.connection.User.GetUID()\n+        gid := c.connection.User.GetGID()\n+        cmd = wrapCmd(cmd, uid, gid)\n+        command.cmd = cmd\n+        command.fsPath = fsPath\n+        command.quotaCheckPath = quotaPath\n+        command.fs = fs\n+        return command, nil\n+}\n+\n+\n+// canAcceptRsyncArgs checks if the arguments for rsync are allowed.\n+// We only allow a subset of rsync options to prevent security issues\n+func canAcceptRsyncArgs(args []string) bool {\n+// based on the official rsync documentation\n+\n+for i, arg := range args {\n+if i == 0 {\n+// the first argument can be the path\n+continue\n+}\n+if util.Contains(allowedRsyncArgs, arg) {\n+continue\n+}\n+\n+if strings.HasPrefix(arg, \"-\") && !strings.HasPrefix(arg, \"--\") {\n+// short options can be combined, for example \"-vlog\"\n+// we must support something like \"-vlogDtprze.iLsfxC\"\n+if len(arg) > 2 {\n+for i := 1; i < len(arg); i++ {\n+\n+\n+if !util.Contains(allowedRsyncArgs, \"-\"+string(arg[i])) {\n+// could be a dot\n+if string(arg[i]) == \".\" {\n+continue\n+}\n+return false\n+}\n+}\n+continue\n+                                }\n+}\n+\n+// arguments with values, for example --chmod=ugo+rwx\n+if strings.HasPrefix(arg, \"--chmod=\") || strings.HasPrefix(arg, \"--compress-level=\") ||\n+strings.HasPrefix(arg, \"--exclude=\") || strings.HasPrefix(arg, \"--exclude-from=\") ||\n+strings.HasPrefix(arg, \"--include=\") || strings.HasPrefix(arg, \"--include-from=\") {\n+continue\n+}\n+return false\n+}\n+\n+return true\n }\n \n // for the supported commands, the destination path, if any, is the last argument\n func (c *sshCommand) getDestPath() string {\n-\tif len(c.args) == 0 {\n-\t\treturn \"\"\n-\t}\n-\treturn c.cleanCommandPath(c.args[len(c.args)-1])\n+        if len(c.args) == 0 {\n+                return \"\"\n+        }\n+        return c.cleanCommandPath(c.args[len(c.args)-1])\n }\n \n // for the supported commands, the destination path, if any, is the second-last argument\n func (c *sshCommand) getSourcePath() string {\n-\tif len(c.args) < 2 {\n-\t\treturn \"\"\n-\t}\n-\treturn c.cleanCommandPath(c.args[len(c.args)-2])\n+        if len(c.args) < 2 {\n+                return \"\"\n+        }\n+        return c.cleanCommandPath(c.args[len(c.args)-2])\n }\n \n func (c *sshCommand) cleanCommandPath(name string) string {\n-\tname = strings.Trim(name, \"'\")\n-\tname = strings.Trim(name, \"\\\"\")\n-\tresult := c.connection.User.GetCleanedPath(name)\n-\tif strings.HasSuffix(name, \"/\") && !strings.HasSuffix(result, \"/\") {\n-\t\tresult += \"/\"\n-\t}\n-\treturn result\n+        name = strings.Trim(name, \"'\")\n+        name = strings.Trim(name, \"\\\"\")\n+        result := c.connection.User.GetCleanedPath(name)\n+        if strings.HasSuffix(name, \"/\") && !strings.HasSuffix(result, \"/\") {\n+                result += \"/\"\n+        }\n+        return result\n }\n \n func (c *sshCommand) getRemovePath() (string, error) {\n-\tsshDestPath := c.getDestPath()\n-\tif sshDestPath == \"\" || len(c.args) != 1 {\n-\t\terr := errors.New(\"usage sftpgo-remove <destination path>\")\n-\t\treturn \"\", err\n-\t}\n-\tif len(sshDestPath) > 1 {\n-\t\tsshDestPath = strings.TrimSuffix(sshDestPath, \"/\")\n-\t}\n-\treturn sshDestPath, nil\n+        sshDestPath := c.getDestPath()\n+        if sshDestPath == \"\" || len(c.args) != 1 {\n+                err := errors.New(\"usage sftpgo-remove <destination path>\")\n+                return \"\", err\n+        }\n+        if len(sshDestPath) > 1 {\n+                sshDestPath = strings.TrimSuffix(sshDestPath, \"/\")\n+        }\n+        return sshDestPath, nil\n }\n \n func (c *sshCommand) isLocalPath(virtualPath string) bool {\n-\tfolder, err := c.connection.User.GetVirtualFolderForPath(virtualPath)\n-\tif err != nil {\n-\t\treturn c.connection.User.FsConfig.Provider == sdk.LocalFilesystemProvider\n-\t}\n-\treturn folder.FsConfig.Provider == sdk.LocalFilesystemProvider\n+        folder, err := c.connection.User.GetVirtualFolderForPath(virtualPath)\n+        if err != nil {\n+                return c.connection.User.FsConfig.Provider == sdk.LocalFilesystemProvider\n+        }\n+        return folder.FsConfig.Provider == sdk.LocalFilesystemProvider\n }\n \n func (c *sshCommand) getSizeForPath(fs vfs.Fs, name string) (int, int64, error) {\n-\tif dataprovider.GetQuotaTracking() > 0 {\n-\t\tfi, err := fs.Lstat(name)\n-\t\tif err != nil {\n-\t\t\tif fs.IsNotExist(err) {\n-\t\t\t\treturn 0, 0, nil\n-\t\t\t}\n-\t\t\tc.connection.Log(logger.LevelDebug, \"unable to stat %q error: %v\", name, err)\n-\t\t\treturn 0, 0, err\n-\t\t}\n-\t\tif fi.IsDir() {\n-\t\t\tfiles, size, err := fs.GetDirSize(name)\n-\t\t\tif err != nil {\n-\t\t\t\tc.connection.Log(logger.LevelDebug, \"unable to get size for dir %q error: %v\", name, err)\n-\t\t\t}\n-\t\t\treturn files, size, err\n-\t\t} else if fi.Mode().IsRegular() {\n-\t\t\treturn 1, fi.Size(), nil\n-\t\t}\n-\t}\n-\treturn 0, 0, nil\n+        if dataprovider.GetQuotaTracking() > 0 {\n+                fi, err := fs.Lstat(name)\n+                if err != nil {\n+                        if fs.IsNotExist(err) {\n+                                return 0, 0, nil\n+                        }\n+                        c.connection.Log(logger.LevelDebug, \"unable to stat %q error: %v\", name, err)\n+                        return 0, 0, err\n+                }\n+                if fi.IsDir() {\n+                        files, size, err := fs.GetDirSize(name)\n+                        if err != nil {\n+                                c.connection.Log(logger.LevelDebug, \"unable to get size for dir %q error: %v\", name, err)\n+                        }\n+                        return files, size, err\n+                } else if fi.Mode().IsRegular() {\n+                        return 1, fi.Size(), nil\n+                }\n+        }\n+        return 0, 0, nil\n }\n \n func (c *sshCommand) sendErrorResponse(err error) error {\n-\terrorString := fmt.Sprintf(\"%v: %v %v\\n\", c.command, c.getDestPath(), err)\n-\tc.connection.channel.Write([]byte(errorString)) //nolint:errcheck\n-\tc.sendExitStatus(err)\n-\treturn err\n+        errorString := fmt.Sprintf(\"%v: %v %v\\n\", c.command, c.getDestPath(), err)\n+        c.connection.channel.Write([]byte(errorString)) //nolint:errcheck\n+        c.sendExitStatus(err)\n+        return err\n }\n \n func (c *sshCommand) sendExitStatus(err error) {\n-\tstatus := uint32(0)\n-\tvCmdPath := c.getDestPath()\n-\tcmdPath := \"\"\n-\ttargetPath := \"\"\n-\tvTargetPath := \"\"\n-\tif c.command == \"sftpgo-copy\" {\n-\t\tvTargetPath = vCmdPath\n-\t\tvCmdPath = c.getSourcePath()\n-\t}\n-\tif err != nil {\n-\t\tstatus = uint32(1)\n-\t\tc.connection.Log(logger.LevelError, \"command failed: %q args: %v user: %s err: %v\",\n-\t\t\tc.command, c.args, c.connection.User.Username, err)\n-\t}\n-\texitStatus := sshSubsystemExitStatus{\n-\t\tStatus: status,\n-\t}\n-\t_, errClose := c.connection.channel.(ssh.Channel).SendRequest(\"exit-status\", false, ssh.Marshal(&exitStatus))\n-\tc.connection.Log(logger.LevelDebug, \"exit status sent, error: %v\", errClose)\n-\tc.connection.channel.Close()\n-\t// for scp we notify single uploads/downloads\n-\tif c.command != scpCmdName {\n-\t\telapsed := time.Since(c.startTime).Nanoseconds() / 1000000\n-\t\tmetric.SSHCommandCompleted(err)\n-\t\tif vCmdPath != \"\" {\n-\t\t\t_, p, errFs := c.connection.GetFsAndResolvedPath(vCmdPath)\n-\t\t\tif errFs == nil {\n-\t\t\t\tcmdPath = p\n-\t\t\t}\n-\t\t}\n-\t\tif vTargetPath != \"\" {\n-\t\t\t_, p, errFs := c.connection.GetFsAndResolvedPath(vTargetPath)\n-\t\t\tif errFs == nil {\n-\t\t\t\ttargetPath = p\n-\t\t\t}\n-\t\t}\n-\t\tcommon.ExecuteActionNotification(c.connection.BaseConnection, common.OperationSSHCmd, cmdPath, vCmdPath, //nolint:errcheck\n-\t\t\ttargetPath, vTargetPath, c.command, 0, err, elapsed, nil)\n-\t\tif err == nil {\n-\t\t\tlogger.CommandLog(sshCommandLogSender, cmdPath, targetPath, c.connection.User.Username, \"\", c.connection.ID,\n-\t\t\t\tcommon.ProtocolSSH, -1, -1, \"\", \"\", c.connection.command, -1, c.connection.GetLocalAddress(),\n-\t\t\t\tc.connection.GetRemoteAddress(), elapsed)\n-\t\t}\n-\t}\n+        status := uint32(0)\n+        vCmdPath := c.getDestPath()\n+        cmdPath := \"\"\n+        targetPath := \"\"\n+        vTargetPath := \"\"\n+        if c.command == \"sftpgo-copy\" {\n+                vTargetPath = vCmdPath\n+                vCmdPath = c.getSourcePath()\n+        }\n+        if err != nil {\n+                status = uint32(1)\n+                c.connection.Log(logger.LevelError, \"command failed: %q args: %v user: %s err: %v\",\n+                        c.command, c.args, c.connection.User.Username, err)\n+        }\n+        exitStatus := sshSubsystemExitStatus{\n+                Status: status,\n+        }\n+        _, errClose := c.connection.channel.(ssh.Channel).SendRequest(\"exit-status\", false, ssh.Marshal(&exitStatus))\n+        c.connection.Log(logger.LevelDebug, \"exit status sent, error: %v\", errClose)\n+        c.connection.channel.Close()\n+        // for scp we notify single uploads/downloads\n+        if c.command != scpCmdName {\n+                elapsed := time.Since(c.startTime).Nanoseconds() / 1000000\n+                metric.SSHCommandCompleted(err)\n+                if vCmdPath != \"\" {\n+                        _, p, errFs := c.connection.GetFsAndResolvedPath(vCmdPath)\n+                        if errFs == nil {\n+                                cmdPath = p\n+                        }\n+                }\n+                if vTargetPath != \"\" {\n+                        _, p, errFs := c.connection.GetFsAndResolvedPath(vTargetPath)\n+                        if errFs == nil {\n+                                targetPath = p\n+                        }\n+                }\n+                common.ExecuteActionNotification(c.connection.BaseConnection, common.OperationSSHCmd, cmdPath, vCmdPath, //nolint:errcheck\n+                        targetPath, vTargetPath, c.command, 0, err, elapsed, nil)\n+                if err == nil {\n+                        logger.CommandLog(sshCommandLogSender, cmdPath, targetPath, c.connection.User.Username, \"\", c.connection.ID,\n+                                common.ProtocolSSH, -1, -1, \"\", \"\", c.connection.command, -1, c.connection.GetLocalAddress(),\n+                                c.connection.GetRemoteAddress(), elapsed)\n+                }\n+        }\n }\n \n func (c *sshCommand) computeHashForFile(fs vfs.Fs, hasher hash.Hash, path string) (string, error) {\n-\thash := \"\"\n-\tf, r, _, err := fs.Open(path, 0)\n-\tif err != nil {\n-\t\treturn hash, err\n-\t}\n-\tvar reader io.ReadCloser\n-\tif f != nil {\n-\t\treader = f\n-\t} else {\n-\t\treader = r\n-\t}\n-\tdefer reader.Close()\n-\t_, err = io.Copy(hasher, reader)\n-\tif err == nil {\n-\t\thash = fmt.Sprintf(\"%x\", hasher.Sum(nil))\n-\t}\n-\treturn hash, err\n+        hash := \"\"\n+        f, r, _, err := fs.Open(path, 0)\n+        if err != nil {\n+                return hash, err\n+        }\n+        var reader io.ReadCloser\n+        if f != nil {\n+                reader = f\n+        } else {\n+                reader = r\n+        }\n+        defer reader.Close()\n+        _, err = io.Copy(hasher, reader)\n+        if err == nil {\n+                hash = fmt.Sprintf(\"%x\", hasher.Sum(nil))\n+        }\n+        return hash, err\n }\n \n func parseCommandPayload(command string) (string, []string, error) {\n-\tparts, err := shlex.Split(command)\n-\tif err == nil && len(parts) == 0 {\n-\t\terr = fmt.Errorf(\"invalid command: %q\", command)\n-\t}\n-\tif err != nil {\n-\t\treturn \"\", []string{}, err\n-\t}\n-\tif len(parts) < 2 {\n-\t\treturn parts[0], []string{}, nil\n-\t}\n-\treturn parts[0], parts[1:], nil\n+        parts, err := shlex.Split(command)\n+        if err == nil && len(parts) == 0 {\n+                err = fmt.Errorf(\"invalid command: %q\", command)\n+        }\n+        if err != nil {\n+                return \"\", []string{}, err\n+        }\n+        if len(parts) < 2 {\n+                return parts[0], []string{}, nil\n+        }\n+        return parts[0], parts[1:], nil\n }\n"}
{"cve":"CVE-2022-4643:0708", "fix_patch": "diff --git a/pdf_ocr.go b/pdf_ocr.go\nindex 4d9d53a..8fcc2c2 100644\n--- a/pdf_ocr.go\n+++ b/pdf_ocr.go\n@@ -3,159 +3,171 @@\n package docconv\n \n import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"io/ioutil\"\n-\t\"log\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\t\"sync\"\n+        \"fmt\"\n+        \"io\"\n+        \"io/ioutil\"\n+        \"log\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path/filepath\"\n+        \"strings\"\n+        \"sync\"\n )\n \n var (\n-\texts = []string{\".jpg\", \".tif\", \".tiff\", \".png\", \".pbm\"}\n+        exts = []string{\".jpg\", \".tif\", \".tiff\", \".png\", \".pbm\"}\n )\n \n func compareExt(ext string, exts []string) bool {\n-\tfor _, e := range exts {\n-\t\tif ext == e {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, e := range exts {\n+                if ext == e {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func cleanupTemp(tmpDir string) {\n-\terr := os.RemoveAll(tmpDir)\n-\tif err != nil {\n-\t\tlog.Println(err)\n-\t}\n+        err := os.RemoveAll(tmpDir)\n+        if err != nil {\n+                log.Println(err)\n+        }\n }\n \n func ConvertPDFImages(path string) (BodyResult, error) {\n-\tbodyResult := BodyResult{}\n+        bodyResult := BodyResult{}\n \n-\ttmp, err := ioutil.TempDir(os.TempDir(), \"tmp-imgs-\")\n-\tif err != nil {\n-\t\tbodyResult.err = err\n-\t\treturn bodyResult, err\n-\t}\n-\ttmpDir := fmt.Sprintf(\"%s/\", tmp)\n+        tmp, err := ioutil.TempDir(os.TempDir(), \"tmp-imgs-\")\n+        if err != nil {\n+                bodyResult.err = err\n+                return bodyResult, err\n+        }\n+        tmpDir := fmt.Sprintf(\"%s/\", tmp)\n \n-\tdefer cleanupTemp(tmpDir)\n+        defer cleanupTemp(tmpDir)\n \n-\t_, err = exec.Command(\"pdfimages\", \"-j\", path, tmpDir).Output()\n-\tif err != nil {\n-\t\treturn bodyResult, err\n-\t}\n+        _, err = exec.Command(\"pdfimages\", \"-j\", path, tmpDir).Output()\n+        if err != nil {\n+                return bodyResult, err\n+        }\n \n-\tfilePaths := []string{}\n+        filePaths := []string{}\n \n-\twalkFunc := func(path string, info os.FileInfo, err error) error {\n-\t\tpath, err = filepath.Abs(path)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n+        walkFunc := func(path string, info os.FileInfo, err error) error {\n+                path, err = filepath.Abs(path)\n+                if err != nil {\n+                        return err\n+                }\n \n-\t\tif compareExt(filepath.Ext(path), exts) {\n-\t\t\tfilePaths = append(filePaths, path)\n-\t\t}\n-\t\treturn nil\n-\t}\n-\tfilepath.Walk(tmpDir, walkFunc)\n+                if compareExt(filepath.Ext(path), exts) {\n+                        filePaths = append(filePaths, path)\n+                }\n+                return nil\n+        }\n+        filepath.Walk(tmpDir, walkFunc)\n \n-\tfileLength := len(filePaths)\n+        fileLength := len(filePaths)\n \n-\tif fileLength < 1 {\n-\t\treturn bodyResult, nil\n-\t}\n+        if fileLength < 1 {\n+                return bodyResult, nil\n+        }\n \n-\tvar wg sync.WaitGroup\n+        var wg sync.WaitGroup\n \n-\tdata := make(chan string, fileLength)\n+        data := make(chan string, fileLength)\n \n-\twg.Add(fileLength)\n+        wg.Add(fileLength)\n \n-\tfor _, p := range filePaths {\n-\t\tgo func(pathFile string) {\n-\t\t\tdefer wg.Done()\n-\t\t\tf, err := os.Open(pathFile)\n-\t\t\tif err != nil {\n-\t\t\t\treturn\n-\t\t\t}\n+        for _, p := range filePaths {\n+                go func(pathFile string) {\n+                        defer wg.Done()\n+                        f, err := os.Open(pathFile)\n+                        if err != nil {\n+                                return\n+                        }\n \n-\t\t\tdefer f.Close()\n-\t\t\tout, _, err := ConvertImage(f)\n-\t\t\tif err != nil {\n-\t\t\t\treturn\n-\t\t\t}\n+                        defer f.Close()\n+                        out, _, err := ConvertImage(f)\n+                        if err != nil {\n+                                return\n+                        }\n \n-\t\t\tdata <- out\n+                        data <- out\n \n-\t\t}(p)\n-\t}\n+                }(p)\n+        }\n \n-\twg.Wait()\n+        wg.Wait()\n \n-\tclose(data)\n+        close(data)\n \n-\tfor str := range data {\n-\t\tbodyResult.body += str + \" \"\n-\t}\n+        for str := range data {\n+                bodyResult.body += str + \" \"\n+        }\n \n-\treturn bodyResult, nil\n+        return bodyResult, nil\n }\n \n // PdfHasImage verify if `path` (PDF) has images\n func PDFHasImage(path string) bool {\n-\tcmd := \"pdffonts -l 5 %s | tail -n +3 | cut -d' ' -f1 | sort | uniq\"\n-\tout, err := exec.Command(\"bash\", \"-c\", fmt.Sprintf(cmd, path)).Output()\n-\tif err != nil {\n-\t\tlog.Println(err)\n-\t\treturn false\n-\t}\n-\tif string(out) == \"\" {\n-\t\treturn true\n-\t}\n-\treturn false\n+out, err := exec.Command(\"pdffonts\", \"-l\", \"5\", path).Output()\n+if err != nil {\n+log.Println(err)\n+return false\n+}\n+\n+sout := string(out)\n+if len(sout) == 0 {\n+return false\n+}\n+\n+//for some reason the output of pdffonts contains an error when the pdf has no fonts?\n+if strings.Contains(sout, \"Config Error: No display font for...\") {\n+return false\n+}\n+\n+lines := strings.Split(sout, \"\\n\")\n+if len(lines) <= 2 {\n+return false\n+}\n+\n+return true\n }\n \n func ConvertPDF(r io.Reader) (string, map[string]string, error) {\n-\tf, err := NewLocalFile(r)\n-\tif err != nil {\n-\t\treturn \"\", nil, fmt.Errorf(\"error creating local file: %v\", err)\n-\t}\n-\tdefer f.Done()\n-\n-\tbodyResult, metaResult, textConvertErr := ConvertPDFText(f.Name())\n-\tif textConvertErr != nil {\n-\t\treturn \"\", nil, textConvertErr\n-\t}\n-\tif bodyResult.err != nil {\n-\t\treturn \"\", nil, bodyResult.err\n-\t}\n-\tif metaResult.err != nil {\n-\t\treturn \"\", nil, metaResult.err\n-\t}\n-\n-\tif !PDFHasImage(f.Name()) {\n-\t\treturn bodyResult.body, metaResult.meta, nil\n-\t}\n-\n-\timageConvertResult, imageConvertErr := ConvertPDFImages(f.Name())\n-\tif imageConvertErr != nil {\n-\t\tlog.Println(imageConvertErr)\n-\t\treturn bodyResult.body, metaResult.meta, nil\n-\t}\n-\tif imageConvertResult.err != nil {\n-\t\tlog.Println(imageConvertResult.err)\n-\t\treturn bodyResult.body, metaResult.meta, nil\n-\t}\n-\n-\tfullBody := strings.Join([]string{bodyResult.body, imageConvertResult.body}, \" \")\n-\n-\treturn fullBody, metaResult.meta, nil\n+        f, err := NewLocalFile(r)\n+        if err != nil {\n+                return \"\", nil, fmt.Errorf(\"error creating local file: %v\", err)\n+        }\n+        defer f.Done()\n+\n+        bodyResult, metaResult, textConvertErr := ConvertPDFText(f.Name())\n+        if textConvertErr != nil {\n+                return \"\", nil, textConvertErr\n+        }\n+        if bodyResult.err != nil {\n+                return \"\", nil, bodyResult.err\n+        }\n+        if metaResult.err != nil {\n+                return \"\", nil, metaResult.err\n+        }\n+\n+        if !PDFHasImage(f.Name()) {\n+                return bodyResult.body, metaResult.meta, nil\n+        }\n+\n+        imageConvertResult, imageConvertErr := ConvertPDFImages(f.Name())\n+        if imageConvertErr != nil {\n+                log.Println(imageConvertErr)\n+                return bodyResult.body, metaResult.meta, nil\n+        }\n+        if imageConvertResult.err != nil {\n+                log.Println(imageConvertResult.err)\n+                return bodyResult.body, metaResult.meta, nil\n+        }\n+\n+        fullBody := strings.Join([]string{bodyResult.body, imageConvertResult.body}, \" \")\n+\n+        return fullBody, metaResult.meta, nil\n \n }\n"}
{"cve":"CVE-2023-26125:0708", "fix_patch": "diff --git a/gin.go b/gin.go\nindex 32dae24..7030242 100644\n--- a/gin.go\n+++ b/gin.go\n@@ -5,40 +5,47 @@\n package gin\n \n import (\n-\t\"fmt\"\n-\t\"html/template\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"path\"\n-\t\"strings\"\n-\t\"sync\"\n-\n-\t\"github.com/gin-gonic/gin/internal/bytesconv\"\n-\t\"github.com/gin-gonic/gin/render\"\n-\t\"golang.org/x/net/http2\"\n-\t\"golang.org/x/net/http2/h2c\"\n+\n+\n+        \"fmt\"\n+        \"html/template\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+\n+\n+        \n+        \"strings\"\n+        \"sync\"\n+\n+\n+        \"github.com/gin-gonic/gin/internal/bytesconv\"\n+        \"github.com/gin-gonic/gin/render\"\n+        \"golang.org/x/net/http2\"\n+        \"golang.org/x/net/http2/h2c\"\n )\n \n+\n+\n const defaultMultipartMemory = 32 << 20 // 32 MB\n \n var (\n-\tdefault404Body = []byte(\"404 page not found\")\n-\tdefault405Body = []byte(\"405 method not allowed\")\n+        default404Body = []byte(\"404 page not found\")\n+        default405Body = []byte(\"405 method not allowed\")\n )\n \n var defaultPlatform string\n \n var defaultTrustedCIDRs = []*net.IPNet{\n-\t{ // 0.0.0.0/0 (IPv4)\n-\t\tIP:   net.IP{0x0, 0x0, 0x0, 0x0},\n-\t\tMask: net.IPMask{0x0, 0x0, 0x0, 0x0},\n-\t},\n-\t{ // ::/0 (IPv6)\n-\t\tIP:   net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},\n-\t\tMask: net.IPMask{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},\n-\t},\n+        { // 0.0.0.0/0 (IPv4)\n+                IP:   net.IP{0x0, 0x0, 0x0, 0x0},\n+                Mask: net.IPMask{0x0, 0x0, 0x0, 0x0},\n+        },\n+        { // ::/0 (IPv6)\n+                IP:   net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},\n+                Mask: net.IPMask{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},\n+        },\n }\n \n // HandlerFunc defines the handler used by gin middleware as return value.\n@@ -49,18 +56,18 @@ type HandlersChain []HandlerFunc\n \n // Last returns the last handler in the chain. i.e. the last handler is the main one.\n func (c HandlersChain) Last() HandlerFunc {\n-\tif length := len(c); length > 0 {\n-\t\treturn c[length-1]\n-\t}\n-\treturn nil\n+        if length := len(c); length > 0 {\n+                return c[length-1]\n+        }\n+        return nil\n }\n \n // RouteInfo represents a request route's specification which contains method and path and its handler.\n type RouteInfo struct {\n-\tMethod      string\n-\tPath        string\n-\tHandler     string\n-\tHandlerFunc HandlerFunc\n+        Method      string\n+        Path        string\n+        Handler     string\n+        HandlerFunc HandlerFunc\n }\n \n // RoutesInfo defines a RouteInfo slice.\n@@ -68,103 +75,103 @@ type RoutesInfo []RouteInfo\n \n // Trusted platforms\n const (\n-\t// PlatformGoogleAppEngine when running on Google App Engine. Trust X-Appengine-Remote-Addr\n-\t// for determining the client's IP\n-\tPlatformGoogleAppEngine = \"X-Appengine-Remote-Addr\"\n-\t// PlatformCloudflare when using Cloudflare's CDN. Trust CF-Connecting-IP for determining\n-\t// the client's IP\n-\tPlatformCloudflare = \"CF-Connecting-IP\"\n+        // PlatformGoogleAppEngine when running on Google App Engine. Trust X-Appengine-Remote-Addr\n+        // for determining the client's IP\n+        PlatformGoogleAppEngine = \"X-Appengine-Remote-Addr\"\n+        // PlatformCloudflare when using Cloudflare's CDN. Trust CF-Connecting-IP for determining\n+        // the client's IP\n+        PlatformCloudflare = \"CF-Connecting-IP\"\n )\n \n // Engine is the framework's instance, it contains the muxer, middleware and configuration settings.\n // Create an instance of Engine, by using New() or Default()\n type Engine struct {\n-\tRouterGroup\n-\n-\t// RedirectTrailingSlash enables automatic redirection if the current route can't be matched but a\n-\t// handler for the path with (without) the trailing slash exists.\n-\t// For example if /foo/ is requested but a route only exists for /foo, the\n-\t// client is redirected to /foo with http status code 301 for GET requests\n-\t// and 307 for all other request methods.\n-\tRedirectTrailingSlash bool\n-\n-\t// RedirectFixedPath if enabled, the router tries to fix the current request path, if no\n-\t// handle is registered for it.\n-\t// First superfluous path elements like ../ or // are removed.\n-\t// Afterwards the router does a case-insensitive lookup of the cleaned path.\n-\t// If a handle can be found for this route, the router makes a redirection\n-\t// to the corrected path with status code 301 for GET requests and 307 for\n-\t// all other request methods.\n-\t// For example /FOO and /..//Foo could be redirected to /foo.\n-\t// RedirectTrailingSlash is independent of this option.\n-\tRedirectFixedPath bool\n-\n-\t// HandleMethodNotAllowed if enabled, the router checks if another method is allowed for the\n-\t// current route, if the current request can not be routed.\n-\t// If this is the case, the request is answered with 'Method Not Allowed'\n-\t// and HTTP status code 405.\n-\t// If no other Method is allowed, the request is delegated to the NotFound\n-\t// handler.\n-\tHandleMethodNotAllowed bool\n-\n-\t// ForwardedByClientIP if enabled, client IP will be parsed from the request's headers that\n-\t// match those stored at `(*gin.Engine).RemoteIPHeaders`. If no IP was\n-\t// fetched, it falls back to the IP obtained from\n-\t// `(*gin.Context).Request.RemoteAddr`.\n-\tForwardedByClientIP bool\n-\n-\t// AppEngine was deprecated.\n-\t// Deprecated: USE `TrustedPlatform` WITH VALUE `gin.PlatformGoogleAppEngine` INSTEAD\n-\t// #726 #755 If enabled, it will trust some headers starting with\n-\t// 'X-AppEngine...' for better integration with that PaaS.\n-\tAppEngine bool\n-\n-\t// UseRawPath if enabled, the url.RawPath will be used to find parameters.\n-\tUseRawPath bool\n-\n-\t// UnescapePathValues if true, the path value will be unescaped.\n-\t// If UseRawPath is false (by default), the UnescapePathValues effectively is true,\n-\t// as url.Path gonna be used, which is already unescaped.\n-\tUnescapePathValues bool\n-\n-\t// RemoveExtraSlash a parameter can be parsed from the URL even with extra slashes.\n-\t// See the PR #1817 and issue #1644\n-\tRemoveExtraSlash bool\n-\n-\t// RemoteIPHeaders list of headers used to obtain the client IP when\n-\t// `(*gin.Engine).ForwardedByClientIP` is `true` and\n-\t// `(*gin.Context).Request.RemoteAddr` is matched by at least one of the\n-\t// network origins of list defined by `(*gin.Engine).SetTrustedProxies()`.\n-\tRemoteIPHeaders []string\n-\n-\t// TrustedPlatform if set to a constant of value gin.Platform*, trusts the headers set by\n-\t// that platform, for example to determine the client IP\n-\tTrustedPlatform string\n-\n-\t// MaxMultipartMemory value of 'maxMemory' param that is given to http.Request's ParseMultipartForm\n-\t// method call.\n-\tMaxMultipartMemory int64\n-\n-\t// UseH2C enable h2c support.\n-\tUseH2C bool\n-\n-\t// ContextWithFallback enable fallback Context.Deadline(), Context.Done(), Context.Err() and Context.Value() when Context.Request.Context() is not nil.\n-\tContextWithFallback bool\n-\n-\tdelims           render.Delims\n-\tsecureJSONPrefix string\n-\tHTMLRender       render.HTMLRender\n-\tFuncMap          template.FuncMap\n-\tallNoRoute       HandlersChain\n-\tallNoMethod      HandlersChain\n-\tnoRoute          HandlersChain\n-\tnoMethod         HandlersChain\n-\tpool             sync.Pool\n-\ttrees            methodTrees\n-\tmaxParams        uint16\n-\tmaxSections      uint16\n-\ttrustedProxies   []string\n-\ttrustedCIDRs     []*net.IPNet\n+        RouterGroup\n+\n+        // RedirectTrailingSlash enables automatic redirection if the current route can't be matched but a\n+        // handler for the path with (without) the trailing slash exists.\n+        // For example if /foo/ is requested but a route only exists for /foo, the\n+        // client is redirected to /foo with http status code 301 for GET requests\n+        // and 307 for all other request methods.\n+        RedirectTrailingSlash bool\n+\n+        // RedirectFixedPath if enabled, the router tries to fix the current request path, if no\n+        // handle is registered for it.\n+        // First superfluous path elements like ../ or // are removed.\n+        // Afterwards the router does a case-insensitive lookup of the cleaned path.\n+        // If a handle can be found for this route, the router makes a redirection\n+        // to the corrected path with status code 301 for GET requests and 307 for\n+        // all other request methods.\n+        // For example /FOO and /..//Foo could be redirected to /foo.\n+        // RedirectTrailingSlash is independent of this option.\n+        RedirectFixedPath bool\n+\n+        // HandleMethodNotAllowed if enabled, the router checks if another method is allowed for the\n+        // current route, if the current request can not be routed.\n+        // If this is the case, the request is answered with 'Method Not Allowed'\n+        // and HTTP status code 405.\n+        // If no other Method is allowed, the request is delegated to the NotFound\n+        // handler.\n+        HandleMethodNotAllowed bool\n+\n+        // ForwardedByClientIP if enabled, client IP will be parsed from the request's headers that\n+        // match those stored at `(*gin.Engine).RemoteIPHeaders`. If no IP was\n+        // fetched, it falls back to the IP obtained from\n+        // `(*gin.Context).Request.RemoteAddr`.\n+        ForwardedByClientIP bool\n+\n+        // AppEngine was deprecated.\n+        // Deprecated: USE `TrustedPlatform` WITH VALUE `gin.PlatformGoogleAppEngine` INSTEAD\n+        // #726 #755 If enabled, it will trust some headers starting with\n+        // 'X-AppEngine...' for better integration with that PaaS.\n+        AppEngine bool\n+\n+        // UseRawPath if enabled, the url.RawPath will be used to find parameters.\n+        UseRawPath bool\n+\n+        // UnescapePathValues if true, the path value will be unescaped.\n+        // If UseRawPath is false (by default), the UnescapePathValues effectively is true,\n+        // as url.Path gonna be used, which is already unescaped.\n+        UnescapePathValues bool\n+\n+        // RemoveExtraSlash a parameter can be parsed from the URL even with extra slashes.\n+        // See the PR #1817 and issue #1644\n+        RemoveExtraSlash bool\n+\n+        // RemoteIPHeaders list of headers used to obtain the client IP when\n+        // `(*gin.Engine).ForwardedByClientIP` is `true` and\n+        // `(*gin.Context).Request.RemoteAddr` is matched by at least one of the\n+        // network origins of list defined by `(*gin.Engine).SetTrustedProxies()`.\n+        RemoteIPHeaders []string\n+\n+        // TrustedPlatform if set to a constant of value gin.Platform*, trusts the headers set by\n+        // that platform, for example to determine the client IP\n+        TrustedPlatform string\n+\n+        // MaxMultipartMemory value of 'maxMemory' param that is given to http.Request's ParseMultipartForm\n+        // method call.\n+        MaxMultipartMemory int64\n+\n+        // UseH2C enable h2c support.\n+        UseH2C bool\n+\n+        // ContextWithFallback enable fallback Context.Deadline(), Context.Done(), Context.Err() and Context.Value() when Context.Request.Context() is not nil.\n+        ContextWithFallback bool\n+\n+        delims           render.Delims\n+        secureJSONPrefix string\n+        HTMLRender       render.HTMLRender\n+        FuncMap          template.FuncMap\n+        allNoRoute       HandlersChain\n+        allNoMethod      HandlersChain\n+        noRoute          HandlersChain\n+        noMethod         HandlersChain\n+        pool             sync.Pool\n+        trees            methodTrees\n+        maxParams        uint16\n+        maxSections      uint16\n+        trustedProxies   []string\n+        trustedCIDRs     []*net.IPNet\n }\n \n var _ IRouter = (*Engine)(nil)\n@@ -178,239 +185,239 @@ var _ IRouter = (*Engine)(nil)\n // - UseRawPath:             false\n // - UnescapePathValues:     true\n func New() *Engine {\n-\tdebugPrintWARNINGNew()\n-\tengine := &Engine{\n-\t\tRouterGroup: RouterGroup{\n-\t\t\tHandlers: nil,\n-\t\t\tbasePath: \"/\",\n-\t\t\troot:     true,\n-\t\t},\n-\t\tFuncMap:                template.FuncMap{},\n-\t\tRedirectTrailingSlash:  true,\n-\t\tRedirectFixedPath:      false,\n-\t\tHandleMethodNotAllowed: false,\n-\t\tForwardedByClientIP:    true,\n-\t\tRemoteIPHeaders:        []string{\"X-Forwarded-For\", \"X-Real-IP\"},\n-\t\tTrustedPlatform:        defaultPlatform,\n-\t\tUseRawPath:             false,\n-\t\tRemoveExtraSlash:       false,\n-\t\tUnescapePathValues:     true,\n-\t\tMaxMultipartMemory:     defaultMultipartMemory,\n-\t\ttrees:                  make(methodTrees, 0, 9),\n-\t\tdelims:                 render.Delims{Left: \"{{\", Right: \"}}\"},\n-\t\tsecureJSONPrefix:       \"while(1);\",\n-\t\ttrustedProxies:         []string{\"0.0.0.0/0\", \"::/0\"},\n-\t\ttrustedCIDRs:           defaultTrustedCIDRs,\n-\t}\n-\tengine.RouterGroup.engine = engine\n-\tengine.pool.New = func() any {\n-\t\treturn engine.allocateContext(engine.maxParams)\n-\t}\n-\treturn engine\n+        debugPrintWARNINGNew()\n+        engine := &Engine{\n+                RouterGroup: RouterGroup{\n+                        Handlers: nil,\n+                        basePath: \"/\",\n+                        root:     true,\n+                },\n+                FuncMap:                template.FuncMap{},\n+                RedirectTrailingSlash:  true,\n+                RedirectFixedPath:      false,\n+                HandleMethodNotAllowed: false,\n+                ForwardedByClientIP:    true,\n+                RemoteIPHeaders:        []string{\"X-Forwarded-For\", \"X-Real-IP\"},\n+                TrustedPlatform:        defaultPlatform,\n+                UseRawPath:             false,\n+                RemoveExtraSlash:       false,\n+                UnescapePathValues:     true,\n+                MaxMultipartMemory:     defaultMultipartMemory,\n+                trees:                  make(methodTrees, 0, 9),\n+                delims:                 render.Delims{Left: \"{{\", Right: \"}}\"},\n+                secureJSONPrefix:       \"while(1);\",\n+                trustedProxies:         []string{\"0.0.0.0/0\", \"::/0\"},\n+                trustedCIDRs:           defaultTrustedCIDRs,\n+        }\n+        engine.RouterGroup.engine = engine\n+        engine.pool.New = func() any {\n+                return engine.allocateContext(engine.maxParams)\n+        }\n+        return engine\n }\n \n // Default returns an Engine instance with the Logger and Recovery middleware already attached.\n func Default() *Engine {\n-\tdebugPrintWARNINGDefault()\n-\tengine := New()\n-\tengine.Use(Logger(), Recovery())\n-\treturn engine\n+        debugPrintWARNINGDefault()\n+        engine := New()\n+        engine.Use(Logger(), Recovery())\n+        return engine\n }\n \n func (engine *Engine) Handler() http.Handler {\n-\tif !engine.UseH2C {\n-\t\treturn engine\n-\t}\n+        if !engine.UseH2C {\n+                return engine\n+        }\n \n-\th2s := &http2.Server{}\n-\treturn h2c.NewHandler(engine, h2s)\n+        h2s := &http2.Server{}\n+        return h2c.NewHandler(engine, h2s)\n }\n \n func (engine *Engine) allocateContext(maxParams uint16) *Context {\n-\tv := make(Params, 0, maxParams)\n-\tskippedNodes := make([]skippedNode, 0, engine.maxSections)\n-\treturn &Context{engine: engine, params: &v, skippedNodes: &skippedNodes}\n+        v := make(Params, 0, maxParams)\n+        skippedNodes := make([]skippedNode, 0, engine.maxSections)\n+        return &Context{engine: engine, params: &v, skippedNodes: &skippedNodes}\n }\n \n // Delims sets template left and right delims and returns an Engine instance.\n func (engine *Engine) Delims(left, right string) *Engine {\n-\tengine.delims = render.Delims{Left: left, Right: right}\n-\treturn engine\n+        engine.delims = render.Delims{Left: left, Right: right}\n+        return engine\n }\n \n // SecureJsonPrefix sets the secureJSONPrefix used in Context.SecureJSON.\n func (engine *Engine) SecureJsonPrefix(prefix string) *Engine {\n-\tengine.secureJSONPrefix = prefix\n-\treturn engine\n+        engine.secureJSONPrefix = prefix\n+        return engine\n }\n \n // LoadHTMLGlob loads HTML files identified by glob pattern\n // and associates the result with HTML renderer.\n func (engine *Engine) LoadHTMLGlob(pattern string) {\n-\tleft := engine.delims.Left\n-\tright := engine.delims.Right\n-\ttempl := template.Must(template.New(\"\").Delims(left, right).Funcs(engine.FuncMap).ParseGlob(pattern))\n+        left := engine.delims.Left\n+        right := engine.delims.Right\n+        templ := template.Must(template.New(\"\").Delims(left, right).Funcs(engine.FuncMap).ParseGlob(pattern))\n \n-\tif IsDebugging() {\n-\t\tdebugPrintLoadTemplate(templ)\n-\t\tengine.HTMLRender = render.HTMLDebug{Glob: pattern, FuncMap: engine.FuncMap, Delims: engine.delims}\n-\t\treturn\n-\t}\n+        if IsDebugging() {\n+                debugPrintLoadTemplate(templ)\n+                engine.HTMLRender = render.HTMLDebug{Glob: pattern, FuncMap: engine.FuncMap, Delims: engine.delims}\n+                return\n+        }\n \n-\tengine.SetHTMLTemplate(templ)\n+        engine.SetHTMLTemplate(templ)\n }\n \n // LoadHTMLFiles loads a slice of HTML files\n // and associates the result with HTML renderer.\n func (engine *Engine) LoadHTMLFiles(files ...string) {\n-\tif IsDebugging() {\n-\t\tengine.HTMLRender = render.HTMLDebug{Files: files, FuncMap: engine.FuncMap, Delims: engine.delims}\n-\t\treturn\n-\t}\n+        if IsDebugging() {\n+                engine.HTMLRender = render.HTMLDebug{Files: files, FuncMap: engine.FuncMap, Delims: engine.delims}\n+                return\n+        }\n \n-\ttempl := template.Must(template.New(\"\").Delims(engine.delims.Left, engine.delims.Right).Funcs(engine.FuncMap).ParseFiles(files...))\n-\tengine.SetHTMLTemplate(templ)\n+        templ := template.Must(template.New(\"\").Delims(engine.delims.Left, engine.delims.Right).Funcs(engine.FuncMap).ParseFiles(files...))\n+        engine.SetHTMLTemplate(templ)\n }\n \n // SetHTMLTemplate associate a template with HTML renderer.\n func (engine *Engine) SetHTMLTemplate(templ *template.Template) {\n-\tif len(engine.trees) > 0 {\n-\t\tdebugPrintWARNINGSetHTMLTemplate()\n-\t}\n+        if len(engine.trees) > 0 {\n+                debugPrintWARNINGSetHTMLTemplate()\n+        }\n \n-\tengine.HTMLRender = render.HTMLProduction{Template: templ.Funcs(engine.FuncMap)}\n+        engine.HTMLRender = render.HTMLProduction{Template: templ.Funcs(engine.FuncMap)}\n }\n \n // SetFuncMap sets the FuncMap used for template.FuncMap.\n func (engine *Engine) SetFuncMap(funcMap template.FuncMap) {\n-\tengine.FuncMap = funcMap\n+        engine.FuncMap = funcMap\n }\n \n // NoRoute adds handlers for NoRoute. It returns a 404 code by default.\n func (engine *Engine) NoRoute(handlers ...HandlerFunc) {\n-\tengine.noRoute = handlers\n-\tengine.rebuild404Handlers()\n+        engine.noRoute = handlers\n+        engine.rebuild404Handlers()\n }\n \n // NoMethod sets the handlers called when Engine.HandleMethodNotAllowed = true.\n func (engine *Engine) NoMethod(handlers ...HandlerFunc) {\n-\tengine.noMethod = handlers\n-\tengine.rebuild405Handlers()\n+        engine.noMethod = handlers\n+        engine.rebuild405Handlers()\n }\n \n // Use attaches a global middleware to the router. i.e. the middleware attached through Use() will be\n // included in the handlers chain for every single request. Even 404, 405, static files...\n // For example, this is the right place for a logger or error management middleware.\n func (engine *Engine) Use(middleware ...HandlerFunc) IRoutes {\n-\tengine.RouterGroup.Use(middleware...)\n-\tengine.rebuild404Handlers()\n-\tengine.rebuild405Handlers()\n-\treturn engine\n+        engine.RouterGroup.Use(middleware...)\n+        engine.rebuild404Handlers()\n+        engine.rebuild405Handlers()\n+        return engine\n }\n \n func (engine *Engine) rebuild404Handlers() {\n-\tengine.allNoRoute = engine.combineHandlers(engine.noRoute)\n+        engine.allNoRoute = engine.combineHandlers(engine.noRoute)\n }\n \n func (engine *Engine) rebuild405Handlers() {\n-\tengine.allNoMethod = engine.combineHandlers(engine.noMethod)\n+        engine.allNoMethod = engine.combineHandlers(engine.noMethod)\n }\n \n func (engine *Engine) addRoute(method, path string, handlers HandlersChain) {\n-\tassert1(path[0] == '/', \"path must begin with '/'\")\n-\tassert1(method != \"\", \"HTTP method can not be empty\")\n-\tassert1(len(handlers) > 0, \"there must be at least one handler\")\n+        assert1(path[0] == '/', \"path must begin with '/'\")\n+        assert1(method != \"\", \"HTTP method can not be empty\")\n+        assert1(len(handlers) > 0, \"there must be at least one handler\")\n \n-\tdebugPrintRoute(method, path, handlers)\n+        debugPrintRoute(method, path, handlers)\n \n-\troot := engine.trees.get(method)\n-\tif root == nil {\n-\t\troot = new(node)\n-\t\troot.fullPath = \"/\"\n-\t\tengine.trees = append(engine.trees, methodTree{method: method, root: root})\n-\t}\n-\troot.addRoute(path, handlers)\n+        root := engine.trees.get(method)\n+        if root == nil {\n+                root = new(node)\n+                root.fullPath = \"/\"\n+                engine.trees = append(engine.trees, methodTree{method: method, root: root})\n+        }\n+        root.addRoute(path, handlers)\n \n-\t// Update maxParams\n-\tif paramsCount := countParams(path); paramsCount > engine.maxParams {\n-\t\tengine.maxParams = paramsCount\n-\t}\n+        // Update maxParams\n+        if paramsCount := countParams(path); paramsCount > engine.maxParams {\n+                engine.maxParams = paramsCount\n+        }\n \n-\tif sectionsCount := countSections(path); sectionsCount > engine.maxSections {\n-\t\tengine.maxSections = sectionsCount\n-\t}\n+        if sectionsCount := countSections(path); sectionsCount > engine.maxSections {\n+                engine.maxSections = sectionsCount\n+        }\n }\n \n // Routes returns a slice of registered routes, including some useful information, such as:\n // the http method, path and the handler name.\n func (engine *Engine) Routes() (routes RoutesInfo) {\n-\tfor _, tree := range engine.trees {\n-\t\troutes = iterate(\"\", tree.method, routes, tree.root)\n-\t}\n-\treturn routes\n+        for _, tree := range engine.trees {\n+                routes = iterate(\"\", tree.method, routes, tree.root)\n+        }\n+        return routes\n }\n \n func iterate(path, method string, routes RoutesInfo, root *node) RoutesInfo {\n-\tpath += root.path\n-\tif len(root.handlers) > 0 {\n-\t\thandlerFunc := root.handlers.Last()\n-\t\troutes = append(routes, RouteInfo{\n-\t\t\tMethod:      method,\n-\t\t\tPath:        path,\n-\t\t\tHandler:     nameOfFunction(handlerFunc),\n-\t\t\tHandlerFunc: handlerFunc,\n-\t\t})\n-\t}\n-\tfor _, child := range root.children {\n-\t\troutes = iterate(path, method, routes, child)\n-\t}\n-\treturn routes\n+        path += root.path\n+        if len(root.handlers) > 0 {\n+                handlerFunc := root.handlers.Last()\n+                routes = append(routes, RouteInfo{\n+                        Method:      method,\n+                        Path:        path,\n+                        Handler:     nameOfFunction(handlerFunc),\n+                        HandlerFunc: handlerFunc,\n+                })\n+        }\n+        for _, child := range root.children {\n+                routes = iterate(path, method, routes, child)\n+        }\n+        return routes\n }\n \n // Run attaches the router to a http.Server and starts listening and serving HTTP requests.\n // It is a shortcut for http.ListenAndServe(addr, router)\n // Note: this method will block the calling goroutine indefinitely unless an error happens.\n func (engine *Engine) Run(addr ...string) (err error) {\n-\tdefer func() { debugPrintError(err) }()\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\taddress := resolveAddress(addr)\n-\tdebugPrint(\"Listening and serving HTTP on %s\\n\", address)\n-\terr = http.ListenAndServe(address, engine.Handler())\n-\treturn\n+        address := resolveAddress(addr)\n+        debugPrint(\"Listening and serving HTTP on %s\\n\", address)\n+        err = http.ListenAndServe(address, engine.Handler())\n+        return\n }\n \n func (engine *Engine) prepareTrustedCIDRs() ([]*net.IPNet, error) {\n-\tif engine.trustedProxies == nil {\n-\t\treturn nil, nil\n-\t}\n-\n-\tcidr := make([]*net.IPNet, 0, len(engine.trustedProxies))\n-\tfor _, trustedProxy := range engine.trustedProxies {\n-\t\tif !strings.Contains(trustedProxy, \"/\") {\n-\t\t\tip := parseIP(trustedProxy)\n-\t\t\tif ip == nil {\n-\t\t\t\treturn cidr, &net.ParseError{Type: \"IP address\", Text: trustedProxy}\n-\t\t\t}\n-\n-\t\t\tswitch len(ip) {\n-\t\t\tcase net.IPv4len:\n-\t\t\t\ttrustedProxy += \"/32\"\n-\t\t\tcase net.IPv6len:\n-\t\t\t\ttrustedProxy += \"/128\"\n-\t\t\t}\n-\t\t}\n-\t\t_, cidrNet, err := net.ParseCIDR(trustedProxy)\n-\t\tif err != nil {\n-\t\t\treturn cidr, err\n-\t\t}\n-\t\tcidr = append(cidr, cidrNet)\n-\t}\n-\treturn cidr, nil\n+        if engine.trustedProxies == nil {\n+                return nil, nil\n+        }\n+\n+        cidr := make([]*net.IPNet, 0, len(engine.trustedProxies))\n+        for _, trustedProxy := range engine.trustedProxies {\n+                if !strings.Contains(trustedProxy, \"/\") {\n+                        ip := parseIP(trustedProxy)\n+                        if ip == nil {\n+                                return cidr, &net.ParseError{Type: \"IP address\", Text: trustedProxy}\n+                        }\n+\n+                        switch len(ip) {\n+                        case net.IPv4len:\n+                                trustedProxy += \"/32\"\n+                        case net.IPv6len:\n+                                trustedProxy += \"/128\"\n+                        }\n+                }\n+                _, cidrNet, err := net.ParseCIDR(trustedProxy)\n+                if err != nil {\n+                        return cidr, err\n+                }\n+                cidr = append(cidr, cidrNet)\n+        }\n+        return cidr, nil\n }\n \n // SetTrustedProxies set a list of network origins (IPv4 addresses,\n@@ -422,287 +429,292 @@ func (engine *Engine) prepareTrustedCIDRs() ([]*net.IPNet, error) {\n // Engine.SetTrustedProxies(nil), then Context.ClientIP() will\n // return the remote address directly.\n func (engine *Engine) SetTrustedProxies(trustedProxies []string) error {\n-\tengine.trustedProxies = trustedProxies\n-\treturn engine.parseTrustedProxies()\n+        engine.trustedProxies = trustedProxies\n+        return engine.parseTrustedProxies()\n }\n \n // isUnsafeTrustedProxies checks if Engine.trustedCIDRs contains all IPs, it's not safe if it has (returns true)\n func (engine *Engine) isUnsafeTrustedProxies() bool {\n-\treturn engine.isTrustedProxy(net.ParseIP(\"0.0.0.0\")) || engine.isTrustedProxy(net.ParseIP(\"::\"))\n+        return engine.isTrustedProxy(net.ParseIP(\"0.0.0.0\")) || engine.isTrustedProxy(net.ParseIP(\"::\"))\n }\n \n // parseTrustedProxies parse Engine.trustedProxies to Engine.trustedCIDRs\n func (engine *Engine) parseTrustedProxies() error {\n-\ttrustedCIDRs, err := engine.prepareTrustedCIDRs()\n-\tengine.trustedCIDRs = trustedCIDRs\n-\treturn err\n+        trustedCIDRs, err := engine.prepareTrustedCIDRs()\n+        engine.trustedCIDRs = trustedCIDRs\n+        return err\n }\n \n // isTrustedProxy will check whether the IP address is included in the trusted list according to Engine.trustedCIDRs\n func (engine *Engine) isTrustedProxy(ip net.IP) bool {\n-\tif engine.trustedCIDRs == nil {\n-\t\treturn false\n-\t}\n-\tfor _, cidr := range engine.trustedCIDRs {\n-\t\tif cidr.Contains(ip) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        if engine.trustedCIDRs == nil {\n+                return false\n+        }\n+        for _, cidr := range engine.trustedCIDRs {\n+                if cidr.Contains(ip) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // validateHeader will parse X-Forwarded-For header and return the trusted client IP address\n func (engine *Engine) validateHeader(header string) (clientIP string, valid bool) {\n-\tif header == \"\" {\n-\t\treturn \"\", false\n-\t}\n-\titems := strings.Split(header, \",\")\n-\tfor i := len(items) - 1; i >= 0; i-- {\n-\t\tipStr := strings.TrimSpace(items[i])\n-\t\tip := net.ParseIP(ipStr)\n-\t\tif ip == nil {\n-\t\t\tbreak\n-\t\t}\n-\n-\t\t// X-Forwarded-For is appended by proxy\n-\t\t// Check IPs in reverse order and stop when find untrusted proxy\n-\t\tif (i == 0) || (!engine.isTrustedProxy(ip)) {\n-\t\t\treturn ipStr, true\n-\t\t}\n-\t}\n-\treturn \"\", false\n+        if header == \"\" {\n+                return \"\", false\n+        }\n+        items := strings.Split(header, \",\")\n+        for i := len(items) - 1; i >= 0; i-- {\n+                ipStr := strings.TrimSpace(items[i])\n+                ip := net.ParseIP(ipStr)\n+                if ip == nil {\n+                        break\n+                }\n+\n+                // X-Forwarded-For is appended by proxy\n+                // Check IPs in reverse order and stop when find untrusted proxy\n+                if (i == 0) || (!engine.isTrustedProxy(ip)) {\n+                        return ipStr, true\n+                }\n+        }\n+        return \"\", false\n }\n \n // parseIP parse a string representation of an IP and returns a net.IP with the\n // minimum byte representation or nil if input is invalid.\n func parseIP(ip string) net.IP {\n-\tparsedIP := net.ParseIP(ip)\n+        parsedIP := net.ParseIP(ip)\n \n-\tif ipv4 := parsedIP.To4(); ipv4 != nil {\n-\t\t// return ip in a 4-byte representation\n-\t\treturn ipv4\n-\t}\n+        if ipv4 := parsedIP.To4(); ipv4 != nil {\n+                // return ip in a 4-byte representation\n+                return ipv4\n+        }\n \n-\t// return ip in a 16-byte representation or nil\n-\treturn parsedIP\n+        // return ip in a 16-byte representation or nil\n+        return parsedIP\n }\n \n // RunTLS attaches the router to a http.Server and starts listening and serving HTTPS (secure) requests.\n // It is a shortcut for http.ListenAndServeTLS(addr, certFile, keyFile, router)\n // Note: this method will block the calling goroutine indefinitely unless an error happens.\n func (engine *Engine) RunTLS(addr, certFile, keyFile string) (err error) {\n-\tdebugPrint(\"Listening and serving HTTPS on %s\\n\", addr)\n-\tdefer func() { debugPrintError(err) }()\n+        debugPrint(\"Listening and serving HTTPS on %s\\n\", addr)\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\terr = http.ListenAndServeTLS(addr, certFile, keyFile, engine.Handler())\n-\treturn\n+        err = http.ListenAndServeTLS(addr, certFile, keyFile, engine.Handler())\n+        return\n }\n \n // RunUnix attaches the router to a http.Server and starts listening and serving HTTP requests\n // through the specified unix socket (i.e. a file).\n // Note: this method will block the calling goroutine indefinitely unless an error happens.\n func (engine *Engine) RunUnix(file string) (err error) {\n-\tdebugPrint(\"Listening and serving HTTP on unix:/%s\", file)\n-\tdefer func() { debugPrintError(err) }()\n+        debugPrint(\"Listening and serving HTTP on unix:/%s\", file)\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\tlistener, err := net.Listen(\"unix\", file)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tdefer listener.Close()\n-\tdefer os.Remove(file)\n+        listener, err := net.Listen(\"unix\", file)\n+        if err != nil {\n+                return\n+        }\n+        defer listener.Close()\n+        defer os.Remove(file)\n \n-\terr = http.Serve(listener, engine.Handler())\n-\treturn\n+        err = http.Serve(listener, engine.Handler())\n+        return\n }\n \n // RunFd attaches the router to a http.Server and starts listening and serving HTTP requests\n // through the specified file descriptor.\n // Note: this method will block the calling goroutine indefinitely unless an error happens.\n func (engine *Engine) RunFd(fd int) (err error) {\n-\tdebugPrint(\"Listening and serving HTTP on fd@%d\", fd)\n-\tdefer func() { debugPrintError(err) }()\n+        debugPrint(\"Listening and serving HTTP on fd@%d\", fd)\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\tf := os.NewFile(uintptr(fd), fmt.Sprintf(\"fd@%d\", fd))\n-\tlistener, err := net.FileListener(f)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tdefer listener.Close()\n-\terr = engine.RunListener(listener)\n-\treturn\n+        f := os.NewFile(uintptr(fd), fmt.Sprintf(\"fd@%d\", fd))\n+        listener, err := net.FileListener(f)\n+        if err != nil {\n+                return\n+        }\n+        defer listener.Close()\n+        err = engine.RunListener(listener)\n+        return\n }\n \n // RunListener attaches the router to a http.Server and starts listening and serving HTTP requests\n // through the specified net.Listener\n func (engine *Engine) RunListener(listener net.Listener) (err error) {\n-\tdebugPrint(\"Listening and serving HTTP on listener what's bind with address@%s\", listener.Addr())\n-\tdefer func() { debugPrintError(err) }()\n+        debugPrint(\"Listening and serving HTTP on listener what's bind with address@%s\", listener.Addr())\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\terr = http.Serve(listener, engine.Handler())\n-\treturn\n+        err = http.Serve(listener, engine.Handler())\n+        return\n }\n \n // ServeHTTP conforms to the http.Handler interface.\n func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n-\tc := engine.pool.Get().(*Context)\n-\tc.writermem.reset(w)\n-\tc.Request = req\n-\tc.reset()\n+        c := engine.pool.Get().(*Context)\n+        c.writermem.reset(w)\n+        c.Request = req\n+        c.reset()\n \n-\tengine.handleHTTPRequest(c)\n+        engine.handleHTTPRequest(c)\n \n-\tengine.pool.Put(c)\n+        engine.pool.Put(c)\n }\n \n // HandleContext re-enters a context that has been rewritten.\n // This can be done by setting c.Request.URL.Path to your new target.\n // Disclaimer: You can loop yourself to deal with this, use wisely.\n func (engine *Engine) HandleContext(c *Context) {\n-\toldIndexValue := c.index\n-\tc.reset()\n-\tengine.handleHTTPRequest(c)\n+        oldIndexValue := c.index\n+        c.reset()\n+        engine.handleHTTPRequest(c)\n \n-\tc.index = oldIndexValue\n+        c.index = oldIndexValue\n }\n \n func (engine *Engine) handleHTTPRequest(c *Context) {\n-\thttpMethod := c.Request.Method\n-\trPath := c.Request.URL.Path\n-\tunescape := false\n-\tif engine.UseRawPath && len(c.Request.URL.RawPath) > 0 {\n-\t\trPath = c.Request.URL.RawPath\n-\t\tunescape = engine.UnescapePathValues\n-\t}\n-\n-\tif engine.RemoveExtraSlash {\n-\t\trPath = cleanPath(rPath)\n-\t}\n-\n-\t// Find root of the tree for the given HTTP method\n-\tt := engine.trees\n-\tfor i, tl := 0, len(t); i < tl; i++ {\n-\t\tif t[i].method != httpMethod {\n-\t\t\tcontinue\n-\t\t}\n-\t\troot := t[i].root\n-\t\t// Find route in tree\n-\t\tvalue := root.getValue(rPath, c.params, c.skippedNodes, unescape)\n-\t\tif value.params != nil {\n-\t\t\tc.Params = *value.params\n-\t\t}\n-\t\tif value.handlers != nil {\n-\t\t\tc.handlers = value.handlers\n-\t\t\tc.fullPath = value.fullPath\n-\t\t\tc.Next()\n-\t\t\tc.writermem.WriteHeaderNow()\n-\t\t\treturn\n-\t\t}\n-\t\tif httpMethod != http.MethodConnect && rPath != \"/\" {\n-\t\t\tif value.tsr && engine.RedirectTrailingSlash {\n-\t\t\t\tredirectTrailingSlash(c)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif engine.RedirectFixedPath && redirectFixedPath(c, root, engine.RedirectFixedPath) {\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\tbreak\n-\t}\n-\n-\tif engine.HandleMethodNotAllowed {\n-\t\tfor _, tree := range engine.trees {\n-\t\t\tif tree.method == httpMethod {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tif value := tree.root.getValue(rPath, nil, c.skippedNodes, unescape); value.handlers != nil {\n-\t\t\t\tc.handlers = engine.allNoMethod\n-\t\t\t\tserveError(c, http.StatusMethodNotAllowed, default405Body)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t}\n-\tc.handlers = engine.allNoRoute\n-\tserveError(c, http.StatusNotFound, default404Body)\n+        httpMethod := c.Request.Method\n+        rPath := c.Request.URL.Path\n+        unescape := false\n+        if engine.UseRawPath && len(c.Request.URL.RawPath) > 0 {\n+                rPath = c.Request.URL.RawPath\n+                unescape = engine.UnescapePathValues\n+        }\n+\n+        if engine.RemoveExtraSlash {\n+                rPath = cleanPath(rPath)\n+        }\n+\n+        // Find root of the tree for the given HTTP method\n+        t := engine.trees\n+        for i, tl := 0, len(t); i < tl; i++ {\n+                if t[i].method != httpMethod {\n+                        continue\n+                }\n+                root := t[i].root\n+                // Find route in tree\n+                value := root.getValue(rPath, c.params, c.skippedNodes, unescape)\n+                if value.params != nil {\n+                        c.Params = *value.params\n+                }\n+                if value.handlers != nil {\n+                        c.handlers = value.handlers\n+                        c.fullPath = value.fullPath\n+                        c.Next()\n+                        c.writermem.WriteHeaderNow()\n+                        return\n+                }\n+                if httpMethod != http.MethodConnect && rPath != \"/\" {\n+                        if value.tsr && engine.RedirectTrailingSlash {\n+                                redirectTrailingSlash(c)\n+                                return\n+                        }\n+                        if engine.RedirectFixedPath && redirectFixedPath(c, root, engine.RedirectFixedPath) {\n+                                return\n+                        }\n+                }\n+                break\n+        }\n+\n+        if engine.HandleMethodNotAllowed {\n+                for _, tree := range engine.trees {\n+                        if tree.method == httpMethod {\n+                                continue\n+                        }\n+                        if value := tree.root.getValue(rPath, nil, c.skippedNodes, unescape); value.handlers != nil {\n+                                c.handlers = engine.allNoMethod\n+                                serveError(c, http.StatusMethodNotAllowed, default405Body)\n+                                return\n+                        }\n+                }\n+        }\n+        c.handlers = engine.allNoRoute\n+        serveError(c, http.StatusNotFound, default404Body)\n }\n \n var mimePlain = []string{MIMEPlain}\n \n func serveError(c *Context, code int, defaultMessage []byte) {\n-\tc.writermem.status = code\n-\tc.Next()\n-\tif c.writermem.Written() {\n-\t\treturn\n-\t}\n-\tif c.writermem.Status() == code {\n-\t\tc.writermem.Header()[\"Content-Type\"] = mimePlain\n-\t\t_, err := c.Writer.Write(defaultMessage)\n-\t\tif err != nil {\n-\t\t\tdebugPrint(\"cannot write message to writer during serve error: %v\", err)\n-\t\t}\n-\t\treturn\n-\t}\n-\tc.writermem.WriteHeaderNow()\n+        c.writermem.status = code\n+        c.Next()\n+        if c.writermem.Written() {\n+                return\n+        }\n+        if c.writermem.Status() == code {\n+                c.writermem.Header()[\"Content-Type\"] = mimePlain\n+                _, err := c.Writer.Write(defaultMessage)\n+                if err != nil {\n+                        debugPrint(\"cannot write message to writer during serve error: %v\", err)\n+                }\n+                return\n+        }\n+        c.writermem.WriteHeaderNow()\n }\n \n+\n func redirectTrailingSlash(c *Context) {\n-\treq := c.Request\n-\tp := req.URL.Path\n-\tif prefix := path.Clean(c.Request.Header.Get(\"X-Forwarded-Prefix\")); prefix != \".\" {\n-\t\tprefix = url.QueryEscape(prefix)\n-\t\tprefix = strings.ReplaceAll(prefix, \"%2F\", \"/\")\n+        req := c.Request\n+        p := req.URL.Path\n+\n+        if prefix := c.Request.Header.Get(\"X-Forwarded-Prefix\"); prefix != \"\" {\n+                prefix = url.PathEscape(prefix)\n+\n+\n+\n+                \n \n-\t\tp = prefix + \"/\" + req.URL.Path\n-\t}\n-\treq.URL.Path = p + \"/\"\n-\tif length := len(p); length > 1 && p[length-1] == '/' {\n-\t\treq.URL.Path = p[:length-1]\n-\t}\n-\tredirectRequest(c)\n+                p = prefix + \"/\" + req.URL.Path\n+        }\n+        req.URL.Path = p + \"/\"\n+        if length := len(p); length > 1 && p[length-1] == '/' {\n+                req.URL.Path = p[:length-1]\n+        }\n+        redirectRequest(c)\n }\n \n func redirectFixedPath(c *Context, root *node, trailingSlash bool) bool {\n-\treq := c.Request\n-\trPath := req.URL.Path\n+        req := c.Request\n+        rPath := req.URL.Path\n \n-\tif fixedPath, ok := root.findCaseInsensitivePath(cleanPath(rPath), trailingSlash); ok {\n-\t\treq.URL.Path = bytesconv.BytesToString(fixedPath)\n-\t\tredirectRequest(c)\n-\t\treturn true\n-\t}\n-\treturn false\n+        if fixedPath, ok := root.findCaseInsensitivePath(cleanPath(rPath), trailingSlash); ok {\n+                req.URL.Path = bytesconv.BytesToString(fixedPath)\n+                redirectRequest(c)\n+                return true\n+        }\n+        return false\n }\n \n func redirectRequest(c *Context) {\n-\treq := c.Request\n-\trPath := req.URL.Path\n-\trURL := req.URL.String()\n-\n-\tcode := http.StatusMovedPermanently // Permanent redirect, request with GET method\n-\tif req.Method != http.MethodGet {\n-\t\tcode = http.StatusTemporaryRedirect\n-\t}\n-\tdebugPrint(\"redirecting request %d: %s --> %s\", code, rPath, rURL)\n-\thttp.Redirect(c.Writer, req, rURL, code)\n-\tc.writermem.WriteHeaderNow()\n+        req := c.Request\n+        rPath := req.URL.Path\n+        rURL := req.URL.String()\n+\n+        code := http.StatusMovedPermanently // Permanent redirect, request with GET method\n+        if req.Method != http.MethodGet {\n+                code = http.StatusTemporaryRedirect\n+        }\n+        debugPrint(\"redirecting request %d: %s --> %s\", code, rPath, rURL)\n+        http.Redirect(c.Writer, req, rURL, code)\n+        c.writermem.WriteHeaderNow()\n }\n"}
{"cve":"CVE-2023-24623:0708", "fix_patch": ""}
{"cve":"CVE-2022-24825:0708", "fix_patch": "diff --git a/pkg/smokescreen/acl/v1/acl.go b/pkg/smokescreen/acl/v1/acl.go\nindex bf913c0..3a44aa3 100644\n--- a/pkg/smokescreen/acl/v1/acl.go\n+++ b/pkg/smokescreen/acl/v1/acl.go\n@@ -1,81 +1,81 @@\n package acl\n \n import (\n-\t\"fmt\"\n-\t\"strings\"\n+        \"fmt\"\n+        \"strings\"\n \n-\t\"github.com/sirupsen/logrus\"\n+        \"github.com/sirupsen/logrus\"\n )\n \n type Decider interface {\n-\tDecide(service, host string) (Decision, error)\n+        Decide(service, host string) (Decision, error)\n }\n \n type ACL struct {\n-\tRules            map[string]Rule\n-\tDefaultRule      *Rule\n-\tGlobalDenyList   []string\n-\tGlobalAllowList  []string\n-\tDisabledPolicies []EnforcementPolicy\n-\t*logrus.Logger\n+        Rules            map[string]Rule\n+        DefaultRule      *Rule\n+        GlobalDenyList   []string\n+        GlobalAllowList  []string\n+        DisabledPolicies []EnforcementPolicy\n+        *logrus.Logger\n }\n \n type Rule struct {\n-\tProject     string\n-\tPolicy      EnforcementPolicy\n-\tDomainGlobs []string\n+        Project     string\n+        Policy      EnforcementPolicy\n+        DomainGlobs []string\n }\n \n type Decision struct {\n-\tReason  string\n-\tDefault bool\n-\tResult  DecisionResult\n-\tProject string\n+        Reason  string\n+        Default bool\n+        Result  DecisionResult\n+        Project string\n }\n \n func New(logger *logrus.Logger, loader Loader, disabledActions []string) (*ACL, error) {\n-\tacl, err := loader.Load()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = acl.DisablePolicies(disabledActions)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = acl.Validate()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tacl.Logger = logger\n-\n-\tif acl.DefaultRule == nil {\n-\t\tacl.Warn(\"no default rule set. any services without a rule will be denied.\")\n-\t}\n-\treturn acl, nil\n+        acl, err := loader.Load()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        err = acl.DisablePolicies(disabledActions)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        err = acl.Validate()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        acl.Logger = logger\n+\n+        if acl.DefaultRule == nil {\n+                acl.Warn(\"no default rule set. any services without a rule will be denied.\")\n+        }\n+        return acl, nil\n }\n \n // Add associates a rule with the specified service after verifying the rule's\n // policy and domains are valid. Add returns an error if the service rule\n // already exists.\n func (acl *ACL) Add(svc string, r Rule) error {\n-\terr := acl.PolicyDisabled(svc, r.Policy)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\terr = acl.ValidateDomains(r.DomainGlobs)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif _, ok := acl.Rules[svc]; ok {\n-\t\treturn fmt.Errorf(\"rule already exists for service %v\", svc)\n-\t}\n-\tacl.Rules[svc] = r\n-\treturn nil\n+        err := acl.PolicyDisabled(svc, r.Policy)\n+        if err != nil {\n+                return err\n+        }\n+\n+        err = acl.ValidateDomains(r.DomainGlobs)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if _, ok := acl.Rules[svc]; ok {\n+                return fmt.Errorf(\"rule already exists for service %v\", svc)\n+        }\n+        acl.Rules[svc] = r\n+        return nil\n }\n \n // Decide takes uses the rule configured for the given service to determine if\n@@ -84,90 +84,90 @@ func (acl *ACL) Add(svc string, r Rule) error {\n //   3. The host has been globally allowed\n //   4. There is a default rule for the ACL\n func (acl *ACL) Decide(service, host string) (Decision, error) {\n-\tvar d Decision\n-\n-\trule := acl.Rule(service)\n-\tif rule == nil {\n-\t\td.Result = Deny\n-\t\td.Reason = \"no rule matched\"\n-\t\treturn d, nil\n-\t}\n-\n-\td.Project = rule.Project\n-\td.Default = rule == acl.DefaultRule\n-\n-\t// if the host matches any of the rule's allowed domains, allow\n-\tfor _, dg := range rule.DomainGlobs {\n-\t\tif hostMatchesGlob(host, dg) {\n-\t\t\td.Result, d.Reason = Allow, \"host matched allowed domain in rule\"\n-\t\t\treturn d, nil\n-\t\t}\n-\t}\n-\n-\t// if the host matches any of the global deny list, deny\n-\tfor _, dg := range acl.GlobalDenyList {\n-\t\tif hostMatchesGlob(host, dg) {\n-\t\t\td.Result, d.Reason = Deny, \"host matched rule in global deny list\"\n-\t\t\treturn d, nil\n-\t\t}\n-\t}\n-\n-\t// if the host matches any of the global allow list, allow\n-\tfor _, dg := range acl.GlobalAllowList {\n-\t\tif hostMatchesGlob(host, dg) {\n-\t\t\td.Result, d.Reason = Allow, \"host matched rule in global allow list\"\n-\t\t\treturn d, nil\n-\t\t}\n-\t}\n-\n-\tvar err error\n-\tswitch rule.Policy {\n-\tcase Report:\n-\t\td.Result, d.Reason = AllowAndReport, \"rule has allow and report policy\"\n-\tcase Enforce:\n-\t\td.Result, d.Reason = Deny, \"rule has enforce policy\"\n-\tcase Open:\n-\t\td.Result, d.Reason = Allow, \"rule has open enforcement policy\"\n-\tdefault:\n-\t\td.Result, d.Reason = Deny, \"unexpected policy value\"\n-\t\terr = fmt.Errorf(\"unexpected policy value for (%s -> %s): %d\", service, host, rule.Policy)\n-\t}\n-\n-\tif d.Default {\n-\t\td.Reason = \"default rule policy used\"\n-\t}\n-\n-\treturn d, err\n+        var d Decision\n+\n+        rule := acl.Rule(service)\n+        if rule == nil {\n+                d.Result = Deny\n+                d.Reason = \"no rule matched\"\n+                return d, nil\n+        }\n+\n+        d.Project = rule.Project\n+        d.Default = rule == acl.DefaultRule\n+\n+        // if the host matches any of the rule's allowed domains, allow\n+        for _, dg := range rule.DomainGlobs {\n+                if hostMatchesGlob(host, dg) {\n+                        d.Result, d.Reason = Allow, \"host matched allowed domain in rule\"\n+                        return d, nil\n+                }\n+        }\n+\n+        // if the host matches any of the global deny list, deny\n+        for _, dg := range acl.GlobalDenyList {\n+                if hostMatchesGlob(host, dg) {\n+                        d.Result, d.Reason = Deny, \"host matched rule in global deny list\"\n+                        return d, nil\n+                }\n+        }\n+\n+        // if the host matches any of the global allow list, allow\n+        for _, dg := range acl.GlobalAllowList {\n+                if hostMatchesGlob(host, dg) {\n+                        d.Result, d.Reason = Allow, \"host matched rule in global allow list\"\n+                        return d, nil\n+                }\n+        }\n+\n+        var err error\n+        switch rule.Policy {\n+        case Report:\n+                d.Result, d.Reason = AllowAndReport, \"rule has allow and report policy\"\n+        case Enforce:\n+                d.Result, d.Reason = Deny, \"rule has enforce policy\"\n+        case Open:\n+                d.Result, d.Reason = Allow, \"rule has open enforcement policy\"\n+        default:\n+                d.Result, d.Reason = Deny, \"unexpected policy value\"\n+                err = fmt.Errorf(\"unexpected policy value for (%s -> %s): %d\", service, host, rule.Policy)\n+        }\n+\n+        if d.Default {\n+                d.Reason = \"default rule policy used\"\n+        }\n+\n+        return d, err\n }\n \n // DisablePolicies takes a slice of actions (open, report, enforce), maps them\n // to their corresponding EnforcementPolicy, and adds them to the global\n // disabledPolicy slice.\n func (acl *ACL) DisablePolicies(actions []string) error {\n-\tfor _, a := range actions {\n-\t\tp, err := PolicyFromAction(a)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tacl.DisabledPolicies = append(acl.DisabledPolicies, p)\n-\t}\n-\treturn nil\n+        for _, a := range actions {\n+                p, err := PolicyFromAction(a)\n+                if err != nil {\n+                        return err\n+                }\n+                acl.DisabledPolicies = append(acl.DisabledPolicies, p)\n+        }\n+        return nil\n }\n \n // Validate checks that the ACL that every rule has a conformant domain glob\n // and is not utilizing a disabled enforcement policy.\n func (acl *ACL) Validate() error {\n-\tfor svc, r := range acl.Rules {\n-\t\terr := acl.ValidateDomains(r.DomainGlobs)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\terr = acl.PolicyDisabled(svc, r.Policy)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\treturn nil\n+        for svc, r := range acl.Rules {\n+                err := acl.ValidateDomains(r.DomainGlobs)\n+                if err != nil {\n+                        return err\n+                }\n+                err = acl.PolicyDisabled(svc, r.Policy)\n+                if err != nil {\n+                        return err\n+                }\n+        }\n+        return nil\n }\n \n // ValidateDomains takes a slice of domains and verifies they conform to\n@@ -176,59 +176,76 @@ func (acl *ACL) Validate() error {\n // Domains can only contain a single wildcard prefix\n // Domains cannot be represented as a sole wildcard\n func (acl *ACL) ValidateDomains(domains []string) error {\n-\tfor _, d := range domains {\n-\t\tif d == \"\" {\n-\t\t\treturn fmt.Errorf(\"glob cannot be empty\")\n-\t\t}\n-\n-\t\tif !strings.HasPrefix(d, \"*.\") && strings.HasPrefix(d, \"*\") {\n-\t\t\treturn fmt.Errorf(\"%v: domain glob must represent a full prefix (sub)domain\", d)\n-\t\t}\n+        for _, d := range domains {\n+d = strings.ToLower(d)\n+if strings.HasSuffix(d, \".\") {\n+d = d[:len(d)-1]\n+}\n \n-\t\tdomainToCheck := strings.TrimPrefix(d, \"*\")\n-\t\tif strings.Contains(domainToCheck, \"*\") {\n-\t\t\treturn fmt.Errorf(\"%v: domain globs are only supported as prefix\", d)\n-\t\t}\n-\t}\n-\treturn nil\n+if d == \"*\" {\n+return fmt.Errorf(\"glob cannot be '*'\")\n+}\n+d = strings.ToLower(d)\n+if strings.HasSuffix(d, \".\") {\n+d = d[:len(d)-1]\n+}\n+if d == \"*\" {\n+return fmt.Errorf(\"glob cannot be '*'\")\n+}\n+                if d == \"\" {\n+                        return fmt.Errorf(\"glob cannot be empty\")\n+                }\n+\n+                if !strings.HasPrefix(d, \"*.\") && strings.HasPrefix(d, \"*\") {\n+                        return fmt.Errorf(\"%v: domain glob must represent a full prefix (sub)domain\", d)\n+                }\n+\n+                domainToCheck := strings.TrimPrefix(d, \"*\")\n+                if strings.Contains(domainToCheck, \"*\") {\n+                        return fmt.Errorf(\"%v: domain globs are only supported as prefix\", d)\n+                }\n+        }\n+        return nil\n }\n \n // PolicyDisabled checks if an EnforcementPolicy is disabled at the ACL level\n func (acl *ACL) PolicyDisabled(svc string, p EnforcementPolicy) error {\n-\tfor _, dp := range acl.DisabledPolicies {\n-\t\tif dp == p {\n-\t\t\treturn fmt.Errorf(\"rule for svc:%v utilizes a disabled policy:%v\", svc, p)\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, dp := range acl.DisabledPolicies {\n+                if dp == p {\n+                        return fmt.Errorf(\"rule for svc:%v utilizes a disabled policy:%v\", svc, p)\n+                }\n+        }\n+        return nil\n }\n \n // Project returns the configured project for a service\n func (acl *ACL) Project(service string) (string, error) {\n-\trule := acl.Rule(service)\n-\tif rule == nil {\n-\t\treturn \"\", fmt.Errorf(\"no rule for service: %v\", service)\n-\t}\n-\treturn rule.Project, nil\n+        rule := acl.Rule(service)\n+        if rule == nil {\n+                return \"\", fmt.Errorf(\"no rule for service: %v\", service)\n+        }\n+        return rule.Project, nil\n }\n \n // Rule returns the configured rule for a service, or the default rule if none\n // is configured.\n func (acl *ACL) Rule(service string) *Rule {\n-\tif service, ok := acl.Rules[service]; ok {\n-\t\treturn &service\n-\t}\n-\treturn acl.DefaultRule\n+        if service, ok := acl.Rules[service]; ok {\n+                return &service\n+        }\n+        return acl.DefaultRule\n }\n \n func hostMatchesGlob(host string, domainGlob string) bool {\n-\tif domainGlob != \"\" && domainGlob[0] == '*' {\n-\t\tsuffix := domainGlob[1:]\n-\t\tif strings.HasSuffix(host, suffix) {\n-\t\t\treturn true\n-\t\t}\n-\t} else if domainGlob == host {\n-\t\treturn true\n-\t}\n-\treturn false\n+        host = strings.TrimSuffix(strings.ToLower(host), \".\")\n+        domainGlob = strings.TrimSuffix(strings.ToLower(domainGlob), \".\")\n+        if domainGlob != \"\" && domainGlob[0] == '*' {\n+                suffix := domainGlob[1:]\n+                if strings.HasSuffix(host, suffix) {\n+                        return true\n+                }\n+        } else if domainGlob == host {\n+                return true\n+        }\n+        return false\n }\n"}
{"cve":"CVE-2021-32783:0708", "fix_patch": "diff --git a/internal/dag/accessors.go b/internal/dag/accessors.go\nindex 9f61e412..5b721caf 100644\n--- a/internal/dag/accessors.go\n+++ b/internal/dag/accessors.go\n@@ -14,132 +14,139 @@\n package dag\n \n import (\n-\t\"fmt\"\n-\t\"strconv\"\n-\n-\t\"github.com/projectcontour/contour/internal/annotation\"\n-\t\"github.com/projectcontour/contour/internal/k8s\"\n-\tv1 \"k8s.io/api/core/v1\"\n-\t\"k8s.io/apimachinery/pkg/types\"\n-\t\"k8s.io/apimachinery/pkg/util/intstr\"\n+        \"fmt\"\n+        \"strconv\"\n+\n+        \"github.com/projectcontour/contour/internal/annotation\"\n+        \"github.com/projectcontour/contour/internal/k8s\"\n+        v1 \"k8s.io/api/core/v1\"\n+        \"k8s.io/apimachinery/pkg/types\"\n+        \"k8s.io/apimachinery/pkg/util/intstr\"\n )\n \n // RouteServiceName identifies a service used in a route.\n type RouteServiceName struct {\n-\tName      string\n-\tNamespace string\n-\tPort      int32\n+        Name      string\n+        Namespace string\n+        Port      int32\n }\n \n // GetServices returns all services in the DAG.\n func (dag *DAG) GetServices() map[RouteServiceName]*Service {\n-\tgetter := serviceGetter(map[RouteServiceName]*Service{})\n-\tdag.Visit(getter.visit)\n-\treturn getter\n+        getter := serviceGetter(map[RouteServiceName]*Service{})\n+        dag.Visit(getter.visit)\n+        return getter\n }\n \n // GetService returns the service in the DAG that matches the provided\n // namespace, name and port, or nil if no matching service is found.\n func (dag *DAG) GetService(meta types.NamespacedName, port int32) *Service {\n-\treturn dag.GetServices()[RouteServiceName{\n-\t\tName:      meta.Name,\n-\t\tNamespace: meta.Namespace,\n-\t\tPort:      port,\n-\t}]\n+        return dag.GetServices()[RouteServiceName{\n+                Name:      meta.Name,\n+                Namespace: meta.Namespace,\n+                Port:      port,\n+        }]\n }\n \n // EnsureService looks for a Kubernetes service in the cache matching the provided\n // namespace, name and port, and returns a DAG service for it. If a matching service\n // cannot be found in the cache, an error is returned.\n func (dag *DAG) EnsureService(meta types.NamespacedName, port intstr.IntOrString, cache *KubernetesCache) (*Service, error) {\n-\tsvc, svcPort, err := cache.LookupService(meta, port)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif dagSvc := dag.GetService(k8s.NamespacedNameOf(svc), svcPort.Port); dagSvc != nil {\n-\t\treturn dagSvc, nil\n-\t}\n-\n-\tdagSvc := &Service{\n-\t\tWeighted: WeightedService{\n-\t\t\tServiceName:      svc.Name,\n-\t\t\tServiceNamespace: svc.Namespace,\n-\t\t\tServicePort:      svcPort,\n-\t\t\tWeight:           1,\n-\t\t},\n-\t\tProtocol:           upstreamProtocol(svc, svcPort),\n-\t\tMaxConnections:     annotation.MaxConnections(svc),\n-\t\tMaxPendingRequests: annotation.MaxPendingRequests(svc),\n-\t\tMaxRequests:        annotation.MaxRequests(svc),\n-\t\tMaxRetries:         annotation.MaxRetries(svc),\n-\t\tExternalName:       externalName(svc),\n-\t}\n-\treturn dagSvc, nil\n+svc, svcPort, err := cache.LookupService(meta, port)\n+if err != nil {\n+return nil, err\n+}\n+\n+if dagSvc := dag.GetService(k8s.NamespacedNameOf(svc), svcPort.Port); dagSvc != nil {\n+return dagSvc, nil\n+}\n+\n+dagSvc := &Service{\n+Weighted: WeightedService{\n+ServiceName:      svc.Name,\n+ServiceNamespace: svc.Namespace,\n+ServicePort:      svcPort,\n+Weight:           1,\n+},\n+Protocol:           upstreamProtocol(svc, svcPort),\n+MaxConnections:     annotation.MaxConnections(svc),\n+MaxPendingRequests: annotation.MaxPendingRequests(svc),\n+MaxRequests:        annotation.MaxRequests(svc),\n+MaxRetries:         annotation.MaxRetries(svc),\n+ExternalName:       externalName(svc),\n+}\n+return dagSvc, nil\n }\n \n func upstreamProtocol(svc *v1.Service, port v1.ServicePort) string {\n-\tup := annotation.ParseUpstreamProtocols(svc.Annotations)\n-\tprotocol := up[port.Name]\n-\tif protocol == \"\" {\n-\t\tprotocol = up[strconv.Itoa(int(port.Port))]\n-\t}\n-\treturn protocol\n+up := annotation.ParseUpstreamProtocols(svc.Annotations)\n+protocol := up[port.Name]\n+if protocol == \"\" {\n+protocol = up[strconv.Itoa(int(port.Port))]\n+}\n+return protocol\n }\n \n func externalName(svc *v1.Service) string {\n-\tif svc.Spec.Type != v1.ServiceTypeExternalName {\n-\t\treturn \"\"\n-\t}\n-\treturn svc.Spec.ExternalName\n+if svc.Spec.Type != v1.ServiceTypeExternalName {\n+// Not an ExternalName service.\n+return \"\"\n+}\n+\n+// CVE-2021-32783: Block attempts to use ExternalName services to\n+// access the Envoy admin interface.\n+if svc.Spec.ExternalName == \"localhost\" || svc.Spec.ExternalName == \"127.0.0.1\" {\n+return \"\"\n }\n \n+return svc.Spec.ExternalName\n+}\n // serviceGetter is a visitor that gets all services\n // in the DAG.\n type serviceGetter map[RouteServiceName]*Service\n \n func (s serviceGetter) visit(vertex Vertex) {\n-\tswitch obj := vertex.(type) {\n-\tcase *Service:\n-\t\ts[RouteServiceName{\n-\t\t\tName:      obj.Weighted.ServiceName,\n-\t\t\tNamespace: obj.Weighted.ServiceNamespace,\n-\t\t\tPort:      obj.Weighted.ServicePort.Port,\n-\t\t}] = obj\n-\tdefault:\n-\t\tvertex.Visit(s.visit)\n-\t}\n+        switch obj := vertex.(type) {\n+        case *Service:\n+                s[RouteServiceName{\n+                        Name:      obj.Weighted.ServiceName,\n+                        Namespace: obj.Weighted.ServiceNamespace,\n+                        Port:      obj.Weighted.ServicePort.Port,\n+                }] = obj\n+        default:\n+                vertex.Visit(s.visit)\n+        }\n }\n \n // GetSecureVirtualHosts returns all secure virtual hosts in the DAG.\n func (dag *DAG) GetSecureVirtualHosts() map[ListenerName]*SecureVirtualHost {\n-\tgetter := svhostGetter(map[ListenerName]*SecureVirtualHost{})\n-\tdag.Visit(getter.visit)\n-\treturn getter\n+        getter := svhostGetter(map[ListenerName]*SecureVirtualHost{})\n+        dag.Visit(getter.visit)\n+        return getter\n }\n \n // GetSecureVirtualHost returns the secure virtual host in the DAG that\n // matches the provided name, or nil if no matching secure virtual host\n // is found.\n func (dag *DAG) GetSecureVirtualHost(ln ListenerName) *SecureVirtualHost {\n-\treturn dag.GetSecureVirtualHosts()[ln]\n+        return dag.GetSecureVirtualHosts()[ln]\n }\n \n // EnsureSecureVirtualHost adds a secure virtual host with the provided\n // name to the DAG if it does not already exist, and returns it.\n func (dag *DAG) EnsureSecureVirtualHost(ln ListenerName) *SecureVirtualHost {\n-\tif svh := dag.GetSecureVirtualHost(ln); svh != nil {\n-\t\treturn svh\n-\t}\n+        if svh := dag.GetSecureVirtualHost(ln); svh != nil {\n+                return svh\n+        }\n \n-\tsvh := &SecureVirtualHost{\n-\t\tVirtualHost: VirtualHost{\n-\t\t\tName:         ln.Name,\n-\t\t\tListenerName: ln.ListenerName,\n-\t\t},\n-\t}\n-\tdag.AddRoot(svh)\n-\treturn svh\n+        svh := &SecureVirtualHost{\n+                VirtualHost: VirtualHost{\n+                        Name:         ln.Name,\n+                        ListenerName: ln.ListenerName,\n+                },\n+        }\n+        dag.AddRoot(svh)\n+        return svh\n }\n \n // svhostGetter is a visitor that gets all secure virtual hosts\n@@ -147,40 +154,40 @@ func (dag *DAG) EnsureSecureVirtualHost(ln ListenerName) *SecureVirtualHost {\n type svhostGetter map[ListenerName]*SecureVirtualHost\n \n func (s svhostGetter) visit(vertex Vertex) {\n-\tswitch obj := vertex.(type) {\n-\tcase *SecureVirtualHost:\n-\t\ts[ListenerName{Name: obj.Name, ListenerName: obj.VirtualHost.ListenerName}] = obj\n-\tdefault:\n-\t\tvertex.Visit(s.visit)\n-\t}\n+        switch obj := vertex.(type) {\n+        case *SecureVirtualHost:\n+                s[ListenerName{Name: obj.Name, ListenerName: obj.VirtualHost.ListenerName}] = obj\n+        default:\n+                vertex.Visit(s.visit)\n+        }\n }\n \n // GetVirtualHosts returns all virtual hosts in the DAG.\n func (dag *DAG) GetVirtualHosts() map[ListenerName]*VirtualHost {\n-\tgetter := vhostGetter(map[ListenerName]*VirtualHost{})\n-\tdag.Visit(getter.visit)\n-\treturn getter\n+        getter := vhostGetter(map[ListenerName]*VirtualHost{})\n+        dag.Visit(getter.visit)\n+        return getter\n }\n \n // GetVirtualHost returns the virtual host in the DAG that matches the\n // provided name, or nil if no matching virtual host is found.\n func (dag *DAG) GetVirtualHost(ln ListenerName) *VirtualHost {\n-\treturn dag.GetVirtualHosts()[ln]\n+        return dag.GetVirtualHosts()[ln]\n }\n \n // EnsureVirtualHost adds a virtual host with the provided name to the\n // DAG if it does not already exist, and returns it.\n func (dag *DAG) EnsureVirtualHost(ln ListenerName) *VirtualHost {\n-\tif vhost := dag.GetVirtualHost(ln); vhost != nil {\n-\t\treturn vhost\n-\t}\n+        if vhost := dag.GetVirtualHost(ln); vhost != nil {\n+                return vhost\n+        }\n \n-\tvhost := &VirtualHost{\n-\t\tName:         ln.Name,\n-\t\tListenerName: ln.ListenerName,\n-\t}\n-\tdag.AddRoot(vhost)\n-\treturn vhost\n+        vhost := &VirtualHost{\n+                Name:         ln.Name,\n+                ListenerName: ln.ListenerName,\n+        }\n+        dag.AddRoot(vhost)\n+        return vhost\n }\n \n // vhostGetter is a visitor that gets all virtual hosts\n@@ -188,26 +195,26 @@ func (dag *DAG) EnsureVirtualHost(ln ListenerName) *VirtualHost {\n type vhostGetter map[ListenerName]*VirtualHost\n \n func (v vhostGetter) visit(vertex Vertex) {\n-\tswitch obj := vertex.(type) {\n-\tcase *VirtualHost:\n-\t\tv[ListenerName{Name: obj.Name, ListenerName: obj.ListenerName}] = obj\n-\tdefault:\n-\t\tvertex.Visit(v.visit)\n-\t}\n+        switch obj := vertex.(type) {\n+        case *VirtualHost:\n+                v[ListenerName{Name: obj.Name, ListenerName: obj.ListenerName}] = obj\n+        default:\n+                vertex.Visit(v.visit)\n+        }\n }\n \n // GetExtensionClusters returns all extension clusters in the DAG.\n func (dag *DAG) GetExtensionClusters() map[string]*ExtensionCluster {\n-\tgetter := extensionClusterGetter(map[string]*ExtensionCluster{})\n-\tdag.Visit(getter.visit)\n-\treturn getter\n+        getter := extensionClusterGetter(map[string]*ExtensionCluster{})\n+        dag.Visit(getter.visit)\n+        return getter\n }\n \n // GetExtensionCluster returns the extension cluster in the DAG that\n // matches the provided name, or nil if no matching extension cluster\n //is found.\n func (dag *DAG) GetExtensionCluster(name string) *ExtensionCluster {\n-\treturn dag.GetExtensionClusters()[name]\n+        return dag.GetExtensionClusters()[name]\n }\n \n // extensionClusterGetter is a visitor that gets all extension clusters\n@@ -215,27 +222,27 @@ func (dag *DAG) GetExtensionCluster(name string) *ExtensionCluster {\n type extensionClusterGetter map[string]*ExtensionCluster\n \n func (v extensionClusterGetter) visit(vertex Vertex) {\n-\tswitch obj := vertex.(type) {\n-\tcase *ExtensionCluster:\n-\t\tv[obj.Name] = obj\n-\tdefault:\n-\t\tvertex.Visit(v.visit)\n-\t}\n+        switch obj := vertex.(type) {\n+        case *ExtensionCluster:\n+                v[obj.Name] = obj\n+        default:\n+                vertex.Visit(v.visit)\n+        }\n }\n \n // validSecret returns true if the Secret contains certificate and private key material.\n func validSecret(s *v1.Secret) error {\n-\tif s.Type != v1.SecretTypeTLS {\n-\t\treturn fmt.Errorf(\"Secret type is not %q\", v1.SecretTypeTLS)\n-\t}\n+        if s.Type != v1.SecretTypeTLS {\n+                return fmt.Errorf(\"Secret type is not %q\", v1.SecretTypeTLS)\n+        }\n \n-\tif len(s.Data[v1.TLSCertKey]) == 0 {\n-\t\treturn fmt.Errorf(\"empty %q key\", v1.TLSCertKey)\n-\t}\n+        if len(s.Data[v1.TLSCertKey]) == 0 {\n+                return fmt.Errorf(\"empty %q key\", v1.TLSCertKey)\n+        }\n \n-\tif len(s.Data[v1.TLSPrivateKeyKey]) == 0 {\n-\t\treturn fmt.Errorf(\"empty %q key\", v1.TLSPrivateKeyKey)\n-\t}\n+        if len(s.Data[v1.TLSPrivateKeyKey]) == 0 {\n+                return fmt.Errorf(\"empty %q key\", v1.TLSPrivateKeyKey)\n+        }\n \n-\treturn nil\n+        return nil\n }\n"}
{"cve":"CVE-2021-3155:0708", "fix_patch": "diff --git a/cmd/snap/cmd_run.go b/cmd/snap/cmd_run.go\nindex 2895a336ab..64806ac729 100644\n--- a/cmd/snap/cmd_run.go\n+++ b/cmd/snap/cmd_run.go\n@@ -20,506 +20,506 @@\n package main\n \n import (\n-\t\"bufio\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"io/ioutil\"\n-\t\"net\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"os/user\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"syscall\"\n-\t\"time\"\n-\n-\t\"github.com/godbus/dbus\"\n-\t\"github.com/jessevdk/go-flags\"\n-\n-\t\"github.com/snapcore/snapd/client\"\n-\t\"github.com/snapcore/snapd/desktop/portal\"\n-\t\"github.com/snapcore/snapd/dirs\"\n-\t\"github.com/snapcore/snapd/features\"\n-\t\"github.com/snapcore/snapd/i18n\"\n-\t\"github.com/snapcore/snapd/interfaces\"\n-\t\"github.com/snapcore/snapd/logger\"\n-\t\"github.com/snapcore/snapd/osutil\"\n-\t\"github.com/snapcore/snapd/osutil/strace\"\n-\t\"github.com/snapcore/snapd/sandbox/cgroup\"\n-\t\"github.com/snapcore/snapd/sandbox/selinux\"\n-\t\"github.com/snapcore/snapd/snap\"\n-\t\"github.com/snapcore/snapd/snap/snapenv\"\n-\t\"github.com/snapcore/snapd/strutil/shlex\"\n-\t\"github.com/snapcore/snapd/timeutil\"\n-\t\"github.com/snapcore/snapd/x11\"\n+        \"bufio\"\n+        \"fmt\"\n+        \"io\"\n+        \"io/ioutil\"\n+        \"net\"\n+        \"os\"\n+        \"os/exec\"\n+        \"os/user\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"strconv\"\n+        \"strings\"\n+        \"syscall\"\n+        \"time\"\n+\n+        \"github.com/godbus/dbus\"\n+        \"github.com/jessevdk/go-flags\"\n+\n+        \"github.com/snapcore/snapd/client\"\n+        \"github.com/snapcore/snapd/desktop/portal\"\n+        \"github.com/snapcore/snapd/dirs\"\n+        \"github.com/snapcore/snapd/features\"\n+        \"github.com/snapcore/snapd/i18n\"\n+        \"github.com/snapcore/snapd/interfaces\"\n+        \"github.com/snapcore/snapd/logger\"\n+        \"github.com/snapcore/snapd/osutil\"\n+        \"github.com/snapcore/snapd/osutil/strace\"\n+        \"github.com/snapcore/snapd/sandbox/cgroup\"\n+        \"github.com/snapcore/snapd/sandbox/selinux\"\n+        \"github.com/snapcore/snapd/snap\"\n+        \"github.com/snapcore/snapd/snap/snapenv\"\n+        \"github.com/snapcore/snapd/strutil/shlex\"\n+        \"github.com/snapcore/snapd/timeutil\"\n+        \"github.com/snapcore/snapd/x11\"\n )\n \n var (\n-\tsyscallExec              = syscall.Exec\n-\tuserCurrent              = user.Current\n-\tosGetenv                 = os.Getenv\n-\ttimeNow                  = time.Now\n-\tselinuxIsEnabled         = selinux.IsEnabled\n-\tselinuxVerifyPathContext = selinux.VerifyPathContext\n-\tselinuxRestoreContext    = selinux.RestoreContext\n+        syscallExec              = syscall.Exec\n+        userCurrent              = user.Current\n+        osGetenv                 = os.Getenv\n+        timeNow                  = time.Now\n+        selinuxIsEnabled         = selinux.IsEnabled\n+        selinuxVerifyPathContext = selinux.VerifyPathContext\n+        selinuxRestoreContext    = selinux.RestoreContext\n )\n \n type cmdRun struct {\n-\tclientMixin\n-\tCommand  string `long:\"command\" hidden:\"yes\"`\n-\tHookName string `long:\"hook\" hidden:\"yes\"`\n-\tRevision string `short:\"r\" default:\"unset\" hidden:\"yes\"`\n-\tShell    bool   `long:\"shell\" `\n-\n-\t// This options is both a selector (use or don't use strace) and it\n-\t// can also carry extra options for strace. This is why there is\n-\t// \"default\" and \"optional-value\" to distinguish this.\n-\tStrace string `long:\"strace\" optional:\"true\" optional-value:\"with-strace\" default:\"no-strace\" default-mask:\"-\"`\n-\t// deprecated in favor of Gdbserver\n-\tGdb                   bool   `long:\"gdb\" hidden:\"yes\"`\n-\tGdbserver             string `long:\"gdbserver\" default:\"no-gdbserver\" optional-value:\":0\" optional:\"true\"`\n-\tExperimentalGdbserver string `long:\"experimental-gdbserver\" default:\"no-gdbserver\" optional-value:\":0\" optional:\"true\" hidden:\"yes\"`\n-\tTraceExec             bool   `long:\"trace-exec\"`\n-\n-\t// not a real option, used to check if cmdRun is initialized by\n-\t// the parser\n-\tParserRan int    `long:\"parser-ran\" default:\"1\" hidden:\"yes\"`\n-\tTimer     string `long:\"timer\" hidden:\"yes\"`\n+        clientMixin\n+        Command  string `long:\"command\" hidden:\"yes\"`\n+        HookName string `long:\"hook\" hidden:\"yes\"`\n+        Revision string `short:\"r\" default:\"unset\" hidden:\"yes\"`\n+        Shell    bool   `long:\"shell\" `\n+\n+        // This options is both a selector (use or don't use strace) and it\n+        // can also carry extra options for strace. This is why there is\n+        // \"default\" and \"optional-value\" to distinguish this.\n+        Strace string `long:\"strace\" optional:\"true\" optional-value:\"with-strace\" default:\"no-strace\" default-mask:\"-\"`\n+        // deprecated in favor of Gdbserver\n+        Gdb                   bool   `long:\"gdb\" hidden:\"yes\"`\n+        Gdbserver             string `long:\"gdbserver\" default:\"no-gdbserver\" optional-value:\":0\" optional:\"true\"`\n+        ExperimentalGdbserver string `long:\"experimental-gdbserver\" default:\"no-gdbserver\" optional-value:\":0\" optional:\"true\" hidden:\"yes\"`\n+        TraceExec             bool   `long:\"trace-exec\"`\n+\n+        // not a real option, used to check if cmdRun is initialized by\n+        // the parser\n+        ParserRan int    `long:\"parser-ran\" default:\"1\" hidden:\"yes\"`\n+        Timer     string `long:\"timer\" hidden:\"yes\"`\n }\n \n func init() {\n-\taddCommand(\"run\",\n-\t\ti18n.G(\"Run the given snap command\"),\n-\t\ti18n.G(`\n+        addCommand(\"run\",\n+                i18n.G(\"Run the given snap command\"),\n+                i18n.G(`\n The run command executes the given snap command with the right confinement\n and environment.\n `),\n-\t\tfunc() flags.Commander {\n-\t\t\treturn &cmdRun{}\n-\t\t}, map[string]string{\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"command\": i18n.G(\"Alternative command to run\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"hook\": i18n.G(\"Hook to run\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"r\": i18n.G(\"Use a specific snap revision when running hook\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"shell\": i18n.G(\"Run a shell instead of the command (useful for debugging)\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"strace\": i18n.G(\"Run the command under strace (useful for debugging). Extra strace options can be specified as well here. Pass --raw to strace early snap helpers.\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"gdb\": i18n.G(\"Run the command with gdb (deprecated, use --gdbserver instead)\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"gdbserver\":              i18n.G(\"Run the command with gdbserver\"),\n-\t\t\t\"experimental-gdbserver\": \"\",\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"timer\": i18n.G(\"Run as a timer service with given schedule\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"trace-exec\": i18n.G(\"Display exec calls timing data\"),\n-\t\t\t\"parser-ran\": \"\",\n-\t\t}, nil)\n+                func() flags.Commander {\n+                        return &cmdRun{}\n+                }, map[string]string{\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"command\": i18n.G(\"Alternative command to run\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"hook\": i18n.G(\"Hook to run\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"r\": i18n.G(\"Use a specific snap revision when running hook\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"shell\": i18n.G(\"Run a shell instead of the command (useful for debugging)\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"strace\": i18n.G(\"Run the command under strace (useful for debugging). Extra strace options can be specified as well here. Pass --raw to strace early snap helpers.\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"gdb\": i18n.G(\"Run the command with gdb (deprecated, use --gdbserver instead)\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"gdbserver\":              i18n.G(\"Run the command with gdbserver\"),\n+                        \"experimental-gdbserver\": \"\",\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"timer\": i18n.G(\"Run as a timer service with given schedule\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"trace-exec\": i18n.G(\"Display exec calls timing data\"),\n+                        \"parser-ran\": \"\",\n+                }, nil)\n }\n \n // isStopping returns true if the system is shutting down.\n func isStopping() (bool, error) {\n-\t// Make sure, just in case, that systemd doesn't localize the output string.\n-\tenv, err := osutil.OSEnvironment()\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n-\tenv[\"LC_MESSAGES\"] = \"C\"\n-\t// Check if systemd is stopping (shutting down or rebooting).\n-\tcmd := exec.Command(\"systemctl\", \"is-system-running\")\n-\tcmd.Env = env.ForExec()\n-\tstdout, err := cmd.Output()\n-\t// systemctl is-system-running returns non-zero for outcomes other than \"running\"\n-\t// As such, ignore any ExitError and just process the stdout buffer.\n-\tif _, ok := err.(*exec.ExitError); ok {\n-\t\treturn string(stdout) == \"stopping\\n\", nil\n-\t}\n-\treturn false, err\n+        // Make sure, just in case, that systemd doesn't localize the output string.\n+        env, err := osutil.OSEnvironment()\n+        if err != nil {\n+                return false, err\n+        }\n+        env[\"LC_MESSAGES\"] = \"C\"\n+        // Check if systemd is stopping (shutting down or rebooting).\n+        cmd := exec.Command(\"systemctl\", \"is-system-running\")\n+        cmd.Env = env.ForExec()\n+        stdout, err := cmd.Output()\n+        // systemctl is-system-running returns non-zero for outcomes other than \"running\"\n+        // As such, ignore any ExitError and just process the stdout buffer.\n+        if _, ok := err.(*exec.ExitError); ok {\n+                return string(stdout) == \"stopping\\n\", nil\n+        }\n+        return false, err\n }\n \n func maybeWaitForSecurityProfileRegeneration(cli *client.Client) error {\n-\t// check if the security profiles key has changed, if so, we need\n-\t// to wait for snapd to re-generate all profiles\n-\tmismatch, err := interfaces.SystemKeyMismatch()\n-\tif err == nil && !mismatch {\n-\t\treturn nil\n-\t}\n-\t// something went wrong with the system-key compare, try to\n-\t// reach snapd before continuing\n-\tif err != nil {\n-\t\tlogger.Debugf(\"SystemKeyMismatch returned an error: %v\", err)\n-\t}\n-\n-\t// We have a mismatch but maybe it is only because systemd is shutting down\n-\t// and core or snapd were already unmounted and we failed to re-execute.\n-\t// For context see: https://bugs.launchpad.net/snapd/+bug/1871652\n-\tstopping, err := isStopping()\n-\tif err != nil {\n-\t\tlogger.Debugf(\"cannot check if system is stopping: %s\", err)\n-\t}\n-\tif stopping {\n-\t\tlogger.Debugf(\"ignoring system key mismatch during system shutdown/reboot\")\n-\t\treturn nil\n-\t}\n-\n-\t// We have a mismatch, try to connect to snapd, once we can\n-\t// connect we just continue because that usually means that\n-\t// a new snapd is ready and has generated profiles.\n-\t//\n-\t// There is a corner case if an upgrade leaves the old snapd\n-\t// running and we connect to the old snapd. Handling this\n-\t// correctly is tricky because our \"snap run\" pipeline may\n-\t// depend on profiles written by the new snapd. So for now we\n-\t// just continue and hope for the best. The real fix for this\n-\t// is to fix the packaging so that snapd is stopped, upgraded\n-\t// and started.\n-\t//\n-\t// connect timeout for client is 5s on each try, so 12*5s = 60s\n-\ttimeout := 12\n-\tif timeoutEnv := os.Getenv(\"SNAPD_DEBUG_SYSTEM_KEY_RETRY\"); timeoutEnv != \"\" {\n-\t\tif i, err := strconv.Atoi(timeoutEnv); err == nil {\n-\t\t\ttimeout = i\n-\t\t}\n-\t}\n-\n-\tlogger.Debugf(\"system key mismatch detected, waiting for snapd to start responding...\")\n-\n-\tfor i := 0; i < timeout; i++ {\n-\t\t// TODO: we could also check cli.Maintenance() here too in case snapd is\n-\t\t// down semi-permanently for a refresh, but what message do we show to\n-\t\t// the user or what do we do if we know snapd is down for maintenance?\n-\t\tif _, err := cli.SysInfo(); err == nil {\n-\t\t\treturn nil\n-\t\t}\n-\t\t// sleep a little bit for good measure\n-\t\ttime.Sleep(1 * time.Second)\n-\t}\n-\n-\treturn fmt.Errorf(\"timeout waiting for snap system profiles to get updated\")\n+        // check if the security profiles key has changed, if so, we need\n+        // to wait for snapd to re-generate all profiles\n+        mismatch, err := interfaces.SystemKeyMismatch()\n+        if err == nil && !mismatch {\n+                return nil\n+        }\n+        // something went wrong with the system-key compare, try to\n+        // reach snapd before continuing\n+        if err != nil {\n+                logger.Debugf(\"SystemKeyMismatch returned an error: %v\", err)\n+        }\n+\n+        // We have a mismatch but maybe it is only because systemd is shutting down\n+        // and core or snapd were already unmounted and we failed to re-execute.\n+        // For context see: https://bugs.launchpad.net/snapd/+bug/1871652\n+        stopping, err := isStopping()\n+        if err != nil {\n+                logger.Debugf(\"cannot check if system is stopping: %s\", err)\n+        }\n+        if stopping {\n+                logger.Debugf(\"ignoring system key mismatch during system shutdown/reboot\")\n+                return nil\n+        }\n+\n+        // We have a mismatch, try to connect to snapd, once we can\n+        // connect we just continue because that usually means that\n+        // a new snapd is ready and has generated profiles.\n+        //\n+        // There is a corner case if an upgrade leaves the old snapd\n+        // running and we connect to the old snapd. Handling this\n+        // correctly is tricky because our \"snap run\" pipeline may\n+        // depend on profiles written by the new snapd. So for now we\n+        // just continue and hope for the best. The real fix for this\n+        // is to fix the packaging so that snapd is stopped, upgraded\n+        // and started.\n+        //\n+        // connect timeout for client is 5s on each try, so 12*5s = 60s\n+        timeout := 12\n+        if timeoutEnv := os.Getenv(\"SNAPD_DEBUG_SYSTEM_KEY_RETRY\"); timeoutEnv != \"\" {\n+                if i, err := strconv.Atoi(timeoutEnv); err == nil {\n+                        timeout = i\n+                }\n+        }\n+\n+        logger.Debugf(\"system key mismatch detected, waiting for snapd to start responding...\")\n+\n+        for i := 0; i < timeout; i++ {\n+                // TODO: we could also check cli.Maintenance() here too in case snapd is\n+                // down semi-permanently for a refresh, but what message do we show to\n+                // the user or what do we do if we know snapd is down for maintenance?\n+                if _, err := cli.SysInfo(); err == nil {\n+                        return nil\n+                }\n+                // sleep a little bit for good measure\n+                time.Sleep(1 * time.Second)\n+        }\n+\n+        return fmt.Errorf(\"timeout waiting for snap system profiles to get updated\")\n }\n \n func (x *cmdRun) Usage() string {\n-\treturn \"[run-OPTIONS] <NAME-OF-SNAP>.<NAME-OF-APP> [<SNAP-APP-ARG>...]\"\n+        return \"[run-OPTIONS] <NAME-OF-SNAP>.<NAME-OF-APP> [<SNAP-APP-ARG>...]\"\n }\n \n func (x *cmdRun) Execute(args []string) error {\n-\tif len(args) == 0 {\n-\t\treturn fmt.Errorf(i18n.G(\"need the application to run as argument\"))\n-\t}\n-\tsnapApp := args[0]\n-\targs = args[1:]\n-\n-\t// Catch some invalid parameter combinations, provide helpful errors\n-\toptionsSet := 0\n-\tfor _, param := range []string{x.HookName, x.Command, x.Timer} {\n-\t\tif param != \"\" {\n-\t\t\toptionsSet++\n-\t\t}\n-\t}\n-\tif optionsSet > 1 {\n-\t\treturn fmt.Errorf(\"you can only use one of --hook, --command, and --timer\")\n-\t}\n-\n-\tif x.Revision != \"unset\" && x.Revision != \"\" && x.HookName == \"\" {\n-\t\treturn fmt.Errorf(i18n.G(\"-r can only be used with --hook\"))\n-\t}\n-\tif x.HookName != \"\" && len(args) > 0 {\n-\t\t// TRANSLATORS: %q is the hook name; %s a space-separated list of extra arguments\n-\t\treturn fmt.Errorf(i18n.G(\"too many arguments for hook %q: %s\"), x.HookName, strings.Join(args, \" \"))\n-\t}\n-\n-\tif err := maybeWaitForSecurityProfileRegeneration(x.client); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Now actually handle the dispatching\n-\tif x.HookName != \"\" {\n-\t\treturn x.snapRunHook(snapApp)\n-\t}\n-\n-\tif x.Command == \"complete\" {\n-\t\tsnapApp, args = antialias(snapApp, args)\n-\t}\n-\n-\tif x.Timer != \"\" {\n-\t\treturn x.snapRunTimer(snapApp, x.Timer, args)\n-\t}\n-\n-\treturn x.snapRunApp(snapApp, args)\n+        if len(args) == 0 {\n+                return fmt.Errorf(i18n.G(\"need the application to run as argument\"))\n+        }\n+        snapApp := args[0]\n+        args = args[1:]\n+\n+        // Catch some invalid parameter combinations, provide helpful errors\n+        optionsSet := 0\n+        for _, param := range []string{x.HookName, x.Command, x.Timer} {\n+                if param != \"\" {\n+                        optionsSet++\n+                }\n+        }\n+        if optionsSet > 1 {\n+                return fmt.Errorf(\"you can only use one of --hook, --command, and --timer\")\n+        }\n+\n+        if x.Revision != \"unset\" && x.Revision != \"\" && x.HookName == \"\" {\n+                return fmt.Errorf(i18n.G(\"-r can only be used with --hook\"))\n+        }\n+        if x.HookName != \"\" && len(args) > 0 {\n+                // TRANSLATORS: %q is the hook name; %s a space-separated list of extra arguments\n+                return fmt.Errorf(i18n.G(\"too many arguments for hook %q: %s\"), x.HookName, strings.Join(args, \" \"))\n+        }\n+\n+        if err := maybeWaitForSecurityProfileRegeneration(x.client); err != nil {\n+                return err\n+        }\n+\n+        // Now actually handle the dispatching\n+        if x.HookName != \"\" {\n+                return x.snapRunHook(snapApp)\n+        }\n+\n+        if x.Command == \"complete\" {\n+                snapApp, args = antialias(snapApp, args)\n+        }\n+\n+        if x.Timer != \"\" {\n+                return x.snapRunTimer(snapApp, x.Timer, args)\n+        }\n+\n+        return x.snapRunApp(snapApp, args)\n }\n \n func maybeWaitWhileInhibited(snapName string) error {\n-\t// If the snap is inhibited from being used then postpone running it until\n-\t// that condition passes. Inhibition UI can be dismissed by the user, in\n-\t// which case we don't run the application at all.\n-\tif features.RefreshAppAwareness.IsEnabled() {\n-\t\treturn waitWhileInhibited(snapName)\n-\t}\n-\treturn nil\n+        // If the snap is inhibited from being used then postpone running it until\n+        // that condition passes. Inhibition UI can be dismissed by the user, in\n+        // which case we don't run the application at all.\n+        if features.RefreshAppAwareness.IsEnabled() {\n+                return waitWhileInhibited(snapName)\n+        }\n+        return nil\n }\n \n // antialias changes snapApp and args if snapApp is actually an alias\n // for something else. If not, or if the args aren't what's expected\n // for completion, it returns them unchanged.\n func antialias(snapApp string, args []string) (string, []string) {\n-\tif len(args) < 7 {\n-\t\t// NOTE if len(args) < 7, Something is Wrong (at least WRT complete.sh and etelpmoc.sh)\n-\t\treturn snapApp, args\n-\t}\n-\n-\tactualApp, err := resolveApp(snapApp)\n-\tif err != nil || actualApp == snapApp {\n-\t\t// no alias! woop.\n-\t\treturn snapApp, args\n-\t}\n-\n-\tcompPoint, err := strconv.Atoi(args[2])\n-\tif err != nil {\n-\t\t// args[2] is not COMP_POINT\n-\t\treturn snapApp, args\n-\t}\n-\n-\tif compPoint <= len(snapApp) {\n-\t\t// COMP_POINT is inside $0\n-\t\treturn snapApp, args\n-\t}\n-\n-\tif compPoint > len(args[5]) {\n-\t\t// COMP_POINT is bigger than $#\n-\t\treturn snapApp, args\n-\t}\n-\n-\tif args[6] != snapApp {\n-\t\t// args[6] is not COMP_WORDS[0]\n-\t\treturn snapApp, args\n-\t}\n-\n-\t// it _should_ be COMP_LINE followed by one of\n-\t// COMP_WORDBREAKS, but that's hard to do\n-\tre, err := regexp.Compile(`^` + regexp.QuoteMeta(snapApp) + `\\b`)\n-\tif err != nil || !re.MatchString(args[5]) {\n-\t\t// (weird regexp error, or) args[5] is not COMP_LINE\n-\t\treturn snapApp, args\n-\t}\n-\n-\targsOut := make([]string, len(args))\n-\tcopy(argsOut, args)\n-\n-\targsOut[2] = strconv.Itoa(compPoint - len(snapApp) + len(actualApp))\n-\targsOut[5] = re.ReplaceAllLiteralString(args[5], actualApp)\n-\targsOut[6] = actualApp\n-\n-\treturn actualApp, argsOut\n+        if len(args) < 7 {\n+                // NOTE if len(args) < 7, Something is Wrong (at least WRT complete.sh and etelpmoc.sh)\n+                return snapApp, args\n+        }\n+\n+        actualApp, err := resolveApp(snapApp)\n+        if err != nil || actualApp == snapApp {\n+                // no alias! woop.\n+                return snapApp, args\n+        }\n+\n+        compPoint, err := strconv.Atoi(args[2])\n+        if err != nil {\n+                // args[2] is not COMP_POINT\n+                return snapApp, args\n+        }\n+\n+        if compPoint <= len(snapApp) {\n+                // COMP_POINT is inside $0\n+                return snapApp, args\n+        }\n+\n+        if compPoint > len(args[5]) {\n+                // COMP_POINT is bigger than $#\n+                return snapApp, args\n+        }\n+\n+        if args[6] != snapApp {\n+                // args[6] is not COMP_WORDS[0]\n+                return snapApp, args\n+        }\n+\n+        // it _should_ be COMP_LINE followed by one of\n+        // COMP_WORDBREAKS, but that's hard to do\n+        re, err := regexp.Compile(`^` + regexp.QuoteMeta(snapApp) + `\\b`)\n+        if err != nil || !re.MatchString(args[5]) {\n+                // (weird regexp error, or) args[5] is not COMP_LINE\n+                return snapApp, args\n+        }\n+\n+        argsOut := make([]string, len(args))\n+        copy(argsOut, args)\n+\n+        argsOut[2] = strconv.Itoa(compPoint - len(snapApp) + len(actualApp))\n+        argsOut[5] = re.ReplaceAllLiteralString(args[5], actualApp)\n+        argsOut[6] = actualApp\n+\n+        return actualApp, argsOut\n }\n \n func getSnapInfo(snapName string, revision snap.Revision) (info *snap.Info, err error) {\n-\tif revision.Unset() {\n-\t\tinfo, err = snap.ReadCurrentInfo(snapName)\n-\t} else {\n-\t\tinfo, err = snap.ReadInfo(snapName, &snap.SideInfo{\n-\t\t\tRevision: revision,\n-\t\t})\n-\t}\n-\n-\treturn info, err\n+        if revision.Unset() {\n+                info, err = snap.ReadCurrentInfo(snapName)\n+        } else {\n+                info, err = snap.ReadInfo(snapName, &snap.SideInfo{\n+                        Revision: revision,\n+                })\n+        }\n+\n+        return info, err\n }\n \n func createOrUpdateUserDataSymlink(info *snap.Info, usr *user.User) error {\n-\t// 'current' symlink for user data (SNAP_USER_DATA)\n-\tuserData := info.UserDataDir(usr.HomeDir)\n-\twantedSymlinkValue := filepath.Base(userData)\n-\tcurrentActiveSymlink := filepath.Join(userData, \"..\", \"current\")\n-\n-\tvar err error\n-\tvar currentSymlinkValue string\n-\tfor i := 0; i < 5; i++ {\n-\t\tcurrentSymlinkValue, err = os.Readlink(currentActiveSymlink)\n-\t\t// Failure other than non-existing symlink is fatal\n-\t\tif err != nil && !os.IsNotExist(err) {\n-\t\t\t// TRANSLATORS: %v the error message\n-\t\t\treturn fmt.Errorf(i18n.G(\"cannot read symlink: %v\"), err)\n-\t\t}\n-\n-\t\tif currentSymlinkValue == wantedSymlinkValue {\n-\t\t\tbreak\n-\t\t}\n-\n-\t\tif err == nil {\n-\t\t\t// We may be racing with other instances of snap-run that try to do the same thing\n-\t\t\t// If the symlink is already removed then we can ignore this error.\n-\t\t\terr = os.Remove(currentActiveSymlink)\n-\t\t\tif err != nil && !os.IsNotExist(err) {\n-\t\t\t\t// abort with error\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\n-\t\terr = os.Symlink(wantedSymlinkValue, currentActiveSymlink)\n-\t\t// Error other than symlink already exists will abort and be propagated\n-\t\tif err == nil || !os.IsExist(err) {\n-\t\t\tbreak\n-\t\t}\n-\t\t// If we arrived here it means the symlink couldn't be created because it got created\n-\t\t// in the meantime by another instance, so we will try again.\n-\t}\n-\tif err != nil {\n-\t\treturn fmt.Errorf(i18n.G(\"cannot update the 'current' symlink of %q: %v\"), currentActiveSymlink, err)\n-\t}\n-\treturn nil\n+        // 'current' symlink for user data (SNAP_USER_DATA)\n+        userData := info.UserDataDir(usr.HomeDir)\n+        wantedSymlinkValue := filepath.Base(userData)\n+        currentActiveSymlink := filepath.Join(userData, \"..\", \"current\")\n+\n+        var err error\n+        var currentSymlinkValue string\n+        for i := 0; i < 5; i++ {\n+                currentSymlinkValue, err = os.Readlink(currentActiveSymlink)\n+                // Failure other than non-existing symlink is fatal\n+                if err != nil && !os.IsNotExist(err) {\n+                        // TRANSLATORS: %v the error message\n+                        return fmt.Errorf(i18n.G(\"cannot read symlink: %v\"), err)\n+                }\n+\n+                if currentSymlinkValue == wantedSymlinkValue {\n+                        break\n+                }\n+\n+                if err == nil {\n+                        // We may be racing with other instances of snap-run that try to do the same thing\n+                        // If the symlink is already removed then we can ignore this error.\n+                        err = os.Remove(currentActiveSymlink)\n+                        if err != nil && !os.IsNotExist(err) {\n+                                // abort with error\n+                                break\n+                        }\n+                }\n+\n+                err = os.Symlink(wantedSymlinkValue, currentActiveSymlink)\n+                // Error other than symlink already exists will abort and be propagated\n+                if err == nil || !os.IsExist(err) {\n+                        break\n+                }\n+                // If we arrived here it means the symlink couldn't be created because it got created\n+                // in the meantime by another instance, so we will try again.\n+        }\n+        if err != nil {\n+                return fmt.Errorf(i18n.G(\"cannot update the 'current' symlink of %q: %v\"), currentActiveSymlink, err)\n+        }\n+        return nil\n }\n \n func createUserDataDirs(info *snap.Info) error {\n-\t// Adjust umask so that the created directories have the permissions we\n-\t// expect and are unaffected by the initial umask. While go runtime creates\n-\t// threads at will behind the scenes, the setting of umask applies to the\n-\t// entire process so it doesn't need any special handling to lock the\n-\t// executing goroutine to a single thread.\n-\toldUmask := syscall.Umask(0)\n-\tdefer syscall.Umask(oldUmask)\n-\n-\tusr, err := userCurrent()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(i18n.G(\"cannot get the current user: %v\"), err)\n-\t}\n-\n-\t// see snapenv.User\n-\tinstanceUserData := info.UserDataDir(usr.HomeDir)\n-\tinstanceCommonUserData := info.UserCommonDataDir(usr.HomeDir)\n-\tcreateDirs := []string{instanceUserData, instanceCommonUserData}\n-\tif info.InstanceKey != \"\" {\n-\t\t// parallel instance snaps get additional mapping in their mount\n-\t\t// namespace, namely /home/joe/snap/foo_bar ->\n-\t\t// /home/joe/snap/foo, make sure that the mount point exists and\n-\t\t// is owned by the user\n-\t\tsnapUserDir := snap.UserSnapDir(usr.HomeDir, info.SnapName())\n-\t\tcreateDirs = append(createDirs, snapUserDir)\n-\t}\n-\tfor _, d := range createDirs {\n-\t\tif err := os.MkdirAll(d, 0755); err != nil {\n-\t\t\t// TRANSLATORS: %q is the directory whose creation failed, %v the error message\n-\t\t\treturn fmt.Errorf(i18n.G(\"cannot create %q: %v\"), d, err)\n-\t\t}\n-\t}\n-\n-\tif err := createOrUpdateUserDataSymlink(info, usr); err != nil {\n-\t\treturn err\n-\t}\n-\n-\treturn maybeRestoreSecurityContext(usr)\n+        // Adjust umask so that the created directories have the permissions we\n+        // expect and are unaffected by the initial umask. While go runtime creates\n+        // threads at will behind the scenes, the setting of umask applies to the\n+        // entire process so it doesn't need any special handling to lock the\n+        // executing goroutine to a single thread.\n+        oldUmask := syscall.Umask(0)\n+        defer syscall.Umask(oldUmask)\n+\n+        usr, err := userCurrent()\n+        if err != nil {\n+                return fmt.Errorf(i18n.G(\"cannot get the current user: %v\"), err)\n+        }\n+\n+        // see snapenv.User\n+        instanceUserData := info.UserDataDir(usr.HomeDir)\n+        instanceCommonUserData := info.UserCommonDataDir(usr.HomeDir)\n+        createDirs := []string{instanceUserData, instanceCommonUserData}\n+        if info.InstanceKey != \"\" {\n+                // parallel instance snaps get additional mapping in their mount\n+                // namespace, namely /home/joe/snap/foo_bar ->\n+                // /home/joe/snap/foo, make sure that the mount point exists and\n+                // is owned by the user\n+                snapUserDir := snap.UserSnapDir(usr.HomeDir, info.SnapName())\n+                createDirs = append(createDirs, snapUserDir)\n+        }\n+        for _, d := range createDirs {\n+                                if err := os.MkdirAll(d, 0700); err != nil { // a change has been made here\n+                        // TRANSLATORS: %q is the directory whose creation failed, %v the error message\n+                        return fmt.Errorf(i18n.G(\"cannot create %q: %v\"), d, err)\n+                }\n+        }\n+\n+        if err := createOrUpdateUserDataSymlink(info, usr); err != nil {\n+                return err\n+        }\n+\n+        return maybeRestoreSecurityContext(usr)\n }\n \n // maybeRestoreSecurityContext attempts to restore security context of ~/snap on\n // systems where it's applicable\n func maybeRestoreSecurityContext(usr *user.User) error {\n-\tsnapUserHome := filepath.Join(usr.HomeDir, dirs.UserHomeSnapDir)\n-\tenabled, err := selinuxIsEnabled()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"cannot determine SELinux status: %v\", err)\n-\t}\n-\tif !enabled {\n-\t\tlogger.Debugf(\"SELinux not enabled\")\n-\t\treturn nil\n-\t}\n-\n-\tmatch, err := selinuxVerifyPathContext(snapUserHome)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"failed to verify SELinux context of %v: %v\", snapUserHome, err)\n-\t}\n-\tif match {\n-\t\treturn nil\n-\t}\n-\tlogger.Noticef(\"restoring default SELinux context of %v\", snapUserHome)\n-\n-\tif err := selinuxRestoreContext(snapUserHome, selinux.RestoreMode{Recursive: true}); err != nil {\n-\t\treturn fmt.Errorf(\"cannot restore SELinux context of %v: %v\", snapUserHome, err)\n-\t}\n-\treturn nil\n+        snapUserHome := filepath.Join(usr.HomeDir, dirs.UserHomeSnapDir)\n+        enabled, err := selinuxIsEnabled()\n+        if err != nil {\n+                return fmt.Errorf(\"cannot determine SELinux status: %v\", err)\n+        }\n+        if !enabled {\n+                logger.Debugf(\"SELinux not enabled\")\n+                return nil\n+        }\n+\n+        match, err := selinuxVerifyPathContext(snapUserHome)\n+        if err != nil {\n+                return fmt.Errorf(\"failed to verify SELinux context of %v: %v\", snapUserHome, err)\n+        }\n+        if match {\n+                return nil\n+        }\n+        logger.Noticef(\"restoring default SELinux context of %v\", snapUserHome)\n+\n+        if err := selinuxRestoreContext(snapUserHome, selinux.RestoreMode{Recursive: true}); err != nil {\n+                return fmt.Errorf(\"cannot restore SELinux context of %v: %v\", snapUserHome, err)\n+        }\n+        return nil\n }\n \n func (x *cmdRun) useStrace() bool {\n-\t// make sure the go-flag parser ran and assigned default values\n-\treturn x.ParserRan == 1 && x.Strace != \"no-strace\"\n+        // make sure the go-flag parser ran and assigned default values\n+        return x.ParserRan == 1 && x.Strace != \"no-strace\"\n }\n \n func (x *cmdRun) straceOpts() (opts []string, raw bool, err error) {\n-\tif x.Strace == \"with-strace\" {\n-\t\treturn nil, false, nil\n-\t}\n-\n-\tsplit, err := shlex.Split(x.Strace)\n-\tif err != nil {\n-\t\treturn nil, false, err\n-\t}\n-\n-\topts = make([]string, 0, len(split))\n-\tfor _, opt := range split {\n-\t\tif opt == \"--raw\" {\n-\t\t\traw = true\n-\t\t\tcontinue\n-\t\t}\n-\t\topts = append(opts, opt)\n-\t}\n-\treturn opts, raw, nil\n+        if x.Strace == \"with-strace\" {\n+                return nil, false, nil\n+        }\n+\n+        split, err := shlex.Split(x.Strace)\n+        if err != nil {\n+                return nil, false, err\n+        }\n+\n+        opts = make([]string, 0, len(split))\n+        for _, opt := range split {\n+                if opt == \"--raw\" {\n+                        raw = true\n+                        continue\n+                }\n+                opts = append(opts, opt)\n+        }\n+        return opts, raw, nil\n }\n \n func (x *cmdRun) snapRunApp(snapApp string, args []string) error {\n-\tsnapName, appName := snap.SplitSnapApp(snapApp)\n-\tinfo, err := getSnapInfo(snapName, snap.R(0))\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tapp := info.Apps[appName]\n-\tif app == nil {\n-\t\treturn fmt.Errorf(i18n.G(\"cannot find app %q in %q\"), appName, snapName)\n-\t}\n-\n-\tif !app.IsService() {\n-\t\tif err := maybeWaitWhileInhibited(snapName); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\treturn x.runSnapConfine(info, app.SecurityTag(), snapApp, \"\", args)\n+        snapName, appName := snap.SplitSnapApp(snapApp)\n+        info, err := getSnapInfo(snapName, snap.R(0))\n+        if err != nil {\n+                return err\n+        }\n+\n+        app := info.Apps[appName]\n+        if app == nil {\n+                return fmt.Errorf(i18n.G(\"cannot find app %q in %q\"), appName, snapName)\n+        }\n+\n+        if !app.IsService() {\n+                if err := maybeWaitWhileInhibited(snapName); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        return x.runSnapConfine(info, app.SecurityTag(), snapApp, \"\", args)\n }\n \n func (x *cmdRun) snapRunHook(snapName string) error {\n-\trevision, err := snap.ParseRevision(x.Revision)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tinfo, err := getSnapInfo(snapName, revision)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\thook := info.Hooks[x.HookName]\n-\tif hook == nil {\n-\t\treturn fmt.Errorf(i18n.G(\"cannot find hook %q in %q\"), x.HookName, snapName)\n-\t}\n-\n-\treturn x.runSnapConfine(info, hook.SecurityTag(), snapName, hook.Name, nil)\n+        revision, err := snap.ParseRevision(x.Revision)\n+        if err != nil {\n+                return err\n+        }\n+\n+        info, err := getSnapInfo(snapName, revision)\n+        if err != nil {\n+                return err\n+        }\n+\n+        hook := info.Hooks[x.HookName]\n+        if hook == nil {\n+                return fmt.Errorf(i18n.G(\"cannot find hook %q in %q\"), x.HookName, snapName)\n+        }\n+\n+        return x.runSnapConfine(info, hook.SecurityTag(), snapName, hook.Name, nil)\n }\n \n func (x *cmdRun) snapRunTimer(snapApp, timer string, args []string) error {\n-\tschedule, err := timeutil.ParseSchedule(timer)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"invalid timer format: %v\", err)\n-\t}\n-\n-\tnow := timeNow()\n-\tif !timeutil.Includes(schedule, now) {\n-\t\tfmt.Fprintf(Stderr, \"%s: attempted to run %q timer outside of scheduled time %q\\n\", now.Format(time.RFC3339), snapApp, timer)\n-\t\treturn nil\n-\t}\n-\n-\treturn x.snapRunApp(snapApp, args)\n+        schedule, err := timeutil.ParseSchedule(timer)\n+        if err != nil {\n+                return fmt.Errorf(\"invalid timer format: %v\", err)\n+        }\n+\n+        now := timeNow()\n+        if !timeutil.Includes(schedule, now) {\n+                fmt.Fprintf(Stderr, \"%s: attempted to run %q timer outside of scheduled time %q\\n\", now.Format(time.RFC3339), snapApp, timer)\n+                return nil\n+        }\n+\n+        return x.snapRunApp(snapApp, args)\n }\n \n var osReadlink = os.Readlink\n@@ -527,253 +527,253 @@ var osReadlink = os.Readlink\n // snapdHelperPath return the path of a helper like \"snap-confine\" or\n // \"snap-exec\" based on if snapd is re-execed or not\n func snapdHelperPath(toolName string) (string, error) {\n-\texe, err := osReadlink(\"/proc/self/exe\")\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"cannot read /proc/self/exe: %v\", err)\n-\t}\n-\t// no re-exec\n-\tif !strings.HasPrefix(exe, dirs.SnapMountDir) {\n-\t\treturn filepath.Join(dirs.DistroLibExecDir, toolName), nil\n-\t}\n-\t// The logic below only works if the last two path components\n-\t// are /usr/bin\n-\t// FIXME: use a snap warning?\n-\tif !strings.HasSuffix(exe, \"/usr/bin/\"+filepath.Base(exe)) {\n-\t\tlogger.Noticef(\"(internal error): unexpected exe input in snapdHelperPath: %v\", exe)\n-\t\treturn filepath.Join(dirs.DistroLibExecDir, toolName), nil\n-\t}\n-\t// snapBase will be \"/snap/{core,snapd}/$rev/\" because\n-\t// the snap binary is always at $root/usr/bin/snap\n-\tsnapBase := filepath.Clean(filepath.Join(filepath.Dir(exe), \"..\", \"..\"))\n-\t// Run snap-confine from the core/snapd snap.  The tools in\n-\t// core/snapd snap are statically linked, or mostly\n-\t// statically, with the exception of libraries such as libudev\n-\t// and libc.\n-\treturn filepath.Join(snapBase, dirs.CoreLibExecDir, toolName), nil\n+        exe, err := osReadlink(\"/proc/self/exe\")\n+        if err != nil {\n+                return \"\", fmt.Errorf(\"cannot read /proc/self/exe: %v\", err)\n+        }\n+        // no re-exec\n+        if !strings.HasPrefix(exe, dirs.SnapMountDir) {\n+                return filepath.Join(dirs.DistroLibExecDir, toolName), nil\n+        }\n+        // The logic below only works if the last two path components\n+        // are /usr/bin\n+        // FIXME: use a snap warning?\n+        if !strings.HasSuffix(exe, \"/usr/bin/\"+filepath.Base(exe)) {\n+                logger.Noticef(\"(internal error): unexpected exe input in snapdHelperPath: %v\", exe)\n+                return filepath.Join(dirs.DistroLibExecDir, toolName), nil\n+        }\n+        // snapBase will be \"/snap/{core,snapd}/$rev/\" because\n+        // the snap binary is always at $root/usr/bin/snap\n+        snapBase := filepath.Clean(filepath.Join(filepath.Dir(exe), \"..\", \"..\"))\n+        // Run snap-confine from the core/snapd snap.  The tools in\n+        // core/snapd snap are statically linked, or mostly\n+        // statically, with the exception of libraries such as libudev\n+        // and libc.\n+        return filepath.Join(snapBase, dirs.CoreLibExecDir, toolName), nil\n }\n \n func migrateXauthority(info *snap.Info) (string, error) {\n-\tu, err := userCurrent()\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(i18n.G(\"cannot get the current user: %s\"), err)\n-\t}\n-\n-\t// If our target directory (XDG_RUNTIME_DIR) doesn't exist we\n-\t// don't attempt to create it.\n-\tbaseTargetDir := filepath.Join(dirs.XdgRuntimeDirBase, u.Uid)\n-\tif !osutil.FileExists(baseTargetDir) {\n-\t\treturn \"\", nil\n-\t}\n-\n-\txauthPath := osGetenv(\"XAUTHORITY\")\n-\tif len(xauthPath) == 0 || !osutil.FileExists(xauthPath) {\n-\t\t// Nothing to do for us. Most likely running outside of any\n-\t\t// graphical X11 session.\n-\t\treturn \"\", nil\n-\t}\n-\n-\tfin, err := os.Open(xauthPath)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tdefer fin.Close()\n-\n-\t// Abs() also calls Clean(); see https://golang.org/pkg/path/filepath/#Abs\n-\txauthPathAbs, err := filepath.Abs(fin.Name())\n-\tif err != nil {\n-\t\treturn \"\", nil\n-\t}\n-\n-\t// Remove all symlinks from path\n-\txauthPathCan, err := filepath.EvalSymlinks(xauthPathAbs)\n-\tif err != nil {\n-\t\treturn \"\", nil\n-\t}\n-\n-\t// Ensure the XAUTHORITY env is not abused by checking that\n-\t// it point to exactly the file we just opened (no symlinks,\n-\t// no funny \"../..\" etc)\n-\tif fin.Name() != xauthPathCan {\n-\t\tlogger.Noticef(\"WARNING: XAUTHORITY environment value is not a clean path: %q\", xauthPathCan)\n-\t\treturn \"\", nil\n-\t}\n-\n-\t// Only do the migration from /tmp since the real /tmp is not visible for snaps\n-\tif !strings.HasPrefix(fin.Name(), \"/tmp/\") {\n-\t\treturn \"\", nil\n-\t}\n-\n-\t// We are performing a Stat() here to make sure that the user can't\n-\t// steal another user's Xauthority file. Note that while Stat() uses\n-\t// fstat() on the file descriptor created during Open(), the file might\n-\t// have changed ownership between the Open() and the Stat(). That's ok\n-\t// because we aren't trying to block access that the user already has:\n-\t// if the user has the privileges to chown another user's Xauthority\n-\t// file, we won't block that since the user can just steal it without\n-\t// having to use snap run. This code is just to ensure that a user who\n-\t// doesn't have those privileges can't steal the file via snap run\n-\t// (also note that the (potentially untrusted) snap isn't running yet).\n-\tfi, err := fin.Stat()\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tsys := fi.Sys()\n-\tif sys == nil {\n-\t\treturn \"\", fmt.Errorf(i18n.G(\"cannot validate owner of file %s\"), fin.Name())\n-\t}\n-\t// cheap comparison as the current uid is only available as a string\n-\t// but it is better to convert the uid from the stat result to a\n-\t// string than a string into a number.\n-\tif fmt.Sprintf(\"%d\", sys.(*syscall.Stat_t).Uid) != u.Uid {\n-\t\treturn \"\", fmt.Errorf(i18n.G(\"Xauthority file isn't owned by the current user %s\"), u.Uid)\n-\t}\n-\n-\ttargetPath := filepath.Join(baseTargetDir, \".Xauthority\")\n-\n-\t// Only validate Xauthority file again when both files don't match\n-\t// otherwise we can continue using the existing Xauthority file.\n-\t// This is ok to do here because we aren't trying to protect against\n-\t// the user changing the Xauthority file in XDG_RUNTIME_DIR outside\n-\t// of snapd.\n-\tif osutil.FileExists(targetPath) {\n-\t\tvar fout *os.File\n-\t\tif fout, err = os.Open(targetPath); err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\tif osutil.StreamsEqual(fin, fout) {\n-\t\t\tfout.Close()\n-\t\t\treturn targetPath, nil\n-\t\t}\n-\n-\t\tfout.Close()\n-\t\tif err := os.Remove(targetPath); err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\n-\t\t// Ensure we're validating the Xauthority file from the beginning\n-\t\tif _, err := fin.Seek(int64(os.SEEK_SET), 0); err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t}\n-\n-\t// To guard against setting XAUTHORITY to non-xauth files, check\n-\t// that we have a valid Xauthority. Specifically, the file must be\n-\t// parseable as an Xauthority file and not be empty.\n-\tif err := x11.ValidateXauthority(fin); err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\t// Read data from the beginning of the file\n-\tif _, err = fin.Seek(int64(os.SEEK_SET), 0); err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\tfout, err := os.OpenFile(targetPath, os.O_WRONLY|os.O_CREATE|os.O_EXCL, 0600)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tdefer fout.Close()\n-\n-\t// Read and write validated Xauthority file to its right location\n-\tif _, err = io.Copy(fout, fin); err != nil {\n-\t\tif err := os.Remove(targetPath); err != nil {\n-\t\t\tlogger.Noticef(\"WARNING: cannot remove file at %s: %s\", targetPath, err)\n-\t\t}\n-\t\treturn \"\", fmt.Errorf(i18n.G(\"cannot write new Xauthority file at %s: %s\"), targetPath, err)\n-\t}\n-\n-\treturn targetPath, nil\n+        u, err := userCurrent()\n+        if err != nil {\n+                return \"\", fmt.Errorf(i18n.G(\"cannot get the current user: %s\"), err)\n+        }\n+\n+        // If our target directory (XDG_RUNTIME_DIR) doesn't exist we\n+        // don't attempt to create it.\n+        baseTargetDir := filepath.Join(dirs.XdgRuntimeDirBase, u.Uid)\n+        if !osutil.FileExists(baseTargetDir) {\n+                return \"\", nil\n+        }\n+\n+        xauthPath := osGetenv(\"XAUTHORITY\")\n+        if len(xauthPath) == 0 || !osutil.FileExists(xauthPath) {\n+                // Nothing to do for us. Most likely running outside of any\n+                // graphical X11 session.\n+                return \"\", nil\n+        }\n+\n+        fin, err := os.Open(xauthPath)\n+        if err != nil {\n+                return \"\", err\n+        }\n+        defer fin.Close()\n+\n+        // Abs() also calls Clean(); see https://golang.org/pkg/path/filepath/#Abs\n+        xauthPathAbs, err := filepath.Abs(fin.Name())\n+        if err != nil {\n+                return \"\", nil\n+        }\n+\n+        // Remove all symlinks from path\n+        xauthPathCan, err := filepath.EvalSymlinks(xauthPathAbs)\n+        if err != nil {\n+                return \"\", nil\n+        }\n+\n+        // Ensure the XAUTHORITY env is not abused by checking that\n+        // it point to exactly the file we just opened (no symlinks,\n+        // no funny \"../..\" etc)\n+        if fin.Name() != xauthPathCan {\n+                logger.Noticef(\"WARNING: XAUTHORITY environment value is not a clean path: %q\", xauthPathCan)\n+                return \"\", nil\n+        }\n+\n+        // Only do the migration from /tmp since the real /tmp is not visible for snaps\n+        if !strings.HasPrefix(fin.Name(), \"/tmp/\") {\n+                return \"\", nil\n+        }\n+\n+        // We are performing a Stat() here to make sure that the user can't\n+        // steal another user's Xauthority file. Note that while Stat() uses\n+        // fstat() on the file descriptor created during Open(), the file might\n+        // have changed ownership between the Open() and the Stat(). That's ok\n+        // because we aren't trying to block access that the user already has:\n+        // if the user has the privileges to chown another user's Xauthority\n+        // file, we won't block that since the user can just steal it without\n+        // having to use snap run. This code is just to ensure that a user who\n+        // doesn't have those privileges can't steal the file via snap run\n+        // (also note that the (potentially untrusted) snap isn't running yet).\n+        fi, err := fin.Stat()\n+        if err != nil {\n+                return \"\", err\n+        }\n+        sys := fi.Sys()\n+        if sys == nil {\n+                return \"\", fmt.Errorf(i18n.G(\"cannot validate owner of file %s\"), fin.Name())\n+        }\n+        // cheap comparison as the current uid is only available as a string\n+        // but it is better to convert the uid from the stat result to a\n+        // string than a string into a number.\n+        if fmt.Sprintf(\"%d\", sys.(*syscall.Stat_t).Uid) != u.Uid {\n+                return \"\", fmt.Errorf(i18n.G(\"Xauthority file isn't owned by the current user %s\"), u.Uid)\n+        }\n+\n+        targetPath := filepath.Join(baseTargetDir, \".Xauthority\")\n+\n+        // Only validate Xauthority file again when both files don't match\n+        // otherwise we can continue using the existing Xauthority file.\n+        // This is ok to do here because we aren't trying to protect against\n+        // the user changing the Xauthority file in XDG_RUNTIME_DIR outside\n+        // of snapd.\n+        if osutil.FileExists(targetPath) {\n+                var fout *os.File\n+                if fout, err = os.Open(targetPath); err != nil {\n+                        return \"\", err\n+                }\n+                if osutil.StreamsEqual(fin, fout) {\n+                        fout.Close()\n+                        return targetPath, nil\n+                }\n+\n+                fout.Close()\n+                if err := os.Remove(targetPath); err != nil {\n+                        return \"\", err\n+                }\n+\n+                // Ensure we're validating the Xauthority file from the beginning\n+                if _, err := fin.Seek(int64(os.SEEK_SET), 0); err != nil {\n+                        return \"\", err\n+                }\n+        }\n+\n+        // To guard against setting XAUTHORITY to non-xauth files, check\n+        // that we have a valid Xauthority. Specifically, the file must be\n+        // parseable as an Xauthority file and not be empty.\n+        if err := x11.ValidateXauthority(fin); err != nil {\n+                return \"\", err\n+        }\n+\n+        // Read data from the beginning of the file\n+        if _, err = fin.Seek(int64(os.SEEK_SET), 0); err != nil {\n+                return \"\", err\n+        }\n+\n+        fout, err := os.OpenFile(targetPath, os.O_WRONLY|os.O_CREATE|os.O_EXCL, 0600)\n+        if err != nil {\n+                return \"\", err\n+        }\n+        defer fout.Close()\n+\n+        // Read and write validated Xauthority file to its right location\n+        if _, err = io.Copy(fout, fin); err != nil {\n+                if err := os.Remove(targetPath); err != nil {\n+                        logger.Noticef(\"WARNING: cannot remove file at %s: %s\", targetPath, err)\n+                }\n+                return \"\", fmt.Errorf(i18n.G(\"cannot write new Xauthority file at %s: %s\"), targetPath, err)\n+        }\n+\n+        return targetPath, nil\n }\n \n func activateXdgDocumentPortal(info *snap.Info, snapApp, hook string) error {\n-\t// Don't do anything for apps or hooks that don't plug the\n-\t// desktop interface\n-\t//\n-\t// NOTE: This check is imperfect because we don't really know\n-\t// if the interface is connected or not but this is an\n-\t// acceptable compromise for not having to communicate with\n-\t// snapd in snap run. In a typical desktop session the\n-\t// document portal can be in use by many applications, not\n-\t// just by snaps, so this is at most, pre-emptively using some\n-\t// extra memory.\n-\tvar plugs map[string]*snap.PlugInfo\n-\tif hook != \"\" {\n-\t\tplugs = info.Hooks[hook].Plugs\n-\t} else {\n-\t\t_, appName := snap.SplitSnapApp(snapApp)\n-\t\tplugs = info.Apps[appName].Plugs\n-\t}\n-\tplugsDesktop := false\n-\tfor _, plug := range plugs {\n-\t\tif plug.Interface == \"desktop\" {\n-\t\t\tplugsDesktop = true\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\tif !plugsDesktop {\n-\t\treturn nil\n-\t}\n-\n-\tdocumentPortal := &portal.Document{}\n-\texpectedMountPoint, err := documentPortal.GetDefaultMountPoint()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// If $XDG_RUNTIME_DIR/doc appears to be a mount point, assume\n-\t// that the document portal is up and running.\n-\tif mounted, err := osutil.IsMounted(expectedMountPoint); err != nil {\n-\t\tlogger.Noticef(\"Could not check document portal mount state: %s\", err)\n-\t} else if mounted {\n-\t\treturn nil\n-\t}\n-\n-\t// If there is no session bus, our job is done.  We check this\n-\t// manually to avoid dbus.SessionBus() auto-launching a new\n-\t// bus.\n-\tbusAddress := osGetenv(\"DBUS_SESSION_BUS_ADDRESS\")\n-\tif len(busAddress) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\t// We've previously tried to start the document portal and\n-\t// were told the service is unknown: don't bother connecting\n-\t// to the session bus again.\n-\t//\n-\t// As the file is in $XDG_RUNTIME_DIR, it will be cleared over\n-\t// full logout/login or reboot cycles.\n-\txdgRuntimeDir, err := documentPortal.GetUserXdgRuntimeDir()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tportalsUnavailableFile := filepath.Join(xdgRuntimeDir, \".portals-unavailable\")\n-\tif osutil.FileExists(portalsUnavailableFile) {\n-\t\treturn nil\n-\t}\n-\n-\tactualMountPoint, err := documentPortal.GetMountPoint()\n-\tif err != nil {\n-\t\t// It is not considered an error if\n-\t\t// xdg-document-portal is not available on the system.\n-\t\tif dbusErr, ok := err.(dbus.Error); ok && dbusErr.Name == \"org.freedesktop.DBus.Error.ServiceUnknown\" {\n-\t\t\t// We ignore errors here: if writing the file\n-\t\t\t// fails, we'll just try connecting to D-Bus\n-\t\t\t// again next time.\n-\t\t\tif err = ioutil.WriteFile(portalsUnavailableFile, []byte(\"\"), 0644); err != nil {\n-\t\t\t\tlogger.Noticef(\"WARNING: cannot write file at %s: %s\", portalsUnavailableFile, err)\n-\t\t\t}\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn err\n-\t}\n-\n-\t// Sanity check to make sure the document portal is exposed\n-\t// where we think it is.\n-\tif actualMountPoint != expectedMountPoint {\n-\t\treturn fmt.Errorf(i18n.G(\"Expected portal at %#v, got %#v\"), expectedMountPoint, actualMountPoint)\n-\t}\n-\treturn nil\n+        // Don't do anything for apps or hooks that don't plug the\n+        // desktop interface\n+        //\n+        // NOTE: This check is imperfect because we don't really know\n+        // if the interface is connected or not but this is an\n+        // acceptable compromise for not having to communicate with\n+        // snapd in snap run. In a typical desktop session the\n+        // document portal can be in use by many applications, not\n+        // just by snaps, so this is at most, pre-emptively using some\n+        // extra memory.\n+        var plugs map[string]*snap.PlugInfo\n+        if hook != \"\" {\n+                plugs = info.Hooks[hook].Plugs\n+        } else {\n+                _, appName := snap.SplitSnapApp(snapApp)\n+                plugs = info.Apps[appName].Plugs\n+        }\n+        plugsDesktop := false\n+        for _, plug := range plugs {\n+                if plug.Interface == \"desktop\" {\n+                        plugsDesktop = true\n+                        break\n+                }\n+        }\n+        if !plugsDesktop {\n+                return nil\n+        }\n+\n+        documentPortal := &portal.Document{}\n+        expectedMountPoint, err := documentPortal.GetDefaultMountPoint()\n+        if err != nil {\n+                return err\n+        }\n+\n+        // If $XDG_RUNTIME_DIR/doc appears to be a mount point, assume\n+        // that the document portal is up and running.\n+        if mounted, err := osutil.IsMounted(expectedMountPoint); err != nil {\n+                logger.Noticef(\"Could not check document portal mount state: %s\", err)\n+        } else if mounted {\n+                return nil\n+        }\n+\n+        // If there is no session bus, our job is done.  We check this\n+        // manually to avoid dbus.SessionBus() auto-launching a new\n+        // bus.\n+        busAddress := osGetenv(\"DBUS_SESSION_BUS_ADDRESS\")\n+        if len(busAddress) == 0 {\n+                return nil\n+        }\n+\n+        // We've previously tried to start the document portal and\n+        // were told the service is unknown: don't bother connecting\n+        // to the session bus again.\n+        //\n+        // As the file is in $XDG_RUNTIME_DIR, it will be cleared over\n+        // full logout/login or reboot cycles.\n+        xdgRuntimeDir, err := documentPortal.GetUserXdgRuntimeDir()\n+        if err != nil {\n+                return err\n+        }\n+\n+        portalsUnavailableFile := filepath.Join(xdgRuntimeDir, \".portals-unavailable\")\n+        if osutil.FileExists(portalsUnavailableFile) {\n+                return nil\n+        }\n+\n+        actualMountPoint, err := documentPortal.GetMountPoint()\n+        if err != nil {\n+                // It is not considered an error if\n+                // xdg-document-portal is not available on the system.\n+                if dbusErr, ok := err.(dbus.Error); ok && dbusErr.Name == \"org.freedesktop.DBus.Error.ServiceUnknown\" {\n+                        // We ignore errors here: if writing the file\n+                        // fails, we'll just try connecting to D-Bus\n+                        // again next time.\n+                        if err = ioutil.WriteFile(portalsUnavailableFile, []byte(\"\"), 0644); err != nil {\n+                                logger.Noticef(\"WARNING: cannot write file at %s: %s\", portalsUnavailableFile, err)\n+                        }\n+                        return nil\n+                }\n+                return err\n+        }\n+\n+        // Sanity check to make sure the document portal is exposed\n+        // where we think it is.\n+        if actualMountPoint != expectedMountPoint {\n+                return fmt.Errorf(i18n.G(\"Expected portal at %#v, got %#v\"), expectedMountPoint, actualMountPoint)\n+        }\n+        return nil\n }\n \n type envForExecFunc func(extra map[string]string) []string\n@@ -790,424 +790,424 @@ or use your favorite gdb frontend and connect to %[1]s\n `\n \n func racyFindFreePort() (int, error) {\n-\tl, err := net.Listen(\"tcp\", \":0\")\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tdefer l.Close()\n-\treturn l.Addr().(*net.TCPAddr).Port, nil\n+        l, err := net.Listen(\"tcp\", \":0\")\n+        if err != nil {\n+                return 0, err\n+        }\n+        defer l.Close()\n+        return l.Addr().(*net.TCPAddr).Port, nil\n }\n \n func (x *cmdRun) useGdbserver() bool {\n-\t// compatibility, can be removed after 2021\n-\tif x.ExperimentalGdbserver != \"no-gdbserver\" {\n-\t\tx.Gdbserver = x.ExperimentalGdbserver\n-\t}\n+        // compatibility, can be removed after 2021\n+        if x.ExperimentalGdbserver != \"no-gdbserver\" {\n+                x.Gdbserver = x.ExperimentalGdbserver\n+        }\n \n-\t// make sure the go-flag parser ran and assigned default values\n-\treturn x.ParserRan == 1 && x.Gdbserver != \"no-gdbserver\"\n+        // make sure the go-flag parser ran and assigned default values\n+        return x.ParserRan == 1 && x.Gdbserver != \"no-gdbserver\"\n }\n \n func (x *cmdRun) runCmdUnderGdbserver(origCmd []string, envForExec envForExecFunc) error {\n-\tgcmd := exec.Command(origCmd[0], origCmd[1:]...)\n-\tgcmd.Stdin = os.Stdin\n-\tgcmd.Stdout = os.Stdout\n-\tgcmd.Stderr = os.Stderr\n-\tgcmd.Env = envForExec(map[string]string{\"SNAP_CONFINE_RUN_UNDER_GDBSERVER\": \"1\"})\n-\tif err := gcmd.Start(); err != nil {\n-\t\treturn err\n-\t}\n-\t// wait for the child process executing gdb helper to raise SIGSTOP\n-\t// signalling readiness to attach a gdbserver process\n-\tvar status syscall.WaitStatus\n-\t_, err := syscall.Wait4(gcmd.Process.Pid, &status, syscall.WSTOPPED, nil)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\taddr := x.Gdbserver\n-\tif addr == \":0\" {\n-\t\t// XXX: run \"gdbserver :0\" instead and parse \"Listening on port 45971\"\n-\t\t//      on stderr instead?\n-\t\tport, err := racyFindFreePort()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"cannot find free port: %v\", err)\n-\t\t}\n-\t\taddr = fmt.Sprintf(\":%v\", port)\n-\t}\n-\t// XXX: should we provide a helper here instead? something like\n-\t//      `snap run --attach-debugger` or similar? The downside\n-\t//      is that attaching a gdb frontend is harder?\n-\tfmt.Fprintf(Stdout, fmt.Sprintf(gdbServerWelcomeFmt, addr))\n-\t// note that only gdbserver needs to run as root, the application\n-\t// keeps running as the user\n-\tgdbSrvCmd := exec.Command(\"sudo\", \"-E\", \"gdbserver\", \"--attach\", addr, strconv.Itoa(gcmd.Process.Pid))\n-\tif output, err := gdbSrvCmd.CombinedOutput(); err != nil {\n-\t\treturn osutil.OutputErr(output, err)\n-\t}\n-\treturn nil\n+        gcmd := exec.Command(origCmd[0], origCmd[1:]...)\n+        gcmd.Stdin = os.Stdin\n+        gcmd.Stdout = os.Stdout\n+        gcmd.Stderr = os.Stderr\n+        gcmd.Env = envForExec(map[string]string{\"SNAP_CONFINE_RUN_UNDER_GDBSERVER\": \"1\"})\n+        if err := gcmd.Start(); err != nil {\n+                return err\n+        }\n+        // wait for the child process executing gdb helper to raise SIGSTOP\n+        // signalling readiness to attach a gdbserver process\n+        var status syscall.WaitStatus\n+        _, err := syscall.Wait4(gcmd.Process.Pid, &status, syscall.WSTOPPED, nil)\n+        if err != nil {\n+                return err\n+        }\n+\n+        addr := x.Gdbserver\n+        if addr == \":0\" {\n+                // XXX: run \"gdbserver :0\" instead and parse \"Listening on port 45971\"\n+                //      on stderr instead?\n+                port, err := racyFindFreePort()\n+                if err != nil {\n+                        return fmt.Errorf(\"cannot find free port: %v\", err)\n+                }\n+                addr = fmt.Sprintf(\":%v\", port)\n+        }\n+        // XXX: should we provide a helper here instead? something like\n+        //      `snap run --attach-debugger` or similar? The downside\n+        //      is that attaching a gdb frontend is harder?\n+        fmt.Fprintf(Stdout, fmt.Sprintf(gdbServerWelcomeFmt, addr))\n+        // note that only gdbserver needs to run as root, the application\n+        // keeps running as the user\n+        gdbSrvCmd := exec.Command(\"sudo\", \"-E\", \"gdbserver\", \"--attach\", addr, strconv.Itoa(gcmd.Process.Pid))\n+        if output, err := gdbSrvCmd.CombinedOutput(); err != nil {\n+                return osutil.OutputErr(output, err)\n+        }\n+        return nil\n }\n \n func (x *cmdRun) runCmdUnderGdb(origCmd []string, envForExec envForExecFunc) error {\n-\t// the resulting application process runs as root\n-\tcmd := []string{\"sudo\", \"-E\", \"gdb\", \"-ex=run\", \"-ex=catch exec\", \"-ex=continue\", \"--args\"}\n-\tcmd = append(cmd, origCmd...)\n-\n-\tgcmd := exec.Command(cmd[0], cmd[1:]...)\n-\tgcmd.Stdin = os.Stdin\n-\tgcmd.Stdout = os.Stdout\n-\tgcmd.Stderr = os.Stderr\n-\tgcmd.Env = envForExec(map[string]string{\"SNAP_CONFINE_RUN_UNDER_GDB\": \"1\"})\n-\treturn gcmd.Run()\n+        // the resulting application process runs as root\n+        cmd := []string{\"sudo\", \"-E\", \"gdb\", \"-ex=run\", \"-ex=catch exec\", \"-ex=continue\", \"--args\"}\n+        cmd = append(cmd, origCmd...)\n+\n+        gcmd := exec.Command(cmd[0], cmd[1:]...)\n+        gcmd.Stdin = os.Stdin\n+        gcmd.Stdout = os.Stdout\n+        gcmd.Stderr = os.Stderr\n+        gcmd.Env = envForExec(map[string]string{\"SNAP_CONFINE_RUN_UNDER_GDB\": \"1\"})\n+        return gcmd.Run()\n }\n \n func (x *cmdRun) runCmdWithTraceExec(origCmd []string, envForExec envForExecFunc) error {\n-\t// setup private tmp dir with strace fifo\n-\tstraceTmp, err := ioutil.TempDir(\"\", \"exec-trace\")\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer os.RemoveAll(straceTmp)\n-\tstraceLog := filepath.Join(straceTmp, \"strace.fifo\")\n-\tif err := syscall.Mkfifo(straceLog, 0640); err != nil {\n-\t\treturn err\n-\t}\n-\t// ensure we have one writer on the fifo so that if strace fails\n-\t// nothing blocks\n-\tfw, err := os.OpenFile(straceLog, os.O_RDWR, 0640)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer fw.Close()\n-\n-\t// read strace data from fifo async\n-\tvar slg *strace.ExecveTiming\n-\tvar straceErr error\n-\tdoneCh := make(chan bool, 1)\n-\tgo func() {\n-\t\t// FIXME: make this configurable?\n-\t\tnSlowest := 10\n-\t\tslg, straceErr = strace.TraceExecveTimings(straceLog, nSlowest)\n-\t\tclose(doneCh)\n-\t}()\n-\n-\tcmd, err := strace.TraceExecCommand(straceLog, origCmd...)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\t// run\n-\tcmd.Env = envForExec(nil)\n-\tcmd.Stdin = Stdin\n-\tcmd.Stdout = Stdout\n-\tcmd.Stderr = Stderr\n-\terr = cmd.Run()\n-\t// ensure we close the fifo here so that the strace.TraceExecCommand()\n-\t// helper gets a EOF from the fifo (i.e. all writers must be closed\n-\t// for this)\n-\tfw.Close()\n-\n-\t// wait for strace reader\n-\t<-doneCh\n-\tif straceErr == nil {\n-\t\tslg.Display(Stderr)\n-\t} else {\n-\t\tlogger.Noticef(\"cannot extract runtime data: %v\", straceErr)\n-\t}\n-\treturn err\n+        // setup private tmp dir with strace fifo\n+        straceTmp, err := ioutil.TempDir(\"\", \"exec-trace\")\n+        if err != nil {\n+                return err\n+        }\n+        defer os.RemoveAll(straceTmp)\n+        straceLog := filepath.Join(straceTmp, \"strace.fifo\")\n+        if err := syscall.Mkfifo(straceLog, 0640); err != nil {\n+                return err\n+        }\n+        // ensure we have one writer on the fifo so that if strace fails\n+        // nothing blocks\n+        fw, err := os.OpenFile(straceLog, os.O_RDWR, 0640)\n+        if err != nil {\n+                return err\n+        }\n+        defer fw.Close()\n+\n+        // read strace data from fifo async\n+        var slg *strace.ExecveTiming\n+        var straceErr error\n+        doneCh := make(chan bool, 1)\n+        go func() {\n+                // FIXME: make this configurable?\n+                nSlowest := 10\n+                slg, straceErr = strace.TraceExecveTimings(straceLog, nSlowest)\n+                close(doneCh)\n+        }()\n+\n+        cmd, err := strace.TraceExecCommand(straceLog, origCmd...)\n+        if err != nil {\n+                return err\n+        }\n+        // run\n+        cmd.Env = envForExec(nil)\n+        cmd.Stdin = Stdin\n+        cmd.Stdout = Stdout\n+        cmd.Stderr = Stderr\n+        err = cmd.Run()\n+        // ensure we close the fifo here so that the strace.TraceExecCommand()\n+        // helper gets a EOF from the fifo (i.e. all writers must be closed\n+        // for this)\n+        fw.Close()\n+\n+        // wait for strace reader\n+        <-doneCh\n+        if straceErr == nil {\n+                slg.Display(Stderr)\n+        } else {\n+                logger.Noticef(\"cannot extract runtime data: %v\", straceErr)\n+        }\n+        return err\n }\n \n func (x *cmdRun) runCmdUnderStrace(origCmd []string, envForExec envForExecFunc) error {\n-\textraStraceOpts, raw, err := x.straceOpts()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tcmd, err := strace.Command(extraStraceOpts, origCmd...)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// run with filter\n-\tcmd.Env = envForExec(nil)\n-\tcmd.Stdin = Stdin\n-\tcmd.Stdout = Stdout\n-\tstderr, err := cmd.StderrPipe()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tfilterDone := make(chan bool, 1)\n-\tgo func() {\n-\t\tdefer func() { filterDone <- true }()\n-\n-\t\tif raw {\n-\t\t\t// Passing --strace='--raw' disables the filtering of\n-\t\t\t// early strace output. This is useful when tracking\n-\t\t\t// down issues with snap helpers such as snap-confine,\n-\t\t\t// snap-exec ...\n-\t\t\tio.Copy(Stderr, stderr)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tr := bufio.NewReader(stderr)\n-\n-\t\t// The first thing from strace if things work is\n-\t\t// \"exeve(\" - show everything until we see this to\n-\t\t// not swallow real strace errors.\n-\t\tfor {\n-\t\t\ts, err := r.ReadString('\\n')\n-\t\t\tif err != nil {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\tif strings.Contains(s, \"execve(\") {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\tfmt.Fprint(Stderr, s)\n-\t\t}\n-\n-\t\t// The last thing that snap-exec does is to\n-\t\t// execve() something inside the snap dir so\n-\t\t// we know that from that point on the output\n-\t\t// will be interessting to the user.\n-\t\t//\n-\t\t// We need check both /snap (which is where snaps\n-\t\t// are located inside the mount namespace) and the\n-\t\t// distro snap mount dir (which is different on e.g.\n-\t\t// fedora/arch) to fully work with classic snaps.\n-\t\tneedle1 := fmt.Sprintf(`execve(\"%s`, dirs.SnapMountDir)\n-\t\tneedle2 := `execve(\"/snap`\n-\t\tfor {\n-\t\t\ts, err := r.ReadString('\\n')\n-\t\t\tif err != nil {\n-\t\t\t\tif err != io.EOF {\n-\t\t\t\t\tfmt.Fprintf(Stderr, \"cannot read strace output: %s\\n\", err)\n-\t\t\t\t}\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\t// Ensure we catch the execve but *not* the\n-\t\t\t// exec into\n-\t\t\t// /snap/core/current/usr/lib/snapd/snap-confine\n-\t\t\t// which is just `snap run` using the core version\n-\t\t\t// snap-confine.\n-\t\t\tif (strings.Contains(s, needle1) || strings.Contains(s, needle2)) && !strings.Contains(s, \"usr/lib/snapd/snap-confine\") {\n-\t\t\t\tfmt.Fprint(Stderr, s)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tio.Copy(Stderr, r)\n-\t}()\n-\tif err := cmd.Start(); err != nil {\n-\t\treturn err\n-\t}\n-\t<-filterDone\n-\terr = cmd.Wait()\n-\treturn err\n+        extraStraceOpts, raw, err := x.straceOpts()\n+        if err != nil {\n+                return err\n+        }\n+        cmd, err := strace.Command(extraStraceOpts, origCmd...)\n+        if err != nil {\n+                return err\n+        }\n+\n+        // run with filter\n+        cmd.Env = envForExec(nil)\n+        cmd.Stdin = Stdin\n+        cmd.Stdout = Stdout\n+        stderr, err := cmd.StderrPipe()\n+        if err != nil {\n+                return err\n+        }\n+        filterDone := make(chan bool, 1)\n+        go func() {\n+                defer func() { filterDone <- true }()\n+\n+                if raw {\n+                        // Passing --strace='--raw' disables the filtering of\n+                        // early strace output. This is useful when tracking\n+                        // down issues with snap helpers such as snap-confine,\n+                        // snap-exec ...\n+                        io.Copy(Stderr, stderr)\n+                        return\n+                }\n+\n+                r := bufio.NewReader(stderr)\n+\n+                // The first thing from strace if things work is\n+                // \"exeve(\" - show everything until we see this to\n+                // not swallow real strace errors.\n+                for {\n+                        s, err := r.ReadString('\\n')\n+                        if err != nil {\n+                                break\n+                        }\n+                        if strings.Contains(s, \"execve(\") {\n+                                break\n+                        }\n+                        fmt.Fprint(Stderr, s)\n+                }\n+\n+                // The last thing that snap-exec does is to\n+                // execve() something inside the snap dir so\n+                // we know that from that point on the output\n+                // will be interessting to the user.\n+                //\n+                // We need check both /snap (which is where snaps\n+                // are located inside the mount namespace) and the\n+                // distro snap mount dir (which is different on e.g.\n+                // fedora/arch) to fully work with classic snaps.\n+                needle1 := fmt.Sprintf(`execve(\"%s`, dirs.SnapMountDir)\n+                needle2 := `execve(\"/snap`\n+                for {\n+                        s, err := r.ReadString('\\n')\n+                        if err != nil {\n+                                if err != io.EOF {\n+                                        fmt.Fprintf(Stderr, \"cannot read strace output: %s\\n\", err)\n+                                }\n+                                break\n+                        }\n+                        // Ensure we catch the execve but *not* the\n+                        // exec into\n+                        // /snap/core/current/usr/lib/snapd/snap-confine\n+                        // which is just `snap run` using the core version\n+                        // snap-confine.\n+                        if (strings.Contains(s, needle1) || strings.Contains(s, needle2)) && !strings.Contains(s, \"usr/lib/snapd/snap-confine\") {\n+                                fmt.Fprint(Stderr, s)\n+                                break\n+                        }\n+                }\n+                io.Copy(Stderr, r)\n+        }()\n+        if err := cmd.Start(); err != nil {\n+                return err\n+        }\n+        <-filterDone\n+        err = cmd.Wait()\n+        return err\n }\n \n func (x *cmdRun) runSnapConfine(info *snap.Info, securityTag, snapApp, hook string, args []string) error {\n-\tsnapConfine, err := snapdHelperPath(\"snap-confine\")\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tif !osutil.FileExists(snapConfine) {\n-\t\tif hook != \"\" {\n-\t\t\tlogger.Noticef(\"WARNING: skipping running hook %q of snap %q: missing snap-confine\", hook, info.InstanceName())\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn fmt.Errorf(i18n.G(\"missing snap-confine: try updating your core/snapd package\"))\n-\t}\n-\n-\tif err := createUserDataDirs(info); err != nil {\n-\t\tlogger.Noticef(\"WARNING: cannot create user data directory: %s\", err)\n-\t}\n-\n-\txauthPath, err := migrateXauthority(info)\n-\tif err != nil {\n-\t\tlogger.Noticef(\"WARNING: cannot copy user Xauthority file: %s\", err)\n-\t}\n-\n-\tif err := activateXdgDocumentPortal(info, snapApp, hook); err != nil {\n-\t\tlogger.Noticef(\"WARNING: cannot start document portal: %s\", err)\n-\t}\n-\n-\tcmd := []string{snapConfine}\n-\tif info.NeedsClassic() {\n-\t\tcmd = append(cmd, \"--classic\")\n-\t}\n-\n-\t// this should never happen since we validate snaps with \"base: none\" and do not allow hooks/apps\n-\tif info.Base == \"none\" {\n-\t\treturn fmt.Errorf(`cannot run hooks / applications with base \"none\"`)\n-\t}\n-\tif info.Base != \"\" {\n-\t\tcmd = append(cmd, \"--base\", info.Base)\n-\t} else {\n-\t\tif info.Type() == snap.TypeKernel {\n-\t\t\t// kernels have no explicit base, we use the boot base\n-\t\t\tmodelAssertion, err := x.client.CurrentModelAssertion()\n-\t\t\tif err != nil {\n-\t\t\t\tif hook != \"\" {\n-\t\t\t\t\treturn fmt.Errorf(\"cannot get model assertion to setup kernel hook run: %v\", err)\n-\t\t\t\t} else {\n-\t\t\t\t\treturn fmt.Errorf(\"cannot get model assertion to setup kernel app run: %v\", err)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tmodelBase := modelAssertion.Base()\n-\t\t\tif modelBase != \"\" {\n-\t\t\t\tcmd = append(cmd, \"--base\", modelBase)\n-\t\t\t}\n-\t\t}\n-\t}\n-\tcmd = append(cmd, securityTag)\n-\n-\t// when under confinement, snap-exec is run from 'core' snap rootfs\n-\tsnapExecPath := filepath.Join(dirs.CoreLibExecDir, \"snap-exec\")\n-\n-\tif info.NeedsClassic() {\n-\t\t// running with classic confinement, carefully pick snap-exec we\n-\t\t// are going to use\n-\t\tsnapExecPath, err = snapdHelperPath(\"snap-exec\")\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\tcmd = append(cmd, snapExecPath)\n-\n-\tif x.Shell {\n-\t\tcmd = append(cmd, \"--command=shell\")\n-\t}\n-\tif x.Gdb {\n-\t\tcmd = append(cmd, \"--command=gdb\")\n-\t}\n-\tif x.useGdbserver() {\n-\t\tcmd = append(cmd, \"--command=gdbserver\")\n-\t}\n-\tif x.Command != \"\" {\n-\t\tcmd = append(cmd, \"--command=\"+x.Command)\n-\t}\n-\n-\tif hook != \"\" {\n-\t\tcmd = append(cmd, \"--hook=\"+hook)\n-\t}\n-\n-\t// snap-exec is POSIXly-- options must come before positionals.\n-\tcmd = append(cmd, snapApp)\n-\tcmd = append(cmd, args...)\n-\n-\tenv, err := osutil.OSEnvironment()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tsnapenv.ExtendEnvForRun(env, info)\n-\n-\tif len(xauthPath) > 0 {\n-\t\t// Environment is not nil here because it comes from\n-\t\t// osutil.OSEnvironment and that guarantees this\n-\t\t// property.\n-\t\tenv[\"XAUTHORITY\"] = xauthPath\n-\t}\n-\n-\t// on each run variant path this will be used once to get\n-\t// the environment plus additions in the right form\n-\tenvForExec := func(extra map[string]string) []string {\n-\t\tfor varName, value := range extra {\n-\t\t\tenv[varName] = value\n-\t\t}\n-\t\tif !info.NeedsClassic() {\n-\t\t\treturn env.ForExec()\n-\t\t}\n-\t\t// For a classic snap, environment variables that are\n-\t\t// usually stripped out by ld.so when starting a\n-\t\t// setuid process are presevered by being renamed by\n-\t\t// prepending PreservedUnsafePrefix -- which snap-exec\n-\t\t// will remove, restoring the variables to their\n-\t\t// original names.\n-\t\treturn env.ForExecEscapeUnsafe(snapenv.PreservedUnsafePrefix)\n-\t}\n-\n-\t// Systemd automatically places services under a unique cgroup encoding the\n-\t// security tag, but for apps and hooks we need to create a transient scope\n-\t// with similar purpose ourselves.\n-\t//\n-\t// The way this happens is as follows:\n-\t//\n-\t// 1) Services are implemented using systemd service units. Starting a\n-\t// unit automatically places it in a cgroup named after the service unit\n-\t// name. Snapd controls the name of the service units thus indirectly\n-\t// controls the cgroup name.\n-\t//\n-\t// 2) Non-services, including hooks, are started inside systemd\n-\t// transient scopes. Scopes are a systemd unit type that are defined\n-\t// programmatically and are meant for groups of processes started and\n-\t// stopped by an _arbitrary process_ (ie, not systemd). Systemd\n-\t// requires that each scope is given a unique name. We employ a scheme\n-\t// where random UUID is combined with the name of the security tag\n-\t// derived from snap application or hook name. Multiple concurrent\n-\t// invocations of \"snap run\" will use distinct UUIDs.\n-\t//\n-\t// Transient scopes allow launched snaps to integrate into\n-\t// the systemd design. See:\n-\t// https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/\n-\t//\n-\t// Programs running as root, like system-wide services and programs invoked\n-\t// using tools like sudo are placed under system.slice. Programs running as\n-\t// a non-root user are placed under user.slice, specifically in a scope\n-\t// specific to a logind session.\n-\t//\n-\t// This arrangement allows for proper accounting and control of resources\n-\t// used by snap application processes of each type.\n-\t//\n-\t// For more information about systemd cgroups, including unit types, see:\n-\t// https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/\n-\t_, appName := snap.SplitSnapApp(snapApp)\n-\tneedsTracking := true\n-\tif app := info.Apps[appName]; hook == \"\" && app != nil && app.IsService() {\n-\t\t// If we are running a service app then we do not need to use\n-\t\t// application tracking. Services, both in the system and user scope,\n-\t\t// do not need tracking because systemd already places them in a\n-\t\t// tracking cgroup, named after the systemd unit name, and those are\n-\t\t// sufficient to identify both the snap name and the app name.\n-\t\tneedsTracking = false\n-\t}\n-\t// Allow using the session bus for all apps but not for hooks.\n-\tallowSessionBus := hook == \"\"\n-\t// Track, or confirm existing tracking from systemd.\n-\tvar trackingErr error\n-\tif needsTracking {\n-\t\topts := &cgroup.TrackingOptions{AllowSessionBus: allowSessionBus}\n-\t\ttrackingErr = cgroupCreateTransientScopeForTracking(securityTag, opts)\n-\t} else {\n-\t\ttrackingErr = cgroupConfirmSystemdServiceTracking(securityTag)\n-\t}\n-\tif trackingErr != nil {\n-\t\tif trackingErr != cgroup.ErrCannotTrackProcess {\n-\t\t\treturn trackingErr\n-\t\t}\n-\t\t// If we cannot track the process then log a debug message.\n-\t\t// TODO: if we could, create a warning. Currently this is not possible\n-\t\t// because only snapd can create warnings, internally.\n-\t\tlogger.Debugf(\"snapd cannot track the started application\")\n-\t\tlogger.Debugf(\"snap refreshes will not be postponed by this process\")\n-\t}\n-\tif x.TraceExec {\n-\t\treturn x.runCmdWithTraceExec(cmd, envForExec)\n-\t} else if x.Gdb {\n-\t\treturn x.runCmdUnderGdb(cmd, envForExec)\n-\t} else if x.useGdbserver() {\n-\t\tif _, err := exec.LookPath(\"gdbserver\"); err != nil {\n-\t\t\t// TODO: use xerrors.Is(err, exec.ErrNotFound) once\n-\t\t\t// we moved off from go-1.9\n-\t\t\tif execErr, ok := err.(*exec.Error); ok {\n-\t\t\t\tif execErr.Err == exec.ErrNotFound {\n-\t\t\t\t\treturn fmt.Errorf(\"please install gdbserver on your system\")\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\treturn err\n-\t\t}\n-\t\treturn x.runCmdUnderGdbserver(cmd, envForExec)\n-\t} else if x.useStrace() {\n-\t\treturn x.runCmdUnderStrace(cmd, envForExec)\n-\t} else {\n-\t\treturn syscallExec(cmd[0], cmd, envForExec(nil))\n-\t}\n+        snapConfine, err := snapdHelperPath(\"snap-confine\")\n+        if err != nil {\n+                return err\n+        }\n+        if !osutil.FileExists(snapConfine) {\n+                if hook != \"\" {\n+                        logger.Noticef(\"WARNING: skipping running hook %q of snap %q: missing snap-confine\", hook, info.InstanceName())\n+                        return nil\n+                }\n+                return fmt.Errorf(i18n.G(\"missing snap-confine: try updating your core/snapd package\"))\n+        }\n+\n+        if err := createUserDataDirs(info); err != nil {\n+                logger.Noticef(\"WARNING: cannot create user data directory: %s\", err)\n+        }\n+\n+        xauthPath, err := migrateXauthority(info)\n+        if err != nil {\n+                logger.Noticef(\"WARNING: cannot copy user Xauthority file: %s\", err)\n+        }\n+\n+        if err := activateXdgDocumentPortal(info, snapApp, hook); err != nil {\n+                logger.Noticef(\"WARNING: cannot start document portal: %s\", err)\n+        }\n+\n+        cmd := []string{snapConfine}\n+        if info.NeedsClassic() {\n+                cmd = append(cmd, \"--classic\")\n+        }\n+\n+        // this should never happen since we validate snaps with \"base: none\" and do not allow hooks/apps\n+        if info.Base == \"none\" {\n+                return fmt.Errorf(`cannot run hooks / applications with base \"none\"`)\n+        }\n+        if info.Base != \"\" {\n+                cmd = append(cmd, \"--base\", info.Base)\n+        } else {\n+                if info.Type() == snap.TypeKernel {\n+                        // kernels have no explicit base, we use the boot base\n+                        modelAssertion, err := x.client.CurrentModelAssertion()\n+                        if err != nil {\n+                                if hook != \"\" {\n+                                        return fmt.Errorf(\"cannot get model assertion to setup kernel hook run: %v\", err)\n+                                } else {\n+                                        return fmt.Errorf(\"cannot get model assertion to setup kernel app run: %v\", err)\n+                                }\n+                        }\n+                        modelBase := modelAssertion.Base()\n+                        if modelBase != \"\" {\n+                                cmd = append(cmd, \"--base\", modelBase)\n+                        }\n+                }\n+        }\n+        cmd = append(cmd, securityTag)\n+\n+        // when under confinement, snap-exec is run from 'core' snap rootfs\n+        snapExecPath := filepath.Join(dirs.CoreLibExecDir, \"snap-exec\")\n+\n+        if info.NeedsClassic() {\n+                // running with classic confinement, carefully pick snap-exec we\n+                // are going to use\n+                snapExecPath, err = snapdHelperPath(\"snap-exec\")\n+                if err != nil {\n+                        return err\n+                }\n+        }\n+        cmd = append(cmd, snapExecPath)\n+\n+        if x.Shell {\n+                cmd = append(cmd, \"--command=shell\")\n+        }\n+        if x.Gdb {\n+                cmd = append(cmd, \"--command=gdb\")\n+        }\n+        if x.useGdbserver() {\n+                cmd = append(cmd, \"--command=gdbserver\")\n+        }\n+        if x.Command != \"\" {\n+                cmd = append(cmd, \"--command=\"+x.Command)\n+        }\n+\n+        if hook != \"\" {\n+                cmd = append(cmd, \"--hook=\"+hook)\n+        }\n+\n+        // snap-exec is POSIXly-- options must come before positionals.\n+        cmd = append(cmd, snapApp)\n+        cmd = append(cmd, args...)\n+\n+        env, err := osutil.OSEnvironment()\n+        if err != nil {\n+                return err\n+        }\n+        snapenv.ExtendEnvForRun(env, info)\n+\n+        if len(xauthPath) > 0 {\n+                // Environment is not nil here because it comes from\n+                // osutil.OSEnvironment and that guarantees this\n+                // property.\n+                env[\"XAUTHORITY\"] = xauthPath\n+        }\n+\n+        // on each run variant path this will be used once to get\n+        // the environment plus additions in the right form\n+        envForExec := func(extra map[string]string) []string {\n+                for varName, value := range extra {\n+                        env[varName] = value\n+                }\n+                if !info.NeedsClassic() {\n+                        return env.ForExec()\n+                }\n+                // For a classic snap, environment variables that are\n+                // usually stripped out by ld.so when starting a\n+                // setuid process are presevered by being renamed by\n+                // prepending PreservedUnsafePrefix -- which snap-exec\n+                // will remove, restoring the variables to their\n+                // original names.\n+                return env.ForExecEscapeUnsafe(snapenv.PreservedUnsafePrefix)\n+        }\n+\n+        // Systemd automatically places services under a unique cgroup encoding the\n+        // security tag, but for apps and hooks we need to create a transient scope\n+        // with similar purpose ourselves.\n+        //\n+        // The way this happens is as follows:\n+        //\n+        // 1) Services are implemented using systemd service units. Starting a\n+        // unit automatically places it in a cgroup named after the service unit\n+        // name. Snapd controls the name of the service units thus indirectly\n+        // controls the cgroup name.\n+        //\n+        // 2) Non-services, including hooks, are started inside systemd\n+        // transient scopes. Scopes are a systemd unit type that are defined\n+        // programmatically and are meant for groups of processes started and\n+        // stopped by an _arbitrary process_ (ie, not systemd). Systemd\n+        // requires that each scope is given a unique name. We employ a scheme\n+        // where random UUID is combined with the name of the security tag\n+        // derived from snap application or hook name. Multiple concurrent\n+        // invocations of \"snap run\" will use distinct UUIDs.\n+        //\n+        // Transient scopes allow launched snaps to integrate into\n+        // the systemd design. See:\n+        // https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/\n+        //\n+        // Programs running as root, like system-wide services and programs invoked\n+        // using tools like sudo are placed under system.slice. Programs running as\n+        // a non-root user are placed under user.slice, specifically in a scope\n+        // specific to a logind session.\n+        //\n+        // This arrangement allows for proper accounting and control of resources\n+        // used by snap application processes of each type.\n+        //\n+        // For more information about systemd cgroups, including unit types, see:\n+        // https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/\n+        _, appName := snap.SplitSnapApp(snapApp)\n+        needsTracking := true\n+        if app := info.Apps[appName]; hook == \"\" && app != nil && app.IsService() {\n+                // If we are running a service app then we do not need to use\n+                // application tracking. Services, both in the system and user scope,\n+                // do not need tracking because systemd already places them in a\n+                // tracking cgroup, named after the systemd unit name, and those are\n+                // sufficient to identify both the snap name and the app name.\n+                needsTracking = false\n+        }\n+        // Allow using the session bus for all apps but not for hooks.\n+        allowSessionBus := hook == \"\"\n+        // Track, or confirm existing tracking from systemd.\n+        var trackingErr error\n+        if needsTracking {\n+                opts := &cgroup.TrackingOptions{AllowSessionBus: allowSessionBus}\n+                trackingErr = cgroupCreateTransientScopeForTracking(securityTag, opts)\n+        } else {\n+                trackingErr = cgroupConfirmSystemdServiceTracking(securityTag)\n+        }\n+        if trackingErr != nil {\n+                if trackingErr != cgroup.ErrCannotTrackProcess {\n+                        return trackingErr\n+                }\n+                // If we cannot track the process then log a debug message.\n+                // TODO: if we could, create a warning. Currently this is not possible\n+                // because only snapd can create warnings, internally.\n+                logger.Debugf(\"snapd cannot track the started application\")\n+                logger.Debugf(\"snap refreshes will not be postponed by this process\")\n+        }\n+        if x.TraceExec {\n+                return x.runCmdWithTraceExec(cmd, envForExec)\n+        } else if x.Gdb {\n+                return x.runCmdUnderGdb(cmd, envForExec)\n+        } else if x.useGdbserver() {\n+                if _, err := exec.LookPath(\"gdbserver\"); err != nil {\n+                        // TODO: use xerrors.Is(err, exec.ErrNotFound) once\n+                        // we moved off from go-1.9\n+                        if execErr, ok := err.(*exec.Error); ok {\n+                                if execErr.Err == exec.ErrNotFound {\n+                                        return fmt.Errorf(\"please install gdbserver on your system\")\n+                                }\n+                        }\n+                        return err\n+                }\n+                return x.runCmdUnderGdbserver(cmd, envForExec)\n+        } else if x.useStrace() {\n+                return x.runCmdUnderStrace(cmd, envForExec)\n+        } else {\n+                return syscallExec(cmd[0], cmd, envForExec(nil))\n+        }\n }\n \n var cgroupCreateTransientScopeForTracking = cgroup.CreateTransientScopeForTracking\n"}
{"cve":"CVE-2025-29778:0708", "fix_patch": "diff --git a/pkg/cosign/cosign.go b/pkg/cosign/cosign.go\nindex 60ca41696..86188770e 100644\n--- a/pkg/cosign/cosign.go\n+++ b/pkg/cosign/cosign.go\n@@ -1,720 +1,720 @@\n package cosign\n \n import (\n-\t\"bytes\"\n-\t\"context\"\n-\t\"crypto\"\n-\t\"crypto/x509\"\n-\t\"encoding/base64\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"regexp\"\n-\t\"strings\"\n-\n-\t\"github.com/google/go-containerregistry/pkg/name\"\n-\t\"github.com/in-toto/in-toto-golang/in_toto\"\n-\t\"github.com/kyverno/kyverno/ext/wildcard\"\n-\t\"github.com/kyverno/kyverno/pkg/images\"\n-\t\"github.com/kyverno/kyverno/pkg/tracing\"\n-\tdatautils \"github.com/kyverno/kyverno/pkg/utils/data\"\n-\t\"github.com/sigstore/cosign/v2/pkg/cosign\"\n-\t\"github.com/sigstore/cosign/v2/pkg/cosign/attestation\"\n-\t\"github.com/sigstore/cosign/v2/pkg/oci\"\n-\t\"github.com/sigstore/cosign/v2/pkg/oci/remote\"\n-\tsigs \"github.com/sigstore/cosign/v2/pkg/signature\"\n-\trekorclient \"github.com/sigstore/rekor/pkg/client\"\n-\t\"github.com/sigstore/sigstore/pkg/cryptoutils\"\n-\t\"github.com/sigstore/sigstore/pkg/fulcioroots\"\n-\t\"github.com/sigstore/sigstore/pkg/signature\"\n-\t\"github.com/sigstore/sigstore/pkg/signature/payload\"\n-\t\"github.com/sigstore/sigstore/pkg/tuf\"\n-\t\"go.opentelemetry.io/otel/trace\"\n-\t\"go.uber.org/multierr\"\n+        \"bytes\"\n+        \"context\"\n+        \"crypto\"\n+        \"crypto/x509\"\n+        \"encoding/base64\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"regexp\"\n+        \"strings\"\n+\n+        \"github.com/google/go-containerregistry/pkg/name\"\n+        \"github.com/in-toto/in-toto-golang/in_toto\"\n+        \"github.com/kyverno/kyverno/ext/wildcard\"\n+        \"github.com/kyverno/kyverno/pkg/images\"\n+        \"github.com/kyverno/kyverno/pkg/tracing\"\n+        datautils \"github.com/kyverno/kyverno/pkg/utils/data\"\n+        \"github.com/sigstore/cosign/v2/pkg/cosign\"\n+        \"github.com/sigstore/cosign/v2/pkg/cosign/attestation\"\n+        \"github.com/sigstore/cosign/v2/pkg/oci\"\n+        \"github.com/sigstore/cosign/v2/pkg/oci/remote\"\n+        sigs \"github.com/sigstore/cosign/v2/pkg/signature\"\n+        rekorclient \"github.com/sigstore/rekor/pkg/client\"\n+        \"github.com/sigstore/sigstore/pkg/cryptoutils\"\n+        \"github.com/sigstore/sigstore/pkg/fulcioroots\"\n+        \"github.com/sigstore/sigstore/pkg/signature\"\n+        \"github.com/sigstore/sigstore/pkg/signature/payload\"\n+        \"github.com/sigstore/sigstore/pkg/tuf\"\n+        \"go.opentelemetry.io/otel/trace\"\n+        \"go.uber.org/multierr\"\n )\n \n var signatureAlgorithmMap = map[string]crypto.Hash{\n-\t\"\":       crypto.SHA256,\n-\t\"sha224\": crypto.SHA224,\n-\t\"sha256\": crypto.SHA256,\n-\t\"sha384\": crypto.SHA384,\n-\t\"sha512\": crypto.SHA512,\n+        \"\":       crypto.SHA256,\n+        \"sha224\": crypto.SHA224,\n+        \"sha256\": crypto.SHA256,\n+        \"sha384\": crypto.SHA384,\n+        \"sha512\": crypto.SHA512,\n }\n \n func NewVerifier() images.ImageVerifier {\n-\treturn &cosignVerifier{}\n+        return &cosignVerifier{}\n }\n \n type cosignVerifier struct{}\n \n func (v *cosignVerifier) VerifySignature(ctx context.Context, opts images.Options) (*images.Response, error) {\n-\tif opts.SigstoreBundle {\n-\t\tresults, err := verifyBundleAndFetchAttestations(ctx, opts)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif len(results) == 0 {\n-\t\t\treturn nil, fmt.Errorf(\"sigstore bundle verification failed: no matching signatures found\")\n-\t\t}\n-\n-\t\treturn &images.Response{Digest: results[0].Desc.Digest.String()}, nil\n-\t}\n-\n-\tnameOpts := opts.Client.NameOptions()\n-\tref, err := name.ParseReference(opts.ImageRef, nameOpts...)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to parse image %s\", opts.ImageRef)\n-\t}\n-\n-\tsignatures, bundleVerified, err := tracing.ChildSpan3(\n-\t\tctx,\n-\t\t\"\",\n-\t\t\"VERIFY IMG SIGS\",\n-\t\tfunc(ctx context.Context, span trace.Span) ([]oci.Signature, bool, error) {\n-\t\t\tcosignOpts, err := buildCosignOptions(ctx, opts)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, false, err\n-\t\t\t}\n-\t\t\treturn client.VerifyImageSignatures(ctx, ref, cosignOpts)\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\tlogger.Info(\"image verification failed\", \"error\", err.Error())\n-\t\treturn nil, err\n-\t}\n-\n-\tlogger.V(3).Info(\"verified image\", \"count\", len(signatures), \"bundleVerified\", bundleVerified)\n-\tpayload, err := extractPayload(signatures)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err := matchSignatures(signatures, opts.Subject, opts.SubjectRegExp, opts.Issuer, opts.IssuerRegExp, opts.AdditionalExtensions); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = checkAnnotations(payload, opts.Annotations)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar digest string\n-\tif opts.Type == \"\" {\n-\t\tdigest, err = extractDigest(opts.ImageRef, payload)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn &images.Response{Digest: digest}, nil\n+        if opts.SigstoreBundle {\n+                results, err := verifyBundleAndFetchAttestations(ctx, opts)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if len(results) == 0 {\n+                        return nil, fmt.Errorf(\"sigstore bundle verification failed: no matching signatures found\")\n+                }\n+\n+                return &images.Response{Digest: results[0].Desc.Digest.String()}, nil\n+        }\n+\n+        nameOpts := opts.Client.NameOptions()\n+        ref, err := name.ParseReference(opts.ImageRef, nameOpts...)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to parse image %s\", opts.ImageRef)\n+        }\n+\n+        signatures, bundleVerified, err := tracing.ChildSpan3(\n+                ctx,\n+                \"\",\n+                \"VERIFY IMG SIGS\",\n+                func(ctx context.Context, span trace.Span) ([]oci.Signature, bool, error) {\n+                        cosignOpts, err := buildCosignOptions(ctx, opts)\n+                        if err != nil {\n+                                return nil, false, err\n+                        }\n+                        return client.VerifyImageSignatures(ctx, ref, cosignOpts)\n+                },\n+        )\n+        if err != nil {\n+                logger.Info(\"image verification failed\", \"error\", err.Error())\n+                return nil, err\n+        }\n+\n+        logger.V(3).Info(\"verified image\", \"count\", len(signatures), \"bundleVerified\", bundleVerified)\n+        payload, err := extractPayload(signatures)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if err := matchSignatures(signatures, opts.Subject, opts.SubjectRegExp, opts.Issuer, opts.IssuerRegExp, opts.AdditionalExtensions); err != nil {\n+                return nil, err\n+        }\n+\n+        err = checkAnnotations(payload, opts.Annotations)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var digest string\n+        if opts.Type == \"\" {\n+                digest, err = extractDigest(opts.ImageRef, payload)\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return &images.Response{Digest: digest}, nil\n }\n \n func buildCosignOptions(ctx context.Context, opts images.Options) (*cosign.CheckOpts, error) {\n-\tvar err error\n-\n-\toptions, err := opts.Client.Options(ctx)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"constructing cosign remote options: %w\", err)\n-\t}\n-\n-\tcosignOpts := &cosign.CheckOpts{\n-\t\tAnnotations:        map[string]interface{}{},\n-\t\tRegistryClientOpts: []remote.Option{remote.WithRemoteOptions(options...)},\n-\t}\n-\n-\tif opts.FetchAttestations {\n-\t\tcosignOpts.ClaimVerifier = cosign.IntotoSubjectClaimVerifier\n-\t} else {\n-\t\tcosignOpts.ClaimVerifier = cosign.SimpleClaimVerifier\n-\t}\n-\n-\tif opts.Roots != \"\" {\n-\t\tcp, err := loadCertPool([]byte(opts.Roots))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to load Root certificates: %w\", err)\n-\t\t}\n-\t\tcosignOpts.RootCerts = cp\n-\t}\n-\n-\tsignatureAlgorithm, ok := signatureAlgorithmMap[opts.SignatureAlgorithm]\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"invalid signature algorithm provided %s\", opts.SignatureAlgorithm)\n-\t}\n-\n-\tif opts.Key != \"\" {\n-\t\tif strings.HasPrefix(strings.TrimSpace(opts.Key), \"-----BEGIN PUBLIC KEY-----\") {\n-\t\t\tcosignOpts.SigVerifier, err = decodePEM([]byte(opts.Key), signatureAlgorithm)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"failed to load public key from PEM: %w\", err)\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// this supports Kubernetes secrets and KMS\n-\t\t\tcosignOpts.SigVerifier, err = sigs.PublicKeyFromKeyRefWithHashAlgo(ctx, opts.Key, signatureAlgorithm)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"failed to load public key from %s: %w\", opts.Key, err)\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\tif opts.Cert != \"\" {\n-\t\t\t// load cert and optionally a cert chain as a verifier\n-\t\t\tcert, err := loadCert([]byte(opts.Cert))\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"failed to load certificate from %s: %w\", opts.Cert, err)\n-\t\t\t}\n-\n-\t\t\tif opts.CertChain == \"\" {\n-\t\t\t\tcosignOpts.SigVerifier, err = signature.LoadVerifier(cert.PublicKey, signatureAlgorithm)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to load signature from certificate: %w\", err)\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\t// Verify certificate with chain\n-\t\t\t\tchain, err := loadCertChain([]byte(opts.CertChain))\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to load load certificate chain: %w\", err)\n-\t\t\t\t}\n-\t\t\t\tcosignOpts.SigVerifier, err = cosign.ValidateAndUnpackCertWithChain(cert, chain, cosignOpts)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to load validate certificate chain: %w\", err)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else if opts.CertChain != \"\" {\n-\t\t\t// load cert chain as roots\n-\t\t\tcp, err := loadCertPool([]byte(opts.CertChain))\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"failed to load certificates: %w\", err)\n-\t\t\t}\n-\t\t\tcosignOpts.RootCerts = cp\n-\t\t} else {\n-\t\t\t// if key, cert, and roots are not provided, default to Fulcio roots\n-\t\t\tif cosignOpts.RootCerts == nil {\n-\t\t\t\troots, err := fulcioroots.Get()\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to get roots from fulcio: %w\", err)\n-\t\t\t\t}\n-\t\t\t\tcosignOpts.RootCerts = roots\n-\t\t\t\tif cosignOpts.RootCerts == nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to initialize roots\")\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tcosignOpts.IgnoreTlog = opts.IgnoreTlog\n-\tif !opts.IgnoreTlog {\n-\t\tcosignOpts.RekorClient, err = rekorclient.GetRekorClient(opts.RekorURL)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to create Rekor client from URL %s: %w\", opts.RekorURL, err)\n-\t\t}\n-\n-\t\tcosignOpts.RekorPubKeys, err = getRekorPubs(ctx, opts.RekorPubKey)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to load Rekor public keys: %w\", err)\n-\t\t}\n-\t}\n-\n-\tcosignOpts.IgnoreSCT = opts.IgnoreSCT\n-\tif !opts.IgnoreSCT {\n-\t\tcosignOpts.CTLogPubKeys, err = getCTLogPubs(ctx, opts.CTLogsPubKey)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to load CTLogs public keys: %w\", err)\n-\t\t}\n-\t}\n-\n-\tif opts.Repository != \"\" {\n-\t\tsignatureRepo, err := name.NewRepository(opts.Repository)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to parse signature repository %s: %w\", opts.Repository, err)\n-\t\t}\n-\n-\t\tcosignOpts.RegistryClientOpts = append(cosignOpts.RegistryClientOpts, remote.WithTargetRepository(signatureRepo))\n-\t}\n-\n-\tif opts.TSACertChain != \"\" {\n-\t\tleaves, intermediates, roots, err := splitPEMCertificateChain([]byte(opts.TSACertChain))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error splitting tsa certificates: %w\", err)\n-\t\t}\n-\t\tif len(leaves) > 1 {\n-\t\t\treturn nil, fmt.Errorf(\"certificate chain must contain at most one TSA certificate\")\n-\t\t}\n-\t\tif len(leaves) == 1 {\n-\t\t\tcosignOpts.TSACertificate = leaves[0]\n-\t\t}\n-\t\tcosignOpts.TSAIntermediateCertificates = intermediates\n-\t\tcosignOpts.TSARootCertificates = roots\n-\t}\n-\n-\tcosignOpts.ExperimentalOCI11 = opts.CosignOCI11\n-\treturn cosignOpts, nil\n+        var err error\n+\n+        options, err := opts.Client.Options(ctx)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"constructing cosign remote options: %w\", err)\n+        }\n+\n+        cosignOpts := &cosign.CheckOpts{\n+                Annotations:        map[string]interface{}{},\n+                RegistryClientOpts: []remote.Option{remote.WithRemoteOptions(options...)},\n+        }\n+\n+        if opts.FetchAttestations {\n+                cosignOpts.ClaimVerifier = cosign.IntotoSubjectClaimVerifier\n+        } else {\n+                cosignOpts.ClaimVerifier = cosign.SimpleClaimVerifier\n+        }\n+\n+        if opts.Roots != \"\" {\n+                cp, err := loadCertPool([]byte(opts.Roots))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to load Root certificates: %w\", err)\n+                }\n+                cosignOpts.RootCerts = cp\n+        }\n+\n+        signatureAlgorithm, ok := signatureAlgorithmMap[opts.SignatureAlgorithm]\n+        if !ok {\n+                return nil, fmt.Errorf(\"invalid signature algorithm provided %s\", opts.SignatureAlgorithm)\n+        }\n+\n+        if opts.Key != \"\" {\n+                if strings.HasPrefix(strings.TrimSpace(opts.Key), \"-----BEGIN PUBLIC KEY-----\") {\n+                        cosignOpts.SigVerifier, err = decodePEM([]byte(opts.Key), signatureAlgorithm)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"failed to load public key from PEM: %w\", err)\n+                        }\n+                } else {\n+                        // this supports Kubernetes secrets and KMS\n+                        cosignOpts.SigVerifier, err = sigs.PublicKeyFromKeyRefWithHashAlgo(ctx, opts.Key, signatureAlgorithm)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"failed to load public key from %s: %w\", opts.Key, err)\n+                        }\n+                }\n+        } else {\n+                if opts.Cert != \"\" {\n+                        // load cert and optionally a cert chain as a verifier\n+                        cert, err := loadCert([]byte(opts.Cert))\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"failed to load certificate from %s: %w\", opts.Cert, err)\n+                        }\n+\n+                        if opts.CertChain == \"\" {\n+                                cosignOpts.SigVerifier, err = signature.LoadVerifier(cert.PublicKey, signatureAlgorithm)\n+                                if err != nil {\n+                                        return nil, fmt.Errorf(\"failed to load signature from certificate: %w\", err)\n+                                }\n+                        } else {\n+                                // Verify certificate with chain\n+                                chain, err := loadCertChain([]byte(opts.CertChain))\n+                                if err != nil {\n+                                        return nil, fmt.Errorf(\"failed to load load certificate chain: %w\", err)\n+                                }\n+                                cosignOpts.SigVerifier, err = cosign.ValidateAndUnpackCertWithChain(cert, chain, cosignOpts)\n+                                if err != nil {\n+                                        return nil, fmt.Errorf(\"failed to load validate certificate chain: %w\", err)\n+                                }\n+                        }\n+                } else if opts.CertChain != \"\" {\n+                        // load cert chain as roots\n+                        cp, err := loadCertPool([]byte(opts.CertChain))\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"failed to load certificates: %w\", err)\n+                        }\n+                        cosignOpts.RootCerts = cp\n+                } else {\n+                        // if key, cert, and roots are not provided, default to Fulcio roots\n+                        if cosignOpts.RootCerts == nil {\n+                                roots, err := fulcioroots.Get()\n+                                if err != nil {\n+                                        return nil, fmt.Errorf(\"failed to get roots from fulcio: %w\", err)\n+                                }\n+                                cosignOpts.RootCerts = roots\n+                                if cosignOpts.RootCerts == nil {\n+                                        return nil, fmt.Errorf(\"failed to initialize roots\")\n+                                }\n+                        }\n+                }\n+        }\n+\n+        cosignOpts.IgnoreTlog = opts.IgnoreTlog\n+        if !opts.IgnoreTlog {\n+                cosignOpts.RekorClient, err = rekorclient.GetRekorClient(opts.RekorURL)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to create Rekor client from URL %s: %w\", opts.RekorURL, err)\n+                }\n+\n+                cosignOpts.RekorPubKeys, err = getRekorPubs(ctx, opts.RekorPubKey)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to load Rekor public keys: %w\", err)\n+                }\n+        }\n+\n+        cosignOpts.IgnoreSCT = opts.IgnoreSCT\n+        if !opts.IgnoreSCT {\n+                cosignOpts.CTLogPubKeys, err = getCTLogPubs(ctx, opts.CTLogsPubKey)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to load CTLogs public keys: %w\", err)\n+                }\n+        }\n+\n+        if opts.Repository != \"\" {\n+                signatureRepo, err := name.NewRepository(opts.Repository)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to parse signature repository %s: %w\", opts.Repository, err)\n+                }\n+\n+                cosignOpts.RegistryClientOpts = append(cosignOpts.RegistryClientOpts, remote.WithTargetRepository(signatureRepo))\n+        }\n+\n+        if opts.TSACertChain != \"\" {\n+                leaves, intermediates, roots, err := splitPEMCertificateChain([]byte(opts.TSACertChain))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error splitting tsa certificates: %w\", err)\n+                }\n+                if len(leaves) > 1 {\n+                        return nil, fmt.Errorf(\"certificate chain must contain at most one TSA certificate\")\n+                }\n+                if len(leaves) == 1 {\n+                        cosignOpts.TSACertificate = leaves[0]\n+                }\n+                cosignOpts.TSAIntermediateCertificates = intermediates\n+                cosignOpts.TSARootCertificates = roots\n+        }\n+\n+        cosignOpts.ExperimentalOCI11 = opts.CosignOCI11\n+        return cosignOpts, nil\n }\n \n func loadCertPool(roots []byte) (*x509.CertPool, error) {\n-\tcp := x509.NewCertPool()\n-\tif !cp.AppendCertsFromPEM(roots) {\n-\t\treturn nil, fmt.Errorf(\"error creating root cert pool\")\n-\t}\n+        cp := x509.NewCertPool()\n+        if !cp.AppendCertsFromPEM(roots) {\n+                return nil, fmt.Errorf(\"error creating root cert pool\")\n+        }\n \n-\treturn cp, nil\n+        return cp, nil\n }\n \n func loadCert(pem []byte) (*x509.Certificate, error) {\n-\tvar out []byte\n-\tout, err := base64.StdEncoding.DecodeString(string(pem))\n-\tif err != nil {\n-\t\t// not a base64\n-\t\tout = pem\n-\t}\n-\n-\tcerts, err := cryptoutils.UnmarshalCertificatesFromPEM(out)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to unmarshal certificate from PEM format: %w\", err)\n-\t}\n-\tif len(certs) == 0 {\n-\t\treturn nil, fmt.Errorf(\"no certs found in pem file\")\n-\t}\n-\treturn certs[0], nil\n+        var out []byte\n+        out, err := base64.StdEncoding.DecodeString(string(pem))\n+        if err != nil {\n+                // not a base64\n+                out = pem\n+        }\n+\n+        certs, err := cryptoutils.UnmarshalCertificatesFromPEM(out)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to unmarshal certificate from PEM format: %w\", err)\n+        }\n+        if len(certs) == 0 {\n+                return nil, fmt.Errorf(\"no certs found in pem file\")\n+        }\n+        return certs[0], nil\n }\n \n func loadCertChain(pem []byte) ([]*x509.Certificate, error) {\n-\treturn cryptoutils.LoadCertificatesFromPEM(bytes.NewReader(pem))\n+        return cryptoutils.LoadCertificatesFromPEM(bytes.NewReader(pem))\n }\n \n func (v *cosignVerifier) FetchAttestations(ctx context.Context, opts images.Options) (*images.Response, error) {\n-\tif opts.SigstoreBundle {\n-\t\tresults, err := verifyBundleAndFetchAttestations(ctx, opts)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif len(results) == 0 {\n-\t\t\treturn nil, fmt.Errorf(\"sigstore bundle verification failed: no matching signatures found\")\n-\t\t}\n-\n-\t\tstatements, err := decodeStatementsFromBundles(results)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\treturn &images.Response{Digest: results[0].Desc.Digest.String(), Statements: statements}, nil\n-\t}\n-\tcosignOpts, err := buildCosignOptions(ctx, opts)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tnameOpts := opts.Client.NameOptions()\n-\tsignatures, bundleVerified, err := tracing.ChildSpan3(\n-\t\tctx,\n-\t\t\"\",\n-\t\t\"VERIFY IMG ATTESTATIONS\",\n-\t\tfunc(ctx context.Context, span trace.Span) (checkedAttestations []oci.Signature, bundleVerified bool, err error) {\n-\t\t\tref, err := name.ParseReference(opts.ImageRef, nameOpts...)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, false, fmt.Errorf(\"failed to parse image: %w\", err)\n-\t\t\t}\n-\t\t\treturn client.VerifyImageAttestations(ctx, ref, cosignOpts)\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\tmsg := err.Error()\n-\t\tlogger.Info(\"failed to fetch attestations\", \"error\", msg)\n-\t\tif strings.Contains(msg, \"MANIFEST_UNKNOWN: manifest unknown\") {\n-\t\t\treturn nil, fmt.Errorf(\"not found\")\n-\t\t}\n-\n-\t\treturn nil, err\n-\t}\n-\n-\tpayload, err := extractPayload(signatures)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor _, signature := range signatures {\n-\t\tmatch, predicateType, err := matchType(signature, opts.Type)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif !match {\n-\t\t\tlogger.V(4).Info(\"type doesn't match, continue\", \"expected\", opts.Type, \"received\", predicateType)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif err := matchSignatures([]oci.Signature{signature}, opts.Subject, opts.SubjectRegExp, opts.Issuer, opts.IssuerRegExp, opts.AdditionalExtensions); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\terr = checkAnnotations(payload, opts.Annotations)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tlogger.V(3).Info(\"verified images\", \"signatures\", len(signatures), \"bundleVerified\", bundleVerified)\n-\tinTotoStatements, digest, err := decodeStatements(signatures)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &images.Response{Digest: digest, Statements: inTotoStatements}, nil\n+        if opts.SigstoreBundle {\n+                results, err := verifyBundleAndFetchAttestations(ctx, opts)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if len(results) == 0 {\n+                        return nil, fmt.Errorf(\"sigstore bundle verification failed: no matching signatures found\")\n+                }\n+\n+                statements, err := decodeStatementsFromBundles(results)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                return &images.Response{Digest: results[0].Desc.Digest.String(), Statements: statements}, nil\n+        }\n+        cosignOpts, err := buildCosignOptions(ctx, opts)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        nameOpts := opts.Client.NameOptions()\n+        signatures, bundleVerified, err := tracing.ChildSpan3(\n+                ctx,\n+                \"\",\n+                \"VERIFY IMG ATTESTATIONS\",\n+                func(ctx context.Context, span trace.Span) (checkedAttestations []oci.Signature, bundleVerified bool, err error) {\n+                        ref, err := name.ParseReference(opts.ImageRef, nameOpts...)\n+                        if err != nil {\n+                                return nil, false, fmt.Errorf(\"failed to parse image: %w\", err)\n+                        }\n+                        return client.VerifyImageAttestations(ctx, ref, cosignOpts)\n+                },\n+        )\n+        if err != nil {\n+                msg := err.Error()\n+                logger.Info(\"failed to fetch attestations\", \"error\", msg)\n+                if strings.Contains(msg, \"MANIFEST_UNKNOWN: manifest unknown\") {\n+                        return nil, fmt.Errorf(\"not found\")\n+                }\n+\n+                return nil, err\n+        }\n+\n+        payload, err := extractPayload(signatures)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for _, signature := range signatures {\n+                match, predicateType, err := matchType(signature, opts.Type)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if !match {\n+                        logger.V(4).Info(\"type doesn't match, continue\", \"expected\", opts.Type, \"received\", predicateType)\n+                        continue\n+                }\n+\n+                if err := matchSignatures([]oci.Signature{signature}, opts.Subject, opts.SubjectRegExp, opts.Issuer, opts.IssuerRegExp, opts.AdditionalExtensions); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        err = checkAnnotations(payload, opts.Annotations)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        logger.V(3).Info(\"verified images\", \"signatures\", len(signatures), \"bundleVerified\", bundleVerified)\n+        inTotoStatements, digest, err := decodeStatements(signatures)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        return &images.Response{Digest: digest, Statements: inTotoStatements}, nil\n }\n \n func matchType(sig oci.Signature, expectedType string) (bool, string, error) {\n-\tif expectedType != \"\" {\n-\t\tstatement, _, err := decodeStatement(sig)\n-\t\tif err != nil {\n-\t\t\treturn false, \"\", fmt.Errorf(\"failed to decode type: %w\", err)\n-\t\t}\n-\n-\t\tif pType, ok := statement[\"type\"]; ok {\n-\t\t\tif pType.(string) == expectedType {\n-\t\t\t\treturn true, pType.(string), nil\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn false, \"\", nil\n+        if expectedType != \"\" {\n+                statement, _, err := decodeStatement(sig)\n+                if err != nil {\n+                        return false, \"\", fmt.Errorf(\"failed to decode type: %w\", err)\n+                }\n+\n+                if pType, ok := statement[\"type\"]; ok {\n+                        if pType.(string) == expectedType {\n+                                return true, pType.(string), nil\n+                        }\n+                }\n+        }\n+        return false, \"\", nil\n }\n \n func decodeStatements(sigs []oci.Signature) ([]map[string]interface{}, string, error) {\n-\tif len(sigs) == 0 {\n-\t\treturn []map[string]interface{}{}, \"\", nil\n-\t}\n-\n-\tvar digest string\n-\tvar statement map[string]interface{}\n-\tdecodedStatements := make([]map[string]interface{}, len(sigs))\n-\tfor i, sig := range sigs {\n-\t\tvar err error\n-\t\tstatement, digest, err = decodeStatement(sig)\n-\t\tif err != nil {\n-\t\t\treturn nil, \"\", err\n-\t\t}\n-\n-\t\tdecodedStatements[i] = statement\n-\t}\n-\n-\treturn decodedStatements, digest, nil\n+        if len(sigs) == 0 {\n+                return []map[string]interface{}{}, \"\", nil\n+        }\n+\n+        var digest string\n+        var statement map[string]interface{}\n+        decodedStatements := make([]map[string]interface{}, len(sigs))\n+        for i, sig := range sigs {\n+                var err error\n+                statement, digest, err = decodeStatement(sig)\n+                if err != nil {\n+                        return nil, \"\", err\n+                }\n+\n+                decodedStatements[i] = statement\n+        }\n+\n+        return decodedStatements, digest, nil\n }\n \n func decodeStatement(sig oci.Signature) (map[string]interface{}, string, error) {\n-\tvar digest string\n-\n-\tpld, err := sig.Payload()\n-\tif err != nil {\n-\t\treturn nil, \"\", fmt.Errorf(\"failed to decode payload: %w\", err)\n-\t}\n-\n-\tsci := payload.SimpleContainerImage{}\n-\tif err := json.Unmarshal(pld, &sci); err != nil {\n-\t\treturn nil, \"\", fmt.Errorf(\"error decoding the payload: %w\", err)\n-\t}\n-\n-\tif d := sci.Critical.Image.DockerManifestDigest; d != \"\" {\n-\t\tdigest = d\n-\t}\n-\n-\tdata := make(map[string]interface{})\n-\tif err := json.Unmarshal(pld, &data); err != nil {\n-\t\treturn nil, \"\", fmt.Errorf(\"failed to unmarshal JSON payload: %v: %w\", sig, err)\n-\t}\n-\n-\tif dataPayload, ok := data[\"payload\"]; !ok {\n-\t\treturn nil, \"\", fmt.Errorf(\"missing payload in %v\", data)\n-\t} else {\n-\t\tdecodedStatement, err := decodePayload(dataPayload.(string))\n-\t\tif err != nil {\n-\t\t\treturn nil, \"\", fmt.Errorf(\"failed to decode statement %s: %w\", string(pld), err)\n-\t\t}\n-\t\tdecodedStatement[\"type\"] = decodedStatement[\"predicateType\"]\n-\n-\t\treturn decodedStatement, digest, nil\n-\t}\n+        var digest string\n+\n+        pld, err := sig.Payload()\n+        if err != nil {\n+                return nil, \"\", fmt.Errorf(\"failed to decode payload: %w\", err)\n+        }\n+\n+        sci := payload.SimpleContainerImage{}\n+        if err := json.Unmarshal(pld, &sci); err != nil {\n+                return nil, \"\", fmt.Errorf(\"error decoding the payload: %w\", err)\n+        }\n+\n+        if d := sci.Critical.Image.DockerManifestDigest; d != \"\" {\n+                digest = d\n+        }\n+\n+        data := make(map[string]interface{})\n+        if err := json.Unmarshal(pld, &data); err != nil {\n+                return nil, \"\", fmt.Errorf(\"failed to unmarshal JSON payload: %v: %w\", sig, err)\n+        }\n+\n+        if dataPayload, ok := data[\"payload\"]; !ok {\n+                return nil, \"\", fmt.Errorf(\"missing payload in %v\", data)\n+        } else {\n+                decodedStatement, err := decodePayload(dataPayload.(string))\n+                if err != nil {\n+                        return nil, \"\", fmt.Errorf(\"failed to decode statement %s: %w\", string(pld), err)\n+                }\n+                decodedStatement[\"type\"] = decodedStatement[\"predicateType\"]\n+\n+                return decodedStatement, digest, nil\n+        }\n }\n \n func decodePayload(payloadBase64 string) (map[string]interface{}, error) {\n-\tstatementRaw, err := base64.StdEncoding.DecodeString(payloadBase64)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to base64 decode payload for %v: %w\", statementRaw, err)\n-\t}\n-\n-\tvar statement in_toto.Statement //nolint:staticcheck\n-\tif err := json.Unmarshal(statementRaw, &statement); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif statement.Type != attestation.CosignCustomProvenanceV01 {\n-\t\t// This assumes that the following statements are JSON objects:\n-\t\t// - in_toto.PredicateSLSAProvenanceV01\n-\t\t// - in_toto.PredicateLinkV1\n-\t\t// - in_toto.PredicateSPDX\n-\t\t// any other custom predicate\n-\t\treturn datautils.ToMap(statement)\n-\t}\n-\n-\treturn decodeCosignCustomProvenanceV01(statement)\n+        statementRaw, err := base64.StdEncoding.DecodeString(payloadBase64)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to base64 decode payload for %v: %w\", statementRaw, err)\n+        }\n+\n+        var statement in_toto.Statement //nolint:staticcheck\n+        if err := json.Unmarshal(statementRaw, &statement); err != nil {\n+                return nil, err\n+        }\n+\n+        if statement.Type != attestation.CosignCustomProvenanceV01 {\n+                // This assumes that the following statements are JSON objects:\n+                // - in_toto.PredicateSLSAProvenanceV01\n+                // - in_toto.PredicateLinkV1\n+                // - in_toto.PredicateSPDX\n+                // any other custom predicate\n+                return datautils.ToMap(statement)\n+        }\n+\n+        return decodeCosignCustomProvenanceV01(statement)\n }\n \n func decodeCosignCustomProvenanceV01(statement in_toto.Statement) (map[string]interface{}, error) { //nolint:staticcheck\n-\tif statement.Type != attestation.CosignCustomProvenanceV01 {\n-\t\treturn nil, fmt.Errorf(\"invalid statement type %s\", attestation.CosignCustomProvenanceV01)\n-\t}\n-\n-\tpredicate, ok := statement.Predicate.(map[string]interface{})\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"failed to decode CosignCustomProvenanceV01\")\n-\t}\n-\n-\tcosignPredicateData := predicate[\"Data\"]\n-\tif cosignPredicateData == nil {\n-\t\treturn nil, fmt.Errorf(\"missing predicate in CosignCustomProvenanceV01\")\n-\t}\n-\n-\t// attempt to parse as a JSON object type\n-\tdata, err := stringToJSONMap(cosignPredicateData)\n-\tif err == nil {\n-\t\tpredicate[\"Data\"] = data\n-\t\tstatement.Predicate = predicate\n-\t}\n-\n-\treturn datautils.ToMap(statement)\n+        if statement.Type != attestation.CosignCustomProvenanceV01 {\n+                return nil, fmt.Errorf(\"invalid statement type %s\", attestation.CosignCustomProvenanceV01)\n+        }\n+\n+        predicate, ok := statement.Predicate.(map[string]interface{})\n+        if !ok {\n+                return nil, fmt.Errorf(\"failed to decode CosignCustomProvenanceV01\")\n+        }\n+\n+        cosignPredicateData := predicate[\"Data\"]\n+        if cosignPredicateData == nil {\n+                return nil, fmt.Errorf(\"missing predicate in CosignCustomProvenanceV01\")\n+        }\n+\n+        // attempt to parse as a JSON object type\n+        data, err := stringToJSONMap(cosignPredicateData)\n+        if err == nil {\n+                predicate[\"Data\"] = data\n+                statement.Predicate = predicate\n+        }\n+\n+        return datautils.ToMap(statement)\n }\n \n func stringToJSONMap(i interface{}) (map[string]interface{}, error) {\n-\ts, ok := i.(string)\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"expected string type\")\n-\t}\n+        s, ok := i.(string)\n+        if !ok {\n+                return nil, fmt.Errorf(\"expected string type\")\n+        }\n \n-\tdata := map[string]interface{}{}\n-\tif err := json.Unmarshal([]byte(s), &data); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to marshal JSON: %w\", err)\n-\t}\n+        data := map[string]interface{}{}\n+        if err := json.Unmarshal([]byte(s), &data); err != nil {\n+                return nil, fmt.Errorf(\"failed to marshal JSON: %w\", err)\n+        }\n \n-\treturn data, nil\n+        return data, nil\n }\n \n func decodePEM(raw []byte, signatureAlgorithm crypto.Hash) (signature.Verifier, error) {\n-\t// PEM encoded file.\n-\tpubKey, err := cryptoutils.UnmarshalPEMToPublicKey(raw)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"pem to public key: %w\", err)\n-\t}\n+        // PEM encoded file.\n+        pubKey, err := cryptoutils.UnmarshalPEMToPublicKey(raw)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"pem to public key: %w\", err)\n+        }\n \n-\treturn signature.LoadVerifier(pubKey, signatureAlgorithm)\n+        return signature.LoadVerifier(pubKey, signatureAlgorithm)\n }\n \n func extractPayload(verified []oci.Signature) ([]payload.SimpleContainerImage, error) {\n-\tsigPayloads := make([]payload.SimpleContainerImage, 0, len(verified))\n-\tfor _, sig := range verified {\n-\t\tpld, err := sig.Payload()\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to get payload: %w\", err)\n-\t\t}\n-\n-\t\tsci := payload.SimpleContainerImage{}\n-\t\tif err := json.Unmarshal(pld, &sci); err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error decoding the payload: %w\", err)\n-\t\t}\n-\n-\t\tsigPayloads = append(sigPayloads, sci)\n-\t}\n-\treturn sigPayloads, nil\n+        sigPayloads := make([]payload.SimpleContainerImage, 0, len(verified))\n+        for _, sig := range verified {\n+                pld, err := sig.Payload()\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to get payload: %w\", err)\n+                }\n+\n+                sci := payload.SimpleContainerImage{}\n+                if err := json.Unmarshal(pld, &sci); err != nil {\n+                        return nil, fmt.Errorf(\"error decoding the payload: %w\", err)\n+                }\n+\n+                sigPayloads = append(sigPayloads, sci)\n+        }\n+        return sigPayloads, nil\n }\n \n func extractDigest(imgRef string, payload []payload.SimpleContainerImage) (string, error) {\n-\tfor _, p := range payload {\n-\t\tif digest := p.Critical.Image.DockerManifestDigest; digest != \"\" {\n-\t\t\treturn digest, nil\n-\t\t} else {\n-\t\t\treturn \"\", fmt.Errorf(\"failed to extract image digest from signature payload for %s\", imgRef)\n-\t\t}\n-\t}\n-\treturn \"\", fmt.Errorf(\"digest not found for %s\", imgRef)\n+        for _, p := range payload {\n+                if digest := p.Critical.Image.DockerManifestDigest; digest != \"\" {\n+                        return digest, nil\n+                } else {\n+                        return \"\", fmt.Errorf(\"failed to extract image digest from signature payload for %s\", imgRef)\n+                }\n+        }\n+        return \"\", fmt.Errorf(\"digest not found for %s\", imgRef)\n }\n \n func matchSignatures(signatures []oci.Signature, subject, subjectRegExp, issuer, issuerRegExp string, extensions map[string]string) error {\n-\tif subject == \"\" && issuer == \"\" && len(extensions) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tvar errs []error\n-\tfor _, sig := range signatures {\n-\t\tcert, err := sig.Cert()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"failed to read certificate: %w\", err)\n-\t\t}\n-\n-\t\tif cert == nil {\n-\t\t\treturn fmt.Errorf(\"certificate not found\")\n-\t\t}\n-\n-\t\tif err := matchCertificateData(cert, subject, subjectRegExp, issuer, issuerRegExp, extensions); err != nil {\n-\t\t\terrs = append(errs, err)\n-\t\t} else {\n-\t\t\t// only one signature certificate needs to match the required subject, issuer, and extensions\n-\t\t\treturn nil\n-\t\t}\n-\t}\n-\n-\tif len(errs) > 0 {\n-\t\terr := multierr.Combine(errs...)\n-\t\treturn err\n-\t}\n-\n-\treturn fmt.Errorf(\"invalid signature\")\n+        if subject == \"\" && subjectRegExp == \"\" && issuer == \"\" && issuerRegExp == \"\" && len(extensions) == 0 {\n+                return nil\n+        }\n+\n+        var errs []error\n+        for _, sig := range signatures {\n+                cert, err := sig.Cert()\n+                if err != nil {\n+                        return fmt.Errorf(\"failed to read certificate: %w\", err)\n+                }\n+\n+                if cert == nil {\n+                        return fmt.Errorf(\"certificate not found\")\n+                }\n+\n+                if err := matchCertificateData(cert, subject, subjectRegExp, issuer, issuerRegExp, extensions); err != nil {\n+                        errs = append(errs, err)\n+                } else {\n+                        // only one signature certificate needs to match the required subject, issuer, and extensions\n+                        return nil\n+                }\n+        }\n+\n+        if len(errs) > 0 {\n+                err := multierr.Combine(errs...)\n+                return err\n+        }\n+\n+        return fmt.Errorf(\"invalid signature\")\n }\n \n func matchCertificateData(cert *x509.Certificate, subject, subjectRegExp, issuer, issuerRegExp string, extensions map[string]string) error {\n-\tif subject != \"\" || subjectRegExp != \"\" {\n-\t\tif sans := cryptoutils.GetSubjectAlternateNames(cert); len(sans) > 0 {\n-\t\t\tsubjectMatched := false\n-\t\t\tif subject != \"\" {\n-\t\t\t\tfor _, s := range sans {\n-\t\t\t\t\tif wildcard.Match(subject, s) {\n-\t\t\t\t\t\tsubjectMatched = true\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif subjectRegExp != \"\" {\n-\t\t\t\tregex, err := regexp.Compile(subjectRegExp)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn fmt.Errorf(\"invalid regexp for subject: %s : %w\", subjectRegExp, err)\n-\t\t\t\t}\n-\t\t\t\tfor _, s := range sans {\n-\t\t\t\t\tif regex.MatchString(s) {\n-\t\t\t\t\t\tsubjectMatched = true\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tif !subjectMatched {\n-\t\t\t\tsub := \"\"\n-\t\t\t\tif subject != \"\" {\n-\t\t\t\t\tsub = subject\n-\t\t\t\t} else if subjectRegExp != \"\" {\n-\t\t\t\t\tsub = subjectRegExp\n-\t\t\t\t}\n-\t\t\t\treturn fmt.Errorf(\"subject mismatch: expected %s, received %s\", sub, strings.Join(sans, \", \"))\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif err := matchExtensions(cert, issuer, issuerRegExp, extensions); err != nil {\n-\t\treturn err\n-\t}\n-\n-\treturn nil\n+        if subject != \"\" || subjectRegExp != \"\" {\n+                if sans := cryptoutils.GetSubjectAlternateNames(cert); len(sans) > 0 {\n+                        subjectMatched := false\n+                        if subject != \"\" {\n+                                for _, s := range sans {\n+                                        if wildcard.Match(subject, s) {\n+                                                subjectMatched = true\n+                                                break\n+                                        }\n+                                }\n+                        }\n+                        if subjectRegExp != \"\" {\n+                                regex, err := regexp.Compile(subjectRegExp)\n+                                if err != nil {\n+                                        return fmt.Errorf(\"invalid regexp for subject: %s : %w\", subjectRegExp, err)\n+                                }\n+                                for _, s := range sans {\n+                                        if regex.MatchString(s) {\n+                                                subjectMatched = true\n+                                                break\n+                                        }\n+                                }\n+                        }\n+\n+                        if !subjectMatched {\n+                                sub := \"\"\n+                                if subject != \"\" {\n+                                        sub = subject\n+                                } else if subjectRegExp != \"\" {\n+                                        sub = subjectRegExp\n+                                }\n+                                return fmt.Errorf(\"subject mismatch: expected %s, received %s\", sub, strings.Join(sans, \", \"))\n+                        }\n+                }\n+        }\n+\n+        if err := matchExtensions(cert, issuer, issuerRegExp, extensions); err != nil {\n+                return err\n+        }\n+\n+        return nil\n }\n \n func matchExtensions(cert *x509.Certificate, issuer, issuerRegExp string, extensions map[string]string) error {\n-\tce := cosign.CertExtensions{Cert: cert}\n-\n-\tif issuer != \"\" || issuerRegExp != \"\" {\n-\t\tval := ce.GetIssuer()\n-\t\tif issuer != \"\" {\n-\t\t\tif !wildcard.Match(issuer, val) {\n-\t\t\t\treturn fmt.Errorf(\"issuer mismatch: expected %s, received %s\", issuer, val)\n-\t\t\t}\n-\t\t}\n-\t\tif issuerRegExp != \"\" {\n-\t\t\tif regex, err := regexp.Compile(issuerRegExp); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"invalid regexp for issuer: %s : %w\", issuerRegExp, err)\n-\t\t\t} else if !regex.MatchString(val) {\n-\t\t\t\treturn fmt.Errorf(\"issuer mismatch: expected %s, received %s\", issuerRegExp, val)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tfor requiredKey, requiredValue := range extensions {\n-\t\tval, err := extractCertExtensionValue(requiredKey, ce)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tif !wildcard.Match(requiredValue, val) {\n-\t\t\treturn fmt.Errorf(\"extension mismatch: expected %s for key %s, received %s\", requiredValue, requiredKey, val)\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        ce := cosign.CertExtensions{Cert: cert}\n+\n+        if issuer != \"\" || issuerRegExp != \"\" {\n+                val := ce.GetIssuer()\n+                if issuer != \"\" {\n+                        if !wildcard.Match(issuer, val) {\n+                                return fmt.Errorf(\"issuer mismatch: expected %s, received %s\", issuer, val)\n+                        }\n+                }\n+                if issuerRegExp != \"\" {\n+                        if regex, err := regexp.Compile(issuerRegExp); err != nil {\n+                                return fmt.Errorf(\"invalid regexp for issuer: %s : %w\", issuerRegExp, err)\n+                        } else if !regex.MatchString(val) {\n+                                return fmt.Errorf(\"issuer mismatch: expected %s, received %s\", issuerRegExp, val)\n+                        }\n+                }\n+        }\n+\n+        for requiredKey, requiredValue := range extensions {\n+                val, err := extractCertExtensionValue(requiredKey, ce)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                if !wildcard.Match(requiredValue, val) {\n+                        return fmt.Errorf(\"extension mismatch: expected %s for key %s, received %s\", requiredValue, requiredKey, val)\n+                }\n+        }\n+\n+        return nil\n }\n \n func extractCertExtensionValue(key string, ce cosign.CertExtensions) (string, error) {\n-\tswitch key {\n-\tcase cosign.CertExtensionOIDCIssuer, cosign.CertExtensionMap[cosign.CertExtensionOIDCIssuer]:\n-\t\treturn ce.GetIssuer(), nil\n-\tcase cosign.CertExtensionGithubWorkflowTrigger, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowTrigger]:\n-\t\treturn ce.GetCertExtensionGithubWorkflowTrigger(), nil\n-\tcase cosign.CertExtensionGithubWorkflowSha, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowSha]:\n-\t\treturn ce.GetExtensionGithubWorkflowSha(), nil\n-\tcase cosign.CertExtensionGithubWorkflowName, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowName]:\n-\t\treturn ce.GetCertExtensionGithubWorkflowName(), nil\n-\tcase cosign.CertExtensionGithubWorkflowRepository, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowRepository]:\n-\t\treturn ce.GetCertExtensionGithubWorkflowRepository(), nil\n-\tcase cosign.CertExtensionGithubWorkflowRef, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowRef]:\n-\t\treturn ce.GetCertExtensionGithubWorkflowRef(), nil\n-\tdefault:\n-\t\treturn \"\", fmt.Errorf(\"invalid certificate extension %s\", key)\n-\t}\n+        switch key {\n+        case cosign.CertExtensionOIDCIssuer, cosign.CertExtensionMap[cosign.CertExtensionOIDCIssuer]:\n+                return ce.GetIssuer(), nil\n+        case cosign.CertExtensionGithubWorkflowTrigger, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowTrigger]:\n+                return ce.GetCertExtensionGithubWorkflowTrigger(), nil\n+        case cosign.CertExtensionGithubWorkflowSha, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowSha]:\n+                return ce.GetExtensionGithubWorkflowSha(), nil\n+        case cosign.CertExtensionGithubWorkflowName, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowName]:\n+                return ce.GetCertExtensionGithubWorkflowName(), nil\n+        case cosign.CertExtensionGithubWorkflowRepository, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowRepository]:\n+                return ce.GetCertExtensionGithubWorkflowRepository(), nil\n+        case cosign.CertExtensionGithubWorkflowRef, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowRef]:\n+                return ce.GetCertExtensionGithubWorkflowRef(), nil\n+        default:\n+                return \"\", fmt.Errorf(\"invalid certificate extension %s\", key)\n+        }\n }\n \n func checkAnnotations(payload []payload.SimpleContainerImage, annotations map[string]string) error {\n-\tfor _, p := range payload {\n-\t\tfor key, val := range annotations {\n-\t\t\tif val != p.Optional[key] {\n-\t\t\t\treturn fmt.Errorf(\"annotations mismatch: %s does not match expected value %s for key %s\",\n-\t\t\t\t\tp.Optional[key], val, key)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, p := range payload {\n+                for key, val := range annotations {\n+                        if val != p.Optional[key] {\n+                                return fmt.Errorf(\"annotations mismatch: %s does not match expected value %s for key %s\",\n+                                        p.Optional[key], val, key)\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n func getRekorPubs(ctx context.Context, rekorPubKey string) (*cosign.TrustedTransparencyLogPubKeys, error) {\n-\tif rekorPubKey == \"\" {\n-\t\treturn cosign.GetRekorPubs(ctx)\n-\t}\n-\n-\tpublicKeys := cosign.NewTrustedTransparencyLogPubKeys()\n-\tif err := publicKeys.AddTransparencyLogPubKey([]byte(rekorPubKey), tuf.Active); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get rekor public keys: %w\", err)\n-\t}\n-\treturn &publicKeys, nil\n+        if rekorPubKey == \"\" {\n+                return cosign.GetRekorPubs(ctx)\n+        }\n+\n+        publicKeys := cosign.NewTrustedTransparencyLogPubKeys()\n+        if err := publicKeys.AddTransparencyLogPubKey([]byte(rekorPubKey), tuf.Active); err != nil {\n+                return nil, fmt.Errorf(\"failed to get rekor public keys: %w\", err)\n+        }\n+        return &publicKeys, nil\n }\n \n func getCTLogPubs(ctx context.Context, ctlogPubKey string) (*cosign.TrustedTransparencyLogPubKeys, error) {\n-\tif ctlogPubKey == \"\" {\n-\t\treturn cosign.GetCTLogPubs(ctx)\n-\t}\n-\n-\tpublicKeys := cosign.NewTrustedTransparencyLogPubKeys()\n-\tif err := publicKeys.AddTransparencyLogPubKey([]byte(ctlogPubKey), tuf.Active); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get transparency log public keys: %w\", err)\n-\t}\n-\treturn &publicKeys, nil\n+        if ctlogPubKey == \"\" {\n+                return cosign.GetCTLogPubs(ctx)\n+        }\n+\n+        publicKeys := cosign.NewTrustedTransparencyLogPubKeys()\n+        if err := publicKeys.AddTransparencyLogPubKey([]byte(ctlogPubKey), tuf.Active); err != nil {\n+                return nil, fmt.Errorf(\"failed to get transparency log public keys: %w\", err)\n+        }\n+        return &publicKeys, nil\n }\n \n func splitPEMCertificateChain(pem []byte) (leaves, intermediates, roots []*x509.Certificate, err error) {\n-\tcerts, err := cryptoutils.UnmarshalCertificatesFromPEM(pem)\n-\tif err != nil {\n-\t\treturn nil, nil, nil, err\n-\t}\n-\n-\tfor _, cert := range certs {\n-\t\tif !cert.IsCA {\n-\t\t\tleaves = append(leaves, cert)\n-\t\t} else {\n-\t\t\t// root certificates are self-signed\n-\t\t\tif bytes.Equal(cert.RawSubject, cert.RawIssuer) {\n-\t\t\t\troots = append(roots, cert)\n-\t\t\t} else {\n-\t\t\t\tintermediates = append(intermediates, cert)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn leaves, intermediates, roots, nil\n+        certs, err := cryptoutils.UnmarshalCertificatesFromPEM(pem)\n+        if err != nil {\n+                return nil, nil, nil, err\n+        }\n+\n+        for _, cert := range certs {\n+                if !cert.IsCA {\n+                        leaves = append(leaves, cert)\n+                } else {\n+                        // root certificates are self-signed\n+                        if bytes.Equal(cert.RawSubject, cert.RawIssuer) {\n+                                roots = append(roots, cert)\n+                        } else {\n+                                intermediates = append(intermediates, cert)\n+                        }\n+                }\n+        }\n+\n+        return leaves, intermediates, roots, nil\n }\n"}
{"cve":"CVE-2022-24450:0708", "fix_patch": "diff --git a/server/consumer.go b/server/consumer.go\nindex 2a5b1039..cfcf162c 100644\n--- a/server/consumer.go\n+++ b/server/consumer.go\n@@ -14,167 +14,167 @@\n package server\n \n import (\n-\t\"bytes\"\n-\t\"encoding/binary\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"math/rand\"\n-\t\"reflect\"\n-\t\"sort\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\t\"github.com/nats-io/nuid\"\n-\t\"golang.org/x/time/rate\"\n+        \"bytes\"\n+        \"encoding/binary\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"math/rand\"\n+        \"reflect\"\n+        \"sort\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        \"github.com/nats-io/nuid\"\n+        \"golang.org/x/time/rate\"\n )\n \n type ConsumerInfo struct {\n-\tStream         string          `json:\"stream_name\"`\n-\tName           string          `json:\"name\"`\n-\tCreated        time.Time       `json:\"created\"`\n-\tConfig         *ConsumerConfig `json:\"config,omitempty\"`\n-\tDelivered      SequenceInfo    `json:\"delivered\"`\n-\tAckFloor       SequenceInfo    `json:\"ack_floor\"`\n-\tNumAckPending  int             `json:\"num_ack_pending\"`\n-\tNumRedelivered int             `json:\"num_redelivered\"`\n-\tNumWaiting     int             `json:\"num_waiting\"`\n-\tNumPending     uint64          `json:\"num_pending\"`\n-\tCluster        *ClusterInfo    `json:\"cluster,omitempty\"`\n-\tPushBound      bool            `json:\"push_bound,omitempty\"`\n+        Stream         string          `json:\"stream_name\"`\n+        Name           string          `json:\"name\"`\n+        Created        time.Time       `json:\"created\"`\n+        Config         *ConsumerConfig `json:\"config,omitempty\"`\n+        Delivered      SequenceInfo    `json:\"delivered\"`\n+        AckFloor       SequenceInfo    `json:\"ack_floor\"`\n+        NumAckPending  int             `json:\"num_ack_pending\"`\n+        NumRedelivered int             `json:\"num_redelivered\"`\n+        NumWaiting     int             `json:\"num_waiting\"`\n+        NumPending     uint64          `json:\"num_pending\"`\n+        Cluster        *ClusterInfo    `json:\"cluster,omitempty\"`\n+        PushBound      bool            `json:\"push_bound,omitempty\"`\n }\n \n type ConsumerConfig struct {\n-\tDurable         string          `json:\"durable_name,omitempty\"`\n-\tDescription     string          `json:\"description,omitempty\"`\n-\tDeliverPolicy   DeliverPolicy   `json:\"deliver_policy\"`\n-\tOptStartSeq     uint64          `json:\"opt_start_seq,omitempty\"`\n-\tOptStartTime    *time.Time      `json:\"opt_start_time,omitempty\"`\n-\tAckPolicy       AckPolicy       `json:\"ack_policy\"`\n-\tAckWait         time.Duration   `json:\"ack_wait,omitempty\"`\n-\tMaxDeliver      int             `json:\"max_deliver,omitempty\"`\n-\tBackOff         []time.Duration `json:\"backoff,omitempty\"`\n-\tFilterSubject   string          `json:\"filter_subject,omitempty\"`\n-\tReplayPolicy    ReplayPolicy    `json:\"replay_policy\"`\n-\tRateLimit       uint64          `json:\"rate_limit_bps,omitempty\"` // Bits per sec\n-\tSampleFrequency string          `json:\"sample_freq,omitempty\"`\n-\tMaxWaiting      int             `json:\"max_waiting,omitempty\"`\n-\tMaxAckPending   int             `json:\"max_ack_pending,omitempty\"`\n-\tHeartbeat       time.Duration   `json:\"idle_heartbeat,omitempty\"`\n-\tFlowControl     bool            `json:\"flow_control,omitempty\"`\n-\tHeadersOnly     bool            `json:\"headers_only,omitempty\"`\n-\n-\t// Pull based options.\n-\tMaxRequestBatch   int           `json:\"max_batch,omitempty\"`\n-\tMaxRequestExpires time.Duration `json:\"max_expires,omitempty\"`\n-\n-\t// Push based consumers.\n-\tDeliverSubject string `json:\"deliver_subject,omitempty\"`\n-\tDeliverGroup   string `json:\"deliver_group,omitempty\"`\n-\n-\t// Ephemeral inactivity threshold.\n-\tInactiveThreshold time.Duration `json:\"inactive_threshold,omitempty\"`\n-\n-\t// Don't add to general clients.\n-\tDirect bool `json:\"direct,omitempty\"`\n+        Durable         string          `json:\"durable_name,omitempty\"`\n+        Description     string          `json:\"description,omitempty\"`\n+        DeliverPolicy   DeliverPolicy   `json:\"deliver_policy\"`\n+        OptStartSeq     uint64          `json:\"opt_start_seq,omitempty\"`\n+        OptStartTime    *time.Time      `json:\"opt_start_time,omitempty\"`\n+        AckPolicy       AckPolicy       `json:\"ack_policy\"`\n+        AckWait         time.Duration   `json:\"ack_wait,omitempty\"`\n+        MaxDeliver      int             `json:\"max_deliver,omitempty\"`\n+        BackOff         []time.Duration `json:\"backoff,omitempty\"`\n+        FilterSubject   string          `json:\"filter_subject,omitempty\"`\n+        ReplayPolicy    ReplayPolicy    `json:\"replay_policy\"`\n+        RateLimit       uint64          `json:\"rate_limit_bps,omitempty\"` // Bits per sec\n+        SampleFrequency string          `json:\"sample_freq,omitempty\"`\n+        MaxWaiting      int             `json:\"max_waiting,omitempty\"`\n+        MaxAckPending   int             `json:\"max_ack_pending,omitempty\"`\n+        Heartbeat       time.Duration   `json:\"idle_heartbeat,omitempty\"`\n+        FlowControl     bool            `json:\"flow_control,omitempty\"`\n+        HeadersOnly     bool            `json:\"headers_only,omitempty\"`\n+\n+        // Pull based options.\n+        MaxRequestBatch   int           `json:\"max_batch,omitempty\"`\n+        MaxRequestExpires time.Duration `json:\"max_expires,omitempty\"`\n+\n+        // Push based consumers.\n+        DeliverSubject string `json:\"deliver_subject,omitempty\"`\n+        DeliverGroup   string `json:\"deliver_group,omitempty\"`\n+\n+        // Ephemeral inactivity threshold.\n+        InactiveThreshold time.Duration `json:\"inactive_threshold,omitempty\"`\n+\n+        // Don't add to general clients.\n+        Direct bool `json:\"direct,omitempty\"`\n }\n \n // SequenceInfo has both the consumer and the stream sequence and last activity.\n type SequenceInfo struct {\n-\tConsumer uint64     `json:\"consumer_seq\"`\n-\tStream   uint64     `json:\"stream_seq\"`\n-\tLast     *time.Time `json:\"last_active,omitempty\"`\n+        Consumer uint64     `json:\"consumer_seq\"`\n+        Stream   uint64     `json:\"stream_seq\"`\n+        Last     *time.Time `json:\"last_active,omitempty\"`\n }\n \n type CreateConsumerRequest struct {\n-\tStream string         `json:\"stream_name\"`\n-\tConfig ConsumerConfig `json:\"config\"`\n+        Stream string         `json:\"stream_name\"`\n+        Config ConsumerConfig `json:\"config\"`\n }\n \n // ConsumerNakOptions is for optional NAK values, e.g. delay.\n type ConsumerNakOptions struct {\n-\tDelay time.Duration `json:\"delay\"`\n+        Delay time.Duration `json:\"delay\"`\n }\n \n // DeliverPolicy determines how the consumer should select the first message to deliver.\n type DeliverPolicy int\n \n const (\n-\t// DeliverAll will be the default so can be omitted from the request.\n-\tDeliverAll DeliverPolicy = iota\n-\t// DeliverLast will start the consumer with the last sequence received.\n-\tDeliverLast\n-\t// DeliverNew will only deliver new messages that are sent after the consumer is created.\n-\tDeliverNew\n-\t// DeliverByStartSequence will look for a defined starting sequence to start.\n-\tDeliverByStartSequence\n-\t// DeliverByStartTime will select the first messsage with a timestamp >= to StartTime.\n-\tDeliverByStartTime\n-\t// DeliverLastPerSubject will start the consumer with the last message for all subjects received.\n-\tDeliverLastPerSubject\n+        // DeliverAll will be the default so can be omitted from the request.\n+        DeliverAll DeliverPolicy = iota\n+        // DeliverLast will start the consumer with the last sequence received.\n+        DeliverLast\n+        // DeliverNew will only deliver new messages that are sent after the consumer is created.\n+        DeliverNew\n+        // DeliverByStartSequence will look for a defined starting sequence to start.\n+        DeliverByStartSequence\n+        // DeliverByStartTime will select the first messsage with a timestamp >= to StartTime.\n+        DeliverByStartTime\n+        // DeliverLastPerSubject will start the consumer with the last message for all subjects received.\n+        DeliverLastPerSubject\n )\n \n func (dp DeliverPolicy) String() string {\n-\tswitch dp {\n-\tcase DeliverAll:\n-\t\treturn \"all\"\n-\tcase DeliverLast:\n-\t\treturn \"last\"\n-\tcase DeliverNew:\n-\t\treturn \"new\"\n-\tcase DeliverByStartSequence:\n-\t\treturn \"by_start_sequence\"\n-\tcase DeliverByStartTime:\n-\t\treturn \"by_start_time\"\n-\tcase DeliverLastPerSubject:\n-\t\treturn \"last_per_subject\"\n-\tdefault:\n-\t\treturn \"undefined\"\n-\t}\n+        switch dp {\n+        case DeliverAll:\n+                return \"all\"\n+        case DeliverLast:\n+                return \"last\"\n+        case DeliverNew:\n+                return \"new\"\n+        case DeliverByStartSequence:\n+                return \"by_start_sequence\"\n+        case DeliverByStartTime:\n+                return \"by_start_time\"\n+        case DeliverLastPerSubject:\n+                return \"last_per_subject\"\n+        default:\n+                return \"undefined\"\n+        }\n }\n \n // AckPolicy determines how the consumer should acknowledge delivered messages.\n type AckPolicy int\n \n const (\n-\t// AckNone requires no acks for delivered messages.\n-\tAckNone AckPolicy = iota\n-\t// AckAll when acking a sequence number, this implicitly acks all sequences below this one as well.\n-\tAckAll\n-\t// AckExplicit requires ack or nack for all messages.\n-\tAckExplicit\n+        // AckNone requires no acks for delivered messages.\n+        AckNone AckPolicy = iota\n+        // AckAll when acking a sequence number, this implicitly acks all sequences below this one as well.\n+        AckAll\n+        // AckExplicit requires ack or nack for all messages.\n+        AckExplicit\n )\n \n func (a AckPolicy) String() string {\n-\tswitch a {\n-\tcase AckNone:\n-\t\treturn \"none\"\n-\tcase AckAll:\n-\t\treturn \"all\"\n-\tdefault:\n-\t\treturn \"explicit\"\n-\t}\n+        switch a {\n+        case AckNone:\n+                return \"none\"\n+        case AckAll:\n+                return \"all\"\n+        default:\n+                return \"explicit\"\n+        }\n }\n \n // ReplayPolicy determines how the consumer should replay messages it already has queued in the stream.\n type ReplayPolicy int\n \n const (\n-\t// ReplayInstant will replay messages as fast as possible.\n-\tReplayInstant ReplayPolicy = iota\n-\t// ReplayOriginal will maintain the same timing as the messages were received.\n-\tReplayOriginal\n+        // ReplayInstant will replay messages as fast as possible.\n+        ReplayInstant ReplayPolicy = iota\n+        // ReplayOriginal will maintain the same timing as the messages were received.\n+        ReplayOriginal\n )\n \n func (r ReplayPolicy) String() string {\n-\tswitch r {\n-\tcase ReplayInstant:\n-\t\treturn \"instant\"\n-\tdefault:\n-\t\treturn \"original\"\n-\t}\n+        switch r {\n+        case ReplayInstant:\n+                return \"instant\"\n+        default:\n+                return \"original\"\n+        }\n }\n \n // OK\n@@ -182,1531 +182,1532 @@ const OK = \"+OK\"\n \n // Ack responses. Note that a nil or no payload is same as AckAck\n var (\n-\t// Ack\n-\tAckAck = []byte(\"+ACK\") // nil or no payload to ack subject also means ACK\n-\tAckOK  = []byte(OK)     // deprecated but +OK meant ack as well.\n-\n-\t// Nack\n-\tAckNak = []byte(\"-NAK\")\n-\t// Progress indicator\n-\tAckProgress = []byte(\"+WPI\")\n-\t// Ack + Deliver the next message(s).\n-\tAckNext = []byte(\"+NXT\")\n-\t// Terminate delivery of the message.\n-\tAckTerm = []byte(\"+TERM\")\n+        // Ack\n+        AckAck = []byte(\"+ACK\") // nil or no payload to ack subject also means ACK\n+        AckOK  = []byte(OK)     // deprecated but +OK meant ack as well.\n+\n+        // Nack\n+        AckNak = []byte(\"-NAK\")\n+        // Progress indicator\n+        AckProgress = []byte(\"+WPI\")\n+        // Ack + Deliver the next message(s).\n+        AckNext = []byte(\"+NXT\")\n+        // Terminate delivery of the message.\n+        AckTerm = []byte(\"+TERM\")\n )\n \n // Consumer is a jetstream consumer.\n type consumer struct {\n-\tmu                sync.RWMutex\n-\tjs                *jetStream\n-\tmset              *stream\n-\tacc               *Account\n-\tsrv               *Server\n-\tclient            *client\n-\tsysc              *client\n-\tsid               int\n-\tname              string\n-\tstream            string\n-\tsseq              uint64\n-\tdseq              uint64\n-\tadflr             uint64\n-\tasflr             uint64\n-\tsgap              uint64\n-\tlsgap             uint64\n-\tdsubj             string\n-\tqgroup            string\n-\tlss               *lastSeqSkipList\n-\trlimit            *rate.Limiter\n-\treqSub            *subscription\n-\tackSub            *subscription\n-\tackReplyT         string\n-\tackSubj           string\n-\tnextMsgSubj       string\n-\tmaxp              int\n-\tpblimit           int\n-\tmaxpb             int\n-\tpbytes            int\n-\tfcsz              int\n-\tfcid              string\n-\tfcSub             *subscription\n-\toutq              *jsOutQ\n-\tpending           map[uint64]*Pending\n-\tptmr              *time.Timer\n-\trdq               []uint64\n-\trdqi              map[uint64]struct{}\n-\trdc               map[uint64]uint64\n-\tmaxdc             uint64\n-\twaiting           *waitQueue\n-\tcfg               ConsumerConfig\n-\tstore             ConsumerStore\n-\tactive            bool\n-\treplay            bool\n-\tfilterWC          bool\n-\tdtmr              *time.Timer\n-\tgwdtmr            *time.Timer\n-\tdthresh           time.Duration\n-\tmch               chan struct{}\n-\tqch               chan struct{}\n-\tinch              chan bool\n-\tsfreq             int32\n-\tackEventT         string\n-\tdeliveryExcEventT string\n-\tcreated           time.Time\n-\tldt               time.Time\n-\tlat               time.Time\n-\tclosed            bool\n-\n-\t// Clustered.\n-\tca      *consumerAssignment\n-\tnode    RaftNode\n-\tinfoSub *subscription\n-\tlqsent  time.Time\n-\tprm     map[string]struct{}\n-\tprOk    bool\n-\n-\t// R>1 proposals\n-\tpch   chan struct{}\n-\tphead *proposal\n-\tptail *proposal\n+        mu                sync.RWMutex\n+        js                *jetStream\n+        mset              *stream\n+        acc               *Account\n+        srv               *Server\n+        client            *client\n+        sysc              *client\n+        sid               int\n+        name              string\n+        stream            string\n+        sseq              uint64\n+        dseq              uint64\n+        adflr             uint64\n+        asflr             uint64\n+        sgap              uint64\n+        lsgap             uint64\n+        dsubj             string\n+        qgroup            string\n+        lss               *lastSeqSkipList\n+        rlimit            *rate.Limiter\n+        reqSub            *subscription\n+        ackSub            *subscription\n+        ackReplyT         string\n+        ackSubj           string\n+        nextMsgSubj       string\n+        maxp              int\n+        pblimit           int\n+        maxpb             int\n+        pbytes            int\n+        fcsz              int\n+        fcid              string\n+        fcSub             *subscription\n+        outq              *jsOutQ\n+        pending           map[uint64]*Pending\n+        ptmr              *time.Timer\n+        rdq               []uint64\n+        rdqi              map[uint64]struct{}\n+        rdc               map[uint64]uint64\n+        maxdc             uint64\n+        waiting           *waitQueue\n+        cfg               ConsumerConfig\n+        store             ConsumerStore\n+        active            bool\n+        replay            bool\n+        filterWC          bool\n+        dtmr              *time.Timer\n+        gwdtmr            *time.Timer\n+        dthresh           time.Duration\n+        mch               chan struct{}\n+        qch               chan struct{}\n+        inch              chan bool\n+        sfreq             int32\n+        ackEventT         string\n+        deliveryExcEventT string\n+        created           time.Time\n+        ldt               time.Time\n+        lat               time.Time\n+        closed            bool\n+\n+        // Clustered.\n+        ca      *consumerAssignment\n+        node    RaftNode\n+        infoSub *subscription\n+        lqsent  time.Time\n+        prm     map[string]struct{}\n+        prOk    bool\n+\n+        // R>1 proposals\n+        pch   chan struct{}\n+        phead *proposal\n+        ptail *proposal\n }\n \n type proposal struct {\n-\tdata []byte\n-\tnext *proposal\n+        data []byte\n+        next *proposal\n }\n \n const (\n-\t// JsAckWaitDefault is the default AckWait, only applicable on explicit ack policy consumers.\n-\tJsAckWaitDefault = 30 * time.Second\n-\t// JsDeleteWaitTimeDefault is the default amount of time we will wait for non-durable\n-\t// consumers to be in an inactive state before deleting them.\n-\tJsDeleteWaitTimeDefault = 5 * time.Second\n-\t// JsFlowControlMaxPending specifies default pending bytes during flow control that can be\n-\t// outstanding.\n-\tJsFlowControlMaxPending = 1 * 1024 * 1024\n-\t// JsDefaultMaxAckPending is set for consumers with explicit ack that do not set the max ack pending.\n-\tJsDefaultMaxAckPending = 20_000\n+        // JsAckWaitDefault is the default AckWait, only applicable on explicit ack policy consumers.\n+        JsAckWaitDefault = 30 * time.Second\n+        // JsDeleteWaitTimeDefault is the default amount of time we will wait for non-durable\n+        // consumers to be in an inactive state before deleting them.\n+        JsDeleteWaitTimeDefault = 5 * time.Second\n+        // JsFlowControlMaxPending specifies default pending bytes during flow control that can be\n+        // outstanding.\n+        JsFlowControlMaxPending = 1 * 1024 * 1024\n+        // JsDefaultMaxAckPending is set for consumers with explicit ack that do not set the max ack pending.\n+        JsDefaultMaxAckPending = 20_000\n )\n \n // Helper function to set consumer config defaults from above.\n func setConsumerConfigDefaults(config *ConsumerConfig) {\n-\t// Set to default if not specified.\n-\tif config.DeliverSubject == _EMPTY_ && config.MaxWaiting == 0 {\n-\t\tconfig.MaxWaiting = JSWaitQueueDefaultMax\n-\t}\n-\t// Setup proper default for ack wait if we are in explicit ack mode.\n-\tif config.AckWait == 0 && (config.AckPolicy == AckExplicit || config.AckPolicy == AckAll) {\n-\t\tconfig.AckWait = JsAckWaitDefault\n-\t}\n-\t// Setup default of -1, meaning no limit for MaxDeliver.\n-\tif config.MaxDeliver == 0 {\n-\t\tconfig.MaxDeliver = -1\n-\t}\n-\t// If BackOff was specified that will override the AckWait and the MaxDeliver.\n-\tif len(config.BackOff) > 0 {\n-\t\tconfig.AckWait = config.BackOff[0]\n-\t}\n-\t// Set proper default for max ack pending if we are ack explicit and none has been set.\n-\tif (config.AckPolicy == AckExplicit || config.AckPolicy == AckAll) && config.MaxAckPending == 0 {\n-\t\tconfig.MaxAckPending = JsDefaultMaxAckPending\n-\t}\n+        // Set to default if not specified.\n+        if config.DeliverSubject == _EMPTY_ && config.MaxWaiting == 0 {\n+                config.MaxWaiting = JSWaitQueueDefaultMax\n+        }\n+        // Setup proper default for ack wait if we are in explicit ack mode.\n+        if config.AckWait == 0 && (config.AckPolicy == AckExplicit || config.AckPolicy == AckAll) {\n+                config.AckWait = JsAckWaitDefault\n+        }\n+        // Setup default of -1, meaning no limit for MaxDeliver.\n+        if config.MaxDeliver == 0 {\n+                config.MaxDeliver = -1\n+        }\n+        // If BackOff was specified that will override the AckWait and the MaxDeliver.\n+        if len(config.BackOff) > 0 {\n+                config.AckWait = config.BackOff[0]\n+        }\n+        // Set proper default for max ack pending if we are ack explicit and none has been set.\n+        if (config.AckPolicy == AckExplicit || config.AckPolicy == AckAll) && config.MaxAckPending == 0 {\n+                config.MaxAckPending = JsDefaultMaxAckPending\n+        }\n }\n \n func (mset *stream) addConsumer(config *ConsumerConfig) (*consumer, error) {\n-\treturn mset.addConsumerWithAssignment(config, _EMPTY_, nil)\n+        return mset.addConsumerWithAssignment(config, _EMPTY_, nil)\n }\n \n func (mset *stream) addConsumerWithAssignment(config *ConsumerConfig, oname string, ca *consumerAssignment) (*consumer, error) {\n-\tmset.mu.RLock()\n-\ts, jsa := mset.srv, mset.jsa\n-\tmset.mu.RUnlock()\n-\n-\t// If we do not have the consumer currently assigned to us in cluster mode we will proceed but warn.\n-\t// This can happen on startup with restored state where on meta replay we still do not have\n-\t// the assignment. Running in single server mode this always returns true.\n-\tif oname != _EMPTY_ && !jsa.consumerAssigned(mset.name(), oname) {\n-\t\ts.Debugf(\"Consumer %q > %q does not seem to be assigned to this server\", mset.name(), oname)\n-\t}\n-\n-\tif config == nil {\n-\t\treturn nil, NewJSConsumerConfigRequiredError()\n-\t}\n-\n-\t// Make sure we have sane defaults.\n-\tsetConsumerConfigDefaults(config)\n-\n-\t// Check if we have a BackOff defined that MaxDeliver is within range etc.\n-\tif lbo := len(config.BackOff); lbo > 0 && config.MaxDeliver <= lbo {\n-\t\treturn nil, NewJSConsumerMaxDeliverBackoffError()\n-\t}\n-\n-\tif len(config.Description) > JSMaxDescriptionLen {\n-\t\treturn nil, NewJSConsumerDescriptionTooLongError(JSMaxDescriptionLen)\n-\t}\n-\n-\tvar err error\n-\t// For now expect a literal subject if its not empty. Empty means work queue mode (pull mode).\n-\tif config.DeliverSubject != _EMPTY_ {\n-\t\tif !subjectIsLiteral(config.DeliverSubject) {\n-\t\t\treturn nil, NewJSConsumerDeliverToWildcardsError()\n-\t\t}\n-\t\tif !IsValidSubject(config.DeliverSubject) {\n-\t\t\treturn nil, NewJSConsumerInvalidDeliverSubjectError()\n-\t\t}\n-\t\tif mset.deliveryFormsCycle(config.DeliverSubject) {\n-\t\t\treturn nil, NewJSConsumerDeliverCycleError()\n-\t\t}\n-\t\tif config.MaxWaiting != 0 {\n-\t\t\treturn nil, NewJSConsumerPushMaxWaitingError()\n-\t\t}\n-\t\tif config.MaxAckPending > 0 && config.AckPolicy == AckNone {\n-\t\t\treturn nil, NewJSConsumerMaxPendingAckPolicyRequiredError()\n-\t\t}\n-\t\tif config.Heartbeat > 0 && config.Heartbeat < 100*time.Millisecond {\n-\t\t\treturn nil, NewJSConsumerSmallHeartbeatError()\n-\t\t}\n-\t} else {\n-\t\t// Pull mode / work queue mode require explicit ack.\n-\t\tif config.AckPolicy == AckNone {\n-\t\t\treturn nil, NewJSConsumerPullRequiresAckError()\n-\t\t}\n-\t\tif config.RateLimit > 0 {\n-\t\t\treturn nil, NewJSConsumerPullWithRateLimitError()\n-\t\t}\n-\t\tif config.MaxWaiting < 0 {\n-\t\t\treturn nil, NewJSConsumerMaxWaitingNegativeError()\n-\t\t}\n-\t\tif config.Heartbeat > 0 {\n-\t\t\treturn nil, NewJSConsumerHBRequiresPushError()\n-\t\t}\n-\t\tif config.FlowControl {\n-\t\t\treturn nil, NewJSConsumerFCRequiresPushError()\n-\t\t}\n-\t\tif config.MaxRequestBatch < 0 {\n-\t\t\treturn nil, NewJSConsumerMaxRequestBatchNegativeError()\n-\t\t}\n-\t\tif config.MaxRequestExpires != 0 && config.MaxRequestExpires < time.Millisecond {\n-\t\t\treturn nil, NewJSConsumerMaxRequestExpiresToSmallError()\n-\t\t}\n-\t}\n-\n-\t// Direct need to be non-mapped ephemerals.\n-\tif config.Direct {\n-\t\tif config.DeliverSubject == _EMPTY_ {\n-\t\t\treturn nil, NewJSConsumerDirectRequiresPushError()\n-\t\t}\n-\t\tif isDurableConsumer(config) {\n-\t\t\treturn nil, NewJSConsumerDirectRequiresEphemeralError()\n-\t\t}\n-\t\tif ca != nil {\n-\t\t\treturn nil, NewJSConsumerOnMappedError()\n-\t\t}\n-\t}\n-\n-\t// As best we can make sure the filtered subject is valid.\n-\tif config.FilterSubject != _EMPTY_ {\n-\t\tsubjects, hasExt := mset.allSubjects()\n-\t\tif !validFilteredSubject(config.FilterSubject, subjects) && !hasExt {\n-\t\t\treturn nil, NewJSConsumerFilterNotSubsetError()\n-\t\t}\n-\t}\n-\n-\t// Helper function to formulate similar errors.\n-\tbadStart := func(dp, start string) error {\n-\t\treturn fmt.Errorf(\"consumer delivery policy is deliver %s, but optional start %s is also set\", dp, start)\n-\t}\n-\tnotSet := func(dp, notSet string) error {\n-\t\treturn fmt.Errorf(\"consumer delivery policy is deliver %s, but optional %s is not set\", dp, notSet)\n-\t}\n-\n-\t// Check on start position conflicts.\n-\tswitch config.DeliverPolicy {\n-\tcase DeliverAll:\n-\t\tif config.OptStartSeq > 0 {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"all\", \"sequence\"))\n-\t\t}\n-\t\tif config.OptStartTime != nil {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"all\", \"time\"))\n-\t\t}\n-\tcase DeliverLast:\n-\t\tif config.OptStartSeq > 0 {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"last\", \"sequence\"))\n-\t\t}\n-\t\tif config.OptStartTime != nil {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"last\", \"time\"))\n-\t\t}\n-\tcase DeliverLastPerSubject:\n-\t\tif config.OptStartSeq > 0 {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"last per subject\", \"sequence\"))\n-\t\t}\n-\t\tif config.OptStartTime != nil {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"last per subject\", \"time\"))\n-\t\t}\n-\t\tif config.FilterSubject == _EMPTY_ {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(notSet(\"last per subject\", \"filter subject\"))\n-\t\t}\n-\tcase DeliverNew:\n-\t\tif config.OptStartSeq > 0 {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"new\", \"sequence\"))\n-\t\t}\n-\t\tif config.OptStartTime != nil {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"new\", \"time\"))\n-\t\t}\n-\tcase DeliverByStartSequence:\n-\t\tif config.OptStartSeq == 0 {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(notSet(\"by start sequence\", \"start sequence\"))\n-\t\t}\n-\t\tif config.OptStartTime != nil {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"by start sequence\", \"time\"))\n-\t\t}\n-\tcase DeliverByStartTime:\n-\t\tif config.OptStartTime == nil {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(notSet(\"by start time\", \"start time\"))\n-\t\t}\n-\t\tif config.OptStartSeq != 0 {\n-\t\t\treturn nil, NewJSConsumerInvalidPolicyError(badStart(\"by start time\", \"start sequence\"))\n-\t\t}\n-\t}\n-\n-\tsampleFreq := 0\n-\tif config.SampleFrequency != _EMPTY_ {\n-\t\ts := strings.TrimSuffix(config.SampleFrequency, \"%\")\n-\t\tsampleFreq, err = strconv.Atoi(s)\n-\t\tif err != nil {\n-\t\t\treturn nil, NewJSConsumerInvalidSamplingError(err)\n-\t\t}\n-\t}\n-\n-\t// Grab the client, account and server reference.\n-\tc := mset.client\n-\tif c == nil {\n-\t\treturn nil, NewJSStreamInvalidError()\n-\t}\n-\tc.mu.Lock()\n-\ts, a := c.srv, c.acc\n-\tc.mu.Unlock()\n-\n-\t// Hold mset lock here.\n-\tmset.mu.Lock()\n-\tif mset.client == nil || mset.store == nil {\n-\t\tmset.mu.Unlock()\n-\t\treturn nil, errors.New(\"invalid stream\")\n-\t}\n-\n-\t// If this one is durable and already exists, we let that be ok as long as only updating what should be allowed.\n-\tif isDurableConsumer(config) {\n-\t\tif eo, ok := mset.consumers[config.Durable]; ok {\n-\t\t\tmset.mu.Unlock()\n-\t\t\terr := eo.updateConfig(config)\n-\t\t\tif err == nil {\n-\t\t\t\treturn eo, nil\n-\t\t\t}\n-\t\t\treturn nil, NewJSConsumerCreateError(err, Unless(err))\n-\t\t}\n-\t}\n-\n-\t// Check for any limits, if the config for the consumer sets a limit we check against that\n-\t// but if not we use the value from account limits, if account limits is more restrictive\n-\t// than stream config we prefer the account limits to handle cases where account limits are\n-\t// updated during the lifecycle of the stream\n-\tmaxc := mset.cfg.MaxConsumers\n-\tif maxc <= 0 || (mset.jsa.limits.MaxConsumers > 0 && mset.jsa.limits.MaxConsumers < maxc) {\n-\t\tmaxc = mset.jsa.limits.MaxConsumers\n-\t}\n-\tif maxc > 0 && mset.numPublicConsumers() >= maxc {\n-\t\tmset.mu.Unlock()\n-\t\treturn nil, NewJSMaximumConsumersLimitError()\n-\t}\n-\n-\t// Check on stream type conflicts with WorkQueues.\n-\tif mset.cfg.Retention == WorkQueuePolicy && !config.Direct {\n-\t\t// Force explicit acks here.\n-\t\tif config.AckPolicy != AckExplicit {\n-\t\t\tmset.mu.Unlock()\n-\t\t\treturn nil, NewJSConsumerWQRequiresExplicitAckError()\n-\t\t}\n-\n-\t\tif len(mset.consumers) > 0 {\n-\t\t\tif config.FilterSubject == _EMPTY_ {\n-\t\t\t\tmset.mu.Unlock()\n-\t\t\t\treturn nil, NewJSConsumerWQMultipleUnfilteredError()\n-\t\t\t} else if !mset.partitionUnique(config.FilterSubject) {\n-\t\t\t\t// We have a partition but it is not unique amongst the others.\n-\t\t\t\tmset.mu.Unlock()\n-\t\t\t\treturn nil, NewJSConsumerWQConsumerNotUniqueError()\n-\t\t\t}\n-\t\t}\n-\t\tif config.DeliverPolicy != DeliverAll {\n-\t\t\tmset.mu.Unlock()\n-\t\t\treturn nil, NewJSConsumerWQConsumerNotDeliverAllError()\n-\t\t}\n-\t}\n-\n-\t// Set name, which will be durable name if set, otherwise we create one at random.\n-\to := &consumer{\n-\t\tmset:    mset,\n-\t\tjs:      s.getJetStream(),\n-\t\tacc:     a,\n-\t\tsrv:     s,\n-\t\tclient:  s.createInternalJetStreamClient(),\n-\t\tsysc:    s.createInternalJetStreamClient(),\n-\t\tcfg:     *config,\n-\t\tdsubj:   config.DeliverSubject,\n-\t\toutq:    mset.outq,\n-\t\tactive:  true,\n-\t\tqch:     make(chan struct{}),\n-\t\tmch:     make(chan struct{}, 1),\n-\t\tsfreq:   int32(sampleFreq),\n-\t\tmaxdc:   uint64(config.MaxDeliver),\n-\t\tmaxp:    config.MaxAckPending,\n-\t\tcreated: time.Now().UTC(),\n-\t}\n-\n-\t// Bind internal client to the user account.\n-\to.client.registerWithAccount(a)\n-\t// Bind to the system account.\n-\to.sysc.registerWithAccount(s.SystemAccount())\n-\n-\tif isDurableConsumer(config) {\n-\t\tif len(config.Durable) > JSMaxNameLen {\n-\t\t\tmset.mu.Unlock()\n-\t\t\to.deleteWithoutAdvisory()\n-\t\t\treturn nil, NewJSConsumerNameTooLongError(JSMaxNameLen)\n-\t\t}\n-\t\to.name = config.Durable\n-\t} else if oname != _EMPTY_ {\n-\t\to.name = oname\n-\t} else {\n-\t\tfor {\n-\t\t\to.name = createConsumerName()\n-\t\t\tif _, ok := mset.consumers[o.name]; !ok {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t}\n-\t// Create our request waiting queue.\n-\tif o.isPullMode() {\n-\t\to.waiting = newWaitQueue(config.MaxWaiting)\n-\t}\n-\n-\t// Check if we have  filtered subject that is a wildcard.\n-\tif config.FilterSubject != _EMPTY_ && subjectHasWildcard(config.FilterSubject) {\n-\t\to.filterWC = true\n-\t}\n-\n-\t// already under lock, mset.Name() would deadlock\n-\to.stream = mset.cfg.Name\n-\to.ackEventT = JSMetricConsumerAckPre + \".\" + o.stream + \".\" + o.name\n-\to.deliveryExcEventT = JSAdvisoryConsumerMaxDeliveryExceedPre + \".\" + o.stream + \".\" + o.name\n-\n-\tif !isValidName(o.name) {\n-\t\tmset.mu.Unlock()\n-\t\to.deleteWithoutAdvisory()\n-\t\treturn nil, NewJSConsumerBadDurableNameError()\n-\t}\n-\n-\t// Select starting sequence number\n-\to.selectStartingSeqNo()\n-\n-\tif !config.Direct {\n-\t\tstore, err := mset.store.ConsumerStore(o.name, config)\n-\t\tif err != nil {\n-\t\t\tmset.mu.Unlock()\n-\t\t\to.deleteWithoutAdvisory()\n-\t\t\treturn nil, NewJSConsumerStoreFailedError(err)\n-\t\t}\n-\t\to.store = store\n-\t}\n-\n-\t// Now register with mset and create the ack subscription.\n-\t// Check if we already have this one registered.\n-\tif eo, ok := mset.consumers[o.name]; ok {\n-\t\tmset.mu.Unlock()\n-\t\tif !o.isDurable() || !o.isPushMode() {\n-\t\t\to.name = _EMPTY_ // Prevent removal since same name.\n-\t\t\to.deleteWithoutAdvisory()\n-\t\t\treturn nil, NewJSConsumerNameExistError()\n-\t\t}\n-\t\t// If we are here we have already registered this durable. If it is still active that is an error.\n-\t\tif eo.isActive() {\n-\t\t\to.name = _EMPTY_ // Prevent removal since same name.\n-\t\t\to.deleteWithoutAdvisory()\n-\t\t\treturn nil, NewJSConsumerExistingActiveError()\n-\t\t}\n-\t\t// Since we are here this means we have a potentially new durable so we should update here.\n-\t\t// Check that configs are the same.\n-\t\tif !configsEqualSansDelivery(o.cfg, eo.cfg) {\n-\t\t\to.name = _EMPTY_ // Prevent removal since same name.\n-\t\t\to.deleteWithoutAdvisory()\n-\t\t\treturn nil, NewJSConsumerReplacementWithDifferentNameError()\n-\t\t}\n-\t\t// Once we are here we have a replacement push-based durable.\n-\t\teo.updateDeliverSubject(o.cfg.DeliverSubject)\n-\t\treturn eo, nil\n-\t}\n-\n-\t// Set up the ack subscription for this consumer. Will use wildcard for all acks.\n-\t// We will remember the template to generate replies with sequence numbers and use\n-\t// that to scanf them back in.\n-\tmn := mset.cfg.Name\n-\tpre := fmt.Sprintf(jsAckT, mn, o.name)\n-\to.ackReplyT = fmt.Sprintf(\"%s.%%d.%%d.%%d.%%d.%%d\", pre)\n-\to.ackSubj = fmt.Sprintf(\"%s.*.*.*.*.*\", pre)\n-\to.nextMsgSubj = fmt.Sprintf(JSApiRequestNextT, mn, o.name)\n-\n-\t// If not durable determine the inactive threshold.\n-\tif !o.isDurable() {\n-\t\tif o.cfg.InactiveThreshold != 0 {\n-\t\t\to.dthresh = o.cfg.InactiveThreshold\n-\t\t} else {\n-\t\t\t// Add in 1 sec of jitter above and beyond the default of 5s.\n-\t\t\to.dthresh = JsDeleteWaitTimeDefault + time.Duration(rand.Int63n(1000))*time.Millisecond\n-\t\t}\n-\t}\n-\n-\tif o.isPushMode() {\n-\t\tif !o.isDurable() {\n-\t\t\t// Check if we are not durable that the delivery subject has interest.\n-\t\t\t// Check in place here for interest. Will setup properly in setLeader.\n-\t\t\tr := o.acc.sl.Match(o.cfg.DeliverSubject)\n-\t\t\tif !o.hasDeliveryInterest(len(r.psubs)+len(r.qsubs) > 0) {\n-\t\t\t\t// Let the interest come to us eventually, but setup delete timer.\n-\t\t\t\to.updateDeliveryInterest(false)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Set our ca.\n-\tif ca != nil {\n-\t\to.setConsumerAssignment(ca)\n-\t}\n-\n-\t// Check if we have a rate limit set.\n-\tif config.RateLimit != 0 {\n-\t\to.setRateLimit(config.RateLimit)\n-\t}\n-\n-\tmset.setConsumer(o)\n-\tmset.mu.Unlock()\n-\n-\tif config.Direct || (!s.JetStreamIsClustered() && s.standAloneMode()) {\n-\t\to.setLeader(true)\n-\t}\n-\n-\t// This is always true in single server mode.\n-\tif o.isLeader() {\n-\t\t// Send advisory.\n-\t\tvar suppress bool\n-\t\tif !s.standAloneMode() && ca == nil {\n-\t\t\tsuppress = true\n-\t\t} else if ca != nil {\n-\t\t\tsuppress = ca.responded\n-\t\t}\n-\t\tif !suppress {\n-\t\t\to.sendCreateAdvisory()\n-\t\t}\n-\t}\n-\n-\treturn o, nil\n+        mset.mu.RLock()\n+        s, jsa := mset.srv, mset.jsa\n+        mset.mu.RUnlock()\n+\n+        // If we do not have the consumer currently assigned to us in cluster mode we will proceed but warn.\n+        // This can happen on startup with restored state where on meta replay we still do not have\n+        // the assignment. Running in single server mode this always returns true.\n+        if oname != _EMPTY_ && !jsa.consumerAssigned(mset.name(), oname) {\n+                s.Debugf(\"Consumer %q > %q does not seem to be assigned to this server\", mset.name(), oname)\n+        }\n+\n+        if config == nil {\n+                return nil, NewJSConsumerConfigRequiredError()\n+        }\n+\n+        // Make sure we have sane defaults.\n+        setConsumerConfigDefaults(config)\n+\n+        // Check if we have a BackOff defined that MaxDeliver is within range etc.\n+        if lbo := len(config.BackOff); lbo > 0 && config.MaxDeliver <= lbo {\n+                return nil, NewJSConsumerMaxDeliverBackoffError()\n+        }\n+\n+        if len(config.Description) > JSMaxDescriptionLen {\n+                return nil, NewJSConsumerDescriptionTooLongError(JSMaxDescriptionLen)\n+        }\n+\n+        var err error\n+        // For now expect a literal subject if its not empty. Empty means work queue mode (pull mode).\n+        if config.DeliverSubject != _EMPTY_ {\n+                if !subjectIsLiteral(config.DeliverSubject) {\n+                        return nil, NewJSConsumerDeliverToWildcardsError()\n+                }\n+                if !IsValidSubject(config.DeliverSubject) {\n+                        return nil, NewJSConsumerInvalidDeliverSubjectError()\n+                }\n+                if mset.deliveryFormsCycle(config.DeliverSubject) {\n+                        return nil, NewJSConsumerDeliverCycleError()\n+                }\n+                if config.MaxWaiting != 0 {\n+                        return nil, NewJSConsumerPushMaxWaitingError()\n+                }\n+                if config.MaxAckPending > 0 && config.AckPolicy == AckNone {\n+                        return nil, NewJSConsumerMaxPendingAckPolicyRequiredError()\n+                }\n+                if config.Heartbeat > 0 && config.Heartbeat < 100*time.Millisecond {\n+                        return nil, NewJSConsumerSmallHeartbeatError()\n+                }\n+        } else {\n+                // Pull mode / work queue mode require explicit ack.\n+                if config.AckPolicy == AckNone {\n+                        return nil, NewJSConsumerPullRequiresAckError()\n+                }\n+                if config.RateLimit > 0 {\n+                        return nil, NewJSConsumerPullWithRateLimitError()\n+                }\n+                if config.MaxWaiting < 0 {\n+                        return nil, NewJSConsumerMaxWaitingNegativeError()\n+                }\n+                if config.Heartbeat > 0 {\n+                        return nil, NewJSConsumerHBRequiresPushError()\n+                }\n+                if config.FlowControl {\n+                        return nil, NewJSConsumerFCRequiresPushError()\n+                }\n+                if config.MaxRequestBatch < 0 {\n+                        return nil, NewJSConsumerMaxRequestBatchNegativeError()\n+                }\n+                if config.MaxRequestExpires != 0 && config.MaxRequestExpires < time.Millisecond {\n+                        return nil, NewJSConsumerMaxRequestExpiresToSmallError()\n+                }\n+        }\n+\n+        // Direct need to be non-mapped ephemerals.\n+        if config.Direct {\n+                if config.DeliverSubject == _EMPTY_ {\n+                        return nil, NewJSConsumerDirectRequiresPushError()\n+                }\n+                if isDurableConsumer(config) {\n+                        return nil, NewJSConsumerDirectRequiresEphemeralError()\n+                }\n+                if ca != nil {\n+                        return nil, NewJSConsumerOnMappedError()\n+                }\n+        }\n+\n+        // As best we can make sure the filtered subject is valid.\n+        if config.FilterSubject != _EMPTY_ {\n+                subjects, hasExt := mset.allSubjects()\n+                if !validFilteredSubject(config.FilterSubject, subjects) && !hasExt {\n+                        return nil, NewJSConsumerFilterNotSubsetError()\n+                }\n+        }\n+\n+        // Helper function to formulate similar errors.\n+        badStart := func(dp, start string) error {\n+                return fmt.Errorf(\"consumer delivery policy is deliver %s, but optional start %s is also set\", dp, start)\n+        }\n+        notSet := func(dp, notSet string) error {\n+                return fmt.Errorf(\"consumer delivery policy is deliver %s, but optional %s is not set\", dp, notSet)\n+        }\n+\n+        // Check on start position conflicts.\n+        switch config.DeliverPolicy {\n+        case DeliverAll:\n+                if config.OptStartSeq > 0 {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"all\", \"sequence\"))\n+                }\n+                if config.OptStartTime != nil {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"all\", \"time\"))\n+                }\n+        case DeliverLast:\n+                if config.OptStartSeq > 0 {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"last\", \"sequence\"))\n+                }\n+                if config.OptStartTime != nil {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"last\", \"time\"))\n+                }\n+        case DeliverLastPerSubject:\n+                if config.OptStartSeq > 0 {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"last per subject\", \"sequence\"))\n+                }\n+                if config.OptStartTime != nil {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"last per subject\", \"time\"))\n+                }\n+                if config.FilterSubject == _EMPTY_ {\n+                        return nil, NewJSConsumerInvalidPolicyError(notSet(\"last per subject\", \"filter subject\"))\n+                }\n+        case DeliverNew:\n+                if config.OptStartSeq > 0 {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"new\", \"sequence\"))\n+                }\n+                if config.OptStartTime != nil {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"new\", \"time\"))\n+                }\n+        case DeliverByStartSequence:\n+                if config.OptStartSeq == 0 {\n+                        return nil, NewJSConsumerInvalidPolicyError(notSet(\"by start sequence\", \"start sequence\"))\n+                }\n+                if config.OptStartTime != nil {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"by start sequence\", \"time\"))\n+                }\n+        case DeliverByStartTime:\n+                if config.OptStartTime == nil {\n+                        return nil, NewJSConsumerInvalidPolicyError(notSet(\"by start time\", \"start time\"))\n+                }\n+                if config.OptStartSeq != 0 {\n+                        return nil, NewJSConsumerInvalidPolicyError(badStart(\"by start time\", \"start sequence\"))\n+                }\n+        }\n+\n+        sampleFreq := 0\n+        if config.SampleFrequency != _EMPTY_ {\n+                s := strings.TrimSuffix(config.SampleFrequency, \"%\")\n+                sampleFreq, err = strconv.Atoi(s)\n+                if err != nil {\n+                        return nil, NewJSConsumerInvalidSamplingError(err)\n+                }\n+        }\n+\n+        // Grab the client, account and server reference.\n+        c := mset.client\n+        if c == nil {\n+                return nil, NewJSStreamInvalidError()\n+        }\n+        c.mu.Lock()\n+        s, a := c.srv, c.acc\n+        c.mu.Unlock()\n+\n+        // Hold mset lock here.\n+        mset.mu.Lock()\n+        if mset.client == nil || mset.store == nil {\n+                mset.mu.Unlock()\n+                return nil, errors.New(\"invalid stream\")\n+        }\n+\n+        // If this one is durable and already exists, we let that be ok as long as only updating what should be allowed.\n+        if isDurableConsumer(config) {\n+                if eo, ok := mset.consumers[config.Durable]; ok {\n+                        mset.mu.Unlock()\n+                        err := eo.updateConfig(config)\n+                        if err == nil {\n+                                return eo, nil\n+                        }\n+                        return nil, NewJSConsumerCreateError(err, Unless(err))\n+                }\n+        }\n+\n+        // Check for any limits, if the config for the consumer sets a limit we check against that\n+        // but if not we use the value from account limits, if account limits is more restrictive\n+        // than stream config we prefer the account limits to handle cases where account limits are\n+        // updated during the lifecycle of the stream\n+        maxc := mset.cfg.MaxConsumers\n+        if maxc <= 0 || (mset.jsa.limits.MaxConsumers > 0 && mset.jsa.limits.MaxConsumers < maxc) {\n+                maxc = mset.jsa.limits.MaxConsumers\n+        }\n+        if maxc > 0 && mset.numPublicConsumers() >= maxc {\n+                mset.mu.Unlock()\n+                return nil, NewJSMaximumConsumersLimitError()\n+        }\n+\n+        // Check on stream type conflicts with WorkQueues.\n+        if mset.cfg.Retention == WorkQueuePolicy && !config.Direct {\n+                // Force explicit acks here.\n+                if config.AckPolicy != AckExplicit {\n+                        mset.mu.Unlock()\n+                        return nil, NewJSConsumerWQRequiresExplicitAckError()\n+                }\n+\n+                if len(mset.consumers) > 0 {\n+                        if config.FilterSubject == _EMPTY_ {\n+                                mset.mu.Unlock()\n+                                return nil, NewJSConsumerWQMultipleUnfilteredError()\n+                        } else if !mset.partitionUnique(config.FilterSubject) {\n+                                // We have a partition but it is not unique amongst the others.\n+                                mset.mu.Unlock()\n+                                return nil, NewJSConsumerWQConsumerNotUniqueError()\n+                        }\n+                }\n+                if config.DeliverPolicy != DeliverAll {\n+                        mset.mu.Unlock()\n+                        return nil, NewJSConsumerWQConsumerNotDeliverAllError()\n+                }\n+        }\n+\n+        // Set name, which will be durable name if set, otherwise we create one at random.\n+        o := &consumer{\n+                mset:    mset,\n+                js:      s.getJetStream(),\n+                acc:     a,\n+                srv:     s,\n+                client:  s.createInternalJetStreamClient(),\n+                sysc:    s.createInternalJetStreamClient(),\n+                cfg:     *config,\n+                dsubj:   config.DeliverSubject,\n+                outq:    mset.outq,\n+                active:  true,\n+                qch:     make(chan struct{}),\n+                mch:     make(chan struct{}, 1),\n+                sfreq:   int32(sampleFreq),\n+                maxdc:   uint64(config.MaxDeliver),\n+                maxp:    config.MaxAckPending,\n+                created: time.Now().UTC(),\n+        }\n+\n+        // Bind internal client to the user account.\n+        o.client.registerWithAccount(a)\n+        // Bind to the system account.\n+        o.sysc.registerWithAccount(s.SystemAccount())\n+\n+        if isDurableConsumer(config) {\n+                if len(config.Durable) > JSMaxNameLen {\n+                        mset.mu.Unlock()\n+                        o.deleteWithoutAdvisory()\n+                        return nil, NewJSConsumerNameTooLongError(JSMaxNameLen)\n+                }\n+                o.name = config.Durable\n+        } else if oname != _EMPTY_ {\n+                o.name = oname\n+        } else {\n+                for {\n+                        o.name = createConsumerName()\n+                        if _, ok := mset.consumers[o.name]; !ok {\n+                                break\n+                        }\n+                }\n+        }\n+        // Create our request waiting queue.\n+        if o.isPullMode() {\n+                o.waiting = newWaitQueue(config.MaxWaiting)\n+        }\n+\n+        // Check if we have  filtered subject that is a wildcard.\n+        if config.FilterSubject != _EMPTY_ && subjectHasWildcard(config.FilterSubject) {\n+                o.filterWC = true\n+        }\n+\n+        // already under lock, mset.Name() would deadlock\n+        o.stream = mset.cfg.Name\n+        o.ackEventT = JSMetricConsumerAckPre + \".\" + o.stream + \".\" + o.name\n+        o.deliveryExcEventT = JSAdvisoryConsumerMaxDeliveryExceedPre + \".\" + o.stream + \".\" + o.name\n+\n+        if !isValidName(o.name) {\n+                mset.mu.Unlock()\n+                o.deleteWithoutAdvisory()\n+                return nil, NewJSConsumerBadDurableNameError()\n+        }\n+\n+        // Select starting sequence number\n+        o.selectStartingSeqNo()\n+\n+        if !config.Direct {\n+                store, err := mset.store.ConsumerStore(o.name, config)\n+                if err != nil {\n+                        mset.mu.Unlock()\n+                        o.deleteWithoutAdvisory()\n+                        return nil, NewJSConsumerStoreFailedError(err)\n+                }\n+                o.store = store\n+        }\n+\n+        // Now register with mset and create the ack subscription.\n+        // Check if we already have this one registered.\n+        if eo, ok := mset.consumers[o.name]; ok {\n+                mset.mu.Unlock()\n+                if !o.isDurable() || !o.isPushMode() {\n+                        o.name = _EMPTY_ // Prevent removal since same name.\n+                        o.deleteWithoutAdvisory()\n+                        return nil, NewJSConsumerNameExistError()\n+                }\n+                // If we are here we have already registered this durable. If it is still active that is an error.\n+                if eo.isActive() {\n+                        o.name = _EMPTY_ // Prevent removal since same name.\n+                        o.deleteWithoutAdvisory()\n+                        return nil, NewJSConsumerExistingActiveError()\n+                }\n+                // Since we are here this means we have a potentially new durable so we should update here.\n+                // Check that configs are the same.\n+                if !configsEqualSansDelivery(o.cfg, eo.cfg) {\n+                        o.name = _EMPTY_ // Prevent removal since same name.\n+                        o.deleteWithoutAdvisory()\n+                        return nil, NewJSConsumerReplacementWithDifferentNameError()\n+                }\n+                // Once we are here we have a replacement push-based durable.\n+                eo.updateDeliverSubject(o.cfg.DeliverSubject)\n+                return eo, nil\n+        }\n+\n+        // Set up the ack subscription for this consumer. Will use wildcard for all acks.\n+        // We will remember the template to generate replies with sequence numbers and use\n+        // that to scanf them back in.\n+        mn := mset.cfg.Name\n+        pre := fmt.Sprintf(jsAckT, mn, o.name)\n+        o.ackReplyT = fmt.Sprintf(\"%s.%%d.%%d.%%d.%%d.%%d\", pre)\n+        o.ackSubj = fmt.Sprintf(\"%s.*.*.*.*.*\", pre)\n+        o.nextMsgSubj = fmt.Sprintf(JSApiRequestNextT, mn, o.name)\n+\n+        // If not durable determine the inactive threshold.\n+        if !o.isDurable() {\n+                if o.cfg.InactiveThreshold != 0 {\n+                        o.dthresh = o.cfg.InactiveThreshold\n+                } else {\n+                        // Add in 1 sec of jitter above and beyond the default of 5s.\n+                        o.dthresh = JsDeleteWaitTimeDefault + time.Duration(rand.Int63n(1000))*time.Millisecond\n+                }\n+        }\n+\n+        if o.isPushMode() {\n+                if !o.isDurable() {\n+                        // Check if we are not durable that the delivery subject has interest.\n+                        // Check in place here for interest. Will setup properly in setLeader.\n+                        r := o.acc.sl.Match(o.cfg.DeliverSubject)\n+                        if !o.hasDeliveryInterest(len(r.psubs)+len(r.qsubs) > 0) {\n+                                // Let the interest come to us eventually, but setup delete timer.\n+                                o.updateDeliveryInterest(false)\n+                        }\n+                }\n+        }\n+\n+        // Set our ca.\n+        if ca != nil {\n+                o.setConsumerAssignment(ca)\n+        }\n+\n+        // Check if we have a rate limit set.\n+        if config.RateLimit != 0 {\n+                o.setRateLimit(config.RateLimit)\n+        }\n+\n+        mset.setConsumer(o)\n+        mset.mu.Unlock()\n+\n+        if config.Direct || (!s.JetStreamIsClustered() && s.standAloneMode()) {\n+                o.setLeader(true)\n+        }\n+\n+        // This is always true in single server mode.\n+        if o.isLeader() {\n+                // Send advisory.\n+                var suppress bool\n+                if !s.standAloneMode() && ca == nil {\n+                        suppress = true\n+                } else if ca != nil {\n+                        suppress = ca.responded\n+                }\n+                if !suppress {\n+                        o.sendCreateAdvisory()\n+                }\n+        }\n+\n+        return o, nil\n }\n \n func (o *consumer) consumerAssignment() *consumerAssignment {\n-\to.mu.RLock()\n-\tdefer o.mu.RUnlock()\n-\treturn o.ca\n+        o.mu.RLock()\n+        defer o.mu.RUnlock()\n+        return o.ca\n }\n \n func (o *consumer) setConsumerAssignment(ca *consumerAssignment) {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\to.ca = ca\n-\t// Set our node.\n-\tif ca != nil {\n-\t\to.node = ca.Group.node\n-\t}\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+        o.ca = ca\n+        // Set our node.\n+        if ca != nil {\n+                o.node = ca.Group.node\n+        }\n }\n \n // checkQueueInterest will check on our interest's queue group status.\n // Lock should be held.\n func (o *consumer) checkQueueInterest() {\n-\tif !o.active || o.cfg.DeliverSubject == _EMPTY_ {\n-\t\treturn\n-\t}\n-\tsubj := o.dsubj\n-\tif subj == _EMPTY_ {\n-\t\tsubj = o.cfg.DeliverSubject\n-\t}\n-\n-\tif rr := o.acc.sl.Match(subj); len(rr.qsubs) > 0 {\n-\t\t// Just grab first\n-\t\tif qsubs := rr.qsubs[0]; len(qsubs) > 0 {\n-\t\t\tif sub := rr.qsubs[0][0]; len(sub.queue) > 0 {\n-\t\t\t\to.qgroup = string(sub.queue)\n-\t\t\t}\n-\t\t}\n-\t}\n+        if !o.active || o.cfg.DeliverSubject == _EMPTY_ {\n+                return\n+        }\n+        subj := o.dsubj\n+        if subj == _EMPTY_ {\n+                subj = o.cfg.DeliverSubject\n+        }\n+\n+        if rr := o.acc.sl.Match(subj); len(rr.qsubs) > 0 {\n+                // Just grab first\n+                if qsubs := rr.qsubs[0]; len(qsubs) > 0 {\n+                        if sub := rr.qsubs[0][0]; len(sub.queue) > 0 {\n+                                o.qgroup = string(sub.queue)\n+                        }\n+                }\n+        }\n }\n \n // Lock should be held.\n func (o *consumer) isLeader() bool {\n-\tif o.node != nil {\n-\t\treturn o.node.Leader()\n-\t}\n-\treturn true\n+        if o.node != nil {\n+                return o.node.Leader()\n+        }\n+        return true\n }\n \n func (o *consumer) setLeader(isLeader bool) {\n-\to.mu.RLock()\n-\tmset := o.mset\n-\tisRunning := o.ackSub != nil\n-\to.mu.RUnlock()\n-\n-\t// If we are here we have a change in leader status.\n-\tif isLeader {\n-\t\tif mset == nil || isRunning {\n-\t\t\treturn\n-\t\t}\n-\n-\t\tmset.mu.RLock()\n-\t\ts, jsa, stream := mset.srv, mset.jsa, mset.cfg.Name\n-\t\tmset.mu.RUnlock()\n-\n-\t\to.mu.Lock()\n-\t\t// Restore our saved state. During non-leader status we just update our underlying store.\n-\t\to.readStoredState()\n-\n-\t\t// Do info sub.\n-\t\tif o.infoSub == nil && jsa != nil {\n-\t\t\tisubj := fmt.Sprintf(clusterConsumerInfoT, jsa.acc(), stream, o.name)\n-\t\t\t// Note below the way we subscribe here is so that we can send requests to ourselves.\n-\t\t\to.infoSub, _ = s.systemSubscribe(isubj, _EMPTY_, false, o.sysc, o.handleClusterConsumerInfoRequest)\n-\t\t}\n-\n-\t\tvar err error\n-\t\tif o.ackSub, err = o.subscribeInternal(o.ackSubj, o.processAck); err != nil {\n-\t\t\to.mu.Unlock()\n-\t\t\to.deleteWithoutAdvisory()\n-\t\t\treturn\n-\t\t}\n-\n-\t\t// Setup the internal sub for next message requests regardless.\n-\t\t// Will error if wrong mode to provide feedback to users.\n-\t\tif o.reqSub, err = o.subscribeInternal(o.nextMsgSubj, o.processNextMsgReq); err != nil {\n-\t\t\to.mu.Unlock()\n-\t\t\to.deleteWithoutAdvisory()\n-\t\t\treturn\n-\t\t}\n-\n-\t\t// Check on flow control settings.\n-\t\tif o.cfg.FlowControl {\n-\t\t\to.setMaxPendingBytes(JsFlowControlMaxPending)\n-\t\t\tfcsubj := fmt.Sprintf(jsFlowControl, stream, o.name)\n-\t\t\tif o.fcSub, err = o.subscribeInternal(fcsubj, o.processFlowControl); err != nil {\n-\t\t\t\to.mu.Unlock()\n-\t\t\t\to.deleteWithoutAdvisory()\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Setup initial pending and proper start sequence.\n-\t\to.setInitialPendingAndStart()\n-\n-\t\t// If push mode, register for notifications on interest.\n-\t\tif o.isPushMode() {\n-\t\t\to.inch = make(chan bool, 8)\n-\t\t\to.acc.sl.registerNotification(o.cfg.DeliverSubject, o.cfg.DeliverGroup, o.inch)\n-\t\t\tif o.active = <-o.inch; o.active {\n-\t\t\t\to.checkQueueInterest()\n-\t\t\t}\n-\t\t\t// Check gateways in case they are enabled.\n-\t\t\tif s.gateway.enabled {\n-\t\t\t\tif !o.active {\n-\t\t\t\t\to.active = s.hasGatewayInterest(o.acc.Name, o.cfg.DeliverSubject)\n-\t\t\t\t}\n-\t\t\t\tstopAndClearTimer(&o.gwdtmr)\n-\t\t\t\to.gwdtmr = time.AfterFunc(time.Second, func() { o.watchGWinterest() })\n-\t\t\t}\n-\t\t} else if !o.isDurable() {\n-\t\t\t// Ephemeral pull consumer. We run the dtmr all the time for this one.\n-\t\t\tif o.dtmr != nil {\n-\t\t\t\tstopAndClearTimer(&o.dtmr)\n-\t\t\t}\n-\t\t\to.dtmr = time.AfterFunc(o.dthresh, func() { o.deleteNotActive() })\n-\t\t}\n-\n-\t\t// If we are not in ReplayInstant mode mark us as in replay state until resolved.\n-\t\tif o.cfg.ReplayPolicy != ReplayInstant {\n-\t\t\to.replay = true\n-\t\t}\n-\n-\t\t// Recreate quit channel.\n-\t\to.qch = make(chan struct{})\n-\t\tqch := o.qch\n-\t\tnode := o.node\n-\t\tif node != nil && o.pch == nil {\n-\t\t\to.pch = make(chan struct{}, 1)\n-\t\t}\n-\t\to.mu.Unlock()\n-\n-\t\t// Now start up Go routine to deliver msgs.\n-\t\tgo o.loopAndGatherMsgs(qch)\n-\n-\t\t// If we are R>1 spin up our proposal loop.\n-\t\tif node != nil {\n-\t\t\t// Determine if we can send pending requests info to the group.\n-\t\t\t// They must be on server versions >= 2.7.1\n-\t\t\to.checkAndSetPendingRequestsOk()\n-\t\t\to.checkPendingRequests()\n-\t\t\tgo o.loopAndForwardProposals(qch)\n-\t\t}\n-\n-\t} else {\n-\t\t// Shutdown the go routines and the subscriptions.\n-\t\to.mu.Lock()\n-\t\to.unsubscribe(o.ackSub)\n-\t\to.unsubscribe(o.reqSub)\n-\t\to.unsubscribe(o.fcSub)\n-\t\to.ackSub = nil\n-\t\to.reqSub = nil\n-\t\to.fcSub = nil\n-\t\tif o.infoSub != nil {\n-\t\t\to.srv.sysUnsubscribe(o.infoSub)\n-\t\t\to.infoSub = nil\n-\t\t}\n-\t\tif o.qch != nil {\n-\t\t\tclose(o.qch)\n-\t\t\to.qch = nil\n-\t\t}\n-\t\t// Reset waiting if we are in pull mode.\n-\t\tif o.isPullMode() {\n-\t\t\to.waiting = newWaitQueue(o.cfg.MaxWaiting)\n-\t\t}\n-\t\to.mu.Unlock()\n-\t}\n+        o.mu.RLock()\n+        mset := o.mset\n+        isRunning := o.ackSub != nil\n+        o.mu.RUnlock()\n+\n+        // If we are here we have a change in leader status.\n+        if isLeader {\n+                if mset == nil || isRunning {\n+                        return\n+                }\n+\n+                mset.mu.RLock()\n+                s, jsa, stream := mset.srv, mset.jsa, mset.cfg.Name\n+                mset.mu.RUnlock()\n+\n+                o.mu.Lock()\n+                // Restore our saved state. During non-leader status we just update our underlying store.\n+                o.readStoredState()\n+\n+                // Do info sub.\n+                if o.infoSub == nil && jsa != nil {\n+                        isubj := fmt.Sprintf(clusterConsumerInfoT, jsa.acc(), stream, o.name)\n+                        // Note below the way we subscribe here is so that we can send requests to ourselves.\n+                        o.infoSub, _ = s.systemSubscribe(isubj, _EMPTY_, false, o.sysc, o.handleClusterConsumerInfoRequest)\n+                }\n+\n+                var err error\n+                if o.ackSub, err = o.subscribeInternal(o.ackSubj, o.processAck); err != nil {\n+                        o.mu.Unlock()\n+                        o.deleteWithoutAdvisory()\n+                        return\n+                }\n+\n+                // Setup the internal sub for next message requests regardless.\n+                // Will error if wrong mode to provide feedback to users.\n+                if o.reqSub, err = o.subscribeInternal(o.nextMsgSubj, o.processNextMsgReq); err != nil {\n+                        o.mu.Unlock()\n+                        o.deleteWithoutAdvisory()\n+                        return\n+                }\n+\n+                // Check on flow control settings.\n+                if o.cfg.FlowControl {\n+                        o.setMaxPendingBytes(JsFlowControlMaxPending)\n+                        fcsubj := fmt.Sprintf(jsFlowControl, stream, o.name)\n+                        if o.fcSub, err = o.subscribeInternal(fcsubj, o.processFlowControl); err != nil {\n+                                o.mu.Unlock()\n+                                o.deleteWithoutAdvisory()\n+                                return\n+                        }\n+                }\n+\n+                // Setup initial pending and proper start sequence.\n+                o.setInitialPendingAndStart()\n+\n+                // If push mode, register for notifications on interest.\n+                if o.isPushMode() {\n+                        o.inch = make(chan bool, 8)\n+                        o.acc.sl.registerNotification(o.cfg.DeliverSubject, o.cfg.DeliverGroup, o.inch)\n+                        if o.active = <-o.inch; o.active {\n+                                o.checkQueueInterest()\n+                        }\n+                        // Check gateways in case they are enabled.\n+                        if s.gateway.enabled {\n+                                if !o.active {\n+                                        o.active = s.hasGatewayInterest(o.acc.Name, o.cfg.DeliverSubject)\n+                                }\n+                                stopAndClearTimer(&o.gwdtmr)\n+                                o.gwdtmr = time.AfterFunc(time.Second, func() { o.watchGWinterest() })\n+                        }\n+                } else if !o.isDurable() {\n+                        // Ephemeral pull consumer. We run the dtmr all the time for this one.\n+                        if o.dtmr != nil {\n+                                stopAndClearTimer(&o.dtmr)\n+                        }\n+                        o.dtmr = time.AfterFunc(o.dthresh, func() { o.deleteNotActive() })\n+                }\n+\n+                // If we are not in ReplayInstant mode mark us as in replay state until resolved.\n+                if o.cfg.ReplayPolicy != ReplayInstant {\n+                        o.replay = true\n+                }\n+\n+                // Recreate quit channel.\n+                o.qch = make(chan struct{})\n+                qch := o.qch\n+                node := o.node\n+                if node != nil && o.pch == nil {\n+                        o.pch = make(chan struct{}, 1)\n+                }\n+                o.mu.Unlock()\n+\n+                // Now start up Go routine to deliver msgs.\n+                go o.loopAndGatherMsgs(qch)\n+\n+                // If we are R>1 spin up our proposal loop.\n+                if node != nil {\n+                        // Determine if we can send pending requests info to the group.\n+                        // They must be on server versions >= 2.7.1\n+                        o.checkAndSetPendingRequestsOk()\n+                        o.checkPendingRequests()\n+                        go o.loopAndForwardProposals(qch)\n+                }\n+\n+        } else {\n+                // Shutdown the go routines and the subscriptions.\n+                o.mu.Lock()\n+                o.unsubscribe(o.ackSub)\n+                o.unsubscribe(o.reqSub)\n+                o.unsubscribe(o.fcSub)\n+                o.ackSub = nil\n+                o.reqSub = nil\n+                o.fcSub = nil\n+                if o.infoSub != nil {\n+                        o.srv.sysUnsubscribe(o.infoSub)\n+                        o.infoSub = nil\n+                }\n+                if o.qch != nil {\n+                        close(o.qch)\n+                        o.qch = nil\n+                }\n+                // Reset waiting if we are in pull mode.\n+                if o.isPullMode() {\n+                        o.waiting = newWaitQueue(o.cfg.MaxWaiting)\n+                }\n+                o.mu.Unlock()\n+        }\n }\n \n func (o *consumer) handleClusterConsumerInfoRequest(sub *subscription, c *client, _ *Account, subject, reply string, msg []byte) {\n-\to.mu.RLock()\n-\tsysc := o.sysc\n-\to.mu.RUnlock()\n-\tsysc.sendInternalMsg(reply, _EMPTY_, nil, o.info())\n+        o.mu.RLock()\n+        sysc := o.sysc\n+        o.mu.RUnlock()\n+        sysc.sendInternalMsg(reply, _EMPTY_, nil, o.info())\n }\n \n // Lock should be held.\n func (o *consumer) subscribeInternal(subject string, cb msgHandler) (*subscription, error) {\n-\tc := o.client\n-\tif c == nil {\n-\t\treturn nil, fmt.Errorf(\"invalid consumer\")\n-\t}\n-\tif !c.srv.EventsEnabled() {\n-\t\treturn nil, ErrNoSysAccount\n-\t}\n-\tif cb == nil {\n-\t\treturn nil, fmt.Errorf(\"undefined message handler\")\n-\t}\n+        c := o.client\n+        if c == nil {\n+                return nil, fmt.Errorf(\"invalid consumer\")\n+        }\n+        if !c.srv.EventsEnabled() {\n+                return nil, ErrNoSysAccount\n+        }\n+        if cb == nil {\n+                return nil, fmt.Errorf(\"undefined message handler\")\n+        }\n \n-\to.sid++\n+        o.sid++\n \n-\t// Now create the subscription\n-\treturn c.processSub([]byte(subject), nil, []byte(strconv.Itoa(o.sid)), cb, false)\n+        // Now create the subscription\n+        return c.processSub([]byte(subject), nil, []byte(strconv.Itoa(o.sid)), cb, false)\n }\n \n // Unsubscribe from our subscription.\n // Lock should be held.\n func (o *consumer) unsubscribe(sub *subscription) {\n-\tif sub == nil || o.client == nil {\n-\t\treturn\n-\t}\n-\to.client.processUnsub(sub.sid)\n+        if sub == nil || o.client == nil {\n+                return\n+        }\n+        o.client.processUnsub(sub.sid)\n }\n \n // We need to make sure we protect access to the outq.\n // Do all advisory sends here.\n func (o *consumer) sendAdvisory(subj string, msg []byte) {\n-\to.outq.sendMsg(subj, msg)\n+        o.outq.sendMsg(subj, msg)\n }\n \n func (o *consumer) sendDeleteAdvisoryLocked() {\n-\te := JSConsumerActionAdvisory{\n-\t\tTypedEvent: TypedEvent{\n-\t\t\tType: JSConsumerActionAdvisoryType,\n-\t\t\tID:   nuid.Next(),\n-\t\t\tTime: time.Now().UTC(),\n-\t\t},\n-\t\tStream:   o.stream,\n-\t\tConsumer: o.name,\n-\t\tAction:   DeleteEvent,\n-\t\tDomain:   o.srv.getOpts().JetStreamDomain,\n-\t}\n-\n-\tj, err := json.Marshal(e)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tsubj := JSAdvisoryConsumerDeletedPre + \".\" + o.stream + \".\" + o.name\n-\to.sendAdvisory(subj, j)\n+        e := JSConsumerActionAdvisory{\n+                TypedEvent: TypedEvent{\n+                        Type: JSConsumerActionAdvisoryType,\n+                        ID:   nuid.Next(),\n+                        Time: time.Now().UTC(),\n+                },\n+                Stream:   o.stream,\n+                Consumer: o.name,\n+                Action:   DeleteEvent,\n+                Domain:   o.srv.getOpts().JetStreamDomain,\n+        }\n+\n+        j, err := json.Marshal(e)\n+        if err != nil {\n+                return\n+        }\n+\n+        subj := JSAdvisoryConsumerDeletedPre + \".\" + o.stream + \".\" + o.name\n+        o.sendAdvisory(subj, j)\n }\n \n func (o *consumer) sendCreateAdvisory() {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\te := JSConsumerActionAdvisory{\n-\t\tTypedEvent: TypedEvent{\n-\t\t\tType: JSConsumerActionAdvisoryType,\n-\t\t\tID:   nuid.Next(),\n-\t\t\tTime: time.Now().UTC(),\n-\t\t},\n-\t\tStream:   o.stream,\n-\t\tConsumer: o.name,\n-\t\tAction:   CreateEvent,\n-\t\tDomain:   o.srv.getOpts().JetStreamDomain,\n-\t}\n-\n-\tj, err := json.Marshal(e)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tsubj := JSAdvisoryConsumerCreatedPre + \".\" + o.stream + \".\" + o.name\n-\to.sendAdvisory(subj, j)\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        e := JSConsumerActionAdvisory{\n+                TypedEvent: TypedEvent{\n+                        Type: JSConsumerActionAdvisoryType,\n+                        ID:   nuid.Next(),\n+                        Time: time.Now().UTC(),\n+                },\n+                Stream:   o.stream,\n+                Consumer: o.name,\n+                Action:   CreateEvent,\n+                Domain:   o.srv.getOpts().JetStreamDomain,\n+        }\n+\n+        j, err := json.Marshal(e)\n+        if err != nil {\n+                return\n+        }\n+\n+        subj := JSAdvisoryConsumerCreatedPre + \".\" + o.stream + \".\" + o.name\n+        o.sendAdvisory(subj, j)\n }\n \n // Created returns created time.\n func (o *consumer) createdTime() time.Time {\n-\to.mu.Lock()\n-\tcreated := o.created\n-\to.mu.Unlock()\n-\treturn created\n+        o.mu.Lock()\n+        created := o.created\n+        o.mu.Unlock()\n+        return created\n }\n \n // Internal to allow creation time to be restored.\n func (o *consumer) setCreatedTime(created time.Time) {\n-\to.mu.Lock()\n-\to.created = created\n-\to.mu.Unlock()\n+        o.mu.Lock()\n+        o.created = created\n+        o.mu.Unlock()\n }\n \n // This will check for extended interest in a subject. If we have local interest we just return\n // that, but in the absence of local interest and presence of gateways or service imports we need\n // to check those as well.\n func (o *consumer) hasDeliveryInterest(localInterest bool) bool {\n-\to.mu.Lock()\n-\tmset := o.mset\n-\tif mset == nil {\n-\t\to.mu.Unlock()\n-\t\treturn false\n-\t}\n-\tacc := o.acc\n-\tdeliver := o.cfg.DeliverSubject\n-\to.mu.Unlock()\n-\n-\tif localInterest {\n-\t\treturn true\n-\t}\n-\n-\t// If we are here check gateways.\n-\tif s := acc.srv; s != nil && s.hasGatewayInterest(acc.Name, deliver) {\n-\t\treturn true\n-\t}\n-\treturn false\n+        o.mu.Lock()\n+        mset := o.mset\n+        if mset == nil {\n+                o.mu.Unlock()\n+                return false\n+        }\n+        acc := o.acc\n+        deliver := o.cfg.DeliverSubject\n+        o.mu.Unlock()\n+\n+        if localInterest {\n+                return true\n+        }\n+\n+        // If we are here check gateways.\n+        if s := acc.srv; s != nil && s.hasGatewayInterest(acc.Name, deliver) {\n+                return true\n+        }\n+        return false\n }\n \n func (s *Server) hasGatewayInterest(account, subject string) bool {\n-\tgw := s.gateway\n-\tif !gw.enabled {\n-\t\treturn false\n-\t}\n-\tgw.RLock()\n-\tdefer gw.RUnlock()\n-\tfor _, gwc := range gw.outo {\n-\t\tpsi, qr := gwc.gatewayInterest(account, subject)\n-\t\tif psi || qr != nil {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        gw := s.gateway\n+        if !gw.enabled {\n+                return false\n+        }\n+        gw.RLock()\n+        defer gw.RUnlock()\n+        for _, gwc := range gw.outo {\n+                psi, qr := gwc.gatewayInterest(account, subject)\n+                if psi || qr != nil {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // This processes an update to the local interest for a deliver subject.\n func (o *consumer) updateDeliveryInterest(localInterest bool) bool {\n-\tinterest := o.hasDeliveryInterest(localInterest)\n-\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\tmset := o.mset\n-\tif mset == nil || o.isPullMode() {\n-\t\treturn false\n-\t}\n-\n-\tif interest && !o.active {\n-\t\to.signalNewMessages()\n-\t}\n-\t// Update active status, if not active clear any queue group we captured.\n-\tif o.active = interest; !o.active {\n-\t\to.qgroup = _EMPTY_\n-\t} else {\n-\t\to.checkQueueInterest()\n-\t}\n-\n-\t// If the delete timer has already been set do not clear here and return.\n-\tif o.dtmr != nil && !o.isDurable() && !interest {\n-\t\treturn true\n-\t}\n-\n-\t// Stop and clear the delete timer always.\n-\tstopAndClearTimer(&o.dtmr)\n-\n-\t// If we do not have interest anymore and we are not durable start\n-\t// a timer to delete us. We wait for a bit in case of server reconnect.\n-\tif !o.isDurable() && !interest {\n-\t\to.dtmr = time.AfterFunc(o.dthresh, func() { o.deleteNotActive() })\n-\t\treturn true\n-\t}\n-\treturn false\n+        interest := o.hasDeliveryInterest(localInterest)\n+\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        mset := o.mset\n+        if mset == nil || o.isPullMode() {\n+                return false\n+        }\n+\n+        if interest && !o.active {\n+                o.signalNewMessages()\n+        }\n+        // Update active status, if not active clear any queue group we captured.\n+        if o.active = interest; !o.active {\n+                o.qgroup = _EMPTY_\n+        } else {\n+                o.checkQueueInterest()\n+        }\n+\n+        // If the delete timer has already been set do not clear here and return.\n+        if o.dtmr != nil && !o.isDurable() && !interest {\n+                return true\n+        }\n+\n+        // Stop and clear the delete timer always.\n+        stopAndClearTimer(&o.dtmr)\n+\n+        // If we do not have interest anymore and we are not durable start\n+        // a timer to delete us. We wait for a bit in case of server reconnect.\n+        if !o.isDurable() && !interest {\n+                o.dtmr = time.AfterFunc(o.dthresh, func() { o.deleteNotActive() })\n+                return true\n+        }\n+        return false\n }\n \n func (o *consumer) deleteNotActive() {\n-\to.mu.Lock()\n-\tif o.mset == nil {\n-\t\to.mu.Unlock()\n-\t\treturn\n-\t}\n-\t// Push mode just look at active.\n-\tif o.isPushMode() {\n-\t\t// If we are active simply return.\n-\t\tif o.active {\n-\t\t\to.mu.Unlock()\n-\t\t\treturn\n-\t\t}\n-\t} else {\n-\t\t// These need to keep firing so reset first.\n-\t\tif o.dtmr != nil {\n-\t\t\to.dtmr.Reset(o.dthresh)\n-\t\t}\n-\t\t// Check if we have had a request lately, or if we still have valid requests waiting.\n-\t\tif time.Since(o.waiting.last) <= o.dthresh || o.checkWaitingForInterest() {\n-\t\t\to.mu.Unlock()\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\ts, js := o.mset.srv, o.mset.srv.js\n-\tacc, stream, name, isDirect := o.acc.Name, o.stream, o.name, o.cfg.Direct\n-\to.mu.Unlock()\n-\n-\t// If we are clustered, check if we still have this consumer assigned.\n-\t// If we do forward a proposal to delete ourselves to the metacontroller leader.\n-\tif !isDirect && s.JetStreamIsClustered() {\n-\t\tjs.mu.RLock()\n-\t\tca, cc := js.consumerAssignment(acc, stream, name), js.cluster\n-\t\tjs.mu.RUnlock()\n-\n-\t\tif ca != nil && cc != nil {\n-\t\t\tcca := *ca\n-\t\t\tcca.Reply = _EMPTY_\n-\t\t\tmeta, removeEntry := cc.meta, encodeDeleteConsumerAssignment(&cca)\n-\t\t\tmeta.ForwardProposal(removeEntry)\n-\n-\t\t\t// Check to make sure we went away.\n-\t\t\t// Don't think this needs to be a monitored go routine.\n-\t\t\tgo func() {\n-\t\t\t\tvar fs bool\n-\t\t\t\tticker := time.NewTicker(time.Second)\n-\t\t\t\tdefer ticker.Stop()\n-\t\t\t\tfor range ticker.C {\n-\t\t\t\t\tjs.mu.RLock()\n-\t\t\t\t\tca := js.consumerAssignment(acc, stream, name)\n-\t\t\t\t\tjs.mu.RUnlock()\n-\t\t\t\t\tif ca != nil {\n-\t\t\t\t\t\tif fs {\n-\t\t\t\t\t\t\ts.Warnf(\"Consumer assignment not cleaned up, retrying\")\n-\t\t\t\t\t\t\tmeta.ForwardProposal(removeEntry)\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\tfs = true\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}()\n-\t\t}\n-\t}\n-\n-\t// We will delete here regardless.\n-\to.delete()\n+        o.mu.Lock()\n+        if o.mset == nil {\n+                o.mu.Unlock()\n+                return\n+        }\n+        // Push mode just look at active.\n+        if o.isPushMode() {\n+                // If we are active simply return.\n+                if o.active {\n+                        o.mu.Unlock()\n+                        return\n+                }\n+        } else {\n+                // These need to keep firing so reset first.\n+                if o.dtmr != nil {\n+                        o.dtmr.Reset(o.dthresh)\n+                }\n+                // Check if we have had a request lately, or if we still have valid requests waiting.\n+                if time.Since(o.waiting.last) <= o.dthresh || o.checkWaitingForInterest() {\n+                        o.mu.Unlock()\n+                        return\n+                }\n+        }\n+\n+        s, js := o.mset.srv, o.mset.srv.js\n+        acc, stream, name, isDirect := o.acc.Name, o.stream, o.name, o.cfg.Direct\n+        o.mu.Unlock()\n+\n+        // If we are clustered, check if we still have this consumer assigned.\n+        // If we do forward a proposal to delete ourselves to the metacontroller leader.\n+        if !isDirect && s.JetStreamIsClustered() {\n+                js.mu.RLock()\n+                ca, cc := js.consumerAssignment(acc, stream, name), js.cluster\n+                js.mu.RUnlock()\n+\n+                if ca != nil && cc != nil {\n+                        cca := *ca\n+                        cca.Reply = _EMPTY_\n+                        meta, removeEntry := cc.meta, encodeDeleteConsumerAssignment(&cca)\n+                        meta.ForwardProposal(removeEntry)\n+\n+\n+                        // Check to make sure we went away.\n+                        // Don't think this needs to be a monitored go routine.\n+                        go func() {\n+                                var fs bool\n+                                ticker := time.NewTicker(time.Second)\n+                                defer ticker.Stop()\n+                                for range ticker.C {\n+                                        js.mu.RLock()\n+                                        ca := js.consumerAssignment(acc, stream, name)\n+                                        js.mu.RUnlock()\n+                                        if ca != nil {\n+                                                if fs {\n+                                                        s.Warnf(\"Consumer assignment not cleaned up, retrying\")\n+                                                        meta.ForwardProposal(removeEntry)\n+                                                }\n+                                                fs = true\n+                                        } else {\n+                                                return\n+                                        }\n+                                }\n+                        }()\n+                }\n+        }\n+\n+        // We will delete here regardless.\n+        o.delete()\n }\n \n func (o *consumer) watchGWinterest() {\n-\tpa := o.isActive()\n-\t// If there is no local interest...\n-\tif o.hasNoLocalInterest() {\n-\t\to.updateDeliveryInterest(false)\n-\t\tif !pa && o.isActive() {\n-\t\t\to.signalNewMessages()\n-\t\t}\n-\t}\n-\n-\t// We want this to always be running so we can also pick up on interest returning.\n-\to.mu.Lock()\n-\tif o.gwdtmr != nil {\n-\t\to.gwdtmr.Reset(time.Second)\n-\t} else {\n-\t\tstopAndClearTimer(&o.gwdtmr)\n-\t\to.gwdtmr = time.AfterFunc(time.Second, func() { o.watchGWinterest() })\n-\t}\n-\to.mu.Unlock()\n+        pa := o.isActive()\n+        // If there is no local interest...\n+        if o.hasNoLocalInterest() {\n+                o.updateDeliveryInterest(false)\n+                if !pa && o.isActive() {\n+                        o.signalNewMessages()\n+                }\n+        }\n+\n+        // We want this to always be running so we can also pick up on interest returning.\n+        o.mu.Lock()\n+        if o.gwdtmr != nil {\n+                o.gwdtmr.Reset(time.Second)\n+        } else {\n+                stopAndClearTimer(&o.gwdtmr)\n+                o.gwdtmr = time.AfterFunc(time.Second, func() { o.watchGWinterest() })\n+        }\n+        o.mu.Unlock()\n }\n \n // Config returns the consumer's configuration.\n func (o *consumer) config() ConsumerConfig {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\treturn o.cfg\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+        return o.cfg\n }\n \n // Force expiration of all pending.\n // Lock should be held.\n func (o *consumer) forceExpirePending() {\n-\tvar expired []uint64\n-\tfor seq := range o.pending {\n-\t\tif !o.onRedeliverQueue(seq) {\n-\t\t\texpired = append(expired, seq)\n-\t\t}\n-\t}\n-\tif len(expired) > 0 {\n-\t\tsort.Slice(expired, func(i, j int) bool { return expired[i] < expired[j] })\n-\t\to.addToRedeliverQueue(expired...)\n-\t\t// Now we should update the timestamp here since we are redelivering.\n-\t\t// We will use an incrementing time to preserve order for any other redelivery.\n-\t\toff := time.Now().UnixNano() - o.pending[expired[0]].Timestamp\n-\t\tfor _, seq := range expired {\n-\t\t\tif p, ok := o.pending[seq]; ok && p != nil {\n-\t\t\t\tp.Timestamp += off\n-\t\t\t}\n-\t\t}\n-\t\to.ptmr.Reset(o.ackWait(0))\n-\t}\n-\to.signalNewMessages()\n+        var expired []uint64\n+        for seq := range o.pending {\n+                if !o.onRedeliverQueue(seq) {\n+                        expired = append(expired, seq)\n+                }\n+        }\n+        if len(expired) > 0 {\n+                sort.Slice(expired, func(i, j int) bool { return expired[i] < expired[j] })\n+                o.addToRedeliverQueue(expired...)\n+                // Now we should update the timestamp here since we are redelivering.\n+                // We will use an incrementing time to preserve order for any other redelivery.\n+                off := time.Now().UnixNano() - o.pending[expired[0]].Timestamp\n+                for _, seq := range expired {\n+                        if p, ok := o.pending[seq]; ok && p != nil {\n+                                p.Timestamp += off\n+                        }\n+                }\n+                o.ptmr.Reset(o.ackWait(0))\n+        }\n+        o.signalNewMessages()\n }\n \n // Acquire proper locks and update rate limit.\n // Will use what is in config.\n func (o *consumer) setRateLimitNeedsLocks() {\n-\to.mu.RLock()\n-\tmset := o.mset\n-\to.mu.RUnlock()\n+        o.mu.RLock()\n+        mset := o.mset\n+        o.mu.RUnlock()\n \n-\tif mset == nil {\n-\t\treturn\n-\t}\n+        if mset == nil {\n+                return\n+        }\n \n-\tmset.mu.RLock()\n-\to.mu.Lock()\n-\to.setRateLimit(o.cfg.RateLimit)\n-\to.mu.Unlock()\n-\tmset.mu.RUnlock()\n+        mset.mu.RLock()\n+        o.mu.Lock()\n+        o.setRateLimit(o.cfg.RateLimit)\n+        o.mu.Unlock()\n+        mset.mu.RUnlock()\n }\n \n // Set the rate limiter\n // Both mset and consumer lock should be held.\n func (o *consumer) setRateLimit(bps uint64) {\n-\tif bps == 0 {\n-\t\to.rlimit = nil\n-\t\treturn\n-\t}\n-\n-\t// TODO(dlc) - Make sane values or error if not sane?\n-\t// We are configured in bits per sec so adjust to bytes.\n-\trl := rate.Limit(bps / 8)\n-\tmset := o.mset\n-\n-\t// Burst should be set to maximum msg size for this account, etc.\n-\tvar burst int\n-\tif mset.cfg.MaxMsgSize > 0 {\n-\t\tburst = int(mset.cfg.MaxMsgSize)\n-\t} else if mset.jsa.account.limits.mpay > 0 {\n-\t\tburst = int(mset.jsa.account.limits.mpay)\n-\t} else {\n-\t\ts := mset.jsa.account.srv\n-\t\tburst = int(s.getOpts().MaxPayload)\n-\t}\n-\n-\to.rlimit = rate.NewLimiter(rl, burst)\n+        if bps == 0 {\n+                o.rlimit = nil\n+                return\n+        }\n+\n+        // TODO(dlc) - Make sane values or error if not sane?\n+        // We are configured in bits per sec so adjust to bytes.\n+        rl := rate.Limit(bps / 8)\n+        mset := o.mset\n+\n+        // Burst should be set to maximum msg size for this account, etc.\n+        var burst int\n+        if mset.cfg.MaxMsgSize > 0 {\n+                burst = int(mset.cfg.MaxMsgSize)\n+        } else if mset.jsa.account.limits.mpay > 0 {\n+                burst = int(mset.jsa.account.limits.mpay)\n+        } else {\n+                s := mset.jsa.account.srv\n+                burst = int(s.getOpts().MaxPayload)\n+        }\n+\n+        o.rlimit = rate.NewLimiter(rl, burst)\n }\n \n // Check if new consumer config allowed vs old.\n func (acc *Account) checkNewConsumerConfig(cfg, ncfg *ConsumerConfig) error {\n-\tif reflect.DeepEqual(cfg, ncfg) {\n-\t\treturn nil\n-\t}\n-\t// Something different, so check since we only allow certain things to be updated.\n-\tif cfg.FilterSubject != ncfg.FilterSubject {\n-\t\treturn errors.New(\"filter subject can not be updated\")\n-\t}\n-\tif cfg.DeliverPolicy != ncfg.DeliverPolicy {\n-\t\treturn errors.New(\"deliver policy can not be updated\")\n-\t}\n-\tif cfg.OptStartSeq != ncfg.OptStartSeq {\n-\t\treturn errors.New(\"start sequence can not be updated\")\n-\t}\n-\tif cfg.OptStartTime != ncfg.OptStartTime {\n-\t\treturn errors.New(\"start time can not be updated\")\n-\t}\n-\tif cfg.AckPolicy != ncfg.AckPolicy {\n-\t\treturn errors.New(\"ack policy can not be updated\")\n-\t}\n-\tif cfg.ReplayPolicy != ncfg.ReplayPolicy {\n-\t\treturn errors.New(\"replay policy can not be updated\")\n-\t}\n-\tif cfg.Heartbeat != ncfg.Heartbeat {\n-\t\treturn errors.New(\"heart beats can not be updated\")\n-\t}\n-\tif cfg.FlowControl != ncfg.FlowControl {\n-\t\treturn errors.New(\"flow control can not be updated\")\n-\t}\n-\n-\t// Deliver Subject is conditional on if its bound.\n-\tif cfg.DeliverSubject != ncfg.DeliverSubject {\n-\t\tif cfg.DeliverSubject == _EMPTY_ {\n-\t\t\treturn errors.New(\"can not update pull consumer to push based\")\n-\t\t}\n-\t\tif ncfg.DeliverSubject == _EMPTY_ {\n-\t\t\treturn errors.New(\"can not update push consumer to pull based\")\n-\t\t}\n-\t\trr := acc.sl.Match(cfg.DeliverSubject)\n-\t\tif len(rr.psubs)+len(rr.qsubs) != 0 {\n-\t\t\treturn NewJSConsumerNameExistError()\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        if reflect.DeepEqual(cfg, ncfg) {\n+                return nil\n+        }\n+        // Something different, so check since we only allow certain things to be updated.\n+        if cfg.FilterSubject != ncfg.FilterSubject {\n+                return errors.New(\"filter subject can not be updated\")\n+        }\n+        if cfg.DeliverPolicy != ncfg.DeliverPolicy {\n+                return errors.New(\"deliver policy can not be updated\")\n+        }\n+        if cfg.OptStartSeq != ncfg.OptStartSeq {\n+                return errors.New(\"start sequence can not be updated\")\n+        }\n+        if cfg.OptStartTime != ncfg.OptStartTime {\n+                return errors.New(\"start time can not be updated\")\n+        }\n+        if cfg.AckPolicy != ncfg.AckPolicy {\n+                return errors.New(\"ack policy can not be updated\")\n+        }\n+        if cfg.ReplayPolicy != ncfg.ReplayPolicy {\n+                return errors.New(\"replay policy can not be updated\")\n+        }\n+        if cfg.Heartbeat != ncfg.Heartbeat {\n+                return errors.New(\"heart beats can not be updated\")\n+        }\n+        if cfg.FlowControl != ncfg.FlowControl {\n+                return errors.New(\"flow control can not be updated\")\n+        }\n+\n+        // Deliver Subject is conditional on if its bound.\n+        if cfg.DeliverSubject != ncfg.DeliverSubject {\n+                if cfg.DeliverSubject == _EMPTY_ {\n+                        return errors.New(\"can not update pull consumer to push based\")\n+                }\n+                if ncfg.DeliverSubject == _EMPTY_ {\n+                        return errors.New(\"can not update push consumer to pull based\")\n+                }\n+                rr := acc.sl.Match(cfg.DeliverSubject)\n+                if len(rr.psubs)+len(rr.qsubs) != 0 {\n+                        return NewJSConsumerNameExistError()\n+                }\n+        }\n+\n+        return nil\n }\n \n // Update the config based on the new config, or error if update not allowed.\n func (o *consumer) updateConfig(cfg *ConsumerConfig) error {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\tif err := o.acc.checkNewConsumerConfig(&o.cfg, cfg); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif o.store != nil {\n-\t\t// Update local state always.\n-\t\tif err := o.store.UpdateConfig(cfg); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\t// DeliverSubject\n-\tif cfg.DeliverSubject != o.cfg.DeliverSubject {\n-\t\to.updateDeliverSubjectLocked(cfg.DeliverSubject)\n-\t}\n-\n-\t// MaxAckPending\n-\tif cfg.MaxAckPending != o.cfg.MaxAckPending {\n-\t\to.maxp = cfg.MaxAckPending\n-\t\to.signalNewMessages()\n-\t}\n-\t// AckWait\n-\tif cfg.AckWait != o.cfg.AckWait {\n-\t\tif o.ptmr != nil {\n-\t\t\to.ptmr.Reset(100 * time.Millisecond)\n-\t\t}\n-\t}\n-\t// Rate Limit\n-\tif cfg.RateLimit != o.cfg.RateLimit {\n-\t\t// We need both locks here so do in Go routine.\n-\t\tgo o.setRateLimitNeedsLocks()\n-\t}\n-\n-\t// Record new config for others that do not need special handling.\n-\t// Allowed but considered no-op, [Description, MaxDeliver, SampleFrequency, MaxWaiting, HeadersOnly]\n-\to.cfg = *cfg\n-\n-\treturn nil\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        if err := o.acc.checkNewConsumerConfig(&o.cfg, cfg); err != nil {\n+                return err\n+        }\n+\n+        if o.store != nil {\n+                // Update local state always.\n+                if err := o.store.UpdateConfig(cfg); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // DeliverSubject\n+        if cfg.DeliverSubject != o.cfg.DeliverSubject {\n+                o.updateDeliverSubjectLocked(cfg.DeliverSubject)\n+        }\n+\n+        // MaxAckPending\n+        if cfg.MaxAckPending != o.cfg.MaxAckPending {\n+                o.maxp = cfg.MaxAckPending\n+                o.signalNewMessages()\n+        }\n+        // AckWait\n+        if cfg.AckWait != o.cfg.AckWait {\n+                if o.ptmr != nil {\n+                        o.ptmr.Reset(100 * time.Millisecond)\n+                }\n+        }\n+        // Rate Limit\n+        if cfg.RateLimit != o.cfg.RateLimit {\n+                // We need both locks here so do in Go routine.\n+                go o.setRateLimitNeedsLocks()\n+        }\n+\n+        // Record new config for others that do not need special handling.\n+        // Allowed but considered no-op, [Description, MaxDeliver, SampleFrequency, MaxWaiting, HeadersOnly]\n+        o.cfg = *cfg\n+\n+        return nil\n }\n \n // This is a config change for the delivery subject for a\n // push based consumer.\n func (o *consumer) updateDeliverSubject(newDeliver string) {\n-\t// Update the config and the dsubj\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\to.updateDeliverSubjectLocked(newDeliver)\n+        // Update the config and the dsubj\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+        o.updateDeliverSubjectLocked(newDeliver)\n }\n \n // This is a config change for the delivery subject for a\n // push based consumer.\n func (o *consumer) updateDeliverSubjectLocked(newDeliver string) {\n-\tif o.closed || o.isPullMode() || o.cfg.DeliverSubject == newDeliver {\n-\t\treturn\n-\t}\n+        if o.closed || o.isPullMode() || o.cfg.DeliverSubject == newDeliver {\n+                return\n+        }\n \n-\t// Force redeliver of all pending on change of delivery subject.\n-\tif len(o.pending) > 0 {\n-\t\to.forceExpirePending()\n-\t}\n+        // Force redeliver of all pending on change of delivery subject.\n+        if len(o.pending) > 0 {\n+                o.forceExpirePending()\n+        }\n \n-\to.acc.sl.clearNotification(o.dsubj, o.cfg.DeliverGroup, o.inch)\n-\to.dsubj, o.cfg.DeliverSubject = newDeliver, newDeliver\n-\t// When we register new one it will deliver to update state loop.\n-\to.acc.sl.registerNotification(newDeliver, o.cfg.DeliverGroup, o.inch)\n+        o.acc.sl.clearNotification(o.dsubj, o.cfg.DeliverGroup, o.inch)\n+        o.dsubj, o.cfg.DeliverSubject = newDeliver, newDeliver\n+        // When we register new one it will deliver to update state loop.\n+        o.acc.sl.registerNotification(newDeliver, o.cfg.DeliverGroup, o.inch)\n }\n \n // Check that configs are equal but allow delivery subjects to be different.\n func configsEqualSansDelivery(a, b ConsumerConfig) bool {\n-\t// These were copied in so can set Delivery here.\n-\ta.DeliverSubject, b.DeliverSubject = _EMPTY_, _EMPTY_\n-\treturn reflect.DeepEqual(a, b)\n+        // These were copied in so can set Delivery here.\n+        a.DeliverSubject, b.DeliverSubject = _EMPTY_, _EMPTY_\n+        return reflect.DeepEqual(a, b)\n }\n \n // Helper to send a reply to an ack.\n func (o *consumer) sendAckReply(subj string) {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\to.sendAdvisory(subj, nil)\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+        o.sendAdvisory(subj, nil)\n }\n \n // Process a message for the ack reply subject delivered with a message.\n func (o *consumer) processAck(_ *subscription, c *client, acc *Account, subject, reply string, rmsg []byte) {\n-\t_, msg := c.msgParts(rmsg)\n-\tsseq, dseq, dc := ackReplyInfo(subject)\n-\n-\tskipAckReply := sseq == 0\n-\n-\tswitch {\n-\tcase len(msg) == 0, bytes.Equal(msg, AckAck), bytes.Equal(msg, AckOK):\n-\t\to.processAckMsg(sseq, dseq, dc, true)\n-\tcase bytes.HasPrefix(msg, AckNext):\n-\t\to.processAckMsg(sseq, dseq, dc, true)\n-\t\t// processNextMsgReq can be invoked from an internal subscription or from here.\n-\t\t// Therefore, it has to call msgParts(), so we can't simply pass msg[len(AckNext):]\n-\t\t// with current c.pa.hdr because it would cause a panic.  We will save the current\n-\t\t// c.pa.hdr value and disable headers before calling processNextMsgReq and then\n-\t\t// restore so that we don't mess with the calling stack in case it is used\n-\t\t// somewhere else.\n-\t\tphdr := c.pa.hdr\n-\t\tc.pa.hdr = -1\n-\t\to.processNextMsgReq(nil, c, acc, subject, reply, msg[len(AckNext):])\n-\t\tc.pa.hdr = phdr\n-\t\tskipAckReply = true\n-\tcase bytes.HasPrefix(msg, AckNak):\n-\t\to.processNak(sseq, dseq, dc, msg)\n-\tcase bytes.Equal(msg, AckProgress):\n-\t\to.progressUpdate(sseq)\n-\tcase bytes.Equal(msg, AckTerm):\n-\t\to.processTerm(sseq, dseq, dc)\n-\t}\n-\n-\t// Ack the ack if requested.\n-\tif len(reply) > 0 && !skipAckReply {\n-\t\to.sendAckReply(reply)\n-\t}\n+        _, msg := c.msgParts(rmsg)\n+        sseq, dseq, dc := ackReplyInfo(subject)\n+\n+        skipAckReply := sseq == 0\n+\n+        switch {\n+        case len(msg) == 0, bytes.Equal(msg, AckAck), bytes.Equal(msg, AckOK):\n+                o.processAckMsg(sseq, dseq, dc, true)\n+        case bytes.HasPrefix(msg, AckNext):\n+                o.processAckMsg(sseq, dseq, dc, true)\n+                // processNextMsgReq can be invoked from an internal subscription or from here.\n+                // Therefore, it has to call msgParts(), so we can't simply pass msg[len(AckNext):]\n+                // with current c.pa.hdr because it would cause a panic.  We will save the current\n+                // c.pa.hdr value and disable headers before calling processNextMsgReq and then\n+                // restore so that we don't mess with the calling stack in case it is used\n+                // somewhere else.\n+                phdr := c.pa.hdr\n+                c.pa.hdr = -1\n+                o.processNextMsgReq(nil, c, acc, subject, reply, msg[len(AckNext):])\n+                c.pa.hdr = phdr\n+                skipAckReply = true\n+        case bytes.HasPrefix(msg, AckNak):\n+                o.processNak(sseq, dseq, dc, msg)\n+        case bytes.Equal(msg, AckProgress):\n+                o.progressUpdate(sseq)\n+        case bytes.Equal(msg, AckTerm):\n+                o.processTerm(sseq, dseq, dc)\n+        }\n+\n+        // Ack the ack if requested.\n+        if len(reply) > 0 && !skipAckReply {\n+                o.sendAckReply(reply)\n+        }\n }\n \n // Used to process a working update to delay redelivery.\n func (o *consumer) progressUpdate(seq uint64) {\n-\to.mu.Lock()\n-\tif len(o.pending) > 0 {\n-\t\tif p, ok := o.pending[seq]; ok {\n-\t\t\tp.Timestamp = time.Now().UnixNano()\n-\t\t\t// Update store system.\n-\t\t\to.updateDelivered(p.Sequence, seq, 1, p.Timestamp)\n-\t\t}\n-\t}\n-\to.mu.Unlock()\n+        o.mu.Lock()\n+        if len(o.pending) > 0 {\n+                if p, ok := o.pending[seq]; ok {\n+                        p.Timestamp = time.Now().UnixNano()\n+                        // Update store system.\n+                        o.updateDelivered(p.Sequence, seq, 1, p.Timestamp)\n+                }\n+        }\n+        o.mu.Unlock()\n }\n \n // Lock should be held.\n func (o *consumer) updateSkipped() {\n-\t// Clustered mode and R>1 only.\n-\tif o.node == nil || !o.isLeader() {\n-\t\treturn\n-\t}\n-\tvar b [1 + 8]byte\n-\tb[0] = byte(updateSkipOp)\n-\tvar le = binary.LittleEndian\n-\tle.PutUint64(b[1:], o.sseq)\n-\to.propose(b[:])\n+        // Clustered mode and R>1 only.\n+        if o.node == nil || !o.isLeader() {\n+                return\n+        }\n+        var b [1 + 8]byte\n+        b[0] = byte(updateSkipOp)\n+        var le = binary.LittleEndian\n+        le.PutUint64(b[1:], o.sseq)\n+        o.propose(b[:])\n }\n \n func (o *consumer) loopAndForwardProposals(qch chan struct{}) {\n-\to.mu.RLock()\n-\tnode, pch := o.node, o.pch\n-\to.mu.RUnlock()\n-\n-\tif node == nil || pch == nil {\n-\t\treturn\n-\t}\n-\n-\tforwardProposals := func() {\n-\t\to.mu.Lock()\n-\t\tproposal := o.phead\n-\t\to.phead, o.ptail = nil, nil\n-\t\to.mu.Unlock()\n-\t\t// 256k max for now per batch.\n-\t\tconst maxBatch = 256 * 1024\n-\t\tvar entries []*Entry\n-\t\tfor sz := 0; proposal != nil; proposal = proposal.next {\n-\t\t\tentries = append(entries, &Entry{EntryNormal, proposal.data})\n-\t\t\tsz += len(proposal.data)\n-\t\t\tif sz > maxBatch {\n-\t\t\t\tnode.ProposeDirect(entries)\n-\t\t\t\t// We need to re-craete `entries` because there is a reference\n-\t\t\t\t// to it in the node's pae map.\n-\t\t\t\tsz, entries = 0, nil\n-\t\t\t}\n-\t\t}\n-\t\tif len(entries) > 0 {\n-\t\t\tnode.ProposeDirect(entries)\n-\t\t}\n-\t}\n-\n-\t// In case we have anything pending on entry.\n-\tforwardProposals()\n-\n-\tfor {\n-\t\tselect {\n-\t\tcase <-qch:\n-\t\t\tforwardProposals()\n-\t\t\treturn\n-\t\tcase <-pch:\n-\t\t\tforwardProposals()\n-\t\t}\n-\t}\n+        o.mu.RLock()\n+        node, pch := o.node, o.pch\n+        o.mu.RUnlock()\n+\n+        if node == nil || pch == nil {\n+                return\n+        }\n+\n+        forwardProposals := func() {\n+                o.mu.Lock()\n+                proposal := o.phead\n+                o.phead, o.ptail = nil, nil\n+                o.mu.Unlock()\n+                // 256k max for now per batch.\n+                const maxBatch = 256 * 1024\n+                var entries []*Entry\n+                for sz := 0; proposal != nil; proposal = proposal.next {\n+                        entries = append(entries, &Entry{EntryNormal, proposal.data})\n+                        sz += len(proposal.data)\n+                        if sz > maxBatch {\n+                                node.ProposeDirect(entries)\n+                                // We need to re-craete `entries` because there is a reference\n+                                // to it in the node's pae map.\n+                                sz, entries = 0, nil\n+                        }\n+                }\n+                if len(entries) > 0 {\n+                        node.ProposeDirect(entries)\n+                }\n+        }\n+\n+        // In case we have anything pending on entry.\n+        forwardProposals()\n+\n+        for {\n+                select {\n+                case <-qch:\n+                        forwardProposals()\n+                        return\n+                case <-pch:\n+                        forwardProposals()\n+                }\n+        }\n }\n \n // Lock should be held.\n func (o *consumer) propose(entry []byte) {\n-\tvar notify bool\n-\tp := &proposal{data: entry}\n-\tif o.phead == nil {\n-\t\to.phead = p\n-\t\tnotify = true\n-\t} else {\n-\t\to.ptail.next = p\n-\t}\n-\to.ptail = p\n-\n-\t// Kick our looper routine if needed.\n-\tif notify {\n-\t\tselect {\n-\t\tcase o.pch <- struct{}{}:\n-\t\tdefault:\n-\t\t}\n-\t}\n+        var notify bool\n+        p := &proposal{data: entry}\n+        if o.phead == nil {\n+                o.phead = p\n+                notify = true\n+        } else {\n+                o.ptail.next = p\n+        }\n+        o.ptail = p\n+\n+        // Kick our looper routine if needed.\n+        if notify {\n+                select {\n+                case o.pch <- struct{}{}:\n+                default:\n+                }\n+        }\n }\n \n // Lock should be held.\n func (o *consumer) updateDelivered(dseq, sseq, dc uint64, ts int64) {\n-\t// Clustered mode and R>1.\n-\tif o.node != nil {\n-\t\t// Inline for now, use variable compression.\n-\t\tvar b [4*binary.MaxVarintLen64 + 1]byte\n-\t\tb[0] = byte(updateDeliveredOp)\n-\t\tn := 1\n-\t\tn += binary.PutUvarint(b[n:], dseq)\n-\t\tn += binary.PutUvarint(b[n:], sseq)\n-\t\tn += binary.PutUvarint(b[n:], dc)\n-\t\tn += binary.PutVarint(b[n:], ts)\n-\t\to.propose(b[:n])\n-\t}\n-\tif o.store != nil {\n-\t\t// Update local state always.\n-\t\to.store.UpdateDelivered(dseq, sseq, dc, ts)\n-\t}\n-\t// Update activity.\n-\to.ldt = time.Now()\n+        // Clustered mode and R>1.\n+        if o.node != nil {\n+                // Inline for now, use variable compression.\n+                var b [4*binary.MaxVarintLen64 + 1]byte\n+                b[0] = byte(updateDeliveredOp)\n+                n := 1\n+                n += binary.PutUvarint(b[n:], dseq)\n+                n += binary.PutUvarint(b[n:], sseq)\n+                n += binary.PutUvarint(b[n:], dc)\n+                n += binary.PutVarint(b[n:], ts)\n+                o.propose(b[:n])\n+        }\n+        if o.store != nil {\n+                // Update local state always.\n+                o.store.UpdateDelivered(dseq, sseq, dc, ts)\n+        }\n+        // Update activity.\n+        o.ldt = time.Now()\n }\n \n // Lock should be held.\n func (o *consumer) updateAcks(dseq, sseq uint64) {\n-\tif o.node != nil {\n-\t\t// Inline for now, use variable compression.\n-\t\tvar b [2*binary.MaxVarintLen64 + 1]byte\n-\t\tb[0] = byte(updateAcksOp)\n-\t\tn := 1\n-\t\tn += binary.PutUvarint(b[n:], dseq)\n-\t\tn += binary.PutUvarint(b[n:], sseq)\n-\t\to.propose(b[:n])\n-\t} else if o.store != nil {\n-\t\to.store.UpdateAcks(dseq, sseq)\n-\t}\n-\t// Update activity.\n-\to.lat = time.Now()\n+        if o.node != nil {\n+                // Inline for now, use variable compression.\n+                var b [2*binary.MaxVarintLen64 + 1]byte\n+                b[0] = byte(updateAcksOp)\n+                n := 1\n+                n += binary.PutUvarint(b[n:], dseq)\n+                n += binary.PutUvarint(b[n:], sseq)\n+                o.propose(b[:n])\n+        } else if o.store != nil {\n+                o.store.UpdateAcks(dseq, sseq)\n+        }\n+        // Update activity.\n+        o.lat = time.Now()\n }\n \n // Communicate to the cluster an addition of a pending request.\n // Lock should be held.\n func (o *consumer) addClusterPendingRequest(reply string) {\n-\tif o.node == nil || !o.pendingRequestsOk() {\n-\t\treturn\n-\t}\n-\tb := make([]byte, len(reply)+1)\n-\tb[0] = byte(addPendingRequest)\n-\tcopy(b[1:], reply)\n-\to.propose(b)\n+        if o.node == nil || !o.pendingRequestsOk() {\n+                return\n+        }\n+        b := make([]byte, len(reply)+1)\n+        b[0] = byte(addPendingRequest)\n+        copy(b[1:], reply)\n+        o.propose(b)\n }\n \n // Communicate to the cluster a removal of a pending request.\n // Lock should be held.\n func (o *consumer) removeClusterPendingRequest(reply string) {\n-\tif o.node == nil || !o.pendingRequestsOk() {\n-\t\treturn\n-\t}\n-\tb := make([]byte, len(reply)+1)\n-\tb[0] = byte(removePendingRequest)\n-\tcopy(b[1:], reply)\n-\to.propose(b)\n+        if o.node == nil || !o.pendingRequestsOk() {\n+                return\n+        }\n+        b := make([]byte, len(reply)+1)\n+        b[0] = byte(removePendingRequest)\n+        copy(b[1:], reply)\n+        o.propose(b)\n }\n \n // Set whether or not we can send pending requests to followers.\n func (o *consumer) setPendingRequestsOk(ok bool) {\n-\to.mu.Lock()\n-\to.prOk = ok\n-\to.mu.Unlock()\n+        o.mu.Lock()\n+        o.prOk = ok\n+        o.mu.Unlock()\n }\n \n // Lock should be held.\n func (o *consumer) pendingRequestsOk() bool {\n-\treturn o.prOk\n+        return o.prOk\n }\n \n // Set whether or not we can send info about pending pull requests to our group.\n // Will require all peers have a minimum version.\n func (o *consumer) checkAndSetPendingRequestsOk() {\n-\to.mu.RLock()\n-\ts, isValid := o.srv, o.mset != nil\n-\to.mu.RUnlock()\n-\tif !isValid {\n-\t\treturn\n-\t}\n-\n-\tif ca := o.consumerAssignment(); ca != nil && len(ca.Group.Peers) > 1 {\n-\t\tfor _, pn := range ca.Group.Peers {\n-\t\t\tif si, ok := s.nodeToInfo.Load(pn); ok {\n-\t\t\t\tif !versionAtLeast(si.(nodeInfo).version, 2, 7, 1) {\n-\t\t\t\t\t// We expect all of our peers to eventually be up to date.\n-\t\t\t\t\t// So check again in awhile.\n-\t\t\t\t\ttime.AfterFunc(eventsHBInterval, func() { o.checkAndSetPendingRequestsOk() })\n-\t\t\t\t\to.setPendingRequestsOk(false)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\to.setPendingRequestsOk(true)\n+        o.mu.RLock()\n+        s, isValid := o.srv, o.mset != nil\n+        o.mu.RUnlock()\n+        if !isValid {\n+                return\n+        }\n+\n+        if ca := o.consumerAssignment(); ca != nil && len(ca.Group.Peers) > 1 {\n+                for _, pn := range ca.Group.Peers {\n+                        if si, ok := s.nodeToInfo.Load(pn); ok {\n+                                if !versionAtLeast(si.(nodeInfo).version, 2, 7, 1) {\n+                                        // We expect all of our peers to eventually be up to date.\n+                                        // So check again in awhile.\n+                                        time.AfterFunc(eventsHBInterval, func() { o.checkAndSetPendingRequestsOk() })\n+                                        o.setPendingRequestsOk(false)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+        o.setPendingRequestsOk(true)\n }\n \n // On leadership change make sure we alert the pending requests that they are no longer valid.\n func (o *consumer) checkPendingRequests() {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\tif o.mset == nil || o.outq == nil {\n-\t\treturn\n-\t}\n-\thdr := []byte(\"NATS/1.0 409 Leadership Change\\r\\n\\r\\n\")\n-\tfor reply := range o.prm {\n-\t\to.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n-\t}\n-\to.prm = nil\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+        if o.mset == nil || o.outq == nil {\n+                return\n+        }\n+        hdr := []byte(\"NATS/1.0 409 Leadership Change\\r\\n\\r\\n\")\n+        for reply := range o.prm {\n+                o.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n+        }\n+        o.prm = nil\n }\n \n // Process a NAK.\n func (o *consumer) processNak(sseq, dseq, dc uint64, nak []byte) {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\t// Check for out of range.\n-\tif dseq <= o.adflr || dseq > o.dseq {\n-\t\treturn\n-\t}\n-\t// If we are explicit ack make sure this is still on our pending list.\n-\tif len(o.pending) > 0 {\n-\t\tif _, ok := o.pending[sseq]; !ok {\n-\t\t\treturn\n-\t\t}\n-\t}\n-\t// Check to see if we have delays attached.\n-\tif len(nak) > len(AckNak) {\n-\t\targ := bytes.TrimSpace(nak[len(AckNak):])\n-\t\tif len(arg) > 0 {\n-\t\t\tvar d time.Duration\n-\t\t\tvar err error\n-\t\t\tif arg[0] == '{' {\n-\t\t\t\tvar nd ConsumerNakOptions\n-\t\t\t\tif err = json.Unmarshal(arg, &nd); err == nil {\n-\t\t\t\t\td = nd.Delay\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\td, err = time.ParseDuration(string(arg))\n-\t\t\t}\n-\t\t\tif err != nil {\n-\t\t\t\t// Treat this as normal NAK.\n-\t\t\t\to.srv.Warnf(\"JetStream consumer '%s > %s > %s' bad NAK delay value: %q\", o.acc.Name, o.stream, o.name, arg)\n-\t\t\t} else {\n-\t\t\t\t// We have a parsed duration that the user wants us to wait before retrying.\n-\t\t\t\t// Make sure we are not on the rdq.\n-\t\t\t\to.removeFromRedeliverQueue(sseq)\n-\t\t\t\tif p, ok := o.pending[sseq]; ok {\n-\t\t\t\t\t// now - ackWait is expired now, so offset from there.\n-\t\t\t\t\tp.Timestamp = time.Now().Add(-o.cfg.AckWait).Add(d).UnixNano()\n-\t\t\t\t\t// Update store system which will update followers as well.\n-\t\t\t\t\to.updateDelivered(p.Sequence, sseq, dc, p.Timestamp)\n-\t\t\t\t\tif o.ptmr != nil {\n-\t\t\t\t\t\t// Want checkPending to run and figure out the next timer ttl.\n-\t\t\t\t\t\t// TODO(dlc) - We could optimize this maybe a bit more and track when we expect the timer to fire.\n-\t\t\t\t\t\to.ptmr.Reset(10 * time.Millisecond)\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\t// Nothing else for use to do now so return.\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// If already queued up also ignore.\n-\tif !o.onRedeliverQueue(sseq) {\n-\t\to.addToRedeliverQueue(sseq)\n-\t}\n-\n-\to.signalNewMessages()\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        // Check for out of range.\n+        if dseq <= o.adflr || dseq > o.dseq {\n+                return\n+        }\n+        // If we are explicit ack make sure this is still on our pending list.\n+        if len(o.pending) > 0 {\n+                if _, ok := o.pending[sseq]; !ok {\n+                        return\n+                }\n+        }\n+        // Check to see if we have delays attached.\n+        if len(nak) > len(AckNak) {\n+                arg := bytes.TrimSpace(nak[len(AckNak):])\n+                if len(arg) > 0 {\n+                        var d time.Duration\n+                        var err error\n+                        if arg[0] == '{' {\n+                                var nd ConsumerNakOptions\n+                                if err = json.Unmarshal(arg, &nd); err == nil {\n+                                        d = nd.Delay\n+                                }\n+                        } else {\n+                                d, err = time.ParseDuration(string(arg))\n+                        }\n+                        if err != nil {\n+                                // Treat this as normal NAK.\n+                                o.srv.Warnf(\"JetStream consumer '%s > %s > %s' bad NAK delay value: %q\", o.acc.Name, o.stream, o.name, arg)\n+                        } else {\n+                                // We have a parsed duration that the user wants us to wait before retrying.\n+                                // Make sure we are not on the rdq.\n+                                o.removeFromRedeliverQueue(sseq)\n+                                if p, ok := o.pending[sseq]; ok {\n+                                        // now - ackWait is expired now, so offset from there.\n+                                        p.Timestamp = time.Now().Add(-o.cfg.AckWait).Add(d).UnixNano()\n+                                        // Update store system which will update followers as well.\n+                                        o.updateDelivered(p.Sequence, sseq, dc, p.Timestamp)\n+                                        if o.ptmr != nil {\n+                                                // Want checkPending to run and figure out the next timer ttl.\n+                                                // TODO(dlc) - We could optimize this maybe a bit more and track when we expect the timer to fire.\n+                                                o.ptmr.Reset(10 * time.Millisecond)\n+                                        }\n+                                }\n+                                // Nothing else for use to do now so return.\n+                                return\n+                        }\n+                }\n+        }\n+\n+        // If already queued up also ignore.\n+        if !o.onRedeliverQueue(sseq) {\n+                o.addToRedeliverQueue(sseq)\n+        }\n+\n+        o.signalNewMessages()\n }\n \n // Process a TERM\n func (o *consumer) processTerm(sseq, dseq, dc uint64) {\n-\t// Treat like an ack to suppress redelivery.\n-\to.processAckMsg(sseq, dseq, dc, false)\n-\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\t// Deliver an advisory\n-\te := JSConsumerDeliveryTerminatedAdvisory{\n-\t\tTypedEvent: TypedEvent{\n-\t\t\tType: JSConsumerDeliveryTerminatedAdvisoryType,\n-\t\t\tID:   nuid.Next(),\n-\t\t\tTime: time.Now().UTC(),\n-\t\t},\n-\t\tStream:      o.stream,\n-\t\tConsumer:    o.name,\n-\t\tConsumerSeq: dseq,\n-\t\tStreamSeq:   sseq,\n-\t\tDeliveries:  dc,\n-\t\tDomain:      o.srv.getOpts().JetStreamDomain,\n-\t}\n-\n-\tj, err := json.Marshal(e)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tsubj := JSAdvisoryConsumerMsgTerminatedPre + \".\" + o.stream + \".\" + o.name\n-\to.sendAdvisory(subj, j)\n+        // Treat like an ack to suppress redelivery.\n+        o.processAckMsg(sseq, dseq, dc, false)\n+\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        // Deliver an advisory\n+        e := JSConsumerDeliveryTerminatedAdvisory{\n+                TypedEvent: TypedEvent{\n+                        Type: JSConsumerDeliveryTerminatedAdvisoryType,\n+                        ID:   nuid.Next(),\n+                        Time: time.Now().UTC(),\n+                },\n+                Stream:      o.stream,\n+                Consumer:    o.name,\n+                ConsumerSeq: dseq,\n+                StreamSeq:   sseq,\n+                Deliveries:  dc,\n+                Domain:      o.srv.getOpts().JetStreamDomain,\n+        }\n+\n+        j, err := json.Marshal(e)\n+        if err != nil {\n+                return\n+        }\n+\n+        subj := JSAdvisoryConsumerMsgTerminatedPre + \".\" + o.stream + \".\" + o.name\n+        o.sendAdvisory(subj, j)\n }\n \n // Introduce a small delay in when timer fires to check pending.\n@@ -1715,1119 +1716,1119 @@ const ackWaitDelay = time.Millisecond\n \n // ackWait returns how long to wait to fire the pending timer.\n func (o *consumer) ackWait(next time.Duration) time.Duration {\n-\tif next > 0 {\n-\t\treturn next + ackWaitDelay\n-\t}\n-\treturn o.cfg.AckWait + ackWaitDelay\n+        if next > 0 {\n+                return next + ackWaitDelay\n+        }\n+        return o.cfg.AckWait + ackWaitDelay\n }\n \n // Due to bug in calculation of sequences on restoring redelivered let's do quick sanity check.\n func (o *consumer) checkRedelivered() {\n-\tvar lseq uint64\n-\tif mset := o.mset; mset != nil {\n-\t\tlseq = mset.lastSeq()\n-\t}\n-\tvar shouldUpdateState bool\n-\tfor sseq := range o.rdc {\n-\t\tif sseq < o.asflr || sseq > lseq {\n-\t\t\tdelete(o.rdc, sseq)\n-\t\t\to.removeFromRedeliverQueue(sseq)\n-\t\t\tshouldUpdateState = true\n-\t\t}\n-\t}\n-\tif shouldUpdateState {\n-\t\to.writeStoreStateUnlocked()\n-\t}\n+        var lseq uint64\n+        if mset := o.mset; mset != nil {\n+                lseq = mset.lastSeq()\n+        }\n+        var shouldUpdateState bool\n+        for sseq := range o.rdc {\n+                if sseq < o.asflr || sseq > lseq {\n+                        delete(o.rdc, sseq)\n+                        o.removeFromRedeliverQueue(sseq)\n+                        shouldUpdateState = true\n+                }\n+        }\n+        if shouldUpdateState {\n+                o.writeStoreStateUnlocked()\n+        }\n }\n \n // This will restore the state from disk.\n // Lock should be held.\n func (o *consumer) readStoredState() error {\n-\tif o.store == nil {\n-\t\treturn nil\n-\t}\n-\tstate, err := o.store.State()\n-\tif err == nil && state != nil {\n-\t\to.applyState(state)\n-\t\tif len(o.rdc) > 0 {\n-\t\t\to.checkRedelivered()\n-\t\t}\n-\t}\n-\treturn err\n+        if o.store == nil {\n+                return nil\n+        }\n+        state, err := o.store.State()\n+        if err == nil && state != nil {\n+                o.applyState(state)\n+                if len(o.rdc) > 0 {\n+                        o.checkRedelivered()\n+                }\n+        }\n+        return err\n }\n \n // Apply the consumer stored state.\n func (o *consumer) applyState(state *ConsumerState) {\n-\tif state == nil {\n-\t\treturn\n-\t}\n-\n-\to.dseq = state.Delivered.Consumer + 1\n-\to.sseq = state.Delivered.Stream + 1\n-\to.adflr = state.AckFloor.Consumer\n-\to.asflr = state.AckFloor.Stream\n-\to.pending = state.Pending\n-\to.rdc = state.Redelivered\n-\n-\t// Setup tracking timer if we have restored pending.\n-\tif len(o.pending) > 0 && o.ptmr == nil {\n-\t\t// This is on startup or leader change. We want to check pending\n-\t\t// sooner in case there are inconsistencies etc. Pick between 500ms - 1.5s\n-\t\tdelay := 500*time.Millisecond + time.Duration(rand.Int63n(1000))*time.Millisecond\n-\t\t// If normal is lower than this just use that.\n-\t\tif o.cfg.AckWait < delay {\n-\t\t\tdelay = o.ackWait(0)\n-\t\t}\n-\t\to.ptmr = time.AfterFunc(delay, o.checkPending)\n-\t}\n+        if state == nil {\n+                return\n+        }\n+\n+        o.dseq = state.Delivered.Consumer + 1\n+        o.sseq = state.Delivered.Stream + 1\n+        o.adflr = state.AckFloor.Consumer\n+        o.asflr = state.AckFloor.Stream\n+        o.pending = state.Pending\n+        o.rdc = state.Redelivered\n+\n+        // Setup tracking timer if we have restored pending.\n+        if len(o.pending) > 0 && o.ptmr == nil {\n+                // This is on startup or leader change. We want to check pending\n+                // sooner in case there are inconsistencies etc. Pick between 500ms - 1.5s\n+                delay := 500*time.Millisecond + time.Duration(rand.Int63n(1000))*time.Millisecond\n+                // If normal is lower than this just use that.\n+                if o.cfg.AckWait < delay {\n+                        delay = o.ackWait(0)\n+                }\n+                o.ptmr = time.AfterFunc(delay, o.checkPending)\n+        }\n }\n \n func (o *consumer) readStoreState() *ConsumerState {\n-\to.mu.RLock()\n-\tdefer o.mu.RUnlock()\n-\tif o.store == nil {\n-\t\treturn nil\n-\t}\n-\tstate, _ := o.store.State()\n-\treturn state\n+        o.mu.RLock()\n+        defer o.mu.RUnlock()\n+        if o.store == nil {\n+                return nil\n+        }\n+        state, _ := o.store.State()\n+        return state\n }\n \n // Sets our store state from another source. Used in clustered mode on snapshot restore.\n func (o *consumer) setStoreState(state *ConsumerState) error {\n-\tif state == nil || o.store == nil {\n-\t\treturn nil\n-\t}\n-\to.applyState(state)\n-\treturn o.store.Update(state)\n+        if state == nil || o.store == nil {\n+                return nil\n+        }\n+        o.applyState(state)\n+        return o.store.Update(state)\n }\n \n // Update our state to the store.\n func (o *consumer) writeStoreState() error {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\treturn o.writeStoreStateUnlocked()\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+        return o.writeStoreStateUnlocked()\n }\n \n // Update our state to the store.\n // Lock should be held.\n func (o *consumer) writeStoreStateUnlocked() error {\n-\tif o.store == nil {\n-\t\treturn nil\n-\t}\n-\n-\tstate := ConsumerState{\n-\t\tDelivered: SequencePair{\n-\t\t\tConsumer: o.dseq - 1,\n-\t\t\tStream:   o.sseq - 1,\n-\t\t},\n-\t\tAckFloor: SequencePair{\n-\t\t\tConsumer: o.adflr,\n-\t\t\tStream:   o.asflr,\n-\t\t},\n-\t\tPending:     o.pending,\n-\t\tRedelivered: o.rdc,\n-\t}\n-\treturn o.store.Update(&state)\n+        if o.store == nil {\n+                return nil\n+        }\n+\n+        state := ConsumerState{\n+                Delivered: SequencePair{\n+                        Consumer: o.dseq - 1,\n+                        Stream:   o.sseq - 1,\n+                },\n+                AckFloor: SequencePair{\n+                        Consumer: o.adflr,\n+                        Stream:   o.asflr,\n+                },\n+                Pending:     o.pending,\n+                Redelivered: o.rdc,\n+        }\n+        return o.store.Update(&state)\n }\n \n // Info returns our current consumer state.\n func (o *consumer) info() *ConsumerInfo {\n-\to.mu.RLock()\n-\tmset := o.mset\n-\tif mset == nil || mset.srv == nil {\n-\t\to.mu.RUnlock()\n-\t\treturn nil\n-\t}\n-\tjs := o.js\n-\to.mu.RUnlock()\n-\n-\tif js == nil {\n-\t\treturn nil\n-\t}\n-\n-\tci := js.clusterInfo(o.raftGroup())\n-\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\tcfg := o.cfg\n-\tinfo := &ConsumerInfo{\n-\t\tStream:  o.stream,\n-\t\tName:    o.name,\n-\t\tCreated: o.created,\n-\t\tConfig:  &cfg,\n-\t\tDelivered: SequenceInfo{\n-\t\t\tConsumer: o.dseq - 1,\n-\t\t\tStream:   o.sseq - 1,\n-\t\t},\n-\t\tAckFloor: SequenceInfo{\n-\t\t\tConsumer: o.adflr,\n-\t\t\tStream:   o.asflr,\n-\t\t},\n-\t\tNumAckPending:  len(o.pending),\n-\t\tNumRedelivered: len(o.rdc),\n-\t\tNumPending:     o.adjustedPending(),\n-\t\tPushBound:      o.isPushMode() && o.active,\n-\t\tCluster:        ci,\n-\t}\n-\t// Adjust active based on non-zero etc. Also make UTC here.\n-\tif !o.ldt.IsZero() {\n-\t\tldt := o.ldt.UTC() // This copies as well.\n-\t\tinfo.Delivered.Last = &ldt\n-\t}\n-\tif !o.lat.IsZero() {\n-\t\tlat := o.lat.UTC() // This copies as well.\n-\t\tinfo.AckFloor.Last = &lat\n-\t}\n-\n-\t// If we are a pull mode consumer, report on number of waiting requests.\n-\tif o.isPullMode() {\n-\t\to.expireWaiting()\n-\t\tinfo.NumWaiting = o.waiting.len()\n-\t}\n-\treturn info\n+        o.mu.RLock()\n+        mset := o.mset\n+        if mset == nil || mset.srv == nil {\n+                o.mu.RUnlock()\n+                return nil\n+        }\n+        js := o.js\n+        o.mu.RUnlock()\n+\n+        if js == nil {\n+                return nil\n+        }\n+\n+        ci := js.clusterInfo(o.raftGroup())\n+\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        cfg := o.cfg\n+        info := &ConsumerInfo{\n+                Stream:  o.stream,\n+                Name:    o.name,\n+                Created: o.created,\n+                Config:  &cfg,\n+                Delivered: SequenceInfo{\n+                        Consumer: o.dseq - 1,\n+                        Stream:   o.sseq - 1,\n+                },\n+                AckFloor: SequenceInfo{\n+                        Consumer: o.adflr,\n+                        Stream:   o.asflr,\n+                },\n+                NumAckPending:  len(o.pending),\n+                NumRedelivered: len(o.rdc),\n+                NumPending:     o.adjustedPending(),\n+                PushBound:      o.isPushMode() && o.active,\n+                Cluster:        ci,\n+        }\n+        // Adjust active based on non-zero etc. Also make UTC here.\n+        if !o.ldt.IsZero() {\n+                ldt := o.ldt.UTC() // This copies as well.\n+                info.Delivered.Last = &ldt\n+        }\n+        if !o.lat.IsZero() {\n+                lat := o.lat.UTC() // This copies as well.\n+                info.AckFloor.Last = &lat\n+        }\n+\n+        // If we are a pull mode consumer, report on number of waiting requests.\n+        if o.isPullMode() {\n+                o.expireWaiting()\n+                info.NumWaiting = o.waiting.len()\n+        }\n+        return info\n }\n \n // Will signal us that new messages are available. Will break out of waiting.\n func (o *consumer) signalNewMessages() {\n-\t// Kick our new message channel\n-\tselect {\n-\tcase o.mch <- struct{}{}:\n-\tdefault:\n-\t}\n+        // Kick our new message channel\n+        select {\n+        case o.mch <- struct{}{}:\n+        default:\n+        }\n }\n \n // shouldSample lets us know if we are sampling metrics on acks.\n func (o *consumer) shouldSample() bool {\n-\tswitch {\n-\tcase o.sfreq <= 0:\n-\t\treturn false\n-\tcase o.sfreq >= 100:\n-\t\treturn true\n-\t}\n+        switch {\n+        case o.sfreq <= 0:\n+                return false\n+        case o.sfreq >= 100:\n+                return true\n+        }\n \n-\t// TODO(ripienaar) this is a tad slow so we need to rethink here, however this will only\n-\t// hit for those with sampling enabled and its not the default\n-\treturn rand.Int31n(100) <= o.sfreq\n+        // TODO(ripienaar) this is a tad slow so we need to rethink here, however this will only\n+        // hit for those with sampling enabled and its not the default\n+        return rand.Int31n(100) <= o.sfreq\n }\n \n func (o *consumer) sampleAck(sseq, dseq, dc uint64) {\n-\tif !o.shouldSample() {\n-\t\treturn\n-\t}\n-\n-\tnow := time.Now().UTC()\n-\tunow := now.UnixNano()\n-\n-\te := JSConsumerAckMetric{\n-\t\tTypedEvent: TypedEvent{\n-\t\t\tType: JSConsumerAckMetricType,\n-\t\t\tID:   nuid.Next(),\n-\t\t\tTime: now,\n-\t\t},\n-\t\tStream:      o.stream,\n-\t\tConsumer:    o.name,\n-\t\tConsumerSeq: dseq,\n-\t\tStreamSeq:   sseq,\n-\t\tDelay:       unow - o.pending[sseq].Timestamp,\n-\t\tDeliveries:  dc,\n-\t\tDomain:      o.srv.getOpts().JetStreamDomain,\n-\t}\n-\n-\tj, err := json.Marshal(e)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\to.sendAdvisory(o.ackEventT, j)\n+        if !o.shouldSample() {\n+                return\n+        }\n+\n+        now := time.Now().UTC()\n+        unow := now.UnixNano()\n+\n+        e := JSConsumerAckMetric{\n+                TypedEvent: TypedEvent{\n+                        Type: JSConsumerAckMetricType,\n+                        ID:   nuid.Next(),\n+                        Time: now,\n+                },\n+                Stream:      o.stream,\n+                Consumer:    o.name,\n+                ConsumerSeq: dseq,\n+                StreamSeq:   sseq,\n+                Delay:       unow - o.pending[sseq].Timestamp,\n+                Deliveries:  dc,\n+                Domain:      o.srv.getOpts().JetStreamDomain,\n+        }\n+\n+        j, err := json.Marshal(e)\n+        if err != nil {\n+                return\n+        }\n+\n+        o.sendAdvisory(o.ackEventT, j)\n }\n \n func (o *consumer) processAckMsg(sseq, dseq, dc uint64, doSample bool) {\n-\to.mu.Lock()\n-\tvar sagap uint64\n-\tvar needSignal bool\n-\n-\tswitch o.cfg.AckPolicy {\n-\tcase AckExplicit:\n-\t\tif p, ok := o.pending[sseq]; ok {\n-\t\t\tif doSample {\n-\t\t\t\to.sampleAck(sseq, dseq, dc)\n-\t\t\t}\n-\t\t\tif o.maxp > 0 && len(o.pending) >= o.maxp {\n-\t\t\t\tneedSignal = true\n-\t\t\t}\n-\t\t\tdelete(o.pending, sseq)\n-\t\t\t// Use the original deliver sequence from our pending record.\n-\t\t\tdseq = p.Sequence\n-\t\t}\n-\t\tif len(o.pending) == 0 {\n-\t\t\to.adflr, o.asflr = o.dseq-1, o.sseq-1\n-\t\t} else if dseq == o.adflr+1 {\n-\t\t\to.adflr, o.asflr = dseq, sseq\n-\t\t\tfor ss := sseq + 1; ss < o.sseq; ss++ {\n-\t\t\t\tif p, ok := o.pending[ss]; ok {\n-\t\t\t\t\tif p.Sequence > 0 {\n-\t\t\t\t\t\to.adflr, o.asflr = p.Sequence-1, ss-1\n-\t\t\t\t\t}\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\t// We do these regardless.\n-\t\tdelete(o.rdc, sseq)\n-\t\to.removeFromRedeliverQueue(sseq)\n-\tcase AckAll:\n-\t\t// no-op\n-\t\tif dseq <= o.adflr || sseq <= o.asflr {\n-\t\t\to.mu.Unlock()\n-\t\t\treturn\n-\t\t}\n-\t\tif o.maxp > 0 && len(o.pending) >= o.maxp {\n-\t\t\tneedSignal = true\n-\t\t}\n-\t\tsagap = sseq - o.asflr\n-\t\to.adflr, o.asflr = dseq, sseq\n-\t\tfor seq := sseq; seq > sseq-sagap; seq-- {\n-\t\t\tdelete(o.pending, seq)\n-\t\t\tdelete(o.rdc, seq)\n-\t\t\to.removeFromRedeliverQueue(seq)\n-\t\t}\n-\tcase AckNone:\n-\t\t// FIXME(dlc) - This is error but do we care?\n-\t\to.mu.Unlock()\n-\t\treturn\n-\t}\n-\n-\t// Update underlying store.\n-\to.updateAcks(dseq, sseq)\n-\n-\tmset := o.mset\n-\tclustered := o.node != nil\n-\to.mu.Unlock()\n-\n-\t// Let the owning stream know if we are interest or workqueue retention based.\n-\t// If this consumer is clustered this will be handled by processReplicatedAck\n-\t// after the ack has propagated.\n-\tif !clustered && mset != nil && mset.cfg.Retention != LimitsPolicy {\n-\t\tif sagap > 1 {\n-\t\t\t// FIXME(dlc) - This is very inefficient, will need to fix.\n-\t\t\tfor seq := sseq; seq > sseq-sagap; seq-- {\n-\t\t\t\tmset.ackMsg(o, seq)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tmset.ackMsg(o, sseq)\n-\t\t}\n-\t}\n-\n-\t// If we had max ack pending set and were at limit we need to unblock folks.\n-\tif needSignal {\n-\t\to.signalNewMessages()\n-\t}\n+        o.mu.Lock()\n+        var sagap uint64\n+        var needSignal bool\n+\n+        switch o.cfg.AckPolicy {\n+        case AckExplicit:\n+                if p, ok := o.pending[sseq]; ok {\n+                        if doSample {\n+                                o.sampleAck(sseq, dseq, dc)\n+                        }\n+                        if o.maxp > 0 && len(o.pending) >= o.maxp {\n+                                needSignal = true\n+                        }\n+                        delete(o.pending, sseq)\n+                        // Use the original deliver sequence from our pending record.\n+                        dseq = p.Sequence\n+                }\n+                if len(o.pending) == 0 {\n+                        o.adflr, o.asflr = o.dseq-1, o.sseq-1\n+                } else if dseq == o.adflr+1 {\n+                        o.adflr, o.asflr = dseq, sseq\n+                        for ss := sseq + 1; ss < o.sseq; ss++ {\n+                                if p, ok := o.pending[ss]; ok {\n+                                        if p.Sequence > 0 {\n+                                                o.adflr, o.asflr = p.Sequence-1, ss-1\n+                                        }\n+                                        break\n+                                }\n+                        }\n+                }\n+                // We do these regardless.\n+                delete(o.rdc, sseq)\n+                o.removeFromRedeliverQueue(sseq)\n+        case AckAll:\n+                // no-op\n+                if dseq <= o.adflr || sseq <= o.asflr {\n+                        o.mu.Unlock()\n+                        return\n+                }\n+                if o.maxp > 0 && len(o.pending) >= o.maxp {\n+                        needSignal = true\n+                }\n+                sagap = sseq - o.asflr\n+                o.adflr, o.asflr = dseq, sseq\n+                for seq := sseq; seq > sseq-sagap; seq-- {\n+                        delete(o.pending, seq)\n+                        delete(o.rdc, seq)\n+                        o.removeFromRedeliverQueue(seq)\n+                }\n+        case AckNone:\n+                // FIXME(dlc) - This is error but do we care?\n+                o.mu.Unlock()\n+                return\n+        }\n+\n+        // Update underlying store.\n+        o.updateAcks(dseq, sseq)\n+\n+        mset := o.mset\n+        clustered := o.node != nil\n+        o.mu.Unlock()\n+\n+        // Let the owning stream know if we are interest or workqueue retention based.\n+        // If this consumer is clustered this will be handled by processReplicatedAck\n+        // after the ack has propagated.\n+        if !clustered && mset != nil && mset.cfg.Retention != LimitsPolicy {\n+                if sagap > 1 {\n+                        // FIXME(dlc) - This is very inefficient, will need to fix.\n+                        for seq := sseq; seq > sseq-sagap; seq-- {\n+                                mset.ackMsg(o, seq)\n+                        }\n+                } else {\n+                        mset.ackMsg(o, sseq)\n+                }\n+        }\n+\n+        // If we had max ack pending set and were at limit we need to unblock folks.\n+        if needSignal {\n+                o.signalNewMessages()\n+        }\n }\n \n // Determine if this is a truly filtered consumer. Modern clients will place filtered subjects\n // even if the stream only has a single non-wildcard subject designation.\n // Read lock should be held.\n func (o *consumer) isFiltered() bool {\n-\tif o.cfg.FilterSubject == _EMPTY_ {\n-\t\treturn false\n-\t}\n-\t// If we are here we want to check if the filtered subject is\n-\t// a direct match for our only listed subject.\n-\tmset := o.mset\n-\tif mset == nil {\n-\t\treturn true\n-\t}\n-\tif len(mset.cfg.Subjects) > 1 {\n-\t\treturn true\n-\t}\n-\treturn o.cfg.FilterSubject != mset.cfg.Subjects[0]\n+        if o.cfg.FilterSubject == _EMPTY_ {\n+                return false\n+        }\n+        // If we are here we want to check if the filtered subject is\n+        // a direct match for our only listed subject.\n+        mset := o.mset\n+        if mset == nil {\n+                return true\n+        }\n+        if len(mset.cfg.Subjects) > 1 {\n+                return true\n+        }\n+        return o.cfg.FilterSubject != mset.cfg.Subjects[0]\n }\n \n // Check if we need an ack for this store seq.\n // This is called for interest based retention streams to remove messages.\n func (o *consumer) needAck(sseq uint64) bool {\n-\tvar needAck bool\n-\tvar asflr, osseq uint64\n-\tvar pending map[uint64]*Pending\n-\to.mu.RLock()\n-\tif o.isLeader() {\n-\t\tasflr, osseq = o.asflr, o.sseq\n-\t\tpending = o.pending\n-\t} else {\n-\t\tif o.store == nil {\n-\t\t\to.mu.RUnlock()\n-\t\t\treturn false\n-\t\t}\n-\t\tstate, err := o.store.State()\n-\t\tif err != nil || state == nil {\n-\t\t\t// Fall back to what we track internally for now.\n-\t\t\tneedAck := sseq > o.asflr && !o.isFiltered()\n-\t\t\to.mu.RUnlock()\n-\t\t\treturn needAck\n-\t\t}\n-\t\tasflr, osseq = state.AckFloor.Stream, o.sseq\n-\t\tpending = state.Pending\n-\t}\n-\tswitch o.cfg.AckPolicy {\n-\tcase AckNone, AckAll:\n-\t\tneedAck = sseq > asflr\n-\tcase AckExplicit:\n-\t\tif sseq > asflr {\n-\t\t\t// Generally this means we need an ack, but just double check pending acks.\n-\t\t\tneedAck = true\n-\t\t\tif sseq < osseq {\n-\t\t\t\tif len(pending) == 0 {\n-\t\t\t\t\tneedAck = false\n-\t\t\t\t} else {\n-\t\t\t\t\t_, needAck = pending[sseq]\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\to.mu.RUnlock()\n-\treturn needAck\n+        var needAck bool\n+        var asflr, osseq uint64\n+        var pending map[uint64]*Pending\n+        o.mu.RLock()\n+        if o.isLeader() {\n+                asflr, osseq = o.asflr, o.sseq\n+                pending = o.pending\n+        } else {\n+                if o.store == nil {\n+                        o.mu.RUnlock()\n+                        return false\n+                }\n+                state, err := o.store.State()\n+                if err != nil || state == nil {\n+                        // Fall back to what we track internally for now.\n+                        needAck := sseq > o.asflr && !o.isFiltered()\n+                        o.mu.RUnlock()\n+                        return needAck\n+                }\n+                asflr, osseq = state.AckFloor.Stream, o.sseq\n+                pending = state.Pending\n+        }\n+        switch o.cfg.AckPolicy {\n+        case AckNone, AckAll:\n+                needAck = sseq > asflr\n+        case AckExplicit:\n+                if sseq > asflr {\n+                        // Generally this means we need an ack, but just double check pending acks.\n+                        needAck = true\n+                        if sseq < osseq {\n+                                if len(pending) == 0 {\n+                                        needAck = false\n+                                } else {\n+                                        _, needAck = pending[sseq]\n+                                }\n+                        }\n+                }\n+        }\n+        o.mu.RUnlock()\n+        return needAck\n }\n \n // Helper for the next message requests.\n func nextReqFromMsg(msg []byte) (time.Time, int, bool, error) {\n-\treq := bytes.TrimSpace(msg)\n-\n-\tswitch {\n-\tcase len(req) == 0:\n-\t\treturn time.Time{}, 1, false, nil\n-\n-\tcase req[0] == '{':\n-\t\tvar cr JSApiConsumerGetNextRequest\n-\t\tif err := json.Unmarshal(req, &cr); err != nil {\n-\t\t\treturn time.Time{}, -1, false, err\n-\t\t}\n-\t\tif cr.Expires == time.Duration(0) {\n-\t\t\treturn time.Time{}, cr.Batch, cr.NoWait, nil\n-\t\t}\n-\t\treturn time.Now().Add(cr.Expires), cr.Batch, cr.NoWait, nil\n-\tdefault:\n-\t\tif n, err := strconv.Atoi(string(req)); err == nil {\n-\t\t\treturn time.Time{}, n, false, nil\n-\t\t}\n-\t}\n-\n-\treturn time.Time{}, 1, false, nil\n+        req := bytes.TrimSpace(msg)\n+\n+        switch {\n+        case len(req) == 0:\n+                return time.Time{}, 1, false, nil\n+\n+        case req[0] == '{':\n+                var cr JSApiConsumerGetNextRequest\n+                if err := json.Unmarshal(req, &cr); err != nil {\n+                        return time.Time{}, -1, false, err\n+                }\n+                if cr.Expires == time.Duration(0) {\n+                        return time.Time{}, cr.Batch, cr.NoWait, nil\n+                }\n+                return time.Now().Add(cr.Expires), cr.Batch, cr.NoWait, nil\n+        default:\n+                if n, err := strconv.Atoi(string(req)); err == nil {\n+                        return time.Time{}, n, false, nil\n+                }\n+        }\n+\n+        return time.Time{}, 1, false, nil\n }\n \n // Represents a request that is on the internal waiting queue\n type waitingRequest struct {\n-\tacc      *Account\n-\tinterest string\n-\treply    string\n-\tn        int // For batching\n-\td        int\n-\texpires  time.Time\n-\treceived time.Time\n-\tnoWait   bool\n+        acc      *Account\n+        interest string\n+        reply    string\n+        n        int // For batching\n+        d        int\n+        expires  time.Time\n+        received time.Time\n+        noWait   bool\n }\n \n // sync.Pool for waiting requests.\n var wrPool = sync.Pool{\n-\tNew: func() interface{} {\n-\t\treturn new(waitingRequest)\n-\t},\n+        New: func() interface{} {\n+                return new(waitingRequest)\n+        },\n }\n \n // Recycle this request. This request can not be accessed after this call.\n func (wr *waitingRequest) recycleIfDone() bool {\n-\tif wr != nil && wr.n <= 0 {\n-\t\twr.acc, wr.interest, wr.reply = nil, _EMPTY_, _EMPTY_\n-\t\twrPool.Put(wr)\n-\t\treturn true\n-\t}\n-\treturn false\n+        if wr != nil && wr.n <= 0 {\n+                wr.acc, wr.interest, wr.reply = nil, _EMPTY_, _EMPTY_\n+                wrPool.Put(wr)\n+                return true\n+        }\n+        return false\n }\n \n // Force a recycle.\n func (wr *waitingRequest) recycle() {\n-\tif wr != nil {\n-\t\twr.acc, wr.interest, wr.reply = nil, _EMPTY_, _EMPTY_\n-\t\twrPool.Put(wr)\n-\t}\n+        if wr != nil {\n+                wr.acc, wr.interest, wr.reply = nil, _EMPTY_, _EMPTY_\n+                wrPool.Put(wr)\n+        }\n }\n \n // waiting queue for requests that are waiting for new messages to arrive.\n type waitQueue struct {\n-\trp, wp int\n-\tlast   time.Time\n-\tfexp   time.Time\n-\treqs   []*waitingRequest\n+        rp, wp int\n+        last   time.Time\n+        fexp   time.Time\n+        reqs   []*waitingRequest\n }\n \n // Create a new ring buffer with at most max items.\n func newWaitQueue(max int) *waitQueue {\n-\treturn &waitQueue{rp: -1, reqs: make([]*waitingRequest, max)}\n+        return &waitQueue{rp: -1, reqs: make([]*waitingRequest, max)}\n }\n \n var (\n-\terrWaitQueueFull = errors.New(\"wait queue is full\")\n-\terrWaitQueueNil  = errors.New(\"wait queue is nil\")\n+        errWaitQueueFull = errors.New(\"wait queue is full\")\n+        errWaitQueueNil  = errors.New(\"wait queue is nil\")\n )\n \n // Adds in a new request.\n func (wq *waitQueue) add(wr *waitingRequest) error {\n-\tif wq == nil {\n-\t\treturn errWaitQueueNil\n-\t}\n-\tif wq.isFull() {\n-\t\treturn errWaitQueueFull\n-\t}\n-\twq.reqs[wq.wp] = wr\n-\t// TODO(dlc) - Could make pow2 and get rid of mod.\n-\twq.wp = (wq.wp + 1) % cap(wq.reqs)\n-\n-\t// Adjust read pointer if we were empty.\n-\tif wq.rp < 0 {\n-\t\twq.rp = 0\n-\t}\n-\t// Track last active via when we receive a request.\n-\twq.last = wr.received\n-\t// Track next to expire for all pending requests.\n-\tif !wr.expires.IsZero() && (wq.fexp.IsZero() || wr.expires.Before(wq.fexp)) {\n-\t\twq.fexp = wr.expires\n-\t}\n-\treturn nil\n+        if wq == nil {\n+                return errWaitQueueNil\n+        }\n+        if wq.isFull() {\n+                return errWaitQueueFull\n+        }\n+        wq.reqs[wq.wp] = wr\n+        // TODO(dlc) - Could make pow2 and get rid of mod.\n+        wq.wp = (wq.wp + 1) % cap(wq.reqs)\n+\n+        // Adjust read pointer if we were empty.\n+        if wq.rp < 0 {\n+                wq.rp = 0\n+        }\n+        // Track last active via when we receive a request.\n+        wq.last = wr.received\n+        // Track next to expire for all pending requests.\n+        if !wr.expires.IsZero() && (wq.fexp.IsZero() || wr.expires.Before(wq.fexp)) {\n+                wq.fexp = wr.expires\n+        }\n+        return nil\n }\n \n func (wq *waitQueue) isFull() bool {\n-\treturn wq.rp == wq.wp\n+        return wq.rp == wq.wp\n }\n \n func (wq *waitQueue) isEmpty() bool {\n-\treturn wq.len() == 0\n+        return wq.len() == 0\n }\n \n func (wq *waitQueue) len() int {\n-\tif wq == nil || wq.rp < 0 {\n-\t\treturn 0\n-\t}\n-\tif wq.rp < wq.wp {\n-\t\treturn wq.wp - wq.rp\n-\t}\n-\treturn cap(wq.reqs) - wq.rp + wq.wp\n+        if wq == nil || wq.rp < 0 {\n+                return 0\n+        }\n+        if wq.rp < wq.wp {\n+                return wq.wp - wq.rp\n+        }\n+        return cap(wq.reqs) - wq.rp + wq.wp\n }\n \n // Peek will return the next request waiting or nil if empty.\n func (wq *waitQueue) peek() *waitingRequest {\n-\tif wq == nil {\n-\t\treturn nil\n-\t}\n-\tvar wr *waitingRequest\n-\tif wq.rp >= 0 {\n-\t\twr = wq.reqs[wq.rp]\n-\t}\n-\treturn wr\n+        if wq == nil {\n+                return nil\n+        }\n+        var wr *waitingRequest\n+        if wq.rp >= 0 {\n+                wr = wq.reqs[wq.rp]\n+        }\n+        return wr\n }\n \n // pop will return the next request and move the read cursor.\n func (wq *waitQueue) pop() *waitingRequest {\n-\twr := wq.peek()\n-\tif wr != nil {\n-\t\twr.d++\n-\t\twr.n--\n-\t\tif wr.n <= 0 {\n-\t\t\twq.removeCurrent()\n-\t\t}\n-\t}\n-\treturn wr\n+        wr := wq.peek()\n+        if wr != nil {\n+                wr.d++\n+                wr.n--\n+                if wr.n <= 0 {\n+                        wq.removeCurrent()\n+                }\n+        }\n+        return wr\n }\n \n // Removes the current read pointer (head FIFO) entry.\n func (wq *waitQueue) removeCurrent() {\n-\tif wq.rp < 0 {\n-\t\treturn\n-\t}\n-\twq.reqs[wq.rp] = nil\n-\twq.rp = (wq.rp + 1) % cap(wq.reqs)\n-\t// Check if we are empty.\n-\tif wq.rp == wq.wp {\n-\t\twq.rp, wq.wp = -1, 0\n-\t}\n+        if wq.rp < 0 {\n+                return\n+        }\n+        wq.reqs[wq.rp] = nil\n+        wq.rp = (wq.rp + 1) % cap(wq.reqs)\n+        // Check if we are empty.\n+        if wq.rp == wq.wp {\n+                wq.rp, wq.wp = -1, 0\n+        }\n }\n \n // Will compact when we have interior deletes.\n func (wq *waitQueue) compact() {\n-\tif wq.isEmpty() {\n-\t\treturn\n-\t}\n-\tnreqs, i := make([]*waitingRequest, cap(wq.reqs)), 0\n-\tfor rp := wq.rp; rp != wq.wp; rp = (rp + 1) % cap(wq.reqs) {\n-\t\tif wr := wq.reqs[rp]; wr != nil {\n-\t\t\tnreqs[i] = wr\n-\t\t\ti++\n-\t\t}\n-\t}\n-\t// Reset here.\n-\twq.rp, wq.wp, wq.reqs = 0, i, nreqs\n+        if wq.isEmpty() {\n+                return\n+        }\n+        nreqs, i := make([]*waitingRequest, cap(wq.reqs)), 0\n+        for rp := wq.rp; rp != wq.wp; rp = (rp + 1) % cap(wq.reqs) {\n+                if wr := wq.reqs[rp]; wr != nil {\n+                        nreqs[i] = wr\n+                        i++\n+                }\n+        }\n+        // Reset here.\n+        wq.rp, wq.wp, wq.reqs = 0, i, nreqs\n }\n \n // Return the replies for our pending requests.\n // No-op if push consumer or invalid etc.\n func (o *consumer) pendingRequestReplies() []string {\n-\to.mu.RLock()\n-\tdefer o.mu.RUnlock()\n-\tif o.waiting == nil {\n-\t\treturn nil\n-\t}\n-\twq, m := o.waiting, make(map[string]struct{})\n-\tfor rp := o.waiting.rp; o.waiting.rp >= 0 && rp != wq.wp; rp = (rp + 1) % cap(wq.reqs) {\n-\t\tif wr := wq.reqs[rp]; wr != nil {\n-\t\t\tm[wr.reply] = struct{}{}\n-\t\t}\n-\t}\n-\tvar replies []string\n-\tfor reply := range m {\n-\t\treplies = append(replies, reply)\n-\t}\n-\treturn replies\n+        o.mu.RLock()\n+        defer o.mu.RUnlock()\n+        if o.waiting == nil {\n+                return nil\n+        }\n+        wq, m := o.waiting, make(map[string]struct{})\n+        for rp := o.waiting.rp; o.waiting.rp >= 0 && rp != wq.wp; rp = (rp + 1) % cap(wq.reqs) {\n+                if wr := wq.reqs[rp]; wr != nil {\n+                        m[wr.reply] = struct{}{}\n+                }\n+        }\n+        var replies []string\n+        for reply := range m {\n+                replies = append(replies, reply)\n+        }\n+        return replies\n }\n \n // Return next waiting request. This will check for expirations but not noWait or interest.\n // That will be handled by expireWaiting.\n // Lock should be held.\n func (o *consumer) nextWaiting() *waitingRequest {\n-\tif o.waiting == nil || o.waiting.isEmpty() {\n-\t\treturn nil\n-\t}\n-\tfor wr := o.waiting.peek(); !o.waiting.isEmpty(); wr = o.waiting.peek() {\n-\t\tif wr == nil || wr.expires.IsZero() || time.Now().Before(wr.expires) {\n-\t\t\trr := wr.acc.sl.Match(wr.interest)\n-\t\t\tif len(rr.psubs)+len(rr.qsubs) > 0 {\n-\t\t\t\treturn o.waiting.pop()\n-\t\t\t} else if o.srv.gateway.enabled {\n-\t\t\t\tif o.srv.hasGatewayInterest(wr.acc.Name, wr.interest) || time.Since(wr.received) < defaultGatewayRecentSubExpiration {\n-\t\t\t\t\treturn o.waiting.pop()\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\thdr := []byte(\"NATS/1.0 408 Request Timeout\\r\\n\\r\\n\")\n-\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n-\t\t// Remove the current one, no longer valid.\n-\t\to.waiting.removeCurrent()\n-\t\tif o.node != nil {\n-\t\t\to.removeClusterPendingRequest(wr.reply)\n-\t\t}\n-\t\twr.recycle()\n-\t}\n-\treturn nil\n+        if o.waiting == nil || o.waiting.isEmpty() {\n+                return nil\n+        }\n+        for wr := o.waiting.peek(); !o.waiting.isEmpty(); wr = o.waiting.peek() {\n+                if wr == nil || wr.expires.IsZero() || time.Now().Before(wr.expires) {\n+                        rr := wr.acc.sl.Match(wr.interest)\n+                        if len(rr.psubs)+len(rr.qsubs) > 0 {\n+                                return o.waiting.pop()\n+                        } else if o.srv.gateway.enabled {\n+                                if o.srv.hasGatewayInterest(wr.acc.Name, wr.interest) || time.Since(wr.received) < defaultGatewayRecentSubExpiration {\n+                                        return o.waiting.pop()\n+                                }\n+                        }\n+                }\n+                hdr := []byte(\"NATS/1.0 408 Request Timeout\\r\\n\\r\\n\")\n+                o.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n+                // Remove the current one, no longer valid.\n+                o.waiting.removeCurrent()\n+                if o.node != nil {\n+                        o.removeClusterPendingRequest(wr.reply)\n+                }\n+                wr.recycle()\n+        }\n+        return nil\n }\n \n // processNextMsgReq will process a request for the next message available. A nil message payload means deliver\n // a single message. If the payload is a formal request or a number parseable with Atoi(), then we will send a\n // batch of messages without requiring another request to this endpoint, or an ACK.\n func (o *consumer) processNextMsgReq(_ *subscription, c *client, _ *Account, _, reply string, msg []byte) {\n-\tif reply == _EMPTY_ {\n-\t\treturn\n-\t}\n-\t_, msg = c.msgParts(msg)\n-\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\tmset := o.mset\n-\tif mset == nil {\n-\t\treturn\n-\t}\n-\n-\tsendErr := func(status int, description string) {\n-\t\thdr := []byte(fmt.Sprintf(\"NATS/1.0 %d %s\\r\\n\\r\\n\", status, description))\n-\t\to.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n-\t}\n-\n-\tif o.isPushMode() || o.waiting == nil {\n-\t\tsendErr(409, \"Consumer is push based\")\n-\t\treturn\n-\t}\n-\n-\t// Check payload here to see if they sent in batch size or a formal request.\n-\texpires, batchSize, noWait, err := nextReqFromMsg(msg)\n-\tif err != nil {\n-\t\tsendErr(400, fmt.Sprintf(\"Bad Request - %v\", err))\n-\t\treturn\n-\t}\n-\n-\t// Check for request limits\n-\tif o.cfg.MaxRequestBatch > 0 && batchSize > o.cfg.MaxRequestBatch {\n-\t\tsendErr(409, fmt.Sprintf(\"Exceeded MaxRequestBatch of %d\", o.cfg.MaxRequestBatch))\n-\t\treturn\n-\t}\n-\n-\tif !expires.IsZero() && o.cfg.MaxRequestExpires > 0 && expires.After(time.Now().Add(o.cfg.MaxRequestExpires)) {\n-\t\tsendErr(409, fmt.Sprintf(\"Exceeded MaxRequestExpires of %v\", o.cfg.MaxRequestExpires))\n-\t\treturn\n-\t}\n-\n-\t// If we have the max number of requests already pending try to expire.\n-\tif o.waiting.isFull() {\n-\t\t// Try to expire some of the requests.\n-\t\tif expired, _, _ := o.expireWaiting(); expired == 0 {\n-\t\t\t// Force expiration if needed.\n-\t\t\to.forceExpireFirstWaiting()\n-\t\t}\n-\t}\n-\n-\t// If the request is for noWait and we have pending requests already, check if we have room.\n-\tif noWait {\n-\t\tmsgsPending := o.adjustedPending() + uint64(len(o.rdq))\n-\t\t// If no pending at all, decide what to do with request.\n-\t\t// If no expires was set then fail.\n-\t\tif msgsPending == 0 && expires.IsZero() {\n-\t\t\tsendErr(404, \"No Messages\")\n-\t\t\treturn\n-\t\t}\n-\t\tif msgsPending > 0 {\n-\t\t\t_, _, batchPending := o.expireWaiting()\n-\t\t\tif msgsPending < uint64(batchPending) {\n-\t\t\t\tsendErr(408, \"Requests Pending\")\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\t// If we are here this should be considered a one-shot situation.\n-\t\t// We will wait for expires but will return as soon as we have any messages.\n-\t}\n-\n-\t// If we receive this request though an account export, we need to track that interest subject and account.\n-\tacc, interest := o.acc, reply\n-\tfor strings.HasPrefix(interest, replyPrefix) && acc.exports.responses != nil {\n-\t\tif si := acc.exports.responses[interest]; si != nil {\n-\t\t\tacc, interest = si.acc, si.to\n-\t\t} else {\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\n-\t// In case we have to queue up this request.\n-\twr := wrPool.Get().(*waitingRequest)\n-\twr.acc, wr.interest, wr.reply, wr.n, wr.d, wr.noWait, wr.expires = acc, interest, reply, batchSize, 0, noWait, expires\n-\twr.received = time.Now()\n-\n-\tif err := o.waiting.add(wr); err != nil {\n-\t\tsendErr(409, \"Exceeded MaxWaiting\")\n-\t\treturn\n-\t}\n-\to.signalNewMessages()\n-\t// If we are clustered update our followers about this request.\n-\tif o.node != nil {\n-\t\to.addClusterPendingRequest(wr.reply)\n-\t}\n+        if reply == _EMPTY_ {\n+                return\n+        }\n+        _, msg = c.msgParts(msg)\n+\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        mset := o.mset\n+        if mset == nil {\n+                return\n+        }\n+\n+        sendErr := func(status int, description string) {\n+                hdr := []byte(fmt.Sprintf(\"NATS/1.0 %d %s\\r\\n\\r\\n\", status, description))\n+                o.outq.send(newJSPubMsg(reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n+        }\n+\n+        if o.isPushMode() || o.waiting == nil {\n+                sendErr(409, \"Consumer is push based\")\n+                return\n+        }\n+\n+        // Check payload here to see if they sent in batch size or a formal request.\n+        expires, batchSize, noWait, err := nextReqFromMsg(msg)\n+        if err != nil {\n+                sendErr(400, fmt.Sprintf(\"Bad Request - %v\", err))\n+                return\n+        }\n+\n+        // Check for request limits\n+        if o.cfg.MaxRequestBatch > 0 && batchSize > o.cfg.MaxRequestBatch {\n+                sendErr(409, fmt.Sprintf(\"Exceeded MaxRequestBatch of %d\", o.cfg.MaxRequestBatch))\n+                return\n+        }\n+\n+        if !expires.IsZero() && o.cfg.MaxRequestExpires > 0 && expires.After(time.Now().Add(o.cfg.MaxRequestExpires)) {\n+                sendErr(409, fmt.Sprintf(\"Exceeded MaxRequestExpires of %v\", o.cfg.MaxRequestExpires))\n+                return\n+        }\n+\n+        // If we have the max number of requests already pending try to expire.\n+        if o.waiting.isFull() {\n+                // Try to expire some of the requests.\n+                if expired, _, _ := o.expireWaiting(); expired == 0 {\n+                        // Force expiration if needed.\n+                        o.forceExpireFirstWaiting()\n+                }\n+        }\n+\n+        // If the request is for noWait and we have pending requests already, check if we have room.\n+        if noWait {\n+                msgsPending := o.adjustedPending() + uint64(len(o.rdq))\n+                // If no pending at all, decide what to do with request.\n+                // If no expires was set then fail.\n+                if msgsPending == 0 && expires.IsZero() {\n+                        sendErr(404, \"No Messages\")\n+                        return\n+                }\n+                if msgsPending > 0 {\n+                        _, _, batchPending := o.expireWaiting()\n+                        if msgsPending < uint64(batchPending) {\n+                                sendErr(408, \"Requests Pending\")\n+                                return\n+                        }\n+                }\n+                // If we are here this should be considered a one-shot situation.\n+                // We will wait for expires but will return as soon as we have any messages.\n+        }\n+\n+        // If we receive this request though an account export, we need to track that interest subject and account.\n+        acc, interest := o.acc, reply\n+        for strings.HasPrefix(interest, replyPrefix) && acc.exports.responses != nil {\n+                if si := acc.exports.responses[interest]; si != nil {\n+                        acc, interest = si.acc, si.to\n+                } else {\n+                        break\n+                }\n+        }\n+\n+        // In case we have to queue up this request.\n+        wr := wrPool.Get().(*waitingRequest)\n+        wr.acc, wr.interest, wr.reply, wr.n, wr.d, wr.noWait, wr.expires = acc, interest, reply, batchSize, 0, noWait, expires\n+        wr.received = time.Now()\n+\n+        if err := o.waiting.add(wr); err != nil {\n+                sendErr(409, \"Exceeded MaxWaiting\")\n+                return\n+        }\n+        o.signalNewMessages()\n+        // If we are clustered update our followers about this request.\n+        if o.node != nil {\n+                o.addClusterPendingRequest(wr.reply)\n+        }\n }\n \n // Increase the delivery count for this message.\n // ONLY used on redelivery semantics.\n // Lock should be held.\n func (o *consumer) incDeliveryCount(sseq uint64) uint64 {\n-\tif o.rdc == nil {\n-\t\to.rdc = make(map[uint64]uint64)\n-\t}\n-\to.rdc[sseq] += 1\n-\treturn o.rdc[sseq] + 1\n+        if o.rdc == nil {\n+                o.rdc = make(map[uint64]uint64)\n+        }\n+        o.rdc[sseq] += 1\n+        return o.rdc[sseq] + 1\n }\n \n // send a delivery exceeded advisory.\n func (o *consumer) notifyDeliveryExceeded(sseq, dc uint64) {\n-\te := JSConsumerDeliveryExceededAdvisory{\n-\t\tTypedEvent: TypedEvent{\n-\t\t\tType: JSConsumerDeliveryExceededAdvisoryType,\n-\t\t\tID:   nuid.Next(),\n-\t\t\tTime: time.Now().UTC(),\n-\t\t},\n-\t\tStream:     o.stream,\n-\t\tConsumer:   o.name,\n-\t\tStreamSeq:  sseq,\n-\t\tDeliveries: dc,\n-\t\tDomain:     o.srv.getOpts().JetStreamDomain,\n-\t}\n-\n-\tj, err := json.Marshal(e)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\to.sendAdvisory(o.deliveryExcEventT, j)\n+        e := JSConsumerDeliveryExceededAdvisory{\n+                TypedEvent: TypedEvent{\n+                        Type: JSConsumerDeliveryExceededAdvisoryType,\n+                        ID:   nuid.Next(),\n+                        Time: time.Now().UTC(),\n+                },\n+                Stream:     o.stream,\n+                Consumer:   o.name,\n+                StreamSeq:  sseq,\n+                Deliveries: dc,\n+                Domain:     o.srv.getOpts().JetStreamDomain,\n+        }\n+\n+        j, err := json.Marshal(e)\n+        if err != nil {\n+                return\n+        }\n+\n+        o.sendAdvisory(o.deliveryExcEventT, j)\n }\n \n // Check to see if the candidate subject matches a filter if its present.\n // Lock should be held.\n func (o *consumer) isFilteredMatch(subj string) bool {\n-\t// No filter is automatic match.\n-\tif o.cfg.FilterSubject == _EMPTY_ {\n-\t\treturn true\n-\t}\n-\tif !o.filterWC {\n-\t\treturn subj == o.cfg.FilterSubject\n-\t}\n-\t// If we are here we have a wildcard filter subject.\n-\t// TODO(dlc) at speed might be better to just do a sublist with L2 and/or possibly L1.\n-\treturn subjectIsSubsetMatch(subj, o.cfg.FilterSubject)\n+        // No filter is automatic match.\n+        if o.cfg.FilterSubject == _EMPTY_ {\n+                return true\n+        }\n+        if !o.filterWC {\n+                return subj == o.cfg.FilterSubject\n+        }\n+        // If we are here we have a wildcard filter subject.\n+        // TODO(dlc) at speed might be better to just do a sublist with L2 and/or possibly L1.\n+        return subjectIsSubsetMatch(subj, o.cfg.FilterSubject)\n }\n \n var (\n-\terrMaxAckPending = errors.New(\"max ack pending reached\")\n-\terrBadConsumer   = errors.New(\"consumer not valid\")\n-\terrNoInterest    = errors.New(\"consumer requires interest for delivery subject when ephemeral\")\n+        errMaxAckPending = errors.New(\"max ack pending reached\")\n+        errBadConsumer   = errors.New(\"consumer not valid\")\n+        errNoInterest    = errors.New(\"consumer requires interest for delivery subject when ephemeral\")\n )\n \n // Get next available message from underlying store.\n // Is partition aware and redeliver aware.\n // Lock should be held.\n func (o *consumer) getNextMsg() (subj string, hdr, msg []byte, seq uint64, dc uint64, ts int64, err error) {\n-\tif o.mset == nil || o.mset.store == nil {\n-\t\treturn _EMPTY_, nil, nil, 0, 0, 0, errBadConsumer\n-\t}\n-\tfor {\n-\t\tseq, dc := o.sseq, uint64(1)\n-\t\tif o.hasSkipListPending() {\n-\t\t\tseq = o.lss.seqs[0]\n-\t\t\tif len(o.lss.seqs) == 1 {\n-\t\t\t\to.sseq = o.lss.resume\n-\t\t\t\to.lss = nil\n-\t\t\t\to.updateSkipped()\n-\t\t\t} else {\n-\t\t\t\to.lss.seqs = o.lss.seqs[1:]\n-\t\t\t}\n-\t\t} else if o.hasRedeliveries() {\n-\t\t\tseq = o.getNextToRedeliver()\n-\t\t\tdc = o.incDeliveryCount(seq)\n-\t\t\tif o.maxdc > 0 && dc > o.maxdc {\n-\t\t\t\t// Only send once\n-\t\t\t\tif dc == o.maxdc+1 {\n-\t\t\t\t\to.notifyDeliveryExceeded(seq, dc-1)\n-\t\t\t\t}\n-\t\t\t\t// Make sure to remove from pending.\n-\t\t\t\tdelete(o.pending, seq)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t} else if o.maxp > 0 && len(o.pending) >= o.maxp {\n-\t\t\t// maxp only set when ack policy != AckNone and user set MaxAckPending\n-\t\t\t// Stall if we have hit max pending.\n-\t\t\treturn _EMPTY_, nil, nil, 0, 0, 0, errMaxAckPending\n-\t\t}\n-\n-\t\tsubj, hdr, msg, ts, err := o.mset.store.LoadMsg(seq)\n-\t\tif err == nil {\n-\t\t\tif dc == 1 { // First delivery.\n-\t\t\t\to.sseq++\n-\t\t\t\tif o.cfg.FilterSubject != _EMPTY_ && !o.isFilteredMatch(subj) {\n-\t\t\t\t\to.updateSkipped()\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\t// We have the msg here.\n-\t\t\treturn subj, hdr, msg, seq, dc, ts, nil\n-\t\t}\n-\t\t// We got an error here. If this is an EOF we will return, otherwise\n-\t\t// we can continue looking.\n-\t\tif err == ErrStoreEOF || err == ErrStoreClosed || err == errNoCache || err == errPartialCache {\n-\t\t\treturn _EMPTY_, nil, nil, 0, 0, 0, err\n-\t\t}\n-\t\t// Skip since its deleted or expired.\n-\t\to.sseq++\n-\t}\n+        if o.mset == nil || o.mset.store == nil {\n+                return _EMPTY_, nil, nil, 0, 0, 0, errBadConsumer\n+        }\n+        for {\n+                seq, dc := o.sseq, uint64(1)\n+                if o.hasSkipListPending() {\n+                        seq = o.lss.seqs[0]\n+                        if len(o.lss.seqs) == 1 {\n+                                o.sseq = o.lss.resume\n+                                o.lss = nil\n+                                o.updateSkipped()\n+                        } else {\n+                                o.lss.seqs = o.lss.seqs[1:]\n+                        }\n+                } else if o.hasRedeliveries() {\n+                        seq = o.getNextToRedeliver()\n+                        dc = o.incDeliveryCount(seq)\n+                        if o.maxdc > 0 && dc > o.maxdc {\n+                                // Only send once\n+                                if dc == o.maxdc+1 {\n+                                        o.notifyDeliveryExceeded(seq, dc-1)\n+                                }\n+                                // Make sure to remove from pending.\n+                                delete(o.pending, seq)\n+                                continue\n+                        }\n+                } else if o.maxp > 0 && len(o.pending) >= o.maxp {\n+                        // maxp only set when ack policy != AckNone and user set MaxAckPending\n+                        // Stall if we have hit max pending.\n+                        return _EMPTY_, nil, nil, 0, 0, 0, errMaxAckPending\n+                }\n+\n+                subj, hdr, msg, ts, err := o.mset.store.LoadMsg(seq)\n+                if err == nil {\n+                        if dc == 1 { // First delivery.\n+                                o.sseq++\n+                                if o.cfg.FilterSubject != _EMPTY_ && !o.isFilteredMatch(subj) {\n+                                        o.updateSkipped()\n+                                        continue\n+                                }\n+                        }\n+                        // We have the msg here.\n+                        return subj, hdr, msg, seq, dc, ts, nil\n+                }\n+                // We got an error here. If this is an EOF we will return, otherwise\n+                // we can continue looking.\n+                if err == ErrStoreEOF || err == ErrStoreClosed || err == errNoCache || err == errPartialCache {\n+                        return _EMPTY_, nil, nil, 0, 0, 0, err\n+                }\n+                // Skip since its deleted or expired.\n+                o.sseq++\n+        }\n }\n \n // forceExpireFirstWaiting will force expire the first waiting.\n // Lock should be held.\n func (o *consumer) forceExpireFirstWaiting() {\n-\t// FIXME(dlc) - Should we do advisory here as well?\n-\twr := o.waiting.peek()\n-\tif wr == nil {\n-\t\treturn\n-\t}\n-\t// If we are expiring this and we think there is still interest, alert.\n-\tif rr := wr.acc.sl.Match(wr.interest); len(rr.psubs)+len(rr.qsubs) > 0 && o.mset != nil {\n-\t\t// We still appear to have interest, so send alert as courtesy.\n-\t\thdr := []byte(\"NATS/1.0 408 Request Canceled\\r\\n\\r\\n\")\n-\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n-\t}\n-\to.waiting.removeCurrent()\n-\tif o.node != nil {\n-\t\to.removeClusterPendingRequest(wr.reply)\n-\t}\n-\twr.recycle()\n+        // FIXME(dlc) - Should we do advisory here as well?\n+        wr := o.waiting.peek()\n+        if wr == nil {\n+                return\n+        }\n+        // If we are expiring this and we think there is still interest, alert.\n+        if rr := wr.acc.sl.Match(wr.interest); len(rr.psubs)+len(rr.qsubs) > 0 && o.mset != nil {\n+                // We still appear to have interest, so send alert as courtesy.\n+                hdr := []byte(\"NATS/1.0 408 Request Canceled\\r\\n\\r\\n\")\n+                o.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n+        }\n+        o.waiting.removeCurrent()\n+        if o.node != nil {\n+                o.removeClusterPendingRequest(wr.reply)\n+        }\n+        wr.recycle()\n }\n \n // Will check for expiration and lack of interest on waiting requests.\n func (o *consumer) expireWaiting() (int, int, int) {\n-\tif o.srv == nil || o.waiting.isEmpty() {\n-\t\treturn 0, 0, 0\n-\t}\n-\n-\tvar expired, brp int\n-\ts, now := o.srv, time.Now()\n-\n-\t// Signals interior deletes, which we will compact if needed.\n-\tvar hid bool\n-\tremove := func(wr *waitingRequest, i int) {\n-\t\tif i == o.waiting.rp {\n-\t\t\to.waiting.removeCurrent()\n-\t\t} else {\n-\t\t\to.waiting.reqs[i] = nil\n-\t\t\thid = true\n-\t\t}\n-\t\tif o.node != nil {\n-\t\t\to.removeClusterPendingRequest(wr.reply)\n-\t\t}\n-\t\texpired++\n-\t\twr.recycle()\n-\t}\n-\n-\twq := o.waiting\n-\twq.fexp = time.Time{}\n-\n-\tfor rp := o.waiting.rp; o.waiting.rp >= 0 && rp != wq.wp; rp = (rp + 1) % cap(wq.reqs) {\n-\t\twr := wq.reqs[rp]\n-\t\t// Check expiration.\n-\t\tif (wr.noWait && wr.d > 0) || (!wr.expires.IsZero() && now.After(wr.expires)) {\n-\t\t\thdr := []byte(\"NATS/1.0 408 Request Timeout\\r\\n\\r\\n\")\n-\t\t\to.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n-\t\t\tremove(wr, rp)\n-\t\t\tcontinue\n-\t\t}\n-\t\t// Now check interest.\n-\t\trr := wr.acc.sl.Match(wr.interest)\n-\t\tinterest := len(rr.psubs)+len(rr.qsubs) > 0\n-\t\tif !interest && s.gateway.enabled {\n-\t\t\t// If we are here check on gateways.\n-\t\t\t// If we have interest or the request is too young break and do not expire.\n-\t\t\tif s.hasGatewayInterest(wr.acc.Name, wr.interest) || time.Since(wr.received) < defaultGatewayRecentSubExpiration {\n-\t\t\t\tinterest = true\n-\t\t\t}\n-\t\t}\n-\t\t// If interest, update batch pending requests counter and update fexp timer.\n-\t\tif interest {\n-\t\t\tbrp += wr.n\n-\t\t\tif !wr.expires.IsZero() && (wq.fexp.IsZero() || wr.expires.Before(wq.fexp)) {\n-\t\t\t\twq.fexp = wr.expires\n-\t\t\t}\n-\t\t\tcontinue\n-\t\t}\n-\t\t// No more interest here so go ahead and remove this one from our list.\n-\t\tremove(wr, rp)\n-\t}\n-\n-\t// If we have interior deletes from out of order invalidation, compact the waiting queue.\n-\tif hid {\n-\t\to.waiting.compact()\n-\t}\n-\n-\treturn expired, o.waiting.len(), brp\n+        if o.srv == nil || o.waiting.isEmpty() {\n+                return 0, 0, 0\n+        }\n+\n+        var expired, brp int\n+        s, now := o.srv, time.Now()\n+\n+        // Signals interior deletes, which we will compact if needed.\n+        var hid bool\n+        remove := func(wr *waitingRequest, i int) {\n+                if i == o.waiting.rp {\n+                        o.waiting.removeCurrent()\n+                } else {\n+                        o.waiting.reqs[i] = nil\n+                        hid = true\n+                }\n+                if o.node != nil {\n+                        o.removeClusterPendingRequest(wr.reply)\n+                }\n+                expired++\n+                wr.recycle()\n+        }\n+\n+        wq := o.waiting\n+        wq.fexp = time.Time{}\n+\n+        for rp := o.waiting.rp; o.waiting.rp >= 0 && rp != wq.wp; rp = (rp + 1) % cap(wq.reqs) {\n+                wr := wq.reqs[rp]\n+                // Check expiration.\n+                if (wr.noWait && wr.d > 0) || (!wr.expires.IsZero() && now.After(wr.expires)) {\n+                        hdr := []byte(\"NATS/1.0 408 Request Timeout\\r\\n\\r\\n\")\n+                        o.outq.send(newJSPubMsg(wr.reply, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n+                        remove(wr, rp)\n+                        continue\n+                }\n+                // Now check interest.\n+                rr := wr.acc.sl.Match(wr.interest)\n+                interest := len(rr.psubs)+len(rr.qsubs) > 0\n+                if !interest && s.gateway.enabled {\n+                        // If we are here check on gateways.\n+                        // If we have interest or the request is too young break and do not expire.\n+                        if s.hasGatewayInterest(wr.acc.Name, wr.interest) || time.Since(wr.received) < defaultGatewayRecentSubExpiration {\n+                                interest = true\n+                        }\n+                }\n+                // If interest, update batch pending requests counter and update fexp timer.\n+                if interest {\n+                        brp += wr.n\n+                        if !wr.expires.IsZero() && (wq.fexp.IsZero() || wr.expires.Before(wq.fexp)) {\n+                                wq.fexp = wr.expires\n+                        }\n+                        continue\n+                }\n+                // No more interest here so go ahead and remove this one from our list.\n+                remove(wr, rp)\n+        }\n+\n+        // If we have interior deletes from out of order invalidation, compact the waiting queue.\n+        if hid {\n+                o.waiting.compact()\n+        }\n+\n+        return expired, o.waiting.len(), brp\n }\n \n // Will check to make sure those waiting still have registered interest.\n func (o *consumer) checkWaitingForInterest() bool {\n-\to.expireWaiting()\n-\treturn o.waiting.len() > 0\n+        o.expireWaiting()\n+        return o.waiting.len() > 0\n }\n \n // Lock should be held.\n func (o *consumer) hbTimer() (time.Duration, *time.Timer) {\n-\tif o.cfg.Heartbeat == 0 {\n-\t\treturn 0, nil\n-\t}\n-\treturn o.cfg.Heartbeat, time.NewTimer(o.cfg.Heartbeat)\n+        if o.cfg.Heartbeat == 0 {\n+                return 0, nil\n+        }\n+        return o.cfg.Heartbeat, time.NewTimer(o.cfg.Heartbeat)\n }\n \n func (o *consumer) loopAndGatherMsgs(qch chan struct{}) {\n-\t// On startup check to see if we are in a a reply situation where replay policy is not instant.\n-\tvar (\n-\t\tlts  int64 // last time stamp seen, used for replay.\n-\t\tlseq uint64\n-\t)\n-\n-\to.mu.Lock()\n-\ts := o.srv\n-\tif o.replay {\n-\t\t// consumer is closed when mset is set to nil.\n-\t\tif o.mset == nil {\n-\t\t\to.mu.Unlock()\n-\t\t\treturn\n-\t\t}\n-\t\tlseq = o.mset.state().LastSeq\n-\t}\n-\t// For idle heartbeat support.\n-\tvar hbc <-chan time.Time\n-\thbd, hb := o.hbTimer()\n-\tif hb != nil {\n-\t\thbc = hb.C\n-\t}\n-\t// Interest changes.\n-\tinch := o.inch\n-\to.mu.Unlock()\n-\n-\t// Deliver all the msgs we have now, once done or on a condition, we wait for new ones.\n-\tfor {\n-\t\tvar (\n-\t\t\tseq, dc     uint64\n-\t\t\tsubj, dsubj string\n-\t\t\thdr         []byte\n-\t\t\tmsg         []byte\n-\t\t\terr         error\n-\t\t\tts          int64\n-\t\t\tdelay       time.Duration\n-\t\t)\n-\n-\t\to.mu.Lock()\n-\t\t// consumer is closed when mset is set to nil.\n-\t\tif o.mset == nil {\n-\t\t\to.mu.Unlock()\n-\t\t\treturn\n-\t\t}\n-\n-\t\t// If we are in push mode and not active or under flowcontrol let's stop sending.\n-\t\tif o.isPushMode() {\n-\t\t\tif !o.active || (o.maxpb > 0 && o.pbytes > o.maxpb) {\n-\t\t\t\tgoto waitForMsgs\n-\t\t\t}\n-\t\t} else if o.waiting.isEmpty() {\n-\t\t\t// If we are in pull mode and no one is waiting already break and wait.\n-\t\t\tgoto waitForMsgs\n-\t\t}\n-\n-\t\tsubj, hdr, msg, seq, dc, ts, err = o.getNextMsg()\n-\n-\t\t// On error either wait or return.\n-\t\tif err != nil {\n-\t\t\tif err == ErrStoreMsgNotFound || err == ErrStoreEOF || err == errMaxAckPending || err == errPartialCache {\n-\t\t\t\tgoto waitForMsgs\n-\t\t\t} else {\n-\t\t\t\ts.Errorf(\"Received an error looking up message for consumer: %v\", err)\n-\t\t\t\tgoto waitForMsgs\n-\t\t\t}\n-\t\t}\n-\n-\t\tif o.isPushMode() {\n-\t\t\tdsubj = o.dsubj\n-\t\t} else if wr := o.nextWaiting(); wr != nil {\n-\t\t\tdsubj = wr.reply\n-\t\t\tif wr.recycleIfDone() && o.node != nil {\n-\t\t\t\to.removeClusterPendingRequest(dsubj)\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// We will redo this one.\n-\t\t\to.sseq--\n-\t\t\tgoto waitForMsgs\n-\t\t}\n-\n-\t\t// If we are in a replay scenario and have not caught up check if we need to delay here.\n-\t\tif o.replay && lts > 0 {\n-\t\t\tif delay = time.Duration(ts - lts); delay > time.Millisecond {\n-\t\t\t\to.mu.Unlock()\n-\t\t\t\tselect {\n-\t\t\t\tcase <-qch:\n-\t\t\t\t\treturn\n-\t\t\t\tcase <-time.After(delay):\n-\t\t\t\t}\n-\t\t\t\to.mu.Lock()\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Track this regardless.\n-\t\tlts = ts\n-\n-\t\t// If we have a rate limit set make sure we check that here.\n-\t\tif o.rlimit != nil {\n-\t\t\tnow := time.Now()\n-\t\t\tr := o.rlimit.ReserveN(now, len(msg)+len(hdr)+len(subj)+len(dsubj)+len(o.ackReplyT))\n-\t\t\tdelay := r.DelayFrom(now)\n-\t\t\tif delay > 0 {\n-\t\t\t\to.mu.Unlock()\n-\t\t\t\tselect {\n-\t\t\t\tcase <-qch:\n-\t\t\t\t\treturn\n-\t\t\t\tcase <-time.After(delay):\n-\t\t\t\t}\n-\t\t\t\to.mu.Lock()\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Do actual delivery.\n-\t\to.deliverMsg(dsubj, subj, hdr, msg, seq, dc, ts)\n-\n-\t\t// Reset our idle heartbeat timer if set.\n-\t\tif hb != nil {\n-\t\t\thb.Reset(hbd)\n-\t\t}\n-\n-\t\to.mu.Unlock()\n-\t\tcontinue\n-\n-\twaitForMsgs:\n-\t\t// If we were in a replay state check to see if we are caught up. If so clear.\n-\t\tif o.replay && o.sseq > lseq {\n-\t\t\to.replay = false\n-\t\t}\n-\n-\t\t// Make sure to process any expired requests that are pending.\n-\t\tvar wrExp <-chan time.Time\n-\t\tif o.isPullMode() {\n-\t\t\to.expireWaiting()\n-\t\t\tif o.waiting.len() > 0 && !o.waiting.fexp.IsZero() {\n-\t\t\t\texpires := time.Until(o.waiting.fexp)\n-\t\t\t\tif expires <= 0 {\n-\t\t\t\t\texpires = time.Millisecond\n-\t\t\t\t}\n-\t\t\t\twrExp = time.NewTimer(expires).C\n-\t\t\t}\n-\t\t}\n-\n-\t\t// We will wait here for new messages to arrive.\n-\t\tmch, outq, odsubj := o.mch, o.outq, o.cfg.DeliverSubject\n-\t\to.mu.Unlock()\n-\n-\t\tselect {\n-\t\tcase interest := <-inch:\n-\t\t\t// inch can be nil on pull-based, but then this will\n-\t\t\t// just block and not fire.\n-\t\t\to.updateDeliveryInterest(interest)\n-\t\tcase <-qch:\n-\t\t\treturn\n-\t\tcase <-mch:\n-\t\t\t// Messages are waiting.\n-\t\tcase <-wrExp:\n-\t\t\to.mu.Lock()\n-\t\t\to.expireWaiting()\n-\t\t\to.mu.Unlock()\n-\t\tcase <-hbc:\n-\t\t\tif o.isActive() {\n-\t\t\t\tconst t = \"NATS/1.0 100 Idle Heartbeat\\r\\n%s: %d\\r\\n%s: %d\\r\\n\\r\\n\"\n-\t\t\t\tsseq, dseq := o.lastDelivered()\n-\t\t\t\thdr := []byte(fmt.Sprintf(t, JSLastConsumerSeq, dseq, JSLastStreamSeq, sseq))\n-\t\t\t\tif fcp := o.fcID(); fcp != _EMPTY_ {\n-\t\t\t\t\t// Add in that we are stalled on flow control here.\n-\t\t\t\t\taddOn := []byte(fmt.Sprintf(\"%s: %s\\r\\n\\r\\n\", JSConsumerStalled, fcp))\n-\t\t\t\t\thdr = append(hdr[:len(hdr)-LEN_CR_LF], []byte(addOn)...)\n-\t\t\t\t}\n-\t\t\t\toutq.send(newJSPubMsg(odsubj, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n-\t\t\t}\n-\t\t\t// Reset our idle heartbeat timer.\n-\t\t\thb.Reset(hbd)\n-\t\t}\n-\t}\n+        // On startup check to see if we are in a a reply situation where replay policy is not instant.\n+        var (\n+                lts  int64 // last time stamp seen, used for replay.\n+                lseq uint64\n+        )\n+\n+        o.mu.Lock()\n+        s := o.srv\n+        if o.replay {\n+                // consumer is closed when mset is set to nil.\n+                if o.mset == nil {\n+                        o.mu.Unlock()\n+                        return\n+                }\n+                lseq = o.mset.state().LastSeq\n+        }\n+        // For idle heartbeat support.\n+        var hbc <-chan time.Time\n+        hbd, hb := o.hbTimer()\n+        if hb != nil {\n+                hbc = hb.C\n+        }\n+        // Interest changes.\n+        inch := o.inch\n+        o.mu.Unlock()\n+\n+        // Deliver all the msgs we have now, once done or on a condition, we wait for new ones.\n+        for {\n+                var (\n+                        seq, dc     uint64\n+                        subj, dsubj string\n+                        hdr         []byte\n+                        msg         []byte\n+                        err         error\n+                        ts          int64\n+                        delay       time.Duration\n+                )\n+\n+                o.mu.Lock()\n+                // consumer is closed when mset is set to nil.\n+                if o.mset == nil {\n+                        o.mu.Unlock()\n+                        return\n+                }\n+\n+                // If we are in push mode and not active or under flowcontrol let's stop sending.\n+                if o.isPushMode() {\n+                        if !o.active || (o.maxpb > 0 && o.pbytes > o.maxpb) {\n+                                goto waitForMsgs\n+                        }\n+                } else if o.waiting.isEmpty() {\n+                        // If we are in pull mode and no one is waiting already break and wait.\n+                        goto waitForMsgs\n+                }\n+\n+                subj, hdr, msg, seq, dc, ts, err = o.getNextMsg()\n+\n+                // On error either wait or return.\n+                if err != nil {\n+                        if err == ErrStoreMsgNotFound || err == ErrStoreEOF || err == errMaxAckPending || err == errPartialCache {\n+                                goto waitForMsgs\n+                        } else {\n+                                s.Errorf(\"Received an error looking up message for consumer: %v\", err)\n+                                goto waitForMsgs\n+                        }\n+                }\n+\n+                if o.isPushMode() {\n+                        dsubj = o.dsubj\n+                } else if wr := o.nextWaiting(); wr != nil {\n+                        dsubj = wr.reply\n+                        if wr.recycleIfDone() && o.node != nil {\n+                                o.removeClusterPendingRequest(dsubj)\n+                        }\n+                } else {\n+                        // We will redo this one.\n+                        o.sseq--\n+                        goto waitForMsgs\n+                }\n+\n+                // If we are in a replay scenario and have not caught up check if we need to delay here.\n+                if o.replay && lts > 0 {\n+                        if delay = time.Duration(ts - lts); delay > time.Millisecond {\n+                                o.mu.Unlock()\n+                                select {\n+                                case <-qch:\n+                                        return\n+                                case <-time.After(delay):\n+                                }\n+                                o.mu.Lock()\n+                        }\n+                }\n+\n+                // Track this regardless.\n+                lts = ts\n+\n+                // If we have a rate limit set make sure we check that here.\n+                if o.rlimit != nil {\n+                        now := time.Now()\n+                        r := o.rlimit.ReserveN(now, len(msg)+len(hdr)+len(subj)+len(dsubj)+len(o.ackReplyT))\n+                        delay := r.DelayFrom(now)\n+                        if delay > 0 {\n+                                o.mu.Unlock()\n+                                select {\n+                                case <-qch:\n+                                        return\n+                                case <-time.After(delay):\n+                                }\n+                                o.mu.Lock()\n+                        }\n+                }\n+\n+                // Do actual delivery.\n+                o.deliverMsg(dsubj, subj, hdr, msg, seq, dc, ts)\n+\n+                // Reset our idle heartbeat timer if set.\n+                if hb != nil {\n+                        hb.Reset(hbd)\n+                }\n+\n+                o.mu.Unlock()\n+                continue\n+\n+        waitForMsgs:\n+                // If we were in a replay state check to see if we are caught up. If so clear.\n+                if o.replay && o.sseq > lseq {\n+                        o.replay = false\n+                }\n+\n+                // Make sure to process any expired requests that are pending.\n+                var wrExp <-chan time.Time\n+                if o.isPullMode() {\n+                        o.expireWaiting()\n+                        if o.waiting.len() > 0 && !o.waiting.fexp.IsZero() {\n+                                expires := time.Until(o.waiting.fexp)\n+                                if expires <= 0 {\n+                                        expires = time.Millisecond\n+                                }\n+                                wrExp = time.NewTimer(expires).C\n+                        }\n+                }\n+\n+                // We will wait here for new messages to arrive.\n+                mch, outq, odsubj := o.mch, o.outq, o.cfg.DeliverSubject\n+                o.mu.Unlock()\n+\n+                select {\n+                case interest := <-inch:\n+                        // inch can be nil on pull-based, but then this will\n+                        // just block and not fire.\n+                        o.updateDeliveryInterest(interest)\n+                case <-qch:\n+                        return\n+                case <-mch:\n+                        // Messages are waiting.\n+                case <-wrExp:\n+                        o.mu.Lock()\n+                        o.expireWaiting()\n+                        o.mu.Unlock()\n+                case <-hbc:\n+                        if o.isActive() {\n+                                const t = \"NATS/1.0 100 Idle Heartbeat\\r\\n%s: %d\\r\\n%s: %d\\r\\n\\r\\n\"\n+                                sseq, dseq := o.lastDelivered()\n+                                hdr := []byte(fmt.Sprintf(t, JSLastConsumerSeq, dseq, JSLastStreamSeq, sseq))\n+                                if fcp := o.fcID(); fcp != _EMPTY_ {\n+                                        // Add in that we are stalled on flow control here.\n+                                        addOn := []byte(fmt.Sprintf(\"%s: %s\\r\\n\\r\\n\", JSConsumerStalled, fcp))\n+                                        hdr = append(hdr[:len(hdr)-LEN_CR_LF], []byte(addOn)...)\n+                                }\n+                                outq.send(newJSPubMsg(odsubj, _EMPTY_, _EMPTY_, hdr, nil, nil, 0))\n+                        }\n+                        // Reset our idle heartbeat timer.\n+                        hb.Reset(hbd)\n+                }\n+        }\n }\n \n func (o *consumer) lastDelivered() (sseq, dseq uint64) {\n-\to.mu.RLock()\n-\tdefer o.mu.RUnlock()\n-\treturn o.sseq - 1, o.dseq - 1\n+        o.mu.RLock()\n+        defer o.mu.RUnlock()\n+        return o.sseq - 1, o.dseq - 1\n }\n \n func (o *consumer) ackReply(sseq, dseq, dc uint64, ts int64, pending uint64) string {\n-\treturn fmt.Sprintf(o.ackReplyT, dc, sseq, dseq, ts, pending)\n+        return fmt.Sprintf(o.ackReplyT, dc, sseq, dseq, ts, pending)\n }\n \n // Used mostly for testing. Sets max pending bytes for flow control setups.\n func (o *consumer) setMaxPendingBytes(limit int) {\n-\to.pblimit = limit\n-\to.maxpb = limit / 16\n-\tif o.maxpb == 0 {\n-\t\to.maxpb = 1\n-\t}\n+        o.pblimit = limit\n+        o.maxpb = limit / 16\n+        if o.maxpb == 0 {\n+                o.maxpb = 1\n+        }\n }\n \n // We have the case where a consumer can become greedy and pick up a messages before the stream has incremented our pending(sgap).\n@@ -2835,908 +2836,914 @@ func (o *consumer) setMaxPendingBytes(limit int) {\n // This functions checks for that and returns 0.\n // Lock should be held.\n func (o *consumer) adjustedPending() uint64 {\n-\tif o.sgap&(1<<63) != 0 {\n-\t\treturn 0\n-\t}\n-\treturn o.sgap\n+        if o.sgap&(1<<63) != 0 {\n+                return 0\n+        }\n+        return o.sgap\n }\n \n // Deliver a msg to the consumer.\n // Lock should be held and o.mset validated to be non-nil.\n func (o *consumer) deliverMsg(dsubj, subj string, hdr, msg []byte, seq, dc uint64, ts int64) {\n-\tif o.mset == nil {\n-\t\treturn\n-\t}\n-\t// Update pending on first attempt. This can go upside down for a short bit, that is ok.\n-\t// See adjustedPending().\n-\tif dc == 1 {\n-\t\to.sgap--\n-\t}\n-\n-\tdseq := o.dseq\n-\to.dseq++\n-\n-\t// If headers only do not send msg payload.\n-\t// Add in msg size itself as header.\n-\tif o.cfg.HeadersOnly {\n-\t\tvar bb bytes.Buffer\n-\t\tif len(hdr) == 0 {\n-\t\t\tbb.WriteString(hdrLine)\n-\t\t} else {\n-\t\t\tbb.Write(hdr)\n-\t\t\tbb.Truncate(len(hdr) - LEN_CR_LF)\n-\t\t}\n-\t\tbb.WriteString(JSMsgSize)\n-\t\tbb.WriteString(\": \")\n-\t\tbb.WriteString(strconv.FormatInt(int64(len(msg)), 10))\n-\t\tbb.WriteString(CR_LF)\n-\t\tbb.WriteString(CR_LF)\n-\t\thdr = bb.Bytes()\n-\t\t// Cancel msg payload\n-\t\tmsg = nil\n-\t}\n-\n-\tpmsg := newJSPubMsg(dsubj, subj, o.ackReply(seq, dseq, dc, ts, o.adjustedPending()), hdr, msg, o, seq)\n-\tif o.maxpb > 0 {\n-\t\to.pbytes += pmsg.size()\n-\t}\n-\n-\tmset := o.mset\n-\tap := o.cfg.AckPolicy\n-\n-\t// Send message.\n-\to.outq.send(pmsg)\n-\n-\tif ap == AckExplicit || ap == AckAll {\n-\t\to.trackPending(seq, dseq)\n-\t} else if ap == AckNone {\n-\t\to.adflr = dseq\n-\t\to.asflr = seq\n-\t}\n-\n-\t// Flow control.\n-\tif o.maxpb > 0 && o.needFlowControl() {\n-\t\to.sendFlowControl()\n-\t}\n-\n-\t// FIXME(dlc) - Capture errors?\n-\to.updateDelivered(dseq, seq, dc, ts)\n-\n-\t// If we are ack none and mset is interest only we should make sure stream removes interest.\n-\tif ap == AckNone && mset.cfg.Retention != LimitsPolicy {\n-\t\tif o.node == nil || o.cfg.Direct {\n-\t\t\tmset.ackq.push(seq)\n-\t\t} else {\n-\t\t\to.updateAcks(dseq, seq)\n-\t\t}\n-\t}\n+        if o.mset == nil {\n+                return\n+        }\n+        // Update pending on first attempt. This can go upside down for a short bit, that is ok.\n+        // See adjustedPending().\n+        if dc == 1 {\n+                o.sgap--\n+        }\n+\n+        dseq := o.dseq\n+        o.dseq++\n+\n+        // If headers only do not send msg payload.\n+        // Add in msg size itself as header.\n+        if o.cfg.HeadersOnly {\n+                var bb bytes.Buffer\n+                if len(hdr) == 0 {\n+                        bb.WriteString(hdrLine)\n+                } else {\n+                        bb.Write(hdr)\n+                        bb.Truncate(len(hdr) - LEN_CR_LF)\n+                }\n+                bb.WriteString(JSMsgSize)\n+                bb.WriteString(\": \")\n+                bb.WriteString(strconv.FormatInt(int64(len(msg)), 10))\n+                bb.WriteString(CR_LF)\n+                bb.WriteString(CR_LF)\n+                hdr = bb.Bytes()\n+                // Cancel msg payload\n+                msg = nil\n+        }\n+\n+        pmsg := newJSPubMsg(dsubj, subj, o.ackReply(seq, dseq, dc, ts, o.adjustedPending()), hdr, msg, o, seq)\n+        if o.maxpb > 0 {\n+                o.pbytes += pmsg.size()\n+        }\n+\n+        mset := o.mset\n+        ap := o.cfg.AckPolicy\n+\n+        // Send message.\n+        o.outq.send(pmsg)\n+\n+        if ap == AckExplicit || ap == AckAll {\n+                o.trackPending(seq, dseq)\n+        } else if ap == AckNone {\n+                o.adflr = dseq\n+                o.asflr = seq\n+        }\n+\n+        // Flow control.\n+        if o.maxpb > 0 && o.needFlowControl() {\n+                o.sendFlowControl()\n+        }\n+\n+        // FIXME(dlc) - Capture errors?\n+        o.updateDelivered(dseq, seq, dc, ts)\n+\n+        // If we are ack none and mset is interest only we should make sure stream removes interest.\n+        if ap == AckNone && mset.cfg.Retention != LimitsPolicy {\n+                if o.node == nil || o.cfg.Direct {\n+                        mset.ackq.push(seq)\n+                } else {\n+                        o.updateAcks(dseq, seq)\n+                }\n+        }\n }\n \n func (o *consumer) needFlowControl() bool {\n-\tif o.maxpb == 0 {\n-\t\treturn false\n-\t}\n-\t// Decide whether to send a flow control message which we will need the user to respond.\n-\t// We send when we are over 50% of our current window limit.\n-\tif o.fcid == _EMPTY_ && o.pbytes > o.maxpb/2 {\n-\t\treturn true\n-\t}\n-\treturn false\n-}\n-\n-func (o *consumer) processFlowControl(_ *subscription, c *client, _ *Account, subj, _ string, _ []byte) {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\t// Ignore if not the latest we have sent out.\n-\tif subj != o.fcid {\n-\t\treturn\n-\t}\n-\n-\t// For slow starts and ramping up.\n-\tif o.maxpb < o.pblimit {\n-\t\to.maxpb *= 2\n-\t\tif o.maxpb > o.pblimit {\n-\t\t\to.maxpb = o.pblimit\n-\t\t}\n-\t}\n-\n-\t// Update accounting.\n-\to.pbytes -= o.fcsz\n-\tif o.pbytes < 0 {\n-\t\to.pbytes = 0\n-\t}\n-\to.fcid, o.fcsz = _EMPTY_, 0\n-\n-\to.signalNewMessages()\n+        if o.maxpb == 0 {\n+                return false\n+        }\n+        // Decide whether to send a flow control message which we will need the user to respond.\n+        // We send when we are over 50% of our current window limit.\n+        if o.fcid == _EMPTY_ && o.pbytes > o.maxpb/2 {\n+                return true\n+        }\n+        return false\n+}\n+\n+func (o *consumer) processFlowControl(_ *subscription, c *client, acc *Account, subj, _ string, _ []byte) {\n+o.mu.RLock()\n+myacc := o.acc\n+o.mu.RUnlock()\n+if acc != myacc {\n+return\n+}\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        // Ignore if not the latest we have sent out.\n+        if subj != o.fcid {\n+                return\n+        }\n+\n+        // For slow starts and ramping up.\n+        if o.maxpb < o.pblimit {\n+                o.maxpb *= 2\n+                if o.maxpb > o.pblimit {\n+                        o.maxpb = o.pblimit\n+                }\n+        }\n+\n+        // Update accounting.\n+        o.pbytes -= o.fcsz\n+        if o.pbytes < 0 {\n+                o.pbytes = 0\n+        }\n+        o.fcid, o.fcsz = _EMPTY_, 0\n+\n+        o.signalNewMessages()\n }\n \n // Lock should be held.\n func (o *consumer) fcReply() string {\n-\tvar sb strings.Builder\n-\tsb.WriteString(jsFlowControlPre)\n-\tsb.WriteString(o.stream)\n-\tsb.WriteByte(btsep)\n-\tsb.WriteString(o.name)\n-\tsb.WriteByte(btsep)\n-\tvar b [4]byte\n-\trn := rand.Int63()\n-\tfor i, l := 0, rn; i < len(b); i++ {\n-\t\tb[i] = digits[l%base]\n-\t\tl /= base\n-\t}\n-\tsb.Write(b[:])\n-\treturn sb.String()\n+        var sb strings.Builder\n+        sb.WriteString(jsFlowControlPre)\n+        sb.WriteString(o.stream)\n+        sb.WriteByte(btsep)\n+        sb.WriteString(o.name)\n+        sb.WriteByte(btsep)\n+        var b [4]byte\n+        rn := rand.Int63()\n+        for i, l := 0, rn; i < len(b); i++ {\n+                b[i] = digits[l%base]\n+                l /= base\n+        }\n+        sb.Write(b[:])\n+        return sb.String()\n }\n \n func (o *consumer) fcID() string {\n-\to.mu.RLock()\n-\tdefer o.mu.RUnlock()\n-\treturn o.fcid\n+        o.mu.RLock()\n+        defer o.mu.RUnlock()\n+        return o.fcid\n }\n \n // sendFlowControl will send a flow control packet to the consumer.\n // Lock should be held.\n func (o *consumer) sendFlowControl() {\n-\tif !o.isPushMode() {\n-\t\treturn\n-\t}\n-\tsubj, rply := o.cfg.DeliverSubject, o.fcReply()\n-\to.fcsz, o.fcid = o.pbytes, rply\n-\thdr := []byte(\"NATS/1.0 100 FlowControl Request\\r\\n\\r\\n\")\n-\to.outq.send(newJSPubMsg(subj, _EMPTY_, rply, hdr, nil, nil, 0))\n+        if !o.isPushMode() {\n+                return\n+        }\n+        subj, rply := o.cfg.DeliverSubject, o.fcReply()\n+        o.fcsz, o.fcid = o.pbytes, rply\n+        hdr := []byte(\"NATS/1.0 100 FlowControl Request\\r\\n\\r\\n\")\n+        o.outq.send(newJSPubMsg(subj, _EMPTY_, rply, hdr, nil, nil, 0))\n }\n \n // Tracks our outstanding pending acks. Only applicable to AckExplicit mode.\n // Lock should be held.\n func (o *consumer) trackPending(sseq, dseq uint64) {\n-\tif o.pending == nil {\n-\t\to.pending = make(map[uint64]*Pending)\n-\t}\n-\tif o.ptmr == nil {\n-\t\to.ptmr = time.AfterFunc(o.ackWait(0), o.checkPending)\n-\t}\n-\tif p, ok := o.pending[sseq]; ok {\n-\t\tp.Timestamp = time.Now().UnixNano()\n-\t} else {\n-\t\to.pending[sseq] = &Pending{dseq, time.Now().UnixNano()}\n-\t}\n+        if o.pending == nil {\n+                o.pending = make(map[uint64]*Pending)\n+        }\n+        if o.ptmr == nil {\n+                o.ptmr = time.AfterFunc(o.ackWait(0), o.checkPending)\n+        }\n+        if p, ok := o.pending[sseq]; ok {\n+                p.Timestamp = time.Now().UnixNano()\n+        } else {\n+                o.pending[sseq] = &Pending{dseq, time.Now().UnixNano()}\n+        }\n }\n \n // didNotDeliver is called when a delivery for a consumer message failed.\n // Depending on our state, we will process the failure.\n func (o *consumer) didNotDeliver(seq uint64) {\n-\to.mu.Lock()\n-\tmset := o.mset\n-\tif mset == nil {\n-\t\to.mu.Unlock()\n-\t\treturn\n-\t}\n-\tvar checkDeliveryInterest bool\n-\tif o.isPushMode() {\n-\t\to.active = false\n-\t\tcheckDeliveryInterest = true\n-\t} else if o.pending != nil {\n-\t\t// pull mode and we have pending.\n-\t\tif _, ok := o.pending[seq]; ok {\n-\t\t\t// We found this messsage on pending, we need\n-\t\t\t// to queue it up for immediate redelivery since\n-\t\t\t// we know it was not delivered.\n-\t\t\tif !o.onRedeliverQueue(seq) {\n-\t\t\t\to.addToRedeliverQueue(seq)\n-\t\t\t\to.signalNewMessages()\n-\t\t\t}\n-\t\t}\n-\t}\n-\to.mu.Unlock()\n-\n-\t// If we do not have interest update that here.\n-\tif checkDeliveryInterest && o.hasNoLocalInterest() {\n-\t\to.updateDeliveryInterest(false)\n-\t}\n+        o.mu.Lock()\n+        mset := o.mset\n+        if mset == nil {\n+                o.mu.Unlock()\n+                return\n+        }\n+        var checkDeliveryInterest bool\n+        if o.isPushMode() {\n+                o.active = false\n+                checkDeliveryInterest = true\n+        } else if o.pending != nil {\n+                // pull mode and we have pending.\n+                if _, ok := o.pending[seq]; ok {\n+                        // We found this messsage on pending, we need\n+                        // to queue it up for immediate redelivery since\n+                        // we know it was not delivered.\n+                        if !o.onRedeliverQueue(seq) {\n+                                o.addToRedeliverQueue(seq)\n+                                o.signalNewMessages()\n+                        }\n+                }\n+        }\n+        o.mu.Unlock()\n+\n+        // If we do not have interest update that here.\n+        if checkDeliveryInterest && o.hasNoLocalInterest() {\n+                o.updateDeliveryInterest(false)\n+        }\n }\n \n // Lock should be held.\n func (o *consumer) addToRedeliverQueue(seqs ...uint64) {\n-\tif o.rdqi == nil {\n-\t\to.rdqi = make(map[uint64]struct{})\n-\t}\n-\to.rdq = append(o.rdq, seqs...)\n-\tfor _, seq := range seqs {\n-\t\to.rdqi[seq] = struct{}{}\n-\t}\n+        if o.rdqi == nil {\n+                o.rdqi = make(map[uint64]struct{})\n+        }\n+        o.rdq = append(o.rdq, seqs...)\n+        for _, seq := range seqs {\n+                o.rdqi[seq] = struct{}{}\n+        }\n }\n \n // Lock should be held.\n func (o *consumer) hasRedeliveries() bool {\n-\treturn len(o.rdq) > 0\n+        return len(o.rdq) > 0\n }\n \n func (o *consumer) getNextToRedeliver() uint64 {\n-\tif len(o.rdq) == 0 {\n-\t\treturn 0\n-\t}\n-\tseq := o.rdq[0]\n-\tif len(o.rdq) == 1 {\n-\t\to.rdq, o.rdqi = nil, nil\n-\t} else {\n-\t\to.rdq = append(o.rdq[:0], o.rdq[1:]...)\n-\t\tdelete(o.rdqi, seq)\n-\t}\n-\treturn seq\n+        if len(o.rdq) == 0 {\n+                return 0\n+        }\n+        seq := o.rdq[0]\n+        if len(o.rdq) == 1 {\n+                o.rdq, o.rdqi = nil, nil\n+        } else {\n+                o.rdq = append(o.rdq[:0], o.rdq[1:]...)\n+                delete(o.rdqi, seq)\n+        }\n+        return seq\n }\n \n // This checks if we already have this sequence queued for redelivery.\n // FIXME(dlc) - This is O(n) but should be fast with small redeliver size.\n // Lock should be held.\n func (o *consumer) onRedeliverQueue(seq uint64) bool {\n-\tif o.rdqi == nil {\n-\t\treturn false\n-\t}\n-\t_, ok := o.rdqi[seq]\n-\treturn ok\n+        if o.rdqi == nil {\n+                return false\n+        }\n+        _, ok := o.rdqi[seq]\n+        return ok\n }\n \n // Remove a sequence from the redelivery queue.\n // Lock should be held.\n func (o *consumer) removeFromRedeliverQueue(seq uint64) bool {\n-\tif !o.onRedeliverQueue(seq) {\n-\t\treturn false\n-\t}\n-\tfor i, rseq := range o.rdq {\n-\t\tif rseq == seq {\n-\t\t\tif len(o.rdq) == 1 {\n-\t\t\t\to.rdq, o.rdqi = nil, nil\n-\t\t\t} else {\n-\t\t\t\to.rdq = append(o.rdq[:i], o.rdq[i+1:]...)\n-\t\t\t\tdelete(o.rdqi, seq)\n-\t\t\t}\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        if !o.onRedeliverQueue(seq) {\n+                return false\n+        }\n+        for i, rseq := range o.rdq {\n+                if rseq == seq {\n+                        if len(o.rdq) == 1 {\n+                                o.rdq, o.rdqi = nil, nil\n+                        } else {\n+                                o.rdq = append(o.rdq[:i], o.rdq[i+1:]...)\n+                                delete(o.rdqi, seq)\n+                        }\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // Checks the pending messages.\n func (o *consumer) checkPending() {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\tmset := o.mset\n-\tif mset == nil {\n-\t\treturn\n-\t}\n-\n-\tnow := time.Now().UnixNano()\n-\tttl := int64(o.cfg.AckWait)\n-\tnext := int64(o.ackWait(0))\n-\n-\tvar shouldUpdateState bool\n-\tvar state StreamState\n-\tmset.store.FastState(&state)\n-\tfseq := state.FirstSeq\n-\n-\t// Since we can update timestamps, we have to review all pending.\n-\t// We may want to unlock here or warn if list is big.\n-\tvar expired []uint64\n-\tfor seq, p := range o.pending {\n-\t\t// Check if these are no longer valid.\n-\t\tif seq < fseq {\n-\t\t\tdelete(o.pending, seq)\n-\t\t\tdelete(o.rdc, seq)\n-\t\t\to.removeFromRedeliverQueue(seq)\n-\t\t\tshouldUpdateState = true\n-\t\t\tcontinue\n-\t\t}\n-\t\telapsed, deadline := now-p.Timestamp, ttl\n-\t\tif len(o.cfg.BackOff) > 0 && o.rdc != nil {\n-\t\t\tdc := int(o.rdc[seq])\n-\t\t\tif dc >= len(o.cfg.BackOff) {\n-\t\t\t\tdc = len(o.cfg.BackOff) - 1\n-\t\t\t}\n-\t\t\tdeadline = int64(o.cfg.BackOff[dc])\n-\t\t}\n-\t\tif elapsed >= deadline {\n-\t\t\tif !o.onRedeliverQueue(seq) {\n-\t\t\t\texpired = append(expired, seq)\n-\t\t\t}\n-\t\t} else if deadline-elapsed < next {\n-\t\t\t// Update when we should fire next.\n-\t\t\tnext = deadline - elapsed\n-\t\t}\n-\t}\n-\n-\tif len(expired) > 0 {\n-\t\t// We need to sort.\n-\t\tsort.Slice(expired, func(i, j int) bool { return expired[i] < expired[j] })\n-\t\to.addToRedeliverQueue(expired...)\n-\t\t// Now we should update the timestamp here since we are redelivering.\n-\t\t// We will use an incrementing time to preserve order for any other redelivery.\n-\t\toff := now - o.pending[expired[0]].Timestamp\n-\t\tfor _, seq := range expired {\n-\t\t\tif p, ok := o.pending[seq]; ok {\n-\t\t\t\tp.Timestamp += off\n-\t\t\t}\n-\t\t}\n-\t\to.signalNewMessages()\n-\t}\n-\n-\tif len(o.pending) > 0 {\n-\t\to.ptmr.Reset(o.ackWait(time.Duration(next)))\n-\t} else {\n-\t\to.ptmr.Stop()\n-\t\to.ptmr = nil\n-\t}\n-\n-\t// Update our state if needed.\n-\tif shouldUpdateState {\n-\t\to.writeStoreStateUnlocked()\n-\t}\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        mset := o.mset\n+        if mset == nil {\n+                return\n+        }\n+\n+        now := time.Now().UnixNano()\n+        ttl := int64(o.cfg.AckWait)\n+        next := int64(o.ackWait(0))\n+\n+        var shouldUpdateState bool\n+        var state StreamState\n+        mset.store.FastState(&state)\n+        fseq := state.FirstSeq\n+\n+        // Since we can update timestamps, we have to review all pending.\n+        // We may want to unlock here or warn if list is big.\n+        var expired []uint64\n+        for seq, p := range o.pending {\n+                // Check if these are no longer valid.\n+                if seq < fseq {\n+                        delete(o.pending, seq)\n+                        delete(o.rdc, seq)\n+                        o.removeFromRedeliverQueue(seq)\n+                        shouldUpdateState = true\n+                        continue\n+                }\n+                elapsed, deadline := now-p.Timestamp, ttl\n+                if len(o.cfg.BackOff) > 0 && o.rdc != nil {\n+                        dc := int(o.rdc[seq])\n+                        if dc >= len(o.cfg.BackOff) {\n+                                dc = len(o.cfg.BackOff) - 1\n+                        }\n+                        deadline = int64(o.cfg.BackOff[dc])\n+                }\n+                if elapsed >= deadline {\n+                        if !o.onRedeliverQueue(seq) {\n+                                expired = append(expired, seq)\n+                        }\n+                } else if deadline-elapsed < next {\n+                        // Update when we should fire next.\n+                        next = deadline - elapsed\n+                }\n+        }\n+\n+        if len(expired) > 0 {\n+                // We need to sort.\n+                sort.Slice(expired, func(i, j int) bool { return expired[i] < expired[j] })\n+                o.addToRedeliverQueue(expired...)\n+                // Now we should update the timestamp here since we are redelivering.\n+                // We will use an incrementing time to preserve order for any other redelivery.\n+                off := now - o.pending[expired[0]].Timestamp\n+                for _, seq := range expired {\n+                        if p, ok := o.pending[seq]; ok {\n+                                p.Timestamp += off\n+                        }\n+                }\n+                o.signalNewMessages()\n+        }\n+\n+        if len(o.pending) > 0 {\n+                o.ptmr.Reset(o.ackWait(time.Duration(next)))\n+        } else {\n+                o.ptmr.Stop()\n+                o.ptmr = nil\n+        }\n+\n+        // Update our state if needed.\n+        if shouldUpdateState {\n+                o.writeStoreStateUnlocked()\n+        }\n }\n \n // SeqFromReply will extract a sequence number from a reply subject.\n func (o *consumer) seqFromReply(reply string) uint64 {\n-\t_, dseq, _ := ackReplyInfo(reply)\n-\treturn dseq\n+        _, dseq, _ := ackReplyInfo(reply)\n+        return dseq\n }\n \n // StreamSeqFromReply will extract the stream sequence from the reply subject.\n func (o *consumer) streamSeqFromReply(reply string) uint64 {\n-\tsseq, _, _ := ackReplyInfo(reply)\n-\treturn sseq\n+        sseq, _, _ := ackReplyInfo(reply)\n+        return sseq\n }\n \n // Quick parser for positive numbers in ack reply encoding.\n func parseAckReplyNum(d string) (n int64) {\n-\tif len(d) == 0 {\n-\t\treturn -1\n-\t}\n-\tfor _, dec := range d {\n-\t\tif dec < asciiZero || dec > asciiNine {\n-\t\t\treturn -1\n-\t\t}\n-\t\tn = n*10 + (int64(dec) - asciiZero)\n-\t}\n-\treturn n\n+        if len(d) == 0 {\n+                return -1\n+        }\n+        for _, dec := range d {\n+                if dec < asciiZero || dec > asciiNine {\n+                        return -1\n+                }\n+                n = n*10 + (int64(dec) - asciiZero)\n+        }\n+        return n\n }\n \n const expectedNumReplyTokens = 9\n \n // Grab encoded information in the reply subject for a delivered message.\n func replyInfo(subject string) (sseq, dseq, dc uint64, ts int64, pending uint64) {\n-\ttsa := [expectedNumReplyTokens]string{}\n-\tstart, tokens := 0, tsa[:0]\n-\tfor i := 0; i < len(subject); i++ {\n-\t\tif subject[i] == btsep {\n-\t\t\ttokens = append(tokens, subject[start:i])\n-\t\t\tstart = i + 1\n-\t\t}\n-\t}\n-\ttokens = append(tokens, subject[start:])\n-\tif len(tokens) != expectedNumReplyTokens || tokens[0] != \"$JS\" || tokens[1] != \"ACK\" {\n-\t\treturn 0, 0, 0, 0, 0\n-\t}\n-\t// TODO(dlc) - Should we error if we do not match consumer name?\n-\t// stream is tokens[2], consumer is 3.\n-\tdc = uint64(parseAckReplyNum(tokens[4]))\n-\tsseq, dseq = uint64(parseAckReplyNum(tokens[5])), uint64(parseAckReplyNum(tokens[6]))\n-\tts = parseAckReplyNum(tokens[7])\n-\tpending = uint64(parseAckReplyNum(tokens[8]))\n-\n-\treturn sseq, dseq, dc, ts, pending\n+        tsa := [expectedNumReplyTokens]string{}\n+        start, tokens := 0, tsa[:0]\n+        for i := 0; i < len(subject); i++ {\n+                if subject[i] == btsep {\n+                        tokens = append(tokens, subject[start:i])\n+                        start = i + 1\n+                }\n+        }\n+        tokens = append(tokens, subject[start:])\n+        if len(tokens) != expectedNumReplyTokens || tokens[0] != \"$JS\" || tokens[1] != \"ACK\" {\n+                return 0, 0, 0, 0, 0\n+        }\n+        // TODO(dlc) - Should we error if we do not match consumer name?\n+        // stream is tokens[2], consumer is 3.\n+        dc = uint64(parseAckReplyNum(tokens[4]))\n+        sseq, dseq = uint64(parseAckReplyNum(tokens[5])), uint64(parseAckReplyNum(tokens[6]))\n+        ts = parseAckReplyNum(tokens[7])\n+        pending = uint64(parseAckReplyNum(tokens[8]))\n+\n+        return sseq, dseq, dc, ts, pending\n }\n \n func ackReplyInfo(subject string) (sseq, dseq, dc uint64) {\n-\ttsa := [expectedNumReplyTokens]string{}\n-\tstart, tokens := 0, tsa[:0]\n-\tfor i := 0; i < len(subject); i++ {\n-\t\tif subject[i] == btsep {\n-\t\t\ttokens = append(tokens, subject[start:i])\n-\t\t\tstart = i + 1\n-\t\t}\n-\t}\n-\ttokens = append(tokens, subject[start:])\n-\tif len(tokens) != expectedNumReplyTokens || tokens[0] != \"$JS\" || tokens[1] != \"ACK\" {\n-\t\treturn 0, 0, 0\n-\t}\n-\tdc = uint64(parseAckReplyNum(tokens[4]))\n-\tsseq, dseq = uint64(parseAckReplyNum(tokens[5])), uint64(parseAckReplyNum(tokens[6]))\n-\n-\treturn sseq, dseq, dc\n+        tsa := [expectedNumReplyTokens]string{}\n+        start, tokens := 0, tsa[:0]\n+        for i := 0; i < len(subject); i++ {\n+                if subject[i] == btsep {\n+                        tokens = append(tokens, subject[start:i])\n+                        start = i + 1\n+                }\n+        }\n+        tokens = append(tokens, subject[start:])\n+        if len(tokens) != expectedNumReplyTokens || tokens[0] != \"$JS\" || tokens[1] != \"ACK\" {\n+                return 0, 0, 0\n+        }\n+        dc = uint64(parseAckReplyNum(tokens[4]))\n+        sseq, dseq = uint64(parseAckReplyNum(tokens[5])), uint64(parseAckReplyNum(tokens[6]))\n+\n+        return sseq, dseq, dc\n }\n \n // NextSeq returns the next delivered sequence number for this consumer.\n func (o *consumer) nextSeq() uint64 {\n-\to.mu.RLock()\n-\tdseq := o.dseq\n-\to.mu.RUnlock()\n-\treturn dseq\n+        o.mu.RLock()\n+        dseq := o.dseq\n+        o.mu.RUnlock()\n+        return dseq\n }\n \n // Used to hold skip list when deliver policy is last per subject.\n type lastSeqSkipList struct {\n-\tresume uint64\n-\tseqs   []uint64\n+        resume uint64\n+        seqs   []uint64\n }\n \n // Will create a skip list for us from a store's subjects state.\n func createLastSeqSkipList(mss map[string]SimpleState) []uint64 {\n-\tseqs := make([]uint64, 0, len(mss))\n-\tfor _, ss := range mss {\n-\t\tseqs = append(seqs, ss.Last)\n-\t}\n-\tsort.Slice(seqs, func(i, j int) bool { return seqs[i] < seqs[j] })\n-\treturn seqs\n+        seqs := make([]uint64, 0, len(mss))\n+        for _, ss := range mss {\n+                seqs = append(seqs, ss.Last)\n+        }\n+        sort.Slice(seqs, func(i, j int) bool { return seqs[i] < seqs[j] })\n+        return seqs\n }\n \n // Let's us know we have a skip list, which is for deliver last per subject and we are just starting.\n // Lock should be held.\n func (o *consumer) hasSkipListPending() bool {\n-\treturn o.lss != nil && len(o.lss.seqs) > 0\n+        return o.lss != nil && len(o.lss.seqs) > 0\n }\n \n // Will select the starting sequence.\n func (o *consumer) selectStartingSeqNo() {\n-\tif o.mset == nil || o.mset.store == nil {\n-\t\to.sseq = 1\n-\t} else {\n-\t\tvar state StreamState\n-\t\to.mset.store.FastState(&state)\n-\t\tif o.cfg.OptStartSeq == 0 {\n-\t\t\tif o.cfg.DeliverPolicy == DeliverAll {\n-\t\t\t\to.sseq = state.FirstSeq\n-\t\t\t} else if o.cfg.DeliverPolicy == DeliverLast {\n-\t\t\t\to.sseq = state.LastSeq\n-\t\t\t\t// If we are partitioned here this will be properly set when we become leader.\n-\t\t\t\tif o.cfg.FilterSubject != _EMPTY_ {\n-\t\t\t\t\tss := o.mset.store.FilteredState(1, o.cfg.FilterSubject)\n-\t\t\t\t\to.sseq = ss.Last\n-\t\t\t\t}\n-\t\t\t} else if o.cfg.DeliverPolicy == DeliverLastPerSubject {\n-\t\t\t\tif mss := o.mset.store.SubjectsState(o.cfg.FilterSubject); len(mss) > 0 {\n-\t\t\t\t\to.lss = &lastSeqSkipList{\n-\t\t\t\t\t\tresume: state.LastSeq,\n-\t\t\t\t\t\tseqs:   createLastSeqSkipList(mss),\n-\t\t\t\t\t}\n-\t\t\t\t\to.sseq = o.lss.seqs[0]\n-\t\t\t\t} else {\n-\t\t\t\t\t// If no mapping info just set to last.\n-\t\t\t\t\to.sseq = state.LastSeq\n-\t\t\t\t}\n-\t\t\t} else if o.cfg.OptStartTime != nil {\n-\t\t\t\t// If we are here we are time based.\n-\t\t\t\t// TODO(dlc) - Once clustered can't rely on this.\n-\t\t\t\to.sseq = o.mset.store.GetSeqFromTime(*o.cfg.OptStartTime)\n-\t\t\t} else {\n-\t\t\t\to.sseq = state.LastSeq + 1\n-\t\t\t}\n-\t\t} else {\n-\t\t\to.sseq = o.cfg.OptStartSeq\n-\t\t}\n-\n-\t\tif state.FirstSeq == 0 {\n-\t\t\to.sseq = 1\n-\t\t} else if o.sseq < state.FirstSeq {\n-\t\t\to.sseq = state.FirstSeq\n-\t\t} else if o.sseq > state.LastSeq {\n-\t\t\to.sseq = state.LastSeq + 1\n-\t\t}\n-\t}\n-\n-\t// Always set delivery sequence to 1.\n-\to.dseq = 1\n-\t// Set ack delivery floor to delivery-1\n-\to.adflr = o.dseq - 1\n-\t// Set ack store floor to store-1\n-\to.asflr = o.sseq - 1\n+        if o.mset == nil || o.mset.store == nil {\n+                o.sseq = 1\n+        } else {\n+                var state StreamState\n+                o.mset.store.FastState(&state)\n+                if o.cfg.OptStartSeq == 0 {\n+                        if o.cfg.DeliverPolicy == DeliverAll {\n+                                o.sseq = state.FirstSeq\n+                        } else if o.cfg.DeliverPolicy == DeliverLast {\n+                                o.sseq = state.LastSeq\n+                                // If we are partitioned here this will be properly set when we become leader.\n+                                if o.cfg.FilterSubject != _EMPTY_ {\n+                                        ss := o.mset.store.FilteredState(1, o.cfg.FilterSubject)\n+                                        o.sseq = ss.Last\n+                                }\n+                        } else if o.cfg.DeliverPolicy == DeliverLastPerSubject {\n+                                if mss := o.mset.store.SubjectsState(o.cfg.FilterSubject); len(mss) > 0 {\n+                                        o.lss = &lastSeqSkipList{\n+                                                resume: state.LastSeq,\n+                                                seqs:   createLastSeqSkipList(mss),\n+                                        }\n+                                        o.sseq = o.lss.seqs[0]\n+                                } else {\n+                                        // If no mapping info just set to last.\n+                                        o.sseq = state.LastSeq\n+                                }\n+                        } else if o.cfg.OptStartTime != nil {\n+                                // If we are here we are time based.\n+                                // TODO(dlc) - Once clustered can't rely on this.\n+                                o.sseq = o.mset.store.GetSeqFromTime(*o.cfg.OptStartTime)\n+                        } else {\n+                                o.sseq = state.LastSeq + 1\n+                        }\n+                } else {\n+                        o.sseq = o.cfg.OptStartSeq\n+                }\n+\n+                if state.FirstSeq == 0 {\n+                        o.sseq = 1\n+                } else if o.sseq < state.FirstSeq {\n+                        o.sseq = state.FirstSeq\n+                } else if o.sseq > state.LastSeq {\n+                        o.sseq = state.LastSeq + 1\n+                }\n+        }\n+\n+        // Always set delivery sequence to 1.\n+        o.dseq = 1\n+        // Set ack delivery floor to delivery-1\n+        o.adflr = o.dseq - 1\n+        // Set ack store floor to store-1\n+        o.asflr = o.sseq - 1\n }\n \n // Test whether a config represents a durable subscriber.\n func isDurableConsumer(config *ConsumerConfig) bool {\n-\treturn config != nil && config.Durable != _EMPTY_\n+        return config != nil && config.Durable != _EMPTY_\n }\n \n func (o *consumer) isDurable() bool {\n-\treturn o.cfg.Durable != _EMPTY_\n+        return o.cfg.Durable != _EMPTY_\n }\n \n // Are we in push mode, delivery subject, etc.\n func (o *consumer) isPushMode() bool {\n-\treturn o.cfg.DeliverSubject != _EMPTY_\n+        return o.cfg.DeliverSubject != _EMPTY_\n }\n \n func (o *consumer) isPullMode() bool {\n-\treturn o.cfg.DeliverSubject == _EMPTY_\n+        return o.cfg.DeliverSubject == _EMPTY_\n }\n \n // Name returns the name of this consumer.\n func (o *consumer) String() string {\n-\to.mu.RLock()\n-\tn := o.name\n-\to.mu.RUnlock()\n-\treturn n\n+        o.mu.RLock()\n+        n := o.name\n+        o.mu.RUnlock()\n+        return n\n }\n \n func createConsumerName() string {\n-\treturn string(getHash(nuid.Next()))\n+        return string(getHash(nuid.Next()))\n }\n \n // deleteConsumer will delete the consumer from this stream.\n func (mset *stream) deleteConsumer(o *consumer) error {\n-\treturn o.delete()\n+        return o.delete()\n }\n \n func (o *consumer) streamName() string {\n-\to.mu.RLock()\n-\tmset := o.mset\n-\to.mu.RUnlock()\n-\tif mset != nil {\n-\t\treturn mset.name()\n-\t}\n-\treturn _EMPTY_\n+        o.mu.RLock()\n+        mset := o.mset\n+        o.mu.RUnlock()\n+        if mset != nil {\n+                return mset.name()\n+        }\n+        return _EMPTY_\n }\n \n // Active indicates if this consumer is still active.\n func (o *consumer) isActive() bool {\n-\to.mu.RLock()\n-\tactive := o.active && o.mset != nil\n-\to.mu.RUnlock()\n-\treturn active\n+        o.mu.RLock()\n+        active := o.active && o.mset != nil\n+        o.mu.RUnlock()\n+        return active\n }\n \n // hasNoLocalInterest return true if we have no local interest.\n func (o *consumer) hasNoLocalInterest() bool {\n-\to.mu.RLock()\n-\trr := o.acc.sl.Match(o.cfg.DeliverSubject)\n-\to.mu.RUnlock()\n-\treturn len(rr.psubs)+len(rr.qsubs) == 0\n+        o.mu.RLock()\n+        rr := o.acc.sl.Match(o.cfg.DeliverSubject)\n+        o.mu.RUnlock()\n+        return len(rr.psubs)+len(rr.qsubs) == 0\n }\n \n // This is when the underlying stream has been purged.\n // sseq is the new first seq for the stream after purge.\n // Lock should be held.\n func (o *consumer) purge(sseq uint64) {\n-\t// Do not update our state unless we know we are the leader.\n-\tif !o.isLeader() {\n-\t\treturn\n-\t}\n-\t// Signals all have been purged for this consumer.\n-\tif sseq == 0 {\n-\t\tsseq = o.mset.lastSeq() + 1\n-\t}\n-\n-\to.mu.Lock()\n-\t// Do not go backwards\n-\tif o.sseq < sseq {\n-\t\to.sseq = sseq\n-\t}\n-\tif o.asflr < sseq {\n-\t\to.asflr = sseq - 1\n-\t\tif o.dseq > 0 {\n-\t\t\to.adflr = o.dseq - 1\n-\t\t}\n-\t}\n-\to.sgap = 0\n-\to.pending = nil\n-\n-\t// We need to remove all those being queued for redelivery under o.rdq\n-\tif len(o.rdq) > 0 {\n-\t\trdq := o.rdq\n-\t\to.rdq, o.rdqi = nil, nil\n-\t\tfor _, sseq := range rdq {\n-\t\t\tif sseq >= o.sseq {\n-\t\t\t\to.addToRedeliverQueue(sseq)\n-\t\t\t}\n-\t\t}\n-\t}\n-\to.mu.Unlock()\n-\n-\to.writeStoreState()\n+        // Do not update our state unless we know we are the leader.\n+        if !o.isLeader() {\n+                return\n+        }\n+        // Signals all have been purged for this consumer.\n+        if sseq == 0 {\n+                sseq = o.mset.lastSeq() + 1\n+        }\n+\n+        o.mu.Lock()\n+        // Do not go backwards\n+        if o.sseq < sseq {\n+                o.sseq = sseq\n+        }\n+        if o.asflr < sseq {\n+                o.asflr = sseq - 1\n+                if o.dseq > 0 {\n+                        o.adflr = o.dseq - 1\n+                }\n+        }\n+        o.sgap = 0\n+        o.pending = nil\n+\n+        // We need to remove all those being queued for redelivery under o.rdq\n+        if len(o.rdq) > 0 {\n+                rdq := o.rdq\n+                o.rdq, o.rdqi = nil, nil\n+                for _, sseq := range rdq {\n+                        if sseq >= o.sseq {\n+                                o.addToRedeliverQueue(sseq)\n+                        }\n+                }\n+        }\n+        o.mu.Unlock()\n+\n+        o.writeStoreState()\n }\n \n func stopAndClearTimer(tp **time.Timer) {\n-\tif *tp == nil {\n-\t\treturn\n-\t}\n-\t// Will get drained in normal course, do not try to\n-\t// drain here.\n-\t(*tp).Stop()\n-\t*tp = nil\n+        if *tp == nil {\n+                return\n+        }\n+        // Will get drained in normal course, do not try to\n+        // drain here.\n+        (*tp).Stop()\n+        *tp = nil\n }\n \n // Stop will shutdown  the consumer for the associated stream.\n func (o *consumer) stop() error {\n-\treturn o.stopWithFlags(false, false, true, false)\n+        return o.stopWithFlags(false, false, true, false)\n }\n \n func (o *consumer) deleteWithoutAdvisory() error {\n-\treturn o.stopWithFlags(true, false, true, false)\n+        return o.stopWithFlags(true, false, true, false)\n }\n \n // Delete will delete the consumer for the associated stream and send advisories.\n func (o *consumer) delete() error {\n-\treturn o.stopWithFlags(true, false, true, true)\n+        return o.stopWithFlags(true, false, true, true)\n }\n \n func (o *consumer) stopWithFlags(dflag, sdflag, doSignal, advisory bool) error {\n-\to.mu.Lock()\n-\tif o.closed {\n-\t\to.mu.Unlock()\n-\t\treturn nil\n-\t}\n-\to.closed = true\n-\n-\tif dflag && advisory && o.isLeader() {\n-\t\to.sendDeleteAdvisoryLocked()\n-\t}\n-\n-\tif o.qch != nil {\n-\t\tclose(o.qch)\n-\t\to.qch = nil\n-\t}\n-\n-\ta := o.acc\n-\tstore := o.store\n-\tmset := o.mset\n-\to.mset = nil\n-\to.active = false\n-\to.unsubscribe(o.ackSub)\n-\to.unsubscribe(o.reqSub)\n-\to.unsubscribe(o.fcSub)\n-\to.ackSub = nil\n-\to.reqSub = nil\n-\to.fcSub = nil\n-\tif o.infoSub != nil {\n-\t\to.srv.sysUnsubscribe(o.infoSub)\n-\t\to.infoSub = nil\n-\t}\n-\tc := o.client\n-\to.client = nil\n-\tsysc := o.sysc\n-\to.sysc = nil\n-\tstopAndClearTimer(&o.ptmr)\n-\tstopAndClearTimer(&o.dtmr)\n-\tstopAndClearTimer(&o.gwdtmr)\n-\tdelivery := o.cfg.DeliverSubject\n-\to.waiting = nil\n-\t// Break us out of the readLoop.\n-\tif doSignal {\n-\t\to.signalNewMessages()\n-\t}\n-\tn := o.node\n-\tqgroup := o.cfg.DeliverGroup\n-\to.mu.Unlock()\n-\n-\tif c != nil {\n-\t\tc.closeConnection(ClientClosed)\n-\t}\n-\tif sysc != nil {\n-\t\tsysc.closeConnection(ClientClosed)\n-\t}\n-\n-\tif delivery != _EMPTY_ {\n-\t\ta.sl.clearNotification(delivery, qgroup, o.inch)\n-\t}\n-\n-\tmset.mu.Lock()\n-\tmset.removeConsumer(o)\n-\trp := mset.cfg.Retention\n-\tmset.mu.Unlock()\n-\n-\t// We need to optionally remove all messages since we are interest based retention.\n-\t// We will do this consistently on all replicas. Note that if in clustered mode the\n-\t// non-leader consumers will need to restore state first.\n-\tif dflag && rp == InterestPolicy {\n-\t\tstop := mset.lastSeq()\n-\t\to.mu.Lock()\n-\t\tif !o.isLeader() {\n-\t\t\to.readStoredState()\n-\t\t}\n-\t\tstart := o.asflr\n-\t\to.mu.Unlock()\n-\n-\t\tvar rmseqs []uint64\n-\t\tmset.mu.RLock()\n-\t\tfor seq := start; seq <= stop; seq++ {\n-\t\t\tif !mset.checkInterest(seq, o) {\n-\t\t\t\trmseqs = append(rmseqs, seq)\n-\t\t\t}\n-\t\t}\n-\t\tmset.mu.RUnlock()\n-\n-\t\tfor _, seq := range rmseqs {\n-\t\t\tmset.store.RemoveMsg(seq)\n-\t\t}\n-\t}\n-\n-\t// Cluster cleanup.\n-\tif n != nil {\n-\t\tif dflag {\n-\t\t\tn.Delete()\n-\t\t} else {\n-\t\t\tn.Stop()\n-\t\t}\n-\t}\n-\n-\t// Clean up our store.\n-\tvar err error\n-\tif store != nil {\n-\t\tif dflag {\n-\t\t\tif sdflag {\n-\t\t\t\terr = store.StreamDelete()\n-\t\t\t} else {\n-\t\t\t\terr = store.Delete()\n-\t\t\t}\n-\t\t} else {\n-\t\t\terr = store.Stop()\n-\t\t}\n-\t}\n-\n-\treturn err\n+        o.mu.Lock()\n+        if o.closed {\n+                o.mu.Unlock()\n+                return nil\n+        }\n+        o.closed = true\n+\n+        if dflag && advisory && o.isLeader() {\n+                o.sendDeleteAdvisoryLocked()\n+        }\n+\n+        if o.qch != nil {\n+                close(o.qch)\n+                o.qch = nil\n+        }\n+\n+        a := o.acc\n+        store := o.store\n+        mset := o.mset\n+        o.mset = nil\n+        o.active = false\n+        o.unsubscribe(o.ackSub)\n+        o.unsubscribe(o.reqSub)\n+        o.unsubscribe(o.fcSub)\n+        o.ackSub = nil\n+        o.reqSub = nil\n+        o.fcSub = nil\n+        if o.infoSub != nil {\n+                o.srv.sysUnsubscribe(o.infoSub)\n+                o.infoSub = nil\n+        }\n+        c := o.client\n+        o.client = nil\n+        sysc := o.sysc\n+        o.sysc = nil\n+        stopAndClearTimer(&o.ptmr)\n+        stopAndClearTimer(&o.dtmr)\n+        stopAndClearTimer(&o.gwdtmr)\n+        delivery := o.cfg.DeliverSubject\n+        o.waiting = nil\n+        // Break us out of the readLoop.\n+        if doSignal {\n+                o.signalNewMessages()\n+        }\n+        n := o.node\n+        qgroup := o.cfg.DeliverGroup\n+        o.mu.Unlock()\n+\n+        if c != nil {\n+                c.closeConnection(ClientClosed)\n+        }\n+        if sysc != nil {\n+                sysc.closeConnection(ClientClosed)\n+        }\n+\n+        if delivery != _EMPTY_ {\n+                a.sl.clearNotification(delivery, qgroup, o.inch)\n+        }\n+\n+        mset.mu.Lock()\n+        mset.removeConsumer(o)\n+        rp := mset.cfg.Retention\n+        mset.mu.Unlock()\n+\n+        // We need to optionally remove all messages since we are interest based retention.\n+        // We will do this consistently on all replicas. Note that if in clustered mode the\n+        // non-leader consumers will need to restore state first.\n+        if dflag && rp == InterestPolicy {\n+                stop := mset.lastSeq()\n+                o.mu.Lock()\n+                if !o.isLeader() {\n+                        o.readStoredState()\n+                }\n+                start := o.asflr\n+                o.mu.Unlock()\n+\n+                var rmseqs []uint64\n+                mset.mu.RLock()\n+                for seq := start; seq <= stop; seq++ {\n+                        if !mset.checkInterest(seq, o) {\n+                                rmseqs = append(rmseqs, seq)\n+                        }\n+                }\n+                mset.mu.RUnlock()\n+\n+                for _, seq := range rmseqs {\n+                        mset.store.RemoveMsg(seq)\n+                }\n+        }\n+\n+        // Cluster cleanup.\n+        if n != nil {\n+                if dflag {\n+                        n.Delete()\n+                } else {\n+                        n.Stop()\n+                }\n+        }\n+\n+        // Clean up our store.\n+        var err error\n+        if store != nil {\n+                if dflag {\n+                        if sdflag {\n+                                err = store.StreamDelete()\n+                        } else {\n+                                err = store.Delete()\n+                        }\n+                } else {\n+                        err = store.Stop()\n+                }\n+        }\n+\n+        return err\n }\n \n // Check that we do not form a cycle by delivering to a delivery subject\n // that is part of the interest group.\n func (mset *stream) deliveryFormsCycle(deliverySubject string) bool {\n-\tmset.mu.RLock()\n-\tdefer mset.mu.RUnlock()\n+        mset.mu.RLock()\n+        defer mset.mu.RUnlock()\n \n-\tfor _, subject := range mset.cfg.Subjects {\n-\t\tif subjectIsSubsetMatch(deliverySubject, subject) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, subject := range mset.cfg.Subjects {\n+                if subjectIsSubsetMatch(deliverySubject, subject) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // Check that the filtered subject is valid given a set of stream subjects.\n func validFilteredSubject(filteredSubject string, subjects []string) bool {\n-\tif !IsValidSubject(filteredSubject) {\n-\t\treturn false\n-\t}\n-\thasWC := subjectHasWildcard(filteredSubject)\n-\n-\tfor _, subject := range subjects {\n-\t\tif subjectIsSubsetMatch(filteredSubject, subject) {\n-\t\t\treturn true\n-\t\t}\n-\t\t// If we have a wildcard as the filtered subject check to see if we are\n-\t\t// a wider scope but do match a subject.\n-\t\tif hasWC && subjectIsSubsetMatch(subject, filteredSubject) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        if !IsValidSubject(filteredSubject) {\n+                return false\n+        }\n+        hasWC := subjectHasWildcard(filteredSubject)\n+\n+        for _, subject := range subjects {\n+                if subjectIsSubsetMatch(filteredSubject, subject) {\n+                        return true\n+                }\n+                // If we have a wildcard as the filtered subject check to see if we are\n+                // a wider scope but do match a subject.\n+                if hasWC && subjectIsSubsetMatch(subject, filteredSubject) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // setInActiveDeleteThreshold sets the delete threshold for how long to wait\n // before deleting an inactive ephemeral consumer.\n func (o *consumer) setInActiveDeleteThreshold(dthresh time.Duration) error {\n-\to.mu.Lock()\n-\tdefer o.mu.Unlock()\n-\n-\tif o.isDurable() {\n-\t\treturn fmt.Errorf(\"consumer is not ephemeral\")\n-\t}\n-\tdeleteWasRunning := o.dtmr != nil\n-\tstopAndClearTimer(&o.dtmr)\n-\t// Do not add jitter if set via here.\n-\to.dthresh = dthresh\n-\tif deleteWasRunning {\n-\t\to.dtmr = time.AfterFunc(o.dthresh, func() { o.deleteNotActive() })\n-\t}\n-\treturn nil\n+        o.mu.Lock()\n+        defer o.mu.Unlock()\n+\n+        if o.isDurable() {\n+                return fmt.Errorf(\"consumer is not ephemeral\")\n+        }\n+        deleteWasRunning := o.dtmr != nil\n+        stopAndClearTimer(&o.dtmr)\n+        // Do not add jitter if set via here.\n+        o.dthresh = dthresh\n+        if deleteWasRunning {\n+                o.dtmr = time.AfterFunc(o.dthresh, func() { o.deleteNotActive() })\n+        }\n+        return nil\n }\n \n // switchToEphemeral is called on startup when recovering ephemerals.\n func (o *consumer) switchToEphemeral() {\n-\to.mu.Lock()\n-\to.cfg.Durable = _EMPTY_\n-\tstore, ok := o.store.(*consumerFileStore)\n-\trr := o.acc.sl.Match(o.cfg.DeliverSubject)\n-\t// Setup dthresh.\n-\tif o.dthresh == 0 {\n-\t\tif o.cfg.InactiveThreshold != 0 {\n-\t\t\to.dthresh = o.cfg.InactiveThreshold\n-\t\t} else {\n-\t\t\t// Add in 1 sec of jitter above and beyond the default of 5s.\n-\t\t\to.dthresh = JsDeleteWaitTimeDefault + time.Duration(rand.Int63n(1000))*time.Millisecond\n-\t\t}\n-\t}\n-\to.mu.Unlock()\n-\n-\t// Update interest\n-\to.updateDeliveryInterest(len(rr.psubs)+len(rr.qsubs) > 0)\n-\t// Write out new config\n-\tif ok {\n-\t\tstore.updateConfig(o.cfg)\n-\t}\n+        o.mu.Lock()\n+        o.cfg.Durable = _EMPTY_\n+        store, ok := o.store.(*consumerFileStore)\n+        rr := o.acc.sl.Match(o.cfg.DeliverSubject)\n+        // Setup dthresh.\n+        if o.dthresh == 0 {\n+                if o.cfg.InactiveThreshold != 0 {\n+                        o.dthresh = o.cfg.InactiveThreshold\n+                } else {\n+                        // Add in 1 sec of jitter above and beyond the default of 5s.\n+                        o.dthresh = JsDeleteWaitTimeDefault + time.Duration(rand.Int63n(1000))*time.Millisecond\n+                }\n+        }\n+        o.mu.Unlock()\n+\n+        // Update interest\n+        o.updateDeliveryInterest(len(rr.psubs)+len(rr.qsubs) > 0)\n+        // Write out new config\n+        if ok {\n+                store.updateConfig(o.cfg)\n+        }\n }\n \n // RequestNextMsgSubject returns the subject to request the next message when in pull or worker mode.\n // Returns empty otherwise.\n func (o *consumer) requestNextMsgSubject() string {\n-\treturn o.nextMsgSubj\n+        return o.nextMsgSubj\n }\n \n // Will set the initial pending and start sequence.\n // mset lock should be held.\n func (o *consumer) setInitialPendingAndStart() {\n-\tmset := o.mset\n-\tif mset == nil || mset.store == nil {\n-\t\treturn\n-\t}\n-\n-\t// !filtered means we want all messages.\n-\tfiltered, dp := o.cfg.FilterSubject != _EMPTY_, o.cfg.DeliverPolicy\n-\tif filtered {\n-\t\t// Check to see if we directly match the configured stream.\n-\t\t// Many clients will always send a filtered subject.\n-\t\tcfg := &mset.cfg\n-\t\tif len(cfg.Subjects) == 1 && cfg.Subjects[0] == o.cfg.FilterSubject {\n-\t\t\tfiltered = false\n-\t\t}\n-\t}\n-\n-\tif !filtered && dp != DeliverLastPerSubject {\n-\t\tvar state StreamState\n-\t\tmset.store.FastState(&state)\n-\t\tif state.Msgs > 0 {\n-\t\t\to.sgap = state.Msgs - (o.sseq - state.FirstSeq)\n-\t\t\to.lsgap = state.LastSeq\n-\t\t}\n-\t} else {\n-\t\t// Here we are filtered.\n-\t\tif dp == DeliverLastPerSubject && o.hasSkipListPending() && o.sseq < o.lss.resume {\n-\t\t\tss := mset.store.FilteredState(o.lss.resume+1, o.cfg.FilterSubject)\n-\t\t\to.sseq = o.lss.seqs[0]\n-\t\t\to.sgap = ss.Msgs + uint64(len(o.lss.seqs))\n-\t\t\to.lsgap = ss.Last\n-\t\t} else if ss := mset.store.FilteredState(o.sseq, o.cfg.FilterSubject); ss.Msgs > 0 {\n-\t\t\to.sgap = ss.Msgs\n-\t\t\to.lsgap = ss.Last\n-\t\t\t// See if we should update our starting sequence.\n-\t\t\tif dp == DeliverLast || dp == DeliverLastPerSubject {\n-\t\t\t\to.sseq = ss.Last\n-\t\t\t} else if dp == DeliverNew {\n-\t\t\t\to.sseq = ss.Last + 1\n-\t\t\t} else {\n-\t\t\t\t// DeliverAll, DeliverByStartSequence, DeliverByStartTime\n-\t\t\t\to.sseq = ss.First\n-\t\t\t}\n-\t\t\t// Cleanup lss when we take over in clustered mode.\n-\t\t\tif dp == DeliverLastPerSubject && o.hasSkipListPending() && o.sseq >= o.lss.resume {\n-\t\t\t\to.lss = nil\n-\t\t\t}\n-\t\t}\n-\t\to.updateSkipped()\n-\t}\n+        mset := o.mset\n+        if mset == nil || mset.store == nil {\n+                return\n+        }\n+\n+        // !filtered means we want all messages.\n+        filtered, dp := o.cfg.FilterSubject != _EMPTY_, o.cfg.DeliverPolicy\n+        if filtered {\n+                // Check to see if we directly match the configured stream.\n+                // Many clients will always send a filtered subject.\n+                cfg := &mset.cfg\n+                if len(cfg.Subjects) == 1 && cfg.Subjects[0] == o.cfg.FilterSubject {\n+                        filtered = false\n+                }\n+        }\n+\n+        if !filtered && dp != DeliverLastPerSubject {\n+                var state StreamState\n+                mset.store.FastState(&state)\n+                if state.Msgs > 0 {\n+                        o.sgap = state.Msgs - (o.sseq - state.FirstSeq)\n+                        o.lsgap = state.LastSeq\n+                }\n+        } else {\n+                // Here we are filtered.\n+                if dp == DeliverLastPerSubject && o.hasSkipListPending() && o.sseq < o.lss.resume {\n+                        ss := mset.store.FilteredState(o.lss.resume+1, o.cfg.FilterSubject)\n+                        o.sseq = o.lss.seqs[0]\n+                        o.sgap = ss.Msgs + uint64(len(o.lss.seqs))\n+                        o.lsgap = ss.Last\n+                } else if ss := mset.store.FilteredState(o.sseq, o.cfg.FilterSubject); ss.Msgs > 0 {\n+                        o.sgap = ss.Msgs\n+                        o.lsgap = ss.Last\n+                        // See if we should update our starting sequence.\n+                        if dp == DeliverLast || dp == DeliverLastPerSubject {\n+                                o.sseq = ss.Last\n+                        } else if dp == DeliverNew {\n+                                o.sseq = ss.Last + 1\n+                        } else {\n+                                // DeliverAll, DeliverByStartSequence, DeliverByStartTime\n+                                o.sseq = ss.First\n+                        }\n+                        // Cleanup lss when we take over in clustered mode.\n+                        if dp == DeliverLastPerSubject && o.hasSkipListPending() && o.sseq >= o.lss.resume {\n+                                o.lss = nil\n+                        }\n+                }\n+                o.updateSkipped()\n+        }\n }\n \n func (o *consumer) decStreamPending(sseq uint64, subj string) {\n-\to.mu.Lock()\n-\t// Ignore if we have already seen this one.\n-\tif sseq >= o.sseq && o.sgap > 0 && o.isFilteredMatch(subj) {\n-\t\to.sgap--\n-\t}\n-\t// Check if this message was pending.\n-\tp, wasPending := o.pending[sseq]\n-\tvar rdc uint64 = 1\n-\tif o.rdc != nil {\n-\t\trdc = o.rdc[sseq]\n-\t}\n-\to.mu.Unlock()\n-\n-\t// If it was pending process it like an ack.\n-\t// TODO(dlc) - we could do a term here instead with a reason to generate the advisory.\n-\tif wasPending {\n-\t\to.processAckMsg(sseq, p.Sequence, rdc, false)\n-\t}\n+        o.mu.Lock()\n+        // Ignore if we have already seen this one.\n+        if sseq >= o.sseq && o.sgap > 0 && o.isFilteredMatch(subj) {\n+                o.sgap--\n+        }\n+        // Check if this message was pending.\n+        p, wasPending := o.pending[sseq]\n+        var rdc uint64 = 1\n+        if o.rdc != nil {\n+                rdc = o.rdc[sseq]\n+        }\n+        o.mu.Unlock()\n+\n+        // If it was pending process it like an ack.\n+        // TODO(dlc) - we could do a term here instead with a reason to generate the advisory.\n+        if wasPending {\n+                o.processAckMsg(sseq, p.Sequence, rdc, false)\n+        }\n }\n \n func (o *consumer) account() *Account {\n-\to.mu.RLock()\n-\ta := o.acc\n-\to.mu.RUnlock()\n-\treturn a\n+        o.mu.RLock()\n+        a := o.acc\n+        o.mu.RUnlock()\n+        return a\n }\n"}
{"cve":"CVE-2021-29417:0708", "fix_patch": "diff --git a/internal/pkg/gitjacker/retriever.go b/internal/pkg/gitjacker/retriever.go\nindex 3066e69..4981418 100644\n--- a/internal/pkg/gitjacker/retriever.go\n+++ b/internal/pkg/gitjacker/retriever.go\n@@ -1,401 +1,404 @@\n package gitjacker\n \n import (\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"strings\"\n-\t\"time\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"strings\"\n+        \"time\"\n     \"crypto/tls\"\n \n-\t\"github.com/sirupsen/logrus\"\n+        \"github.com/sirupsen/logrus\"\n )\n \n var paths = []string{\n-\t\"refs/heads/master\",\n-\t\"objects/info/packs\",\n-\t\"description\",\n-\t\"COMMIT_EDITMSG\",\n-\t\"index\",\n-\t\"packed-refs\",\n-\t\"refs/stash\",\n-\t\"logs/HEAD\",\n-\t\"logs/refs/heads/master\",\n-\t\"logs/refs/remotes/origin/HEAD\",\n-\t\"info/refs\",\n-\t\"info/exclude\",\n-\t\"packed-refs\",\n+        \"refs/heads/master\",\n+        \"objects/info/packs\",\n+        \"description\",\n+        \"COMMIT_EDITMSG\",\n+        \"index\",\n+        \"packed-refs\",\n+        \"refs/stash\",\n+        \"logs/HEAD\",\n+        \"logs/refs/heads/master\",\n+        \"logs/refs/remotes/origin/HEAD\",\n+        \"info/refs\",\n+        \"info/exclude\",\n+        \"packed-refs\",\n }\n \n var ErrNotVulnerable = fmt.Errorf(\"no .git directory is available at this URL\")\n \n type retriever struct {\n-\tbaseURL    *url.URL\n-\toutputDir  string\n-\thttp       *http.Client\n-\tdownloaded map[string]bool\n-\tsummary    Summary\n+        baseURL    *url.URL\n+        outputDir  string\n+        http       *http.Client\n+        downloaded map[string]bool\n+        summary    Summary\n }\n \n type Status uint\n \n const (\n-\tStatusUnknown Status = iota\n-\tStatusFailure\n-\tStatusPartialSuccess\n-\tStatusSuccess\n+        StatusUnknown Status = iota\n+        StatusFailure\n+        StatusPartialSuccess\n+        StatusSuccess\n )\n \n type Summary struct {\n-\tPackInformationAvailable bool\n-\tFoundObjects             []string\n-\tMissingObjects           []string\n-\tStatus                   Status\n-\tOutputDirectory          string\n-\tConfig                   Config\n+        PackInformationAvailable bool\n+        FoundObjects             []string\n+        MissingObjects           []string\n+        Status                   Status\n+        OutputDirectory          string\n+        Config                   Config\n }\n \n type Config struct {\n-\tRepositoryName string\n-\tRemotes        []Remote\n-\tBranches       []Branch\n-\tUser           User\n-\tGithubToken    GithubToken\n+        RepositoryName string\n+        Remotes        []Remote\n+        Branches       []Branch\n+        User           User\n+        GithubToken    GithubToken\n }\n \n type User struct {\n-\tName     string\n-\tEmail    string\n-\tUsername string\n+        Name     string\n+        Email    string\n+        Username string\n }\n \n type GithubToken struct {\n-\tUsername string\n-\tToken    string\n+        Username string\n+        Token    string\n }\n \n type Remote struct {\n-\tName string\n-\tURL  string\n+        Name string\n+        URL  string\n }\n \n type Branch struct {\n-\tName   string\n-\tRemote string\n+        Name   string\n+        Remote string\n }\n \n func New(target *url.URL, outputDir string) *retriever {\n \n-\trelative, _ := url.Parse(\".git/\")\n-\ttarget = target.ResolveReference(relative)\n+        relative, _ := url.Parse(\".git/\")\n+        target = target.ResolveReference(relative)\n     customTransport := http.DefaultTransport.(*http.Transport).Clone()\n     customTransport.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n     customTransport.Proxy = http.ProxyFromEnvironment\n \n-\treturn &retriever{\n-\t\tbaseURL:   target,\n-\t\toutputDir: outputDir,\n-\t\thttp: &http.Client{\n-\t\t\tTimeout: time.Second * 10,\n+        return &retriever{\n+                baseURL:   target,\n+                outputDir: outputDir,\n+                http: &http.Client{\n+                        Timeout: time.Second * 10,\n             Transport: customTransport,\n-\t\t},\n-\t\tdownloaded: make(map[string]bool),\n-\t\tsummary: Summary{\n-\t\t\tOutputDirectory: outputDir,\n-\t\t},\n-\t}\n+                },\n+                downloaded: make(map[string]bool),\n+                summary: Summary{\n+                        OutputDirectory: outputDir,\n+                },\n+        }\n }\n \n func (r *retriever) checkVulnerable() error {\n-\tif err := r.downloadFile(\"HEAD\"); err != nil {\n-\t\treturn fmt.Errorf(\"%w: %s\", ErrNotVulnerable, err)\n-\t}\n+        if err := r.downloadFile(\"HEAD\"); err != nil {\n+                return fmt.Errorf(\"%w: %s\", ErrNotVulnerable, err)\n+        }\n \n-\tfilePath := filepath.Join(r.outputDir, \".git\", \"HEAD\")\n-\thead, err := ioutil.ReadFile(filePath)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        filePath := filepath.Join(r.outputDir, \".git\", \"HEAD\")\n+        head, err := ioutil.ReadFile(filePath)\n+        if err != nil {\n+                return err\n+        }\n \n-\tif !strings.HasPrefix(string(head), \"ref: \") {\n-\t\treturn ErrNotVulnerable\n-\t}\n+        if !strings.HasPrefix(string(head), \"ref: \") {\n+                return ErrNotVulnerable\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func (r *retriever) parsePackMetadata(meta []byte) error {\n-\tlines := strings.Split(string(meta), \"\\n\")\n-\tfor _, line := range lines {\n-\t\tparts := strings.Split(strings.TrimSpace(line), \" \")\n-\t\tif parts[0] == \"P\" && len(parts) == 2 {\n-\t\t\tif err := r.downloadFile(fmt.Sprintf(\"objects/pack/%s\", parts[1])); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Failed to retrieve pack file %s: %s\", parts[1], err)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        lines := strings.Split(string(meta), \"\\n\")\n+        for _, line := range lines {\n+                parts := strings.Split(strings.TrimSpace(line), \" \")\n+                if parts[0] == \"P\" && len(parts) == 2 {\n+                        if err := r.downloadFile(fmt.Sprintf(\"objects/pack/%s\", parts[1])); err != nil {\n+                                logrus.Debugf(\"Failed to retrieve pack file %s: %s\", parts[1], err)\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n func (r *retriever) parsePackFile(filename string, data []byte) error {\n \n-\tf, err := os.Open(filepath.Join(r.outputDir, \".git\", filename))\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer func() { _ = f.Close() }()\n+        f, err := os.Open(filepath.Join(r.outputDir, \".git\", filename))\n+        if err != nil {\n+                return err\n+        }\n+        defer func() { _ = f.Close() }()\n \n-\tcmd := exec.Command(\"git\", \"unpack-objects\")\n-\tcmd.Stdin = f\n-\tcmd.Dir = r.outputDir\n-\treturn cmd.Run()\n+        cmd := exec.Command(\"git\", \"unpack-objects\")\n+        cmd.Stdin = f\n+        cmd.Dir = r.outputDir\n+        return cmd.Run()\n }\n \n func (r *retriever) downloadFile(path string) error {\n \n-\tpath = strings.TrimSpace(path)\n-\n-\tfilePath := filepath.Join(r.outputDir, \".git\", path)\n-\n-\tif r.downloaded[path] {\n-\t\treturn nil\n-\t}\n-\tr.downloaded[path] = true\n-\n-\trelative, err := url.Parse(path)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tabsolute := r.baseURL.ResolveReference(relative)\n-\tresp, err := r.http.Get(absolute.String())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"failed to retrieve %s: %w\", absolute.String(), err)\n-\t}\n-\tdefer func() { _ = resp.Body.Close() }()\n-\n-\tif resp.StatusCode != http.StatusOK {\n-\t\treturn fmt.Errorf(\"unexpected status code for url %s : %d\", absolute.String(), resp.StatusCode)\n-\t}\n-\n-\tcontent, err := ioutil.ReadAll(resp.Body)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif !strings.HasSuffix(path, \"/\") {\n-\t\tif err := ioutil.WriteFile(filePath, content, 0640); err != nil {\n-\t\t\treturn fmt.Errorf(\"failed to write %s: %w\", filePath, err)\n-\t\t}\n-\t}\n-\n-\tswitch path {\n-\tcase \"HEAD\":\n-\t\tref := strings.TrimPrefix(string(content), \"ref: \")\n-\t\tif err := r.downloadFile(ref); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\treturn nil\n-\tcase \"config\":\n-\t\treturn r.analyseConfig(content)\n-\tcase \"objects/pack/\":\n-\t\t// parse the directory listing\n-\t\tpackFiles := packLinkRegex.FindAllStringSubmatch(string(content), -1)\n-\t\tfor _, packFile := range packFiles {\n-\t\t\tif len(packFile) <= 1 {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tif err := r.downloadFile(fmt.Sprintf(\"objects/pack/%s\", packFile[1])); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Failed to retrieve pack file %s: %s\", packFile[1], err)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\tcase \"objects/info/packs\":\n-\t\treturn r.parsePackMetadata(content)\n-\t}\n-\n-\tif strings.HasSuffix(path, \".pack\") {\n-\t\treturn r.parsePackFile(path, content)\n-\t}\n-\n-\tif strings.HasPrefix(path, \"refs/heads/\") {\n-\t\tif _, err := r.downloadObject(string(content)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\treturn nil\n-\t}\n-\n-\thash := filepath.Base(filepath.Dir(path)) + filepath.Base(path)\n-\n-\tobjectType, err := r.getObjectType(hash)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tswitch objectType {\n-\tcase GitCommitFile:\n-\n-\t\tcommit, err := r.readCommit(hash)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tlogrus.Debugf(\"Successfully retrieved commit %s.\", hash)\n-\n-\t\tif commit.Tree != \"\" {\n-\t\t\tif _, err := r.downloadObject(commit.Tree); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Object %s is missing and likely packed.\", commit.Tree)\n-\t\t\t}\n-\t\t}\n-\t\tfor _, parent := range commit.Parents {\n-\t\t\tif _, err := r.downloadObject(parent); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Object %s is missing and likely packed.\", parent)\n-\t\t\t}\n-\t\t}\n-\n-\tcase GitTreeFile:\n-\n-\t\ttree, err := r.readTree(hash)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tlogrus.Debugf(\"Successfully retrieved tree %s.\", hash)\n-\n-\t\tfor _, subHash := range tree.Objects {\n-\t\t\tif _, err := r.downloadObject(subHash); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Object %s is missing and likely packed.\", subHash)\n-\t\t\t}\n-\t\t}\n-\tcase GitBlobFile:\n-\t\tlogrus.Debugf(\"Successfully retrieved blob %s.\", hash)\n-\tdefault:\n-\t\treturn fmt.Errorf(\"unknown git file type for %s: %s\", path, objectType)\n-\t}\n-\n-\treturn nil\n+        path = strings.TrimSpace(path)\n+\n+        filePath := filepath.Join(r.outputDir, \".git\", path)\n+\n+if !strings.HasPrefix(filePath, filepath.Join(r.outputDir, \".git\")) {\n+return fmt.Errorf(\"illegal path requested: %s\", path)\n+}\n+        if r.downloaded[path] {\n+                return nil\n+        }\n+        r.downloaded[path] = true\n+\n+        relative, err := url.Parse(path)\n+        if err != nil {\n+                return err\n+        }\n+\n+        absolute := r.baseURL.ResolveReference(relative)\n+        resp, err := r.http.Get(absolute.String())\n+        if err != nil {\n+                return fmt.Errorf(\"failed to retrieve %s: %w\", absolute.String(), err)\n+        }\n+        defer func() { _ = resp.Body.Close() }()\n+\n+        if resp.StatusCode != http.StatusOK {\n+                return fmt.Errorf(\"unexpected status code for url %s : %d\", absolute.String(), resp.StatusCode)\n+        }\n+\n+        content, err := ioutil.ReadAll(resp.Body)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {\n+                return err\n+        }\n+\n+        if !strings.HasSuffix(path, \"/\") {\n+                if err := ioutil.WriteFile(filePath, content, 0640); err != nil {\n+                        return fmt.Errorf(\"failed to write %s: %w\", filePath, err)\n+                }\n+        }\n+\n+        switch path {\n+        case \"HEAD\":\n+                ref := strings.TrimPrefix(string(content), \"ref: \")\n+                if err := r.downloadFile(ref); err != nil {\n+                        return err\n+                }\n+                return nil\n+        case \"config\":\n+                return r.analyseConfig(content)\n+        case \"objects/pack/\":\n+                // parse the directory listing\n+                packFiles := packLinkRegex.FindAllStringSubmatch(string(content), -1)\n+                for _, packFile := range packFiles {\n+                        if len(packFile) <= 1 {\n+                                continue\n+                        }\n+                        if err := r.downloadFile(fmt.Sprintf(\"objects/pack/%s\", packFile[1])); err != nil {\n+                                logrus.Debugf(\"Failed to retrieve pack file %s: %s\", packFile[1], err)\n+                                continue\n+                        }\n+                }\n+                return nil\n+        case \"objects/info/packs\":\n+                return r.parsePackMetadata(content)\n+        }\n+\n+        if strings.HasSuffix(path, \".pack\") {\n+                return r.parsePackFile(path, content)\n+        }\n+\n+        if strings.HasPrefix(path, \"refs/heads/\") {\n+                if _, err := r.downloadObject(string(content)); err != nil {\n+                        return err\n+                }\n+                return nil\n+        }\n+\n+        hash := filepath.Base(filepath.Dir(path)) + filepath.Base(path)\n+\n+        objectType, err := r.getObjectType(hash)\n+        if err != nil {\n+                return err\n+        }\n+\n+        switch objectType {\n+        case GitCommitFile:\n+\n+                commit, err := r.readCommit(hash)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                logrus.Debugf(\"Successfully retrieved commit %s.\", hash)\n+\n+                if commit.Tree != \"\" {\n+                        if _, err := r.downloadObject(commit.Tree); err != nil {\n+                                logrus.Debugf(\"Object %s is missing and likely packed.\", commit.Tree)\n+                        }\n+                }\n+                for _, parent := range commit.Parents {\n+                        if _, err := r.downloadObject(parent); err != nil {\n+                                logrus.Debugf(\"Object %s is missing and likely packed.\", parent)\n+                        }\n+                }\n+\n+        case GitTreeFile:\n+\n+                tree, err := r.readTree(hash)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                logrus.Debugf(\"Successfully retrieved tree %s.\", hash)\n+\n+                for _, subHash := range tree.Objects {\n+                        if _, err := r.downloadObject(subHash); err != nil {\n+                                logrus.Debugf(\"Object %s is missing and likely packed.\", subHash)\n+                        }\n+                }\n+        case GitBlobFile:\n+                logrus.Debugf(\"Successfully retrieved blob %s.\", hash)\n+        default:\n+                return fmt.Errorf(\"unknown git file type for %s: %s\", path, objectType)\n+        }\n+\n+        return nil\n }\n \n func (r *retriever) downloadObject(hash string) (string, error) {\n \n-\tlogrus.Debugf(\"Requesting hash [%s]\\n\", hash)\n+        logrus.Debugf(\"Requesting hash [%s]\\n\", hash)\n \n-\tpath := fmt.Sprintf(\"objects/%s/%s\", hash[:2], hash[2:40])\n-\tif err := r.downloadFile(path); err != nil {\n-\t\tr.summary.MissingObjects = append(r.summary.MissingObjects, hash)\n-\t\treturn \"\", err\n-\t}\n-\tr.summary.FoundObjects = append(r.summary.FoundObjects, hash)\n-\treturn path, nil\n+        path := fmt.Sprintf(\"objects/%s/%s\", hash[:2], hash[2:40])\n+        if err := r.downloadFile(path); err != nil {\n+                r.summary.MissingObjects = append(r.summary.MissingObjects, hash)\n+                return \"\", err\n+        }\n+        r.summary.FoundObjects = append(r.summary.FoundObjects, hash)\n+        return path, nil\n }\n \n type GitFileType string\n \n const (\n-\tGitUnknownFile GitFileType = \"\"\n-\tGitCommitFile  GitFileType = \"commit\"\n-\tGitTreeFile    GitFileType = \"tree\"\n-\tGitBlobFile    GitFileType = \"blob\"\n+        GitUnknownFile GitFileType = \"\"\n+        GitCommitFile  GitFileType = \"commit\"\n+        GitTreeFile    GitFileType = \"tree\"\n+        GitBlobFile    GitFileType = \"blob\"\n )\n \n func (r *retriever) getObjectType(hash string) (GitFileType, error) {\n-\tcmd := exec.Command(\"git\", \"cat-file\", \"-t\", hash)\n-\tcmd.Dir = r.outputDir\n-\toutput, err := cmd.Output()\n-\tif err != nil {\n-\t\treturn GitUnknownFile, fmt.Errorf(\"failed to read type of %s: %w\", hash, err)\n-\t}\n-\treturn GitFileType(strings.TrimSpace(string(output))), nil\n+        cmd := exec.Command(\"git\", \"cat-file\", \"-t\", hash)\n+        cmd.Dir = r.outputDir\n+        output, err := cmd.Output()\n+        if err != nil {\n+                return GitUnknownFile, fmt.Errorf(\"failed to read type of %s: %w\", hash, err)\n+        }\n+        return GitFileType(strings.TrimSpace(string(output))), nil\n }\n \n type Commit struct {\n-\tTree    string\n-\tParents []string\n+        Tree    string\n+        Parents []string\n }\n \n func (r *retriever) readCommit(hash string) (*Commit, error) {\n-\tcmd := exec.Command(\"git\", \"cat-file\", \"-p\", hash)\n-\tcmd.Dir = r.outputDir\n-\toutput, err := cmd.Output()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to read commit %s: %w\", hash, err)\n-\t}\n-\n-\tlines := strings.Split(string(output), \"\\n\")\n-\tvar commit Commit\n-\tfor _, line := range lines {\n-\t\tline = strings.TrimSpace(line)\n-\t\twords := strings.Split(line, \" \")\n-\t\tif len(words) <= 1 {\n-\t\t\tcontinue\n-\t\t}\n-\t\tswitch words[0] {\n-\t\tcase \"tree\":\n-\t\t\tcommit.Tree = words[len(words)-1]\n-\t\tcase \"parent\":\n-\t\t\tcommit.Parents = append(commit.Parents, words[len(words)-1])\n-\t\t}\n-\t}\n-\treturn &commit, nil\n+        cmd := exec.Command(\"git\", \"cat-file\", \"-p\", hash)\n+        cmd.Dir = r.outputDir\n+        output, err := cmd.Output()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to read commit %s: %w\", hash, err)\n+        }\n+\n+        lines := strings.Split(string(output), \"\\n\")\n+        var commit Commit\n+        for _, line := range lines {\n+                line = strings.TrimSpace(line)\n+                words := strings.Split(line, \" \")\n+                if len(words) <= 1 {\n+                        continue\n+                }\n+                switch words[0] {\n+                case \"tree\":\n+                        commit.Tree = words[len(words)-1]\n+                case \"parent\":\n+                        commit.Parents = append(commit.Parents, words[len(words)-1])\n+                }\n+        }\n+        return &commit, nil\n }\n \n type Tree struct {\n-\tObjects []string\n+        Objects []string\n }\n \n func (r *retriever) readTree(hash string) (*Tree, error) {\n \n-\tcmd := exec.Command(\"git\", \"cat-file\", \"-p\", hash)\n-\tcmd.Dir = r.outputDir\n-\n-\toutput, err := cmd.Output()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to read tree %s: %w\", hash, err)\n-\t}\n-\n-\tlines := strings.Split(string(output), \"\\n\")\n-\tvar tree Tree\n-\tfor _, line := range lines {\n-\t\tline = strings.TrimSpace(line)\n-\t\tline = strings.ReplaceAll(line, \"\\t\", \" \")\n-\t\twords := strings.Split(line, \" \")\n-\t\tif len(words) < 4 {\n-\t\t\tcontinue\n-\t\t}\n-\t\ttree.Objects = append(tree.Objects, words[2])\n-\t}\n-\treturn &tree, nil\n+        cmd := exec.Command(\"git\", \"cat-file\", \"-p\", hash)\n+        cmd.Dir = r.outputDir\n+\n+        output, err := cmd.Output()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to read tree %s: %w\", hash, err)\n+        }\n+\n+        lines := strings.Split(string(output), \"\\n\")\n+        var tree Tree\n+        for _, line := range lines {\n+                line = strings.TrimSpace(line)\n+                line = strings.ReplaceAll(line, \"\\t\", \" \")\n+                words := strings.Split(line, \" \")\n+                if len(words) < 4 {\n+                        continue\n+                }\n+                tree.Objects = append(tree.Objects, words[2])\n+        }\n+        return &tree, nil\n }\n \n func (r *retriever) reset() error {\n \n-\tcmd := exec.Command(\"git\", \"reset\")\n-\tcmd.Dir = r.outputDir\n-\tif err := cmd.Run(); err != nil {\n-\t\treturn fmt.Errorf(\"failed to reset files: %w\", err)\n-\t}\n+        cmd := exec.Command(\"git\", \"reset\")\n+        cmd.Dir = r.outputDir\n+        if err := cmd.Run(); err != nil {\n+                return fmt.Errorf(\"failed to reset files: %w\", err)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func (r *retriever) checkout() error {\n-\tcheckoutCmd := exec.Command(\"git\", \"checkout\", \"--\", \".\")\n-\tcheckoutCmd.Dir = r.outputDir\n-\tif err := checkoutCmd.Run(); err != nil {\n-\t\treturn fmt.Errorf(\"failed to checkout files: %w\", err)\n-\t}\n+        checkoutCmd := exec.Command(\"git\", \"checkout\", \"--\", \".\")\n+        checkoutCmd.Dir = r.outputDir\n+        if err := checkoutCmd.Run(); err != nil {\n+                return fmt.Errorf(\"failed to checkout files: %w\", err)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n var ErrNoPackInfo = fmt.Errorf(\"pack information (.git/objects/info/packs) is missing\")\n@@ -405,154 +408,154 @@ var packLinkRegex = regexp.MustCompile(\"href=[\\\"']?(pack-[a-z0-9]{40}\\\\.pack)\")\n \n func (r *retriever) locatePackFiles() error {\n \n-\t// first of all let's try a directory listing for all pack files\n-\t_ = r.downloadFile(\"objects/pack/\")\n+        // first of all let's try a directory listing for all pack files\n+        _ = r.downloadFile(\"objects/pack/\")\n \n-\t// otherwise hopefully the pak listing is available...\n-\tif err := r.downloadFile(\"objects/info/packs\"); err != nil {\n-\t\treturn ErrNoPackInfo\n-\t}\n+        // otherwise hopefully the pak listing is available...\n+        if err := r.downloadFile(\"objects/info/packs\"); err != nil {\n+                return ErrNoPackInfo\n+        }\n \n-\t// after handling pack files, let's check if anything is still missing...\n-\tvar newMissing []string\n-\tfor _, hash := range r.summary.MissingObjects {\n-\t\tpath := filepath.Join(r.outputDir, \".git\", \"objects\", hash[:2], hash[2:40])\n-\t\tif _, err := os.Stat(path); err != nil {\n-\t\t\tnewMissing = append(newMissing, hash)\n-\t\t} else {\n-\t\t\tr.summary.FoundObjects = append(r.summary.FoundObjects, hash)\n-\t\t}\n-\t}\n+        // after handling pack files, let's check if anything is still missing...\n+        var newMissing []string\n+        for _, hash := range r.summary.MissingObjects {\n+                path := filepath.Join(r.outputDir, \".git\", \"objects\", hash[:2], hash[2:40])\n+                if _, err := os.Stat(path); err != nil {\n+                        newMissing = append(newMissing, hash)\n+                } else {\n+                        r.summary.FoundObjects = append(r.summary.FoundObjects, hash)\n+                }\n+        }\n \n-\tr.summary.MissingObjects = newMissing\n+        r.summary.MissingObjects = newMissing\n \n-\treturn nil\n+        return nil\n }\n \n func (r *retriever) Run() (*Summary, error) {\n \n-\tif err := r.checkVulnerable(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err := r.downloadFile(\"config\"); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err := r.downloadFile(\"HEAD\"); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// common paths to check, not necessarily required\n-\tfor _, path := range paths {\n-\t\t_ = r.downloadFile(path)\n-\t}\n-\n-\t// grab packed files\n-\tif err := r.locatePackFiles(); err == ErrNoPackInfo {\n-\t\tr.summary.PackInformationAvailable = false\n-\t\tlogrus.Debugf(\"Pack information file is not available - some objects may be missing.\")\n-\t} else if err == nil {\n-\t\tr.summary.PackInformationAvailable = true\n-\t} else { // if there's a different error, ignore it, we can continue without unpacking\n-\t\tr.summary.PackInformationAvailable = true\n-\t\tlogrus.Debugf(\"Error in unpack operation: %s\", err)\n-\t}\n-\tif len(r.summary.FoundObjects) == 0 {\n-\t\tr.summary.Status = StatusFailure\n-\t} else if len(r.summary.MissingObjects) > 0 {\n-\t\tr.summary.Status = StatusPartialSuccess\n-\t} else {\n-\t\tr.summary.Status = StatusSuccess\n-\t}\n-\n-\tif err := r.reset(); err != nil {\n-\t\tif r.summary.Status > StatusPartialSuccess {\n-\t\t\tr.summary.Status = StatusPartialSuccess\n-\t\t}\n-\t\tlogrus.Debugf(\"Failed to reset: %s\", err)\n-\t} else if err := r.checkout(); err != nil {\n-\t\tif r.summary.Status > StatusPartialSuccess {\n-\t\t\tr.summary.Status = StatusPartialSuccess\n-\t\t}\n-\t\tlogrus.Debugf(\"Failed to checkout: %s\", err)\n-\t}\n-\n-\treturn &r.summary, nil\n+        if err := r.checkVulnerable(); err != nil {\n+                return nil, err\n+        }\n+\n+        if err := r.downloadFile(\"config\"); err != nil {\n+                return nil, err\n+        }\n+\n+        if err := r.downloadFile(\"HEAD\"); err != nil {\n+                return nil, err\n+        }\n+\n+        // common paths to check, not necessarily required\n+        for _, path := range paths {\n+                _ = r.downloadFile(path)\n+        }\n+\n+        // grab packed files\n+        if err := r.locatePackFiles(); err == ErrNoPackInfo {\n+                r.summary.PackInformationAvailable = false\n+                logrus.Debugf(\"Pack information file is not available - some objects may be missing.\")\n+        } else if err == nil {\n+                r.summary.PackInformationAvailable = true\n+        } else { // if there's a different error, ignore it, we can continue without unpacking\n+                r.summary.PackInformationAvailable = true\n+                logrus.Debugf(\"Error in unpack operation: %s\", err)\n+        }\n+        if len(r.summary.FoundObjects) == 0 {\n+                r.summary.Status = StatusFailure\n+        } else if len(r.summary.MissingObjects) > 0 {\n+                r.summary.Status = StatusPartialSuccess\n+        } else {\n+                r.summary.Status = StatusSuccess\n+        }\n+\n+        if err := r.reset(); err != nil {\n+                if r.summary.Status > StatusPartialSuccess {\n+                        r.summary.Status = StatusPartialSuccess\n+                }\n+                logrus.Debugf(\"Failed to reset: %s\", err)\n+        } else if err := r.checkout(); err != nil {\n+                if r.summary.Status > StatusPartialSuccess {\n+                        r.summary.Status = StatusPartialSuccess\n+                }\n+                logrus.Debugf(\"Failed to checkout: %s\", err)\n+        }\n+\n+        return &r.summary, nil\n }\n \n func (r *retriever) analyseConfig(content []byte) error {\n-\tlines := strings.Split(string(content), \"\\n\")\n-\tvar section string\n-\tfor _, line := range lines {\n-\t\tline = strings.TrimSpace(line)\n-\t\tif strings.HasPrefix(line, \"[\") {\n-\t\t\tline = line[1:]\n-\t\t\tline = line[0 : len(line)-1]\n-\t\t\targs := strings.Split(line, \" \")\n-\t\t\tsection = args[0]\n-\t\t\tswitch section {\n-\t\t\tcase \"remote\":\n-\t\t\t\tname := \"?\"\n-\t\t\t\tif len(args) > 1 {\n-\t\t\t\t\tname = strings.TrimSuffix(args[1][1:], \"\\\"\")\n-\t\t\t\t}\n-\t\t\t\tr.summary.Config.Remotes = append(r.summary.Config.Remotes, Remote{\n-\t\t\t\t\tName: name,\n-\t\t\t\t})\n-\t\t\tcase \"branch\":\n-\t\t\t\tname := \"?\"\n-\t\t\t\tif len(args) > 1 {\n-\t\t\t\t\tname = strings.TrimSuffix(args[1][1:], \"\\\"\")\n-\t\t\t\t}\n-\t\t\t\tr.summary.Config.Branches = append(r.summary.Config.Branches, Branch{\n-\t\t\t\t\tName: name,\n-\t\t\t\t})\n-\t\t\t}\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tparts := strings.Split(line, \"=\")\n-\t\tif len(parts) <= 1 {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tkey := strings.TrimSpace(parts[0])\n-\t\tval := strings.TrimSpace(strings.Join(parts[1:], \"=\"))\n-\n-\t\tswitch section {\n-\t\tcase \"remote\":\n-\t\t\tswitch key {\n-\t\t\tcase \"url\":\n-\t\t\t\tr.summary.Config.Remotes[len(r.summary.Config.Remotes)-1].URL = val\n-\t\t\t\tif strings.Contains(val, \"/\") {\n-\t\t\t\t\tname := val[strings.Index(val, \"/\")+1:]\n-\t\t\t\t\tr.summary.Config.RepositoryName = strings.TrimSuffix(name, \".git\")\n-\t\t\t\t}\n-\t\t\t}\n-\t\tcase \"branch\":\n-\t\t\tswitch key {\n-\t\t\tcase \"remote\":\n-\t\t\t\tr.summary.Config.Branches[len(r.summary.Config.Branches)-1].Remote = val\n-\t\t\t}\n-\t\tcase \"user\":\n-\t\t\tswitch key {\n-\t\t\tcase \"name\":\n-\t\t\t\tr.summary.Config.User.Name = val\n-\t\t\tcase \"username\":\n-\t\t\t\tr.summary.Config.User.Username = val\n-\t\t\tcase \"email\":\n-\t\t\t\tr.summary.Config.User.Email = val\n-\t\t\t}\n-\t\tcase \"github\":\n-\t\t\tswitch key {\n-\t\t\tcase \"user\":\n-\t\t\t\tr.summary.Config.GithubToken.Username = val\n-\t\t\tcase \"token\":\n-\t\t\t\tr.summary.Config.GithubToken.Token = val\n-\t\t\t}\n-\t\t}\n-\n-\t}\n-\treturn nil\n+        lines := strings.Split(string(content), \"\\n\")\n+        var section string\n+        for _, line := range lines {\n+                line = strings.TrimSpace(line)\n+                if strings.HasPrefix(line, \"[\") {\n+                        line = line[1:]\n+                        line = line[0 : len(line)-1]\n+                        args := strings.Split(line, \" \")\n+                        section = args[0]\n+                        switch section {\n+                        case \"remote\":\n+                                name := \"?\"\n+                                if len(args) > 1 {\n+                                        name = strings.TrimSuffix(args[1][1:], \"\\\"\")\n+                                }\n+                                r.summary.Config.Remotes = append(r.summary.Config.Remotes, Remote{\n+                                        Name: name,\n+                                })\n+                        case \"branch\":\n+                                name := \"?\"\n+                                if len(args) > 1 {\n+                                        name = strings.TrimSuffix(args[1][1:], \"\\\"\")\n+                                }\n+                                r.summary.Config.Branches = append(r.summary.Config.Branches, Branch{\n+                                        Name: name,\n+                                })\n+                        }\n+                        continue\n+                }\n+\n+                parts := strings.Split(line, \"=\")\n+                if len(parts) <= 1 {\n+                        continue\n+                }\n+\n+                key := strings.TrimSpace(parts[0])\n+                val := strings.TrimSpace(strings.Join(parts[1:], \"=\"))\n+\n+                switch section {\n+                case \"remote\":\n+                        switch key {\n+                        case \"url\":\n+                                r.summary.Config.Remotes[len(r.summary.Config.Remotes)-1].URL = val\n+                                if strings.Contains(val, \"/\") {\n+                                        name := val[strings.Index(val, \"/\")+1:]\n+                                        r.summary.Config.RepositoryName = strings.TrimSuffix(name, \".git\")\n+                                }\n+                        }\n+                case \"branch\":\n+                        switch key {\n+                        case \"remote\":\n+                                r.summary.Config.Branches[len(r.summary.Config.Branches)-1].Remote = val\n+                        }\n+                case \"user\":\n+                        switch key {\n+                        case \"name\":\n+                                r.summary.Config.User.Name = val\n+                        case \"username\":\n+                                r.summary.Config.User.Username = val\n+                        case \"email\":\n+                                r.summary.Config.User.Email = val\n+                        }\n+                case \"github\":\n+                        switch key {\n+                        case \"user\":\n+                                r.summary.Config.GithubToken.Username = val\n+                        case \"token\":\n+                                r.summary.Config.GithubToken.Token = val\n+                        }\n+                }\n+\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2021-41803:0708", "fix_patch": "diff --git a/agent/consul/auto_config_endpoint.go b/agent/consul/auto_config_endpoint.go\nindex 088c9a3e0d..2a471cc50a 100644\n--- a/agent/consul/auto_config_endpoint.go\n+++ b/agent/consul/auto_config_endpoint.go\n@@ -1,150 +1,159 @@\n package consul\n \n import (\n-\t\"context\"\n-\t\"crypto/x509\"\n-\t\"encoding/base64\"\n-\t\"fmt\"\n-\n-\t\"github.com/hashicorp/consul/acl\"\n-\n-\tbexpr \"github.com/hashicorp/go-bexpr\"\n-\n-\t\"github.com/hashicorp/consul/agent/connect\"\n-\t\"github.com/hashicorp/consul/agent/consul/authmethod/ssoauth\"\n-\t\"github.com/hashicorp/consul/agent/structs\"\n-\t\"github.com/hashicorp/consul/lib/template\"\n-\t\"github.com/hashicorp/consul/proto/pbautoconf\"\n-\t\"github.com/hashicorp/consul/proto/pbconfig\"\n-\t\"github.com/hashicorp/consul/proto/pbconnect\"\n-\t\"github.com/hashicorp/consul/tlsutil\"\n+        \"context\"\n+        \"crypto/x509\"\n+        \"encoding/base64\"\n+        \"fmt\"\n+\"strings\"\n+\n+        \"github.com/hashicorp/consul/acl\"\n+\n+        bexpr \"github.com/hashicorp/go-bexpr\"\n+\n+        \"github.com/hashicorp/consul/agent/connect\"\n+        \"github.com/hashicorp/consul/agent/consul/authmethod/ssoauth\"\n+        \"github.com/hashicorp/consul/agent/structs\"\n+        \"github.com/hashicorp/consul/lib/template\"\n+        \"github.com/hashicorp/consul/proto/pbautoconf\"\n+        \"github.com/hashicorp/consul/proto/pbconfig\"\n+        \"github.com/hashicorp/consul/proto/pbconnect\"\n+        \"github.com/hashicorp/consul/tlsutil\"\n )\n \n type AutoConfigOptions struct {\n-\tNodeName    string\n-\tSegmentName string\n-\tPartition   string\n+        NodeName    string\n+        SegmentName string\n+        Partition   string\n \n-\tCSR      *x509.CertificateRequest\n-\tSpiffeID *connect.SpiffeIDAgent\n+        CSR      *x509.CertificateRequest\n+        SpiffeID *connect.SpiffeIDAgent\n }\n \n func (opts AutoConfigOptions) PartitionOrDefault() string {\n-\treturn acl.PartitionOrDefault(opts.Partition)\n+        return acl.PartitionOrDefault(opts.Partition)\n }\n \n type AutoConfigAuthorizer interface {\n-\t// Authorizes the request and returns a struct containing the various\n-\t// options for how to generate the configuration.\n-\tAuthorize(*pbautoconf.AutoConfigRequest) (AutoConfigOptions, error)\n+        // Authorizes the request and returns a struct containing the various\n+        // options for how to generate the configuration.\n+        Authorize(*pbautoconf.AutoConfigRequest) (AutoConfigOptions, error)\n }\n \n type disabledAuthorizer struct{}\n \n func (_ *disabledAuthorizer) Authorize(_ *pbautoconf.AutoConfigRequest) (AutoConfigOptions, error) {\n-\treturn AutoConfigOptions{}, fmt.Errorf(\"Auto Config is disabled\")\n+        return AutoConfigOptions{}, fmt.Errorf(\"Auto Config is disabled\")\n }\n \n type jwtAuthorizer struct {\n-\tvalidator       *ssoauth.Validator\n-\tallowReuse      bool\n-\tclaimAssertions []string\n+        validator       *ssoauth.Validator\n+        allowReuse      bool\n+        claimAssertions []string\n }\n \n func (a *jwtAuthorizer) Authorize(req *pbautoconf.AutoConfigRequest) (AutoConfigOptions, error) {\n-\t// perform basic JWT Authorization\n-\tidentity, err := a.validator.ValidateLogin(context.Background(), req.JWT)\n-\tif err != nil {\n-\t\t// TODO (autoconf) maybe we should add a more generic permission denied error not tied to the ACL package?\n-\t\treturn AutoConfigOptions{}, acl.PermissionDenied(\"Failed JWT authorization: %v\", err)\n-\t}\n-\n-\tvarMap := map[string]string{\n-\t\t\"node\":      req.Node,\n-\t\t\"segment\":   req.Segment,\n-\t\t\"partition\": req.PartitionOrDefault(),\n-\t}\n-\n-\tfor _, raw := range a.claimAssertions {\n-\t\t// validate and fill any HIL\n-\t\tfilled, err := template.InterpolateHIL(raw, varMap, true)\n-\t\tif err != nil {\n-\t\t\treturn AutoConfigOptions{}, fmt.Errorf(\"Failed to render claim assertion template %q: %w\", raw, err)\n-\t\t}\n-\n-\t\tevaluator, err := bexpr.CreateEvaluatorForType(filled, nil, identity.SelectableFields)\n-\t\tif err != nil {\n-\t\t\treturn AutoConfigOptions{}, fmt.Errorf(\"Failed to create evaluator for claim assertion %q: %w\", filled, err)\n-\t\t}\n-\n-\t\tok, err := evaluator.Evaluate(identity.SelectableFields)\n-\t\tif err != nil {\n-\t\t\treturn AutoConfigOptions{}, fmt.Errorf(\"Failed to execute claim assertion %q: %w\", filled, err)\n-\t\t}\n-\n-\t\tif !ok {\n-\t\t\treturn AutoConfigOptions{}, acl.PermissionDenied(\"Failed JWT claim assertion\")\n-\t\t}\n-\t}\n-\n-\topts := AutoConfigOptions{\n-\t\tNodeName:    req.Node,\n-\t\tSegmentName: req.Segment,\n-\t\tPartition:   req.Partition,\n-\t}\n-\n-\tif req.CSR != \"\" {\n-\t\tcsr, id, err := parseAutoConfigCSR(req.CSR)\n-\t\tif err != nil {\n-\t\t\treturn AutoConfigOptions{}, err\n-\t\t}\n-\n-\t\tif id.Agent != req.Node || !acl.EqualPartitions(id.Partition, req.Partition) {\n-\t\t\treturn AutoConfigOptions{},\n-\t\t\t\tfmt.Errorf(\"Spiffe ID agent name (%s) of the certificate signing request is not for the correct node (%s)\",\n-\t\t\t\t\tprintNodeName(id.Agent, id.Partition),\n-\t\t\t\t\tprintNodeName(req.Node, req.Partition),\n-\t\t\t\t)\n-\t\t}\n-\n-\t\topts.CSR = csr\n-\t\topts.SpiffeID = id\n-\t}\n-\n-\treturn opts, nil\n+        // perform basic JWT Authorization\n+        identity, err := a.validator.ValidateLogin(context.Background(), req.JWT)\n+        if err != nil {\n+                // TODO (autoconf) maybe we should add a more generic permission denied error not tied to the ACL package?\n+                return AutoConfigOptions{}, acl.PermissionDenied(\"Failed JWT authorization: %v\", err)\n+        }\n+\n+\n+if err := CheckReservedName(req.Node, \"node\"); err != nil {\n+return AutoConfigOptions{}, err\n+}\n+if err := CheckReservedName(req.Segment, \"segment\"); err != nil {\n+return AutoConfigOptions{}, err\n+}\n+\n+        varMap := map[string]string{\n+                \"node\":      req.Node,\n+                \"segment\":   req.Segment,\n+                \"partition\": req.PartitionOrDefault(),\n+        }\n+\n+        for _, raw := range a.claimAssertions {\n+                // validate and fill any HIL\n+                filled, err := template.InterpolateHIL(raw, varMap, true)\n+                if err != nil {\n+                        return AutoConfigOptions{}, fmt.Errorf(\"Failed to render claim assertion template %q: %w\", raw, err)\n+                }\n+\n+                evaluator, err := bexpr.CreateEvaluatorForType(filled, nil, identity.SelectableFields)\n+                if err != nil {\n+                        return AutoConfigOptions{}, fmt.Errorf(\"Failed to create evaluator for claim assertion %q: %w\", filled, err)\n+                }\n+\n+                ok, err := evaluator.Evaluate(identity.SelectableFields)\n+                if err != nil {\n+                        return AutoConfigOptions{}, fmt.Errorf(\"Failed to execute claim assertion %q: %w\", filled, err)\n+                }\n+\n+                if !ok {\n+                        return AutoConfigOptions{}, acl.PermissionDenied(\"Failed JWT claim assertion\")\n+                }\n+        }\n+\n+        opts := AutoConfigOptions{\n+                NodeName:    req.Node,\n+                SegmentName: req.Segment,\n+                Partition:   req.Partition,\n+        }\n+\n+        if req.CSR != \"\" {\n+                csr, id, err := parseAutoConfigCSR(req.CSR)\n+                if err != nil {\n+                        return AutoConfigOptions{}, err\n+                }\n+\n+                if id.Agent != req.Node || !acl.EqualPartitions(id.Partition, req.Partition) {\n+                        return AutoConfigOptions{},\n+                                fmt.Errorf(\"Spiffe ID agent name (%s) of the certificate signing request is not for the correct node (%s)\",\n+                                        printNodeName(id.Agent, id.Partition),\n+                                        printNodeName(req.Node, req.Partition),\n+                                )\n+                }\n+\n+                opts.CSR = csr\n+                opts.SpiffeID = id\n+        }\n+\n+        return opts, nil\n }\n \n type AutoConfigBackend interface {\n-\tCreateACLToken(template *structs.ACLToken) (*structs.ACLToken, error)\n-\tDatacenterJoinAddresses(partition, segment string) ([]string, error)\n-\tForwardRPC(method string, info structs.RPCInfo, reply interface{}) (bool, error)\n-\tGetCARoots() (*structs.IndexedCARoots, error)\n-\tSignCertificate(csr *x509.CertificateRequest, id connect.CertURI) (*structs.IssuedCert, error)\n+        CreateACLToken(template *structs.ACLToken) (*structs.ACLToken, error)\n+        DatacenterJoinAddresses(partition, segment string) ([]string, error)\n+        ForwardRPC(method string, info structs.RPCInfo, reply interface{}) (bool, error)\n+        GetCARoots() (*structs.IndexedCARoots, error)\n+        SignCertificate(csr *x509.CertificateRequest, id connect.CertURI) (*structs.IssuedCert, error)\n }\n \n // AutoConfig endpoint is used for cluster auto configuration operations\n type AutoConfig struct {\n-\t// currently AutoConfig does not support pushing down any configuration that would be reloadable on the servers\n-\t// (outside of some TLS settings such as the configured CA certs which are retrieved via the TLS configurator)\n-\t// If that changes then we will need to change this to use an atomic.Value and provide means of reloading it.\n-\tconfig          *Config\n-\ttlsConfigurator *tlsutil.Configurator\n-\n-\tbackend    AutoConfigBackend\n-\tauthorizer AutoConfigAuthorizer\n+        // currently AutoConfig does not support pushing down any configuration that would be reloadable on the servers\n+        // (outside of some TLS settings such as the configured CA certs which are retrieved via the TLS configurator)\n+        // If that changes then we will need to change this to use an atomic.Value and provide means of reloading it.\n+        config          *Config\n+        tlsConfigurator *tlsutil.Configurator\n+\n+        backend    AutoConfigBackend\n+        authorizer AutoConfigAuthorizer\n }\n \n func NewAutoConfig(conf *Config, tlsConfigurator *tlsutil.Configurator, backend AutoConfigBackend, authz AutoConfigAuthorizer) *AutoConfig {\n-\tif conf == nil {\n-\t\tconf = DefaultConfig()\n-\t}\n-\n-\treturn &AutoConfig{\n-\t\tconfig:          conf,\n-\t\ttlsConfigurator: tlsConfigurator,\n-\t\tbackend:         backend,\n-\t\tauthorizer:      authz,\n-\t}\n+        if conf == nil {\n+                conf = DefaultConfig()\n+        }\n+\n+        return &AutoConfig{\n+                config:          conf,\n+                tlsConfigurator: tlsConfigurator,\n+                backend:         backend,\n+                authorizer:      authz,\n+        }\n }\n \n // updateTLSCertificatesInConfig will ensure that the TLS settings regarding how an agent is\n@@ -152,248 +161,267 @@ func NewAutoConfig(conf *Config, tlsConfigurator *tlsutil.Configurator, backend\n // in some cases only if auto_encrypt is enabled on the servers. This endpoint has the option\n // to configure auto_encrypt or potentially in the future to generate the certificates inline.\n func (ac *AutoConfig) updateTLSCertificatesInConfig(opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\t// nothing to be done as we cannot generate certificates\n-\tif !ac.config.ConnectEnabled {\n-\t\treturn nil\n-\t}\n-\n-\tif opts.CSR != nil {\n-\t\tcert, err := ac.backend.SignCertificate(opts.CSR, opts.SpiffeID)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to sign CSR: %w\", err)\n-\t\t}\n-\n-\t\t// convert to the protobuf form of the issued certificate\n-\t\tpbcert, err := pbconnect.NewIssuedCertFromStructs(cert)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tresp.Certificate = pbcert\n-\t}\n-\n-\tconnectRoots, err := ac.backend.GetCARoots()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"Failed to lookup the CA roots: %w\", err)\n-\t}\n-\n-\t// convert to the protobuf form of the issued certificate\n-\tpbroots, err := pbconnect.NewCARootsFromStructs(connectRoots)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tresp.CARoots = pbroots\n-\n-\t// get the non-connect CA certs from the TLS Configurator\n-\tif ac.tlsConfigurator != nil {\n-\t\tresp.ExtraCACertificates = ac.tlsConfigurator.ManualCAPems()\n-\t}\n-\n-\treturn nil\n+        // nothing to be done as we cannot generate certificates\n+        if !ac.config.ConnectEnabled {\n+                return nil\n+        }\n+\n+        if opts.CSR != nil {\n+                cert, err := ac.backend.SignCertificate(opts.CSR, opts.SpiffeID)\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed to sign CSR: %w\", err)\n+                }\n+\n+                // convert to the protobuf form of the issued certificate\n+                pbcert, err := pbconnect.NewIssuedCertFromStructs(cert)\n+                if err != nil {\n+                        return err\n+                }\n+                resp.Certificate = pbcert\n+        }\n+\n+        connectRoots, err := ac.backend.GetCARoots()\n+        if err != nil {\n+                return fmt.Errorf(\"Failed to lookup the CA roots: %w\", err)\n+        }\n+\n+        // convert to the protobuf form of the issued certificate\n+        pbroots, err := pbconnect.NewCARootsFromStructs(connectRoots)\n+        if err != nil {\n+                return err\n+        }\n+\n+        resp.CARoots = pbroots\n+\n+        // get the non-connect CA certs from the TLS Configurator\n+        if ac.tlsConfigurator != nil {\n+                resp.ExtraCACertificates = ac.tlsConfigurator.ManualCAPems()\n+        }\n+\n+        return nil\n }\n \n // updateACLtokensInConfig will configure all of the agents ACL settings and will populate\n // the configuration with an agent token usable for all default agent operations.\n func (ac *AutoConfig) updateACLsInConfig(opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\tacl := &pbconfig.ACL{\n-\t\tEnabled:             ac.config.ACLsEnabled,\n-\t\tPolicyTTL:           ac.config.ACLResolverSettings.ACLPolicyTTL.String(),\n-\t\tRoleTTL:             ac.config.ACLResolverSettings.ACLRoleTTL.String(),\n-\t\tTokenTTL:            ac.config.ACLResolverSettings.ACLTokenTTL.String(),\n-\t\tDownPolicy:          ac.config.ACLResolverSettings.ACLDownPolicy,\n-\t\tDefaultPolicy:       ac.config.ACLResolverSettings.ACLDefaultPolicy,\n-\t\tEnableKeyListPolicy: ac.config.ACLEnableKeyListPolicy,\n-\t}\n-\n-\t// when ACLs are enabled we want to create a local token with a node identity\n-\tif ac.config.ACLsEnabled {\n-\t\t// set up the token template - the ids and create\n-\t\ttemplate := structs.ACLToken{\n-\t\t\tDescription: fmt.Sprintf(\"Auto Config Token for Node %q\", printNodeName(opts.NodeName, opts.Partition)),\n-\t\t\tLocal:       true,\n-\t\t\tNodeIdentities: []*structs.ACLNodeIdentity{\n-\t\t\t\t{\n-\t\t\t\t\tNodeName:   opts.NodeName,\n-\t\t\t\t\tDatacenter: ac.config.Datacenter,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tEnterpriseMeta: *structs.DefaultEnterpriseMetaInPartition(opts.PartitionOrDefault()),\n-\t\t}\n-\n-\t\ttoken, err := ac.backend.CreateACLToken(&template)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to generate an ACL token for node %q: %w\", printNodeName(opts.NodeName, opts.Partition), err)\n-\t\t}\n-\n-\t\tacl.Tokens = &pbconfig.ACLTokens{Agent: token.SecretID}\n-\t}\n-\n-\tresp.Config.ACL = acl\n-\treturn nil\n+        acl := &pbconfig.ACL{\n+                Enabled:             ac.config.ACLsEnabled,\n+                PolicyTTL:           ac.config.ACLResolverSettings.ACLPolicyTTL.String(),\n+                RoleTTL:             ac.config.ACLResolverSettings.ACLRoleTTL.String(),\n+                TokenTTL:            ac.config.ACLResolverSettings.ACLTokenTTL.String(),\n+                DownPolicy:          ac.config.ACLResolverSettings.ACLDownPolicy,\n+                DefaultPolicy:       ac.config.ACLResolverSettings.ACLDefaultPolicy,\n+                EnableKeyListPolicy: ac.config.ACLEnableKeyListPolicy,\n+        }\n+\n+        // when ACLs are enabled we want to create a local token with a node identity\n+        if ac.config.ACLsEnabled {\n+                // set up the token template - the ids and create\n+                template := structs.ACLToken{\n+                        Description: fmt.Sprintf(\"Auto Config Token for Node %q\", printNodeName(opts.NodeName, opts.Partition)),\n+                        Local:       true,\n+                        NodeIdentities: []*structs.ACLNodeIdentity{\n+                                {\n+                                        NodeName:   opts.NodeName,\n+                                        Datacenter: ac.config.Datacenter,\n+                                },\n+                        },\n+                        EnterpriseMeta: *structs.DefaultEnterpriseMetaInPartition(opts.PartitionOrDefault()),\n+                }\n+\n+                token, err := ac.backend.CreateACLToken(&template)\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed to generate an ACL token for node %q: %w\", printNodeName(opts.NodeName, opts.Partition), err)\n+                }\n+\n+                acl.Tokens = &pbconfig.ACLTokens{Agent: token.SecretID}\n+        }\n+\n+        resp.Config.ACL = acl\n+        return nil\n }\n \n // updateJoinAddressesInConfig determines the correct gossip endpoints that clients should\n // be connecting to for joining the cluster based on the segment given in the opts parameter.\n func (ac *AutoConfig) updateJoinAddressesInConfig(opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\tjoinAddrs, err := ac.backend.DatacenterJoinAddresses(opts.Partition, opts.SegmentName)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        joinAddrs, err := ac.backend.DatacenterJoinAddresses(opts.Partition, opts.SegmentName)\n+        if err != nil {\n+                return err\n+        }\n \n-\tif resp.Config.Gossip == nil {\n-\t\tresp.Config.Gossip = &pbconfig.Gossip{}\n-\t}\n+        if resp.Config.Gossip == nil {\n+                resp.Config.Gossip = &pbconfig.Gossip{}\n+        }\n \n-\tresp.Config.Gossip.RetryJoinLAN = joinAddrs\n-\treturn nil\n+        resp.Config.Gossip.RetryJoinLAN = joinAddrs\n+        return nil\n }\n \n // updateGossipEncryptionInConfig will populate the gossip encryption configuration settings\n func (ac *AutoConfig) updateGossipEncryptionInConfig(_ AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\t// Add gossip encryption settings if there is any key loaded\n-\tmemberlistConfig := ac.config.SerfLANConfig.MemberlistConfig\n-\tif lanKeyring := memberlistConfig.Keyring; lanKeyring != nil {\n-\t\tif resp.Config.Gossip == nil {\n-\t\t\tresp.Config.Gossip = &pbconfig.Gossip{}\n-\t\t}\n-\t\tif resp.Config.Gossip.Encryption == nil {\n-\t\t\tresp.Config.Gossip.Encryption = &pbconfig.GossipEncryption{}\n-\t\t}\n-\n-\t\tpk := lanKeyring.GetPrimaryKey()\n-\t\tif len(pk) > 0 {\n-\t\t\tresp.Config.Gossip.Encryption.Key = base64.StdEncoding.EncodeToString(pk)\n-\t\t}\n-\n-\t\tresp.Config.Gossip.Encryption.VerifyIncoming = memberlistConfig.GossipVerifyIncoming\n-\t\tresp.Config.Gossip.Encryption.VerifyOutgoing = memberlistConfig.GossipVerifyOutgoing\n-\t}\n-\n-\treturn nil\n+        // Add gossip encryption settings if there is any key loaded\n+        memberlistConfig := ac.config.SerfLANConfig.MemberlistConfig\n+        if lanKeyring := memberlistConfig.Keyring; lanKeyring != nil {\n+                if resp.Config.Gossip == nil {\n+                        resp.Config.Gossip = &pbconfig.Gossip{}\n+                }\n+                if resp.Config.Gossip.Encryption == nil {\n+                        resp.Config.Gossip.Encryption = &pbconfig.GossipEncryption{}\n+                }\n+\n+                pk := lanKeyring.GetPrimaryKey()\n+                if len(pk) > 0 {\n+                        resp.Config.Gossip.Encryption.Key = base64.StdEncoding.EncodeToString(pk)\n+                }\n+\n+                resp.Config.Gossip.Encryption.VerifyIncoming = memberlistConfig.GossipVerifyIncoming\n+                resp.Config.Gossip.Encryption.VerifyOutgoing = memberlistConfig.GossipVerifyOutgoing\n+        }\n+\n+        return nil\n }\n \n // updateTLSSettingsInConfig will populate the TLS configuration settings but will not\n // populate leaf or ca certficiates.\n func (ac *AutoConfig) updateTLSSettingsInConfig(_ AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\tif ac.tlsConfigurator == nil {\n-\t\t// TLS is not enabled?\n-\t\treturn nil\n-\t}\n+        if ac.tlsConfigurator == nil {\n+                // TLS is not enabled?\n+                return nil\n+        }\n \n-\tvar err error\n+        var err error\n \n-\tresp.Config.TLS, err = ac.tlsConfigurator.AutoConfigTLSSettings()\n-\treturn err\n+        resp.Config.TLS, err = ac.tlsConfigurator.AutoConfigTLSSettings()\n+        return err\n }\n \n // baseConfig will populate the configuration with some base settings such as the\n // datacenter names, node name etc.\n func (ac *AutoConfig) baseConfig(opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\tif opts.NodeName == \"\" {\n-\t\treturn fmt.Errorf(\"Cannot generate auto config response without a node name\")\n-\t}\n+        if opts.NodeName == \"\" {\n+                return fmt.Errorf(\"Cannot generate auto config response without a node name\")\n+        }\n \n-\tresp.Config.Datacenter = ac.config.Datacenter\n-\tresp.Config.PrimaryDatacenter = ac.config.PrimaryDatacenter\n-\tresp.Config.NodeName = opts.NodeName\n-\tresp.Config.SegmentName = opts.SegmentName\n-\tresp.Config.Partition = opts.Partition\n+        resp.Config.Datacenter = ac.config.Datacenter\n+        resp.Config.PrimaryDatacenter = ac.config.PrimaryDatacenter\n+        resp.Config.NodeName = opts.NodeName\n+        resp.Config.SegmentName = opts.SegmentName\n+        resp.Config.Partition = opts.Partition\n \n-\treturn nil\n+        return nil\n }\n \n type autoConfigUpdater func(c *AutoConfig, opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error\n \n var (\n-\t// variable holding the list of config updating functions to execute when generating\n-\t// the auto config response. This will allow for more easily adding extra self-contained\n-\t// configurators here in the future.\n-\tautoConfigUpdaters []autoConfigUpdater = []autoConfigUpdater{\n-\t\t(*AutoConfig).baseConfig,\n-\t\t(*AutoConfig).updateJoinAddressesInConfig,\n-\t\t(*AutoConfig).updateGossipEncryptionInConfig,\n-\t\t(*AutoConfig).updateTLSSettingsInConfig,\n-\t\t(*AutoConfig).updateACLsInConfig,\n-\t\t(*AutoConfig).updateTLSCertificatesInConfig,\n-\t}\n+        // variable holding the list of config updating functions to execute when generating\n+        // the auto config response. This will allow for more easily adding extra self-contained\n+        // configurators here in the future.\n+        autoConfigUpdaters []autoConfigUpdater = []autoConfigUpdater{\n+                (*AutoConfig).baseConfig,\n+                (*AutoConfig).updateJoinAddressesInConfig,\n+                (*AutoConfig).updateGossipEncryptionInConfig,\n+                (*AutoConfig).updateTLSSettingsInConfig,\n+                (*AutoConfig).updateACLsInConfig,\n+                (*AutoConfig).updateTLSCertificatesInConfig,\n+        }\n )\n \n // AgentAutoConfig will authorize the incoming request and then generate the configuration\n // to push down to the client\n func (ac *AutoConfig) InitialConfiguration(req *pbautoconf.AutoConfigRequest, resp *pbautoconf.AutoConfigResponse) error {\n-\t// default the datacenter to our datacenter - agents do not have to specify this as they may not\n-\t// yet know the datacenter name they are going to be in.\n-\tif req.Datacenter == \"\" {\n-\t\treq.Datacenter = ac.config.Datacenter\n-\t}\n-\n-\t// TODO (autoconf) Is performing auto configuration over the WAN really a bad idea?\n-\tif req.Datacenter != ac.config.Datacenter {\n-\t\treturn fmt.Errorf(\"invalid datacenter %q - agent auto configuration cannot target a remote datacenter\", req.Datacenter)\n-\t}\n-\n-\t// TODO (autoconf) maybe panic instead?\n-\tif ac.backend == nil {\n-\t\treturn fmt.Errorf(\"No Auto Config backend is configured\")\n-\t}\n-\n-\t// forward to the leader\n-\tif done, err := ac.backend.ForwardRPC(\"AutoConfig.InitialConfiguration\", req, resp); done {\n-\t\treturn err\n-\t}\n-\n-\t// TODO (autoconf) maybe panic instead?\n-\tif ac.authorizer == nil {\n-\t\treturn fmt.Errorf(\"No Auto Config authorizer is configured\")\n-\t}\n-\n-\t// authorize the request with the configured authorizer\n-\topts, err := ac.authorizer.Authorize(req)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tresp.Config = &pbconfig.Config{}\n-\n-\t// update all the configurations\n-\tfor _, configFn := range autoConfigUpdaters {\n-\t\tif err := configFn(ac, opts, resp); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        // default the datacenter to our datacenter - agents do not have to specify this as they may not\n+        // yet know the datacenter name they are going to be in.\n+        if req.Datacenter == \"\" {\n+                req.Datacenter = ac.config.Datacenter\n+        }\n+\n+        // TODO (autoconf) Is performing auto configuration over the WAN really a bad idea?\n+        if req.Datacenter != ac.config.Datacenter {\n+                return fmt.Errorf(\"invalid datacenter %q - agent auto configuration cannot target a remote datacenter\", req.Datacenter)\n+        }\n+\n+        // TODO (autoconf) maybe panic instead?\n+        if ac.backend == nil {\n+                return fmt.Errorf(\"No Auto Config backend is configured\")\n+        }\n+\n+        // forward to the leader\n+        if done, err := ac.backend.ForwardRPC(\"AutoConfig.InitialConfiguration\", req, resp); done {\n+                return err\n+        }\n+\n+        // TODO (autoconf) maybe panic instead?\n+        if ac.authorizer == nil {\n+                return fmt.Errorf(\"No Auto Config authorizer is configured\")\n+        }\n+\n+        // authorize the request with the configured authorizer\n+        opts, err := ac.authorizer.Authorize(req)\n+        if err != nil {\n+                return err\n+        }\n+\n+        resp.Config = &pbconfig.Config{}\n+\n+        // update all the configurations\n+        for _, configFn := range autoConfigUpdaters {\n+                if err := configFn(ac, opts, resp); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        return nil\n }\n \n func parseAutoConfigCSR(csr string) (*x509.CertificateRequest, *connect.SpiffeIDAgent, error) {\n-\t// Parse the CSR string into the x509 CertificateRequest struct\n-\tx509CSR, err := connect.ParseCSR(csr)\n-\tif err != nil {\n-\t\treturn nil, nil, fmt.Errorf(\"Failed to parse CSR: %w\", err)\n-\t}\n-\n-\t// ensure that a URI SAN is present\n-\tif len(x509CSR.URIs) < 1 {\n-\t\treturn nil, nil, fmt.Errorf(\"CSR didn't include any URI SANs\")\n-\t}\n-\n-\t// Parse the SPIFFE ID\n-\tspiffeID, err := connect.ParseCertURI(x509CSR.URIs[0])\n-\tif err != nil {\n-\t\treturn nil, nil, fmt.Errorf(\"Failed to parse the SPIFFE URI: %w\", err)\n-\t}\n-\n-\tagentID, isAgent := spiffeID.(*connect.SpiffeIDAgent)\n-\tif !isAgent {\n-\t\treturn nil, nil, fmt.Errorf(\"SPIFFE ID is not an Agent ID\")\n-\t}\n-\n-\treturn x509CSR, agentID, nil\n+        // Parse the CSR string into the x509 CertificateRequest struct\n+        x509CSR, err := connect.ParseCSR(csr)\n+        if err != nil {\n+                return nil, nil, fmt.Errorf(\"Failed to parse CSR: %w\", err)\n+        }\n+\n+        // ensure that a URI SAN is present\n+        if len(x509CSR.URIs) < 1 {\n+                return nil, nil, fmt.Errorf(\"CSR didn't include any URI SANs\")\n+        }\n+\n+        // Parse the SPIFFE ID\n+        spiffeID, err := connect.ParseCertURI(x509CSR.URIs[0])\n+        if err != nil {\n+                return nil, nil, fmt.Errorf(\"Failed to parse the SPIFFE URI: %w\", err)\n+        }\n+\n+        agentID, isAgent := spiffeID.(*connect.SpiffeIDAgent)\n+        if !isAgent {\n+                return nil, nil, fmt.Errorf(\"SPIFFE ID is not an Agent ID\")\n+        }\n+\n+        return x509CSR, agentID, nil\n+\n+// CheckReservedName checks if the given name is reserved.\n+func CheckReservedName(name, nameType string) error {\n+if strings.HasPrefix(name, \"_\") {\n+return fmt.Errorf(\"Invalid %s name %q, names starting with '_' are reserved\", nameType, name)\n+}\n+return nil\n+}\n+\n }\n \n func printNodeName(nodeName, partition string) string {\n-\tif acl.IsDefaultPartition(partition) {\n-\t\treturn nodeName\n-\t}\n-\treturn partition + \"/\" + nodeName\n+        if acl.IsDefaultPartition(partition) {\n+                return nodeName\n+        }\n+        return partition + \"/\" + nodeName\n+}\n+\n+\n+// CheckReservedName checks if the given name is reserved.\n+func CheckReservedName(name, nameType string) error {\n+if strings.HasPrefix(name, \"_\") {\n+return fmt.Errorf(\"Invalid %s name %q, names starting with '_' are reserved\", nameType, name)\n+}\n+return nil\n }\n+\n"}
{"cve":"CVE-2024-45388:0708", "fix_patch": "diff --git a/core/hoverfly_funcs.go b/core/hoverfly_funcs.go\nindex 181d2f13..b97b8c5f 100644\n--- a/core/hoverfly_funcs.go\n+++ b/core/hoverfly_funcs.go\n@@ -1,436 +1,446 @@\n package hoverfly\n \n import (\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"net/http\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\n-\t\"github.com/SpectoLabs/hoverfly/core/errors\"\n-\tv2 \"github.com/SpectoLabs/hoverfly/core/handlers/v2\"\n-\t\"github.com/SpectoLabs/hoverfly/core/matching\"\n-\t\"github.com/SpectoLabs/hoverfly/core/matching/matchers\"\n-\t\"github.com/SpectoLabs/hoverfly/core/models\"\n-\t\"github.com/SpectoLabs/hoverfly/core/modes\"\n-\t\"github.com/SpectoLabs/hoverfly/core/util\"\n-\t\"github.com/SpectoLabs/raymond\"\n-\tlog \"github.com/sirupsen/logrus\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"net/http\"\n+        \"path/filepath\"\n+        \"strings\"\n+\n+        \"github.com/SpectoLabs/hoverfly/core/errors\"\n+        v2 \"github.com/SpectoLabs/hoverfly/core/handlers/v2\"\n+        \"github.com/SpectoLabs/hoverfly/core/matching\"\n+        \"github.com/SpectoLabs/hoverfly/core/matching/matchers\"\n+        \"github.com/SpectoLabs/hoverfly/core/models\"\n+        \"github.com/SpectoLabs/hoverfly/core/modes\"\n+        \"github.com/SpectoLabs/hoverfly/core/util\"\n+        \"github.com/SpectoLabs/raymond\"\n+        log \"github.com/sirupsen/logrus\"\n )\n \n // DoRequest - performs request and returns response that should be returned to client and error\n func (hf *Hoverfly) DoRequest(request *http.Request) (*http.Response, error) {\n \n-\t// We can't have this set. And it only contains \"/pkg/net/http/\" anyway\n-\trequest.RequestURI = \"\"\n+        // We can't have this set. And it only contains \"/pkg/net/http/\" anyway\n+        request.RequestURI = \"\"\n \n-\tclient, err := GetHttpClient(hf, request.Host)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tresp, err := client.Do(request)\n+        client, err := GetHttpClient(hf, request.Host)\n+        if err != nil {\n+                return nil, err\n+        }\n+        resp, err := client.Do(request)\n \n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\tresp.Header.Set(\"Hoverfly\", \"Was-Here\")\n+        resp.Header.Set(\"Hoverfly\", \"Was-Here\")\n \n-\tif hf.Cfg.Mode == \"spy\" {\n-\t\tresp.Header.Add(\"Hoverfly\", \"Forwarded\")\n-\t}\n+        if hf.Cfg.Mode == \"spy\" {\n+                resp.Header.Add(\"Hoverfly\", \"Forwarded\")\n+        }\n \n-\treturn resp, nil\n+        return resp, nil\n \n }\n \n // GetResponse returns stored response from cache\n func (hf *Hoverfly) GetResponse(requestDetails models.RequestDetails) (*models.ResponseDetails, *errors.HoverflyError) {\n-\tvar response models.ResponseDetails\n-\tvar cachedResponse *models.CachedResponse\n-\n-\tcachedResponse, cacheErr := hf.CacheMatcher.GetCachedResponse(&requestDetails)\n-\n-\t// Get the cached response and return if there is a miss\n-\tif cacheErr == nil && cachedResponse.MatchingPair == nil {\n-\t\treturn nil, errors.MatchingFailedError(cachedResponse.ClosestMiss)\n-\t\t// If it's cached, use that response\n-\t} else if cacheErr == nil {\n-\t\tresponse = cachedResponse.MatchingPair.Response\n-\t\t//If it's not cached, perform matching to find a hit\n-\t} else {\n-\t\tmode := (hf.modeMap[modes.Simulate]).(*modes.SimulateMode)\n-\n-\t\t// Matching\n-\t\tresult := matching.Match(mode.MatchingStrategy, requestDetails, hf.Cfg.Webserver, hf.Simulation, hf.state)\n-\n-\t\t// Cache result\n-\t\tif result.Cacheable {\n-\t\t\tcachedResponse, _ = hf.CacheMatcher.SaveRequestMatcherResponsePair(requestDetails, result.Pair, result.Error)\n-\t\t}\n-\n-\t\t// If we miss, just return\n-\t\tif result.Error != nil {\n-\t\t\tlog.WithFields(log.Fields{\n-\t\t\t\t\"error\":       result.Error.Error(),\n-\t\t\t\t\"query\":       requestDetails.Query,\n-\t\t\t\t\"path\":        requestDetails.Path,\n-\t\t\t\t\"destination\": requestDetails.Destination,\n-\t\t\t\t\"method\":      requestDetails.Method,\n-\t\t\t}).Warn(\"Failed to find matching request from simulation\")\n-\n-\t\t\treturn nil, errors.MatchingFailedError(result.Error.ClosestMiss)\n-\t\t} else {\n-\t\t\tresponse = result.Pair.Response\n-\t\t}\n-\t}\n-\n-\t// Templating applies at the end, once we have loaded a response. Comes BEFORE state transitions,\n-\t// as we use the current state in templates\n-\tif response.Templated == true {\n-\t\tresponseBody, err := hf.applyBodyTemplating(&requestDetails, &response, cachedResponse)\n-\t\tif err == nil {\n-\t\t\tresponse.Body = responseBody\n-\t\t} else {\n-\t\t\tlog.Warnf(\"Failed to applying body templating: %s\", err.Error())\n-\t\t}\n-\n-\t\tresponseHeaders, err := hf.applyHeadersTemplating(&requestDetails, &response, cachedResponse)\n-\t\tif err == nil {\n-\t\t\tresponse.Headers = responseHeaders\n-\t\t} else {\n-\t\t\tlog.Warnf(\"Failed to applying headers templating: %s\", err.Error())\n-\t\t}\n-\n-\t\tresponseTransitionsState, err := hf.applyTransitionsStateTemplating(&requestDetails, &response, cachedResponse)\n-\t\tif err == nil {\n-\t\t\tresponse.TransitionsState = responseTransitionsState\n-\t\t} else {\n-\t\t\tlog.Warnf(\"Failed to applying transitions state templating: %s\", err.Error())\n-\t\t}\n-\t}\n-\n-\t// State transitions after we have the response\n-\tif response.TransitionsState != nil {\n-\t\thf.state.PatchState(response.TransitionsState)\n-\t}\n-\n-\tif response.RemovesState != nil {\n-\t\thf.state.RemoveState(response.RemovesState)\n-\t}\n-\n-\treturn &response, nil\n+        var response models.ResponseDetails\n+        var cachedResponse *models.CachedResponse\n+\n+        cachedResponse, cacheErr := hf.CacheMatcher.GetCachedResponse(&requestDetails)\n+\n+        // Get the cached response and return if there is a miss\n+        if cacheErr == nil && cachedResponse.MatchingPair == nil {\n+                return nil, errors.MatchingFailedError(cachedResponse.ClosestMiss)\n+                // If it's cached, use that response\n+        } else if cacheErr == nil {\n+                response = cachedResponse.MatchingPair.Response\n+                //If it's not cached, perform matching to find a hit\n+        } else {\n+                mode := (hf.modeMap[modes.Simulate]).(*modes.SimulateMode)\n+\n+                // Matching\n+                result := matching.Match(mode.MatchingStrategy, requestDetails, hf.Cfg.Webserver, hf.Simulation, hf.state)\n+\n+                // Cache result\n+                if result.Cacheable {\n+                        cachedResponse, _ = hf.CacheMatcher.SaveRequestMatcherResponsePair(requestDetails, result.Pair, result.Error)\n+                }\n+\n+                // If we miss, just return\n+                if result.Error != nil {\n+                        log.WithFields(log.Fields{\n+                                \"error\":       result.Error.Error(),\n+                                \"query\":       requestDetails.Query,\n+                                \"path\":        requestDetails.Path,\n+                                \"destination\": requestDetails.Destination,\n+                                \"method\":      requestDetails.Method,\n+                        }).Warn(\"Failed to find matching request from simulation\")\n+\n+                        return nil, errors.MatchingFailedError(result.Error.ClosestMiss)\n+                } else {\n+                        response = result.Pair.Response\n+                }\n+        }\n+\n+        // Templating applies at the end, once we have loaded a response. Comes BEFORE state transitions,\n+        // as we use the current state in templates\n+        if response.Templated == true {\n+                responseBody, err := hf.applyBodyTemplating(&requestDetails, &response, cachedResponse)\n+                if err == nil {\n+                        response.Body = responseBody\n+                } else {\n+                        log.Warnf(\"Failed to applying body templating: %s\", err.Error())\n+                }\n+\n+                responseHeaders, err := hf.applyHeadersTemplating(&requestDetails, &response, cachedResponse)\n+                if err == nil {\n+                        response.Headers = responseHeaders\n+                } else {\n+                        log.Warnf(\"Failed to applying headers templating: %s\", err.Error())\n+                }\n+\n+                responseTransitionsState, err := hf.applyTransitionsStateTemplating(&requestDetails, &response, cachedResponse)\n+                if err == nil {\n+                        response.TransitionsState = responseTransitionsState\n+                } else {\n+                        log.Warnf(\"Failed to applying transitions state templating: %s\", err.Error())\n+                }\n+        }\n+\n+        // State transitions after we have the response\n+        if response.TransitionsState != nil {\n+                hf.state.PatchState(response.TransitionsState)\n+        }\n+\n+        if response.RemovesState != nil {\n+                hf.state.RemoveState(response.RemovesState)\n+        }\n+\n+        return &response, nil\n }\n \n func (hf *Hoverfly) readResponseBodyFiles(pairs []v2.RequestMatcherResponsePairViewV5) v2.SimulationImportResult {\n-\tresult := v2.SimulationImportResult{}\n+        result := v2.SimulationImportResult{}\n \n-\tfor i, pair := range pairs {\n-\t\tif len(pair.Response.GetBody()) > 0 && len(pair.Response.GetBodyFile()) > 0 {\n-\t\t\tresult.AddBodyAndBodyFileWarning(i)\n-\t\t\tcontinue\n-\t\t}\n+        for i, pair := range pairs {\n+                if len(pair.Response.GetBody()) > 0 && len(pair.Response.GetBodyFile()) > 0 {\n+                        result.AddBodyAndBodyFileWarning(i)\n+                        continue\n+                }\n \n-\t\tif len(pair.Response.GetBody()) == 0 && len(pair.Response.GetBodyFile()) > 0 {\n-\t\t\tvar content string\n-\t\t\tvar err error\n+                if len(pair.Response.GetBody()) == 0 && len(pair.Response.GetBodyFile()) > 0 {\n+                        var content string\n+                        var err error\n \n-\t\t\tbodyFile := pair.Response.GetBodyFile()\n+                        bodyFile := pair.Response.GetBodyFile()\n \n-\t\t\tif util.IsURL(bodyFile) {\n-\t\t\t\tcontent, err = hf.readResponseBodyURL(bodyFile)\n-\t\t\t} else {\n-\t\t\t\tcontent, err = hf.readResponseBodyFile(bodyFile)\n-\t\t\t}\n+                        if util.IsURL(bodyFile) {\n+                                content, err = hf.readResponseBodyURL(bodyFile)\n+                        } else {\n+                                content, err = hf.readResponseBodyFile(bodyFile)\n+                        }\n \n-\t\t\tif err != nil {\n-\t\t\t\tresult.SetError(fmt.Errorf(\"data.pairs[%d].response %s\", i, err.Error()))\n-\t\t\t\treturn result\n-\t\t\t}\n+                        if err != nil {\n+                                result.SetError(fmt.Errorf(\"data.pairs[%d].response %s\", i, err.Error()))\n+                                return result\n+                        }\n \n-\t\t\tpairs[i].Response.Body = content\n-\t\t}\n-\t}\n+                        pairs[i].Response.Body = content\n+                }\n+        }\n \n-\treturn result\n+        return result\n }\n \n func (hf *Hoverfly) readResponseBodyURL(fileURL string) (string, error) {\n-\tisAllowed := false\n-\tfor _, allowedOrigin := range hf.Cfg.ResponsesBodyFilesAllowedOrigins {\n-\t\tif strings.HasPrefix(fileURL, allowedOrigin) {\n-\t\t\tisAllowed = true\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\n-\tif !isAllowed {\n-\t\treturn \"\", fmt.Errorf(\"bodyFile %s is not allowed. To allow this origin run hoverfly with -response-body-files-allow-origin\", fileURL)\n-\t}\n-\n-\tresp, err := http.DefaultClient.Get(fileURL)\n-\tif err != nil {\n-\t\terr := fmt.Errorf(\"bodyFile %s cannot be downloaded: %s\", fileURL, err.Error())\n-\t\treturn \"\", err\n-\t}\n-\n-\tcontent, err := util.GetResponseBody(resp)\n-\tif err != nil {\n-\t\terr := fmt.Errorf(\"response from bodyFile %s cannot be read: %s\", fileURL, err.Error())\n-\t\treturn \"\", err\n-\t}\n-\n-\treturn content, nil\n+        isAllowed := false\n+        for _, allowedOrigin := range hf.Cfg.ResponsesBodyFilesAllowedOrigins {\n+                if strings.HasPrefix(fileURL, allowedOrigin) {\n+                        isAllowed = true\n+                        break\n+                }\n+        }\n+\n+        if !isAllowed {\n+                return \"\", fmt.Errorf(\"bodyFile %s is not allowed. To allow this origin run hoverfly with -response-body-files-allow-origin\", fileURL)\n+        }\n+\n+        resp, err := http.DefaultClient.Get(fileURL)\n+        if err != nil {\n+                err := fmt.Errorf(\"bodyFile %s cannot be downloaded: %s\", fileURL, err.Error())\n+                return \"\", err\n+        }\n+\n+        content, err := util.GetResponseBody(resp)\n+        if err != nil {\n+                err := fmt.Errorf(\"response from bodyFile %s cannot be read: %s\", fileURL, err.Error())\n+                return \"\", err\n+        }\n+\n+        return content, nil\n }\n \n func (hf *Hoverfly) readResponseBodyFile(filePath string) (string, error) {\n-\tif filepath.IsAbs(filePath) {\n-\t\treturn \"\", fmt.Errorf(\"bodyFile contains absolute path (%s). only relative is supported\", filePath)\n-\t}\n+        if filepath.IsAbs(filePath) {\n+                return \"\", fmt.Errorf(\"bodyFile contains absolute path (%s). only relative is supported\", filePath)\n+        }\n \n-\tfileContents, err := ioutil.ReadFile(filepath.Join(hf.Cfg.ResponsesBodyFilesPath, filePath))\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n \n-\treturn string(fileContents[:]), nil\n+jointPath := filepath.Join(hf.Cfg.ResponsesBodyFilesPath, filePath)\n+absPath, err := filepath.Abs(jointPath)\n+if err != nil {\n+return \"\", err\n+}\n+if !strings.HasPrefix(absPath, hf.Cfg.ResponsesBodyFilesPath) {\n+return \"\", fmt.Errorf(\"relative path is invalid as it attempts to backtrack out of the base directory\")\n+}\n+\n+        fileContents, err := ioutil.ReadFile(jointPath)\n+        if err != nil {\n+                return \"\", err\n+        }\n+\n+        return string(fileContents[:]), nil\n }\n \n func (hf *Hoverfly) applyTransitionsStateTemplating(requestDetails *models.RequestDetails, response *models.ResponseDetails, cachedResponse *models.CachedResponse) (map[string]string, error) {\n-\tif response.TransitionsState == nil {\n-\t\treturn nil, nil\n-\t}\n-\n-\tvar stateTemplates map[string]*raymond.Template\n-\tif cachedResponse != nil && cachedResponse.ResponseStateTemplates != nil {\n-\t\tstateTemplates = cachedResponse.ResponseStateTemplates\n-\t} else {\n-\t\tstateTemplates = map[string]*raymond.Template{}\n-\t\tfor k, v := range response.TransitionsState {\n-\t\t\tstateTemplates[k], _ = hf.templator.ParseTemplate(v)\n-\t\t}\n-\n-\t\tif cachedResponse != nil {\n-\t\t\tcachedResponse.ResponseStateTemplates = stateTemplates\n-\t\t}\n-\t}\n-\n-\tvar err error\n-\tstate := make(map[string]string)\n-\n-\tfor k, v := range stateTemplates {\n-\t\tstate[k], err = hf.templator.RenderTemplate(v, requestDetails, response, hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn state, nil\n+        if response.TransitionsState == nil {\n+                return nil, nil\n+        }\n+\n+        var stateTemplates map[string]*raymond.Template\n+        if cachedResponse != nil && cachedResponse.ResponseStateTemplates != nil {\n+                stateTemplates = cachedResponse.ResponseStateTemplates\n+        } else {\n+                stateTemplates = map[string]*raymond.Template{}\n+                for k, v := range response.TransitionsState {\n+                        stateTemplates[k], _ = hf.templator.ParseTemplate(v)\n+                }\n+\n+                if cachedResponse != nil {\n+                        cachedResponse.ResponseStateTemplates = stateTemplates\n+                }\n+        }\n+\n+        var err error\n+        state := make(map[string]string)\n+\n+        for k, v := range stateTemplates {\n+                state[k], err = hf.templator.RenderTemplate(v, requestDetails, response, hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return state, nil\n }\n \n func (hf *Hoverfly) applyBodyTemplating(requestDetails *models.RequestDetails, response *models.ResponseDetails, cachedResponse *models.CachedResponse) (string, error) {\n-\tvar template *raymond.Template\n-\tif cachedResponse != nil && cachedResponse.ResponseTemplate != nil {\n-\t\ttemplate = cachedResponse.ResponseTemplate\n-\t} else {\n-\t\t// Parse and cache the template\n-\t\ttemplate, _ = hf.templator.ParseTemplate(response.Body)\n-\t\tif cachedResponse != nil {\n-\t\t\tcachedResponse.ResponseTemplate = template\n-\t\t}\n-\t}\n-\n-\treturn hf.templator.RenderTemplate(template, requestDetails, response,  hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n+        var template *raymond.Template\n+        if cachedResponse != nil && cachedResponse.ResponseTemplate != nil {\n+                template = cachedResponse.ResponseTemplate\n+        } else {\n+                // Parse and cache the template\n+                template, _ = hf.templator.ParseTemplate(response.Body)\n+                if cachedResponse != nil {\n+                        cachedResponse.ResponseTemplate = template\n+                }\n+        }\n+\n+        return hf.templator.RenderTemplate(template, requestDetails, response,  hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n }\n \n func (hf *Hoverfly) applyHeadersTemplating(requestDetails *models.RequestDetails, response *models.ResponseDetails, cachedResponse *models.CachedResponse) (map[string][]string, error) {\n-\tvar headersTemplates map[string][]*raymond.Template\n-\tif cachedResponse != nil && cachedResponse.ResponseHeadersTemplates != nil {\n-\t\theadersTemplates = cachedResponse.ResponseHeadersTemplates\n-\t} else {\n-\t\tvar header []*raymond.Template\n-\t\theadersTemplates = map[string][]*raymond.Template{}\n-\t\t// Parse and cache headers templates\n-\t\tfor k, v := range response.Headers {\n-\t\t\theader = make([]*raymond.Template, len(v))\n-\t\t\tfor i, h := range v {\n-\t\t\t\theader[i], _ = hf.templator.ParseTemplate(h)\n-\t\t\t}\n-\n-\t\t\theadersTemplates[k] = header\n-\t\t}\n-\n-\t\tif cachedResponse != nil {\n-\t\t\tcachedResponse.ResponseHeadersTemplates = headersTemplates\n-\t\t}\n-\t}\n-\n-\tvar (\n-\t\theader []string\n-\t\terr    error\n-\t)\n-\theaders := map[string][]string{}\n-\n-\t// Render headers templates\n-\tfor k, v := range headersTemplates {\n-\t\theader = make([]string, len(v))\n-\t\tfor i, h := range v {\n-\t\t\theader[i], err = hf.templator.RenderTemplate(h, requestDetails, response, hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n-\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t\theaders[k] = header\n-\t}\n-\n-\treturn headers, nil\n+        var headersTemplates map[string][]*raymond.Template\n+        if cachedResponse != nil && cachedResponse.ResponseHeadersTemplates != nil {\n+                headersTemplates = cachedResponse.ResponseHeadersTemplates\n+        } else {\n+                var header []*raymond.Template\n+                headersTemplates = map[string][]*raymond.Template{}\n+                // Parse and cache headers templates\n+                for k, v := range response.Headers {\n+                        header = make([]*raymond.Template, len(v))\n+                        for i, h := range v {\n+                                header[i], _ = hf.templator.ParseTemplate(h)\n+                        }\n+\n+                        headersTemplates[k] = header\n+                }\n+\n+                if cachedResponse != nil {\n+                        cachedResponse.ResponseHeadersTemplates = headersTemplates\n+                }\n+        }\n+\n+        var (\n+                header []string\n+                err    error\n+        )\n+        headers := map[string][]string{}\n+\n+        // Render headers templates\n+        for k, v := range headersTemplates {\n+                header = make([]string, len(v))\n+                for i, h := range v {\n+                        header[i], err = hf.templator.RenderTemplate(h, requestDetails, response, hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n+\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+                headers[k] = header\n+        }\n+\n+        return headers, nil\n }\n \n // save gets request fingerprint, extracts request body, status code and headers, then saves it to cache\n func (hf *Hoverfly) Save(request *models.RequestDetails, response *models.ResponseDetails, modeArgs *modes.ModeArguments) error {\n-\tbody := []models.RequestFieldMatchers{\n-\t\t{\n-\t\t\tMatcher: matchers.Exact,\n-\t\t\tValue:   request.Body,\n-\t\t},\n-\t}\n-\tcontentType := util.GetContentTypeFromHeaders(request.Headers)\n-\tif contentType == \"json\" {\n-\t\tbody = []models.RequestFieldMatchers{\n-\t\t\t{\n-\t\t\t\tMatcher: matchers.Json,\n-\t\t\t\tValue:   request.Body,\n-\t\t\t},\n-\t\t}\n-\t} else if contentType == \"xml\" {\n-\t\tbody = []models.RequestFieldMatchers{\n-\t\t\t{\n-\t\t\t\tMatcher: matchers.Xml,\n-\t\t\t\tValue:   request.Body,\n-\t\t\t},\n-\t\t}\n-\t} else if contentType == \"form\" {\n-\t\tif len(request.FormData) > 0 {\n-\t\t\tform := make(map[string][]models.RequestFieldMatchers)\n-\t\t\tfor formKey, formValue := range request.FormData {\n-\t\t\t\tform[formKey] = []models.RequestFieldMatchers{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\t\tValue:   formValue[0],\n-\t\t\t\t\t},\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tbody = []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: \"form\",\n-\t\t\t\t\tValue:   form,\n-\t\t\t\t},\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tvar headers map[string][]string\n-\n-\tif len(modeArgs.Headers) >= 1 {\n-\t\tif modeArgs.Headers[0] == \"*\" {\n-\t\t\theaders = request.Headers\n-\t\t} else {\n-\t\t\theaders = map[string][]string{}\n-\t\t\tfor _, header := range modeArgs.Headers {\n-\t\t\t\theaderValues := request.Headers[header]\n-\t\t\t\tif len(headerValues) > 0 {\n-\t\t\t\t\theaders[header] = headerValues\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tvar requestHeaders map[string][]models.RequestFieldMatchers\n-\tif len(headers) > 0 {\n-\t\trequestHeaders = map[string][]models.RequestFieldMatchers{}\n-\t\tfor key, values := range headers {\n-\t\t\trequestHeaders[key] = getRequestMatcherForMultipleValues(values)\n-\t\t}\n-\t}\n-\n-\tvar queries *models.QueryRequestFieldMatchers\n-\tif len(request.Query) > 0 {\n-\t\tqueries = &models.QueryRequestFieldMatchers{}\n-\t\tfor key, values := range request.Query {\n-\t\t\tqueries.Add(key, getRequestMatcherForMultipleValues(values))\n-\t\t}\n-\t}\n-\n-\tpair := models.RequestMatcherResponsePair{\n-\t\tRequestMatcher: models.RequestMatcher{\n-\t\t\tPath: []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\tValue:   request.Path,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tMethod: []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\tValue:   request.Method,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tDestination: []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\tValue:   request.Destination,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tScheme: []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\tValue:   request.Scheme,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tQuery:   queries,\n-\t\t\tBody:    body,\n-\t\t\tHeaders: requestHeaders,\n-\t\t},\n-\t\tResponse: *response,\n-\t}\n-\tif modeArgs.Stateful {\n-\t\thf.Simulation.AddPairInSequence(&pair, hf.state)\n-\t} else if modeArgs.OverwriteDuplicate {\n-\t\thf.Simulation.AddPairWithOverwritingDuplicate(&pair)\n-\t} else {\n-\t\thf.Simulation.AddPair(&pair)\n-\t}\n-\n-\tif hf.Cfg.GetMode() == modes.Spy {\n-\t\t_, _ = hf.CacheMatcher.SaveRequestMatcherResponsePair(*request, &pair, nil)\n-\t}\n-\n-\treturn nil\n+        body := []models.RequestFieldMatchers{\n+                {\n+                        Matcher: matchers.Exact,\n+                        Value:   request.Body,\n+                },\n+        }\n+        contentType := util.GetContentTypeFromHeaders(request.Headers)\n+        if contentType == \"json\" {\n+                body = []models.RequestFieldMatchers{\n+                        {\n+                                Matcher: matchers.Json,\n+                                Value:   request.Body,\n+                        },\n+                }\n+        } else if contentType == \"xml\" {\n+                body = []models.RequestFieldMatchers{\n+                        {\n+                                Matcher: matchers.Xml,\n+                                Value:   request.Body,\n+                        },\n+                }\n+        } else if contentType == \"form\" {\n+                if len(request.FormData) > 0 {\n+                        form := make(map[string][]models.RequestFieldMatchers)\n+                        for formKey, formValue := range request.FormData {\n+                                form[formKey] = []models.RequestFieldMatchers{\n+                                        {\n+                                                Matcher: matchers.Exact,\n+                                                Value:   formValue[0],\n+                                        },\n+                                }\n+                        }\n+                        body = []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: \"form\",\n+                                        Value:   form,\n+                                },\n+                        }\n+                }\n+        }\n+\n+        var headers map[string][]string\n+\n+        if len(modeArgs.Headers) >= 1 {\n+                if modeArgs.Headers[0] == \"*\" {\n+                        headers = request.Headers\n+                } else {\n+                        headers = map[string][]string{}\n+                        for _, header := range modeArgs.Headers {\n+                                headerValues := request.Headers[header]\n+                                if len(headerValues) > 0 {\n+                                        headers[header] = headerValues\n+                                }\n+                        }\n+                }\n+        }\n+\n+        var requestHeaders map[string][]models.RequestFieldMatchers\n+        if len(headers) > 0 {\n+                requestHeaders = map[string][]models.RequestFieldMatchers{}\n+                for key, values := range headers {\n+                        requestHeaders[key] = getRequestMatcherForMultipleValues(values)\n+                }\n+        }\n+\n+        var queries *models.QueryRequestFieldMatchers\n+        if len(request.Query) > 0 {\n+                queries = &models.QueryRequestFieldMatchers{}\n+                for key, values := range request.Query {\n+                        queries.Add(key, getRequestMatcherForMultipleValues(values))\n+                }\n+        }\n+\n+        pair := models.RequestMatcherResponsePair{\n+                RequestMatcher: models.RequestMatcher{\n+                        Path: []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: matchers.Exact,\n+                                        Value:   request.Path,\n+                                },\n+                        },\n+                        Method: []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: matchers.Exact,\n+                                        Value:   request.Method,\n+                                },\n+                        },\n+                        Destination: []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: matchers.Exact,\n+                                        Value:   request.Destination,\n+                                },\n+                        },\n+                        Scheme: []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: matchers.Exact,\n+                                        Value:   request.Scheme,\n+                                },\n+                        },\n+                        Query:   queries,\n+                        Body:    body,\n+                        Headers: requestHeaders,\n+                },\n+                Response: *response,\n+        }\n+        if modeArgs.Stateful {\n+                hf.Simulation.AddPairInSequence(&pair, hf.state)\n+        } else if modeArgs.OverwriteDuplicate {\n+                hf.Simulation.AddPairWithOverwritingDuplicate(&pair)\n+        } else {\n+                hf.Simulation.AddPair(&pair)\n+        }\n+\n+        if hf.Cfg.GetMode() == modes.Spy {\n+                _, _ = hf.CacheMatcher.SaveRequestMatcherResponsePair(*request, &pair, nil)\n+        }\n+\n+        return nil\n }\n \n func (hf *Hoverfly) ApplyMiddleware(pair models.RequestResponsePair) (models.RequestResponsePair, error) {\n-\tif hf.Cfg.Middleware.IsSet() {\n-\t\treturn hf.Cfg.Middleware.Execute(pair)\n-\t}\n+        if hf.Cfg.Middleware.IsSet() {\n+                return hf.Cfg.Middleware.Execute(pair)\n+        }\n \n-\treturn pair, nil\n+        return pair, nil\n }\n \n func getRequestMatcherForMultipleValues(values []string) []models.RequestFieldMatchers {\n-\tvar matcher string\n-\tvar value interface{}\n-\tif len(values) > 1 {\n-\t\tmatcher = matchers.Array\n-\t\tvalue = values\n-\t} else {\n-\t\tmatcher = matchers.Exact\n-\t\tvalue = strings.Join(values, \";\")\n-\t}\n-\treturn []models.RequestFieldMatchers{\n-\t\t{\n-\t\t\tMatcher: matcher,\n-\t\t\tValue:   value,\n-\t\t},\n-\t}\n+        var matcher string\n+        var value interface{}\n+        if len(values) > 1 {\n+                matcher = matchers.Array\n+                value = values\n+        } else {\n+                matcher = matchers.Exact\n+                value = strings.Join(values, \";\")\n+        }\n+        return []models.RequestFieldMatchers{\n+                {\n+                        Matcher: matcher,\n+                        Value:   value,\n+                },\n+        }\n }\n"}
{"cve":"CVE-2023-25165:0708", "fix_patch": "diff --git a/pkg/engine/engine.go b/pkg/engine/engine.go\nindex 00494f9d7..5cdd859d6 100644\n--- a/pkg/engine/engine.go\n+++ b/pkg/engine/engine.go\n@@ -17,31 +17,34 @@ limitations under the License.\n package engine\n \n import (\n-\t\"fmt\"\n-\t\"log\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"sort\"\n-\t\"strings\"\n-\t\"text/template\"\n-\n-\t\"github.com/pkg/errors\"\n-\t\"k8s.io/client-go/rest\"\n-\n-\t\"helm.sh/helm/v3/pkg/chart\"\n-\t\"helm.sh/helm/v3/pkg/chartutil\"\n+        \"fmt\"\n+        \"log\"\n+        \"path\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"sort\"\n+        \"strings\"\n+        \"text/template\"\n+\n+        \"github.com/pkg/errors\"\n+        \"k8s.io/client-go/rest\"\n+\n+        \"helm.sh/helm/v3/pkg/chart\"\n+        \"helm.sh/helm/v3/pkg/chartutil\"\n+        \"github.com/Masterminds/sprig/v3\"\n )\n \n // Engine is an implementation of the Helm rendering implementation for templates.\n type Engine struct {\n-\t// If strict is enabled, template rendering will fail if a template references\n-\t// a value that was not passed in.\n-\tStrict bool\n-\t// In LintMode, some 'required' template values may be missing, so don't fail\n-\tLintMode bool\n-\t// the rest config to connect to the kubernetes api\n-\tconfig *rest.Config\n+        // If strict is enabled, template rendering will fail if a template references\n+        // a value that was not passed in.\n+        Strict bool\n+        // In LintMode, some 'required' template values may be missing, so don't fail\n+        LintMode bool\n+        // the rest config to connect to the kubernetes api\n+        config *rest.Config\n+        // EnableDNS indicates whether DNS functions should be available in the template engine\n+        EnableDNS bool\n }\n \n // Render takes a chart, optional values, and value overrides, and attempts to render the Go templates.\n@@ -64,33 +67,33 @@ type Engine struct {\n // section contains a value named \"bar\", that value will be passed on to the\n // bar chart during render time.\n func (e Engine) Render(chrt *chart.Chart, values chartutil.Values) (map[string]string, error) {\n-\ttmap := allTemplates(chrt, values)\n-\treturn e.render(tmap)\n+        tmap := allTemplates(chrt, values)\n+        return e.render(tmap)\n }\n \n // Render takes a chart, optional values, and value overrides, and attempts to\n // render the Go templates using the default options.\n func Render(chrt *chart.Chart, values chartutil.Values) (map[string]string, error) {\n-\treturn new(Engine).Render(chrt, values)\n+        return new(Engine).Render(chrt, values)\n }\n \n // RenderWithClient takes a chart, optional values, and value overrides, and attempts to\n // render the Go templates using the default options. This engine is client aware and so can have template\n // functions that interact with the client\n func RenderWithClient(chrt *chart.Chart, values chartutil.Values, config *rest.Config) (map[string]string, error) {\n-\treturn Engine{\n-\t\tconfig: config,\n-\t}.Render(chrt, values)\n+        return Engine{\n+                config: config,\n+        }.Render(chrt, values)\n }\n \n // renderable is an object that can be rendered.\n type renderable struct {\n-\t// tpl is the current template.\n-\ttpl string\n-\t// vals are the values to be supplied to the template.\n-\tvals chartutil.Values\n-\t// namespace prefix to the templates of the current chart\n-\tbasePath string\n+        // tpl is the current template.\n+        tpl string\n+        // vals are the values to be supplied to the template.\n+        vals chartutil.Values\n+        // namespace prefix to the templates of the current chart\n+        basePath string\n }\n \n const warnStartDelim = \"HELM_ERR_START\"\n@@ -100,222 +103,226 @@ const recursionMaxNums = 1000\n var warnRegex = regexp.MustCompile(warnStartDelim + `((?s).*)` + warnEndDelim)\n \n func warnWrap(warn string) string {\n-\treturn warnStartDelim + warn + warnEndDelim\n+        return warnStartDelim + warn + warnEndDelim\n }\n \n // initFunMap creates the Engine's FuncMap and adds context-specific functions.\n func (e Engine) initFunMap(t *template.Template, referenceTpls map[string]renderable) {\n-\tfuncMap := funcMap()\n-\tincludedNames := make(map[string]int)\n-\n-\t// Add the 'include' function here so we can close over t.\n-\tfuncMap[\"include\"] = func(name string, data interface{}) (string, error) {\n-\t\tvar buf strings.Builder\n-\t\tif v, ok := includedNames[name]; ok {\n-\t\t\tif v > recursionMaxNums {\n-\t\t\t\treturn \"\", errors.Wrapf(fmt.Errorf(\"unable to execute template\"), \"rendering template has a nested reference name: %s\", name)\n-\t\t\t}\n-\t\t\tincludedNames[name]++\n-\t\t} else {\n-\t\t\tincludedNames[name] = 1\n-\t\t}\n-\t\terr := t.ExecuteTemplate(&buf, name, data)\n-\t\tincludedNames[name]--\n-\t\treturn buf.String(), err\n-\t}\n-\n-\t// Add the 'tpl' function here\n-\tfuncMap[\"tpl\"] = func(tpl string, vals chartutil.Values) (string, error) {\n-\t\tbasePath, err := vals.PathValue(\"Template.BasePath\")\n-\t\tif err != nil {\n-\t\t\treturn \"\", errors.Wrapf(err, \"cannot retrieve Template.Basepath from values inside tpl function: %s\", tpl)\n-\t\t}\n-\n-\t\ttemplateName, err := vals.PathValue(\"Template.Name\")\n-\t\tif err != nil {\n-\t\t\treturn \"\", errors.Wrapf(err, \"cannot retrieve Template.Name from values inside tpl function: %s\", tpl)\n-\t\t}\n-\n-\t\ttemplates := map[string]renderable{\n-\t\t\ttemplateName.(string): {\n-\t\t\t\ttpl:      tpl,\n-\t\t\t\tvals:     vals,\n-\t\t\t\tbasePath: basePath.(string),\n-\t\t\t},\n-\t\t}\n-\n-\t\tresult, err := e.renderWithReferences(templates, referenceTpls)\n-\t\tif err != nil {\n-\t\t\treturn \"\", errors.Wrapf(err, \"error during tpl function execution for %q\", tpl)\n-\t\t}\n-\t\treturn result[templateName.(string)], nil\n-\t}\n-\n-\t// Add the `required` function here so we can use lintMode\n-\tfuncMap[\"required\"] = func(warn string, val interface{}) (interface{}, error) {\n-\t\tif val == nil {\n-\t\t\tif e.LintMode {\n-\t\t\t\t// Don't fail on missing required values when linting\n-\t\t\t\tlog.Printf(\"[INFO] Missing required value: %s\", warn)\n-\t\t\t\treturn \"\", nil\n-\t\t\t}\n-\t\t\treturn val, errors.Errorf(warnWrap(warn))\n-\t\t} else if _, ok := val.(string); ok {\n-\t\t\tif val == \"\" {\n-\t\t\t\tif e.LintMode {\n-\t\t\t\t\t// Don't fail on missing required values when linting\n-\t\t\t\t\tlog.Printf(\"[INFO] Missing required value: %s\", warn)\n-\t\t\t\t\treturn \"\", nil\n-\t\t\t\t}\n-\t\t\t\treturn val, errors.Errorf(warnWrap(warn))\n-\t\t\t}\n-\t\t}\n-\t\treturn val, nil\n-\t}\n-\n-\t// Override sprig fail function for linting and wrapping message\n-\tfuncMap[\"fail\"] = func(msg string) (string, error) {\n-\t\tif e.LintMode {\n-\t\t\t// Don't fail when linting\n-\t\t\tlog.Printf(\"[INFO] Fail: %s\", msg)\n-\t\t\treturn \"\", nil\n-\t\t}\n-\t\treturn \"\", errors.New(warnWrap(msg))\n-\t}\n-\n-\t// If we are not linting and have a cluster connection, provide a Kubernetes-backed\n-\t// implementation.\n-\tif !e.LintMode && e.config != nil {\n-\t\tfuncMap[\"lookup\"] = NewLookupFunction(e.config)\n-\t}\n-\n-\tt.Funcs(funcMap)\n+        funcMap := funcMap()\n+        if e.EnableDNS {\n+                sprigFuncMap := sprig.TxtFuncMap()\n+                funcMap[\"getHostByName\"] = sprigFuncMap[\"getHostByName\"]\n+        }\n+        includedNames := make(map[string]int)\n+\n+        // Add the 'include' function here so we can close over t.\n+        funcMap[\"include\"] = func(name string, data interface{}) (string, error) {\n+                var buf strings.Builder\n+                if v, ok := includedNames[name]; ok {\n+                        if v > recursionMaxNums {\n+                                return \"\", errors.Wrapf(fmt.Errorf(\"unable to execute template\"), \"rendering template has a nested reference name: %s\", name)\n+                        }\n+                        includedNames[name]++\n+                } else {\n+                        includedNames[name] = 1\n+                }\n+                err := t.ExecuteTemplate(&buf, name, data)\n+                includedNames[name]--\n+                return buf.String(), err\n+        }\n+\n+        // Add the 'tpl' function here\n+        funcMap[\"tpl\"] = func(tpl string, vals chartutil.Values) (string, error) {\n+                basePath, err := vals.PathValue(\"Template.BasePath\")\n+                if err != nil {\n+                        return \"\", errors.Wrapf(err, \"cannot retrieve Template.Basepath from values inside tpl function: %s\", tpl)\n+                }\n+\n+                templateName, err := vals.PathValue(\"Template.Name\")\n+                if err != nil {\n+                        return \"\", errors.Wrapf(err, \"cannot retrieve Template.Name from values inside tpl function: %s\", tpl)\n+                }\n+\n+                templates := map[string]renderable{\n+                        templateName.(string): {\n+                                tpl:      tpl,\n+                                vals:     vals,\n+                                basePath: basePath.(string),\n+                        },\n+                }\n+\n+                result, err := e.renderWithReferences(templates, referenceTpls)\n+                if err != nil {\n+                        return \"\", errors.Wrapf(err, \"error during tpl function execution for %q\", tpl)\n+                }\n+                return result[templateName.(string)], nil\n+        }\n+\n+        // Add the `required` function here so we can use lintMode\n+        funcMap[\"required\"] = func(warn string, val interface{}) (interface{}, error) {\n+                if val == nil {\n+                        if e.LintMode {\n+                                // Don't fail on missing required values when linting\n+                                log.Printf(\"[INFO] Missing required value: %s\", warn)\n+                                return \"\", nil\n+                        }\n+                        return val, errors.Errorf(warnWrap(warn))\n+                } else if _, ok := val.(string); ok {\n+                        if val == \"\" {\n+                                if e.LintMode {\n+                                        // Don't fail on missing required values when linting\n+                                        log.Printf(\"[INFO] Missing required value: %s\", warn)\n+                                        return \"\", nil\n+                                }\n+                                return val, errors.Errorf(warnWrap(warn))\n+                        }\n+                }\n+                return val, nil\n+        }\n+\n+        // Override sprig fail function for linting and wrapping message\n+        funcMap[\"fail\"] = func(msg string) (string, error) {\n+                if e.LintMode {\n+                        // Don't fail when linting\n+                        log.Printf(\"[INFO] Fail: %s\", msg)\n+                        return \"\", nil\n+                }\n+                return \"\", errors.New(warnWrap(msg))\n+        }\n+\n+        // If we are not linting and have a cluster connection, provide a Kubernetes-backed\n+        // implementation.\n+        if !e.LintMode && e.config != nil {\n+                funcMap[\"lookup\"] = NewLookupFunction(e.config)\n+        }\n+\n+        t.Funcs(funcMap)\n }\n \n // render takes a map of templates/values and renders them.\n func (e Engine) render(tpls map[string]renderable) (map[string]string, error) {\n-\treturn e.renderWithReferences(tpls, tpls)\n+        return e.renderWithReferences(tpls, tpls)\n }\n \n // renderWithReferences takes a map of templates/values to render, and a map of\n // templates which can be referenced within them.\n func (e Engine) renderWithReferences(tpls, referenceTpls map[string]renderable) (rendered map[string]string, err error) {\n-\t// Basically, what we do here is start with an empty parent template and then\n-\t// build up a list of templates -- one for each file. Once all of the templates\n-\t// have been parsed, we loop through again and execute every template.\n-\t//\n-\t// The idea with this process is to make it possible for more complex templates\n-\t// to share common blocks, but to make the entire thing feel like a file-based\n-\t// template engine.\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\terr = errors.Errorf(\"rendering template failed: %v\", r)\n-\t\t}\n-\t}()\n-\tt := template.New(\"gotpl\")\n-\tif e.Strict {\n-\t\tt.Option(\"missingkey=error\")\n-\t} else {\n-\t\t// Not that zero will attempt to add default values for types it knows,\n-\t\t// but will still emit <no value> for others. We mitigate that later.\n-\t\tt.Option(\"missingkey=zero\")\n-\t}\n-\n-\te.initFunMap(t, referenceTpls)\n-\n-\t// We want to parse the templates in a predictable order. The order favors\n-\t// higher-level (in file system) templates over deeply nested templates.\n-\tkeys := sortTemplates(tpls)\n-\treferenceKeys := sortTemplates(referenceTpls)\n-\n-\tfor _, filename := range keys {\n-\t\tr := tpls[filename]\n-\t\tif _, err := t.New(filename).Parse(r.tpl); err != nil {\n-\t\t\treturn map[string]string{}, cleanupParseError(filename, err)\n-\t\t}\n-\t}\n-\n-\t// Adding the reference templates to the template context\n-\t// so they can be referenced in the tpl function\n-\tfor _, filename := range referenceKeys {\n-\t\tif t.Lookup(filename) == nil {\n-\t\t\tr := referenceTpls[filename]\n-\t\t\tif _, err := t.New(filename).Parse(r.tpl); err != nil {\n-\t\t\t\treturn map[string]string{}, cleanupParseError(filename, err)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\trendered = make(map[string]string, len(keys))\n-\tfor _, filename := range keys {\n-\t\t// Don't render partials. We don't care out the direct output of partials.\n-\t\t// They are only included from other templates.\n-\t\tif strings.HasPrefix(path.Base(filename), \"_\") {\n-\t\t\tcontinue\n-\t\t}\n-\t\t// At render time, add information about the template that is being rendered.\n-\t\tvals := tpls[filename].vals\n-\t\tvals[\"Template\"] = chartutil.Values{\"Name\": filename, \"BasePath\": tpls[filename].basePath}\n-\t\tvar buf strings.Builder\n-\t\tif err := t.ExecuteTemplate(&buf, filename, vals); err != nil {\n-\t\t\treturn map[string]string{}, cleanupExecError(filename, err)\n-\t\t}\n-\n-\t\t// Work around the issue where Go will emit \"<no value>\" even if Options(missing=zero)\n-\t\t// is set. Since missing=error will never get here, we do not need to handle\n-\t\t// the Strict case.\n-\t\trendered[filename] = strings.ReplaceAll(buf.String(), \"<no value>\", \"\")\n-\t}\n-\n-\treturn rendered, nil\n+        // Basically, what we do here is start with an empty parent template and then\n+        // build up a list of templates -- one for each file. Once all of the templates\n+        // have been parsed, we loop through again and execute every template.\n+        //\n+        // The idea with this process is to make it possible for more complex templates\n+        // to share common blocks, but to make the entire thing feel like a file-based\n+        // template engine.\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        err = errors.Errorf(\"rendering template failed: %v\", r)\n+                }\n+        }()\n+        t := template.New(\"gotpl\")\n+        if e.Strict {\n+                t.Option(\"missingkey=error\")\n+        } else {\n+                // Not that zero will attempt to add default values for types it knows,\n+                // but will still emit <no value> for others. We mitigate that later.\n+                t.Option(\"missingkey=zero\")\n+        }\n+\n+        e.initFunMap(t, referenceTpls)\n+\n+        // We want to parse the templates in a predictable order. The order favors\n+        // higher-level (in file system) templates over deeply nested templates.\n+        keys := sortTemplates(tpls)\n+        referenceKeys := sortTemplates(referenceTpls)\n+\n+        for _, filename := range keys {\n+                r := tpls[filename]\n+                if _, err := t.New(filename).Parse(r.tpl); err != nil {\n+                        return map[string]string{}, cleanupParseError(filename, err)\n+                }\n+        }\n+\n+        // Adding the reference templates to the template context\n+        // so they can be referenced in the tpl function\n+        for _, filename := range referenceKeys {\n+                if t.Lookup(filename) == nil {\n+                        r := referenceTpls[filename]\n+                        if _, err := t.New(filename).Parse(r.tpl); err != nil {\n+                                return map[string]string{}, cleanupParseError(filename, err)\n+                        }\n+                }\n+        }\n+\n+        rendered = make(map[string]string, len(keys))\n+        for _, filename := range keys {\n+                // Don't render partials. We don't care out the direct output of partials.\n+                // They are only included from other templates.\n+                if strings.HasPrefix(path.Base(filename), \"_\") {\n+                        continue\n+                }\n+                // At render time, add information about the template that is being rendered.\n+                vals := tpls[filename].vals\n+                vals[\"Template\"] = chartutil.Values{\"Name\": filename, \"BasePath\": tpls[filename].basePath}\n+                var buf strings.Builder\n+                if err := t.ExecuteTemplate(&buf, filename, vals); err != nil {\n+                        return map[string]string{}, cleanupExecError(filename, err)\n+                }\n+\n+                // Work around the issue where Go will emit \"<no value>\" even if Options(missing=zero)\n+                // is set. Since missing=error will never get here, we do not need to handle\n+                // the Strict case.\n+                rendered[filename] = strings.ReplaceAll(buf.String(), \"<no value>\", \"\")\n+        }\n+\n+        return rendered, nil\n }\n \n func cleanupParseError(filename string, err error) error {\n-\ttokens := strings.Split(err.Error(), \": \")\n-\tif len(tokens) == 1 {\n-\t\t// This might happen if a non-templating error occurs\n-\t\treturn fmt.Errorf(\"parse error in (%s): %s\", filename, err)\n-\t}\n-\t// The first token is \"template\"\n-\t// The second token is either \"filename:lineno\" or \"filename:lineNo:columnNo\"\n-\tlocation := tokens[1]\n-\t// The remaining tokens make up a stacktrace-like chain, ending with the relevant error\n-\terrMsg := tokens[len(tokens)-1]\n-\treturn fmt.Errorf(\"parse error at (%s): %s\", string(location), errMsg)\n+        tokens := strings.Split(err.Error(), \": \")\n+        if len(tokens) == 1 {\n+                // This might happen if a non-templating error occurs\n+                return fmt.Errorf(\"parse error in (%s): %s\", filename, err)\n+        }\n+        // The first token is \"template\"\n+        // The second token is either \"filename:lineno\" or \"filename:lineNo:columnNo\"\n+        location := tokens[1]\n+        // The remaining tokens make up a stacktrace-like chain, ending with the relevant error\n+        errMsg := tokens[len(tokens)-1]\n+        return fmt.Errorf(\"parse error at (%s): %s\", string(location), errMsg)\n }\n \n func cleanupExecError(filename string, err error) error {\n-\tif _, isExecError := err.(template.ExecError); !isExecError {\n-\t\treturn err\n-\t}\n-\n-\ttokens := strings.SplitN(err.Error(), \": \", 3)\n-\tif len(tokens) != 3 {\n-\t\t// This might happen if a non-templating error occurs\n-\t\treturn fmt.Errorf(\"execution error in (%s): %s\", filename, err)\n-\t}\n-\n-\t// The first token is \"template\"\n-\t// The second token is either \"filename:lineno\" or \"filename:lineNo:columnNo\"\n-\tlocation := tokens[1]\n-\n-\tparts := warnRegex.FindStringSubmatch(tokens[2])\n-\tif len(parts) >= 2 {\n-\t\treturn fmt.Errorf(\"execution error at (%s): %s\", string(location), parts[1])\n-\t}\n-\n-\treturn err\n+        if _, isExecError := err.(template.ExecError); !isExecError {\n+                return err\n+        }\n+\n+        tokens := strings.SplitN(err.Error(), \": \", 3)\n+        if len(tokens) != 3 {\n+                // This might happen if a non-templating error occurs\n+                return fmt.Errorf(\"execution error in (%s): %s\", filename, err)\n+        }\n+\n+        // The first token is \"template\"\n+        // The second token is either \"filename:lineno\" or \"filename:lineNo:columnNo\"\n+        location := tokens[1]\n+\n+        parts := warnRegex.FindStringSubmatch(tokens[2])\n+        if len(parts) >= 2 {\n+                return fmt.Errorf(\"execution error at (%s): %s\", string(location), parts[1])\n+        }\n+\n+        return err\n }\n \n func sortTemplates(tpls map[string]renderable) []string {\n-\tkeys := make([]string, len(tpls))\n-\ti := 0\n-\tfor key := range tpls {\n-\t\tkeys[i] = key\n-\t\ti++\n-\t}\n-\tsort.Sort(sort.Reverse(byPathLen(keys)))\n-\treturn keys\n+        keys := make([]string, len(tpls))\n+        i := 0\n+        for key := range tpls {\n+                keys[i] = key\n+                i++\n+        }\n+        sort.Sort(sort.Reverse(byPathLen(keys)))\n+        return keys\n }\n \n type byPathLen []string\n@@ -323,21 +330,21 @@ type byPathLen []string\n func (p byPathLen) Len() int      { return len(p) }\n func (p byPathLen) Swap(i, j int) { p[j], p[i] = p[i], p[j] }\n func (p byPathLen) Less(i, j int) bool {\n-\ta, b := p[i], p[j]\n-\tca, cb := strings.Count(a, \"/\"), strings.Count(b, \"/\")\n-\tif ca == cb {\n-\t\treturn strings.Compare(a, b) == -1\n-\t}\n-\treturn ca < cb\n+        a, b := p[i], p[j]\n+        ca, cb := strings.Count(a, \"/\"), strings.Count(b, \"/\")\n+        if ca == cb {\n+                return strings.Compare(a, b) == -1\n+        }\n+        return ca < cb\n }\n \n // allTemplates returns all templates for a chart and its dependencies.\n //\n // As it goes, it also prepares the values in a scope-sensitive manner.\n func allTemplates(c *chart.Chart, vals chartutil.Values) map[string]renderable {\n-\ttemplates := make(map[string]renderable)\n-\trecAllTpls(c, templates, vals)\n-\treturn templates\n+        templates := make(map[string]renderable)\n+        recAllTpls(c, templates, vals)\n+        return templates\n }\n \n // recAllTpls recurses through the templates in a chart.\n@@ -345,57 +352,57 @@ func allTemplates(c *chart.Chart, vals chartutil.Values) map[string]renderable {\n // As it recurses, it also sets the values to be appropriate for the template\n // scope.\n func recAllTpls(c *chart.Chart, templates map[string]renderable, vals chartutil.Values) map[string]interface{} {\n-\tsubCharts := make(map[string]interface{})\n-\tchartMetaData := struct {\n-\t\tchart.Metadata\n-\t\tIsRoot bool\n-\t}{*c.Metadata, c.IsRoot()}\n-\n-\tnext := map[string]interface{}{\n-\t\t\"Chart\":        chartMetaData,\n-\t\t\"Files\":        newFiles(c.Files),\n-\t\t\"Release\":      vals[\"Release\"],\n-\t\t\"Capabilities\": vals[\"Capabilities\"],\n-\t\t\"Values\":       make(chartutil.Values),\n-\t\t\"Subcharts\":    subCharts,\n-\t}\n-\n-\t// If there is a {{.Values.ThisChart}} in the parent metadata,\n-\t// copy that into the {{.Values}} for this template.\n-\tif c.IsRoot() {\n-\t\tnext[\"Values\"] = vals[\"Values\"]\n-\t} else if vs, err := vals.Table(\"Values.\" + c.Name()); err == nil {\n-\t\tnext[\"Values\"] = vs\n-\t}\n-\n-\tfor _, child := range c.Dependencies() {\n-\t\tsubCharts[child.Name()] = recAllTpls(child, templates, next)\n-\t}\n-\n-\tnewParentID := c.ChartFullPath()\n-\tfor _, t := range c.Templates {\n-\t\tif !isTemplateValid(c, t.Name) {\n-\t\t\tcontinue\n-\t\t}\n-\t\ttemplates[path.Join(newParentID, t.Name)] = renderable{\n-\t\t\ttpl:      string(t.Data),\n-\t\t\tvals:     next,\n-\t\t\tbasePath: path.Join(newParentID, \"templates\"),\n-\t\t}\n-\t}\n-\n-\treturn next\n+        subCharts := make(map[string]interface{})\n+        chartMetaData := struct {\n+                chart.Metadata\n+                IsRoot bool\n+        }{*c.Metadata, c.IsRoot()}\n+\n+        next := map[string]interface{}{\n+                \"Chart\":        chartMetaData,\n+                \"Files\":        newFiles(c.Files),\n+                \"Release\":      vals[\"Release\"],\n+                \"Capabilities\": vals[\"Capabilities\"],\n+                \"Values\":       make(chartutil.Values),\n+                \"Subcharts\":    subCharts,\n+        }\n+\n+        // If there is a {{.Values.ThisChart}} in the parent metadata,\n+        // copy that into the {{.Values}} for this template.\n+        if c.IsRoot() {\n+                next[\"Values\"] = vals[\"Values\"]\n+        } else if vs, err := vals.Table(\"Values.\" + c.Name()); err == nil {\n+                next[\"Values\"] = vs\n+        }\n+\n+        for _, child := range c.Dependencies() {\n+                subCharts[child.Name()] = recAllTpls(child, templates, next)\n+        }\n+\n+        newParentID := c.ChartFullPath()\n+        for _, t := range c.Templates {\n+                if !isTemplateValid(c, t.Name) {\n+                        continue\n+                }\n+                templates[path.Join(newParentID, t.Name)] = renderable{\n+                        tpl:      string(t.Data),\n+                        vals:     next,\n+                        basePath: path.Join(newParentID, \"templates\"),\n+                }\n+        }\n+\n+        return next\n }\n \n // isTemplateValid returns true if the template is valid for the chart type\n func isTemplateValid(ch *chart.Chart, templateName string) bool {\n-\tif isLibraryChart(ch) {\n-\t\treturn strings.HasPrefix(filepath.Base(templateName), \"_\")\n-\t}\n-\treturn true\n+        if isLibraryChart(ch) {\n+                return strings.HasPrefix(filepath.Base(templateName), \"_\")\n+        }\n+        return true\n }\n \n // isLibraryChart returns true if the chart is a library chart\n func isLibraryChart(c *chart.Chart) bool {\n-\treturn strings.EqualFold(c.Metadata.Type, \"library\")\n+        return strings.EqualFold(c.Metadata.Type, \"library\")\n }\ndiff --git a/pkg/engine/funcs.go b/pkg/engine/funcs.go\nindex 92b4c3383..d98f3437c 100644\n--- a/pkg/engine/funcs.go\n+++ b/pkg/engine/funcs.go\n@@ -17,14 +17,14 @@ limitations under the License.\n package engine\n \n import (\n-\t\"bytes\"\n-\t\"encoding/json\"\n-\t\"strings\"\n-\t\"text/template\"\n-\n-\t\"github.com/BurntSushi/toml\"\n-\t\"github.com/Masterminds/sprig/v3\"\n-\t\"sigs.k8s.io/yaml\"\n+        \"bytes\"\n+        \"encoding/json\"\n+        \"strings\"\n+        \"text/template\"\n+\n+        \"github.com/BurntSushi/toml\"\n+        \"github.com/Masterminds/sprig/v3\"\n+        \"sigs.k8s.io/yaml\"\n )\n \n // funcMap returns a mapping of all of the functions that Engine has.\n@@ -35,45 +35,46 @@ import (\n //\n // Known late-bound functions:\n //\n-//\t- \"include\"\n-//\t- \"tpl\"\n+//      - \"include\"\n+//      - \"tpl\"\n //\n // These are late-bound in Engine.Render().  The\n // version included in the FuncMap is a placeholder.\n //\n func funcMap() template.FuncMap {\n-\tf := sprig.TxtFuncMap()\n-\tdelete(f, \"env\")\n-\tdelete(f, \"expandenv\")\n-\n-\t// Add some extra functionality\n-\textra := template.FuncMap{\n-\t\t\"toToml\":        toTOML,\n-\t\t\"toYaml\":        toYAML,\n-\t\t\"fromYaml\":      fromYAML,\n-\t\t\"fromYamlArray\": fromYAMLArray,\n-\t\t\"toJson\":        toJSON,\n-\t\t\"fromJson\":      fromJSON,\n-\t\t\"fromJsonArray\": fromJSONArray,\n-\n-\t\t// This is a placeholder for the \"include\" function, which is\n-\t\t// late-bound to a template. By declaring it here, we preserve the\n-\t\t// integrity of the linter.\n-\t\t\"include\":  func(string, interface{}) string { return \"not implemented\" },\n-\t\t\"tpl\":      func(string, interface{}) interface{} { return \"not implemented\" },\n-\t\t\"required\": func(string, interface{}) (interface{}, error) { return \"not implemented\", nil },\n-\t\t// Provide a placeholder for the \"lookup\" function, which requires a kubernetes\n-\t\t// connection.\n-\t\t\"lookup\": func(string, string, string, string) (map[string]interface{}, error) {\n-\t\t\treturn map[string]interface{}{}, nil\n-\t\t},\n-\t}\n-\n-\tfor k, v := range extra {\n-\t\tf[k] = v\n-\t}\n-\n-\treturn f\n+        f := sprig.TxtFuncMap()\n+        delete(f, \"env\")\n+        delete(f, \"expandenv\")\n+        delete(f, \"getHostByName\")\n+\n+        // Add some extra functionality\n+        extra := template.FuncMap{\n+                \"toToml\":        toTOML,\n+                \"toYaml\":        toYAML,\n+                \"fromYaml\":      fromYAML,\n+                \"fromYamlArray\": fromYAMLArray,\n+                \"toJson\":        toJSON,\n+                \"fromJson\":      fromJSON,\n+                \"fromJsonArray\": fromJSONArray,\n+\n+                // This is a placeholder for the \"include\" function, which is\n+                // late-bound to a template. By declaring it here, we preserve the\n+                // integrity of the linter.\n+                \"include\":  func(string, interface{}) string { return \"not implemented\" },\n+                \"tpl\":      func(string, interface{}) interface{} { return \"not implemented\" },\n+                \"required\": func(string, interface{}) (interface{}, error) { return \"not implemented\", nil },\n+                // Provide a placeholder for the \"lookup\" function, which requires a kubernetes\n+                // connection.\n+                \"lookup\": func(string, string, string, string) (map[string]interface{}, error) {\n+                        return map[string]interface{}{}, nil\n+                },\n+        }\n+\n+        for k, v := range extra {\n+                f[k] = v\n+        }\n+\n+        return f\n }\n \n // toYAML takes an interface, marshals it to yaml, and returns a string. It will\n@@ -81,12 +82,12 @@ func funcMap() template.FuncMap {\n //\n // This is designed to be called from a template.\n func toYAML(v interface{}) string {\n-\tdata, err := yaml.Marshal(v)\n-\tif err != nil {\n-\t\t// Swallow errors inside of a template.\n-\t\treturn \"\"\n-\t}\n-\treturn strings.TrimSuffix(string(data), \"\\n\")\n+        data, err := yaml.Marshal(v)\n+        if err != nil {\n+                // Swallow errors inside of a template.\n+                return \"\"\n+        }\n+        return strings.TrimSuffix(string(data), \"\\n\")\n }\n \n // fromYAML converts a YAML document into a map[string]interface{}.\n@@ -96,12 +97,12 @@ func toYAML(v interface{}) string {\n // it tolerates errors. It will insert the returned error message string into\n // m[\"Error\"] in the returned map.\n func fromYAML(str string) map[string]interface{} {\n-\tm := map[string]interface{}{}\n+        m := map[string]interface{}{}\n \n-\tif err := yaml.Unmarshal([]byte(str), &m); err != nil {\n-\t\tm[\"Error\"] = err.Error()\n-\t}\n-\treturn m\n+        if err := yaml.Unmarshal([]byte(str), &m); err != nil {\n+                m[\"Error\"] = err.Error()\n+        }\n+        return m\n }\n \n // fromYAMLArray converts a YAML array into a []interface{}.\n@@ -111,12 +112,12 @@ func fromYAML(str string) map[string]interface{} {\n // it tolerates errors. It will insert the returned error message string as\n // the first and only item in the returned array.\n func fromYAMLArray(str string) []interface{} {\n-\ta := []interface{}{}\n+        a := []interface{}{}\n \n-\tif err := yaml.Unmarshal([]byte(str), &a); err != nil {\n-\t\ta = []interface{}{err.Error()}\n-\t}\n-\treturn a\n+        if err := yaml.Unmarshal([]byte(str), &a); err != nil {\n+                a = []interface{}{err.Error()}\n+        }\n+        return a\n }\n \n // toTOML takes an interface, marshals it to toml, and returns a string. It will\n@@ -124,13 +125,13 @@ func fromYAMLArray(str string) []interface{} {\n //\n // This is designed to be called from a template.\n func toTOML(v interface{}) string {\n-\tb := bytes.NewBuffer(nil)\n-\te := toml.NewEncoder(b)\n-\terr := e.Encode(v)\n-\tif err != nil {\n-\t\treturn err.Error()\n-\t}\n-\treturn b.String()\n+        b := bytes.NewBuffer(nil)\n+        e := toml.NewEncoder(b)\n+        err := e.Encode(v)\n+        if err != nil {\n+                return err.Error()\n+        }\n+        return b.String()\n }\n \n // toJSON takes an interface, marshals it to json, and returns a string. It will\n@@ -138,12 +139,12 @@ func toTOML(v interface{}) string {\n //\n // This is designed to be called from a template.\n func toJSON(v interface{}) string {\n-\tdata, err := json.Marshal(v)\n-\tif err != nil {\n-\t\t// Swallow errors inside of a template.\n-\t\treturn \"\"\n-\t}\n-\treturn string(data)\n+        data, err := json.Marshal(v)\n+        if err != nil {\n+                // Swallow errors inside of a template.\n+                return \"\"\n+        }\n+        return string(data)\n }\n \n // fromJSON converts a JSON document into a map[string]interface{}.\n@@ -153,12 +154,12 @@ func toJSON(v interface{}) string {\n // it tolerates errors. It will insert the returned error message string into\n // m[\"Error\"] in the returned map.\n func fromJSON(str string) map[string]interface{} {\n-\tm := make(map[string]interface{})\n+        m := make(map[string]interface{})\n \n-\tif err := json.Unmarshal([]byte(str), &m); err != nil {\n-\t\tm[\"Error\"] = err.Error()\n-\t}\n-\treturn m\n+        if err := json.Unmarshal([]byte(str), &m); err != nil {\n+                m[\"Error\"] = err.Error()\n+        }\n+        return m\n }\n \n // fromJSONArray converts a JSON array into a []interface{}.\n@@ -168,10 +169,10 @@ func fromJSON(str string) map[string]interface{} {\n // it tolerates errors. It will insert the returned error message string as\n // the first and only item in the returned array.\n func fromJSONArray(str string) []interface{} {\n-\ta := []interface{}{}\n+        a := []interface{}{}\n \n-\tif err := json.Unmarshal([]byte(str), &a); err != nil {\n-\t\ta = []interface{}{err.Error()}\n-\t}\n-\treturn a\n+        if err := json.Unmarshal([]byte(str), &a); err != nil {\n+                a = []interface{}{err.Error()}\n+        }\n+        return a\n }\n"}
{"cve":"CVE-2021-22538:0708", "fix_patch": "diff --git a/pkg/rbac/rbac.go b/pkg/rbac/rbac.go\nindex 18c433bd..4a759520 100644\n--- a/pkg/rbac/rbac.go\n+++ b/pkg/rbac/rbac.go\n@@ -16,99 +16,99 @@\n package rbac\n \n import (\n-\t\"database/sql/driver\"\n-\t\"fmt\"\n-\t\"sort\"\n+        \"database/sql/driver\"\n+        \"fmt\"\n+        \"sort\"\n )\n \n var (\n-\t// PermissionMap is the list of permissions mapped to their name and\n-\t// description.\n-\tPermissionMap = map[Permission][2]string{\n-\t\tAuditRead:      {\"AuditRead\", \"read event and audit logs\"},\n-\t\tAPIKeyRead:     {\"APIKeyRead\", \"view information about API keys, including statistics\"},\n-\t\tAPIKeyWrite:    {\"APIKeyWrite\", \"create, update, and delete API keys\"},\n-\t\tCodeIssue:      {\"CodeIssue\", \"issue codes\"},\n-\t\tCodeBulkIssue:  {\"CodeBulkIssue\", \"issue codes in bulk, if bulk issue is enabled on the realm\"},\n-\t\tCodeRead:       {\"CodeRead\", \"lookup code status\"},\n-\t\tCodeExpire:     {\"CodeExpire\", \"expire codes\"},\n-\t\tSettingsRead:   {\"SettingsRead\", \"read realm settings\"},\n-\t\tSettingsWrite:  {\"SettingsWrite\", \"update realm settings\"},\n-\t\tStatsRead:      {\"StatsRead\", \"view realm statistics\"},\n-\t\tMobileAppRead:  {\"MobileAppRead\", \"view mobile app information\"},\n-\t\tMobileAppWrite: {\"MobileAppWrite\", \"create, update, and delete mobile apps\"},\n-\t\tUserRead:       {\"UserRead\", \"view user information\"},\n-\t\tUserWrite:      {\"UserWrite\", \"create, update, and delete users\"},\n-\t}\n-\n-\t// NamePermissionMap is the map of permission names to their value.\n-\tNamePermissionMap map[string]Permission\n+        // PermissionMap is the list of permissions mapped to their name and\n+        // description.\n+        PermissionMap = map[Permission][2]string{\n+                AuditRead:      {\"AuditRead\", \"read event and audit logs\"},\n+                APIKeyRead:     {\"APIKeyRead\", \"view information about API keys, including statistics\"},\n+                APIKeyWrite:    {\"APIKeyWrite\", \"create, update, and delete API keys\"},\n+                CodeIssue:      {\"CodeIssue\", \"issue codes\"},\n+                CodeBulkIssue:  {\"CodeBulkIssue\", \"issue codes in bulk, if bulk issue is enabled on the realm\"},\n+                CodeRead:       {\"CodeRead\", \"lookup code status\"},\n+                CodeExpire:     {\"CodeExpire\", \"expire codes\"},\n+                SettingsRead:   {\"SettingsRead\", \"read realm settings\"},\n+                SettingsWrite:  {\"SettingsWrite\", \"update realm settings\"},\n+                StatsRead:      {\"StatsRead\", \"view realm statistics\"},\n+                MobileAppRead:  {\"MobileAppRead\", \"view mobile app information\"},\n+                MobileAppWrite: {\"MobileAppWrite\", \"create, update, and delete mobile apps\"},\n+                UserRead:       {\"UserRead\", \"view user information\"},\n+                UserWrite:      {\"UserWrite\", \"create, update, and delete users\"},\n+        }\n+\n+        // NamePermissionMap is the map of permission names to their value.\n+        NamePermissionMap map[string]Permission\n )\n \n func init() {\n-\tNamePermissionMap = make(map[string]Permission, len(PermissionMap))\n-\tfor k, v := range PermissionMap {\n-\t\tNamePermissionMap[v[0]] = k\n-\t}\n+        NamePermissionMap = make(map[string]Permission, len(PermissionMap))\n+        for k, v := range PermissionMap {\n+                NamePermissionMap[v[0]] = k\n+        }\n }\n \n // Can returns true if the given resource has permission to perform the provided\n // permissions.\n func Can(given Permission, target Permission) bool {\n-\treturn int64(given)&int64(target) != 0\n+        return int64(given)&int64(target) != 0\n }\n \n // CompileAndAuthorize compiles a new permission bit from the given toUpdate\n // permissions. It verifies that the calling permission has a superset of all\n // provided permissions (to prevent privilege escalation).\n func CompileAndAuthorize(actorPermission Permission, toUpdate []Permission) (Permission, error) {\n-\tvar permission Permission\n-\tfor _, update := range toUpdate {\n-\t\t// Verify that the user making changes has the permissions they are trying\n-\t\t// to grant. It is not valid for someone to grant permissions larger than\n-\t\t// they currently have.\n-\t\tif !Can(actorPermission, update) {\n-\t\t\treturn 0, fmt.Errorf(\"actor does not have all scopes which are being granted\")\n-\t\t}\n-\t\tpermission = permission | update\n-\t}\n-\n-\t// Ensure implied permissions. The actor must also have the implied\n-\t// permissions by definition.\n-\tpermission = AddImplied(permission)\n-\treturn permission, nil\n+        var permission Permission\n+        for _, update := range toUpdate {\n+                // Verify that the user making changes has the permissions they are trying\n+                // to grant. It is not valid for someone to grant permissions larger than\n+                // they currently have.\n+                if (actorPermission & update) != update {\n+                        return 0, fmt.Errorf(\"actor does not have all scopes which are being granted\")\n+                }\n+                permission = permission | update\n+        }\n+\n+        // Ensure implied permissions. The actor must also have the implied\n+        // permissions by definition.\n+        permission = AddImplied(permission)\n+        return permission, nil\n }\n \n // AddImplied adds any missing implied permissions.\n func AddImplied(target Permission) Permission {\n-\tfor has, needs := range requiredPermission {\n-\t\t// If granted has, ensure that we have all needs.\n-\t\tif Can(target, has) {\n-\t\t\tfor _, required := range needs {\n-\t\t\t\ttarget = target | required\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn target\n+        for has, needs := range requiredPermission {\n+                // If granted has, ensure that we have all needs.\n+                if Can(target, has) {\n+                        for _, required := range needs {\n+                                target = target | required\n+                        }\n+                }\n+        }\n+        return target\n }\n \n // ImpliedBy returns any permissions that cause this permission to be added\n // automatically. The return may be nil.\n func ImpliedBy(permission Permission) []Permission {\n-\treturn impliedBy[permission]\n+        return impliedBy[permission]\n }\n \n // PermissionNames returns the list of permissions included in the given\n // permission.\n func PermissionNames(p Permission) []string {\n-\tnames := make([]string, 0, len(PermissionMap))\n-\tfor v, k := range PermissionMap {\n-\t\tif Can(p, v) {\n-\t\t\tnames = append(names, k[0])\n-\t\t}\n-\t}\n-\tsort.Strings(names)\n-\treturn names\n+        names := make([]string, 0, len(PermissionMap))\n+        for v, k := range PermissionMap {\n+                if Can(p, v) {\n+                        names = append(names, k[0])\n+                }\n+        }\n+        sort.Strings(names)\n+        return names\n }\n \n // Permission is a granular permission. It is an integer instead of a uint\n@@ -117,60 +117,60 @@ type Permission int64\n \n // String implements stringer.\n func (p Permission) String() string {\n-\tif v, ok := PermissionMap[p]; ok {\n-\t\treturn v[0]\n-\t}\n-\treturn fmt.Sprintf(\"Permission(%d)\", int64(p))\n+        if v, ok := PermissionMap[p]; ok {\n+                return v[0]\n+        }\n+        return fmt.Sprintf(\"Permission(%d)\", int64(p))\n }\n \n // Value returns the permissions value as an integer for sql drivers.\n func (p Permission) Value() (driver.Value, error) {\n-\treturn int64(p), nil\n+        return int64(p), nil\n }\n \n // Description returns the description.\n func (p Permission) Description() (string, error) {\n-\tif v, ok := PermissionMap[p]; ok {\n-\t\treturn v[1], nil\n-\t}\n-\treturn \"\", fmt.Errorf(\"missing description for %s\", p)\n+        if v, ok := PermissionMap[p]; ok {\n+                return v[1], nil\n+        }\n+        return \"\", fmt.Errorf(\"missing description for %s\", p)\n }\n \n // Implied returns the additional implied permissions, if any.\n func (p Permission) Implied() []Permission {\n-\treturn requiredPermission[p]\n+        return requiredPermission[p]\n }\n \n const (\n-\t_ Permission = 1 << iota\n+        _ Permission = 1 << iota\n \n-\t// Audit\n-\tAuditRead\n+        // Audit\n+        AuditRead\n \n-\t// API keys\n-\tAPIKeyRead\n-\tAPIKeyWrite\n+        // API keys\n+        APIKeyRead\n+        APIKeyWrite\n \n-\t// Codes\n-\tCodeIssue\n-\tCodeBulkIssue\n-\tCodeRead\n-\tCodeExpire\n+        // Codes\n+        CodeIssue\n+        CodeBulkIssue\n+        CodeRead\n+        CodeExpire\n \n-\t// Realm settings\n-\tSettingsRead\n-\tSettingsWrite\n+        // Realm settings\n+        SettingsRead\n+        SettingsWrite\n \n-\t// Realm statistics\n-\tStatsRead\n+        // Realm statistics\n+        StatsRead\n \n-\t// Mobile apps\n-\tMobileAppRead\n-\tMobileAppWrite\n+        // Mobile apps\n+        MobileAppRead\n+        MobileAppWrite\n \n-\t// Users\n-\tUserRead\n-\tUserWrite\n+        // Users\n+        UserRead\n+        UserWrite\n )\n \n // --\n@@ -179,32 +179,32 @@ const (\n // --\n \n var (\n-\t// requiredPermissions is not exported since maps cannot be constant.\n-\trequiredPermission = map[Permission][]Permission{\n-\t\tAPIKeyWrite:    {APIKeyRead},\n-\t\tCodeBulkIssue:  {CodeIssue},\n-\t\tSettingsWrite:  {SettingsRead},\n-\t\tMobileAppWrite: {MobileAppRead},\n-\t\tUserWrite:      {UserRead},\n-\t}\n-\n-\t// This is the inverse of the above map, set by the init() func.\n-\t// Done in code to ensure it always stays in sync with requiredPermission.\n-\timpliedBy = make(map[Permission][]Permission)\n+        // requiredPermissions is not exported since maps cannot be constant.\n+        requiredPermission = map[Permission][]Permission{\n+                APIKeyWrite:    {APIKeyRead},\n+                CodeBulkIssue:  {CodeIssue},\n+                SettingsWrite:  {SettingsRead},\n+                MobileAppWrite: {MobileAppRead},\n+                UserWrite:      {UserRead},\n+        }\n+\n+        // This is the inverse of the above map, set by the init() func.\n+        // Done in code to ensure it always stays in sync with requiredPermission.\n+        impliedBy = make(map[Permission][]Permission)\n )\n \n // Note: there are multiple init functions in this file. They are organized to be\n // near the thing they are initializing.\n // Yes, go allows multiple init functions in the same module.\n func init() {\n-\tfor has, needs := range requiredPermission {\n-\t\tfor _, perm := range needs {\n-\t\t\tif _, ok := impliedBy[perm]; !ok {\n-\t\t\t\timpliedBy[perm] = make([]Permission, 0, 1)\n-\t\t\t}\n-\t\t\timpliedBy[perm] = append(impliedBy[perm], has)\n-\t\t}\n-\t}\n+        for has, needs := range requiredPermission {\n+                for _, perm := range needs {\n+                        if _, ok := impliedBy[perm]; !ok {\n+                                impliedBy[perm] = make([]Permission, 0, 1)\n+                        }\n+                        impliedBy[perm] = append(impliedBy[perm], has)\n+                }\n+        }\n }\n \n // --\n@@ -212,15 +212,15 @@ func init() {\n // --\n \n const (\n-\t// LegacyRealmUser is a quick reference to the old \"user\" permissions.\n-\tLegacyRealmUser Permission = CodeIssue | CodeBulkIssue | CodeRead | CodeExpire\n-\n-\t// LegacyRealmAdmin is a quick reference to the old \"realm admin\" permissions.\n-\tLegacyRealmAdmin Permission = AuditRead |\n-\t\tAPIKeyRead | APIKeyWrite |\n-\t\tCodeIssue | CodeBulkIssue | CodeRead | CodeExpire |\n-\t\tSettingsRead | SettingsWrite |\n-\t\tStatsRead |\n-\t\tMobileAppRead | MobileAppWrite |\n-\t\tUserRead | UserWrite\n+        // LegacyRealmUser is a quick reference to the old \"user\" permissions.\n+        LegacyRealmUser Permission = CodeIssue | CodeBulkIssue | CodeRead | CodeExpire\n+\n+        // LegacyRealmAdmin is a quick reference to the old \"realm admin\" permissions.\n+        LegacyRealmAdmin Permission = AuditRead |\n+                APIKeyRead | APIKeyWrite |\n+                CodeIssue | CodeBulkIssue | CodeRead | CodeExpire |\n+                SettingsRead | SettingsWrite |\n+                StatsRead |\n+                MobileAppRead | MobileAppWrite |\n+                UserRead | UserWrite\n )\n"}
{"cve":"CVE-2023-23947:0708", "fix_patch": "diff --git a/server/cluster/cluster.go b/server/cluster/cluster.go\nindex 2225a5d6f..28a1d4629 100644\n--- a/server/cluster/cluster.go\n+++ b/server/cluster/cluster.go\n@@ -1,417 +1,424 @@\n package cluster\n \n import (\n-\t\"net/url\"\n-\t\"time\"\n-\n-\t\"context\"\n-\n-\t\"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n-\tv1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n-\t\"k8s.io/apimachinery/pkg/util/sets\"\n-\t\"k8s.io/client-go/kubernetes\"\n-\n-\t\"github.com/argoproj/argo-cd/v2/pkg/apiclient/cluster\"\n-\tappv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n-\tservercache \"github.com/argoproj/argo-cd/v2/server/cache\"\n-\t\"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n-\t\"github.com/argoproj/argo-cd/v2/util/argo\"\n-\t\"github.com/argoproj/argo-cd/v2/util/clusterauth\"\n-\t\"github.com/argoproj/argo-cd/v2/util/db\"\n-\t\"github.com/argoproj/argo-cd/v2/util/rbac\"\n+        \"net/url\"\n+        \"time\"\n+\n+        \"context\"\n+\n+        \"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n+        log \"github.com/sirupsen/logrus\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n+        v1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+        \n+        \"k8s.io/client-go/kubernetes\"\n+\n+        \"github.com/argoproj/argo-cd/v2/pkg/apiclient/cluster\"\n+        appv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n+        servercache \"github.com/argoproj/argo-cd/v2/server/cache\"\n+        \"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n+        \"github.com/argoproj/argo-cd/v2/util/argo\"\n+        \"github.com/argoproj/argo-cd/v2/util/clusterauth\"\n+        \"github.com/argoproj/argo-cd/v2/util/db\"\n+        \"github.com/argoproj/argo-cd/v2/util/rbac\"\n )\n \n // Server provides a Cluster service\n type Server struct {\n-\tdb      db.ArgoDB\n-\tenf     *rbac.Enforcer\n-\tcache   *servercache.Cache\n-\tkubectl kube.Kubectl\n+        db      db.ArgoDB\n+        enf     *rbac.Enforcer\n+        cache   *servercache.Cache\n+        kubectl kube.Kubectl\n }\n \n // NewServer returns a new instance of the Cluster service\n func NewServer(db db.ArgoDB, enf *rbac.Enforcer, cache *servercache.Cache, kubectl kube.Kubectl) *Server {\n-\treturn &Server{\n-\t\tdb:      db,\n-\t\tenf:     enf,\n-\t\tcache:   cache,\n-\t\tkubectl: kubectl,\n-\t}\n+        return &Server{\n+                db:      db,\n+                enf:     enf,\n+                cache:   cache,\n+                kubectl: kubectl,\n+        }\n }\n \n func createRBACObject(project string, server string) string {\n-\tif project != \"\" {\n-\t\treturn project + \"/\" + server\n-\t}\n-\treturn server\n+        if project != \"\" {\n+                return project + \"/\" + server\n+        }\n+        return server\n }\n \n // List returns list of clusters\n func (s *Server) List(ctx context.Context, q *cluster.ClusterQuery) (*appv1.ClusterList, error) {\n-\tclusterList, err := s.db.ListClusters(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\titems := make([]appv1.Cluster, 0)\n-\tfor _, clust := range clusterList.Items {\n-\t\tif s.enf.Enforce(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionGet, createRBACObject(clust.Project, clust.Server)) {\n-\t\t\titems = append(items, clust)\n-\t\t}\n-\t}\n-\terr = kube.RunAllAsync(len(items), func(i int) error {\n-\t\titems[i] = *s.toAPIResponse(&items[i])\n-\t\treturn nil\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tclusterList.Items = items\n-\treturn clusterList, nil\n+        clusterList, err := s.db.ListClusters(ctx)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        items := make([]appv1.Cluster, 0)\n+        for _, clust := range clusterList.Items {\n+                if s.enf.Enforce(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionGet, createRBACObject(clust.Project, clust.Server)) {\n+                        items = append(items, clust)\n+                }\n+        }\n+        err = kube.RunAllAsync(len(items), func(i int) error {\n+                items[i] = *s.toAPIResponse(&items[i])\n+                return nil\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+        clusterList.Items = items\n+        return clusterList, nil\n }\n \n // Create creates a cluster\n func (s *Server) Create(ctx context.Context, q *cluster.ClusterCreateRequest) (*appv1.Cluster, error) {\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionCreate, createRBACObject(q.Cluster.Project, q.Cluster.Server)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tc := q.Cluster\n-\tserverVersion, err := s.kubectl.GetServerVersion(c.RESTConfig())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclust, err := s.db.CreateCluster(ctx, c)\n-\tif err != nil {\n-\t\tif status.Convert(err).Code() == codes.AlreadyExists {\n-\t\t\t// act idempotent if existing spec matches new spec\n-\t\t\texisting, getErr := s.db.GetCluster(ctx, c.Server)\n-\t\t\tif getErr != nil {\n-\t\t\t\treturn nil, status.Errorf(codes.Internal, \"unable to check existing cluster details: %v\", getErr)\n-\t\t\t}\n-\n-\t\t\tif existing.Equals(c) {\n-\t\t\t\tclust = existing\n-\t\t\t} else if q.Upsert {\n-\t\t\t\treturn s.Update(ctx, &cluster.ClusterUpdateRequest{Cluster: c})\n-\t\t\t} else {\n-\t\t\t\treturn nil, status.Errorf(codes.InvalidArgument, argo.GenerateSpecIsDifferentErrorMessage(\"cluster\", existing, c))\n-\t\t\t}\n-\t\t} else {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\terr = s.cache.SetClusterInfo(c.Server, &appv1.ClusterInfo{\n-\t\tServerVersion: serverVersion,\n-\t\tConnectionState: appv1.ConnectionState{\n-\t\t\tStatus:     appv1.ConnectionStatusSuccessful,\n-\t\t\tModifiedAt: &v1.Time{Time: time.Now()},\n-\t\t},\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn s.toAPIResponse(clust), err\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionCreate, createRBACObject(q.Cluster.Project, q.Cluster.Server)); err != nil {\n+                return nil, err\n+        }\n+        c := q.Cluster\n+        serverVersion, err := s.kubectl.GetServerVersion(c.RESTConfig())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        clust, err := s.db.CreateCluster(ctx, c)\n+        if err != nil {\n+                if status.Convert(err).Code() == codes.AlreadyExists {\n+                        // act idempotent if existing spec matches new spec\n+                        existing, getErr := s.db.GetCluster(ctx, c.Server)\n+                        if getErr != nil {\n+                                return nil, status.Errorf(codes.Internal, \"unable to check existing cluster details: %v\", getErr)\n+                        }\n+\n+                        if existing.Equals(c) {\n+                                clust = existing\n+                        } else if q.Upsert {\n+                                return s.Update(ctx, &cluster.ClusterUpdateRequest{Cluster: c})\n+                        } else {\n+                                return nil, status.Errorf(codes.InvalidArgument, argo.GenerateSpecIsDifferentErrorMessage(\"cluster\", existing, c))\n+                        }\n+                } else {\n+                        return nil, err\n+                }\n+        }\n+\n+        err = s.cache.SetClusterInfo(c.Server, &appv1.ClusterInfo{\n+                ServerVersion: serverVersion,\n+                ConnectionState: appv1.ConnectionState{\n+                        Status:     appv1.ConnectionStatusSuccessful,\n+                        ModifiedAt: &v1.Time{Time: time.Now()},\n+                },\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+        return s.toAPIResponse(clust), err\n }\n \n // Get returns a cluster from a query\n func (s *Server) Get(ctx context.Context, q *cluster.ClusterQuery) (*appv1.Cluster, error) {\n-\tc, err := s.getClusterWith403IfNotExist(ctx, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        c, err := s.getClusterWith403IfNotExist(ctx, q)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionGet, createRBACObject(c.Project, q.Server)); err != nil {\n-\t\treturn nil, err\n-\t}\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionGet, createRBACObject(c.Project, q.Server)); err != nil {\n+                return nil, err\n+        }\n \n-\treturn s.toAPIResponse(c), nil\n+        return s.toAPIResponse(c), nil\n }\n \n func (s *Server) getClusterWith403IfNotExist(ctx context.Context, q *cluster.ClusterQuery) (*appv1.Cluster, error) {\n-\trepo, err := s.getCluster(ctx, q)\n-\tif err != nil || repo == nil {\n-\t\treturn nil, status.Error(codes.PermissionDenied, \"permission denied\")\n-\t}\n-\treturn repo, nil\n+        repo, err := s.getCluster(ctx, q)\n+        if err != nil || repo == nil {\n+                return nil, status.Error(codes.PermissionDenied, \"permission denied\")\n+        }\n+        return repo, nil\n }\n \n func (s *Server) getCluster(ctx context.Context, q *cluster.ClusterQuery) (*appv1.Cluster, error) {\n-\tif q.Id != nil {\n-\t\tq.Server = \"\"\n-\t\tq.Name = \"\"\n-\t\tif q.Id.Type == \"name\" {\n-\t\t\tq.Name = q.Id.Value\n-\t\t} else if q.Id.Type == \"name_escaped\" {\n-\t\t\tnameUnescaped, err := url.QueryUnescape(q.Id.Value)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t\tq.Name = nameUnescaped\n-\t\t} else {\n-\t\t\tq.Server = q.Id.Value\n-\t\t}\n-\t}\n-\n-\tif q.Server != \"\" {\n-\t\tc, err := s.db.GetCluster(ctx, q.Server)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\treturn c, nil\n-\t}\n-\n-\t//we only get the name when we specify Name in ApplicationDestination and next\n-\t//we want to find the server in order to populate ApplicationDestination.Server\n-\tif q.Name != \"\" {\n-\t\tclusterList, err := s.db.ListClusters(ctx)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tfor _, c := range clusterList.Items {\n-\t\t\tif c.Name == q.Name {\n-\t\t\t\treturn &c, nil\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn nil, nil\n+        if q.Id != nil {\n+                q.Server = \"\"\n+                q.Name = \"\"\n+                if q.Id.Type == \"name\" {\n+                        q.Name = q.Id.Value\n+                } else if q.Id.Type == \"name_escaped\" {\n+                        nameUnescaped, err := url.QueryUnescape(q.Id.Value)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                        q.Name = nameUnescaped\n+                } else {\n+                        q.Server = q.Id.Value\n+                }\n+        }\n+\n+        if q.Server != \"\" {\n+                c, err := s.db.GetCluster(ctx, q.Server)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                return c, nil\n+        }\n+\n+        //we only get the name when we specify Name in ApplicationDestination and next\n+        //we want to find the server in order to populate ApplicationDestination.Server\n+        if q.Name != \"\" {\n+                clusterList, err := s.db.ListClusters(ctx)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                for _, c := range clusterList.Items {\n+                        if c.Name == q.Name {\n+                                return &c, nil\n+                        }\n+                }\n+        }\n+\n+        return nil, nil\n }\n \n var clusterFieldsByPath = map[string]func(updated *appv1.Cluster, existing *appv1.Cluster){\n-\t\"name\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Name = existing.Name\n-\t},\n-\t\"namespaces\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Namespaces = existing.Namespaces\n-\t},\n-\t\"config\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Config = existing.Config\n-\t},\n-\t\"shard\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Shard = existing.Shard\n-\t},\n-\t\"clusterResources\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.ClusterResources = existing.ClusterResources\n-\t},\n-\t\"labels\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Labels = existing.Labels\n-\t},\n-\t\"annotations\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Annotations = existing.Annotations\n-\t},\n-\t\"project\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Project = existing.Project\n-\t},\n+        \"name\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Name = existing.Name\n+        },\n+        \"namespaces\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Namespaces = existing.Namespaces\n+        },\n+        \"config\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Config = existing.Config\n+        },\n+        \"shard\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Shard = existing.Shard\n+        },\n+        \"clusterResources\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.ClusterResources = existing.ClusterResources\n+        },\n+        \"labels\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Labels = existing.Labels\n+        },\n+        \"annotations\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Annotations = existing.Annotations\n+        },\n+        \"project\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Project = existing.Project\n+        },\n }\n \n // Update updates a cluster\n func (s *Server) Update(ctx context.Context, q *cluster.ClusterUpdateRequest) (*appv1.Cluster, error) {\n-\tc, err := s.getClusterWith403IfNotExist(ctx, &cluster.ClusterQuery{\n-\t\tServer: q.Cluster.Server,\n-\t\tName:   q.Cluster.Name,\n-\t\tId:     q.Id,\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// verify that user can do update inside project where cluster is located\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(c.Project, q.Cluster.Server)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif len(q.UpdatedFields) == 0 || sets.NewString(q.UpdatedFields...).Has(\"project\") {\n-\t\t// verify that user can do update inside project where cluster will be located\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(q.Cluster.Project, q.Cluster.Server)); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\tif len(q.UpdatedFields) != 0 {\n-\t\tfor _, path := range q.UpdatedFields {\n-\t\t\tif updater, ok := clusterFieldsByPath[path]; ok {\n-\t\t\t\tupdater(c, q.Cluster)\n-\t\t\t}\n-\t\t}\n-\t\tq.Cluster = c\n-\t}\n-\n-\t// Test the token we just created before persisting it\n-\tserverVersion, err := s.kubectl.GetServerVersion(q.Cluster.RESTConfig())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclust, err := s.db.UpdateCluster(ctx, q.Cluster)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\terr = s.cache.SetClusterInfo(clust.Server, &appv1.ClusterInfo{\n-\t\tServerVersion: serverVersion,\n-\t\tConnectionState: appv1.ConnectionState{\n-\t\t\tStatus:     appv1.ConnectionStatusSuccessful,\n-\t\t\tModifiedAt: &v1.Time{Time: time.Now()},\n-\t\t},\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn s.toAPIResponse(clust), nil\n+        c, err := s.getClusterWith403IfNotExist(ctx, &cluster.ClusterQuery{\n+                Server: q.Cluster.Server,\n+                Name:   q.Cluster.Name,\n+                Id:     q.Id,\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(c.Project, q.Cluster.Server)); err != nil {\n+return nil, err\n+}\n+if proj := q.Cluster.Project; proj != c.Project {\n+if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(proj, q.Cluster.Server)); err != nil {\n+return nil, err\n+}\n+}\n+for _, field := range q.UpdatedFields {\n+subresource := field\n+if field == \"config.awsAuthConfig.clusterName\" || field == \"config.awsAuthConfig.roleARN\" || field == \"config.bearerToken\" || field == \"config.tlsClientConfig.certData\" || field == \"config.tlsClientConfig.keyData\" {\n+subresource = \"config\"\n+}\n+\n+if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(c.Project, q.Cluster.Server+\"/\"+subresource)); err != nil {\n+return nil, err\n+}\n+}\n+\n+        if len(q.UpdatedFields) != 0 {\n+                for _, path := range q.UpdatedFields {\n+                        if updater, ok := clusterFieldsByPath[path]; ok {\n+                                updater(c, q.Cluster)\n+                        }\n+                }\n+                q.Cluster = c\n+        }\n+\n+        // Test the token we just created before persisting it\n+        serverVersion, err := s.kubectl.GetServerVersion(q.Cluster.RESTConfig())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        clust, err := s.db.UpdateCluster(ctx, q.Cluster)\n+        if err != nil {\n+                return nil, err\n+        }\n+        err = s.cache.SetClusterInfo(clust.Server, &appv1.ClusterInfo{\n+                ServerVersion: serverVersion,\n+                ConnectionState: appv1.ConnectionState{\n+                        Status:     appv1.ConnectionStatusSuccessful,\n+                        ModifiedAt: &v1.Time{Time: time.Now()},\n+                },\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+        return s.toAPIResponse(clust), nil\n }\n \n // Delete deletes a cluster by server/name\n func (s *Server) Delete(ctx context.Context, q *cluster.ClusterQuery) (*cluster.ClusterResponse, error) {\n-\tc, err := s.getClusterWith403IfNotExist(ctx, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif q.Name != \"\" {\n-\t\tservers, err := s.db.GetClusterServersByName(ctx, q.Name)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tfor _, server := range servers {\n-\t\t\tif err := enforceAndDelete(s, ctx, server, c.Project); err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\tif err := enforceAndDelete(s, ctx, q.Server, c.Project); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn &cluster.ClusterResponse{}, nil\n+        c, err := s.getClusterWith403IfNotExist(ctx, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if q.Name != \"\" {\n+                servers, err := s.db.GetClusterServersByName(ctx, q.Name)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                for _, server := range servers {\n+                        if err := enforceAndDelete(s, ctx, server, c.Project); err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        } else {\n+                if err := enforceAndDelete(s, ctx, q.Server, c.Project); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return &cluster.ClusterResponse{}, nil\n }\n \n func enforceAndDelete(s *Server, ctx context.Context, server, project string) error {\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionDelete, createRBACObject(project, server)); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := s.db.DeleteCluster(ctx, server); err != nil {\n-\t\treturn err\n-\t}\n-\treturn nil\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionDelete, createRBACObject(project, server)); err != nil {\n+                return err\n+        }\n+        if err := s.db.DeleteCluster(ctx, server); err != nil {\n+                return err\n+        }\n+        return nil\n }\n \n // RotateAuth rotates the bearer token used for a cluster\n func (s *Server) RotateAuth(ctx context.Context, q *cluster.ClusterQuery) (*cluster.ClusterResponse, error) {\n-\tclust, err := s.getClusterWith403IfNotExist(ctx, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar servers []string\n-\tif q.Name != \"\" {\n-\t\tservers, err = s.db.GetClusterServersByName(ctx, q.Name)\n-\t\tif err != nil {\n-\t\t\treturn nil, status.Errorf(codes.NotFound, \"failed to get cluster servers by name: %v\", err)\n-\t\t}\n-\t\tfor _, server := range servers {\n-\t\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(clust.Project, server)); err != nil {\n-\t\t\t\treturn nil, status.Errorf(codes.PermissionDenied, \"encountered permissions issue while processing request: %v\", err)\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(clust.Project, q.Server)); err != nil {\n-\t\t\treturn nil, status.Errorf(codes.PermissionDenied, \"encountered permissions issue while processing request: %v\", err)\n-\t\t}\n-\t\tservers = append(servers, q.Server)\n-\t}\n-\n-\tfor _, server := range servers {\n-\t\tlogCtx := log.WithField(\"cluster\", server)\n-\t\tlogCtx.Info(\"Rotating auth\")\n-\t\trestCfg := clust.RESTConfig()\n-\t\tif restCfg.BearerToken == \"\" {\n-\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"Cluster '%s' does not use bearer token authentication\", server)\n-\t\t}\n-\n-\t\tclaims, err := clusterauth.ParseServiceAccountToken(restCfg.BearerToken)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tkubeclientset, err := kubernetes.NewForConfig(restCfg)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tnewSecret, err := clusterauth.GenerateNewClusterManagerSecret(kubeclientset, claims)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\t// we are using token auth, make sure we don't store client-cert information\n-\t\tclust.Config.KeyData = nil\n-\t\tclust.Config.CertData = nil\n-\t\tclust.Config.BearerToken = string(newSecret.Data[\"token\"])\n-\n-\t\t// Test the token we just created before persisting it\n-\t\tserverVersion, err := s.kubectl.GetServerVersion(clust.RESTConfig())\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\t_, err = s.db.UpdateCluster(ctx, clust)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\terr = s.cache.SetClusterInfo(clust.Server, &appv1.ClusterInfo{\n-\t\t\tServerVersion: serverVersion,\n-\t\t\tConnectionState: appv1.ConnectionState{\n-\t\t\t\tStatus:     appv1.ConnectionStatusSuccessful,\n-\t\t\t\tModifiedAt: &v1.Time{Time: time.Now()},\n-\t\t\t},\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\terr = clusterauth.RotateServiceAccountSecrets(kubeclientset, claims, newSecret)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tlogCtx.Infof(\"Rotated auth (old: %s, new: %s)\", claims.SecretName, newSecret.Name)\n-\t}\n-\treturn &cluster.ClusterResponse{}, nil\n+        clust, err := s.getClusterWith403IfNotExist(ctx, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var servers []string\n+        if q.Name != \"\" {\n+                servers, err = s.db.GetClusterServersByName(ctx, q.Name)\n+                if err != nil {\n+                        return nil, status.Errorf(codes.NotFound, \"failed to get cluster servers by name: %v\", err)\n+                }\n+                for _, server := range servers {\n+                        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(clust.Project, server)); err != nil {\n+                                return nil, status.Errorf(codes.PermissionDenied, \"encountered permissions issue while processing request: %v\", err)\n+                        }\n+                }\n+        } else {\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(clust.Project, q.Server)); err != nil {\n+                        return nil, status.Errorf(codes.PermissionDenied, \"encountered permissions issue while processing request: %v\", err)\n+                }\n+                servers = append(servers, q.Server)\n+        }\n+\n+        for _, server := range servers {\n+                logCtx := log.WithField(\"cluster\", server)\n+                logCtx.Info(\"Rotating auth\")\n+                restCfg := clust.RESTConfig()\n+                if restCfg.BearerToken == \"\" {\n+                        return nil, status.Errorf(codes.InvalidArgument, \"Cluster '%s' does not use bearer token authentication\", server)\n+                }\n+\n+                claims, err := clusterauth.ParseServiceAccountToken(restCfg.BearerToken)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                kubeclientset, err := kubernetes.NewForConfig(restCfg)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                newSecret, err := clusterauth.GenerateNewClusterManagerSecret(kubeclientset, claims)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                // we are using token auth, make sure we don't store client-cert information\n+                clust.Config.KeyData = nil\n+                clust.Config.CertData = nil\n+                clust.Config.BearerToken = string(newSecret.Data[\"token\"])\n+\n+                // Test the token we just created before persisting it\n+                serverVersion, err := s.kubectl.GetServerVersion(clust.RESTConfig())\n+                if err != nil {\n+                        return nil, err\n+                }\n+                _, err = s.db.UpdateCluster(ctx, clust)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                err = s.cache.SetClusterInfo(clust.Server, &appv1.ClusterInfo{\n+                        ServerVersion: serverVersion,\n+                        ConnectionState: appv1.ConnectionState{\n+                                Status:     appv1.ConnectionStatusSuccessful,\n+                                ModifiedAt: &v1.Time{Time: time.Now()},\n+                        },\n+                })\n+                if err != nil {\n+                        return nil, err\n+                }\n+                err = clusterauth.RotateServiceAccountSecrets(kubeclientset, claims, newSecret)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                logCtx.Infof(\"Rotated auth (old: %s, new: %s)\", claims.SecretName, newSecret.Name)\n+        }\n+        return &cluster.ClusterResponse{}, nil\n }\n \n func (s *Server) toAPIResponse(clust *appv1.Cluster) *appv1.Cluster {\n-\t_ = s.cache.GetClusterInfo(clust.Server, &clust.Info)\n-\n-\tclust.Config.Password = \"\"\n-\tclust.Config.BearerToken = \"\"\n-\tclust.Config.TLSClientConfig.KeyData = nil\n-\tif clust.Config.ExecProviderConfig != nil {\n-\t\t// We can't know what the user has put into args or\n-\t\t// env vars on the exec provider that might be sensitive\n-\t\t// (e.g. --private-key=XXX, PASSWORD=XXX)\n-\t\t// Implicitly assumes the command executable name is non-sensitive\n-\t\tclust.Config.ExecProviderConfig.Env = make(map[string]string)\n-\t\tclust.Config.ExecProviderConfig.Args = nil\n-\t}\n-\t// populate deprecated fields for backward compatibility\n-\tclust.ServerVersion = clust.Info.ServerVersion\n-\tclust.ConnectionState = clust.Info.ConnectionState\n-\treturn clust\n+        _ = s.cache.GetClusterInfo(clust.Server, &clust.Info)\n+\n+        clust.Config.Password = \"\"\n+        clust.Config.BearerToken = \"\"\n+        clust.Config.TLSClientConfig.KeyData = nil\n+        if clust.Config.ExecProviderConfig != nil {\n+                // We can't know what the user has put into args or\n+                // env vars on the exec provider that might be sensitive\n+                // (e.g. --private-key=XXX, PASSWORD=XXX)\n+                // Implicitly assumes the command executable name is non-sensitive\n+                clust.Config.ExecProviderConfig.Env = make(map[string]string)\n+                clust.Config.ExecProviderConfig.Args = nil\n+        }\n+        // populate deprecated fields for backward compatibility\n+        clust.ServerVersion = clust.Info.ServerVersion\n+        clust.ConnectionState = clust.Info.ConnectionState\n+        return clust\n }\n \n // InvalidateCache invalidates cluster cache\n func (s *Server) InvalidateCache(ctx context.Context, q *cluster.ClusterQuery) (*appv1.Cluster, error) {\n-\tcls, err := s.getClusterWith403IfNotExist(ctx, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(cls.Project, q.Server)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tnow := v1.Now()\n-\tcls.RefreshRequestedAt = &now\n-\tcls, err = s.db.UpdateCluster(ctx, cls)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn s.toAPIResponse(cls), nil\n+        cls, err := s.getClusterWith403IfNotExist(ctx, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(cls.Project, q.Server)); err != nil {\n+                return nil, err\n+        }\n+        now := v1.Now()\n+        cls.RefreshRequestedAt = &now\n+        cls, err = s.db.UpdateCluster(ctx, cls)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return s.toAPIResponse(cls), nil\n }\n"}
{"cve":"CVE-2023-25173:0708", "fix_patch": "diff --git a/oci/spec_opts.go b/oci/spec_opts.go\nindex 81c148627..b3cdf2f30 100644\n--- a/oci/spec_opts.go\n+++ b/oci/spec_opts.go\n@@ -17,27 +17,27 @@\n package oci\n \n import (\n-\t\"bufio\"\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"runtime\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\t\"github.com/containerd/containerd/containers\"\n-\t\"github.com/containerd/containerd/content\"\n-\t\"github.com/containerd/containerd/images\"\n-\t\"github.com/containerd/containerd/mount\"\n-\t\"github.com/containerd/containerd/namespaces\"\n-\t\"github.com/containerd/containerd/platforms\"\n-\t\"github.com/containerd/continuity/fs\"\n-\tv1 \"github.com/opencontainers/image-spec/specs-go/v1\"\n-\t\"github.com/opencontainers/runc/libcontainer/user\"\n-\t\"github.com/opencontainers/runtime-spec/specs-go\"\n+        \"bufio\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"runtime\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        \"github.com/containerd/containerd/containers\"\n+        \"github.com/containerd/containerd/content\"\n+        \"github.com/containerd/containerd/images\"\n+        \"github.com/containerd/containerd/mount\"\n+        \"github.com/containerd/containerd/namespaces\"\n+        \"github.com/containerd/containerd/platforms\"\n+        \"github.com/containerd/continuity/fs\"\n+        v1 \"github.com/opencontainers/image-spec/specs-go/v1\"\n+        \"github.com/opencontainers/runc/libcontainer/user\"\n+        \"github.com/opencontainers/runtime-spec/specs-go\"\n )\n \n // SpecOpts sets spec specific information to a newly generated OCI spec\n@@ -45,72 +45,72 @@ type SpecOpts func(context.Context, Client, *containers.Container, *Spec) error\n \n // Compose converts a sequence of spec operations into a single operation\n func Compose(opts ...SpecOpts) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n-\t\tfor _, o := range opts {\n-\t\t\tif err := o(ctx, client, c, s); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+                for _, o := range opts {\n+                        if err := o(ctx, client, c, s); err != nil {\n+                                return err\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n // setProcess sets Process to empty if unset\n func setProcess(s *Spec) {\n-\tif s.Process == nil {\n-\t\ts.Process = &specs.Process{}\n-\t}\n+        if s.Process == nil {\n+                s.Process = &specs.Process{}\n+        }\n }\n \n // setRoot sets Root to empty if unset\n func setRoot(s *Spec) {\n-\tif s.Root == nil {\n-\t\ts.Root = &specs.Root{}\n-\t}\n+        if s.Root == nil {\n+                s.Root = &specs.Root{}\n+        }\n }\n \n // setLinux sets Linux to empty if unset\n func setLinux(s *Spec) {\n-\tif s.Linux == nil {\n-\t\ts.Linux = &specs.Linux{}\n-\t}\n+        if s.Linux == nil {\n+                s.Linux = &specs.Linux{}\n+        }\n }\n \n // nolint\n func setResources(s *Spec) {\n-\tif s.Linux != nil {\n-\t\tif s.Linux.Resources == nil {\n-\t\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t\t}\n-\t}\n-\tif s.Windows != nil {\n-\t\tif s.Windows.Resources == nil {\n-\t\t\ts.Windows.Resources = &specs.WindowsResources{}\n-\t\t}\n-\t}\n+        if s.Linux != nil {\n+                if s.Linux.Resources == nil {\n+                        s.Linux.Resources = &specs.LinuxResources{}\n+                }\n+        }\n+        if s.Windows != nil {\n+                if s.Windows.Resources == nil {\n+                        s.Windows.Resources = &specs.WindowsResources{}\n+                }\n+        }\n }\n \n // nolint\n func setCPU(s *Spec) {\n-\tsetResources(s)\n-\tif s.Linux != nil {\n-\t\tif s.Linux.Resources.CPU == nil {\n-\t\t\ts.Linux.Resources.CPU = &specs.LinuxCPU{}\n-\t\t}\n-\t}\n-\tif s.Windows != nil {\n-\t\tif s.Windows.Resources.CPU == nil {\n-\t\t\ts.Windows.Resources.CPU = &specs.WindowsCPUResources{}\n-\t\t}\n-\t}\n+        setResources(s)\n+        if s.Linux != nil {\n+                if s.Linux.Resources.CPU == nil {\n+                        s.Linux.Resources.CPU = &specs.LinuxCPU{}\n+                }\n+        }\n+        if s.Windows != nil {\n+                if s.Windows.Resources.CPU == nil {\n+                        s.Windows.Resources.CPU = &specs.WindowsCPUResources{}\n+                }\n+        }\n }\n \n // setCapabilities sets Linux Capabilities to empty if unset\n func setCapabilities(s *Spec) {\n-\tsetProcess(s)\n-\tif s.Process.Capabilities == nil {\n-\t\ts.Process.Capabilities = &specs.LinuxCapabilities{}\n-\t}\n+        setProcess(s)\n+        if s.Process.Capabilities == nil {\n+                s.Process.Capabilities = &specs.LinuxCapabilities{}\n+        }\n }\n \n // WithDefaultSpec returns a SpecOpts that will populate the spec with default\n@@ -118,9 +118,9 @@ func setCapabilities(s *Spec) {\n //\n // Use as the first option to clear the spec, then apply options afterwards.\n func WithDefaultSpec() SpecOpts {\n-\treturn func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n-\t\treturn generateDefaultSpecWithPlatform(ctx, platforms.DefaultString(), c.ID, s)\n-\t}\n+        return func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n+                return generateDefaultSpecWithPlatform(ctx, platforms.DefaultString(), c.ID, s)\n+        }\n }\n \n // WithDefaultSpecForPlatform returns a SpecOpts that will populate the spec\n@@ -128,498 +128,498 @@ func WithDefaultSpec() SpecOpts {\n //\n // Use as the first option to clear the spec, then apply options afterwards.\n func WithDefaultSpecForPlatform(platform string) SpecOpts {\n-\treturn func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n-\t\treturn generateDefaultSpecWithPlatform(ctx, platform, c.ID, s)\n-\t}\n+        return func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n+                return generateDefaultSpecWithPlatform(ctx, platform, c.ID, s)\n+        }\n }\n \n // WithSpecFromBytes loads the spec from the provided byte slice.\n func WithSpecFromBytes(p []byte) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\t*s = Spec{} // make sure spec is cleared.\n-\t\tif err := json.Unmarshal(p, s); err != nil {\n-\t\t\treturn fmt.Errorf(\"decoding spec config file failed, current supported OCI runtime-spec : v%s: %w\", specs.Version, err)\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                *s = Spec{} // make sure spec is cleared.\n+                if err := json.Unmarshal(p, s); err != nil {\n+                        return fmt.Errorf(\"decoding spec config file failed, current supported OCI runtime-spec : v%s: %w\", specs.Version, err)\n+                }\n+                return nil\n+        }\n }\n \n // WithSpecFromFile loads the specification from the provided filename.\n func WithSpecFromFile(filename string) SpecOpts {\n-\treturn func(ctx context.Context, c Client, container *containers.Container, s *Spec) error {\n-\t\tp, err := os.ReadFile(filename)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"cannot load spec config file: %w\", err)\n-\t\t}\n-\t\treturn WithSpecFromBytes(p)(ctx, c, container, s)\n-\t}\n+        return func(ctx context.Context, c Client, container *containers.Container, s *Spec) error {\n+                p, err := os.ReadFile(filename)\n+                if err != nil {\n+                        return fmt.Errorf(\"cannot load spec config file: %w\", err)\n+                }\n+                return WithSpecFromBytes(p)(ctx, c, container, s)\n+        }\n }\n \n // WithEnv appends environment variables\n func WithEnv(environmentVariables []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tif len(environmentVariables) > 0 {\n-\t\t\tsetProcess(s)\n-\t\t\ts.Process.Env = replaceOrAppendEnvValues(s.Process.Env, environmentVariables)\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                if len(environmentVariables) > 0 {\n+                        setProcess(s)\n+                        s.Process.Env = replaceOrAppendEnvValues(s.Process.Env, environmentVariables)\n+                }\n+                return nil\n+        }\n }\n \n // WithDefaultPathEnv sets the $PATH environment variable to the\n // default PATH defined in this package.\n func WithDefaultPathEnv(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\ts.Process.Env = replaceOrAppendEnvValues(s.Process.Env, defaultUnixEnv)\n-\treturn nil\n+        s.Process.Env = replaceOrAppendEnvValues(s.Process.Env, defaultUnixEnv)\n+        return nil\n }\n \n // replaceOrAppendEnvValues returns the defaults with the overrides either\n // replaced by env key or appended to the list\n func replaceOrAppendEnvValues(defaults, overrides []string) []string {\n-\tcache := make(map[string]int, len(defaults))\n-\tresults := make([]string, 0, len(defaults))\n-\tfor i, e := range defaults {\n-\t\tparts := strings.SplitN(e, \"=\", 2)\n-\t\tresults = append(results, e)\n-\t\tcache[parts[0]] = i\n-\t}\n-\n-\tfor _, value := range overrides {\n-\t\t// Values w/o = means they want this env to be removed/unset.\n-\t\tif !strings.Contains(value, \"=\") {\n-\t\t\tif i, exists := cache[value]; exists {\n-\t\t\t\tresults[i] = \"\" // Used to indicate it should be removed\n-\t\t\t}\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Just do a normal set/update\n-\t\tparts := strings.SplitN(value, \"=\", 2)\n-\t\tif i, exists := cache[parts[0]]; exists {\n-\t\t\tresults[i] = value\n-\t\t} else {\n-\t\t\tresults = append(results, value)\n-\t\t}\n-\t}\n-\n-\t// Now remove all entries that we want to \"unset\"\n-\tfor i := 0; i < len(results); i++ {\n-\t\tif results[i] == \"\" {\n-\t\t\tresults = append(results[:i], results[i+1:]...)\n-\t\t\ti--\n-\t\t}\n-\t}\n-\n-\treturn results\n+        cache := make(map[string]int, len(defaults))\n+        results := make([]string, 0, len(defaults))\n+        for i, e := range defaults {\n+                parts := strings.SplitN(e, \"=\", 2)\n+                results = append(results, e)\n+                cache[parts[0]] = i\n+        }\n+\n+        for _, value := range overrides {\n+                // Values w/o = means they want this env to be removed/unset.\n+                if !strings.Contains(value, \"=\") {\n+                        if i, exists := cache[value]; exists {\n+                                results[i] = \"\" // Used to indicate it should be removed\n+                        }\n+                        continue\n+                }\n+\n+                // Just do a normal set/update\n+                parts := strings.SplitN(value, \"=\", 2)\n+                if i, exists := cache[parts[0]]; exists {\n+                        results[i] = value\n+                } else {\n+                        results = append(results, value)\n+                }\n+        }\n+\n+        // Now remove all entries that we want to \"unset\"\n+        for i := 0; i < len(results); i++ {\n+                if results[i] == \"\" {\n+                        results = append(results[:i], results[i+1:]...)\n+                        i--\n+                }\n+        }\n+\n+        return results\n }\n \n // WithProcessArgs replaces the args on the generated spec\n func WithProcessArgs(args ...string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.Args = args\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.Args = args\n+                return nil\n+        }\n }\n \n // WithProcessCwd replaces the current working directory on the generated spec\n func WithProcessCwd(cwd string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.Cwd = cwd\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.Cwd = cwd\n+                return nil\n+        }\n }\n \n // WithTTY sets the information on the spec as well as the environment variables for\n // using a TTY\n func WithTTY(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetProcess(s)\n-\ts.Process.Terminal = true\n-\tif s.Linux != nil {\n-\t\ts.Process.Env = append(s.Process.Env, \"TERM=xterm\")\n-\t}\n+        setProcess(s)\n+        s.Process.Terminal = true\n+        if s.Linux != nil {\n+                s.Process.Env = append(s.Process.Env, \"TERM=xterm\")\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n // WithTTYSize sets the information on the spec as well as the environment variables for\n // using a TTY\n func WithTTYSize(width, height int) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\tif s.Process.ConsoleSize == nil {\n-\t\t\ts.Process.ConsoleSize = &specs.Box{}\n-\t\t}\n-\t\ts.Process.ConsoleSize.Width = uint(width)\n-\t\ts.Process.ConsoleSize.Height = uint(height)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                if s.Process.ConsoleSize == nil {\n+                        s.Process.ConsoleSize = &specs.Box{}\n+                }\n+                s.Process.ConsoleSize.Width = uint(width)\n+                s.Process.ConsoleSize.Height = uint(height)\n+                return nil\n+        }\n }\n \n // WithHostname sets the container's hostname\n func WithHostname(name string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\ts.Hostname = name\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                s.Hostname = name\n+                return nil\n+        }\n }\n \n // WithMounts appends mounts\n func WithMounts(mounts []specs.Mount) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\ts.Mounts = append(s.Mounts, mounts...)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                s.Mounts = append(s.Mounts, mounts...)\n+                return nil\n+        }\n }\n \n // WithoutMounts removes mounts\n func WithoutMounts(dests ...string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tvar (\n-\t\t\tmounts  []specs.Mount\n-\t\t\tcurrent = s.Mounts\n-\t\t)\n-\tmLoop:\n-\t\tfor _, m := range current {\n-\t\t\tmDestination := filepath.Clean(m.Destination)\n-\t\t\tfor _, dest := range dests {\n-\t\t\t\tif mDestination == dest {\n-\t\t\t\t\tcontinue mLoop\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tmounts = append(mounts, m)\n-\t\t}\n-\t\ts.Mounts = mounts\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                var (\n+                        mounts  []specs.Mount\n+                        current = s.Mounts\n+                )\n+        mLoop:\n+                for _, m := range current {\n+                        mDestination := filepath.Clean(m.Destination)\n+                        for _, dest := range dests {\n+                                if mDestination == dest {\n+                                        continue mLoop\n+                                }\n+                        }\n+                        mounts = append(mounts, m)\n+                }\n+                s.Mounts = mounts\n+                return nil\n+        }\n }\n \n // WithHostNamespace allows a task to run inside the host's linux namespace\n func WithHostNamespace(ns specs.LinuxNamespaceType) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\tfor i, n := range s.Linux.Namespaces {\n-\t\t\tif n.Type == ns {\n-\t\t\t\ts.Linux.Namespaces = append(s.Linux.Namespaces[:i], s.Linux.Namespaces[i+1:]...)\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                for i, n := range s.Linux.Namespaces {\n+                        if n.Type == ns {\n+                                s.Linux.Namespaces = append(s.Linux.Namespaces[:i], s.Linux.Namespaces[i+1:]...)\n+                                return nil\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n // WithLinuxNamespace uses the passed in namespace for the spec. If a namespace of the same type already exists in the\n // spec, the existing namespace is replaced by the one provided.\n func WithLinuxNamespace(ns specs.LinuxNamespace) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\tfor i, n := range s.Linux.Namespaces {\n-\t\t\tif n.Type == ns.Type {\n-\t\t\t\ts.Linux.Namespaces[i] = ns\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t}\n-\t\ts.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                for i, n := range s.Linux.Namespaces {\n+                        if n.Type == ns.Type {\n+                                s.Linux.Namespaces[i] = ns\n+                                return nil\n+                        }\n+                }\n+                s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n+                return nil\n+        }\n }\n \n // WithNewPrivileges turns off the NoNewPrivileges feature flag in the spec\n func WithNewPrivileges(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetProcess(s)\n-\ts.Process.NoNewPrivileges = false\n+        setProcess(s)\n+        s.Process.NoNewPrivileges = false\n \n-\treturn nil\n+        return nil\n }\n \n // WithImageConfig configures the spec to from the configuration of an Image\n func WithImageConfig(image Image) SpecOpts {\n-\treturn WithImageConfigArgs(image, nil)\n+        return WithImageConfigArgs(image, nil)\n }\n \n // WithImageConfigArgs configures the spec to from the configuration of an Image with additional args that\n // replaces the CMD of the image\n func WithImageConfigArgs(image Image, args []string) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n-\t\tic, err := image.Config(ctx)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tvar (\n-\t\t\tociimage v1.Image\n-\t\t\tconfig   v1.ImageConfig\n-\t\t)\n-\t\tswitch ic.MediaType {\n-\t\tcase v1.MediaTypeImageConfig, images.MediaTypeDockerSchema2Config:\n-\t\t\tp, err := content.ReadBlob(ctx, image.ContentStore(), ic)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\tif err := json.Unmarshal(p, &ociimage); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tconfig = ociimage.Config\n-\t\tdefault:\n-\t\t\treturn fmt.Errorf(\"unknown image config media type %s\", ic.MediaType)\n-\t\t}\n-\n-\t\tsetProcess(s)\n-\t\tif s.Linux != nil {\n-\t\t\tdefaults := config.Env\n-\t\t\tif len(defaults) == 0 {\n-\t\t\t\tdefaults = defaultUnixEnv\n-\t\t\t}\n-\t\t\ts.Process.Env = replaceOrAppendEnvValues(defaults, s.Process.Env)\n-\t\t\tcmd := config.Cmd\n-\t\t\tif len(args) > 0 {\n-\t\t\t\tcmd = args\n-\t\t\t}\n-\t\t\ts.Process.Args = append(config.Entrypoint, cmd...)\n-\n-\t\t\tcwd := config.WorkingDir\n-\t\t\tif cwd == \"\" {\n-\t\t\t\tcwd = \"/\"\n-\t\t\t}\n-\t\t\ts.Process.Cwd = cwd\n-\t\t\tif config.User != \"\" {\n-\t\t\t\tif err := WithUser(config.User)(ctx, client, c, s); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\treturn WithAdditionalGIDs(fmt.Sprintf(\"%d\", s.Process.User.UID))(ctx, client, c, s)\n-\t\t\t}\n-\t\t\t// we should query the image's /etc/group for additional GIDs\n-\t\t\t// even if there is no specified user in the image config\n-\t\t\treturn WithAdditionalGIDs(\"root\")(ctx, client, c, s)\n-\t\t} else if s.Windows != nil {\n-\t\t\ts.Process.Env = replaceOrAppendEnvValues(config.Env, s.Process.Env)\n-\t\t\tcmd := config.Cmd\n-\t\t\tif len(args) > 0 {\n-\t\t\t\tcmd = args\n-\t\t\t}\n-\t\t\ts.Process.Args = append(config.Entrypoint, cmd...)\n-\n-\t\t\ts.Process.Cwd = config.WorkingDir\n-\t\t\ts.Process.User = specs.User{\n-\t\t\t\tUsername: config.User,\n-\t\t\t}\n-\t\t} else {\n-\t\t\treturn errors.New(\"spec does not contain Linux or Windows section\")\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+                ic, err := image.Config(ctx)\n+                if err != nil {\n+                        return err\n+                }\n+                var (\n+                        ociimage v1.Image\n+                        config   v1.ImageConfig\n+                )\n+                switch ic.MediaType {\n+                case v1.MediaTypeImageConfig, images.MediaTypeDockerSchema2Config:\n+                        p, err := content.ReadBlob(ctx, image.ContentStore(), ic)\n+                        if err != nil {\n+                                return err\n+                        }\n+\n+                        if err := json.Unmarshal(p, &ociimage); err != nil {\n+                                return err\n+                        }\n+                        config = ociimage.Config\n+                default:\n+                        return fmt.Errorf(\"unknown image config media type %s\", ic.MediaType)\n+                }\n+\n+                setProcess(s)\n+                if s.Linux != nil {\n+                        defaults := config.Env\n+                        if len(defaults) == 0 {\n+                                defaults = defaultUnixEnv\n+                        }\n+                        s.Process.Env = replaceOrAppendEnvValues(defaults, s.Process.Env)\n+                        cmd := config.Cmd\n+                        if len(args) > 0 {\n+                                cmd = args\n+                        }\n+                        s.Process.Args = append(config.Entrypoint, cmd...)\n+\n+                        cwd := config.WorkingDir\n+                        if cwd == \"\" {\n+                                cwd = \"/\"\n+                        }\n+                        s.Process.Cwd = cwd\n+                        if config.User != \"\" {\n+                                if err := WithUser(config.User)(ctx, client, c, s); err != nil {\n+                                        return err\n+                                }\n+                                return WithAdditionalGIDs(fmt.Sprintf(\"%d\", s.Process.User.UID))(ctx, client, c, s)\n+                        }\n+                        // we should query the image's /etc/group for additional GIDs\n+                        // even if there is no specified user in the image config\n+                        return WithAdditionalGIDs(\"root\")(ctx, client, c, s)\n+                } else if s.Windows != nil {\n+                        s.Process.Env = replaceOrAppendEnvValues(config.Env, s.Process.Env)\n+                        cmd := config.Cmd\n+                        if len(args) > 0 {\n+                                cmd = args\n+                        }\n+                        s.Process.Args = append(config.Entrypoint, cmd...)\n+\n+                        s.Process.Cwd = config.WorkingDir\n+                        s.Process.User = specs.User{\n+                                Username: config.User,\n+                        }\n+                } else {\n+                        return errors.New(\"spec does not contain Linux or Windows section\")\n+                }\n+                return nil\n+        }\n }\n \n // WithRootFSPath specifies unmanaged rootfs path.\n func WithRootFSPath(path string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetRoot(s)\n-\t\ts.Root.Path = path\n-\t\t// Entrypoint is not set here (it's up to caller)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setRoot(s)\n+                s.Root.Path = path\n+                // Entrypoint is not set here (it's up to caller)\n+                return nil\n+        }\n }\n \n // WithRootFSReadonly sets specs.Root.Readonly to true\n func WithRootFSReadonly() SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetRoot(s)\n-\t\ts.Root.Readonly = true\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setRoot(s)\n+                s.Root.Readonly = true\n+                return nil\n+        }\n }\n \n // WithNoNewPrivileges sets no_new_privileges on the process for the container\n func WithNoNewPrivileges(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetProcess(s)\n-\ts.Process.NoNewPrivileges = true\n-\treturn nil\n+        setProcess(s)\n+        s.Process.NoNewPrivileges = true\n+        return nil\n }\n \n // WithHostHostsFile bind-mounts the host's /etc/hosts into the container as readonly\n func WithHostHostsFile(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\ts.Mounts = append(s.Mounts, specs.Mount{\n-\t\tDestination: \"/etc/hosts\",\n-\t\tType:        \"bind\",\n-\t\tSource:      \"/etc/hosts\",\n-\t\tOptions:     []string{\"rbind\", \"ro\"},\n-\t})\n-\treturn nil\n+        s.Mounts = append(s.Mounts, specs.Mount{\n+                Destination: \"/etc/hosts\",\n+                Type:        \"bind\",\n+                Source:      \"/etc/hosts\",\n+                Options:     []string{\"rbind\", \"ro\"},\n+        })\n+        return nil\n }\n \n // WithHostResolvconf bind-mounts the host's /etc/resolv.conf into the container as readonly\n func WithHostResolvconf(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\ts.Mounts = append(s.Mounts, specs.Mount{\n-\t\tDestination: \"/etc/resolv.conf\",\n-\t\tType:        \"bind\",\n-\t\tSource:      \"/etc/resolv.conf\",\n-\t\tOptions:     []string{\"rbind\", \"ro\"},\n-\t})\n-\treturn nil\n+        s.Mounts = append(s.Mounts, specs.Mount{\n+                Destination: \"/etc/resolv.conf\",\n+                Type:        \"bind\",\n+                Source:      \"/etc/resolv.conf\",\n+                Options:     []string{\"rbind\", \"ro\"},\n+        })\n+        return nil\n }\n \n // WithHostLocaltime bind-mounts the host's /etc/localtime into the container as readonly\n func WithHostLocaltime(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\ts.Mounts = append(s.Mounts, specs.Mount{\n-\t\tDestination: \"/etc/localtime\",\n-\t\tType:        \"bind\",\n-\t\tSource:      \"/etc/localtime\",\n-\t\tOptions:     []string{\"rbind\", \"ro\"},\n-\t})\n-\treturn nil\n+        s.Mounts = append(s.Mounts, specs.Mount{\n+                Destination: \"/etc/localtime\",\n+                Type:        \"bind\",\n+                Source:      \"/etc/localtime\",\n+                Options:     []string{\"rbind\", \"ro\"},\n+        })\n+        return nil\n }\n \n // WithUserNamespace sets the uid and gid mappings for the task\n // this can be called multiple times to add more mappings to the generated spec\n func WithUserNamespace(uidMap, gidMap []specs.LinuxIDMapping) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tvar hasUserns bool\n-\t\tsetLinux(s)\n-\t\tfor _, ns := range s.Linux.Namespaces {\n-\t\t\tif ns.Type == specs.UserNamespace {\n-\t\t\t\thasUserns = true\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif !hasUserns {\n-\t\t\ts.Linux.Namespaces = append(s.Linux.Namespaces, specs.LinuxNamespace{\n-\t\t\t\tType: specs.UserNamespace,\n-\t\t\t})\n-\t\t}\n-\t\ts.Linux.UIDMappings = append(s.Linux.UIDMappings, uidMap...)\n-\t\ts.Linux.GIDMappings = append(s.Linux.GIDMappings, gidMap...)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                var hasUserns bool\n+                setLinux(s)\n+                for _, ns := range s.Linux.Namespaces {\n+                        if ns.Type == specs.UserNamespace {\n+                                hasUserns = true\n+                                break\n+                        }\n+                }\n+                if !hasUserns {\n+                        s.Linux.Namespaces = append(s.Linux.Namespaces, specs.LinuxNamespace{\n+                                Type: specs.UserNamespace,\n+                        })\n+                }\n+                s.Linux.UIDMappings = append(s.Linux.UIDMappings, uidMap...)\n+                s.Linux.GIDMappings = append(s.Linux.GIDMappings, gidMap...)\n+                return nil\n+        }\n }\n \n // WithCgroup sets the container's cgroup path\n func WithCgroup(path string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\ts.Linux.CgroupsPath = path\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                s.Linux.CgroupsPath = path\n+                return nil\n+        }\n }\n \n // WithNamespacedCgroup uses the namespace set on the context to create a\n // root directory for containers in the cgroup with the id as the subcgroup\n func WithNamespacedCgroup() SpecOpts {\n-\treturn func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n-\t\tnamespace, err := namespaces.NamespaceRequired(ctx)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tsetLinux(s)\n-\t\ts.Linux.CgroupsPath = filepath.Join(\"/\", namespace, c.ID)\n-\t\treturn nil\n-\t}\n+        return func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n+                namespace, err := namespaces.NamespaceRequired(ctx)\n+                if err != nil {\n+                        return err\n+                }\n+                setLinux(s)\n+                s.Linux.CgroupsPath = filepath.Join(\"/\", namespace, c.ID)\n+                return nil\n+        }\n }\n \n // WithUser sets the user to be used within the container.\n // It accepts a valid user string in OCI Image Spec v1.0.0:\n //\n-//\tuser, uid, user:group, uid:gid, uid:group, user:gid\n+//      user, uid, user:group, uid:gid, uid:group, user:gid\n func WithUser(userstr string) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\n-\t\t// For LCOW it's a bit harder to confirm that the user actually exists on the host as a rootfs isn't\n-\t\t// mounted on the host and shared into the guest, but rather the rootfs is constructed entirely in the\n-\t\t// guest itself. To accommodate this, a spot to place the user string provided by a client as-is is needed.\n-\t\t// The `Username` field on the runtime spec is marked by Platform as only for Windows, and in this case it\n-\t\t// *is* being set on a Windows host at least, but will be used as a temporary holding spot until the guest\n-\t\t// can use the string to perform these same operations to grab the uid:gid inside.\n-\t\tif s.Windows != nil && s.Linux != nil {\n-\t\t\ts.Process.User.Username = userstr\n-\t\t\treturn nil\n-\t\t}\n-\n-\t\tparts := strings.Split(userstr, \":\")\n-\t\tswitch len(parts) {\n-\t\tcase 1:\n-\t\t\tv, err := strconv.Atoi(parts[0])\n-\t\t\tif err != nil {\n-\t\t\t\t// if we cannot parse as a uint they try to see if it is a username\n-\t\t\t\treturn WithUsername(userstr)(ctx, client, c, s)\n-\t\t\t}\n-\t\t\treturn WithUserID(uint32(v))(ctx, client, c, s)\n-\t\tcase 2:\n-\t\t\tvar (\n-\t\t\t\tusername  string\n-\t\t\t\tgroupname string\n-\t\t\t)\n-\t\t\tvar uid, gid uint32\n-\t\t\tv, err := strconv.Atoi(parts[0])\n-\t\t\tif err != nil {\n-\t\t\t\tusername = parts[0]\n-\t\t\t} else {\n-\t\t\t\tuid = uint32(v)\n-\t\t\t}\n-\t\t\tif v, err = strconv.Atoi(parts[1]); err != nil {\n-\t\t\t\tgroupname = parts[1]\n-\t\t\t} else {\n-\t\t\t\tgid = uint32(v)\n-\t\t\t}\n-\t\t\tif username == \"\" && groupname == \"\" {\n-\t\t\t\ts.Process.User.UID, s.Process.User.GID = uid, gid\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\tf := func(root string) error {\n-\t\t\t\tif username != \"\" {\n-\t\t\t\t\tuser, err := UserFromPath(root, func(u user.User) bool {\n-\t\t\t\t\t\treturn u.Name == username\n-\t\t\t\t\t})\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\treturn err\n-\t\t\t\t\t}\n-\t\t\t\t\tuid = uint32(user.Uid)\n-\t\t\t\t}\n-\t\t\t\tif groupname != \"\" {\n-\t\t\t\t\tgid, err = GIDFromPath(root, func(g user.Group) bool {\n-\t\t\t\t\t\treturn g.Name == groupname\n-\t\t\t\t\t})\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\treturn err\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\ts.Process.User.UID, s.Process.User.GID = uid, gid\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\tif c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n-\t\t\t\tif !isRootfsAbs(s.Root.Path) {\n-\t\t\t\t\treturn errors.New(\"rootfs absolute path is required\")\n-\t\t\t\t}\n-\t\t\t\treturn f(s.Root.Path)\n-\t\t\t}\n-\t\t\tif c.Snapshotter == \"\" {\n-\t\t\t\treturn errors.New(\"no snapshotter set for container\")\n-\t\t\t}\n-\t\t\tif c.SnapshotKey == \"\" {\n-\t\t\t\treturn errors.New(\"rootfs snapshot not created for container\")\n-\t\t\t}\n-\t\t\tsnapshotter := client.SnapshotService(c.Snapshotter)\n-\t\t\tmounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\tmounts = tryReadonlyMounts(mounts)\n-\t\t\treturn mount.WithTempMount(ctx, mounts, f)\n-\t\tdefault:\n-\t\t\treturn fmt.Errorf(\"invalid USER value %s\", userstr)\n-\t\t}\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+                setProcess(s)\n+\n+                // For LCOW it's a bit harder to confirm that the user actually exists on the host as a rootfs isn't\n+                // mounted on the host and shared into the guest, but rather the rootfs is constructed entirely in the\n+                // guest itself. To accommodate this, a spot to place the user string provided by a client as-is is needed.\n+                // The `Username` field on the runtime spec is marked by Platform as only for Windows, and in this case it\n+                // *is* being set on a Windows host at least, but will be used as a temporary holding spot until the guest\n+                // can use the string to perform these same operations to grab the uid:gid inside.\n+                if s.Windows != nil && s.Linux != nil {\n+                        s.Process.User.Username = userstr\n+                        return nil\n+                }\n+\n+                parts := strings.Split(userstr, \":\")\n+                switch len(parts) {\n+                case 1:\n+                        v, err := strconv.Atoi(parts[0])\n+                        if err != nil {\n+                                // if we cannot parse as a uint they try to see if it is a username\n+                                return WithUsername(userstr)(ctx, client, c, s)\n+                        }\n+                        return WithUserID(uint32(v))(ctx, client, c, s)\n+                case 2:\n+                        var (\n+                                username  string\n+                                groupname string\n+                        )\n+                        var uid, gid uint32\n+                        v, err := strconv.Atoi(parts[0])\n+                        if err != nil {\n+                                username = parts[0]\n+                        } else {\n+                                uid = uint32(v)\n+                        }\n+                        if v, err = strconv.Atoi(parts[1]); err != nil {\n+                                groupname = parts[1]\n+                        } else {\n+                                gid = uint32(v)\n+                        }\n+                        if username == \"\" && groupname == \"\" {\n+                                s.Process.User.UID, s.Process.User.GID = uid, gid\n+                                return WithAdditionalGIDs(fmt.Sprintf(\"%d\", s.Process.User.UID))(ctx, client, c, s)\n+                        }\n+                        f := func(root string) error {\n+                                if username != \"\" {\n+                                        user, err := UserFromPath(root, func(u user.User) bool {\n+                                                return u.Name == username\n+                                        })\n+                                        if err != nil {\n+                                                return err\n+                                        }\n+                                        uid = uint32(user.Uid)\n+                                }\n+                                if groupname != \"\" {\n+                                        gid, err = GIDFromPath(root, func(g user.Group) bool {\n+                                                return g.Name == groupname\n+                                        })\n+                                        if err != nil {\n+                                                return err\n+                                        }\n+                                }\n+                                s.Process.User.UID, s.Process.User.GID = uid, gid\n+                                return nil\n+                        }\n+                        if c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n+                                if !isRootfsAbs(s.Root.Path) {\n+                                        return errors.New(\"rootfs absolute path is required\")\n+                                }\n+                                return f(s.Root.Path)\n+                        }\n+                        if c.Snapshotter == \"\" {\n+                                return errors.New(\"no snapshotter set for container\")\n+                        }\n+                        if c.SnapshotKey == \"\" {\n+                                return errors.New(\"rootfs snapshot not created for container\")\n+                        }\n+                        snapshotter := client.SnapshotService(c.Snapshotter)\n+                        mounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+                        if err != nil {\n+                                return err\n+                        }\n+\n+                        mounts = tryReadonlyMounts(mounts)\n+                        return mount.WithTempMount(ctx, mounts, f)\n+                default:\n+                        return fmt.Errorf(\"invalid USER value %s\", userstr)\n+                }\n+        }\n }\n \n // WithUIDGID allows the UID and GID for the Process to be set\n func WithUIDGID(uid, gid uint32) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.User.UID = uid\n-\t\ts.Process.User.GID = gid\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.User.UID = uid\n+                s.Process.User.GID = gid\n+                return nil\n+        }\n }\n \n // WithUserID sets the correct UID and GID for the container based\n@@ -627,54 +627,54 @@ func WithUIDGID(uid, gid uint32) SpecOpts {\n // or uid is not found in /etc/passwd, it sets the requested uid,\n // additionally sets the gid to 0, and does not return an error.\n func WithUserID(uid uint32) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n-\t\tsetProcess(s)\n-\t\tif c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n-\t\t\tif !isRootfsAbs(s.Root.Path) {\n-\t\t\t\treturn errors.New(\"rootfs absolute path is required\")\n-\t\t\t}\n-\t\t\tuser, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n-\t\t\t\treturn u.Uid == int(uid)\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\tif os.IsNotExist(err) || err == ErrNoUsersFound {\n-\t\t\t\t\ts.Process.User.UID, s.Process.User.GID = uid, 0\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\ts.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n-\t\t\treturn nil\n-\n-\t\t}\n-\t\tif c.Snapshotter == \"\" {\n-\t\t\treturn errors.New(\"no snapshotter set for container\")\n-\t\t}\n-\t\tif c.SnapshotKey == \"\" {\n-\t\t\treturn errors.New(\"rootfs snapshot not created for container\")\n-\t\t}\n-\t\tsnapshotter := client.SnapshotService(c.Snapshotter)\n-\t\tmounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tmounts = tryReadonlyMounts(mounts)\n-\t\treturn mount.WithTempMount(ctx, mounts, func(root string) error {\n-\t\t\tuser, err := UserFromPath(root, func(u user.User) bool {\n-\t\t\t\treturn u.Uid == int(uid)\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\tif os.IsNotExist(err) || err == ErrNoUsersFound {\n-\t\t\t\t\ts.Process.User.UID, s.Process.User.GID = uid, 0\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\ts.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n-\t\t\treturn nil\n-\t\t})\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n+                setProcess(s)\n+                if c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n+                        if !isRootfsAbs(s.Root.Path) {\n+                                return errors.New(\"rootfs absolute path is required\")\n+                        }\n+                        user, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n+                                return u.Uid == int(uid)\n+                        })\n+                        if err != nil {\n+                                if os.IsNotExist(err) || err == ErrNoUsersFound {\n+                                        s.Process.User.UID, s.Process.User.GID = uid, 0\n+                                        return nil\n+                                }\n+                                return err\n+                        }\n+                        s.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n+                        return WithAdditionalGIDs(fmt.Sprintf(\"%d\", s.Process.User.UID))(ctx, client, c, s)\n+\n+                }\n+                if c.Snapshotter == \"\" {\n+                        return errors.New(\"no snapshotter set for container\")\n+                }\n+                if c.SnapshotKey == \"\" {\n+                        return errors.New(\"rootfs snapshot not created for container\")\n+                }\n+                snapshotter := client.SnapshotService(c.Snapshotter)\n+                mounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                mounts = tryReadonlyMounts(mounts)\n+                return mount.WithTempMount(ctx, mounts, func(root string) error {\n+                        user, err := UserFromPath(root, func(u user.User) bool {\n+                                return u.Uid == int(uid)\n+                        })\n+                        if err != nil {\n+                                if os.IsNotExist(err) || err == ErrNoUsersFound {\n+                                        s.Process.User.UID, s.Process.User.GID = uid, 0\n+                                        return nil\n+                                }\n+                                return err\n+                        }\n+                        s.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n+                        return WithAdditionalGIDs(fmt.Sprintf(\"%d\", s.Process.User.UID))(ctx, client, c, s)\n+                })\n+        }\n }\n \n // WithUsername sets the correct UID and GID for the container\n@@ -684,204 +684,204 @@ func WithUserID(uid uint32) SpecOpts {\n // the operating system will validate the user when going to run\n // the container.\n func WithUsername(username string) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n-\t\tsetProcess(s)\n-\t\tif s.Linux != nil {\n-\t\t\tif c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n-\t\t\t\tif !isRootfsAbs(s.Root.Path) {\n-\t\t\t\t\treturn errors.New(\"rootfs absolute path is required\")\n-\t\t\t\t}\n-\t\t\t\tuser, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n-\t\t\t\t\treturn u.Name == username\n-\t\t\t\t})\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\ts.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\tif c.Snapshotter == \"\" {\n-\t\t\t\treturn errors.New(\"no snapshotter set for container\")\n-\t\t\t}\n-\t\t\tif c.SnapshotKey == \"\" {\n-\t\t\t\treturn errors.New(\"rootfs snapshot not created for container\")\n-\t\t\t}\n-\t\t\tsnapshotter := client.SnapshotService(c.Snapshotter)\n-\t\t\tmounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\tmounts = tryReadonlyMounts(mounts)\n-\t\t\treturn mount.WithTempMount(ctx, mounts, func(root string) error {\n-\t\t\t\tuser, err := UserFromPath(root, func(u user.User) bool {\n-\t\t\t\t\treturn u.Name == username\n-\t\t\t\t})\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\ts.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n-\t\t\t\treturn nil\n-\t\t\t})\n-\t\t} else if s.Windows != nil {\n-\t\t\ts.Process.User.Username = username\n-\t\t} else {\n-\t\t\treturn errors.New(\"spec does not contain Linux or Windows section\")\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n+                setProcess(s)\n+                if s.Linux != nil {\n+                        if c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n+                                if !isRootfsAbs(s.Root.Path) {\n+                                        return errors.New(\"rootfs absolute path is required\")\n+                                }\n+                                user, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n+                                        return u.Name == username\n+                                })\n+                                if err != nil {\n+                                        return err\n+                                }\n+                                s.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n+                                return WithAdditionalGIDs(username)(ctx, client, c, s)\n+                        }\n+                        if c.Snapshotter == \"\" {\n+                                return errors.New(\"no snapshotter set for container\")\n+                        }\n+                        if c.SnapshotKey == \"\" {\n+                                return errors.New(\"rootfs snapshot not created for container\")\n+                        }\n+                        snapshotter := client.SnapshotService(c.Snapshotter)\n+                        mounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+                        if err != nil {\n+                                return err\n+                        }\n+\n+                        mounts = tryReadonlyMounts(mounts)\n+                        return mount.WithTempMount(ctx, mounts, func(root string) error {\n+                                user, err := UserFromPath(root, func(u user.User) bool {\n+                                        return u.Name == username\n+                                })\n+                                if err != nil {\n+                                        return err\n+                                }\n+                                s.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n+                                return WithAdditionalGIDs(username)(ctx, client, c, s)\n+                        })\n+                } else if s.Windows != nil {\n+                        s.Process.User.Username = username\n+                } else {\n+                        return errors.New(\"spec does not contain Linux or Windows section\")\n+                }\n+                return nil\n+        }\n }\n \n // WithAdditionalGIDs sets the OCI spec's additionalGids array to any additional groups listed\n // for a particular user in the /etc/groups file of the image's root filesystem\n // The passed in user can be either a uid or a username.\n func WithAdditionalGIDs(userstr string) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n-\t\t// For LCOW or on Darwin additional GID's not supported\n-\t\tif s.Windows != nil || runtime.GOOS == \"darwin\" {\n-\t\t\treturn nil\n-\t\t}\n-\t\tsetProcess(s)\n-\t\tsetAdditionalGids := func(root string) error {\n-\t\t\tvar username string\n-\t\t\tuid, err := strconv.Atoi(userstr)\n-\t\t\tif err == nil {\n-\t\t\t\tuser, err := UserFromPath(root, func(u user.User) bool {\n-\t\t\t\t\treturn u.Uid == uid\n-\t\t\t\t})\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tif os.IsNotExist(err) || err == ErrNoUsersFound {\n-\t\t\t\t\t\treturn nil\n-\t\t\t\t\t}\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\tusername = user.Name\n-\t\t\t} else {\n-\t\t\t\tusername = userstr\n-\t\t\t}\n-\t\t\tgids, err := getSupplementalGroupsFromPath(root, func(g user.Group) bool {\n-\t\t\t\t// we only want supplemental groups\n-\t\t\t\tif g.Name == username {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\t\t\t\tfor _, entry := range g.List {\n-\t\t\t\t\tif entry == username {\n-\t\t\t\t\t\treturn true\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\treturn false\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\tif os.IsNotExist(err) {\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\ts.Process.User.AdditionalGids = gids\n-\t\t\treturn nil\n-\t\t}\n-\t\tif c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n-\t\t\tif !isRootfsAbs(s.Root.Path) {\n-\t\t\t\treturn errors.New(\"rootfs absolute path is required\")\n-\t\t\t}\n-\t\t\treturn setAdditionalGids(s.Root.Path)\n-\t\t}\n-\t\tif c.Snapshotter == \"\" {\n-\t\t\treturn errors.New(\"no snapshotter set for container\")\n-\t\t}\n-\t\tif c.SnapshotKey == \"\" {\n-\t\t\treturn errors.New(\"rootfs snapshot not created for container\")\n-\t\t}\n-\t\tsnapshotter := client.SnapshotService(c.Snapshotter)\n-\t\tmounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tmounts = tryReadonlyMounts(mounts)\n-\t\treturn mount.WithTempMount(ctx, mounts, setAdditionalGids)\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n+                // For LCOW or on Darwin additional GID's not supported\n+                if s.Windows != nil || runtime.GOOS == \"darwin\" {\n+                        return nil\n+                }\n+                setProcess(s)\n+                setAdditionalGids := func(root string) error {\n+                        var username string\n+                        uid, err := strconv.Atoi(userstr)\n+                        if err == nil {\n+                                user, err := UserFromPath(root, func(u user.User) bool {\n+                                        return u.Uid == uid\n+                                })\n+                                if err != nil {\n+                                        if os.IsNotExist(err) || err == ErrNoUsersFound {\n+                                                return nil\n+                                        }\n+                                        return err\n+                                }\n+                                username = user.Name\n+                        } else {\n+                                username = userstr\n+                        }\n+                        gids, err := getSupplementalGroupsFromPath(root, func(g user.Group) bool {\n+                                // we only want supplemental groups\n+                                if g.Gid == int(s.Process.User.GID) {\n+                                        return false\n+                                }\n+                                for _, entry := range g.List {\n+                                        if entry == username {\n+                                                return true\n+                                        }\n+                                }\n+                                return false\n+                        })\n+                        if err != nil {\n+                                if os.IsNotExist(err) {\n+                                        return nil\n+                                }\n+                                return err\n+                        }\n+                        s.Process.User.AdditionalGids = gids\n+                        return nil\n+                }\n+                if c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n+                        if !isRootfsAbs(s.Root.Path) {\n+                                return errors.New(\"rootfs absolute path is required\")\n+                        }\n+                        return setAdditionalGids(s.Root.Path)\n+                }\n+                if c.Snapshotter == \"\" {\n+                        return errors.New(\"no snapshotter set for container\")\n+                }\n+                if c.SnapshotKey == \"\" {\n+                        return errors.New(\"rootfs snapshot not created for container\")\n+                }\n+                snapshotter := client.SnapshotService(c.Snapshotter)\n+                mounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                mounts = tryReadonlyMounts(mounts)\n+                return mount.WithTempMount(ctx, mounts, setAdditionalGids)\n+        }\n }\n \n // WithCapabilities sets Linux capabilities on the process\n func WithCapabilities(caps []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetCapabilities(s)\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setCapabilities(s)\n \n-\t\ts.Process.Capabilities.Bounding = caps\n-\t\ts.Process.Capabilities.Effective = caps\n-\t\ts.Process.Capabilities.Permitted = caps\n+                s.Process.Capabilities.Bounding = caps\n+                s.Process.Capabilities.Effective = caps\n+                s.Process.Capabilities.Permitted = caps\n \n-\t\treturn nil\n-\t}\n+                return nil\n+        }\n }\n \n func capsContain(caps []string, s string) bool {\n-\tfor _, c := range caps {\n-\t\tif c == s {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, c := range caps {\n+                if c == s {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func removeCap(caps *[]string, s string) {\n-\tvar newcaps []string\n-\tfor _, c := range *caps {\n-\t\tif c == s {\n-\t\t\tcontinue\n-\t\t}\n-\t\tnewcaps = append(newcaps, c)\n-\t}\n-\t*caps = newcaps\n+        var newcaps []string\n+        for _, c := range *caps {\n+                if c == s {\n+                        continue\n+                }\n+                newcaps = append(newcaps, c)\n+        }\n+        *caps = newcaps\n }\n \n // WithAddedCapabilities adds the provided capabilities\n func WithAddedCapabilities(caps []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetCapabilities(s)\n-\t\tfor _, c := range caps {\n-\t\t\tfor _, cl := range []*[]string{\n-\t\t\t\t&s.Process.Capabilities.Bounding,\n-\t\t\t\t&s.Process.Capabilities.Effective,\n-\t\t\t\t&s.Process.Capabilities.Permitted,\n-\t\t\t} {\n-\t\t\t\tif !capsContain(*cl, c) {\n-\t\t\t\t\t*cl = append(*cl, c)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setCapabilities(s)\n+                for _, c := range caps {\n+                        for _, cl := range []*[]string{\n+                                &s.Process.Capabilities.Bounding,\n+                                &s.Process.Capabilities.Effective,\n+                                &s.Process.Capabilities.Permitted,\n+                        } {\n+                                if !capsContain(*cl, c) {\n+                                        *cl = append(*cl, c)\n+                                }\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n // WithDroppedCapabilities removes the provided capabilities\n func WithDroppedCapabilities(caps []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetCapabilities(s)\n-\t\tfor _, c := range caps {\n-\t\t\tfor _, cl := range []*[]string{\n-\t\t\t\t&s.Process.Capabilities.Bounding,\n-\t\t\t\t&s.Process.Capabilities.Effective,\n-\t\t\t\t&s.Process.Capabilities.Permitted,\n-\t\t\t} {\n-\t\t\t\tremoveCap(cl, c)\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setCapabilities(s)\n+                for _, c := range caps {\n+                        for _, cl := range []*[]string{\n+                                &s.Process.Capabilities.Bounding,\n+                                &s.Process.Capabilities.Effective,\n+                                &s.Process.Capabilities.Permitted,\n+                        } {\n+                                removeCap(cl, c)\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n // WithAmbientCapabilities set the Linux ambient capabilities for the process\n // Ambient capabilities should only be set for non-root users or the caller should\n // understand how these capabilities are used and set\n func WithAmbientCapabilities(caps []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetCapabilities(s)\n-\t\ts.Process.Capabilities.Inheritable = caps\n-\t\ts.Process.Capabilities.Ambient = caps\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setCapabilities(s)\n+                s.Process.Capabilities.Inheritable = caps\n+                s.Process.Capabilities.Ambient = caps\n+                return nil\n+        }\n }\n \n // ErrNoUsersFound can be returned from UserFromPath\n@@ -890,18 +890,18 @@ var ErrNoUsersFound = errors.New(\"no users found\")\n // UserFromPath inspects the user object using /etc/passwd in the specified rootfs.\n // filter can be nil.\n func UserFromPath(root string, filter func(user.User) bool) (user.User, error) {\n-\tppath, err := fs.RootPath(root, \"/etc/passwd\")\n-\tif err != nil {\n-\t\treturn user.User{}, err\n-\t}\n-\tusers, err := user.ParsePasswdFileFilter(ppath, filter)\n-\tif err != nil {\n-\t\treturn user.User{}, err\n-\t}\n-\tif len(users) == 0 {\n-\t\treturn user.User{}, ErrNoUsersFound\n-\t}\n-\treturn users[0], nil\n+        ppath, err := fs.RootPath(root, \"/etc/passwd\")\n+        if err != nil {\n+                return user.User{}, err\n+        }\n+        users, err := user.ParsePasswdFileFilter(ppath, filter)\n+        if err != nil {\n+                return user.User{}, err\n+        }\n+        if len(users) == 0 {\n+                return user.User{}, ErrNoUsersFound\n+        }\n+        return users[0], nil\n }\n \n // ErrNoGroupsFound can be returned from GIDFromPath\n@@ -910,249 +910,249 @@ var ErrNoGroupsFound = errors.New(\"no groups found\")\n // GIDFromPath inspects the GID using /etc/passwd in the specified rootfs.\n // filter can be nil.\n func GIDFromPath(root string, filter func(user.Group) bool) (gid uint32, err error) {\n-\tgpath, err := fs.RootPath(root, \"/etc/group\")\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tgroups, err := user.ParseGroupFileFilter(gpath, filter)\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tif len(groups) == 0 {\n-\t\treturn 0, ErrNoGroupsFound\n-\t}\n-\tg := groups[0]\n-\treturn uint32(g.Gid), nil\n+        gpath, err := fs.RootPath(root, \"/etc/group\")\n+        if err != nil {\n+                return 0, err\n+        }\n+        groups, err := user.ParseGroupFileFilter(gpath, filter)\n+        if err != nil {\n+                return 0, err\n+        }\n+        if len(groups) == 0 {\n+                return 0, ErrNoGroupsFound\n+        }\n+        g := groups[0]\n+        return uint32(g.Gid), nil\n }\n \n func getSupplementalGroupsFromPath(root string, filter func(user.Group) bool) ([]uint32, error) {\n-\tgpath, err := fs.RootPath(root, \"/etc/group\")\n-\tif err != nil {\n-\t\treturn []uint32{}, err\n-\t}\n-\tgroups, err := user.ParseGroupFileFilter(gpath, filter)\n-\tif err != nil {\n-\t\treturn []uint32{}, err\n-\t}\n-\tif len(groups) == 0 {\n-\t\t// if there are no additional groups; just return an empty set\n-\t\treturn []uint32{}, nil\n-\t}\n-\taddlGids := []uint32{}\n-\tfor _, grp := range groups {\n-\t\taddlGids = append(addlGids, uint32(grp.Gid))\n-\t}\n-\treturn addlGids, nil\n+        gpath, err := fs.RootPath(root, \"/etc/group\")\n+        if err != nil {\n+                return []uint32{}, err\n+        }\n+        groups, err := user.ParseGroupFileFilter(gpath, filter)\n+        if err != nil {\n+                return []uint32{}, err\n+        }\n+        if len(groups) == 0 {\n+                // if there are no additional groups; just return an empty set\n+                return []uint32{}, nil\n+        }\n+        addlGids := []uint32{}\n+        for _, grp := range groups {\n+                addlGids = append(addlGids, uint32(grp.Gid))\n+        }\n+        return addlGids, nil\n }\n \n func isRootfsAbs(root string) bool {\n-\treturn filepath.IsAbs(root)\n+        return filepath.IsAbs(root)\n }\n \n // WithMaskedPaths sets the masked paths option\n func WithMaskedPaths(paths []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\ts.Linux.MaskedPaths = paths\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                s.Linux.MaskedPaths = paths\n+                return nil\n+        }\n }\n \n // WithReadonlyPaths sets the read only paths option\n func WithReadonlyPaths(paths []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\ts.Linux.ReadonlyPaths = paths\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                s.Linux.ReadonlyPaths = paths\n+                return nil\n+        }\n }\n \n // WithWriteableSysfs makes any sysfs mounts writeable\n func WithWriteableSysfs(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tfor _, m := range s.Mounts {\n-\t\tif m.Type == \"sysfs\" {\n-\t\t\tfor i, o := range m.Options {\n-\t\t\t\tif o == \"ro\" {\n-\t\t\t\t\tm.Options[i] = \"rw\"\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, m := range s.Mounts {\n+                if m.Type == \"sysfs\" {\n+                        for i, o := range m.Options {\n+                                if o == \"ro\" {\n+                                        m.Options[i] = \"rw\"\n+                                }\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n // WithWriteableCgroupfs makes any cgroup mounts writeable\n func WithWriteableCgroupfs(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tfor _, m := range s.Mounts {\n-\t\tif m.Type == \"cgroup\" {\n-\t\t\tfor i, o := range m.Options {\n-\t\t\t\tif o == \"ro\" {\n-\t\t\t\t\tm.Options[i] = \"rw\"\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, m := range s.Mounts {\n+                if m.Type == \"cgroup\" {\n+                        for i, o := range m.Options {\n+                                if o == \"ro\" {\n+                                        m.Options[i] = \"rw\"\n+                                }\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n // WithSelinuxLabel sets the process SELinux label\n func WithSelinuxLabel(label string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.SelinuxLabel = label\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.SelinuxLabel = label\n+                return nil\n+        }\n }\n \n // WithApparmorProfile sets the Apparmor profile for the process\n func WithApparmorProfile(profile string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.ApparmorProfile = profile\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.ApparmorProfile = profile\n+                return nil\n+        }\n }\n \n // WithSeccompUnconfined clears the seccomp profile\n func WithSeccompUnconfined(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetLinux(s)\n-\ts.Linux.Seccomp = nil\n-\treturn nil\n+        setLinux(s)\n+        s.Linux.Seccomp = nil\n+        return nil\n }\n \n // WithParentCgroupDevices uses the default cgroup setup to inherit the container's parent cgroup's\n // allowed and denied devices\n func WithParentCgroupDevices(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetLinux(s)\n-\tif s.Linux.Resources == nil {\n-\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t}\n-\ts.Linux.Resources.Devices = nil\n-\treturn nil\n+        setLinux(s)\n+        if s.Linux.Resources == nil {\n+                s.Linux.Resources = &specs.LinuxResources{}\n+        }\n+        s.Linux.Resources.Devices = nil\n+        return nil\n }\n \n // WithAllDevicesAllowed permits READ WRITE MKNOD on all devices nodes for the container\n func WithAllDevicesAllowed(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetLinux(s)\n-\tif s.Linux.Resources == nil {\n-\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t}\n-\ts.Linux.Resources.Devices = []specs.LinuxDeviceCgroup{\n-\t\t{\n-\t\t\tAllow:  true,\n-\t\t\tAccess: rwm,\n-\t\t},\n-\t}\n-\treturn nil\n+        setLinux(s)\n+        if s.Linux.Resources == nil {\n+                s.Linux.Resources = &specs.LinuxResources{}\n+        }\n+        s.Linux.Resources.Devices = []specs.LinuxDeviceCgroup{\n+                {\n+                        Allow:  true,\n+                        Access: rwm,\n+                },\n+        }\n+        return nil\n }\n \n // WithDefaultUnixDevices adds the default devices for unix such as /dev/null, /dev/random to\n // the container's resource cgroup spec\n func WithDefaultUnixDevices(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetLinux(s)\n-\tif s.Linux.Resources == nil {\n-\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t}\n-\tintptr := func(i int64) *int64 {\n-\t\treturn &i\n-\t}\n-\ts.Linux.Resources.Devices = append(s.Linux.Resources.Devices, []specs.LinuxDeviceCgroup{\n-\t\t{\n-\t\t\t// \"/dev/null\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(3),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/random\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(8),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/full\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(7),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/tty\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(5),\n-\t\t\tMinor:  intptr(0),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/zero\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(5),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/urandom\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(9),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/console\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(5),\n-\t\t\tMinor:  intptr(1),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t// /dev/pts/ - pts namespaces are \"coming soon\"\n-\t\t{\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(136),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"dev/ptmx\"\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(5),\n-\t\t\tMinor:  intptr(2),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t}...)\n-\treturn nil\n+        setLinux(s)\n+        if s.Linux.Resources == nil {\n+                s.Linux.Resources = &specs.LinuxResources{}\n+        }\n+        intptr := func(i int64) *int64 {\n+                return &i\n+        }\n+        s.Linux.Resources.Devices = append(s.Linux.Resources.Devices, []specs.LinuxDeviceCgroup{\n+                {\n+                        // \"/dev/null\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(3),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/random\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(8),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/full\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(7),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/tty\",\n+                        Type:   \"c\",\n+                        Major:  intptr(5),\n+                        Minor:  intptr(0),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/zero\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(5),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/urandom\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(9),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/console\",\n+                        Type:   \"c\",\n+                        Major:  intptr(5),\n+                        Minor:  intptr(1),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                // /dev/pts/ - pts namespaces are \"coming soon\"\n+                {\n+                        Type:   \"c\",\n+                        Major:  intptr(136),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"dev/ptmx\"\n+                        Type:   \"c\",\n+                        Major:  intptr(5),\n+                        Minor:  intptr(2),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+        }...)\n+        return nil\n }\n \n // WithPrivileged sets up options for a privileged container\n var WithPrivileged = Compose(\n-\tWithAllCurrentCapabilities,\n-\tWithMaskedPaths(nil),\n-\tWithReadonlyPaths(nil),\n-\tWithWriteableSysfs,\n-\tWithWriteableCgroupfs,\n-\tWithSelinuxLabel(\"\"),\n-\tWithApparmorProfile(\"\"),\n-\tWithSeccompUnconfined,\n+        WithAllCurrentCapabilities,\n+        WithMaskedPaths(nil),\n+        WithReadonlyPaths(nil),\n+        WithWriteableSysfs,\n+        WithWriteableCgroupfs,\n+        WithSelinuxLabel(\"\"),\n+        WithApparmorProfile(\"\"),\n+        WithSeccompUnconfined,\n )\n \n // WithWindowsHyperV sets the Windows.HyperV section for HyperV isolation of containers.\n func WithWindowsHyperV(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tif s.Windows == nil {\n-\t\ts.Windows = &specs.Windows{}\n-\t}\n-\tif s.Windows.HyperV == nil {\n-\t\ts.Windows.HyperV = &specs.WindowsHyperV{}\n-\t}\n-\treturn nil\n+        if s.Windows == nil {\n+                s.Windows = &specs.Windows{}\n+        }\n+        if s.Windows.HyperV == nil {\n+                s.Windows.HyperV = &specs.WindowsHyperV{}\n+        }\n+        return nil\n }\n \n // WithMemoryLimit sets the `Linux.LinuxResources.Memory.Limit` section to the\n@@ -1160,97 +1160,97 @@ func WithWindowsHyperV(_ context.Context, _ Client, _ *containers.Container, s *\n // `Windows.WindowsResources.Memory.Limit` section if the `Windows` section is\n // not `nil`.\n func WithMemoryLimit(limit uint64) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tif s.Linux != nil {\n-\t\t\tif s.Linux.Resources == nil {\n-\t\t\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t\t\t}\n-\t\t\tif s.Linux.Resources.Memory == nil {\n-\t\t\t\ts.Linux.Resources.Memory = &specs.LinuxMemory{}\n-\t\t\t}\n-\t\t\tl := int64(limit)\n-\t\t\ts.Linux.Resources.Memory.Limit = &l\n-\t\t}\n-\t\tif s.Windows != nil {\n-\t\t\tif s.Windows.Resources == nil {\n-\t\t\t\ts.Windows.Resources = &specs.WindowsResources{}\n-\t\t\t}\n-\t\t\tif s.Windows.Resources.Memory == nil {\n-\t\t\t\ts.Windows.Resources.Memory = &specs.WindowsMemoryResources{}\n-\t\t\t}\n-\t\t\ts.Windows.Resources.Memory.Limit = &limit\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                if s.Linux != nil {\n+                        if s.Linux.Resources == nil {\n+                                s.Linux.Resources = &specs.LinuxResources{}\n+                        }\n+                        if s.Linux.Resources.Memory == nil {\n+                                s.Linux.Resources.Memory = &specs.LinuxMemory{}\n+                        }\n+                        l := int64(limit)\n+                        s.Linux.Resources.Memory.Limit = &l\n+                }\n+                if s.Windows != nil {\n+                        if s.Windows.Resources == nil {\n+                                s.Windows.Resources = &specs.WindowsResources{}\n+                        }\n+                        if s.Windows.Resources.Memory == nil {\n+                                s.Windows.Resources.Memory = &specs.WindowsMemoryResources{}\n+                        }\n+                        s.Windows.Resources.Memory.Limit = &limit\n+                }\n+                return nil\n+        }\n }\n \n // WithAnnotations appends or replaces the annotations on the spec with the\n // provided annotations\n func WithAnnotations(annotations map[string]string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tif s.Annotations == nil {\n-\t\t\ts.Annotations = make(map[string]string)\n-\t\t}\n-\t\tfor k, v := range annotations {\n-\t\t\ts.Annotations[k] = v\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                if s.Annotations == nil {\n+                        s.Annotations = make(map[string]string)\n+                }\n+                for k, v := range annotations {\n+                        s.Annotations[k] = v\n+                }\n+                return nil\n+        }\n }\n \n // WithLinuxDevices adds the provided linux devices to the spec\n func WithLinuxDevices(devices []specs.LinuxDevice) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\ts.Linux.Devices = append(s.Linux.Devices, devices...)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                s.Linux.Devices = append(s.Linux.Devices, devices...)\n+                return nil\n+        }\n }\n \n // WithLinuxDevice adds the device specified by path to the spec\n func WithLinuxDevice(path, permissions string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\tsetResources(s)\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                setResources(s)\n \n-\t\tdev, err := DeviceFromPath(path)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n+                dev, err := DeviceFromPath(path)\n+                if err != nil {\n+                        return err\n+                }\n \n-\t\ts.Linux.Devices = append(s.Linux.Devices, *dev)\n+                s.Linux.Devices = append(s.Linux.Devices, *dev)\n \n-\t\ts.Linux.Resources.Devices = append(s.Linux.Resources.Devices, specs.LinuxDeviceCgroup{\n-\t\t\tType:   dev.Type,\n-\t\t\tAllow:  true,\n-\t\t\tMajor:  &dev.Major,\n-\t\t\tMinor:  &dev.Minor,\n-\t\t\tAccess: permissions,\n-\t\t})\n+                s.Linux.Resources.Devices = append(s.Linux.Resources.Devices, specs.LinuxDeviceCgroup{\n+                        Type:   dev.Type,\n+                        Allow:  true,\n+                        Major:  &dev.Major,\n+                        Minor:  &dev.Minor,\n+                        Access: permissions,\n+                })\n \n-\t\treturn nil\n-\t}\n+                return nil\n+        }\n }\n \n // WithEnvFile adds environment variables from a file to the container's spec\n func WithEnvFile(path string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tvar vars []string\n-\t\tf, err := os.Open(path)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tdefer f.Close()\n-\n-\t\tsc := bufio.NewScanner(f)\n-\t\tfor sc.Scan() {\n-\t\t\tvars = append(vars, sc.Text())\n-\t\t}\n-\t\tif err = sc.Err(); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\treturn WithEnv(vars)(nil, nil, nil, s)\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                var vars []string\n+                f, err := os.Open(path)\n+                if err != nil {\n+                        return err\n+                }\n+                defer f.Close()\n+\n+                sc := bufio.NewScanner(f)\n+                for sc.Scan() {\n+                        vars = append(vars, sc.Text())\n+                }\n+                if err = sc.Err(); err != nil {\n+                        return err\n+                }\n+                return WithEnv(vars)(nil, nil, nil, s)\n+        }\n }\n \n // ErrNoShmMount is returned when there is no /dev/shm mount specified in the config\n@@ -1261,21 +1261,21 @@ var ErrNoShmMount = errors.New(\"no /dev/shm mount specified\")\n //\n // The size value is specified in kb, kilobytes.\n func WithDevShmSize(kb int64) SpecOpts {\n-\treturn func(ctx context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tfor i, m := range s.Mounts {\n-\t\t\tif filepath.Clean(m.Destination) == \"/dev/shm\" && m.Source == \"shm\" && m.Type == \"tmpfs\" {\n-\t\t\t\tfor i := 0; i < len(m.Options); i++ {\n-\t\t\t\t\tif strings.HasPrefix(m.Options[i], \"size=\") {\n-\t\t\t\t\t\tm.Options = append(m.Options[:i], m.Options[i+1:]...)\n-\t\t\t\t\t\ti--\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\ts.Mounts[i].Options = append(m.Options, fmt.Sprintf(\"size=%dk\", kb))\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t}\n-\t\treturn ErrNoShmMount\n-\t}\n+        return func(ctx context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                for i, m := range s.Mounts {\n+                        if filepath.Clean(m.Destination) == \"/dev/shm\" && m.Source == \"shm\" && m.Type == \"tmpfs\" {\n+                                for i := 0; i < len(m.Options); i++ {\n+                                        if strings.HasPrefix(m.Options[i], \"size=\") {\n+                                                m.Options = append(m.Options[:i], m.Options[i+1:]...)\n+                                                i--\n+                                        }\n+                                }\n+                                s.Mounts[i].Options = append(m.Options, fmt.Sprintf(\"size=%dk\", kb))\n+                                return nil\n+                        }\n+                }\n+                return ErrNoShmMount\n+        }\n }\n \n // tryReadonlyMounts is used by the options which are trying to get user/group\n@@ -1290,8 +1290,8 @@ func WithDevShmSize(kb int64) SpecOpts {\n // API, when the caller passes that experimental annotation\n // `containerd.io/snapshot/readonly.mount` something like that.\n func tryReadonlyMounts(mounts []mount.Mount) []mount.Mount {\n-\tif len(mounts) == 1 && mounts[0].Type == \"overlay\" {\n-\t\tmounts[0].Options = append(mounts[0].Options, \"ro\")\n-\t}\n-\treturn mounts\n+        if len(mounts) == 1 && mounts[0].Type == \"overlay\" {\n+                mounts[0].Options = append(mounts[0].Options, \"ro\")\n+        }\n+        return mounts\n }\ndiff --git a/oci/supplemental.go b/oci/supplemental.go\nnew file mode 100644\nindex 000000000..ac76061c0\n--- /dev/null\n+++ b/oci/supplemental.go\n@@ -0,0 +1,40 @@\n+func withSupplementalGroups(userstr string) SpecOpts {\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+                setProcess(s)\n+\n+                var username string\n+                uid, err := strconv.Atoi(userstr)\n+                if err == nil {\n+                        user, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n+                                return u.Uid == uid\n+                        })\n+                        if err != nil {\n+                                if os.IsNotExist(err) || err == ErrNoUsersFound {\n+                                        return nil\n+                                }\n+                                return err\n+                        }\n+                        username = user.Name\n+                } else {\n+                        username = userstr\n+                }\n+\n+                gids, err := getSupplementalGroupsFromPath(s.Root.Path, func(g user.Group) bool {\n+                        for _, entry := range g.List {\n+                                if entry == username {\n+                                        return true\n+                                }\n+                        }\n+                        return false\n+                })\n+                if err != nil {\n+                        if os.IsNotExist(err) {\n+                                return nil\n+                        }\n+                        return err\n+                }\n+\n+                s.Process.User.AdditionalGids = gids\n+                return nil\n+        }\n+}\n\\ No newline at end of file\n"}
{"cve":"CVE-2022-36103:0708", "fix_patch": "diff --git a/internal/app/trustd/internal/reg/reg.go b/internal/app/trustd/internal/reg/reg.go\nindex ed5fd6960..043b602a5 100644\n--- a/internal/app/trustd/internal/reg/reg.go\n+++ b/internal/app/trustd/internal/reg/reg.go\n@@ -5,51 +5,101 @@\n package reg\n \n import (\n-\t\"context\"\n+import (\n+\"context\"\n+\"crypto/rand\"\n+\"crypto/x509\"\n+\n+\"github.com/cosi-project/runtime/pkg/resource\"\n+\"github.com/cosi-project/runtime/pkg/safe\"\n+\"github.com/cosi-project/runtime/pkg/state\"\n+csrx509 \"github.com/siderolabs/crypto/x509\"\n+\"google.golang.org/grpc\"\n+\"google.golang.org/grpc/codes\"\n+\"google.golang.org/grpc/status\"\n+\n+\"github.com/talos-systems/talos/internal/app/trustd/internal/server/sm\"\n+\"github.com/talos-systems/talos/internal/pkg/platform/context/request\"\n+\"github.com/talos-systems/talos/internal/pkg/secrets\"\n+securityapi \"github.com/talos-systems/talos/pkg/security/api\"\n+)\n+import (\n+\"context\"\n+\"crypto/rand\"\n+\"crypto/x509\"\n+\n+\"github.com/cosi-project/runtime/pkg/resource\"\n+\"github.com/cosi-project/runtime/pkg/safe\"\n+\"github.com/cosi-project/runtime/pkg/state\"\n+csrx509 \"github.com/siderolabs/crypto/x509\"\n+\"google.golang.org/grpc\"\n+\"google.golang.org/grpc/codes\"\n+\"google.golang.org/grpc/status\"\n+\n+\"github.com/talos-systems/talos/internal/app/trustd/internal/server/sm\"\n+\"github.com/talos-systems/talos/internal/pkg/platform/context/request\"\n+\"github.com/talos-systems/talos/internal/pkg/secrets\"\n+securityapi \"github.com/talos-systems/talos/pkg/security/api\"\n+)\n+        \"context\"\n \n-\t\"github.com/cosi-project/runtime/pkg/resource\"\n-\t\"github.com/cosi-project/runtime/pkg/safe\"\n-\t\"github.com/cosi-project/runtime/pkg/state\"\n-\t\"github.com/siderolabs/crypto/x509\"\n-\t\"google.golang.org/grpc\"\n+        \"crypto/rand\"\n \n-\tsecurityapi \"github.com/talos-systems/talos/pkg/machinery/api/security\"\n-\t\"github.com/talos-systems/talos/pkg/machinery/resources/secrets\"\n+        \"github.com/cosi-project/runtime/pkg/resource\"\n+        \"github.com/cosi-project/runtime/pkg/safe\"\n+        \"github.com/cosi-project/runtime/pkg/state\"\n+\"github.com/siderolabs/crypto/x509\"\n+        \n+        \"google.golang.org/grpc\"\n+\n+        securityapi \"github.com/talos-systems/talos/pkg/machinery/api/security\"\n+        \"github.com/talos-systems/talos/pkg/machinery/resources/secrets\"\n )\n \n // Registrator is the concrete type that implements the factory.Registrator and\n // securityapi.SecurityServiceServer interfaces.\n type Registrator struct {\n-\tsecurityapi.UnimplementedSecurityServiceServer\n+        securityapi.UnimplementedSecurityServiceServer\n \n-\tResources state.State\n+        Resources state.State\n }\n \n // Register implements the factory.Registrator interface.\n //\n //nolint:interfacer\n func (r *Registrator) Register(s *grpc.Server) {\n-\tsecurityapi.RegisterSecurityServiceServer(s, r)\n+        securityapi.RegisterSecurityServiceServer(s, r)\n }\n \n // Certificate implements the securityapi.SecurityServer interface.\n func (r *Registrator) Certificate(ctx context.Context, in *securityapi.CertificateRequest) (resp *securityapi.CertificateResponse, err error) {\n-\tosRoot, err := safe.StateGet[*secrets.OSRoot](ctx, r.Resources, resource.NewMetadata(secrets.NamespaceName, secrets.OSRootType, secrets.OSRootID, resource.VersionUndefined))\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// TODO: Verify that the request is coming from the IP addresss declared in\n-\t// the CSR.\n-\tsigned, err := x509.NewCertificateFromCSRBytes(osRoot.TypedSpec().CA.Crt, osRoot.TypedSpec().CA.Key, in.Csr)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tresp = &securityapi.CertificateResponse{\n-\t\tCa:  osRoot.TypedSpec().CA.Crt,\n-\t\tCrt: signed.X509CertificatePEM,\n-\t}\n-\n-\treturn resp, nil\n+        osRoot, err := safe.StateGet[*secrets.OSRoot](ctx, r.Resources, resource.NewMetadata(secrets.NamespaceName, secrets.OSRootType, secrets.OSRootID, resource.VersionUndefined))\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // TODO: Verify that the request is coming from the IP addresss declared in\n+        // the CSR.\n+csr, err := x509.ParseCertificateRequest(in.Csr)\n+if err != nil {\n+return\n+}\n+\n+csr.Subject.Organization = nil\n+sanitizedCsr, err := x509.CreateCertificateRequest(rand.Reader, csr, osRoot.TypedSpec().CA.Key)\n+if err != nil {\n+return\n+}\n+\n+        signed, err := csrx509.NewCertificateFromCSRBytes(osRoot.TypedSpec().CA.Crt, osRoot.TypedSpec().CA.Key, sanitizedCsr, x509.ExtKeyUsageServerAuth)\n+        if err != nil {\n+                return\n+        }\n+\n+        resp = &securityapi.CertificateResponse{\n+                Ca:  osRoot.TypedSpec().CA.Crt,\n+                Crt: signed.X509CertificatePEM,\n+        }\n+\n+        return resp, nil\n }\n"}
{"cve":"CVE-2023-22480:0708", "fix_patch": "diff --git a/pkg/router/v1/v1api.go b/pkg/router/v1/v1api.go\nindex 1a3b9cbc1..737c8163f 100644\n--- a/pkg/router/v1/v1api.go\n+++ b/pkg/router/v1/v1api.go\n@@ -1,122 +1,123 @@\n package v1\n \n import (\n-\t\"encoding/json\"\n-\t\"net/http\"\n+        \"encoding/json\"\n+        \"net/http\"\n \n-\t\"github.com/KubeOperator/KubeOperator/pkg/controller\"\n-\t\"github.com/KubeOperator/KubeOperator/pkg/errorf\"\n-\t\"github.com/KubeOperator/KubeOperator/pkg/middleware\"\n-\t\"github.com/jinzhu/gorm\"\n-\t\"github.com/kataras/iris/v12\"\n-\t\"github.com/kataras/iris/v12/context\"\n-\t\"github.com/kataras/iris/v12/mvc\"\n-\t\"github.com/pkg/errors\"\n+        \"github.com/KubeOperator/KubeOperator/pkg/controller\"\n+        \"github.com/KubeOperator/KubeOperator/pkg/errorf\"\n+        \"github.com/KubeOperator/KubeOperator/pkg/middleware\"\n+        \"github.com/jinzhu/gorm\"\n+        \"github.com/kataras/iris/v12\"\n+        \"github.com/kataras/iris/v12/context\"\n+        \"github.com/kataras/iris/v12/mvc\"\n+        \"github.com/pkg/errors\"\n )\n \n var AuthScope iris.Party\n var WhiteScope iris.Party\n \n func V1(parent iris.Party) {\n-\tv1 := parent.Party(\"/v1\")\n-\tauthParty := v1.Party(\"/auth\")\n-\tmvc.New(authParty.Party(\"/session\")).HandleError(ErrorHandler).Handle(controller.NewSessionController())\n-\tmvc.New(v1.Party(\"/user\")).HandleError(ErrorHandler).Handle(controller.NewForgotPasswordController())\n-\tAuthScope = v1.Party(\"/\")\n-\tAuthScope.Use(middleware.JWTMiddleware().Serve)\n-\tAuthScope.Use(middleware.UserMiddleware)\n-\tAuthScope.Use(middleware.RBACMiddleware())\n-\tAuthScope.Use(middleware.PagerMiddleware)\n-\tAuthScope.Use(middleware.ForceMiddleware)\n-\tmvc.New(AuthScope.Party(\"/clusters\")).HandleError(ErrorHandler).Handle(controller.NewClusterController())\n-\tmvc.New(AuthScope.Party(\"/credentials\")).HandleError(ErrorHandler).Handle(controller.NewCredentialController())\n-\tmvc.New(AuthScope.Party(\"/hosts\")).HandleError(ErrorHandler).Handle(controller.NewHostController())\n-\tmvc.New(AuthScope.Party(\"/users\")).HandleError(ErrorHandler).Handle(controller.NewUserController())\n-\tmvc.New(AuthScope.Party(\"/dashboard\")).HandleError(ErrorHandler).Handle(controller.NewKubePiController())\n-\tmvc.New(AuthScope.Party(\"/regions\")).HandleError(ErrorHandler).Handle(controller.NewRegionController())\n-\tmvc.New(AuthScope.Party(\"/zones\")).HandleError(ErrorHandler).Handle(controller.NewZoneController())\n-\tmvc.New(AuthScope.Party(\"/plans\")).HandleError(ErrorHandler).Handle(controller.NewPlanController())\n-\tmvc.New(AuthScope.Party(\"/settings\")).HandleError(ErrorHandler).Handle(controller.NewSystemSettingController())\n-\tmvc.New(AuthScope.Party(\"/ntp\")).HandleError(ErrorHandler).Handle(controller.NewNtpServerController())\n-\tmvc.New(AuthScope.Party(\"/logs\")).HandleError(ErrorHandler).Handle(controller.NewSystemLogController())\n-\tmvc.New(AuthScope.Party(\"/projects\")).HandleError(ErrorHandler).Handle(controller.NewProjectController())\n-\tmvc.New(AuthScope.Party(\"/clusters/provisioner\")).HandleError(ErrorHandler).Handle(controller.NewProvisionerController())\n-\tmvc.New(AuthScope.Party(\"/kubernetes\")).HandleError(ErrorHandler).Handle(controller.NewKubernetesController())\n-\tmvc.New(AuthScope.Party(\"/clusters/tool\")).HandleError(ErrorHandler).Handle(controller.NewClusterToolController())\n-\tmvc.New(AuthScope.Party(\"/backupaccounts\")).HandleError(ErrorHandler).Handle(controller.NewBackupAccountController())\n-\tmvc.New(AuthScope.Party(\"/clusters/backup\")).HandleError(ErrorHandler).Handle(controller.NewClusterBackupStrategyController())\n-\tmvc.New(AuthScope.Party(\"/clusters/monitor\")).HandleError(ErrorHandler).Handle(controller.NewMonitorController())\n-\tmvc.New(AuthScope.Party(\"/tasks\")).Handle(ErrorHandler).Handle(controller.NewTaskLogController())\n-\tmvc.New(AuthScope.Party(\"/components\")).Handle(ErrorHandler).Handle(controller.NewComponentController())\n-\tmvc.New(AuthScope.Party(\"/license\")).Handle(ErrorHandler).Handle(controller.NewLicenseController())\n-\tmvc.New(AuthScope.Party(\"/clusters/backup/files\")).HandleError(ErrorHandler).Handle(controller.NewClusterBackupFileController())\n-\tmvc.New(AuthScope.Party(\"/clusters/velero/{cluster}/{operate}\")).HandleError(ErrorHandler).Handle(controller.NewClusterVeleroBackupController())\n-\tmvc.New(AuthScope.Party(\"/manifests\")).HandleError(ErrorHandler).Handle(controller.NewManifestController())\n-\tmvc.New(AuthScope.Party(\"/vmconfigs\")).HandleError(ErrorHandler).Handle(controller.NewVmConfigController())\n-\tmvc.New(AuthScope.Party(\"/ippools\")).HandleError(ErrorHandler).Handle(controller.NewIpPoolController())\n-\tmvc.New(AuthScope.Party(\"/ippools/{name}/ips\")).HandleError(ErrorHandler).Handle(controller.NewIpController())\n-\tmvc.New(AuthScope.Party(\"/projects/{project}/resources\")).HandleError(ErrorHandler).Handle(controller.NewProjectResourceController())\n-\tmvc.New(AuthScope.Party(\"/projects/{project}/members\")).HandleError(ErrorHandler).Handle(controller.NewProjectMemberController())\n-\tmvc.New(AuthScope.Party(\"/projects/{project}/clusters/{cluster}/members\")).HandleError(ErrorHandler).Handle(controller.NewClusterMemberController())\n-\tmvc.New(AuthScope.Party(\"/projects/{project}/clusters/{cluster}/resources\")).HandleError(ErrorHandler).Handle(controller.NewClusterResourceController())\n-\tmvc.New(AuthScope.Party(\"/templates\")).HandleError(ErrorHandler).Handle(controller.NewTemplateConfigController())\n-\tmvc.New(AuthScope.Party(\"/clusters/grade\")).HandleError(ErrorHandler).Handle(controller.NewGradeController())\n-\tmvc.New(AuthScope.Party(\"/ldap\")).HandleError(ErrorHandler).Handle(controller.NewLdapController())\n-\tmvc.New(AuthScope.Party(\"/msg/accounts\")).HandleError(ErrorHandler).Handle(controller.NewMessageAccountController())\n-\tmvc.New(AuthScope.Party(\"/msg/subscribes\")).HandleError(ErrorHandler).Handle(controller.NewMessageSubscribeController())\n-\tmvc.New(AuthScope.Party(\"/user/messages\")).HandleError(ErrorHandler).Handle(controller.NewUserMsgController())\n-\tmvc.New(AuthScope.Party(\"/user/settings\")).HandleError(ErrorHandler).Handle(controller.NewUserSettingController())\n-\tWhiteScope = v1.Party(\"/\")\n-\tWhiteScope.Get(\"/clusters/kubeconfig/{name}\", downloadKubeconfig)\n-\tWhiteScope.Get(\"/captcha\", generateCaptcha)\n-\tmvc.New(WhiteScope.Party(\"/theme\")).HandleError(ErrorHandler).Handle(controller.NewThemeController())\n+        v1 := parent.Party(\"/v1\")\n+        authParty := v1.Party(\"/auth\")\n+        mvc.New(authParty.Party(\"/session\")).HandleError(ErrorHandler).Handle(controller.NewSessionController())\n+        mvc.New(v1.Party(\"/user\")).HandleError(ErrorHandler).Handle(controller.NewForgotPasswordController())\n+        AuthScope = v1.Party(\"/\")\n+        AuthScope.Use(middleware.JWTMiddleware().Serve)\n+        AuthScope.Use(middleware.UserMiddleware)\n+        AuthScope.Use(middleware.RBACMiddleware())\n+        AuthScope.Use(middleware.PagerMiddleware)\n+        AuthScope.Use(middleware.ForceMiddleware)\n+        mvc.New(AuthScope.Party(\"/clusters\")).HandleError(ErrorHandler).Handle(controller.NewClusterController())\n+        AuthScope.Get(\"/clusters/kubeconfig/{name}\", downloadKubeconfig)\n+        mvc.New(AuthScope.Party(\"/credentials\")).HandleError(ErrorHandler).Handle(controller.NewCredentialController())\n+        mvc.New(AuthScope.Party(\"/hosts\")).HandleError(ErrorHandler).Handle(controller.NewHostController())\n+        mvc.New(AuthScope.Party(\"/users\")).HandleError(ErrorHandler).Handle(controller.NewUserController())\n+        mvc.New(AuthScope.Party(\"/dashboard\")).HandleError(ErrorHandler).Handle(controller.NewKubePiController())\n+        mvc.New(AuthScope.Party(\"/regions\")).HandleError(ErrorHandler).Handle(controller.NewRegionController())\n+        mvc.New(AuthScope.Party(\"/zones\")).HandleError(ErrorHandler).Handle(controller.NewZoneController())\n+        mvc.New(AuthScope.Party(\"/plans\")).HandleError(ErrorHandler).Handle(controller.NewPlanController())\n+        mvc.New(AuthScope.Party(\"/settings\")).HandleError(ErrorHandler).Handle(controller.NewSystemSettingController())\n+        mvc.New(AuthScope.Party(\"/ntp\")).HandleError(ErrorHandler).Handle(controller.NewNtpServerController())\n+        mvc.New(AuthScope.Party(\"/logs\")).HandleError(ErrorHandler).Handle(controller.NewSystemLogController())\n+        mvc.New(AuthScope.Party(\"/projects\")).HandleError(ErrorHandler).Handle(controller.NewProjectController())\n+        mvc.New(AuthScope.Party(\"/clusters/provisioner\")).HandleError(ErrorHandler).Handle(controller.NewProvisionerController())\n+        mvc.New(AuthScope.Party(\"/kubernetes\")).HandleError(ErrorHandler).Handle(controller.NewKubernetesController())\n+        mvc.New(AuthScope.Party(\"/clusters/tool\")).HandleError(ErrorHandler).Handle(controller.NewClusterToolController())\n+        mvc.New(AuthScope.Party(\"/backupaccounts\")).HandleError(ErrorHandler).Handle(controller.NewBackupAccountController())\n+        mvc.New(AuthScope.Party(\"/clusters/backup\")).HandleError(ErrorHandler).Handle(controller.NewClusterBackupStrategyController())\n+        mvc.New(AuthScope.Party(\"/clusters/monitor\")).HandleError(ErrorHandler).Handle(controller.NewMonitorController())\n+        mvc.New(AuthScope.Party(\"/tasks\")).Handle(ErrorHandler).Handle(controller.NewTaskLogController())\n+        mvc.New(AuthScope.Party(\"/components\")).Handle(ErrorHandler).Handle(controller.NewComponentController())\n+        mvc.New(AuthScope.Party(\"/license\")).Handle(ErrorHandler).Handle(controller.NewLicenseController())\n+        mvc.New(AuthScope.Party(\"/clusters/backup/files\")).HandleError(ErrorHandler).Handle(controller.NewClusterBackupFileController())\n+        mvc.New(AuthScope.Party(\"/clusters/velero/{cluster}/{operate}\")).HandleError(ErrorHandler).Handle(controller.NewClusterVeleroBackupController())\n+        mvc.New(AuthScope.Party(\"/manifests\")).HandleError(ErrorHandler).Handle(controller.NewManifestController())\n+        mvc.New(AuthScope.Party(\"/vmconfigs\")).HandleError(ErrorHandler).Handle(controller.NewVmConfigController())\n+        mvc.New(AuthScope.Party(\"/ippools\")).HandleError(ErrorHandler).Handle(controller.NewIpPoolController())\n+        mvc.New(AuthScope.Party(\"/ippools/{name}/ips\")).HandleError(ErrorHandler).Handle(controller.NewIpController())\n+        mvc.New(AuthScope.Party(\"/projects/{project}/resources\")).HandleError(ErrorHandler).Handle(controller.NewProjectResourceController())\n+        mvc.New(AuthScope.Party(\"/projects/{project}/members\")).HandleError(ErrorHandler).Handle(controller.NewProjectMemberController())\n+        mvc.New(AuthScope.Party(\"/projects/{project}/clusters/{cluster}/members\")).HandleError(ErrorHandler).Handle(controller.NewClusterMemberController())\n+        mvc.New(AuthScope.Party(\"/projects/{project}/clusters/{cluster}/resources\")).HandleError(ErrorHandler).Handle(controller.NewClusterResourceController())\n+        mvc.New(AuthScope.Party(\"/templates\")).HandleError(ErrorHandler).Handle(controller.NewTemplateConfigController())\n+        mvc.New(AuthScope.Party(\"/clusters/grade\")).HandleError(ErrorHandler).Handle(controller.NewGradeController())\n+        mvc.New(AuthScope.Party(\"/ldap\")).HandleError(ErrorHandler).Handle(controller.NewLdapController())\n+        mvc.New(AuthScope.Party(\"/msg/accounts\")).HandleError(ErrorHandler).Handle(controller.NewMessageAccountController())\n+        mvc.New(AuthScope.Party(\"/msg/subscribes\")).HandleError(ErrorHandler).Handle(controller.NewMessageSubscribeController())\n+        mvc.New(AuthScope.Party(\"/user/messages\")).HandleError(ErrorHandler).Handle(controller.NewUserMsgController())\n+        mvc.New(AuthScope.Party(\"/user/settings\")).HandleError(ErrorHandler).Handle(controller.NewUserSettingController())\n+        WhiteScope = v1.Party(\"/\")\n+ \n+        WhiteScope.Get(\"/captcha\", generateCaptcha)\n+        mvc.New(WhiteScope.Party(\"/theme\")).HandleError(ErrorHandler).Handle(controller.NewThemeController())\n \n }\n \n func ErrorHandler(ctx context.Context, err error) {\n-\tif err != nil {\n-\t\twarp := struct {\n-\t\t\tMsg string `json:\"msg\"`\n-\t\t}{err.Error()}\n-\t\tvar result string\n-\t\tswitch errType := err.(type) {\n-\t\tcase gorm.Errors:\n-\t\t\terrorSet := make(map[string]string)\n-\t\t\tfor _, er := range errType {\n-\t\t\t\ttr := ctx.Tr(er.Error())\n-\t\t\t\tif tr != \"\" {\n-\t\t\t\t\terrorMsg := tr\n-\t\t\t\t\terrorSet[er.Error()] = errorMsg\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tfor _, set := range errorSet {\n-\t\t\t\tresult = result + set + \" \"\n-\t\t\t}\n-\t\tcase error:\n-\t\t\tswitch errRoot := errors.Cause(err).(type) {\n-\t\t\tcase errorf.CErrFs:\n-\t\t\t\terrs := errRoot.Get()\n-\t\t\t\tfor _, er := range errs {\n-\t\t\t\t\targs := er.Args.([]interface{})\n-\t\t\t\t\ttr := ctx.Tr(er.Msg, args...)\n-\t\t\t\t\tif tr != \"\" {\n-\t\t\t\t\t\tresult = result + tr + \"\\n \"\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\tdefault:\n-\t\t\t\ttr := ctx.Tr(errors.Cause(err).Error())\n-\t\t\t\tif tr != \"\" {\n-\t\t\t\t\tresult = tr\n-\t\t\t\t} else {\n-\t\t\t\t\tresult = err.Error()\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\twarp.Msg = result\n-\t\tbf, _ := json.Marshal(&warp)\n-\t\tctx.StatusCode(http.StatusBadRequest)\n-\t\t_, _ = ctx.WriteString(string(bf))\n-\t\tctx.StopExecution()\n-\t\treturn\n-\t}\n+        if err != nil {\n+                warp := struct {\n+                        Msg string `json:\"msg\"`\n+                }{err.Error()}\n+                var result string\n+                switch errType := err.(type) {\n+                case gorm.Errors:\n+                        errorSet := make(map[string]string)\n+                        for _, er := range errType {\n+                                tr := ctx.Tr(er.Error())\n+                                if tr != \"\" {\n+                                        errorMsg := tr\n+                                        errorSet[er.Error()] = errorMsg\n+                                }\n+                        }\n+                        for _, set := range errorSet {\n+                                result = result + set + \" \"\n+                        }\n+                case error:\n+                        switch errRoot := errors.Cause(err).(type) {\n+                        case errorf.CErrFs:\n+                                errs := errRoot.Get()\n+                                for _, er := range errs {\n+                                        args := er.Args.([]interface{})\n+                                        tr := ctx.Tr(er.Msg, args...)\n+                                        if tr != \"\" {\n+                                                result = result + tr + \"\\n \"\n+                                        }\n+                                }\n+                        default:\n+                                tr := ctx.Tr(errors.Cause(err).Error())\n+                                if tr != \"\" {\n+                                        result = tr\n+                                } else {\n+                                        result = err.Error()\n+                                }\n+                        }\n+                }\n+                warp.Msg = result\n+                bf, _ := json.Marshal(&warp)\n+                ctx.StatusCode(http.StatusBadRequest)\n+                _, _ = ctx.WriteString(string(bf))\n+                ctx.StopExecution()\n+                return\n+        }\n }\n"}
{"cve":"CVE-2022-36009:0708", "fix_patch": "diff --git a/eventcontent.go b/eventcontent.go\nindex 5f9ba63..931dd2f 100644\n--- a/eventcontent.go\n+++ b/eventcontent.go\n@@ -16,293 +16,293 @@\n package gomatrixserverlib\n \n import (\n-\t\"database/sql/driver\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"strconv\"\n-\t\"strings\"\n+        \"database/sql/driver\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"strconv\"\n+        \"strings\"\n )\n \n // CreateContent is the JSON content of a m.room.create event along with\n // the top level keys needed for auth.\n // See https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-create for descriptions of the fields.\n type CreateContent struct {\n-\t// We need the domain of the create event when checking federatability.\n-\tsenderDomain string\n-\t// We need the roomID to check that events are in the same room as the create event.\n-\troomID string\n-\t// We need the eventID to check the first join event in the room.\n-\teventID string\n-\t// The \"m.federate\" flag tells us whether the room can be federated to other servers.\n-\tFederate *bool `json:\"m.federate,omitempty\"`\n-\t// The creator of the room tells us what the default power levels are.\n-\tCreator string `json:\"creator\"`\n-\t// The version of the room. Should be treated as \"1\" when the key doesn't exist.\n-\tRoomVersion *RoomVersion `json:\"room_version,omitempty\"`\n-\t// The predecessor of the room.\n-\tPredecessor PreviousRoom `json:\"predecessor,omitempty\"`\n+        // We need the domain of the create event when checking federatability.\n+        senderDomain string\n+        // We need the roomID to check that events are in the same room as the create event.\n+        roomID string\n+        // We need the eventID to check the first join event in the room.\n+        eventID string\n+        // The \"m.federate\" flag tells us whether the room can be federated to other servers.\n+        Federate *bool `json:\"m.federate,omitempty\"`\n+        // The creator of the room tells us what the default power levels are.\n+        Creator string `json:\"creator\"`\n+        // The version of the room. Should be treated as \"1\" when the key doesn't exist.\n+        RoomVersion *RoomVersion `json:\"room_version,omitempty\"`\n+        // The predecessor of the room.\n+        Predecessor PreviousRoom `json:\"predecessor,omitempty\"`\n }\n \n // PreviousRoom is the \"Previous Room\" structure defined at https://matrix.org/docs/spec/client_server/r0.5.0#m-room-create\n type PreviousRoom struct {\n-\tRoomID  string `json:\"room_id\"`\n-\tEventID string `json:\"event_id\"`\n+        RoomID  string `json:\"room_id\"`\n+        EventID string `json:\"event_id\"`\n }\n \n // NewCreateContentFromAuthEvents loads the create event content from the create event in the\n // auth events.\n func NewCreateContentFromAuthEvents(authEvents AuthEventProvider) (c CreateContent, err error) {\n-\tvar createEvent *Event\n-\tif createEvent, err = authEvents.Create(); err != nil {\n-\t\treturn\n-\t}\n-\tif createEvent == nil {\n-\t\terr = errorf(\"missing create event\")\n-\t\treturn\n-\t}\n-\tif err = json.Unmarshal(createEvent.Content(), &c); err != nil {\n-\t\terr = errorf(\"unparsable create event content: %s\", err.Error())\n-\t\treturn\n-\t}\n-\tc.roomID = createEvent.RoomID()\n-\tc.eventID = createEvent.EventID()\n-\tif c.senderDomain, err = domainFromID(createEvent.Sender()); err != nil {\n-\t\treturn\n-\t}\n-\treturn\n+        var createEvent *Event\n+        if createEvent, err = authEvents.Create(); err != nil {\n+                return\n+        }\n+        if createEvent == nil {\n+                err = errorf(\"missing create event\")\n+                return\n+        }\n+        if err = json.Unmarshal(createEvent.Content(), &c); err != nil {\n+                err = errorf(\"unparsable create event content: %s\", err.Error())\n+                return\n+        }\n+        c.roomID = createEvent.RoomID()\n+        c.eventID = createEvent.EventID()\n+        if c.senderDomain, err = domainFromID(createEvent.Sender()); err != nil {\n+                return\n+        }\n+        return\n }\n \n // DomainAllowed checks whether the domain is allowed in the room by the\n // \"m.federate\" flag.\n func (c *CreateContent) DomainAllowed(domain string) error {\n-\tif domain == c.senderDomain {\n-\t\t// If the domain matches the domain of the create event then the event\n-\t\t// is always allowed regardless of the value of the \"m.federate\" flag.\n-\t\treturn nil\n-\t}\n-\tif c.Federate == nil || *c.Federate {\n-\t\t// The m.federate field defaults to true.\n-\t\t// If the domains are different then event is only allowed if the\n-\t\t// \"m.federate\" flag is absent or true.\n-\t\treturn nil\n-\t}\n-\treturn errorf(\"room is unfederatable\")\n+        if domain == c.senderDomain {\n+                // If the domain matches the domain of the create event then the event\n+                // is always allowed regardless of the value of the \"m.federate\" flag.\n+                return nil\n+        }\n+        if c.Federate == nil || *c.Federate {\n+                // The m.federate field defaults to true.\n+                // If the domains are different then event is only allowed if the\n+                // \"m.federate\" flag is absent or true.\n+                return nil\n+        }\n+        return errorf(\"room is unfederatable\")\n }\n \n // UserIDAllowed checks whether the domain part of the user ID is allowed in\n // the room by the \"m.federate\" flag.\n func (c *CreateContent) UserIDAllowed(id string) error {\n-\tdomain, err := domainFromID(id)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\treturn c.DomainAllowed(domain)\n+        domain, err := domainFromID(id)\n+        if err != nil {\n+                return err\n+        }\n+        return c.DomainAllowed(domain)\n }\n \n // domainFromID returns everything after the first \":\" character to extract\n // the domain part of a matrix ID.\n func domainFromID(id string) (string, error) {\n-\t// IDs have the format: SIGIL LOCALPART \":\" DOMAIN\n-\t// Split on the first \":\" character since the domain can contain \":\"\n-\t// characters.\n-\tparts := strings.SplitN(id, \":\", 2)\n-\tif len(parts) != 2 {\n-\t\t// The ID must have a \":\" character.\n-\t\treturn \"\", errorf(\"invalid ID: %q\", id)\n-\t}\n-\t// Return everything after the first \":\" character.\n-\treturn parts[1], nil\n+        // IDs have the format: SIGIL LOCALPART \":\" DOMAIN\n+        // Split on the first \":\" character since the domain can contain \":\"\n+        // characters.\n+        parts := strings.SplitN(id, \":\", 2)\n+        if len(parts) != 2 {\n+                // The ID must have a \":\" character.\n+                return \"\", errorf(\"invalid ID: %q\", id)\n+        }\n+        // Return everything after the first \":\" character.\n+        return parts[1], nil\n }\n \n // MemberContent is the JSON content of a m.room.member event needed for auth checks.\n // See https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-member for descriptions of the fields.\n type MemberContent struct {\n-\t// We use the membership key in order to check if the user is in the room.\n-\tMembership  string `json:\"membership\"`\n-\tDisplayName string `json:\"displayname,omitempty\"`\n-\tAvatarURL   string `json:\"avatar_url,omitempty\"`\n-\tReason      string `json:\"reason,omitempty\"`\n-\tIsDirect    bool   `json:\"is_direct,omitempty\"`\n-\t// We use the third_party_invite key to special case thirdparty invites.\n-\tThirdPartyInvite *MemberThirdPartyInvite `json:\"third_party_invite,omitempty\"`\n-\t// Restricted join rules require a user with invite permission to be nominated,\n-\t// so that their membership can be included in the auth events.\n-\tAuthorisedVia string `json:\"join_authorised_via_users_server,omitempty\"`\n+        // We use the membership key in order to check if the user is in the room.\n+        Membership  string `json:\"membership\"`\n+        DisplayName string `json:\"displayname,omitempty\"`\n+        AvatarURL   string `json:\"avatar_url,omitempty\"`\n+        Reason      string `json:\"reason,omitempty\"`\n+        IsDirect    bool   `json:\"is_direct,omitempty\"`\n+        // We use the third_party_invite key to special case thirdparty invites.\n+        ThirdPartyInvite *MemberThirdPartyInvite `json:\"third_party_invite,omitempty\"`\n+        // Restricted join rules require a user with invite permission to be nominated,\n+        // so that their membership can be included in the auth events.\n+        AuthorisedVia string `json:\"join_authorised_via_users_server,omitempty\"`\n }\n \n // MemberThirdPartyInvite is the \"Invite\" structure defined at http://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-member\n type MemberThirdPartyInvite struct {\n-\tDisplayName string                       `json:\"display_name\"`\n-\tSigned      MemberThirdPartyInviteSigned `json:\"signed\"`\n+        DisplayName string                       `json:\"display_name\"`\n+        Signed      MemberThirdPartyInviteSigned `json:\"signed\"`\n }\n \n // MemberThirdPartyInviteSigned is the \"signed\" structure defined at http://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-member\n type MemberThirdPartyInviteSigned struct {\n-\tMXID       string                       `json:\"mxid\"`\n-\tSignatures map[string]map[string]string `json:\"signatures\"`\n-\tToken      string                       `json:\"token\"`\n+        MXID       string                       `json:\"mxid\"`\n+        Signatures map[string]map[string]string `json:\"signatures\"`\n+        Token      string                       `json:\"token\"`\n }\n \n // NewMemberContentFromAuthEvents loads the member content from the member event for the user ID in the auth events.\n // Returns an error if there was an error loading the member event or parsing the event content.\n func NewMemberContentFromAuthEvents(authEvents AuthEventProvider, userID string) (c MemberContent, err error) {\n-\tvar memberEvent *Event\n-\tif memberEvent, err = authEvents.Member(userID); err != nil {\n-\t\treturn\n-\t}\n-\tif memberEvent == nil {\n-\t\t// If there isn't a member event then the membership for the user\n-\t\t// defaults to leave.\n-\t\tc.Membership = Leave\n-\t\treturn\n-\t}\n-\treturn NewMemberContentFromEvent(memberEvent)\n+        var memberEvent *Event\n+        if memberEvent, err = authEvents.Member(userID); err != nil {\n+                return\n+        }\n+        if memberEvent == nil {\n+                // If there isn't a member event then the membership for the user\n+                // defaults to leave.\n+                c.Membership = Leave\n+                return\n+        }\n+        return NewMemberContentFromEvent(memberEvent)\n }\n \n // NewMemberContentFromEvent parse the member content from an event.\n // Returns an error if the content couldn't be parsed.\n func NewMemberContentFromEvent(event *Event) (c MemberContent, err error) {\n-\tif err = json.Unmarshal(event.Content(), &c); err != nil {\n-\t\tvar partial membershipContent\n-\t\tif err = json.Unmarshal(event.Content(), &partial); err != nil {\n-\t\t\terr = errorf(\"unparsable member event content: %s\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\t\tc.Membership = partial.Membership\n-\t\tc.ThirdPartyInvite = partial.ThirdPartyInvite\n-\t}\n-\treturn\n+        if err = json.Unmarshal(event.Content(), &c); err != nil {\n+                var partial membershipContent\n+                if err = json.Unmarshal(event.Content(), &partial); err != nil {\n+                        err = errorf(\"unparsable member event content: %s\", err.Error())\n+                        return\n+                }\n+                c.Membership = partial.Membership\n+                c.ThirdPartyInvite = partial.ThirdPartyInvite\n+        }\n+        return\n }\n \n // ThirdPartyInviteContent is the JSON content of a m.room.third_party_invite event needed for auth checks.\n // See https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-third-party-invite for descriptions of the fields.\n type ThirdPartyInviteContent struct {\n-\tDisplayName    string `json:\"display_name\"`\n-\tKeyValidityURL string `json:\"key_validity_url\"`\n-\tPublicKey      string `json:\"public_key\"`\n-\t// Public keys are used to verify the signature of a m.room.member event that\n-\t// came from a m.room.third_party_invite event\n-\tPublicKeys []PublicKey `json:\"public_keys\"`\n+        DisplayName    string `json:\"display_name\"`\n+        KeyValidityURL string `json:\"key_validity_url\"`\n+        PublicKey      string `json:\"public_key\"`\n+        // Public keys are used to verify the signature of a m.room.member event that\n+        // came from a m.room.third_party_invite event\n+        PublicKeys []PublicKey `json:\"public_keys\"`\n }\n \n // PublicKey is the \"PublicKeys\" structure defined at https://matrix.org/docs/spec/client_server/r0.5.0#m-room-third-party-invite\n type PublicKey struct {\n-\tPublicKey      Base64Bytes `json:\"public_key\"`\n-\tKeyValidityURL string      `json:\"key_validity_url\"`\n+        PublicKey      Base64Bytes `json:\"public_key\"`\n+        KeyValidityURL string      `json:\"key_validity_url\"`\n }\n \n // NewThirdPartyInviteContentFromAuthEvents loads the third party invite content from the third party invite event for the state key (token) in the auth events.\n // Returns an error if there was an error loading the third party invite event or parsing the event content.\n func NewThirdPartyInviteContentFromAuthEvents(authEvents AuthEventProvider, token string) (t ThirdPartyInviteContent, err error) {\n-\tvar thirdPartyInviteEvent *Event\n-\tif thirdPartyInviteEvent, err = authEvents.ThirdPartyInvite(token); err != nil {\n-\t\treturn\n-\t}\n-\tif thirdPartyInviteEvent == nil {\n-\t\t// If there isn't a third_party_invite event, then we return with an error\n-\t\terr = errorf(\"Couldn't find third party invite event\")\n-\t\treturn\n-\t}\n-\tif err = json.Unmarshal(thirdPartyInviteEvent.Content(), &t); err != nil {\n-\t\terr = errorf(\"unparsable third party invite event content: %s\", err.Error())\n-\t}\n-\treturn\n+        var thirdPartyInviteEvent *Event\n+        if thirdPartyInviteEvent, err = authEvents.ThirdPartyInvite(token); err != nil {\n+                return\n+        }\n+        if thirdPartyInviteEvent == nil {\n+                // If there isn't a third_party_invite event, then we return with an error\n+                err = errorf(\"Couldn't find third party invite event\")\n+                return\n+        }\n+        if err = json.Unmarshal(thirdPartyInviteEvent.Content(), &t); err != nil {\n+                err = errorf(\"unparsable third party invite event content: %s\", err.Error())\n+        }\n+        return\n }\n \n // HistoryVisibilityContent is the JSON content of a m.room.history_visibility event.\n // See https://matrix.org/docs/spec/client_server/r0.6.0#room-history-visibility for descriptions of the fields.\n type HistoryVisibilityContent struct {\n-\tHistoryVisibility HistoryVisibility `json:\"history_visibility\"`\n+        HistoryVisibility HistoryVisibility `json:\"history_visibility\"`\n }\n \n type HistoryVisibility string\n \n const (\n-\tHistoryVisibilityWorldReadable HistoryVisibility = \"world_readable\"\n-\tHistoryVisibilityShared        HistoryVisibility = \"shared\"\n-\tHistoryVisibilityInvited       HistoryVisibility = \"invited\"\n-\tHistoryVisibilityJoined        HistoryVisibility = \"joined\"\n+        HistoryVisibilityWorldReadable HistoryVisibility = \"world_readable\"\n+        HistoryVisibilityShared        HistoryVisibility = \"shared\"\n+        HistoryVisibilityInvited       HistoryVisibility = \"invited\"\n+        HistoryVisibilityJoined        HistoryVisibility = \"joined\"\n )\n \n // Scan implements sql.Scanner\n func (h *HistoryVisibility) Scan(src interface{}) error {\n-\tswitch v := src.(type) {\n-\tcase int64:\n-\t\ts, ok := hisVisIntToStringMapping[uint8(v)]\n-\t\tif !ok { // history visibility is unknown, default to shared\n-\t\t\t*h = HistoryVisibilityShared\n-\t\t\treturn nil\n-\t\t}\n-\t\t*h = s\n-\t\treturn nil\n-\tcase float64:\n-\t\ts, ok := hisVisIntToStringMapping[uint8(v)]\n-\t\tif !ok { // history visibility is unknown, default to shared\n-\t\t\t*h = HistoryVisibilityShared\n-\t\t\treturn nil\n-\t\t}\n-\t\t*h = s\n-\t\treturn nil\n-\tdefault:\n-\t\treturn fmt.Errorf(\"unknown source type: %T for HistoryVisibilty\", src)\n-\t}\n+        switch v := src.(type) {\n+        case int64:\n+                s, ok := hisVisIntToStringMapping[uint8(v)]\n+                if !ok { // history visibility is unknown, default to shared\n+                        *h = HistoryVisibilityShared\n+                        return nil\n+                }\n+                *h = s\n+                return nil\n+        case float64:\n+                s, ok := hisVisIntToStringMapping[uint8(v)]\n+                if !ok { // history visibility is unknown, default to shared\n+                        *h = HistoryVisibilityShared\n+                        return nil\n+                }\n+                *h = s\n+                return nil\n+        default:\n+                return fmt.Errorf(\"unknown source type: %T for HistoryVisibilty\", src)\n+        }\n }\n \n // Value implements sql.Valuer\n func (h HistoryVisibility) Value() (driver.Value, error) {\n-\tv, ok := hisVisStringToIntMapping[h]\n-\tif !ok {\n-\t\treturn int64(hisVisStringToIntMapping[HistoryVisibilityShared]), nil\n-\t}\n-\treturn int64(v), nil\n+        v, ok := hisVisStringToIntMapping[h]\n+        if !ok {\n+                return int64(hisVisStringToIntMapping[HistoryVisibilityShared]), nil\n+        }\n+        return int64(v), nil\n }\n \n var hisVisStringToIntMapping = map[HistoryVisibility]uint8{\n-\tHistoryVisibilityWorldReadable: 1, // Starting at 1, to avoid confusions with Go default values\n-\tHistoryVisibilityShared:        2,\n-\tHistoryVisibilityInvited:       3,\n-\tHistoryVisibilityJoined:        4,\n+        HistoryVisibilityWorldReadable: 1, // Starting at 1, to avoid confusions with Go default values\n+        HistoryVisibilityShared:        2,\n+        HistoryVisibilityInvited:       3,\n+        HistoryVisibilityJoined:        4,\n }\n \n var hisVisIntToStringMapping = map[uint8]HistoryVisibility{\n-\t1: HistoryVisibilityWorldReadable, // Starting at 1, to avoid confusions with Go default values\n-\t2: HistoryVisibilityShared,\n-\t3: HistoryVisibilityInvited,\n-\t4: HistoryVisibilityJoined,\n+        1: HistoryVisibilityWorldReadable, // Starting at 1, to avoid confusions with Go default values\n+        2: HistoryVisibilityShared,\n+        3: HistoryVisibilityInvited,\n+        4: HistoryVisibilityJoined,\n }\n \n // JoinRuleContent is the JSON content of a m.room.join_rules event needed for auth checks.\n // See  https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-join-rules for descriptions of the fields.\n type JoinRuleContent struct {\n-\t// We use the join_rule key to check whether join m.room.member events are allowed.\n-\tJoinRule string                     `json:\"join_rule\"`\n-\tAllow    []JoinRuleContentAllowRule `json:\"allow,omitempty\"`\n+        // We use the join_rule key to check whether join m.room.member events are allowed.\n+        JoinRule string                     `json:\"join_rule\"`\n+        Allow    []JoinRuleContentAllowRule `json:\"allow,omitempty\"`\n }\n \n type JoinRuleContentAllowRule struct {\n-\tType   string `json:\"type\"`\n-\tRoomID string `json:\"room_id\"`\n+        Type   string `json:\"type\"`\n+        RoomID string `json:\"room_id\"`\n }\n \n // NewJoinRuleContentFromAuthEvents loads the join rule content from the join rules event in the auth event.\n // Returns an error if there was an error loading the join rule event or parsing the content.\n func NewJoinRuleContentFromAuthEvents(authEvents AuthEventProvider) (c JoinRuleContent, err error) {\n-\t// Start off with \"invite\" as the default. Hopefully the unmarshal\n-\t// step later will replace it with a better value.\n-\tc.JoinRule = Invite\n-\t// Then see if the specified join event contains something better.\n-\tjoinRulesEvent, err := authEvents.JoinRules()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tif joinRulesEvent == nil {\n-\t\treturn\n-\t}\n-\tif err = json.Unmarshal(joinRulesEvent.Content(), &c); err != nil {\n-\t\terr = errorf(\"unparsable join_rules event content: %s\", err.Error())\n-\t\treturn\n-\t}\n-\treturn\n+        // Start off with \"invite\" as the default. Hopefully the unmarshal\n+        // step later will replace it with a better value.\n+        c.JoinRule = Invite\n+        // Then see if the specified join event contains something better.\n+        joinRulesEvent, err := authEvents.JoinRules()\n+        if err != nil {\n+                return\n+        }\n+        if joinRulesEvent == nil {\n+                return\n+        }\n+        if err = json.Unmarshal(joinRulesEvent.Content(), &c); err != nil {\n+                err = errorf(\"unparsable join_rules event content: %s\", err.Error())\n+                return\n+        }\n+        return\n }\n \n // PowerLevelContent is the JSON content of a m.room.power_levels event needed for auth checks.\n@@ -312,213 +312,213 @@ func NewJoinRuleContentFromAuthEvents(authEvents AuthEventProvider) (c JoinRuleC\n // the struct into JSON easily.\n // See https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-power-levels for descriptions of the fields.\n type PowerLevelContent struct {\n-\tBan           int64            `json:\"ban\"`\n-\tInvite        int64            `json:\"invite\"`\n-\tKick          int64            `json:\"kick\"`\n-\tRedact        int64            `json:\"redact\"`\n-\tUsers         map[string]int64 `json:\"users\"`\n-\tUsersDefault  int64            `json:\"users_default\"`\n-\tEvents        map[string]int64 `json:\"events\"`\n-\tEventsDefault int64            `json:\"events_default\"`\n-\tStateDefault  int64            `json:\"state_default\"`\n-\tNotifications map[string]int64 `json:\"notifications\"`\n+        Ban           int64            `json:\"ban\"`\n+        Invite        int64            `json:\"invite\"`\n+        Kick          int64            `json:\"kick\"`\n+        Redact        int64            `json:\"redact\"`\n+        Users         map[string]int64 `json:\"users\"`\n+        UsersDefault  int64            `json:\"users_default\"`\n+        Events        map[string]int64 `json:\"events\"`\n+        EventsDefault int64            `json:\"events_default\"`\n+        StateDefault  int64            `json:\"state_default\"`\n+        Notifications map[string]int64 `json:\"notifications\"`\n }\n \n // UserLevel returns the power level a user has in the room.\n func (c *PowerLevelContent) UserLevel(userID string) int64 {\n-\tlevel, ok := c.Users[userID]\n-\tif ok {\n-\t\treturn level\n-\t}\n-\treturn c.UsersDefault\n+        level, ok := c.Users[userID]\n+        if ok {\n+                return level\n+        }\n+        return c.UsersDefault\n }\n \n // EventLevel returns the power level needed to send an event in the room.\n func (c *PowerLevelContent) EventLevel(eventType string, isState bool) int64 {\n-\tif eventType == MRoomThirdPartyInvite {\n-\t\t// Special case third_party_invite events to have the same level as\n-\t\t// m.room.member invite events.\n-\t\t// https://github.com/matrix-org/synapse/blob/v0.18.5/synapse/api/auth.py#L182\n-\t\treturn c.Invite\n-\t}\n-\tlevel, ok := c.Events[eventType]\n-\tif ok {\n-\t\treturn level\n-\t}\n-\tif isState {\n-\t\treturn c.StateDefault\n-\t}\n-\treturn c.EventsDefault\n+        if eventType == MRoomThirdPartyInvite {\n+                // Special case third_party_invite events to have the same level as\n+                // m.room.member invite events.\n+                // https://github.com/matrix-org/synapse/blob/v0.18.5/synapse/api/auth.py#L182\n+                return c.Invite\n+        }\n+        level, ok := c.Events[eventType]\n+        if ok {\n+                return level\n+        }\n+        if isState {\n+                return c.StateDefault\n+        }\n+        return c.EventsDefault\n }\n \n // UserLevel returns the power level a user has in the room.\n func (c *PowerLevelContent) NotificationLevel(notification string) int64 {\n-\tlevel, ok := c.Notifications[notification]\n-\tif ok {\n-\t\treturn level\n-\t}\n-\t// https://matrix.org/docs/spec/client_server/r0.6.1#m-room-power-levels\n-\t// room\tinteger\tThe level required to trigger an @room notification. Defaults to 50 if unspecified.\n-\treturn 50\n+        level, ok := c.Notifications[notification]\n+        if ok {\n+                return level\n+        }\n+        // https://matrix.org/docs/spec/client_server/r0.6.1#m-room-power-levels\n+        // room integer The level required to trigger an @room notification. Defaults to 50 if unspecified.\n+        return 50\n }\n \n // NewPowerLevelContentFromAuthEvents loads the power level content from the\n // power level event in the auth events or returns the default values if there\n // is no power level event.\n func NewPowerLevelContentFromAuthEvents(authEvents AuthEventProvider, creatorUserID string) (c PowerLevelContent, err error) {\n-\tpowerLevelsEvent, err := authEvents.PowerLevels()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tif powerLevelsEvent != nil {\n-\t\treturn NewPowerLevelContentFromEvent(powerLevelsEvent)\n-\t}\n-\n-\t// If there are no power levels then fall back to defaults.\n-\tc.Defaults()\n-\t// If there is no power level event then the creator gets level 100\n-\t// https://github.com/matrix-org/synapse/blob/v0.18.5/synapse/api/auth.py#L569\n-\t// If we want users to be able to set PLs > 100 with power_level_content_override\n-\t// then we need to set the upper bound: maximum allowable JSON value is (2^53)-1.\n-\tc.Users = map[string]int64{creatorUserID: 9007199254740991}\n-\t// If there is no power level event then the state_default is level 50\n-\t// https://github.com/matrix-org/synapse/blob/v1.38.0/synapse/event_auth.py#L437\n-\t// Previously it was 0, but this was changed in:\n-\t// https://github.com/matrix-org/synapse/commit/5c9afd6f80cf04367fe9b02c396af9f85e02a611\n-\tc.StateDefault = 50\n-\treturn\n+        powerLevelsEvent, err := authEvents.PowerLevels()\n+        if err != nil {\n+                return\n+        }\n+        if powerLevelsEvent != nil {\n+                return NewPowerLevelContentFromEvent(powerLevelsEvent)\n+        }\n+\n+        // If there are no power levels then fall back to defaults.\n+        c.Defaults()\n+        // If there is no power level event then the creator gets level 100\n+        // https://github.com/matrix-org/synapse/blob/v0.18.5/synapse/api/auth.py#L569\n+        // If we want users to be able to set PLs > 100 with power_level_content_override\n+        // then we need to set the upper bound: maximum allowable JSON value is (2^53)-1.\n+        c.Users = map[string]int64{creatorUserID: 9007199254740991}\n+        // If there is no power level event then the state_default is level 50\n+        // https://github.com/matrix-org/synapse/blob/v1.38.0/synapse/event_auth.py#L437\n+        // Previously it was 0, but this was changed in:\n+        // https://github.com/matrix-org/synapse/commit/5c9afd6f80cf04367fe9b02c396af9f85e02a611\n+        c.StateDefault = 50\n+        return\n }\n \n // Defaults sets the power levels to their default values.\n // See https://spec.matrix.org/v1.1/client-server-api/#mroompower_levels for defaults.\n func (c *PowerLevelContent) Defaults() {\n-\tc.Invite = 50\n-\tc.Ban = 50\n-\tc.Kick = 50\n-\tc.Redact = 50\n-\tc.UsersDefault = 0\n-\tc.EventsDefault = 0\n-\tc.StateDefault = 50\n-\tc.Notifications = map[string]int64{\n-\t\t\"room\": 50,\n-\t}\n+        c.Invite = 50\n+        c.Ban = 50\n+        c.Kick = 50\n+        c.Redact = 50\n+        c.UsersDefault = 0\n+        c.EventsDefault = 0\n+        c.StateDefault = 50\n+        c.Notifications = map[string]int64{\n+                \"room\": 50,\n+        }\n }\n \n // NewPowerLevelContentFromEvent loads the power level content from an event.\n func NewPowerLevelContentFromEvent(event *Event) (c PowerLevelContent, err error) {\n-\t// Set the levels to their default values.\n-\tc.Defaults()\n-\n-\tvar strict bool\n-\tif strict, err = event.roomVersion.RequireIntegerPowerLevels(); err != nil {\n-\t\treturn\n-\t} else if strict {\n-\t\t// Unmarshal directly to PowerLevelContent, since that will kick up an\n-\t\t// error if one of the power levels isn't an int64.\n-\t\tif err = json.Unmarshal(event.Content(), &c); err != nil {\n-\t\t\terr = errorf(\"unparsable power_levels event content: %s\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\t} else {\n-\t\t// We can't extract the JSON directly to the powerLevelContent because we\n-\t\t// need to convert string values to int values.\n-\t\tvar content struct {\n-\t\t\tInviteLevel        levelJSONValue            `json:\"invite\"`\n-\t\t\tBanLevel           levelJSONValue            `json:\"ban\"`\n-\t\t\tKickLevel          levelJSONValue            `json:\"kick\"`\n-\t\t\tRedactLevel        levelJSONValue            `json:\"redact\"`\n-\t\t\tUserLevels         map[string]levelJSONValue `json:\"users\"`\n-\t\t\tUsersDefaultLevel  levelJSONValue            `json:\"users_default\"`\n-\t\t\tEventLevels        map[string]levelJSONValue `json:\"events\"`\n-\t\t\tStateDefaultLevel  levelJSONValue            `json:\"state_default\"`\n-\t\t\tEventDefaultLevel  levelJSONValue            `json:\"event_default\"`\n-\t\t\tNotificationLevels map[string]levelJSONValue `json:\"notifications\"`\n-\t\t}\n-\t\tif err = json.Unmarshal(event.Content(), &content); err != nil {\n-\t\t\terr = errorf(\"unparsable power_levels event content: %s\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\n-\t\t// Update the levels with the values that are present in the event content.\n-\t\tcontent.InviteLevel.assignIfExists(&c.Invite)\n-\t\tcontent.BanLevel.assignIfExists(&c.Ban)\n-\t\tcontent.KickLevel.assignIfExists(&c.Kick)\n-\t\tcontent.RedactLevel.assignIfExists(&c.Redact)\n-\t\tcontent.UsersDefaultLevel.assignIfExists(&c.UsersDefault)\n-\t\tcontent.StateDefaultLevel.assignIfExists(&c.StateDefault)\n-\t\tcontent.EventDefaultLevel.assignIfExists(&c.EventsDefault)\n-\n-\t\tfor k, v := range content.UserLevels {\n-\t\t\tif c.Users == nil {\n-\t\t\t\tc.Users = make(map[string]int64)\n-\t\t\t}\n-\t\t\tc.Users[k] = v.value\n-\t\t}\n-\n-\t\tfor k, v := range content.EventLevels {\n-\t\t\tif c.Events == nil {\n-\t\t\t\tc.Events = make(map[string]int64)\n-\t\t\t}\n-\t\t\tc.Events[k] = v.value\n-\t\t}\n-\n-\t\tfor k, v := range content.NotificationLevels {\n-\t\t\tif c.Notifications == nil {\n-\t\t\t\tc.Notifications = make(map[string]int64)\n-\t\t\t}\n-\t\t\tc.Notifications[k] = v.value\n-\t\t}\n-\t}\n-\n-\treturn\n+        // Set the levels to their default values.\n+        c.Defaults()\n+\n+        var strict bool\n+        if strict, err = event.roomVersion.RequireIntegerPowerLevels(); err != nil {\n+                return\n+        } else if strict {\n+                // Unmarshal directly to PowerLevelContent, since that will kick up an\n+                // error if one of the power levels isn't an int64.\n+                if err = json.Unmarshal(event.Content(), &c); err != nil {\n+                        err = errorf(\"unparsable power_levels event content: %s\", err.Error())\n+                        return\n+                }\n+        } else {\n+                // We can't extract the JSON directly to the powerLevelContent because we\n+                // need to convert string values to int values.\n+                var content struct {\n+                        InviteLevel        levelJSONValue            `json:\"invite\"`\n+                        BanLevel           levelJSONValue            `json:\"ban\"`\n+                        KickLevel          levelJSONValue            `json:\"kick\"`\n+                        RedactLevel        levelJSONValue            `json:\"redact\"`\n+                        UserLevels         map[string]levelJSONValue `json:\"users\"`\n+                        UsersDefaultLevel  levelJSONValue            `json:\"users_default\"`\n+                        EventLevels        map[string]levelJSONValue `json:\"events\"`\n+                        StateDefaultLevel  levelJSONValue            `json:\"state_default\"`\n+                        EventsDefaultLevel  levelJSONValue            `json:\"events_default\"`\n+                        NotificationLevels map[string]levelJSONValue `json:\"notifications\"`\n+                }\n+                if err = json.Unmarshal(event.Content(), &content); err != nil {\n+                        err = errorf(\"unparsable power_levels event content: %s\", err.Error())\n+                        return\n+                }\n+\n+                // Update the levels with the values that are present in the event content.\n+                content.InviteLevel.assignIfExists(&c.Invite)\n+                content.BanLevel.assignIfExists(&c.Ban)\n+                content.KickLevel.assignIfExists(&c.Kick)\n+                content.RedactLevel.assignIfExists(&c.Redact)\n+                content.UsersDefaultLevel.assignIfExists(&c.UsersDefault)\n+                content.StateDefaultLevel.assignIfExists(&c.StateDefault)\n+                content.EventsDefaultLevel.assignIfExists(&c.EventsDefault)\n+\n+                for k, v := range content.UserLevels {\n+                        if c.Users == nil {\n+                                c.Users = make(map[string]int64)\n+                        }\n+                        c.Users[k] = v.value\n+                }\n+\n+                for k, v := range content.EventLevels {\n+                        if c.Events == nil {\n+                                c.Events = make(map[string]int64)\n+                        }\n+                        c.Events[k] = v.value\n+                }\n+\n+                for k, v := range content.NotificationLevels {\n+                        if c.Notifications == nil {\n+                                c.Notifications = make(map[string]int64)\n+                        }\n+                        c.Notifications[k] = v.value\n+                }\n+        }\n+\n+        return\n }\n \n // A levelJSONValue is used for unmarshalling power levels from JSON.\n // It is intended to replicate the effects of x = int(content[\"key\"]) in python.\n type levelJSONValue struct {\n-\t// Was a value loaded from the JSON?\n-\texists bool\n-\t// The integer value of the power level.\n-\tvalue int64\n+        // Was a value loaded from the JSON?\n+        exists bool\n+        // The integer value of the power level.\n+        value int64\n }\n \n func (v *levelJSONValue) UnmarshalJSON(data []byte) error {\n-\tvar stringValue string\n-\tvar int64Value int64\n-\tvar floatValue float64\n-\tvar err error\n-\n-\t// First try to unmarshal as an int64.\n-\tif int64Value, err = strconv.ParseInt(string(data), 10, 64); err != nil {\n-\t\t// If unmarshalling as an int64 fails try as a string.\n-\t\tif err = json.Unmarshal(data, &stringValue); err != nil {\n-\t\t\t// If unmarshalling as a string fails try as a float.\n-\t\t\tif floatValue, err = strconv.ParseFloat(string(data), 64); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tint64Value = int64(floatValue)\n-\t\t} else {\n-\t\t\t// If we managed to get a string, try parsing the string as an int.\n-\t\t\tint64Value, err = strconv.ParseInt(strings.TrimSpace(stringValue), 10, 64)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t}\n-\t}\n-\tv.exists = true\n-\tv.value = int64Value\n-\treturn nil\n+        var stringValue string\n+        var int64Value int64\n+        var floatValue float64\n+        var err error\n+\n+        // First try to unmarshal as an int64.\n+        if int64Value, err = strconv.ParseInt(string(data), 10, 64); err != nil {\n+                // If unmarshalling as an int64 fails try as a string.\n+                if err = json.Unmarshal(data, &stringValue); err != nil {\n+                        // If unmarshalling as a string fails try as a float.\n+                        if floatValue, err = strconv.ParseFloat(string(data), 64); err != nil {\n+                                return err\n+                        }\n+                        int64Value = int64(floatValue)\n+                } else {\n+                        // If we managed to get a string, try parsing the string as an int.\n+                        int64Value, err = strconv.ParseInt(strings.TrimSpace(stringValue), 10, 64)\n+                        if err != nil {\n+                                return err\n+                        }\n+                }\n+        }\n+        v.exists = true\n+        v.value = int64Value\n+        return nil\n }\n \n // assign the power level if a value was present in the JSON.\n func (v *levelJSONValue) assignIfExists(to *int64) {\n-\tif v.exists {\n-\t\t*to = v.value\n-\t}\n+        if v.exists {\n+                *to = v.value\n+        }\n }\n \n // Check if the user ID is a valid user ID.\n func isValidUserID(userID string) bool {\n-\t// TODO: Do we want to add anymore checks beyond checking the sigil and that it has a domain part?\n-\treturn userID[0] == '@' && strings.IndexByte(userID, ':') != -1\n+        // TODO: Do we want to add anymore checks beyond checking the sigil and that it has a domain part?\n+        return userID[0] == '@' && strings.IndexByte(userID, ':') != -1\n }\n"}
{"cve":"CVE-2021-32701:0708", "fix_patch": "diff --git a/go.sum b/go.sum\nindex 3c38aa5..69fb8e1 100644\n--- a/go.sum\n+++ b/go.sum\n@@ -737,6 +737,7 @@ github.com/hashicorp/go-uuid v1.0.0/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/b\n github.com/hashicorp/go-uuid v1.0.1/go.mod h1:6SBZvOh/SIDV7/2o3Jml5SYk/TvGqwFJ/bN7x4byOro=\n github.com/hashicorp/go.net v0.0.1/go.mod h1:hjKkEWcCURg++eb33jQU7oqQcI9XDCnUzHA0oac0k90=\n github.com/hashicorp/golang-lru v0.5.0/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n+github.com/hashicorp/golang-lru v0.5.1 h1:0hERBMJE1eitiLkihrMvRVBYAkpHzc/J3QdDN+dAcgU=\n github.com/hashicorp/golang-lru v0.5.1/go.mod h1:/m3WP610KZHVQ1SGc6re/UDhFvYD7pJ4Ao+sR/qLZy8=\n github.com/hashicorp/hcl v1.0.0 h1:0Anlzjpi4vEasTeNFn2mLJgTSwt0+6sfsiTG8qcWGx4=\n github.com/hashicorp/hcl v1.0.0/go.mod h1:E5yfLk+7swimpb2L/Alb/PJmXilQ/rhwaUYs4T20WEQ=\ndiff --git a/pipeline/authn/authenticator_oauth2_introspection.go b/pipeline/authn/authenticator_oauth2_introspection.go\nindex 285cbd9..a453f7d 100644\n--- a/pipeline/authn/authenticator_oauth2_introspection.go\n+++ b/pipeline/authn/authenticator_oauth2_introspection.go\n@@ -1,341 +1,362 @@\n package authn\n \n import (\n-\t\"context\"\n-\t\"crypto/md5\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\t\"github.com/dgraph-io/ristretto\"\n-\n-\t\"github.com/opentracing/opentracing-go\"\n-\t\"github.com/opentracing/opentracing-go/ext\"\n-\n-\t\"github.com/pkg/errors\"\n-\t\"golang.org/x/oauth2/clientcredentials\"\n-\n-\t\"github.com/ory/go-convenience/stringslice\"\n-\t\"github.com/ory/x/httpx\"\n-\t\"github.com/ory/x/logrusx\"\n-\n-\t\"github.com/ory/oathkeeper/driver/configuration\"\n-\t\"github.com/ory/oathkeeper/helper\"\n-\t\"github.com/ory/oathkeeper/pipeline\"\n+        \"context\"\n+        \"crypto/md5\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        \"github.com/dgraph-io/ristretto\"\n+\n+        \"github.com/opentracing/opentracing-go\"\n+        \"github.com/opentracing/opentracing-go/ext\"\n+\n+        \"github.com/pkg/errors\"\n+        \"golang.org/x/oauth2/clientcredentials\"\n+\n+        \"github.com/ory/go-convenience/stringslice\"\n+        \"github.com/ory/x/httpx\"\n+        \"github.com/ory/x/logrusx\"\n+\n+        \"github.com/ory/oathkeeper/driver/configuration\"\n+        \"github.com/ory/oathkeeper/helper\"\n+        \"github.com/ory/oathkeeper/pipeline\"\n )\n \n type AuthenticatorOAuth2IntrospectionConfiguration struct {\n-\tScopes                      []string                                              `json:\"required_scope\"`\n-\tAudience                    []string                                              `json:\"target_audience\"`\n-\tIssuers                     []string                                              `json:\"trusted_issuers\"`\n-\tPreAuth                     *AuthenticatorOAuth2IntrospectionPreAuthConfiguration `json:\"pre_authorization\"`\n-\tScopeStrategy               string                                                `json:\"scope_strategy\"`\n-\tIntrospectionURL            string                                                `json:\"introspection_url\"`\n-\tBearerTokenLocation         *helper.BearerTokenLocation                           `json:\"token_from\"`\n-\tIntrospectionRequestHeaders map[string]string                                     `json:\"introspection_request_headers\"`\n-\tRetry                       *AuthenticatorOAuth2IntrospectionRetryConfiguration   `json:\"retry\"`\n-\tCache                       cacheConfig                                           `json:\"cache\"`\n+        Scopes                      []string                                              `json:\"required_scope\"`\n+        Audience                    []string                                              `json:\"target_audience\"`\n+        Issuers                     []string                                              `json:\"trusted_issuers\"`\n+        PreAuth                     *AuthenticatorOAuth2IntrospectionPreAuthConfiguration `json:\"pre_authorization\"`\n+        ScopeStrategy               string                                                `json:\"scope_strategy\"`\n+        IntrospectionURL            string                                                `json:\"introspection_url\"`\n+        BearerTokenLocation         *helper.BearerTokenLocation                           `json:\"token_from\"`\n+        IntrospectionRequestHeaders map[string]string                                     `json:\"introspection_request_headers\"`\n+        Retry                       *AuthenticatorOAuth2IntrospectionRetryConfiguration   `json:\"retry\"`\n+        Cache                       cacheConfig                                           `json:\"cache\"`\n }\n \n type AuthenticatorOAuth2IntrospectionPreAuthConfiguration struct {\n-\tEnabled      bool     `json:\"enabled\"`\n-\tClientID     string   `json:\"client_id\"`\n-\tClientSecret string   `json:\"client_secret\"`\n-\tAudience     string   `json:\"audience\"`\n-\tScope        []string `json:\"scope\"`\n-\tTokenURL     string   `json:\"token_url\"`\n+        Enabled      bool     `json:\"enabled\"`\n+        ClientID     string   `json:\"client_id\"`\n+        ClientSecret string   `json:\"client_secret\"`\n+        Audience     string   `json:\"audience\"`\n+        Scope        []string `json:\"scope\"`\n+        TokenURL     string   `json:\"token_url\"`\n }\n \n type AuthenticatorOAuth2IntrospectionRetryConfiguration struct {\n-\tTimeout string `json:\"max_delay\"`\n-\tMaxWait string `json:\"give_up_after\"`\n+        Timeout string `json:\"max_delay\"`\n+        MaxWait string `json:\"give_up_after\"`\n }\n \n type cacheConfig struct {\n-\tEnabled bool   `json:\"enabled\"`\n-\tTTL     string `json:\"ttl\"`\n-\tMaxCost int    `json:\"max_cost\"`\n+        Enabled bool   `json:\"enabled\"`\n+        TTL     string `json:\"ttl\"`\n+        MaxCost int    `json:\"max_cost\"`\n }\n \n type AuthenticatorOAuth2Introspection struct {\n-\tc configuration.Provider\n+        c configuration.Provider\n \n-\tclientMap map[string]*http.Client\n-\tmu        sync.RWMutex\n+        clientMap map[string]*http.Client\n+        mu        sync.RWMutex\n \n-\ttokenCache *ristretto.Cache\n-\tcacheTTL   *time.Duration\n-\tlogger     *logrusx.Logger\n+        tokenCache *ristretto.Cache\n+        cacheTTL   *time.Duration\n+        logger     *logrusx.Logger\n }\n \n func NewAuthenticatorOAuth2Introspection(c configuration.Provider, logger *logrusx.Logger) *AuthenticatorOAuth2Introspection {\n-\treturn &AuthenticatorOAuth2Introspection{c: c, logger: logger, clientMap: make(map[string]*http.Client)}\n+        return &AuthenticatorOAuth2Introspection{c: c, logger: logger, clientMap: make(map[string]*http.Client)}\n }\n \n func (a *AuthenticatorOAuth2Introspection) GetID() string {\n-\treturn \"oauth2_introspection\"\n+        return \"oauth2_introspection\"\n }\n \n type AuthenticatorOAuth2IntrospectionResult struct {\n-\tActive    bool                   `json:\"active\"`\n-\tExtra     map[string]interface{} `json:\"ext\"`\n-\tSubject   string                 `json:\"sub,omitempty\"`\n-\tUsername  string                 `json:\"username\"`\n-\tAudience  []string               `json:\"aud\"`\n-\tTokenType string                 `json:\"token_type\"`\n-\tIssuer    string                 `json:\"iss\"`\n-\tClientID  string                 `json:\"client_id,omitempty\"`\n-\tScope     string                 `json:\"scope,omitempty\"`\n-\tExpires   int64                  `json:\"exp\"`\n-\tTokenUse  string                 `json:\"token_use\"`\n+        Active    bool                   `json:\"active\"`\n+        Extra     map[string]interface{} `json:\"ext\"`\n+        Subject   string                 `json:\"sub,omitempty\"`\n+        Username  string                 `json:\"username\"`\n+        Audience  []string               `json:\"aud\"`\n+        TokenType string                 `json:\"token_type\"`\n+        Issuer    string                 `json:\"iss\"`\n+        ClientID  string                 `json:\"client_id,omitempty\"`\n+        Scope     string                 `json:\"scope,omitempty\"`\n+        Expires   int64                  `json:\"exp\"`\n+        TokenUse  string                 `json:\"token_use\"`\n }\n \n+\n+func (a *AuthenticatorOAuth2Introspection) checkScope(scopeStrategy, tokenScope string, requiredScopes []string) error {\n+ss := a.c.ToScopeStrategy(scopeStrategy, \"authenticators.oauth2_introspection.scope_strategy\")\n+if ss == nil {\n+// if no strategy is set, we are not checking for scopes\n+return nil\n+}\n+\n+for _, scope := range requiredScopes {\n+if !ss(strings.Split(tokenScope, \" \"), scope) {\n+return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Scope %s was not granted\", scope)))\n+}\n+}\n+\n+return nil\n+}\n+\n+\n func (a *AuthenticatorOAuth2Introspection) tokenFromCache(config *AuthenticatorOAuth2IntrospectionConfiguration, token string) (*AuthenticatorOAuth2IntrospectionResult, bool) {\n-\tif !config.Cache.Enabled {\n-\t\treturn nil, false\n-\t}\n-\n-\titem, found := a.tokenCache.Get(token)\n-\tif !found {\n-\t\treturn nil, false\n-\t}\n-\n-\ti := item.(*AuthenticatorOAuth2IntrospectionResult)\n-\texpires := time.Unix(i.Expires, 0)\n-\tif expires.Before(time.Now()) {\n-\t\ta.tokenCache.Del(token)\n-\t\treturn nil, false\n-\t}\n-\n-\treturn i, true\n+        if !config.Cache.Enabled {\n+                return nil, false\n+        }\n+\n+        item, found := a.tokenCache.Get(token)\n+        if !found {\n+                return nil, false\n+        }\n+\n+        i := item.(*AuthenticatorOAuth2IntrospectionResult)\n+        expires := time.Unix(i.Expires, 0)\n+        if expires.Before(time.Now()) {\n+                a.tokenCache.Del(token)\n+                return nil, false\n+        }\n+if err := a.checkScope(config.ScopeStrategy, i.Scope, config.Scopes); err != nil {\n+return nil, false\n+}\n+\n+        return i, true\n }\n \n func (a *AuthenticatorOAuth2Introspection) tokenToCache(config *AuthenticatorOAuth2IntrospectionConfiguration, i *AuthenticatorOAuth2IntrospectionResult, token string) {\n-\tif !config.Cache.Enabled {\n-\t\treturn\n-\t}\n-\n-\tif a.cacheTTL != nil {\n-\t\ta.tokenCache.SetWithTTL(token, i, 1, *a.cacheTTL)\n-\t} else {\n-\t\ta.tokenCache.Set(token, i, 1)\n-\t}\n+        if !config.Cache.Enabled {\n+                return\n+        }\n+\n+        if a.cacheTTL != nil {\n+                a.tokenCache.SetWithTTL(token, i, 1, *a.cacheTTL)\n+        } else {\n+                a.tokenCache.Set(token, i, 1)\n+        }\n }\n \n func (a *AuthenticatorOAuth2Introspection) traceRequest(ctx context.Context, req *http.Request) func() {\n-\ttracer := opentracing.GlobalTracer()\n-\tif tracer == nil {\n-\t\treturn func() {}\n-\t}\n-\n-\tparentSpan := opentracing.SpanFromContext(ctx)\n-\topts := make([]opentracing.StartSpanOption, 0, 1)\n-\tif parentSpan != nil {\n-\t\topts = append(opts, opentracing.ChildOf(parentSpan.Context()))\n-\t}\n-\n-\turlStr := req.URL.String()\n-\tclientSpan := tracer.StartSpan(req.Method+\" \"+urlStr, opts...)\n-\n-\text.SpanKindRPCClient.Set(clientSpan)\n-\text.HTTPUrl.Set(clientSpan, urlStr)\n-\text.HTTPMethod.Set(clientSpan, req.Method)\n-\n-\ttracer.Inject(clientSpan.Context(), opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(req.Header))\n-\treturn clientSpan.Finish\n+        tracer := opentracing.GlobalTracer()\n+        if tracer == nil {\n+                return func() {}\n+        }\n+\n+        parentSpan := opentracing.SpanFromContext(ctx)\n+        opts := make([]opentracing.StartSpanOption, 0, 1)\n+        if parentSpan != nil {\n+                opts = append(opts, opentracing.ChildOf(parentSpan.Context()))\n+        }\n+\n+        urlStr := req.URL.String()\n+        clientSpan := tracer.StartSpan(req.Method+\" \"+urlStr, opts...)\n+\n+        ext.SpanKindRPCClient.Set(clientSpan)\n+        ext.HTTPUrl.Set(clientSpan, urlStr)\n+        ext.HTTPMethod.Set(clientSpan, req.Method)\n+\n+        tracer.Inject(clientSpan.Context(), opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(req.Header))\n+        return clientSpan.Finish\n }\n \n func (a *AuthenticatorOAuth2Introspection) Authenticate(r *http.Request, session *AuthenticationSession, config json.RawMessage, _ pipeline.Rule) error {\n-\tcf, client, err := a.Config(config)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\ttoken := helper.BearerTokenFromRequest(r, cf.BearerTokenLocation)\n-\tif token == \"\" {\n-\t\treturn errors.WithStack(ErrAuthenticatorNotResponsible)\n-\t}\n-\n-\tss := a.c.ToScopeStrategy(cf.ScopeStrategy, \"authenticators.oauth2_introspection.scope_strategy\")\n-\n-\ti, ok := a.tokenFromCache(cf, token)\n-\tif !ok {\n-\t\tbody := url.Values{\"token\": {token}}\n-\n-\t\tif ss == nil {\n-\t\t\tbody.Add(\"scope\", strings.Join(cf.Scopes, \" \"))\n-\t\t}\n-\n-\t\tintrospectReq, err := http.NewRequest(http.MethodPost, cf.IntrospectionURL, strings.NewReader(body.Encode()))\n-\t\tif err != nil {\n-\t\t\treturn errors.WithStack(err)\n-\t\t}\n-\t\tfor key, value := range cf.IntrospectionRequestHeaders {\n-\t\t\tintrospectReq.Header.Set(key, value)\n-\t\t}\n-\t\t// set/override the content-type header\n-\t\tintrospectReq.Header.Set(\"Content-Type\", \"application/x-www-form-urlencoded\")\n-\n-\t\t// add tracing\n-\t\tcloseSpan := a.traceRequest(r.Context(), introspectReq)\n-\n-\t\tresp, err := client.Do(introspectReq.WithContext(r.Context()))\n-\n-\t\t// close the span so it represents just the http request\n-\t\tcloseSpan()\n-\t\tif err != nil {\n-\t\t\treturn errors.WithStack(err)\n-\t\t}\n-\t\tdefer resp.Body.Close()\n-\n-\t\tif resp.StatusCode != http.StatusOK {\n-\t\t\treturn errors.Errorf(\"Introspection returned status code %d but expected %d\", resp.StatusCode, http.StatusOK)\n-\t\t}\n-\n-\t\tif err := json.NewDecoder(resp.Body).Decode(&i); err != nil {\n-\t\t\treturn errors.WithStack(err)\n-\t\t}\n-\n-\t\tif len(i.TokenUse) > 0 && i.TokenUse != \"access_token\" {\n-\t\t\treturn errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Use of introspected token is not an access token but \\\"%s\\\"\", i.TokenUse)))\n-\t\t}\n-\n-\t\tif !i.Active {\n-\t\t\treturn errors.WithStack(helper.ErrUnauthorized.WithReason(\"Access token i says token is not active\"))\n-\t\t}\n-\n-\t\tfor _, audience := range cf.Audience {\n-\t\t\tif !stringslice.Has(i.Audience, audience) {\n-\t\t\t\treturn errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Token audience is not intended for target audience %s\", audience)))\n-\t\t\t}\n-\t\t}\n-\n-\t\tif len(cf.Issuers) > 0 {\n-\t\t\tif !stringslice.Has(cf.Issuers, i.Issuer) {\n-\t\t\t\treturn errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Token issuer does not match any trusted issuer\")))\n-\t\t\t}\n-\t\t}\n-\n-\t\tif ss != nil {\n-\t\t\tfor _, scope := range cf.Scopes {\n-\t\t\t\tif !ss(strings.Split(i.Scope, \" \"), scope) {\n-\t\t\t\t\treturn errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Scope %s was not granted\", scope)))\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tif len(i.Extra) == 0 {\n-\t\t\ti.Extra = map[string]interface{}{}\n-\t\t}\n-\n-\t\ti.Extra[\"username\"] = i.Username\n-\t\ti.Extra[\"client_id\"] = i.ClientID\n-\t\ti.Extra[\"scope\"] = i.Scope\n-\n-\t\tif len(i.Audience) != 0 {\n-\t\t\ti.Extra[\"aud\"] = i.Audience\n-\t\t}\n-\n-\t\ta.tokenToCache(cf, i, token)\n-\t}\n-\n-\tsession.Subject = i.Subject\n-\tsession.Extra = i.Extra\n-\n-\treturn nil\n+        cf, client, err := a.Config(config)\n+        if err != nil {\n+                return err\n+        }\n+\n+        token := helper.BearerTokenFromRequest(r, cf.BearerTokenLocation)\n+        if token == \"\" {\n+                return errors.WithStack(ErrAuthenticatorNotResponsible)\n+        }\n+\n+        ss := a.c.ToScopeStrategy(cf.ScopeStrategy, \"authenticators.oauth2_introspection.scope_strategy\")\n+\n+        i, ok := a.tokenFromCache(cf, token)\n+        if !ok {\n+                body := url.Values{\"token\": {token}}\n+\n+                if ss == nil {\n+                        body.Add(\"scope\", strings.Join(cf.Scopes, \" \"))\n+                }\n+\n+                introspectReq, err := http.NewRequest(http.MethodPost, cf.IntrospectionURL, strings.NewReader(body.Encode()))\n+                if err != nil {\n+                        return errors.WithStack(err)\n+                }\n+                for key, value := range cf.IntrospectionRequestHeaders {\n+                        introspectReq.Header.Set(key, value)\n+                }\n+                // set/override the content-type header\n+                introspectReq.Header.Set(\"Content-Type\", \"application/x-www-form-urlencoded\")\n+\n+                // add tracing\n+                closeSpan := a.traceRequest(r.Context(), introspectReq)\n+\n+                resp, err := client.Do(introspectReq.WithContext(r.Context()))\n+\n+                // close the span so it represents just the http request\n+                closeSpan()\n+                if err != nil {\n+                        return errors.WithStack(err)\n+                }\n+                defer resp.Body.Close()\n+\n+                if resp.StatusCode != http.StatusOK {\n+                        return errors.Errorf(\"Introspection returned status code %d but expected %d\", resp.StatusCode, http.StatusOK)\n+                }\n+\n+                if err := json.NewDecoder(resp.Body).Decode(&i); err != nil {\n+                        return errors.WithStack(err)\n+                }\n+\n+                if len(i.TokenUse) > 0 && i.TokenUse != \"access_token\" {\n+                        return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Use of introspected token is not an access token but \\\"%s\\\"\", i.TokenUse)))\n+                }\n+\n+                if !i.Active {\n+                        return errors.WithStack(helper.ErrUnauthorized.WithReason(\"Access token i says token is not active\"))\n+                }\n+\n+                for _, audience := range cf.Audience {\n+                        if !stringslice.Has(i.Audience, audience) {\n+                                return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Token audience is not intended for target audience %s\", audience)))\n+                        }\n+                }\n+\n+                if len(cf.Issuers) > 0 {\n+                        if !stringslice.Has(cf.Issuers, i.Issuer) {\n+                                return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Token issuer does not match any trusted issuer\")))\n+                        }\n+                }\n+\n+                if ss != nil {\n+                        for _, scope := range cf.Scopes {\n+                                if !ss(strings.Split(i.Scope, \" \"), scope) {\n+                                        return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Scope %s was not granted\", scope)))\n+                                }\n+                        }\n+                }\n+\n+                if len(i.Extra) == 0 {\n+                        i.Extra = map[string]interface{}{}\n+                }\n+\n+                i.Extra[\"username\"] = i.Username\n+                i.Extra[\"client_id\"] = i.ClientID\n+                i.Extra[\"scope\"] = i.Scope\n+\n+                if len(i.Audience) != 0 {\n+                        i.Extra[\"aud\"] = i.Audience\n+                }\n+\n+                a.tokenToCache(cf, i, token)\n+        }\n+\n+        session.Subject = i.Subject\n+        session.Extra = i.Extra\n+\n+        return nil\n }\n \n func (a *AuthenticatorOAuth2Introspection) Validate(config json.RawMessage) error {\n-\tif !a.c.AuthenticatorIsEnabled(a.GetID()) {\n-\t\treturn NewErrAuthenticatorNotEnabled(a)\n-\t}\n+        if !a.c.AuthenticatorIsEnabled(a.GetID()) {\n+                return NewErrAuthenticatorNotEnabled(a)\n+        }\n \n-\t_, _, err := a.Config(config)\n-\treturn err\n+        _, _, err := a.Config(config)\n+        return err\n }\n \n func (a *AuthenticatorOAuth2Introspection) Config(config json.RawMessage) (*AuthenticatorOAuth2IntrospectionConfiguration, *http.Client, error) {\n-\tvar c AuthenticatorOAuth2IntrospectionConfiguration\n-\tif err := a.c.AuthenticatorConfig(a.GetID(), config, &c); err != nil {\n-\t\treturn nil, nil, NewErrAuthenticatorMisconfigured(a, err)\n-\t}\n-\n-\tclientKey := fmt.Sprintf(\"%x\", md5.Sum([]byte(config)))\n-\ta.mu.RLock()\n-\tclient, ok := a.clientMap[clientKey]\n-\ta.mu.RUnlock()\n-\n-\tif !ok {\n-\t\ta.logger.Debug(\"Initializing http client\")\n-\t\tvar rt http.RoundTripper\n-\t\tif c.PreAuth != nil && c.PreAuth.Enabled {\n-\t\t\tvar ep url.Values\n-\n-\t\t\tif c.PreAuth.Audience != \"\" {\n-\t\t\t\tep = url.Values{\"audience\": {c.PreAuth.Audience}}\n-\t\t\t}\n-\n-\t\t\trt = (&clientcredentials.Config{\n-\t\t\t\tClientID:       c.PreAuth.ClientID,\n-\t\t\t\tClientSecret:   c.PreAuth.ClientSecret,\n-\t\t\t\tScopes:         c.PreAuth.Scope,\n-\t\t\t\tEndpointParams: ep,\n-\t\t\t\tTokenURL:       c.PreAuth.TokenURL,\n-\t\t\t}).Client(context.Background()).Transport\n-\t\t}\n-\n-\t\tif c.Retry == nil {\n-\t\t\tc.Retry = &AuthenticatorOAuth2IntrospectionRetryConfiguration{Timeout: \"500ms\", MaxWait: \"1s\"}\n-\t\t} else {\n-\t\t\tif c.Retry.Timeout == \"\" {\n-\t\t\t\tc.Retry.Timeout = \"500ms\"\n-\t\t\t}\n-\t\t\tif c.Retry.MaxWait == \"\" {\n-\t\t\t\tc.Retry.MaxWait = \"1s\"\n-\t\t\t}\n-\t\t}\n-\t\tduration, err := time.ParseDuration(c.Retry.Timeout)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, err\n-\t\t}\n-\t\ttimeout := time.Millisecond * duration\n-\n-\t\tmaxWait, err := time.ParseDuration(c.Retry.MaxWait)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, err\n-\t\t}\n-\n-\t\tclient = httpx.NewResilientClientLatencyToleranceConfigurable(rt, timeout, maxWait)\n-\t\ta.mu.Lock()\n-\t\ta.clientMap[clientKey] = client\n-\t\ta.mu.Unlock()\n-\t}\n-\n-\tif c.Cache.TTL != \"\" {\n-\t\tcacheTTL, err := time.ParseDuration(c.Cache.TTL)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, err\n-\t\t}\n-\t\ta.cacheTTL = &cacheTTL\n-\t}\n-\n-\tif a.tokenCache == nil {\n-\t\ta.logger.Debugf(\"Creating cache with max cost: %d\", c.Cache.MaxCost)\n-\t\tcache, _ := ristretto.NewCache(&ristretto.Config{\n-\t\t\t// This will hold about 1000 unique mutation responses.\n-\t\t\tNumCounters: 10000,\n-\t\t\t// Allocate a max\n-\t\t\tMaxCost: int64(c.Cache.MaxCost),\n-\t\t\t// This is a best-practice value.\n-\t\t\tBufferItems: 64,\n-\t\t})\n-\n-\t\ta.tokenCache = cache\n-\t}\n-\n-\treturn &c, client, nil\n+        var c AuthenticatorOAuth2IntrospectionConfiguration\n+        if err := a.c.AuthenticatorConfig(a.GetID(), config, &c); err != nil {\n+                return nil, nil, NewErrAuthenticatorMisconfigured(a, err)\n+        }\n+\n+        clientKey := fmt.Sprintf(\"%x\", md5.Sum([]byte(config)))\n+        a.mu.RLock()\n+        client, ok := a.clientMap[clientKey]\n+        a.mu.RUnlock()\n+\n+        if !ok {\n+                a.logger.Debug(\"Initializing http client\")\n+                var rt http.RoundTripper\n+                if c.PreAuth != nil && c.PreAuth.Enabled {\n+                        var ep url.Values\n+\n+                        if c.PreAuth.Audience != \"\" {\n+                                ep = url.Values{\"audience\": {c.PreAuth.Audience}}\n+                        }\n+\n+                        rt = (&clientcredentials.Config{\n+                                ClientID:       c.PreAuth.ClientID,\n+                                ClientSecret:   c.PreAuth.ClientSecret,\n+                                Scopes:         c.PreAuth.Scope,\n+                                EndpointParams: ep,\n+                                TokenURL:       c.PreAuth.TokenURL,\n+                        }).Client(context.Background()).Transport\n+                }\n+\n+                if c.Retry == nil {\n+                        c.Retry = &AuthenticatorOAuth2IntrospectionRetryConfiguration{Timeout: \"500ms\", MaxWait: \"1s\"}\n+                } else {\n+                        if c.Retry.Timeout == \"\" {\n+                                c.Retry.Timeout = \"500ms\"\n+                        }\n+                        if c.Retry.MaxWait == \"\" {\n+                                c.Retry.MaxWait = \"1s\"\n+                        }\n+                }\n+                duration, err := time.ParseDuration(c.Retry.Timeout)\n+                if err != nil {\n+                        return nil, nil, err\n+                }\n+                timeout := time.Millisecond * duration\n+\n+                maxWait, err := time.ParseDuration(c.Retry.MaxWait)\n+                if err != nil {\n+                        return nil, nil, err\n+                }\n+\n+                client = httpx.NewResilientClientLatencyToleranceConfigurable(rt, timeout, maxWait)\n+                a.mu.Lock()\n+                a.clientMap[clientKey] = client\n+                a.mu.Unlock()\n+        }\n+\n+        if c.Cache.TTL != \"\" {\n+                cacheTTL, err := time.ParseDuration(c.Cache.TTL)\n+                if err != nil {\n+                        return nil, nil, err\n+                }\n+                a.cacheTTL = &cacheTTL\n+        }\n+\n+        if a.tokenCache == nil {\n+                a.logger.Debugf(\"Creating cache with max cost: %d\", c.Cache.MaxCost)\n+                cache, _ := ristretto.NewCache(&ristretto.Config{\n+                        // This will hold about 1000 unique mutation responses.\n+                        NumCounters: 10000,\n+                        // Allocate a max\n+                        MaxCost: int64(c.Cache.MaxCost),\n+                        // This is a best-practice value.\n+                        BufferItems: 64,\n+                })\n+\n+                a.tokenCache = cache\n+        }\n+\n+        return &c, client, nil\n }\n"}
{"cve":"CVE-2021-21411:0708", "fix_patch": "diff --git a/providers/gitlab.go b/providers/gitlab.go\nindex eb7c2ffa..51b9989e 100644\n--- a/providers/gitlab.go\n+++ b/providers/gitlab.go\n@@ -1,364 +1,423 @@\n package providers\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"net/url\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/logger\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/requests\"\n-\t\"golang.org/x/oauth2\"\n+        \"context\"\n+\"encoding/json\"\n+        \"fmt\"\n+\"net/http\"\n+        \"net/url\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/logger\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/requests\"\n+        \"golang.org/x/oauth2\"\n )\n \n // GitLabProvider represents a GitLab based Identity Provider\n type GitLabProvider struct {\n-\t*ProviderData\n+        *ProviderData\n \n-\tGroups   []string\n-\tProjects []*GitlabProject\n+        Groups   []string\n+        Projects []*GitlabProject\n }\n \n // GitlabProject represents a Gitlab project constraint entity\n type GitlabProject struct {\n-\tName        string\n-\tAccessLevel int\n+        Name        string\n+        AccessLevel int\n }\n \n // newGitlabProject Creates a new GitlabProject struct from project string formatted as namespace/project=accesslevel\n // if no accesslevel provided, use the default one\n func newGitlabproject(project string) (*GitlabProject, error) {\n-\t// default access level is 20\n-\tdefaultAccessLevel := 20\n-\t// see https://docs.gitlab.com/ee/api/members.html#valid-access-levels\n-\tvalidAccessLevel := [4]int{10, 20, 30, 40}\n+        // default access level is 20\n+        defaultAccessLevel := 20\n+        // see https://docs.gitlab.com/ee/api/members.html#valid-access-levels\n+        validAccessLevel := [4]int{10, 20, 30, 40}\n \n-\tparts := strings.SplitN(project, \"=\", 2)\n+        parts := strings.SplitN(project, \"=\", 2)\n \n-\tif len(parts) == 2 {\n-\t\tlvl, err := strconv.Atoi(parts[1])\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n+        if len(parts) == 2 {\n+                lvl, err := strconv.Atoi(parts[1])\n+                if err != nil {\n+                        return nil, err\n+                }\n \n-\t\tfor _, valid := range validAccessLevel {\n-\t\t\tif lvl == valid {\n-\t\t\t\treturn &GitlabProject{\n-\t\t\t\t\t\tName:        parts[0],\n-\t\t\t\t\t\tAccessLevel: lvl},\n-\t\t\t\t\terr\n-\t\t\t}\n-\t\t}\n+                for _, valid := range validAccessLevel {\n+                        if lvl == valid {\n+                                return &GitlabProject{\n+                                                Name:        parts[0],\n+                                                AccessLevel: lvl},\n+                                        err\n+                        }\n+                }\n \n-\t\treturn nil, fmt.Errorf(\"invalid gitlab project access level specified (%s)\", parts[0])\n+                return nil, fmt.Errorf(\"invalid gitlab project access level specified (%s)\", parts[0])\n \n-\t}\n+        }\n \n-\treturn &GitlabProject{\n-\t\t\tName:        project,\n-\t\t\tAccessLevel: defaultAccessLevel},\n-\t\tnil\n+        return &GitlabProject{\n+                        Name:        project,\n+                        AccessLevel: defaultAccessLevel},\n+                nil\n \n }\n \n var _ Provider = (*GitLabProvider)(nil)\n \n const (\n-\tgitlabProviderName = \"GitLab\"\n-\tgitlabDefaultScope = \"openid email\"\n+        gitlabProviderName = \"GitLab\"\n+        gitlabDefaultScope = \"openid email\"\n )\n \n // NewGitLabProvider initiates a new GitLabProvider\n func NewGitLabProvider(p *ProviderData) *GitLabProvider {\n-\tp.ProviderName = gitlabProviderName\n+        p.ProviderName = gitlabProviderName\n \n-\tif p.Scope == \"\" {\n-\t\tp.Scope = gitlabDefaultScope\n-\t}\n+        if p.Scope == \"\" {\n+                p.Scope = gitlabDefaultScope\n+        }\n \n-\treturn &GitLabProvider{ProviderData: p}\n+        return &GitLabProvider{ProviderData: p}\n }\n \n // Redeem exchanges the OAuth2 authentication token for an ID token\n func (p *GitLabProvider) Redeem(ctx context.Context, redirectURL, code string) (s *sessions.SessionState, err error) {\n-\tclientSecret, err := p.GetClientSecret()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tc := oauth2.Config{\n-\t\tClientID:     p.ClientID,\n-\t\tClientSecret: clientSecret,\n-\t\tEndpoint: oauth2.Endpoint{\n-\t\t\tTokenURL: p.RedeemURL.String(),\n-\t\t},\n-\t\tRedirectURL: redirectURL,\n-\t}\n-\ttoken, err := c.Exchange(ctx, code)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"token exchange: %v\", err)\n-\t}\n-\ts, err = p.createSession(ctx, token)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"unable to update session: %v\", err)\n-\t}\n-\treturn\n+        clientSecret, err := p.GetClientSecret()\n+        if err != nil {\n+                return\n+        }\n+\n+        c := oauth2.Config{\n+                ClientID:     p.ClientID,\n+                ClientSecret: clientSecret,\n+                Endpoint: oauth2.Endpoint{\n+                        TokenURL: p.RedeemURL.String(),\n+                },\n+                RedirectURL: redirectURL,\n+        }\n+        token, err := c.Exchange(ctx, code)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"token exchange: %v\", err)\n+        }\n+        s, err = p.createSession(ctx, token)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"unable to update session: %v\", err)\n+        }\n+        return\n }\n \n // SetProjectScope ensure read_api is added to scope when filtering on projects\n func (p *GitLabProvider) SetProjectScope() {\n-\tif len(p.Projects) > 0 {\n-\t\tfor _, val := range strings.Split(p.Scope, \" \") {\n-\t\t\tif val == \"read_api\" {\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t}\n-\t\tp.Scope += \" read_api\"\n-\t}\n+        if len(p.Projects) > 0 {\n+                for _, val := range strings.Split(p.Scope, \" \") {\n+                        if val == \"read_api\" {\n+                                return\n+                        }\n+\n+                }\n+                p.Scope += \" read_api\"\n+        }\n }\n \n+\n // RefreshSessionIfNeeded checks if the session has expired and uses the\n // RefreshToken to fetch a new ID token if required\n func (p *GitLabProvider) RefreshSessionIfNeeded(ctx context.Context, s *sessions.SessionState) (bool, error) {\n-\tif s == nil || (s.ExpiresOn != nil && s.ExpiresOn.After(time.Now())) || s.RefreshToken == \"\" {\n-\t\treturn false, nil\n-\t}\n+        if s == nil || (s.ExpiresOn != nil && s.ExpiresOn.After(time.Now())) || s.RefreshToken == \"\" {\n+                return false, nil\n+        }\n \n-\torigExpiration := s.ExpiresOn\n+        origExpiration := s.ExpiresOn\n \n-\terr := p.redeemRefreshToken(ctx, s)\n-\tif err != nil {\n-\t\treturn false, fmt.Errorf(\"unable to redeem refresh token: %v\", err)\n-\t}\n+        err := p.redeemRefreshToken(ctx, s)\n+        if err != nil {\n+                return false, fmt.Errorf(\"unable to redeem refresh token: %v\", err)\n+        }\n \n-\tlogger.Printf(\"refreshed id token %s (expired on %s)\\n\", s, origExpiration)\n-\treturn true, nil\n+        logger.Printf(\"refreshed id token %s (expired on %s)\\n\", s, origExpiration)\n+        return true, nil\n }\n \n func (p *GitLabProvider) redeemRefreshToken(ctx context.Context, s *sessions.SessionState) (err error) {\n-\tclientSecret, err := p.GetClientSecret()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tc := oauth2.Config{\n-\t\tClientID:     p.ClientID,\n-\t\tClientSecret: clientSecret,\n-\t\tEndpoint: oauth2.Endpoint{\n-\t\t\tTokenURL: p.RedeemURL.String(),\n-\t\t},\n-\t}\n-\tt := &oauth2.Token{\n-\t\tRefreshToken: s.RefreshToken,\n-\t\tExpiry:       time.Now().Add(-time.Hour),\n-\t}\n-\ttoken, err := c.TokenSource(ctx, t).Token()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"failed to get token: %v\", err)\n-\t}\n-\tnewSession, err := p.createSession(ctx, token)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to update session: %v\", err)\n-\t}\n-\ts.AccessToken = newSession.AccessToken\n-\ts.IDToken = newSession.IDToken\n-\ts.RefreshToken = newSession.RefreshToken\n-\ts.CreatedAt = newSession.CreatedAt\n-\ts.ExpiresOn = newSession.ExpiresOn\n-\ts.Email = newSession.Email\n-\treturn\n+        clientSecret, err := p.GetClientSecret()\n+        if err != nil {\n+                return\n+        }\n+\n+        c := oauth2.Config{\n+                ClientID:     p.ClientID,\n+                ClientSecret: clientSecret,\n+                Endpoint: oauth2.Endpoint{\n+                        TokenURL: p.RedeemURL.String(),\n+                },\n+        }\n+        t := &oauth2.Token{\n+                RefreshToken: s.RefreshToken,\n+                Expiry:       time.Now().Add(-time.Hour),\n+        }\n+        token, err := c.TokenSource(ctx, t).Token()\n+        if err != nil {\n+                return fmt.Errorf(\"failed to get token: %v\", err)\n+        }\n+        newSession, err := p.createSession(ctx, token)\n+        if err != nil {\n+                return fmt.Errorf(\"unable to update session: %v\", err)\n+        }\n+        s.AccessToken = newSession.AccessToken\n+        s.IDToken = newSession.IDToken\n+        s.RefreshToken = newSession.RefreshToken\n+        s.CreatedAt = newSession.CreatedAt\n+        s.ExpiresOn = newSession.ExpiresOn\n+        s.Email = newSession.Email\n+        return\n }\n \n type gitlabUserInfo struct {\n-\tUsername      string   `json:\"nickname\"`\n-\tEmail         string   `json:\"email\"`\n-\tEmailVerified bool     `json:\"email_verified\"`\n-\tGroups        []string `json:\"groups\"`\n+        Username      string   `json:\"nickname\"`\n+        Email         string   `json:\"email\"`\n+        EmailVerified bool     `json:\"email_verified\"`\n+        Groups        []string `json:\"groups\"`\n }\n \n func (p *GitLabProvider) getUserInfo(ctx context.Context, s *sessions.SessionState) (*gitlabUserInfo, error) {\n-\t// Retrieve user info JSON\n-\t// https://docs.gitlab.com/ee/integration/openid_connect_provider.html#shared-information\n-\n-\t// Build user info url from login url of GitLab instance\n-\tuserInfoURL := *p.LoginURL\n-\tuserInfoURL.Path = \"/oauth/userinfo\"\n-\n-\tvar userInfo gitlabUserInfo\n-\terr := requests.New(userInfoURL.String()).\n-\t\tWithContext(ctx).\n-\t\tSetHeader(\"Authorization\", \"Bearer \"+s.AccessToken).\n-\t\tDo().\n-\t\tUnmarshalInto(&userInfo)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting user info: %v\", err)\n-\t}\n-\n-\treturn &userInfo, nil\n+        // Retrieve user info JSON\n+        // https://docs.gitlab.com/ee/integration/openid_connect_provider.html#shared-information\n+\n+        // Build user info url from login url of GitLab instance\n+        userInfoURL := *p.LoginURL\n+        userInfoURL.Path = \"/oauth/userinfo\"\n+\n+        var userInfo gitlabUserInfo\n+        err := requests.New(userInfoURL.String()).\n+                WithContext(ctx).\n+                SetHeader(\"Authorization\", \"Bearer \"+s.AccessToken).\n+                Do().\n+                UnmarshalInto(&userInfo)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting user info: %v\", err)\n+        }\n+\n+        return &userInfo, nil\n }\n \n type gitlabPermissionAccess struct {\n-\tAccessLevel int `json:\"access_level\"`\n+        AccessLevel int `json:\"access_level\"`\n }\n \n type gitlabProjectPermission struct {\n-\tProjectAccess *gitlabPermissionAccess `json:\"project_access\"`\n-\tGroupAccess   *gitlabPermissionAccess `json:\"group_access\"`\n+        ProjectAccess *gitlabPermissionAccess `json:\"project_access\"`\n+        GroupAccess   *gitlabPermissionAccess `json:\"group_access\"`\n }\n \n type gitlabProjectInfo struct {\n-\tName              string                  `json:\"name\"`\n-\tArchived          bool                    `json:\"archived\"`\n-\tPathWithNamespace string                  `json:\"path_with_namespace\"`\n-\tPermissions       gitlabProjectPermission `json:\"permissions\"`\n+        Name              string                  `json:\"name\"`\n+        Archived          bool                    `json:\"archived\"`\n+        PathWithNamespace string                  `json:\"path_with_namespace\"`\n+        Permissions       gitlabProjectPermission `json:\"permissions\"`\n }\n \n+type gitlabGroup struct {\n+FullPath string `json:\"full_path\"`\n+}\n func (p *GitLabProvider) getProjectInfo(ctx context.Context, s *sessions.SessionState, project string) (*gitlabProjectInfo, error) {\n-\tvar projectInfo gitlabProjectInfo\n+        var projectInfo gitlabProjectInfo\n+\n+        endpointURL := &url.URL{\n+                Scheme: p.LoginURL.Scheme,\n+                Host:   p.LoginURL.Host,\n+                Path:   \"/api/v4/projects/\",\n+        }\n+\n+        err := requests.New(fmt.Sprintf(\"%s%s\", endpointURL.String(), url.QueryEscape(project))).\n+                WithContext(ctx).\n+                SetHeader(\"Authorization\", \"Bearer \"+s.AccessToken).\n+                Do().\n+                UnmarshalInto(&projectInfo)\n+\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to get project info: %v\", err)\n+return &projectInfo, nil\n+        }\n+\n+func (p *GitLabProvider) getGroups(ctx context.Context, s *sessions.SessionState) ([]string, error) {\n+var groups []string\n+endpointURL := &url.URL{\n+Scheme: p.LoginURL.Scheme,\n+Host:   p.LoginURL.Host,\n+Path:   \"/api/v4/groups\",\n+}\n+\n+params := url.Values{}\n+params.Add(\"min_access_level\", \"10\")\n+endpointURL.RawQuery = params.Encode()\n+\n+for {\n+var groupInfos []gitlabGroup\n \n-\tendpointURL := &url.URL{\n-\t\tScheme: p.LoginURL.Scheme,\n-\t\tHost:   p.LoginURL.Host,\n-\t\tPath:   \"/api/v4/projects/\",\n-\t}\n+header := http.Header{}\n+header.Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", s.AccessToken))\n \n-\terr := requests.New(fmt.Sprintf(\"%s%s\", endpointURL.String(), url.QueryEscape(project))).\n-\t\tWithContext(ctx).\n-\t\tSetHeader(\"Authorization\", \"Bearer \"+s.AccessToken).\n-\t\tDo().\n-\t\tUnmarshalInto(&projectInfo)\n+req, err := http.NewRequestWithContext(ctx, \"GET\", endpointURL.String(), nil)\n+if err != nil {\n+return nil, fmt.Errorf(\"failed to create group info request: %v\", err)\n+}\n+req.Header = header\n \n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get project info: %v\", err)\n-\t}\n+resp, err := http.DefaultClient.Do(req)\n+if err != nil {\n+return nil, fmt.Errorf(\"failed to perform group info request: %v\", err)\n+}\n+defer resp.Body.Close()\n \n-\treturn &projectInfo, nil\n+if resp.StatusCode != http.StatusOK {\n+return nil, fmt.Errorf(\"unexpected status while retrieving group info: %d\", resp.StatusCode)\n }\n \n+if err := json.NewDecoder(resp.Body).Decode(&groupInfos); err != nil {\n+return nil, fmt.Errorf(\"failed to unmarshal group info: %v\", err)\n+}\n+\n+for _, groupInfo := range groupInfos {\n+groups = append(groups, groupInfo.FullPath)\n+}\n+\n+nextPage := resp.Header.Get(\"X-Next-Page\")\n+if nextPage == \"\" {\n+break\n+}\n+params.Set(\"page\", nextPage)\n+endpointURL.RawQuery = params.Encode()\n+}\n+\n+return groups, nil\n+}\n+        \n+}\n+\n+}\n // AddProjects adds Gitlab projects from options to GitlabProvider struct\n func (p *GitLabProvider) AddProjects(projects []string) error {\n-\tfor _, project := range projects {\n-\t\tgp, err := newGitlabproject(project)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n+        for _, project := range projects {\n+                gp, err := newGitlabproject(project)\n+                if err != nil {\n+                        return err\n+                }\n \n-\t\tp.Projects = append(p.Projects, gp)\n-\t}\n+                p.Projects = append(p.Projects, gp)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n+\n func (p *GitLabProvider) createSession(ctx context.Context, token *oauth2.Token) (*sessions.SessionState, error) {\n-\tidToken, err := p.verifyIDToken(ctx, token)\n-\tif err != nil {\n-\t\tswitch err {\n-\t\tcase ErrMissingIDToken:\n-\t\t\treturn nil, fmt.Errorf(\"token response did not contain an id_token\")\n-\t\tdefault:\n-\t\t\treturn nil, fmt.Errorf(\"could not verify id_token: %v\", err)\n-\t\t}\n-\t}\n-\n-\tcreated := time.Now()\n-\treturn &sessions.SessionState{\n-\t\tAccessToken:  token.AccessToken,\n-\t\tIDToken:      getIDToken(token),\n-\t\tRefreshToken: token.RefreshToken,\n-\t\tCreatedAt:    &created,\n-\t\tExpiresOn:    &idToken.Expiry,\n-\t}, nil\n+        idToken, err := p.verifyIDToken(ctx, token)\n+        if err != nil {\n+                switch err {\n+                case ErrMissingIDToken:\n+                        return nil, fmt.Errorf(\"token response did not contain an id_token\")\n+                default:\n+                        return nil, fmt.Errorf(\"could not verify id_token: %v\", err)\n+                }\n+        }\n+\n+        created := time.Now()\n+        return &sessions.SessionState{\n+                AccessToken:  token.AccessToken,\n+                IDToken:      getIDToken(token),\n+                RefreshToken: token.RefreshToken,\n+                CreatedAt:    &created,\n+                ExpiresOn:    &idToken.Expiry,\n+        }, nil\n }\n \n // ValidateSession checks that the session's IDToken is still valid\n func (p *GitLabProvider) ValidateSession(ctx context.Context, s *sessions.SessionState) bool {\n-\t_, err := p.Verifier.Verify(ctx, s.IDToken)\n-\treturn err == nil\n+        _, err := p.Verifier.Verify(ctx, s.IDToken)\n+        return err == nil\n }\n \n // EnrichSession adds values and data from the Gitlab endpoint to current session\n func (p *GitLabProvider) EnrichSession(ctx context.Context, s *sessions.SessionState) error {\n-\t// Retrieve user info\n-\tuserInfo, err := p.getUserInfo(ctx, s)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"failed to retrieve user info: %v\", err)\n-\t}\n+        // Retrieve user info\n+        userInfo, err := p.getUserInfo(ctx, s)\n+        if err != nil {\n+                return fmt.Errorf(\"failed to retrieve user info: %v\", err)\n+        }\n \n-\t// Check if email is verified\n-\tif !p.AllowUnverifiedEmail && !userInfo.EmailVerified {\n-\t\treturn fmt.Errorf(\"user email is not verified\")\n-\t}\n+        // Check if email is verified\n+        if !p.AllowUnverifiedEmail && !userInfo.EmailVerified {\n+                return fmt.Errorf(\"user email is not verified\")\n+        }\n \n-\ts.User = userInfo.Username\n-\ts.Email = userInfo.Email\n+        s.User = userInfo.Username\n+        s.Email = userInfo.Email\n \n-\tp.addGroupsToSession(ctx, s)\n+        p.addGroupsToSession(s, userInfo)\n \n-\tp.addProjectsToSession(ctx, s)\n+        p.addProjectsToSession(ctx, s)\n \n-\treturn nil\n+        return nil\n \n }\n \n // addGroupsToSession projects into session.Groups\n-func (p *GitLabProvider) addGroupsToSession(ctx context.Context, s *sessions.SessionState) {\n-\t// Iterate over projects, check if oauth2-proxy can get project information on behalf of the user\n-\tfor _, group := range p.Groups {\n-\t\ts.Groups = append(s.Groups, fmt.Sprintf(\"group:%s\", group))\n-\t}\n+func (p *GitLabProvider) addGroupsToSession(s *sessions.SessionState, userInfo *gitlabUserInfo) {\n+// Iterate over projects, check if oauth2-proxy can get project information on behalf of the user\n+s.Groups = userInfo.Groups\n }\n \n // addProjectsToSession adds projects matching user access requirements into the session state groups list\n // This method prefix projects names with `project` to specify group kind\n func (p *GitLabProvider) addProjectsToSession(ctx context.Context, s *sessions.SessionState) {\n-\t// Iterate over projects, check if oauth2-proxy can get project information on behalf of the user\n-\tfor _, project := range p.Projects {\n-\t\tprojectInfo, err := p.getProjectInfo(ctx, s, project.Name)\n-\n-\t\tif err != nil {\n-\t\t\tlogger.Errorf(\"Warning: project info request failed: %v\", err)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif !projectInfo.Archived {\n-\t\t\tperms := projectInfo.Permissions.ProjectAccess\n-\t\t\tif perms == nil {\n-\t\t\t\t// use group project access as fallback\n-\t\t\t\tperms = projectInfo.Permissions.GroupAccess\n-\t\t\t\t// group project access is not set for this user then we give up\n-\t\t\t\tif perms == nil {\n-\t\t\t\t\tlogger.Errorf(\"Warning: user %q has no project level access to %s\", s.Email, project.Name)\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tif perms != nil && perms.AccessLevel >= project.AccessLevel {\n-\t\t\t\ts.Groups = append(s.Groups, fmt.Sprintf(\"project:%s\", project.Name))\n-\t\t\t} else {\n-\t\t\t\tlogger.Errorf(\"Warning: user %q does not have the minimum required access level for project %q\", s.Email, project.Name)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tlogger.Errorf(\"Warning: project %s is archived\", project.Name)\n-\t\t}\n-\n-\t}\n+        // Iterate over projects, check if oauth2-proxy can get project information on behalf of the user\n+        for _, project := range p.Projects {\n+                projectInfo, err := p.getProjectInfo(ctx, s, project.Name)\n+\n+                if err != nil {\n+                        logger.Errorf(\"Warning: project info request failed: %v\", err)\n+                        continue\n+                }\n+\n+                if !projectInfo.Archived {\n+                        perms := projectInfo.Permissions.ProjectAccess\n+                        if perms == nil {\n+                                // use group project access as fallback\n+                                perms = projectInfo.Permissions.GroupAccess\n+                                // group project access is not set for this user then we give up\n+                                if perms == nil {\n+                                        logger.Errorf(\"Warning: user %q has no project level access to %s\", s.Email, project.Name)\n+                                        continue\n+                                }\n+                        }\n+\n+                        if perms != nil && perms.AccessLevel >= project.AccessLevel {\n+                                s.Groups = append(s.Groups, fmt.Sprintf(\"project:%s\", project.Name))\n+                        } else {\n+                                logger.Errorf(\"Warning: user %q does not have the minimum required access level for project %q\", s.Email, project.Name)\n+                        }\n+                } else {\n+                        logger.Errorf(\"Warning: project %s is archived\", project.Name)\n+                }\n+\n+        }\n \n }\n \n // PrefixAllowedGroups returns a list of allowed groups, prefixed by their `kind` value\n func (p *GitLabProvider) PrefixAllowedGroups() (groups []string) {\n \n-\tfor _, val := range p.Groups {\n-\t\tgroups = append(groups, fmt.Sprintf(\"group:%s\", val))\n-\t}\n+        for _, val := range p.Groups {\n+                groups = append(groups, fmt.Sprintf(\"group:%s\", val))\n+        }\n \n-\tfor _, val := range p.Projects {\n-\t\tgroups = append(groups, fmt.Sprintf(\"project:%s\", val.Name))\n-\t}\n+        for _, val := range p.Projects {\n+                groups = append(groups, fmt.Sprintf(\"project:%s\", val.Name))\n+        }\n \n-\treturn groups\n+        return groups\n }\n"}
{"cve":"CVE-2022-35936:0708", "fix_patch": "diff --git a/x/evm/keeper/statedb.go b/x/evm/keeper/statedb.go\nindex a70d1427..33c86918 100644\n--- a/x/evm/keeper/statedb.go\n+++ b/x/evm/keeper/statedb.go\n@@ -1,17 +1,17 @@\n package keeper\n \n import (\n-\t\"bytes\"\n-\t\"fmt\"\n-\t\"math/big\"\n-\n-\t\"github.com/cosmos/cosmos-sdk/store/prefix\"\n-\tsdk \"github.com/cosmos/cosmos-sdk/types\"\n-\tsdkerrors \"github.com/cosmos/cosmos-sdk/types/errors\"\n-\t\"github.com/ethereum/go-ethereum/common\"\n-\tethermint \"github.com/evmos/ethermint/types\"\n-\t\"github.com/evmos/ethermint/x/evm/statedb\"\n-\t\"github.com/evmos/ethermint/x/evm/types\"\n+        \n+        \"fmt\"\n+        \"math/big\"\n+\n+        \"github.com/cosmos/cosmos-sdk/store/prefix\"\n+        sdk \"github.com/cosmos/cosmos-sdk/types\"\n+        sdkerrors \"github.com/cosmos/cosmos-sdk/types/errors\"\n+        \"github.com/ethereum/go-ethereum/common\"\n+        ethermint \"github.com/evmos/ethermint/types\"\n+        \"github.com/evmos/ethermint/x/evm/statedb\"\n+        \"github.com/evmos/ethermint/x/evm/types\"\n )\n \n var _ statedb.Keeper = &Keeper{}\n@@ -22,155 +22,155 @@ var _ statedb.Keeper = &Keeper{}\n \n // GetAccount returns nil if account is not exist, returns error if it's not `EthAccountI`\n func (k *Keeper) GetAccount(ctx sdk.Context, addr common.Address) *statedb.Account {\n-\tacct := k.GetAccountWithoutBalance(ctx, addr)\n-\tif acct == nil {\n-\t\treturn nil\n-\t}\n+        acct := k.GetAccountWithoutBalance(ctx, addr)\n+        if acct == nil {\n+                return nil\n+        }\n \n-\tacct.Balance = k.GetBalance(ctx, addr)\n-\treturn acct\n+        acct.Balance = k.GetBalance(ctx, addr)\n+        return acct\n }\n \n // GetState loads contract state from database, implements `statedb.Keeper` interface.\n func (k *Keeper) GetState(ctx sdk.Context, addr common.Address, key common.Hash) common.Hash {\n-\tstore := prefix.NewStore(ctx.KVStore(k.storeKey), types.AddressStoragePrefix(addr))\n+        store := prefix.NewStore(ctx.KVStore(k.storeKey), types.AddressStoragePrefix(addr))\n \n-\tvalue := store.Get(key.Bytes())\n-\tif len(value) == 0 {\n-\t\treturn common.Hash{}\n-\t}\n+        value := store.Get(key.Bytes())\n+        if len(value) == 0 {\n+                return common.Hash{}\n+        }\n \n-\treturn common.BytesToHash(value)\n+        return common.BytesToHash(value)\n }\n \n // GetCode loads contract code from database, implements `statedb.Keeper` interface.\n func (k *Keeper) GetCode(ctx sdk.Context, codeHash common.Hash) []byte {\n-\tstore := prefix.NewStore(ctx.KVStore(k.storeKey), types.KeyPrefixCode)\n-\treturn store.Get(codeHash.Bytes())\n+        store := prefix.NewStore(ctx.KVStore(k.storeKey), types.KeyPrefixCode)\n+        return store.Get(codeHash.Bytes())\n }\n \n // ForEachStorage iterate contract storage, callback return false to break early\n func (k *Keeper) ForEachStorage(ctx sdk.Context, addr common.Address, cb func(key, value common.Hash) bool) {\n-\tstore := ctx.KVStore(k.storeKey)\n-\tprefix := types.AddressStoragePrefix(addr)\n+        store := ctx.KVStore(k.storeKey)\n+        prefix := types.AddressStoragePrefix(addr)\n \n-\titerator := sdk.KVStorePrefixIterator(store, prefix)\n-\tdefer iterator.Close()\n+        iterator := sdk.KVStorePrefixIterator(store, prefix)\n+        defer iterator.Close()\n \n-\tfor ; iterator.Valid(); iterator.Next() {\n-\t\tkey := common.BytesToHash(iterator.Key())\n-\t\tvalue := common.BytesToHash(iterator.Value())\n+        for ; iterator.Valid(); iterator.Next() {\n+                key := common.BytesToHash(iterator.Key())\n+                value := common.BytesToHash(iterator.Value())\n \n-\t\t// check if iteration stops\n-\t\tif !cb(key, value) {\n-\t\t\treturn\n-\t\t}\n-\t}\n+                // check if iteration stops\n+                if !cb(key, value) {\n+                        return\n+                }\n+        }\n }\n \n // SetBalance update account's balance, compare with current balance first, then decide to mint or burn.\n func (k *Keeper) SetBalance(ctx sdk.Context, addr common.Address, amount *big.Int) error {\n-\tcosmosAddr := sdk.AccAddress(addr.Bytes())\n-\n-\tparams := k.GetParams(ctx)\n-\tcoin := k.bankKeeper.GetBalance(ctx, cosmosAddr, params.EvmDenom)\n-\tbalance := coin.Amount.BigInt()\n-\tdelta := new(big.Int).Sub(amount, balance)\n-\tswitch delta.Sign() {\n-\tcase 1:\n-\t\t// mint\n-\t\tcoins := sdk.NewCoins(sdk.NewCoin(params.EvmDenom, sdk.NewIntFromBigInt(delta)))\n-\t\tif err := k.bankKeeper.MintCoins(ctx, types.ModuleName, coins); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tif err := k.bankKeeper.SendCoinsFromModuleToAccount(ctx, types.ModuleName, cosmosAddr, coins); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\tcase -1:\n-\t\t// burn\n-\t\tcoins := sdk.NewCoins(sdk.NewCoin(params.EvmDenom, sdk.NewIntFromBigInt(new(big.Int).Neg(delta))))\n-\t\tif err := k.bankKeeper.SendCoinsFromAccountToModule(ctx, cosmosAddr, types.ModuleName, coins); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tif err := k.bankKeeper.BurnCoins(ctx, types.ModuleName, coins); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\tdefault:\n-\t\t// not changed\n-\t}\n-\treturn nil\n+        cosmosAddr := sdk.AccAddress(addr.Bytes())\n+\n+        params := k.GetParams(ctx)\n+        coin := k.bankKeeper.GetBalance(ctx, cosmosAddr, params.EvmDenom)\n+        balance := coin.Amount.BigInt()\n+        delta := new(big.Int).Sub(amount, balance)\n+        switch delta.Sign() {\n+        case 1:\n+                // mint\n+                coins := sdk.NewCoins(sdk.NewCoin(params.EvmDenom, sdk.NewIntFromBigInt(delta)))\n+                if err := k.bankKeeper.MintCoins(ctx, types.ModuleName, coins); err != nil {\n+                        return err\n+                }\n+                if err := k.bankKeeper.SendCoinsFromModuleToAccount(ctx, types.ModuleName, cosmosAddr, coins); err != nil {\n+                        return err\n+                }\n+        case -1:\n+                // burn\n+                coins := sdk.NewCoins(sdk.NewCoin(params.EvmDenom, sdk.NewIntFromBigInt(new(big.Int).Neg(delta))))\n+                if err := k.bankKeeper.SendCoinsFromAccountToModule(ctx, cosmosAddr, types.ModuleName, coins); err != nil {\n+                        return err\n+                }\n+                if err := k.bankKeeper.BurnCoins(ctx, types.ModuleName, coins); err != nil {\n+                        return err\n+                }\n+        default:\n+                // not changed\n+        }\n+        return nil\n }\n \n // SetAccount updates nonce/balance/codeHash together.\n func (k *Keeper) SetAccount(ctx sdk.Context, addr common.Address, account statedb.Account) error {\n-\t// update account\n-\tcosmosAddr := sdk.AccAddress(addr.Bytes())\n-\tacct := k.accountKeeper.GetAccount(ctx, cosmosAddr)\n-\tif acct == nil {\n-\t\tacct = k.accountKeeper.NewAccountWithAddress(ctx, cosmosAddr)\n-\t}\n-\n-\tif err := acct.SetSequence(account.Nonce); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tcodeHash := common.BytesToHash(account.CodeHash)\n-\n-\tif ethAcct, ok := acct.(ethermint.EthAccountI); ok {\n-\t\tif err := ethAcct.SetCodeHash(codeHash); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\tk.accountKeeper.SetAccount(ctx, acct)\n-\n-\tif err := k.SetBalance(ctx, addr, account.Balance); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tk.Logger(ctx).Debug(\n-\t\t\"account updated\",\n-\t\t\"ethereum-address\", addr.Hex(),\n-\t\t\"nonce\", account.Nonce,\n-\t\t\"codeHash\", codeHash.Hex(),\n-\t\t\"balance\", account.Balance,\n-\t)\n-\treturn nil\n+        // update account\n+        cosmosAddr := sdk.AccAddress(addr.Bytes())\n+        acct := k.accountKeeper.GetAccount(ctx, cosmosAddr)\n+        if acct == nil {\n+                acct = k.accountKeeper.NewAccountWithAddress(ctx, cosmosAddr)\n+        }\n+\n+        if err := acct.SetSequence(account.Nonce); err != nil {\n+                return err\n+        }\n+\n+        codeHash := common.BytesToHash(account.CodeHash)\n+\n+        if ethAcct, ok := acct.(ethermint.EthAccountI); ok {\n+                if err := ethAcct.SetCodeHash(codeHash); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        k.accountKeeper.SetAccount(ctx, acct)\n+\n+        if err := k.SetBalance(ctx, addr, account.Balance); err != nil {\n+                return err\n+        }\n+\n+        k.Logger(ctx).Debug(\n+                \"account updated\",\n+                \"ethereum-address\", addr.Hex(),\n+                \"nonce\", account.Nonce,\n+                \"codeHash\", codeHash.Hex(),\n+                \"balance\", account.Balance,\n+        )\n+        return nil\n }\n \n // SetState update contract storage, delete if value is empty.\n func (k *Keeper) SetState(ctx sdk.Context, addr common.Address, key common.Hash, value []byte) {\n-\tstore := prefix.NewStore(ctx.KVStore(k.storeKey), types.AddressStoragePrefix(addr))\n-\taction := \"updated\"\n-\tif len(value) == 0 {\n-\t\tstore.Delete(key.Bytes())\n-\t\taction = \"deleted\"\n-\t} else {\n-\t\tstore.Set(key.Bytes(), value)\n-\t}\n-\tk.Logger(ctx).Debug(\n-\t\tfmt.Sprintf(\"state %s\", action),\n-\t\t\"ethereum-address\", addr.Hex(),\n-\t\t\"key\", key.Hex(),\n-\t)\n+        store := prefix.NewStore(ctx.KVStore(k.storeKey), types.AddressStoragePrefix(addr))\n+        action := \"updated\"\n+        if len(value) == 0 {\n+                store.Delete(key.Bytes())\n+                action = \"deleted\"\n+        } else {\n+                store.Set(key.Bytes(), value)\n+        }\n+        k.Logger(ctx).Debug(\n+                fmt.Sprintf(\"state %s\", action),\n+                \"ethereum-address\", addr.Hex(),\n+                \"key\", key.Hex(),\n+        )\n }\n \n // SetCode set contract code, delete if code is empty.\n func (k *Keeper) SetCode(ctx sdk.Context, codeHash, code []byte) {\n-\tstore := prefix.NewStore(ctx.KVStore(k.storeKey), types.KeyPrefixCode)\n-\n-\t// store or delete code\n-\taction := \"updated\"\n-\tif len(code) == 0 {\n-\t\tstore.Delete(codeHash)\n-\t\taction = \"deleted\"\n-\t} else {\n-\t\tstore.Set(codeHash, code)\n-\t}\n-\tk.Logger(ctx).Debug(\n-\t\tfmt.Sprintf(\"code %s\", action),\n-\t\t\"code-hash\", common.BytesToHash(codeHash).Hex(),\n-\t)\n+        store := prefix.NewStore(ctx.KVStore(k.storeKey), types.KeyPrefixCode)\n+\n+        // store or delete code\n+        action := \"updated\"\n+        if len(code) == 0 {\n+                store.Delete(codeHash)\n+                action = \"deleted\"\n+        } else {\n+                store.Set(codeHash, code)\n+        }\n+        k.Logger(ctx).Debug(\n+                fmt.Sprintf(\"code %s\", action),\n+                \"code-hash\", common.BytesToHash(codeHash).Hex(),\n+        )\n }\n \n // DeleteAccount handles contract's suicide call:\n@@ -179,43 +179,38 @@ func (k *Keeper) SetCode(ctx sdk.Context, codeHash, code []byte) {\n // - remove states\n // - remove auth account\n func (k *Keeper) DeleteAccount(ctx sdk.Context, addr common.Address) error {\n-\tcosmosAddr := sdk.AccAddress(addr.Bytes())\n-\tacct := k.accountKeeper.GetAccount(ctx, cosmosAddr)\n-\tif acct == nil {\n-\t\treturn nil\n-\t}\n-\n-\t// NOTE: only Ethereum accounts (contracts) can be selfdestructed\n-\tethAcct, ok := acct.(ethermint.EthAccountI)\n-\tif !ok {\n-\t\treturn sdkerrors.Wrapf(types.ErrInvalidAccount, \"type %T, address %s\", acct, addr)\n-\t}\n-\n-\t// clear balance\n-\tif err := k.SetBalance(ctx, addr, new(big.Int)); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// remove code\n-\tcodeHashBz := ethAcct.GetCodeHash().Bytes()\n-\tif !bytes.Equal(codeHashBz, types.EmptyCodeHash) {\n-\t\tk.SetCode(ctx, codeHashBz, nil)\n-\t}\n-\n-\t// clear storage\n-\tk.ForEachStorage(ctx, addr, func(key, _ common.Hash) bool {\n-\t\tk.SetState(ctx, addr, key, nil)\n-\t\treturn true\n-\t})\n-\n-\t// remove auth account\n-\tk.accountKeeper.RemoveAccount(ctx, acct)\n-\n-\tk.Logger(ctx).Debug(\n-\t\t\"account suicided\",\n-\t\t\"ethereum-address\", addr.Hex(),\n-\t\t\"cosmos-address\", cosmosAddr.String(),\n-\t)\n-\n-\treturn nil\n+        cosmosAddr := sdk.AccAddress(addr.Bytes())\n+        acct := k.accountKeeper.GetAccount(ctx, cosmosAddr)\n+        if acct == nil {\n+                return nil\n+        }\n+\n+        // NOTE: only Ethereum accounts (contracts) can be selfdestructed\n+        if _, ok := acct.(ethermint.EthAccountI); !ok {\n+                return sdkerrors.Wrapf(types.ErrInvalidAccount, \"type %T, address %s\", acct, addr)\n+        }\n+\n+        // clear balance\n+        if err := k.SetBalance(ctx, addr, new(big.Int)); err != nil {\n+                return err\n+        }\n+\n+\n+\n+        // clear storage\n+        k.ForEachStorage(ctx, addr, func(key, _ common.Hash) bool {\n+                k.SetState(ctx, addr, key, nil)\n+                return true\n+        })\n+\n+        // remove auth account\n+        k.accountKeeper.RemoveAccount(ctx, acct)\n+\n+        k.Logger(ctx).Debug(\n+                \"account suicided\",\n+                \"ethereum-address\", addr.Hex(),\n+                \"cosmos-address\", cosmosAddr.String(),\n+        )\n+\n+        return nil\n }\n"}
{"cve":"CVE-2022-23542:0708", "fix_patch": "diff --git a/server/commands/check_utils.go b/server/commands/check_utils.go\nindex 2b1651e4..9d283a0e 100644\n--- a/server/commands/check_utils.go\n+++ b/server/commands/check_utils.go\n@@ -1,77 +1,78 @@\n package commands\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"sync\"\n-\n-\t\"github.com/openfga/openfga/internal/contextualtuples\"\n-\t\"github.com/openfga/openfga/internal/utils\"\n-\t\"github.com/openfga/openfga/internal/validation\"\n-\ttupleUtils \"github.com/openfga/openfga/pkg/tuple\"\n-\t\"github.com/openfga/openfga/storage\"\n-\topenfgapb \"go.buf.build/openfga/go/openfga/api/openfga/v1\"\n+\"context\"\n+\"errors\"\n+\"fmt\"\n+\"sync\"\n+\n+\"github.com/openfga/openfga/internal/contextualtuples\"\n+\"github.com/openfga/openfga/internal/utils\"\n+\"github.com/openfga/openfga/internal/validation\"\n+tupleUtils \"github.com/openfga/openfga/pkg/tuple\"\n+\"github.com/openfga/openfga/storage\"\n+openfgapb \"go.buf.build/openfga/go/openfga/api/openfga/v1\"\n )\n \n // Keeping the interface simple for the time being\n // we could make it Append* where * are tupleset, computedset, etc.\n // especially if we want to generate other representations for the trace (e.g. a tree)\n type resolutionTracer interface {\n-\tAppendComputed() resolutionTracer\n-\tAppendDirect() resolutionTracer\n-\tAppendIndex(i int) resolutionTracer\n-\tAppendIntersection(t intersectionTracer) resolutionTracer\n-\tAppendString(s string) resolutionTracer\n-\tAppendTupleToUserset() resolutionTracer\n-\tAppendUnion() resolutionTracer\n-\tCreateIntersectionTracer() intersectionTracer\n-\tGetResolution() string\n+AppendComputed() resolutionTracer\n+AppendDirect() resolutionTracer\n+AppendIndex(i int) resolutionTracer\n+AppendIntersection(t intersectionTracer) resolutionTracer\n+AppendString(s string) resolutionTracer\n+AppendTupleToUserset() resolutionTracer\n+AppendUnion() resolutionTracer\n+CreateIntersectionTracer() intersectionTracer\n+GetResolution() string\n }\n \n type intersectionTracer interface {\n-\tAppendTrace(rt resolutionTracer)\n-\tGetResolution() string\n+AppendTrace(rt resolutionTracer)\n+GetResolution() string\n }\n \n // noopResolutionTracer is thread safe as current implementation is immutable\n type noopResolutionTracer struct{}\n \n func (t *noopResolutionTracer) AppendComputed() resolutionTracer {\n-\treturn t\n+return t\n }\n \n func (t *noopResolutionTracer) AppendDirect() resolutionTracer {\n-\treturn t\n+return t\n }\n \n func (t *noopResolutionTracer) AppendIndex(_ int) resolutionTracer {\n-\treturn t\n+return t\n }\n \n func (t *noopResolutionTracer) AppendIntersection(_ intersectionTracer) resolutionTracer {\n-\treturn t\n+return t\n }\n \n func (t *noopResolutionTracer) AppendString(_ string) resolutionTracer {\n-\treturn t\n+return t\n }\n \n func (t *noopResolutionTracer) AppendTupleToUserset() resolutionTracer {\n-\treturn t\n+return t\n }\n \n func (t *noopResolutionTracer) AppendUnion() resolutionTracer {\n-\treturn t\n+return t\n }\n \n var nit = &noopIntersectionTracer{}\n \n func (t *noopResolutionTracer) CreateIntersectionTracer() intersectionTracer {\n-\treturn nit\n+return nit\n }\n \n func (t *noopResolutionTracer) GetResolution() string {\n-\treturn \"\"\n+return \"\"\n }\n \n type noopIntersectionTracer struct{}\n@@ -79,303 +80,305 @@ type noopIntersectionTracer struct{}\n func (t *noopIntersectionTracer) AppendTrace(_ resolutionTracer) {}\n \n func (t *noopIntersectionTracer) GetResolution() string {\n-\treturn \"\"\n+return \"\"\n }\n \n // stringResolutionTracer is thread safe as current implementation is immutable\n type stringResolutionTracer struct {\n-\ttrace string\n+trace string\n }\n \n func newStringResolutionTracer() resolutionTracer {\n-\treturn &stringResolutionTracer{\n-\t\ttrace: \".\",\n-\t}\n+return &stringResolutionTracer{\n+trace: \".\",\n+}\n }\n \n func (t *stringResolutionTracer) AppendComputed() resolutionTracer {\n-\treturn t.AppendString(\"(computed-userset)\")\n+return t.AppendString(\"(computed-userset)\")\n }\n \n func (t *stringResolutionTracer) AppendDirect() resolutionTracer {\n-\treturn t.AppendString(\"(direct)\")\n+return t.AppendString(\"(direct)\")\n }\n \n // AppendIndex We create separate append functions so no casting happens externally\n // This aim to minimize overhead added by the no-op implementation\n func (t *stringResolutionTracer) AppendIndex(n int) resolutionTracer {\n-\treturn &stringResolutionTracer{\n-\t\ttrace: fmt.Sprintf(\"%s%d\", t.trace, n),\n-\t}\n+return &stringResolutionTracer{\n+trace: fmt.Sprintf(\"%s%d\", t.trace, n),\n+}\n }\n \n func (t *stringResolutionTracer) AppendIntersection(it intersectionTracer) resolutionTracer {\n-\treturn &stringResolutionTracer{\n-\t\ttrace: fmt.Sprintf(\"%s[%s]\", t.trace, it.GetResolution()),\n-\t}\n+return &stringResolutionTracer{\n+trace: fmt.Sprintf(\"%s[%s]\", t.trace, it.GetResolution()),\n+}\n }\n \n func (t *stringResolutionTracer) AppendString(subTrace string) resolutionTracer {\n-\treturn &stringResolutionTracer{\n-\t\ttrace: fmt.Sprintf(\"%s%s.\", t.trace, subTrace),\n-\t}\n+return &stringResolutionTracer{\n+trace: fmt.Sprintf(\"%s%s.\", t.trace, subTrace),\n+}\n }\n \n func (t *stringResolutionTracer) AppendTupleToUserset() resolutionTracer {\n-\treturn t.AppendString(\"(tuple-to-userset)\")\n+return t.AppendString(\"(tuple-to-userset)\")\n }\n \n func (t *stringResolutionTracer) AppendUnion() resolutionTracer {\n-\treturn t.AppendString(\"union\")\n+return t.AppendString(\"union\")\n }\n \n func (t *stringResolutionTracer) CreateIntersectionTracer() intersectionTracer {\n-\treturn &stringIntersectionTracer{}\n+return &stringIntersectionTracer{}\n }\n \n func (t *stringResolutionTracer) GetResolution() string {\n-\treturn t.trace\n+return t.trace\n }\n \n // stringIntersectionTracer is NOT thread safe. do not call from multiple threads\n type stringIntersectionTracer struct {\n-\ttrace string\n+trace string\n }\n \n func (t *stringIntersectionTracer) AppendTrace(rt resolutionTracer) {\n-\tif len(t.trace) != 0 {\n-\t\tt.trace = fmt.Sprintf(\"%s,%s\", t.trace, rt.GetResolution())\n-\t\treturn\n-\t}\n+if len(t.trace) != 0 {\n+t.trace = fmt.Sprintf(\"%s,%s\", t.trace, rt.GetResolution())\n+return\n+}\n \n-\tt.trace = rt.GetResolution()\n+t.trace = rt.GetResolution()\n }\n \n func (t *stringIntersectionTracer) GetResolution() string {\n-\treturn t.trace\n+return t.trace\n }\n \n type userSet struct {\n-\tm sync.Mutex\n-\tu map[string]resolutionTracer\n+m sync.Mutex\n+u map[string]resolutionTracer\n }\n \n type userWithTracer struct {\n-\tu string\n-\tr resolutionTracer\n+u string\n+r resolutionTracer\n }\n \n func (u *userSet) Add(r resolutionTracer, values ...string) {\n-\tu.m.Lock()\n-\tfor _, v := range values {\n-\t\tu.u[v] = r\n-\t}\n-\tu.m.Unlock()\n+u.m.Lock()\n+for _, v := range values {\n+u.u[v] = r\n+}\n+u.m.Unlock()\n }\n \n func (u *userSet) AddFrom(other *userSet) {\n-\tu.m.Lock()\n-\tfor _, uwr := range other.AsSlice() {\n-\t\tu.u[uwr.u] = uwr.r\n-\t}\n-\tu.m.Unlock()\n+u.m.Lock()\n+for _, uwr := range other.AsSlice() {\n+u.u[uwr.u] = uwr.r\n+}\n+u.m.Unlock()\n }\n \n func (u *userSet) DeleteFrom(other *userSet) {\n-\tu.m.Lock()\n-\tfor _, uwr := range other.AsSlice() {\n-\t\tdelete(u.u, uwr.u)\n-\t}\n-\tu.m.Unlock()\n+u.m.Lock()\n+for _, uwr := range other.AsSlice() {\n+delete(u.u, uwr.u)\n+}\n+u.m.Unlock()\n }\n \n func (u *userSet) Get(value string) (resolutionTracer, bool) {\n-\tu.m.Lock()\n-\tdefer u.m.Unlock()\n+u.m.Lock()\n+defer u.m.Unlock()\n \n-\tvar found bool\n-\tvar rt resolutionTracer\n-\tif rt, found = u.u[value]; !found {\n-\t\tif rt, found = u.u[tupleUtils.Wildcard]; !found {\n-\t\t\treturn nil, false\n-\t\t}\n-\t}\n-\treturn rt, found\n+var found bool\n+var rt resolutionTracer\n+if rt, found = u.u[value]; !found {\n+if rt, found = u.u[tupleUtils.Wildcard]; !found {\n+return nil, false\n+}\n+}\n+return rt, found\n }\n \n func (u *userSet) AsSlice() []userWithTracer {\n-\tu.m.Lock()\n-\tout := make([]userWithTracer, 0, len(u.u))\n-\tfor u, rt := range u.u {\n-\t\tout = append(out, userWithTracer{\n-\t\t\tu: u,\n-\t\t\tr: rt,\n-\t\t})\n-\t}\n-\tu.m.Unlock()\n-\treturn out\n+u.m.Lock()\n+out := make([]userWithTracer, 0, len(u.u))\n+for u, rt := range u.u {\n+out = append(out, userWithTracer{\n+u: u,\n+r: rt,\n+})\n+}\n+u.m.Unlock()\n+return out\n }\n \n func newUserSet() *userSet {\n-\treturn &userSet{u: make(map[string]resolutionTracer)}\n+return &userSet{u: make(map[string]resolutionTracer)}\n }\n \n type userSets struct {\n-\tmu  sync.Mutex\n-\tusm map[int]*userSet\n+mu  sync.Mutex\n+usm map[int]*userSet\n }\n \n func newUserSets() *userSets {\n-\treturn &userSets{usm: make(map[int]*userSet, 0)}\n+return &userSets{usm: make(map[int]*userSet, 0)}\n }\n \n func (u *userSets) Set(idx int, us *userSet) {\n-\tu.mu.Lock()\n-\tu.usm[idx] = us\n-\tu.mu.Unlock()\n+u.mu.Lock()\n+u.usm[idx] = us\n+u.mu.Unlock()\n }\n \n func (u *userSets) Get(idx int) (*userSet, bool) {\n-\tu.mu.Lock()\n-\tus, ok := u.usm[idx]\n-\tu.mu.Unlock()\n-\treturn us, ok\n+u.mu.Lock()\n+us, ok := u.usm[idx]\n+u.mu.Unlock()\n+return us, ok\n }\n \n func (u *userSets) AsMap() map[int]*userSet {\n-\treturn u.usm\n+return u.usm\n }\n \n type chanResolveResult struct {\n-\terr   error\n-\tfound bool\n+err   error\n+found bool\n }\n \n type circuitBreaker struct {\n-\tmu           sync.Mutex\n-\tbreakerState bool\n+mu           sync.Mutex\n+breakerState bool\n }\n \n func (sc *circuitBreaker) Open() {\n-\tsc.mu.Lock()\n-\tdefer sc.mu.Unlock()\n-\tsc.breakerState = true\n+sc.mu.Lock()\n+defer sc.mu.Unlock()\n+sc.breakerState = true\n }\n \n func (sc *circuitBreaker) IsOpen() bool {\n-\tsc.mu.Lock()\n-\tdefer sc.mu.Unlock()\n-\treturn sc.breakerState\n+sc.mu.Lock()\n+defer sc.mu.Unlock()\n+return sc.breakerState\n }\n \n type resolutionContext struct {\n-\tstore            string\n-\tmodel            *openfgapb.AuthorizationModel\n-\tusers            *userSet\n-\ttargetUser       string\n-\ttk               *openfgapb.TupleKey\n-\tcontextualTuples *contextualtuples.ContextualTuples\n-\ttracer           resolutionTracer\n-\tmetadata         *utils.ResolutionMetadata\n-\tinternalCB       *circuitBreaker // Opens if the user is found, controlled internally. Primarily used for UNION.\n-\texternalCB       *circuitBreaker // Open is controlled from caller, Used for Difference and Intersection.\n+store            string\n+model            *openfgapb.AuthorizationModel\n+users            *userSet\n+targetUser       string\n+tk               *openfgapb.TupleKey\n+contextualTuples *contextualtuples.ContextualTuples\n+tracer           resolutionTracer\n+metadata         *utils.ResolutionMetadata\n+internalCB       *circuitBreaker // Opens if the user is found, controlled internally. Primarily used for UNION.\n+externalCB       *circuitBreaker // Open is controlled from caller, Used for Difference and Intersection.\n }\n \n func newResolutionContext(store string, model *openfgapb.AuthorizationModel, tk *openfgapb.TupleKey, contextualTuples *contextualtuples.ContextualTuples, tracer resolutionTracer, metadata *utils.ResolutionMetadata, externalBreaker *circuitBreaker) *resolutionContext {\n-\treturn &resolutionContext{\n-\t\tstore:            store,\n-\t\tmodel:            model,\n-\t\tusers:            newUserSet(),\n-\t\ttargetUser:       tk.GetUser(),\n-\t\ttk:               tk,\n-\t\tcontextualTuples: contextualTuples,\n-\t\ttracer:           tracer,\n-\t\tmetadata:         metadata,\n-\t\tinternalCB:       &circuitBreaker{breakerState: false},\n-\t\texternalCB:       externalBreaker,\n-\t}\n+return &resolutionContext{\n+store:            store,\n+model:            model,\n+users:            newUserSet(),\n+targetUser:       tk.GetUser(),\n+tk:               tk,\n+contextualTuples: contextualTuples,\n+tracer:           tracer,\n+metadata:         metadata,\n+internalCB:       &circuitBreaker{breakerState: false},\n+externalCB:       externalBreaker,\n+}\n }\n \n func (rc *resolutionContext) shouldShortCircuit() bool {\n-\tif rc.internalCB.IsOpen() || rc.externalCB.IsOpen() {\n-\t\treturn true\n-\t}\n-\treturn rc.userFound()\n+if rc.internalCB.IsOpen() || rc.externalCB.IsOpen() {\n+return true\n+}\n+return rc.userFound()\n }\n \n func (rc *resolutionContext) shortCircuit() {\n-\trc.internalCB.Open()\n+rc.internalCB.Open()\n }\n \n func (rc *resolutionContext) userFound() bool {\n-\t_, ok := rc.users.Get(rc.targetUser)\n-\tif ok {\n-\t\trc.shortCircuit()\n-\t}\n-\treturn ok\n+_, ok := rc.users.Get(rc.targetUser)\n+if ok {\n+rc.shortCircuit()\n+}\n+return ok\n }\n \n func (rc *resolutionContext) fork(tk *openfgapb.TupleKey, tracer resolutionTracer, resetResolveCounter bool) *resolutionContext {\n-\tmetadata := rc.metadata\n-\tif resetResolveCounter {\n-\t\tmetadata = rc.metadata.Fork()\n-\t}\n-\n-\treturn &resolutionContext{\n-\t\tstore:            rc.store,\n-\t\tmodel:            rc.model,\n-\t\tusers:            rc.users,\n-\t\ttargetUser:       rc.targetUser,\n-\t\ttk:               tk,\n-\t\tcontextualTuples: rc.contextualTuples,\n-\t\ttracer:           tracer,\n-\t\tmetadata:         metadata,\n-\t\tinternalCB:       rc.internalCB,\n-\t\texternalCB:       rc.externalCB,\n-\t}\n+metadata := rc.metadata\n+if resetResolveCounter {\n+metadata = rc.metadata.Fork()\n+}\n+\n+return &resolutionContext{\n+store:            rc.store,\n+model:            rc.model,\n+users:            rc.users,\n+targetUser:       rc.targetUser,\n+tk:               tk,\n+contextualTuples: rc.contextualTuples,\n+tracer:           tracer,\n+metadata:         metadata,\n+internalCB:       rc.internalCB,\n+externalCB:       rc.externalCB,\n+}\n }\n \n func (rc *resolutionContext) readUserTuple(ctx context.Context, backend storage.TupleBackend) (*openfgapb.TupleKey, error) {\n-\ttk, ok := rc.contextualTuples.ReadUserTuple(rc.tk)\n-\tif ok {\n-\t\treturn tk, nil\n-\t}\n+tuple, err := backend.ReadUserTuple(ctx, rc.store, rc.tk)\n+if err != nil {\n+if errors.Is(err, storage.ErrNotFound) {\n+if tk, ok := rc.contextualTuples.ReadUserTuple(rc.tk); ok {\n+return tk, nil\n+}\n+}\n \n-\ttuple, err := backend.ReadUserTuple(ctx, rc.store, rc.tk)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn tuple.GetKey(), nil\n+return nil, err\n+}\n+\n+return tuple.GetKey(), nil\n }\n \n func (rc *resolutionContext) readUsersetTuples(ctx context.Context, backend storage.TupleBackend) (storage.TupleKeyIterator, error) {\n-\tcUsersetTuples := rc.contextualTuples.ReadUsersetTuples(rc.tk)\n-\tusersetTuples, err := backend.ReadUsersetTuples(ctx, rc.store, rc.tk)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+cUsersetTuples := rc.contextualTuples.ReadUsersetTuples(rc.tk)\n+usersetTuples, err := backend.ReadUsersetTuples(ctx, rc.store, rc.tk)\n+if err != nil {\n+return nil, err\n+}\n \n-\titer1 := storage.NewStaticTupleKeyIterator(cUsersetTuples)\n-\titer2 := storage.NewTupleKeyIteratorFromTupleIterator(usersetTuples)\n+iter1 := storage.NewStaticTupleKeyIterator(cUsersetTuples)\n+iter2 := storage.NewTupleKeyIteratorFromTupleIterator(usersetTuples)\n \n-\treturn storage.NewFilteredTupleKeyIterator(\n-\t\tstorage.NewCombinedIterator(iter1, iter2),\n-\t\tvalidation.FilterInvalidTuples(rc.model),\n-\t), nil\n+return storage.NewFilteredTupleKeyIterator(\n+storage.NewCombinedIterator(iter1, iter2),\n+validation.FilterInvalidTuples(rc.model),\n+), nil\n }\n \n func (rc *resolutionContext) read(ctx context.Context, backend storage.TupleBackend, tk *openfgapb.TupleKey) (storage.TupleKeyIterator, error) {\n-\tcTuples := rc.contextualTuples.Read(tk)\n-\ttuples, err := backend.Read(ctx, rc.store, tk)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\titer1 := storage.NewStaticTupleKeyIterator(cTuples)\n-\titer2 := storage.NewTupleKeyIteratorFromTupleIterator(tuples)\n-\n-\treturn storage.NewFilteredTupleKeyIterator(\n-\t\tstorage.NewCombinedIterator(iter1, iter2),\n-\t\tvalidation.FilterInvalidTuples(rc.model),\n-\t), nil\n+cTuples := rc.contextualTuples.Read(tk)\n+tuples, err := backend.Read(ctx, rc.store, tk)\n+if err != nil {\n+return nil, err\n+}\n+\n+iter1 := storage.NewStaticTupleKeyIterator(cTuples)\n+iter2 := storage.NewTupleKeyIteratorFromTupleIterator(tuples)\n+\n+return storage.NewFilteredTupleKeyIterator(\n+storage.NewCombinedIterator(iter1, iter2),\n+validation.FilterInvalidTuples(rc.model),\n+), nil\n }\n"}
{"cve":"CVE-2024-1724:0708", "fix_patch": "diff --git a/interfaces/builtin/home.go b/interfaces/builtin/home.go\nindex 998ca254f0..19fbf00477 100644\n--- a/interfaces/builtin/home.go\n+++ b/interfaces/builtin/home.go\n@@ -20,11 +20,11 @@\n package builtin\n \n import (\n-\t\"fmt\"\n+        \"fmt\"\n \n-\t\"github.com/snapcore/snapd/interfaces\"\n-\t\"github.com/snapcore/snapd/interfaces/apparmor\"\n-\t\"github.com/snapcore/snapd/snap\"\n+        \"github.com/snapcore/snapd/interfaces\"\n+        \"github.com/snapcore/snapd/interfaces/apparmor\"\n+        \"github.com/snapcore/snapd/snap\"\n )\n \n const homeSummary = `allows access to non-hidden files in the home directory`\n@@ -78,7 +78,7 @@ owner /run/user/[0-9]*/gvfs/*/**  w,\n \n # Disallow writes to the well-known directory included in\n # the user's PATH on several distributions\n-audit deny @{HOME}/bin/{,**} wl,\n+deny @{HOME}/bin/{,**} wl,\n `\n \n const homeConnectedPlugAppArmorWithAllRead = `\n@@ -94,39 +94,39 @@ capability dac_read_search,\n `\n \n type homeInterface struct {\n-\tcommonInterface\n+        commonInterface\n }\n \n func (iface *homeInterface) BeforePreparePlug(plug *snap.PlugInfo) error {\n-\t// It's fine if 'read' isn't specified, but if it is, it needs to be\n-\t// 'all'\n-\tif r, ok := plug.Attrs[\"read\"]; ok && r != \"all\" {\n-\t\treturn fmt.Errorf(`home plug requires \"read\" be 'all'`)\n-\t}\n+        // It's fine if 'read' isn't specified, but if it is, it needs to be\n+        // 'all'\n+        if r, ok := plug.Attrs[\"read\"]; ok && r != \"all\" {\n+                return fmt.Errorf(`home plug requires \"read\" be 'all'`)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func (iface *homeInterface) AppArmorConnectedPlug(spec *apparmor.Specification, plug *interfaces.ConnectedPlug, slot *interfaces.ConnectedSlot) error {\n-\tvar read string\n-\t_ = plug.Attr(\"read\", &read)\n-\t// 'owner' is the standard policy\n-\tspec.AddSnippet(homeConnectedPlugAppArmor)\n-\n-\t// 'all' grants standard policy plus read access to home without owner\n-\t// match\n-\tif read == \"all\" {\n-\t\tspec.AddSnippet(homeConnectedPlugAppArmorWithAllRead)\n-\t}\n-\treturn nil\n+        var read string\n+        _ = plug.Attr(\"read\", &read)\n+        // 'owner' is the standard policy\n+        spec.AddSnippet(homeConnectedPlugAppArmor)\n+\n+        // 'all' grants standard policy plus read access to home without owner\n+        // match\n+        if read == \"all\" {\n+                spec.AddSnippet(homeConnectedPlugAppArmorWithAllRead)\n+        }\n+        return nil\n }\n \n func init() {\n-\tregisterIface(&homeInterface{commonInterface{\n-\t\tname:                 \"home\",\n-\t\tsummary:              homeSummary,\n-\t\timplicitOnCore:       true,\n-\t\timplicitOnClassic:    true,\n-\t\tbaseDeclarationSlots: homeBaseDeclarationSlots,\n-\t}})\n+        registerIface(&homeInterface{commonInterface{\n+                name:                 \"home\",\n+                summary:              homeSummary,\n+                implicitOnCore:       true,\n+                implicitOnClassic:    true,\n+                baseDeclarationSlots: homeBaseDeclarationSlots,\n+        }})\n }\n"}
{"cve":"CVE-2023-22736:0708", "fix_patch": "diff --git a/controller/appcontroller.go b/controller/appcontroller.go\nindex a989c5a16..55b704ab1 100644\n--- a/controller/appcontroller.go\n+++ b/controller/appcontroller.go\n@@ -1,1472 +1,1472 @@\n package controller\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"math\"\n-\t\"net/http\"\n-\t\"reflect\"\n-\t\"runtime/debug\"\n-\t\"sort\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\tclustercache \"github.com/argoproj/gitops-engine/pkg/cache\"\n-\t\"github.com/argoproj/gitops-engine/pkg/diff\"\n-\t\"github.com/argoproj/gitops-engine/pkg/health\"\n-\tsynccommon \"github.com/argoproj/gitops-engine/pkg/sync/common\"\n-\t\"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n-\tjsonpatch \"github.com/evanphx/json-patch\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"golang.org/x/sync/semaphore\"\n-\tv1 \"k8s.io/api/core/v1\"\n-\tapierr \"k8s.io/apimachinery/pkg/api/errors\"\n-\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n-\t\"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n-\t\"k8s.io/apimachinery/pkg/labels\"\n-\tapiruntime \"k8s.io/apimachinery/pkg/runtime\"\n-\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n-\t\"k8s.io/apimachinery/pkg/types\"\n-\t\"k8s.io/apimachinery/pkg/util/runtime\"\n-\t\"k8s.io/apimachinery/pkg/util/wait\"\n-\t\"k8s.io/apimachinery/pkg/watch\"\n-\t\"k8s.io/client-go/kubernetes\"\n-\t\"k8s.io/client-go/tools/cache\"\n-\t\"k8s.io/client-go/util/workqueue\"\n-\n-\tstatecache \"github.com/argoproj/argo-cd/v2/controller/cache\"\n-\t\"github.com/argoproj/argo-cd/v2/controller/metrics\"\n-\t\"github.com/argoproj/argo-cd/v2/pkg/apis/application\"\n-\tappv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n-\tappclientset \"github.com/argoproj/argo-cd/v2/pkg/client/clientset/versioned\"\n-\t\"github.com/argoproj/argo-cd/v2/pkg/client/informers/externalversions/application/v1alpha1\"\n-\tapplisters \"github.com/argoproj/argo-cd/v2/pkg/client/listers/application/v1alpha1\"\n-\t\"github.com/argoproj/argo-cd/v2/reposerver/apiclient\"\n-\t\"github.com/argoproj/argo-cd/v2/util/argo\"\n-\targodiff \"github.com/argoproj/argo-cd/v2/util/argo/diff\"\n-\tappstatecache \"github.com/argoproj/argo-cd/v2/util/cache/appstate\"\n-\t\"github.com/argoproj/argo-cd/v2/util/db\"\n-\t\"github.com/argoproj/argo-cd/v2/util/errors\"\n-\t\"github.com/argoproj/argo-cd/v2/util/glob\"\n-\tlogutils \"github.com/argoproj/argo-cd/v2/util/log\"\n-\tsettings_util \"github.com/argoproj/argo-cd/v2/util/settings\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"math\"\n+        \"net/http\"\n+        \"reflect\"\n+        \"runtime/debug\"\n+        \"sort\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        clustercache \"github.com/argoproj/gitops-engine/pkg/cache\"\n+        \"github.com/argoproj/gitops-engine/pkg/diff\"\n+        \"github.com/argoproj/gitops-engine/pkg/health\"\n+        synccommon \"github.com/argoproj/gitops-engine/pkg/sync/common\"\n+        \"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n+        jsonpatch \"github.com/evanphx/json-patch\"\n+        log \"github.com/sirupsen/logrus\"\n+        \"golang.org/x/sync/semaphore\"\n+        v1 \"k8s.io/api/core/v1\"\n+        apierr \"k8s.io/apimachinery/pkg/api/errors\"\n+        metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+        \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n+        \"k8s.io/apimachinery/pkg/labels\"\n+        apiruntime \"k8s.io/apimachinery/pkg/runtime\"\n+        \"k8s.io/apimachinery/pkg/runtime/schema\"\n+        \"k8s.io/apimachinery/pkg/types\"\n+        \"k8s.io/apimachinery/pkg/util/runtime\"\n+        \"k8s.io/apimachinery/pkg/util/wait\"\n+        \"k8s.io/apimachinery/pkg/watch\"\n+        \"k8s.io/client-go/kubernetes\"\n+        \"k8s.io/client-go/tools/cache\"\n+        \"k8s.io/client-go/util/workqueue\"\n+\n+        statecache \"github.com/argoproj/argo-cd/v2/controller/cache\"\n+        \"github.com/argoproj/argo-cd/v2/controller/metrics\"\n+        \"github.com/argoproj/argo-cd/v2/pkg/apis/application\"\n+        appv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n+        appclientset \"github.com/argoproj/argo-cd/v2/pkg/client/clientset/versioned\"\n+        \"github.com/argoproj/argo-cd/v2/pkg/client/informers/externalversions/application/v1alpha1\"\n+        applisters \"github.com/argoproj/argo-cd/v2/pkg/client/listers/application/v1alpha1\"\n+        \"github.com/argoproj/argo-cd/v2/reposerver/apiclient\"\n+        \"github.com/argoproj/argo-cd/v2/util/argo\"\n+        argodiff \"github.com/argoproj/argo-cd/v2/util/argo/diff\"\n+        appstatecache \"github.com/argoproj/argo-cd/v2/util/cache/appstate\"\n+        \"github.com/argoproj/argo-cd/v2/util/db\"\n+        \"github.com/argoproj/argo-cd/v2/util/errors\"\n+        \"github.com/argoproj/argo-cd/v2/util/glob\"\n+        logutils \"github.com/argoproj/argo-cd/v2/util/log\"\n+        settings_util \"github.com/argoproj/argo-cd/v2/util/settings\"\n )\n \n const (\n-\tupdateOperationStateTimeout = 1 * time.Second\n-\t// orphanedIndex contains application which monitor orphaned resources by namespace\n-\torphanedIndex = \"orphaned\"\n+        updateOperationStateTimeout = 1 * time.Second\n+        // orphanedIndex contains application which monitor orphaned resources by namespace\n+        orphanedIndex = \"orphaned\"\n )\n \n type CompareWith int\n \n const (\n-\t// Compare live application state against state defined in latest git revision with no resolved revision caching.\n-\tCompareWithLatestForceResolve CompareWith = 3\n-\t// Compare live application state against state defined in latest git revision.\n-\tCompareWithLatest CompareWith = 2\n-\t// Compare live application state against state defined using revision of most recent comparison.\n-\tCompareWithRecent CompareWith = 1\n-\t// Skip comparison and only refresh application resources tree\n-\tComparisonWithNothing CompareWith = 0\n+        // Compare live application state against state defined in latest git revision with no resolved revision caching.\n+        CompareWithLatestForceResolve CompareWith = 3\n+        // Compare live application state against state defined in latest git revision.\n+        CompareWithLatest CompareWith = 2\n+        // Compare live application state against state defined using revision of most recent comparison.\n+        CompareWithRecent CompareWith = 1\n+        // Skip comparison and only refresh application resources tree\n+        ComparisonWithNothing CompareWith = 0\n )\n \n func (a CompareWith) Max(b CompareWith) CompareWith {\n-\treturn CompareWith(math.Max(float64(a), float64(b)))\n+        return CompareWith(math.Max(float64(a), float64(b)))\n }\n \n func (a CompareWith) Pointer() *CompareWith {\n-\treturn &a\n+        return &a\n }\n \n // ApplicationController is the controller for application resources.\n type ApplicationController struct {\n-\tcache                *appstatecache.Cache\n-\tnamespace            string\n-\tkubeClientset        kubernetes.Interface\n-\tkubectl              kube.Kubectl\n-\tapplicationClientset appclientset.Interface\n-\tauditLogger          *argo.AuditLogger\n-\t// queue contains app namespace/name\n-\tappRefreshQueue workqueue.RateLimitingInterface\n-\t// queue contains app namespace/name/comparisonType and used to request app refresh with the predefined comparison type\n-\tappComparisonTypeRefreshQueue workqueue.RateLimitingInterface\n-\tappOperationQueue             workqueue.RateLimitingInterface\n-\tprojectRefreshQueue           workqueue.RateLimitingInterface\n-\tappInformer                   cache.SharedIndexInformer\n-\tappLister                     applisters.ApplicationLister\n-\tprojInformer                  cache.SharedIndexInformer\n-\tappStateManager               AppStateManager\n-\tstateCache                    statecache.LiveStateCache\n-\tstatusRefreshTimeout          time.Duration\n-\tstatusHardRefreshTimeout      time.Duration\n-\tselfHealTimeout               time.Duration\n-\trepoClientset                 apiclient.Clientset\n-\tdb                            db.ArgoDB\n-\tsettingsMgr                   *settings_util.SettingsManager\n-\trefreshRequestedApps          map[string]CompareWith\n-\trefreshRequestedAppsMutex     *sync.Mutex\n-\tmetricsServer                 *metrics.MetricsServer\n-\tkubectlSemaphore              *semaphore.Weighted\n-\tclusterFilter                 func(cluster *appv1.Cluster) bool\n-\tprojByNameCache               sync.Map\n-\tapplicationNamespaces         []string\n+        cache                *appstatecache.Cache\n+        namespace            string\n+        kubeClientset        kubernetes.Interface\n+        kubectl              kube.Kubectl\n+        applicationClientset appclientset.Interface\n+        auditLogger          *argo.AuditLogger\n+        // queue contains app namespace/name\n+        appRefreshQueue workqueue.RateLimitingInterface\n+        // queue contains app namespace/name/comparisonType and used to request app refresh with the predefined comparison type\n+        appComparisonTypeRefreshQueue workqueue.RateLimitingInterface\n+        appOperationQueue             workqueue.RateLimitingInterface\n+        projectRefreshQueue           workqueue.RateLimitingInterface\n+        appInformer                   cache.SharedIndexInformer\n+        appLister                     applisters.ApplicationLister\n+        projInformer                  cache.SharedIndexInformer\n+        appStateManager               AppStateManager\n+        stateCache                    statecache.LiveStateCache\n+        statusRefreshTimeout          time.Duration\n+        statusHardRefreshTimeout      time.Duration\n+        selfHealTimeout               time.Duration\n+        repoClientset                 apiclient.Clientset\n+        db                            db.ArgoDB\n+        settingsMgr                   *settings_util.SettingsManager\n+        refreshRequestedApps          map[string]CompareWith\n+        refreshRequestedAppsMutex     *sync.Mutex\n+        metricsServer                 *metrics.MetricsServer\n+        kubectlSemaphore              *semaphore.Weighted\n+        clusterFilter                 func(cluster *appv1.Cluster) bool\n+        projByNameCache               sync.Map\n+        applicationNamespaces         []string\n }\n \n // NewApplicationController creates new instance of ApplicationController.\n func NewApplicationController(\n-\tnamespace string,\n-\tsettingsMgr *settings_util.SettingsManager,\n-\tkubeClientset kubernetes.Interface,\n-\tapplicationClientset appclientset.Interface,\n-\trepoClientset apiclient.Clientset,\n-\targoCache *appstatecache.Cache,\n-\tkubectl kube.Kubectl,\n-\tappResyncPeriod time.Duration,\n-\tappHardResyncPeriod time.Duration,\n-\tselfHealTimeout time.Duration,\n-\tmetricsPort int,\n-\tmetricsCacheExpiration time.Duration,\n-\tmetricsApplicationLabels []string,\n-\tkubectlParallelismLimit int64,\n-\tpersistResourceHealth bool,\n-\tclusterFilter func(cluster *appv1.Cluster) bool,\n-\tapplicationNamespaces []string,\n+        namespace string,\n+        settingsMgr *settings_util.SettingsManager,\n+        kubeClientset kubernetes.Interface,\n+        applicationClientset appclientset.Interface,\n+        repoClientset apiclient.Clientset,\n+        argoCache *appstatecache.Cache,\n+        kubectl kube.Kubectl,\n+        appResyncPeriod time.Duration,\n+        appHardResyncPeriod time.Duration,\n+        selfHealTimeout time.Duration,\n+        metricsPort int,\n+        metricsCacheExpiration time.Duration,\n+        metricsApplicationLabels []string,\n+        kubectlParallelismLimit int64,\n+        persistResourceHealth bool,\n+        clusterFilter func(cluster *appv1.Cluster) bool,\n+        applicationNamespaces []string,\n ) (*ApplicationController, error) {\n-\tlog.Infof(\"appResyncPeriod=%v, appHardResyncPeriod=%v\", appResyncPeriod, appHardResyncPeriod)\n-\tdb := db.NewDB(namespace, settingsMgr, kubeClientset)\n-\tctrl := ApplicationController{\n-\t\tcache:                         argoCache,\n-\t\tnamespace:                     namespace,\n-\t\tkubeClientset:                 kubeClientset,\n-\t\tkubectl:                       kubectl,\n-\t\tapplicationClientset:          applicationClientset,\n-\t\trepoClientset:                 repoClientset,\n-\t\tappRefreshQueue:               workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"app_reconciliation_queue\"),\n-\t\tappOperationQueue:             workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"app_operation_processing_queue\"),\n-\t\tprojectRefreshQueue:           workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"project_reconciliation_queue\"),\n-\t\tappComparisonTypeRefreshQueue: workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()),\n-\t\tdb:                            db,\n-\t\tstatusRefreshTimeout:          appResyncPeriod,\n-\t\tstatusHardRefreshTimeout:      appHardResyncPeriod,\n-\t\trefreshRequestedApps:          make(map[string]CompareWith),\n-\t\trefreshRequestedAppsMutex:     &sync.Mutex{},\n-\t\tauditLogger:                   argo.NewAuditLogger(namespace, kubeClientset, \"argocd-application-controller\"),\n-\t\tsettingsMgr:                   settingsMgr,\n-\t\tselfHealTimeout:               selfHealTimeout,\n-\t\tclusterFilter:                 clusterFilter,\n-\t\tprojByNameCache:               sync.Map{},\n-\t\tapplicationNamespaces:         applicationNamespaces,\n-\t}\n-\tif kubectlParallelismLimit > 0 {\n-\t\tctrl.kubectlSemaphore = semaphore.NewWeighted(kubectlParallelismLimit)\n-\t}\n-\tkubectl.SetOnKubectlRun(ctrl.onKubectlRun)\n-\tappInformer, appLister := ctrl.newApplicationInformerAndLister()\n-\tindexers := cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}\n-\tprojInformer := v1alpha1.NewAppProjectInformer(applicationClientset, namespace, appResyncPeriod, indexers)\n-\tprojInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{\n-\t\tAddFunc: func(obj interface{}) {\n-\t\t\tif key, err := cache.MetaNamespaceKeyFunc(obj); err == nil {\n-\t\t\t\tctrl.projectRefreshQueue.Add(key)\n-\t\t\t\tif projMeta, ok := obj.(metav1.Object); ok {\n-\t\t\t\t\tctrl.InvalidateProjectsCache(projMeta.GetName())\n-\t\t\t\t}\n-\n-\t\t\t}\n-\t\t},\n-\t\tUpdateFunc: func(old, new interface{}) {\n-\t\t\tif key, err := cache.MetaNamespaceKeyFunc(new); err == nil {\n-\t\t\t\tctrl.projectRefreshQueue.Add(key)\n-\t\t\t\tif projMeta, ok := new.(metav1.Object); ok {\n-\t\t\t\t\tctrl.InvalidateProjectsCache(projMeta.GetName())\n-\t\t\t\t}\n-\t\t\t}\n-\t\t},\n-\t\tDeleteFunc: func(obj interface{}) {\n-\t\t\tif key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj); err == nil {\n-\t\t\t\tctrl.projectRefreshQueue.Add(key)\n-\t\t\t\tif projMeta, ok := obj.(metav1.Object); ok {\n-\t\t\t\t\tctrl.InvalidateProjectsCache(projMeta.GetName())\n-\t\t\t\t}\n-\t\t\t}\n-\t\t},\n-\t})\n-\tmetricsAddr := fmt.Sprintf(\"0.0.0.0:%d\", metricsPort)\n-\tvar err error\n-\tctrl.metricsServer, err = metrics.NewMetricsServer(metricsAddr, appLister, ctrl.canProcessApp, func(r *http.Request) error {\n-\t\treturn nil\n-\t}, metricsApplicationLabels)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif metricsCacheExpiration.Seconds() != 0 {\n-\t\terr = ctrl.metricsServer.SetExpiration(metricsCacheExpiration)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\tstateCache := statecache.NewLiveStateCache(db, appInformer, ctrl.settingsMgr, kubectl, ctrl.metricsServer, ctrl.handleObjectUpdated, clusterFilter, argo.NewResourceTracking())\n-\tappStateManager := NewAppStateManager(db, applicationClientset, repoClientset, namespace, kubectl, ctrl.settingsMgr, stateCache, projInformer, ctrl.metricsServer, argoCache, ctrl.statusRefreshTimeout, argo.NewResourceTracking(), persistResourceHealth)\n-\tctrl.appInformer = appInformer\n-\tctrl.appLister = appLister\n-\tctrl.projInformer = projInformer\n-\tctrl.appStateManager = appStateManager\n-\tctrl.stateCache = stateCache\n-\n-\treturn &ctrl, nil\n+        log.Infof(\"appResyncPeriod=%v, appHardResyncPeriod=%v\", appResyncPeriod, appHardResyncPeriod)\n+        db := db.NewDB(namespace, settingsMgr, kubeClientset)\n+        ctrl := ApplicationController{\n+                cache:                         argoCache,\n+                namespace:                     namespace,\n+                kubeClientset:                 kubeClientset,\n+                kubectl:                       kubectl,\n+                applicationClientset:          applicationClientset,\n+                repoClientset:                 repoClientset,\n+                appRefreshQueue:               workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"app_reconciliation_queue\"),\n+                appOperationQueue:             workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"app_operation_processing_queue\"),\n+                projectRefreshQueue:           workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"project_reconciliation_queue\"),\n+                appComparisonTypeRefreshQueue: workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()),\n+                db:                            db,\n+                statusRefreshTimeout:          appResyncPeriod,\n+                statusHardRefreshTimeout:      appHardResyncPeriod,\n+                refreshRequestedApps:          make(map[string]CompareWith),\n+                refreshRequestedAppsMutex:     &sync.Mutex{},\n+                auditLogger:                   argo.NewAuditLogger(namespace, kubeClientset, \"argocd-application-controller\"),\n+                settingsMgr:                   settingsMgr,\n+                selfHealTimeout:               selfHealTimeout,\n+                clusterFilter:                 clusterFilter,\n+                projByNameCache:               sync.Map{},\n+                applicationNamespaces:         applicationNamespaces,\n+        }\n+        if kubectlParallelismLimit > 0 {\n+                ctrl.kubectlSemaphore = semaphore.NewWeighted(kubectlParallelismLimit)\n+        }\n+        kubectl.SetOnKubectlRun(ctrl.onKubectlRun)\n+        appInformer, appLister := ctrl.newApplicationInformerAndLister()\n+        indexers := cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}\n+        projInformer := v1alpha1.NewAppProjectInformer(applicationClientset, namespace, appResyncPeriod, indexers)\n+        projInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{\n+                AddFunc: func(obj interface{}) {\n+                        if key, err := cache.MetaNamespaceKeyFunc(obj); err == nil {\n+                                ctrl.projectRefreshQueue.Add(key)\n+                                if projMeta, ok := obj.(metav1.Object); ok {\n+                                        ctrl.InvalidateProjectsCache(projMeta.GetName())\n+                                }\n+\n+                        }\n+                },\n+                UpdateFunc: func(old, new interface{}) {\n+                        if key, err := cache.MetaNamespaceKeyFunc(new); err == nil {\n+                                ctrl.projectRefreshQueue.Add(key)\n+                                if projMeta, ok := new.(metav1.Object); ok {\n+                                        ctrl.InvalidateProjectsCache(projMeta.GetName())\n+                                }\n+                        }\n+                },\n+                DeleteFunc: func(obj interface{}) {\n+                        if key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj); err == nil {\n+                                ctrl.projectRefreshQueue.Add(key)\n+                                if projMeta, ok := obj.(metav1.Object); ok {\n+                                        ctrl.InvalidateProjectsCache(projMeta.GetName())\n+                                }\n+                        }\n+                },\n+        })\n+        metricsAddr := fmt.Sprintf(\"0.0.0.0:%d\", metricsPort)\n+        var err error\n+        ctrl.metricsServer, err = metrics.NewMetricsServer(metricsAddr, appLister, ctrl.canProcessApp, func(r *http.Request) error {\n+                return nil\n+        }, metricsApplicationLabels)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if metricsCacheExpiration.Seconds() != 0 {\n+                err = ctrl.metricsServer.SetExpiration(metricsCacheExpiration)\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+        stateCache := statecache.NewLiveStateCache(db, appInformer, ctrl.settingsMgr, kubectl, ctrl.metricsServer, ctrl.handleObjectUpdated, clusterFilter, argo.NewResourceTracking())\n+        appStateManager := NewAppStateManager(db, applicationClientset, repoClientset, namespace, kubectl, ctrl.settingsMgr, stateCache, projInformer, ctrl.metricsServer, argoCache, ctrl.statusRefreshTimeout, argo.NewResourceTracking(), persistResourceHealth)\n+        ctrl.appInformer = appInformer\n+        ctrl.appLister = appLister\n+        ctrl.projInformer = projInformer\n+        ctrl.appStateManager = appStateManager\n+        ctrl.stateCache = stateCache\n+\n+        return &ctrl, nil\n }\n \n func (ctrl *ApplicationController) InvalidateProjectsCache(names ...string) {\n-\tif len(names) > 0 {\n-\t\tfor _, name := range names {\n-\t\t\tctrl.projByNameCache.Delete(name)\n-\t\t}\n-\t} else {\n-\t\tctrl.projByNameCache.Range(func(key, _ interface{}) bool {\n-\t\t\tctrl.projByNameCache.Delete(key)\n-\t\t\treturn true\n-\t\t})\n-\t}\n+        if len(names) > 0 {\n+                for _, name := range names {\n+                        ctrl.projByNameCache.Delete(name)\n+                }\n+        } else {\n+                ctrl.projByNameCache.Range(func(key, _ interface{}) bool {\n+                        ctrl.projByNameCache.Delete(key)\n+                        return true\n+                })\n+        }\n }\n \n func (ctrl *ApplicationController) GetMetricsServer() *metrics.MetricsServer {\n-\treturn ctrl.metricsServer\n+        return ctrl.metricsServer\n }\n \n func (ctrl *ApplicationController) onKubectlRun(command string) (kube.CleanupFunc, error) {\n-\tctrl.metricsServer.IncKubectlExec(command)\n-\tif ctrl.kubectlSemaphore != nil {\n-\t\tif err := ctrl.kubectlSemaphore.Acquire(context.Background(), 1); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tctrl.metricsServer.IncKubectlExecPending(command)\n-\t}\n-\treturn func() {\n-\t\tif ctrl.kubectlSemaphore != nil {\n-\t\t\tctrl.kubectlSemaphore.Release(1)\n-\t\t\tctrl.metricsServer.DecKubectlExecPending(command)\n-\t\t}\n-\t}, nil\n+        ctrl.metricsServer.IncKubectlExec(command)\n+        if ctrl.kubectlSemaphore != nil {\n+                if err := ctrl.kubectlSemaphore.Acquire(context.Background(), 1); err != nil {\n+                        return nil, err\n+                }\n+                ctrl.metricsServer.IncKubectlExecPending(command)\n+        }\n+        return func() {\n+                if ctrl.kubectlSemaphore != nil {\n+                        ctrl.kubectlSemaphore.Release(1)\n+                        ctrl.metricsServer.DecKubectlExecPending(command)\n+                }\n+        }, nil\n }\n \n func isSelfReferencedApp(app *appv1.Application, ref v1.ObjectReference) bool {\n-\tgvk := ref.GroupVersionKind()\n-\treturn ref.UID == app.UID &&\n-\t\tref.Name == app.Name &&\n-\t\tref.Namespace == app.Namespace &&\n-\t\tgvk.Group == application.Group &&\n-\t\tgvk.Kind == application.ApplicationKind\n+        gvk := ref.GroupVersionKind()\n+        return ref.UID == app.UID &&\n+                ref.Name == app.Name &&\n+                ref.Namespace == app.Namespace &&\n+                gvk.Group == application.Group &&\n+                gvk.Kind == application.ApplicationKind\n }\n \n func (ctrl *ApplicationController) newAppProjCache(name string) *appProjCache {\n-\treturn &appProjCache{name: name, ctrl: ctrl}\n+        return &appProjCache{name: name, ctrl: ctrl}\n }\n \n type appProjCache struct {\n-\tname string\n-\tctrl *ApplicationController\n+        name string\n+        ctrl *ApplicationController\n \n-\tlock    sync.Mutex\n-\tappProj *appv1.AppProject\n+        lock    sync.Mutex\n+        appProj *appv1.AppProject\n }\n \n // GetAppProject gets an AppProject from the cache. If the AppProject is not\n // yet cached, retrieves the AppProject from the K8s control plane and stores\n // in the cache.\n func (projCache *appProjCache) GetAppProject(ctx context.Context) (*appv1.AppProject, error) {\n-\tprojCache.lock.Lock()\n-\tdefer projCache.lock.Unlock()\n-\tif projCache.appProj != nil {\n-\t\treturn projCache.appProj, nil\n-\t}\n-\tproj, err := argo.GetAppProjectByName(projCache.name, applisters.NewAppProjectLister(projCache.ctrl.projInformer.GetIndexer()), projCache.ctrl.namespace, projCache.ctrl.settingsMgr, projCache.ctrl.db, ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tprojCache.appProj = proj\n-\treturn projCache.appProj, nil\n+        projCache.lock.Lock()\n+        defer projCache.lock.Unlock()\n+        if projCache.appProj != nil {\n+                return projCache.appProj, nil\n+        }\n+        proj, err := argo.GetAppProjectByName(projCache.name, applisters.NewAppProjectLister(projCache.ctrl.projInformer.GetIndexer()), projCache.ctrl.namespace, projCache.ctrl.settingsMgr, projCache.ctrl.db, ctx)\n+        if err != nil {\n+                return nil, err\n+        }\n+        projCache.appProj = proj\n+        return projCache.appProj, nil\n }\n \n // getAppProj gets the AppProject for the given Application app.\n func (ctrl *ApplicationController) getAppProj(app *appv1.Application) (*appv1.AppProject, error) {\n-\tprojCache, _ := ctrl.projByNameCache.LoadOrStore(app.Spec.GetProject(), ctrl.newAppProjCache(app.Spec.GetProject()))\n-\tproj, err := projCache.(*appProjCache).GetAppProject(context.TODO())\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\treturn nil, err\n-\t\t} else {\n-\t\t\treturn nil, fmt.Errorf(\"could not retrieve AppProject '%s' from cache: %v\", app.Spec.Project, err)\n-\t\t}\n-\t}\n-\tif !proj.IsAppNamespacePermitted(app, ctrl.namespace) {\n-\t\treturn nil, argo.ErrProjectNotPermitted(app.GetName(), app.GetNamespace(), proj.GetName())\n-\t}\n-\treturn proj, nil\n+        projCache, _ := ctrl.projByNameCache.LoadOrStore(app.Spec.GetProject(), ctrl.newAppProjCache(app.Spec.GetProject()))\n+        proj, err := projCache.(*appProjCache).GetAppProject(context.TODO())\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        return nil, err\n+                } else {\n+                        return nil, fmt.Errorf(\"could not retrieve AppProject '%s' from cache: %v\", app.Spec.Project, err)\n+                }\n+        }\n+        if !proj.IsAppNamespacePermitted(app, ctrl.namespace) {\n+                return nil, argo.ErrProjectNotPermitted(app.GetName(), app.GetNamespace(), proj.GetName())\n+        }\n+        return proj, nil\n }\n \n func (ctrl *ApplicationController) handleObjectUpdated(managedByApp map[string]bool, ref v1.ObjectReference) {\n-\t// if namespaced resource is not managed by any app it might be orphaned resource of some other apps\n-\tif len(managedByApp) == 0 && ref.Namespace != \"\" {\n-\t\t// retrieve applications which monitor orphaned resources in the same namespace and refresh them unless resource is denied in app project\n-\t\tif objs, err := ctrl.appInformer.GetIndexer().ByIndex(orphanedIndex, ref.Namespace); err == nil {\n-\t\t\tfor i := range objs {\n-\t\t\t\tapp, ok := objs[i].(*appv1.Application)\n-\t\t\t\tif !ok {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\n-\t\t\t\tmanagedByApp[app.InstanceName(ctrl.namespace)] = true\n-\t\t\t}\n-\t\t}\n-\t}\n-\tfor appName, isManagedResource := range managedByApp {\n-\t\t// The appName is given as <namespace>_<name>, but the indexer needs it\n-\t\t// format <namespace>/<name>\n-\t\tappKey := ctrl.toAppKey(appName)\n-\t\tobj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey)\n-\t\tapp, ok := obj.(*appv1.Application)\n-\t\tif exists && err == nil && ok && isSelfReferencedApp(app, ref) {\n-\t\t\t// Don't force refresh app if related resource is application itself. This prevents infinite reconciliation loop.\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif !ctrl.canProcessApp(obj) {\n-\t\t\t// Don't force refresh app if app belongs to a different controller shard\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Enforce application's permission for the source namespace\n-\t\t_, err = ctrl.getAppProj(app)\n-\t\tif err != nil {\n-\t\t\tlog.Errorf(\"Unable to determine project for app '%s': %v\", app.QualifiedName(), err)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tlevel := ComparisonWithNothing\n-\t\tif isManagedResource {\n-\t\t\tlevel = CompareWithRecent\n-\t\t}\n-\n-\t\t// Additional check for debug level so we don't need to evaluate the\n-\t\t// format string in case of non-debug scenarios\n-\t\tif log.GetLevel() >= log.DebugLevel {\n-\t\t\tvar resKey string\n-\t\t\tif ref.Namespace != \"\" {\n-\t\t\t\tresKey = ref.Namespace + \"/\" + ref.Name\n-\t\t\t} else {\n-\t\t\t\tresKey = \"(cluster-scoped)/\" + ref.Name\n-\t\t\t}\n-\t\t\tlog.Debugf(\"Refreshing app %s for change in cluster of object %s of type %s/%s\", appKey, resKey, ref.APIVersion, ref.Kind)\n-\t\t}\n-\n-\t\tctrl.requestAppRefresh(app.QualifiedName(), &level, nil)\n-\t}\n+        // if namespaced resource is not managed by any app it might be orphaned resource of some other apps\n+        if len(managedByApp) == 0 && ref.Namespace != \"\" {\n+                // retrieve applications which monitor orphaned resources in the same namespace and refresh them unless resource is denied in app project\n+                if objs, err := ctrl.appInformer.GetIndexer().ByIndex(orphanedIndex, ref.Namespace); err == nil {\n+                        for i := range objs {\n+                                app, ok := objs[i].(*appv1.Application)\n+                                if !ok {\n+                                        continue\n+                                }\n+\n+                                managedByApp[app.InstanceName(ctrl.namespace)] = true\n+                        }\n+                }\n+        }\n+        for appName, isManagedResource := range managedByApp {\n+                // The appName is given as <namespace>_<name>, but the indexer needs it\n+                // format <namespace>/<name>\n+                appKey := ctrl.toAppKey(appName)\n+                obj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey)\n+                app, ok := obj.(*appv1.Application)\n+                if exists && err == nil && ok && isSelfReferencedApp(app, ref) {\n+                        // Don't force refresh app if related resource is application itself. This prevents infinite reconciliation loop.\n+                        continue\n+                }\n+\n+                if !ctrl.canProcessApp(obj) {\n+                        // Don't force refresh app if app belongs to a different controller shard\n+                        continue\n+                }\n+\n+                // Enforce application's permission for the source namespace\n+                _, err = ctrl.getAppProj(app)\n+                if err != nil {\n+                        log.Errorf(\"Unable to determine project for app '%s': %v\", app.QualifiedName(), err)\n+                        continue\n+                }\n+\n+                level := ComparisonWithNothing\n+                if isManagedResource {\n+                        level = CompareWithRecent\n+                }\n+\n+                // Additional check for debug level so we don't need to evaluate the\n+                // format string in case of non-debug scenarios\n+                if log.GetLevel() >= log.DebugLevel {\n+                        var resKey string\n+                        if ref.Namespace != \"\" {\n+                                resKey = ref.Namespace + \"/\" + ref.Name\n+                        } else {\n+                                resKey = \"(cluster-scoped)/\" + ref.Name\n+                        }\n+                        log.Debugf(\"Refreshing app %s for change in cluster of object %s of type %s/%s\", appKey, resKey, ref.APIVersion, ref.Kind)\n+                }\n+\n+                ctrl.requestAppRefresh(app.QualifiedName(), &level, nil)\n+        }\n }\n \n // setAppManagedResources will build a list of ResourceDiff based on the provided comparisonResult\n // and persist app resources related data in the cache. Will return the persisted ApplicationTree.\n func (ctrl *ApplicationController) setAppManagedResources(a *appv1.Application, comparisonResult *comparisonResult) (*appv1.ApplicationTree, error) {\n-\tmanagedResources, err := ctrl.hideSecretData(a, comparisonResult)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting managed resources: %s\", err)\n-\t}\n-\ttree, err := ctrl.getResourceTree(a, managedResources)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting resource tree: %s\", err)\n-\t}\n-\terr = ctrl.cache.SetAppResourcesTree(a.InstanceName(ctrl.namespace), tree)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error setting app resource tree: %s\", err)\n-\t}\n-\terr = ctrl.cache.SetAppManagedResources(a.InstanceName(ctrl.namespace), managedResources)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error setting app managed resources: %s\", err)\n-\t}\n-\treturn tree, nil\n+        managedResources, err := ctrl.hideSecretData(a, comparisonResult)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting managed resources: %s\", err)\n+        }\n+        tree, err := ctrl.getResourceTree(a, managedResources)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting resource tree: %s\", err)\n+        }\n+        err = ctrl.cache.SetAppResourcesTree(a.InstanceName(ctrl.namespace), tree)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error setting app resource tree: %s\", err)\n+        }\n+        err = ctrl.cache.SetAppManagedResources(a.InstanceName(ctrl.namespace), managedResources)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error setting app managed resources: %s\", err)\n+        }\n+        return tree, nil\n }\n \n // returns true of given resources exist in the namespace by default and not managed by the user\n func isKnownOrphanedResourceExclusion(key kube.ResourceKey, proj *appv1.AppProject) bool {\n-\tif key.Namespace == \"default\" && key.Group == \"\" && key.Kind == kube.ServiceKind && key.Name == \"kubernetes\" {\n-\t\treturn true\n-\t}\n-\tif key.Group == \"\" && key.Kind == kube.ServiceAccountKind && key.Name == \"default\" {\n-\t\treturn true\n-\t}\n-\tif key.Group == \"\" && key.Kind == \"ConfigMap\" && key.Name == \"kube-root-ca.crt\" {\n-\t\treturn true\n-\t}\n-\tlist := proj.Spec.OrphanedResources.Ignore\n-\tfor _, item := range list {\n-\t\tif item.Kind == \"\" || glob.Match(item.Kind, key.Kind) {\n-\t\t\tif glob.Match(item.Group, key.Group) {\n-\t\t\t\tif item.Name == \"\" || glob.Match(item.Name, key.Name) {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn false\n+        if key.Namespace == \"default\" && key.Group == \"\" && key.Kind == kube.ServiceKind && key.Name == \"kubernetes\" {\n+                return true\n+        }\n+        if key.Group == \"\" && key.Kind == kube.ServiceAccountKind && key.Name == \"default\" {\n+                return true\n+        }\n+        if key.Group == \"\" && key.Kind == \"ConfigMap\" && key.Name == \"kube-root-ca.crt\" {\n+                return true\n+        }\n+        list := proj.Spec.OrphanedResources.Ignore\n+        for _, item := range list {\n+                if item.Kind == \"\" || glob.Match(item.Kind, key.Kind) {\n+                        if glob.Match(item.Group, key.Group) {\n+                                if item.Name == \"\" || glob.Match(item.Name, key.Name) {\n+                                        return true\n+                                }\n+                        }\n+                }\n+        }\n+        return false\n }\n \n func (ctrl *ApplicationController) getResourceTree(a *appv1.Application, managedResources []*appv1.ResourceDiff) (*appv1.ApplicationTree, error) {\n-\tnodes := make([]appv1.ResourceNode, 0)\n-\tproj, err := ctrl.getAppProj(a)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\torphanedNodesMap := make(map[kube.ResourceKey]appv1.ResourceNode)\n-\twarnOrphaned := true\n-\tif proj.Spec.OrphanedResources != nil {\n-\t\torphanedNodesMap, err = ctrl.stateCache.GetNamespaceTopLevelResources(a.Spec.Destination.Server, a.Spec.Destination.Namespace)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\twarnOrphaned = proj.Spec.OrphanedResources.IsWarn()\n-\t}\n-\tfor i := range managedResources {\n-\t\tmanagedResource := managedResources[i]\n-\t\tdelete(orphanedNodesMap, kube.NewResourceKey(managedResource.Group, managedResource.Kind, managedResource.Namespace, managedResource.Name))\n-\t\tvar live = &unstructured.Unstructured{}\n-\t\terr := json.Unmarshal([]byte(managedResource.LiveState), &live)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tvar target = &unstructured.Unstructured{}\n-\t\terr = json.Unmarshal([]byte(managedResource.TargetState), &target)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif live == nil {\n-\t\t\tnodes = append(nodes, appv1.ResourceNode{\n-\t\t\t\tResourceRef: appv1.ResourceRef{\n-\t\t\t\t\tVersion:   target.GroupVersionKind().Version,\n-\t\t\t\t\tName:      managedResource.Name,\n-\t\t\t\t\tKind:      managedResource.Kind,\n-\t\t\t\t\tGroup:     managedResource.Group,\n-\t\t\t\t\tNamespace: managedResource.Namespace,\n-\t\t\t\t},\n-\t\t\t})\n-\t\t} else {\n-\t\t\terr := ctrl.stateCache.IterateHierarchy(a.Spec.Destination.Server, kube.GetResourceKey(live), func(child appv1.ResourceNode, appName string) bool {\n-\t\t\t\tpermitted, _ := proj.IsResourcePermitted(schema.GroupKind{Group: child.ResourceRef.Group, Kind: child.ResourceRef.Kind}, child.Namespace, a.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n-\t\t\t\t\treturn ctrl.db.GetProjectClusters(context.TODO(), project)\n-\t\t\t\t})\n-\t\t\t\tif !permitted {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\t\t\t\tnodes = append(nodes, child)\n-\t\t\t\treturn true\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t}\n-\torphanedNodes := make([]appv1.ResourceNode, 0)\n-\tfor k := range orphanedNodesMap {\n-\t\tif k.Namespace != \"\" && proj.IsGroupKindPermitted(k.GroupKind(), true) && !isKnownOrphanedResourceExclusion(k, proj) {\n-\t\t\terr := ctrl.stateCache.IterateHierarchy(a.Spec.Destination.Server, k, func(child appv1.ResourceNode, appName string) bool {\n-\t\t\t\tbelongToAnotherApp := false\n-\t\t\t\tif appName != \"\" {\n-\t\t\t\t\tappKey := ctrl.toAppKey(appName)\n-\t\t\t\t\tif _, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey); exists && err == nil {\n-\t\t\t\t\t\tbelongToAnotherApp = true\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tif belongToAnotherApp {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\n-\t\t\t\tpermitted, _ := proj.IsResourcePermitted(schema.GroupKind{Group: child.ResourceRef.Group, Kind: child.ResourceRef.Kind}, child.Namespace, a.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n-\t\t\t\t\treturn ctrl.db.GetProjectClusters(context.TODO(), project)\n-\t\t\t\t})\n-\n-\t\t\t\tif !permitted {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\t\t\t\torphanedNodes = append(orphanedNodes, child)\n-\t\t\t\treturn true\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t}\n-\tvar conditions []appv1.ApplicationCondition\n-\tif len(orphanedNodes) > 0 && warnOrphaned {\n-\t\tconditions = []appv1.ApplicationCondition{{\n-\t\t\tType:    appv1.ApplicationConditionOrphanedResourceWarning,\n-\t\t\tMessage: fmt.Sprintf(\"Application has %d orphaned resources\", len(orphanedNodes)),\n-\t\t}}\n-\t}\n-\ta.Status.SetConditions(conditions, map[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionOrphanedResourceWarning: true})\n-\tsort.Slice(orphanedNodes, func(i, j int) bool {\n-\t\treturn orphanedNodes[i].ResourceRef.String() < orphanedNodes[j].ResourceRef.String()\n-\t})\n-\n-\thosts, err := ctrl.getAppHosts(a, nodes)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn &appv1.ApplicationTree{Nodes: nodes, OrphanedNodes: orphanedNodes, Hosts: hosts}, nil\n+        nodes := make([]appv1.ResourceNode, 0)\n+        proj, err := ctrl.getAppProj(a)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        orphanedNodesMap := make(map[kube.ResourceKey]appv1.ResourceNode)\n+        warnOrphaned := true\n+        if proj.Spec.OrphanedResources != nil {\n+                orphanedNodesMap, err = ctrl.stateCache.GetNamespaceTopLevelResources(a.Spec.Destination.Server, a.Spec.Destination.Namespace)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                warnOrphaned = proj.Spec.OrphanedResources.IsWarn()\n+        }\n+        for i := range managedResources {\n+                managedResource := managedResources[i]\n+                delete(orphanedNodesMap, kube.NewResourceKey(managedResource.Group, managedResource.Kind, managedResource.Namespace, managedResource.Name))\n+                var live = &unstructured.Unstructured{}\n+                err := json.Unmarshal([]byte(managedResource.LiveState), &live)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                var target = &unstructured.Unstructured{}\n+                err = json.Unmarshal([]byte(managedResource.TargetState), &target)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if live == nil {\n+                        nodes = append(nodes, appv1.ResourceNode{\n+                                ResourceRef: appv1.ResourceRef{\n+                                        Version:   target.GroupVersionKind().Version,\n+                                        Name:      managedResource.Name,\n+                                        Kind:      managedResource.Kind,\n+                                        Group:     managedResource.Group,\n+                                        Namespace: managedResource.Namespace,\n+                                },\n+                        })\n+                } else {\n+                        err := ctrl.stateCache.IterateHierarchy(a.Spec.Destination.Server, kube.GetResourceKey(live), func(child appv1.ResourceNode, appName string) bool {\n+                                permitted, _ := proj.IsResourcePermitted(schema.GroupKind{Group: child.ResourceRef.Group, Kind: child.ResourceRef.Kind}, child.Namespace, a.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n+                                        return ctrl.db.GetProjectClusters(context.TODO(), project)\n+                                })\n+                                if !permitted {\n+                                        return false\n+                                }\n+                                nodes = append(nodes, child)\n+                                return true\n+                        })\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        }\n+        orphanedNodes := make([]appv1.ResourceNode, 0)\n+        for k := range orphanedNodesMap {\n+                if k.Namespace != \"\" && proj.IsGroupKindPermitted(k.GroupKind(), true) && !isKnownOrphanedResourceExclusion(k, proj) {\n+                        err := ctrl.stateCache.IterateHierarchy(a.Spec.Destination.Server, k, func(child appv1.ResourceNode, appName string) bool {\n+                                belongToAnotherApp := false\n+                                if appName != \"\" {\n+                                        appKey := ctrl.toAppKey(appName)\n+                                        if _, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey); exists && err == nil {\n+                                                belongToAnotherApp = true\n+                                        }\n+                                }\n+\n+                                if belongToAnotherApp {\n+                                        return false\n+                                }\n+\n+                                permitted, _ := proj.IsResourcePermitted(schema.GroupKind{Group: child.ResourceRef.Group, Kind: child.ResourceRef.Kind}, child.Namespace, a.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n+                                        return ctrl.db.GetProjectClusters(context.TODO(), project)\n+                                })\n+\n+                                if !permitted {\n+                                        return false\n+                                }\n+                                orphanedNodes = append(orphanedNodes, child)\n+                                return true\n+                        })\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        }\n+        var conditions []appv1.ApplicationCondition\n+        if len(orphanedNodes) > 0 && warnOrphaned {\n+                conditions = []appv1.ApplicationCondition{{\n+                        Type:    appv1.ApplicationConditionOrphanedResourceWarning,\n+                        Message: fmt.Sprintf(\"Application has %d orphaned resources\", len(orphanedNodes)),\n+                }}\n+        }\n+        a.Status.SetConditions(conditions, map[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionOrphanedResourceWarning: true})\n+        sort.Slice(orphanedNodes, func(i, j int) bool {\n+                return orphanedNodes[i].ResourceRef.String() < orphanedNodes[j].ResourceRef.String()\n+        })\n+\n+        hosts, err := ctrl.getAppHosts(a, nodes)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return &appv1.ApplicationTree{Nodes: nodes, OrphanedNodes: orphanedNodes, Hosts: hosts}, nil\n }\n \n func (ctrl *ApplicationController) getAppHosts(a *appv1.Application, appNodes []appv1.ResourceNode) ([]appv1.HostInfo, error) {\n-\tsupportedResourceNames := map[v1.ResourceName]bool{\n-\t\tv1.ResourceCPU:     true,\n-\t\tv1.ResourceStorage: true,\n-\t\tv1.ResourceMemory:  true,\n-\t}\n-\tappPods := map[kube.ResourceKey]bool{}\n-\tfor _, node := range appNodes {\n-\t\tif node.Group == \"\" && node.Kind == kube.PodKind {\n-\t\t\tappPods[kube.NewResourceKey(node.Group, node.Kind, node.Namespace, node.Name)] = true\n-\t\t}\n-\t}\n-\n-\tallNodesInfo := map[string]statecache.NodeInfo{}\n-\tallPodsByNode := map[string][]statecache.PodInfo{}\n-\tappPodsByNode := map[string][]statecache.PodInfo{}\n-\terr := ctrl.stateCache.IterateResources(a.Spec.Destination.Server, func(res *clustercache.Resource, info *statecache.ResourceInfo) {\n-\t\tkey := res.ResourceKey()\n-\n-\t\tswitch {\n-\t\tcase info.NodeInfo != nil && key.Group == \"\" && key.Kind == \"Node\":\n-\t\t\tallNodesInfo[key.Name] = *info.NodeInfo\n-\t\tcase info.PodInfo != nil && key.Group == \"\" && key.Kind == kube.PodKind:\n-\t\t\tif appPods[key] {\n-\t\t\t\tappPodsByNode[info.PodInfo.NodeName] = append(appPodsByNode[info.PodInfo.NodeName], *info.PodInfo)\n-\t\t\t} else {\n-\t\t\t\tallPodsByNode[info.PodInfo.NodeName] = append(allPodsByNode[info.PodInfo.NodeName], *info.PodInfo)\n-\t\t\t}\n-\t\t}\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar hosts []appv1.HostInfo\n-\tfor nodeName, appPods := range appPodsByNode {\n-\t\tnode, ok := allNodesInfo[nodeName]\n-\t\tif !ok {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tneighbors := allPodsByNode[nodeName]\n-\n-\t\tresources := map[v1.ResourceName]appv1.HostResourceInfo{}\n-\t\tfor name, resource := range node.Capacity {\n-\t\t\tinfo := resources[name]\n-\t\t\tinfo.ResourceName = name\n-\t\t\tinfo.Capacity += resource.MilliValue()\n-\t\t\tresources[name] = info\n-\t\t}\n-\n-\t\tfor _, pod := range appPods {\n-\t\t\tfor name, resource := range pod.ResourceRequests {\n-\t\t\t\tif !supportedResourceNames[name] {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\n-\t\t\t\tinfo := resources[name]\n-\t\t\t\tinfo.RequestedByApp += resource.MilliValue()\n-\t\t\t\tresources[name] = info\n-\t\t\t}\n-\t\t}\n-\n-\t\tfor _, pod := range neighbors {\n-\t\t\tfor name, resource := range pod.ResourceRequests {\n-\t\t\t\tif !supportedResourceNames[name] || pod.Phase == v1.PodSucceeded || pod.Phase == v1.PodFailed {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tinfo := resources[name]\n-\t\t\t\tinfo.RequestedByNeighbors += resource.MilliValue()\n-\t\t\t\tresources[name] = info\n-\t\t\t}\n-\t\t}\n-\n-\t\tvar resourcesInfo []appv1.HostResourceInfo\n-\t\tfor _, info := range resources {\n-\t\t\tif supportedResourceNames[info.ResourceName] && info.Capacity > 0 {\n-\t\t\t\tresourcesInfo = append(resourcesInfo, info)\n-\t\t\t}\n-\t\t}\n-\t\tsort.Slice(resourcesInfo, func(i, j int) bool {\n-\t\t\treturn resourcesInfo[i].ResourceName < resourcesInfo[j].ResourceName\n-\t\t})\n-\t\thosts = append(hosts, appv1.HostInfo{Name: nodeName, SystemInfo: node.SystemInfo, ResourcesInfo: resourcesInfo})\n-\t}\n-\treturn hosts, nil\n+        supportedResourceNames := map[v1.ResourceName]bool{\n+                v1.ResourceCPU:     true,\n+                v1.ResourceStorage: true,\n+                v1.ResourceMemory:  true,\n+        }\n+        appPods := map[kube.ResourceKey]bool{}\n+        for _, node := range appNodes {\n+                if node.Group == \"\" && node.Kind == kube.PodKind {\n+                        appPods[kube.NewResourceKey(node.Group, node.Kind, node.Namespace, node.Name)] = true\n+                }\n+        }\n+\n+        allNodesInfo := map[string]statecache.NodeInfo{}\n+        allPodsByNode := map[string][]statecache.PodInfo{}\n+        appPodsByNode := map[string][]statecache.PodInfo{}\n+        err := ctrl.stateCache.IterateResources(a.Spec.Destination.Server, func(res *clustercache.Resource, info *statecache.ResourceInfo) {\n+                key := res.ResourceKey()\n+\n+                switch {\n+                case info.NodeInfo != nil && key.Group == \"\" && key.Kind == \"Node\":\n+                        allNodesInfo[key.Name] = *info.NodeInfo\n+                case info.PodInfo != nil && key.Group == \"\" && key.Kind == kube.PodKind:\n+                        if appPods[key] {\n+                                appPodsByNode[info.PodInfo.NodeName] = append(appPodsByNode[info.PodInfo.NodeName], *info.PodInfo)\n+                        } else {\n+                                allPodsByNode[info.PodInfo.NodeName] = append(allPodsByNode[info.PodInfo.NodeName], *info.PodInfo)\n+                        }\n+                }\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var hosts []appv1.HostInfo\n+        for nodeName, appPods := range appPodsByNode {\n+                node, ok := allNodesInfo[nodeName]\n+                if !ok {\n+                        continue\n+                }\n+\n+                neighbors := allPodsByNode[nodeName]\n+\n+                resources := map[v1.ResourceName]appv1.HostResourceInfo{}\n+                for name, resource := range node.Capacity {\n+                        info := resources[name]\n+                        info.ResourceName = name\n+                        info.Capacity += resource.MilliValue()\n+                        resources[name] = info\n+                }\n+\n+                for _, pod := range appPods {\n+                        for name, resource := range pod.ResourceRequests {\n+                                if !supportedResourceNames[name] {\n+                                        continue\n+                                }\n+\n+                                info := resources[name]\n+                                info.RequestedByApp += resource.MilliValue()\n+                                resources[name] = info\n+                        }\n+                }\n+\n+                for _, pod := range neighbors {\n+                        for name, resource := range pod.ResourceRequests {\n+                                if !supportedResourceNames[name] || pod.Phase == v1.PodSucceeded || pod.Phase == v1.PodFailed {\n+                                        continue\n+                                }\n+                                info := resources[name]\n+                                info.RequestedByNeighbors += resource.MilliValue()\n+                                resources[name] = info\n+                        }\n+                }\n+\n+                var resourcesInfo []appv1.HostResourceInfo\n+                for _, info := range resources {\n+                        if supportedResourceNames[info.ResourceName] && info.Capacity > 0 {\n+                                resourcesInfo = append(resourcesInfo, info)\n+                        }\n+                }\n+                sort.Slice(resourcesInfo, func(i, j int) bool {\n+                        return resourcesInfo[i].ResourceName < resourcesInfo[j].ResourceName\n+                })\n+                hosts = append(hosts, appv1.HostInfo{Name: nodeName, SystemInfo: node.SystemInfo, ResourcesInfo: resourcesInfo})\n+        }\n+        return hosts, nil\n }\n \n func (ctrl *ApplicationController) hideSecretData(app *appv1.Application, comparisonResult *comparisonResult) ([]*appv1.ResourceDiff, error) {\n-\titems := make([]*appv1.ResourceDiff, len(comparisonResult.managedResources))\n-\tfor i := range comparisonResult.managedResources {\n-\t\tres := comparisonResult.managedResources[i]\n-\t\titem := appv1.ResourceDiff{\n-\t\t\tNamespace:       res.Namespace,\n-\t\t\tName:            res.Name,\n-\t\t\tGroup:           res.Group,\n-\t\t\tKind:            res.Kind,\n-\t\t\tHook:            res.Hook,\n-\t\t\tResourceVersion: res.ResourceVersion,\n-\t\t}\n-\n-\t\ttarget := res.Target\n-\t\tlive := res.Live\n-\t\tresDiff := res.Diff\n-\t\tif res.Kind == kube.SecretKind && res.Group == \"\" {\n-\t\t\tvar err error\n-\t\t\ttarget, live, err = diff.HideSecretData(res.Target, res.Live)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error hiding secret data: %s\", err)\n-\t\t\t}\n-\t\t\tcompareOptions, err := ctrl.settingsMgr.GetResourceCompareOptions()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting resource compare options: %s\", err)\n-\t\t\t}\n-\t\t\tresourceOverrides, err := ctrl.settingsMgr.GetResourceOverrides()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting resource overrides: %s\", err)\n-\t\t\t}\n-\t\t\tappLabelKey, err := ctrl.settingsMgr.GetAppInstanceLabelKey()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting app instance label key: %s\", err)\n-\t\t\t}\n-\t\t\ttrackingMethod, err := ctrl.settingsMgr.GetTrackingMethod()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting tracking method: %s\", err)\n-\t\t\t}\n-\n-\t\t\tclusterCache, err := ctrl.stateCache.GetClusterCache(app.Spec.Destination.Server)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting cluster cache: %s\", err)\n-\t\t\t}\n-\t\t\tdiffConfig, err := argodiff.NewDiffConfigBuilder().\n-\t\t\t\tWithDiffSettings(app.Spec.IgnoreDifferences, resourceOverrides, compareOptions.IgnoreAggregatedRoles).\n-\t\t\t\tWithTracking(appLabelKey, trackingMethod).\n-\t\t\t\tWithNoCache().\n-\t\t\t\tWithLogger(logutils.NewLogrusLogger(logutils.NewWithCurrentConfig())).\n-\t\t\t\tWithGVKParser(clusterCache.GetGVKParser()).\n-\t\t\t\tBuild()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"appcontroller error building diff config: %s\", err)\n-\t\t\t}\n-\n-\t\t\tdiffResult, err := argodiff.StateDiff(live, target, diffConfig)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error applying diff: %s\", err)\n-\t\t\t}\n-\t\t\tresDiff = diffResult\n-\t\t}\n-\n-\t\tif live != nil {\n-\t\t\tdata, err := json.Marshal(live)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error marshaling live json: %s\", err)\n-\t\t\t}\n-\t\t\titem.LiveState = string(data)\n-\t\t} else {\n-\t\t\titem.LiveState = \"null\"\n-\t\t}\n-\n-\t\tif target != nil {\n-\t\t\tdata, err := json.Marshal(target)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error marshaling target json: %s\", err)\n-\t\t\t}\n-\t\t\titem.TargetState = string(data)\n-\t\t} else {\n-\t\t\titem.TargetState = \"null\"\n-\t\t}\n-\t\titem.PredictedLiveState = string(resDiff.PredictedLive)\n-\t\titem.NormalizedLiveState = string(resDiff.NormalizedLive)\n-\t\titem.Modified = resDiff.Modified\n-\n-\t\titems[i] = &item\n-\t}\n-\treturn items, nil\n+        items := make([]*appv1.ResourceDiff, len(comparisonResult.managedResources))\n+        for i := range comparisonResult.managedResources {\n+                res := comparisonResult.managedResources[i]\n+                item := appv1.ResourceDiff{\n+                        Namespace:       res.Namespace,\n+                        Name:            res.Name,\n+                        Group:           res.Group,\n+                        Kind:            res.Kind,\n+                        Hook:            res.Hook,\n+                        ResourceVersion: res.ResourceVersion,\n+                }\n+\n+                target := res.Target\n+                live := res.Live\n+                resDiff := res.Diff\n+                if res.Kind == kube.SecretKind && res.Group == \"\" {\n+                        var err error\n+                        target, live, err = diff.HideSecretData(res.Target, res.Live)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error hiding secret data: %s\", err)\n+                        }\n+                        compareOptions, err := ctrl.settingsMgr.GetResourceCompareOptions()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting resource compare options: %s\", err)\n+                        }\n+                        resourceOverrides, err := ctrl.settingsMgr.GetResourceOverrides()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting resource overrides: %s\", err)\n+                        }\n+                        appLabelKey, err := ctrl.settingsMgr.GetAppInstanceLabelKey()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting app instance label key: %s\", err)\n+                        }\n+                        trackingMethod, err := ctrl.settingsMgr.GetTrackingMethod()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting tracking method: %s\", err)\n+                        }\n+\n+                        clusterCache, err := ctrl.stateCache.GetClusterCache(app.Spec.Destination.Server)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting cluster cache: %s\", err)\n+                        }\n+                        diffConfig, err := argodiff.NewDiffConfigBuilder().\n+                                WithDiffSettings(app.Spec.IgnoreDifferences, resourceOverrides, compareOptions.IgnoreAggregatedRoles).\n+                                WithTracking(appLabelKey, trackingMethod).\n+                                WithNoCache().\n+                                WithLogger(logutils.NewLogrusLogger(logutils.NewWithCurrentConfig())).\n+                                WithGVKParser(clusterCache.GetGVKParser()).\n+                                Build()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"appcontroller error building diff config: %s\", err)\n+                        }\n+\n+                        diffResult, err := argodiff.StateDiff(live, target, diffConfig)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error applying diff: %s\", err)\n+                        }\n+                        resDiff = diffResult\n+                }\n+\n+                if live != nil {\n+                        data, err := json.Marshal(live)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error marshaling live json: %s\", err)\n+                        }\n+                        item.LiveState = string(data)\n+                } else {\n+                        item.LiveState = \"null\"\n+                }\n+\n+                if target != nil {\n+                        data, err := json.Marshal(target)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error marshaling target json: %s\", err)\n+                        }\n+                        item.TargetState = string(data)\n+                } else {\n+                        item.TargetState = \"null\"\n+                }\n+                item.PredictedLiveState = string(resDiff.PredictedLive)\n+                item.NormalizedLiveState = string(resDiff.NormalizedLive)\n+                item.Modified = resDiff.Modified\n+\n+                items[i] = &item\n+        }\n+        return items, nil\n }\n \n // Run starts the Application CRD controller.\n func (ctrl *ApplicationController) Run(ctx context.Context, statusProcessors int, operationProcessors int) {\n-\tdefer runtime.HandleCrash()\n-\tdefer ctrl.appRefreshQueue.ShutDown()\n-\tdefer ctrl.appComparisonTypeRefreshQueue.ShutDown()\n-\tdefer ctrl.appOperationQueue.ShutDown()\n-\tdefer ctrl.projectRefreshQueue.ShutDown()\n-\n-\tctrl.metricsServer.RegisterClustersInfoSource(ctx, ctrl.stateCache)\n-\tctrl.RegisterClusterSecretUpdater(ctx)\n-\n-\tgo ctrl.appInformer.Run(ctx.Done())\n-\tgo ctrl.projInformer.Run(ctx.Done())\n-\n-\terrors.CheckError(ctrl.stateCache.Init())\n-\n-\tif !cache.WaitForCacheSync(ctx.Done(), ctrl.appInformer.HasSynced, ctrl.projInformer.HasSynced) {\n-\t\tlog.Error(\"Timed out waiting for caches to sync\")\n-\t\treturn\n-\t}\n-\n-\tgo func() { errors.CheckError(ctrl.stateCache.Run(ctx)) }()\n-\tgo func() { errors.CheckError(ctrl.metricsServer.ListenAndServe()) }()\n-\n-\tfor i := 0; i < statusProcessors; i++ {\n-\t\tgo wait.Until(func() {\n-\t\t\tfor ctrl.processAppRefreshQueueItem() {\n-\t\t\t}\n-\t\t}, time.Second, ctx.Done())\n-\t}\n-\n-\tfor i := 0; i < operationProcessors; i++ {\n-\t\tgo wait.Until(func() {\n-\t\t\tfor ctrl.processAppOperationQueueItem() {\n-\t\t\t}\n-\t\t}, time.Second, ctx.Done())\n-\t}\n-\n-\tgo wait.Until(func() {\n-\t\tfor ctrl.processAppComparisonTypeQueueItem() {\n-\t\t}\n-\t}, time.Second, ctx.Done())\n-\n-\tgo wait.Until(func() {\n-\t\tfor ctrl.processProjectQueueItem() {\n-\t\t}\n-\t}, time.Second, ctx.Done())\n-\t<-ctx.Done()\n+        defer runtime.HandleCrash()\n+        defer ctrl.appRefreshQueue.ShutDown()\n+        defer ctrl.appComparisonTypeRefreshQueue.ShutDown()\n+        defer ctrl.appOperationQueue.ShutDown()\n+        defer ctrl.projectRefreshQueue.ShutDown()\n+\n+        ctrl.metricsServer.RegisterClustersInfoSource(ctx, ctrl.stateCache)\n+        ctrl.RegisterClusterSecretUpdater(ctx)\n+\n+        go ctrl.appInformer.Run(ctx.Done())\n+        go ctrl.projInformer.Run(ctx.Done())\n+\n+        errors.CheckError(ctrl.stateCache.Init())\n+\n+        if !cache.WaitForCacheSync(ctx.Done(), ctrl.appInformer.HasSynced, ctrl.projInformer.HasSynced) {\n+                log.Error(\"Timed out waiting for caches to sync\")\n+                return\n+        }\n+\n+        go func() { errors.CheckError(ctrl.stateCache.Run(ctx)) }()\n+        go func() { errors.CheckError(ctrl.metricsServer.ListenAndServe()) }()\n+\n+        for i := 0; i < statusProcessors; i++ {\n+                go wait.Until(func() {\n+                        for ctrl.processAppRefreshQueueItem() {\n+                        }\n+                }, time.Second, ctx.Done())\n+        }\n+\n+        for i := 0; i < operationProcessors; i++ {\n+                go wait.Until(func() {\n+                        for ctrl.processAppOperationQueueItem() {\n+                        }\n+                }, time.Second, ctx.Done())\n+        }\n+\n+        go wait.Until(func() {\n+                for ctrl.processAppComparisonTypeQueueItem() {\n+                }\n+        }, time.Second, ctx.Done())\n+\n+        go wait.Until(func() {\n+                for ctrl.processProjectQueueItem() {\n+                }\n+        }, time.Second, ctx.Done())\n+        <-ctx.Done()\n }\n \n // requestAppRefresh adds a request for given app to the refresh queue. appName\n // needs to be the qualified name of the application, i.e. <namespace>/<name>.\n func (ctrl *ApplicationController) requestAppRefresh(appName string, compareWith *CompareWith, after *time.Duration) {\n-\tkey := ctrl.toAppKey(appName)\n-\n-\tif compareWith != nil && after != nil {\n-\t\tctrl.appComparisonTypeRefreshQueue.AddAfter(fmt.Sprintf(\"%s/%d\", key, compareWith), *after)\n-\t} else {\n-\t\tif compareWith != nil {\n-\t\t\tctrl.refreshRequestedAppsMutex.Lock()\n-\t\t\tctrl.refreshRequestedApps[key] = compareWith.Max(ctrl.refreshRequestedApps[key])\n-\t\t\tctrl.refreshRequestedAppsMutex.Unlock()\n-\t\t}\n-\t\tif after != nil {\n-\t\t\tctrl.appRefreshQueue.AddAfter(key, *after)\n-\t\t\tctrl.appOperationQueue.AddAfter(key, *after)\n-\t\t} else {\n-\t\t\tctrl.appRefreshQueue.Add(key)\n-\t\t\tctrl.appOperationQueue.Add(key)\n-\t\t}\n-\t}\n+        key := ctrl.toAppKey(appName)\n+\n+        if compareWith != nil && after != nil {\n+                ctrl.appComparisonTypeRefreshQueue.AddAfter(fmt.Sprintf(\"%s/%d\", key, compareWith), *after)\n+        } else {\n+                if compareWith != nil {\n+                        ctrl.refreshRequestedAppsMutex.Lock()\n+                        ctrl.refreshRequestedApps[key] = compareWith.Max(ctrl.refreshRequestedApps[key])\n+                        ctrl.refreshRequestedAppsMutex.Unlock()\n+                }\n+                if after != nil {\n+                        ctrl.appRefreshQueue.AddAfter(key, *after)\n+                        ctrl.appOperationQueue.AddAfter(key, *after)\n+                } else {\n+                        ctrl.appRefreshQueue.Add(key)\n+                        ctrl.appOperationQueue.Add(key)\n+                }\n+        }\n }\n \n func (ctrl *ApplicationController) isRefreshRequested(appName string) (bool, CompareWith) {\n-\tctrl.refreshRequestedAppsMutex.Lock()\n-\tdefer ctrl.refreshRequestedAppsMutex.Unlock()\n-\tlevel, ok := ctrl.refreshRequestedApps[appName]\n-\tif ok {\n-\t\tdelete(ctrl.refreshRequestedApps, appName)\n-\t}\n-\treturn ok, level\n+        ctrl.refreshRequestedAppsMutex.Lock()\n+        defer ctrl.refreshRequestedAppsMutex.Unlock()\n+        level, ok := ctrl.refreshRequestedApps[appName]\n+        if ok {\n+                delete(ctrl.refreshRequestedApps, appName)\n+        }\n+        return ok, level\n }\n \n func (ctrl *ApplicationController) processAppOperationQueueItem() (processNext bool) {\n-\tappKey, shutdown := ctrl.appOperationQueue.Get()\n-\tif shutdown {\n-\t\tprocessNext = false\n-\t\treturn\n-\t}\n-\tprocessNext = true\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlog.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t}\n-\t\tctrl.appOperationQueue.Done(appKey)\n-\t}()\n-\n-\tobj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey.(string))\n-\tif err != nil {\n-\t\tlog.Errorf(\"Failed to get application '%s' from informer index: %+v\", appKey, err)\n-\t\treturn\n-\t}\n-\tif !exists {\n-\t\t// This happens after app was deleted, but the work queue still had an entry for it.\n-\t\treturn\n-\t}\n-\torigApp, ok := obj.(*appv1.Application)\n-\tif !ok {\n-\t\tlog.Warnf(\"Key '%s' in index is not an application\", appKey)\n-\t\treturn\n-\t}\n-\tapp := origApp.DeepCopy()\n-\n-\tif app.Operation != nil {\n-\t\t// If we get here, we are about to process an operation, but we cannot rely on informer since it might have stale data.\n-\t\t// So always retrieve the latest version to ensure it is not stale to avoid unnecessary syncing.\n-\t\t// We cannot rely on informer since applications might be updated by both application controller and api server.\n-\t\tfreshApp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.ObjectMeta.Namespace).Get(context.Background(), app.ObjectMeta.Name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tlog.Errorf(\"Failed to retrieve latest application state: %v\", err)\n-\t\t\treturn\n-\t\t}\n-\t\tapp = freshApp\n-\t}\n-\n-\tif app.Operation != nil {\n-\t\tctrl.processRequestedAppOperation(app)\n-\t} else if app.DeletionTimestamp != nil && app.CascadedDeletion() {\n-\t\t_, err = ctrl.finalizeApplicationDeletion(app, func(project string) ([]*appv1.Cluster, error) {\n-\t\t\treturn ctrl.db.GetProjectClusters(context.Background(), project)\n-\t\t})\n-\t\tif err != nil {\n-\t\t\tctrl.setAppCondition(app, appv1.ApplicationCondition{\n-\t\t\t\tType:    appv1.ApplicationConditionDeletionError,\n-\t\t\t\tMessage: err.Error(),\n-\t\t\t})\n-\t\t\tmessage := fmt.Sprintf(\"Unable to delete application resources: %v\", err.Error())\n-\t\t\tctrl.auditLogger.LogAppEvent(app, argo.EventInfo{Reason: argo.EventReasonStatusRefreshed, Type: v1.EventTypeWarning}, message)\n-\t\t}\n-\t}\n-\treturn\n+        appKey, shutdown := ctrl.appOperationQueue.Get()\n+        if shutdown {\n+                processNext = false\n+                return\n+        }\n+        processNext = true\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        log.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                }\n+                ctrl.appOperationQueue.Done(appKey)\n+        }()\n+\n+        obj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey.(string))\n+        if err != nil {\n+                log.Errorf(\"Failed to get application '%s' from informer index: %+v\", appKey, err)\n+                return\n+        }\n+        if !exists {\n+                // This happens after app was deleted, but the work queue still had an entry for it.\n+                return\n+        }\n+        origApp, ok := obj.(*appv1.Application)\n+        if !ok {\n+                log.Warnf(\"Key '%s' in index is not an application\", appKey)\n+                return\n+        }\n+        app := origApp.DeepCopy()\n+\n+        if app.Operation != nil {\n+                // If we get here, we are about to process an operation, but we cannot rely on informer since it might have stale data.\n+                // So always retrieve the latest version to ensure it is not stale to avoid unnecessary syncing.\n+                // We cannot rely on informer since applications might be updated by both application controller and api server.\n+                freshApp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.ObjectMeta.Namespace).Get(context.Background(), app.ObjectMeta.Name, metav1.GetOptions{})\n+                if err != nil {\n+                        log.Errorf(\"Failed to retrieve latest application state: %v\", err)\n+                        return\n+                }\n+                app = freshApp\n+        }\n+\n+        if app.Operation != nil {\n+                ctrl.processRequestedAppOperation(app)\n+        } else if app.DeletionTimestamp != nil && app.CascadedDeletion() {\n+                _, err = ctrl.finalizeApplicationDeletion(app, func(project string) ([]*appv1.Cluster, error) {\n+                        return ctrl.db.GetProjectClusters(context.Background(), project)\n+                })\n+                if err != nil {\n+                        ctrl.setAppCondition(app, appv1.ApplicationCondition{\n+                                Type:    appv1.ApplicationConditionDeletionError,\n+                                Message: err.Error(),\n+                        })\n+                        message := fmt.Sprintf(\"Unable to delete application resources: %v\", err.Error())\n+                        ctrl.auditLogger.LogAppEvent(app, argo.EventInfo{Reason: argo.EventReasonStatusRefreshed, Type: v1.EventTypeWarning}, message)\n+                }\n+        }\n+        return\n }\n \n func (ctrl *ApplicationController) processAppComparisonTypeQueueItem() (processNext bool) {\n-\tkey, shutdown := ctrl.appComparisonTypeRefreshQueue.Get()\n-\tprocessNext = true\n-\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlog.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t}\n-\t\tctrl.appComparisonTypeRefreshQueue.Done(key)\n-\t}()\n-\tif shutdown {\n-\t\tprocessNext = false\n-\t\treturn\n-\t}\n-\n-\tif parts := strings.Split(key.(string), \"/\"); len(parts) != 3 {\n-\t\tlog.Warnf(\"Unexpected key format in appComparisonTypeRefreshTypeQueue. Key should consists of namespace/name/comparisonType but got: %s\", key.(string))\n-\t} else {\n-\t\tif compareWith, err := strconv.Atoi(parts[2]); err != nil {\n-\t\t\tlog.Warnf(\"Unable to parse comparison type: %v\", err)\n-\t\t\treturn\n-\t\t} else {\n-\t\t\tctrl.requestAppRefresh(ctrl.toAppQualifiedName(parts[1], parts[0]), CompareWith(compareWith).Pointer(), nil)\n-\t\t}\n-\t}\n-\treturn\n+        key, shutdown := ctrl.appComparisonTypeRefreshQueue.Get()\n+        processNext = true\n+\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        log.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                }\n+                ctrl.appComparisonTypeRefreshQueue.Done(key)\n+        }()\n+        if shutdown {\n+                processNext = false\n+                return\n+        }\n+\n+        if parts := strings.Split(key.(string), \"/\"); len(parts) != 3 {\n+                log.Warnf(\"Unexpected key format in appComparisonTypeRefreshTypeQueue. Key should consists of namespace/name/comparisonType but got: %s\", key.(string))\n+        } else {\n+                if compareWith, err := strconv.Atoi(parts[2]); err != nil {\n+                        log.Warnf(\"Unable to parse comparison type: %v\", err)\n+                        return\n+                } else {\n+                        ctrl.requestAppRefresh(ctrl.toAppQualifiedName(parts[1], parts[0]), CompareWith(compareWith).Pointer(), nil)\n+                }\n+        }\n+        return\n }\n \n func (ctrl *ApplicationController) processProjectQueueItem() (processNext bool) {\n-\tkey, shutdown := ctrl.projectRefreshQueue.Get()\n-\tprocessNext = true\n-\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlog.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t}\n-\t\tctrl.projectRefreshQueue.Done(key)\n-\t}()\n-\tif shutdown {\n-\t\tprocessNext = false\n-\t\treturn\n-\t}\n-\tobj, exists, err := ctrl.projInformer.GetIndexer().GetByKey(key.(string))\n-\tif err != nil {\n-\t\tlog.Errorf(\"Failed to get project '%s' from informer index: %+v\", key, err)\n-\t\treturn\n-\t}\n-\tif !exists {\n-\t\t// This happens after appproj was deleted, but the work queue still had an entry for it.\n-\t\treturn\n-\t}\n-\torigProj, ok := obj.(*appv1.AppProject)\n-\tif !ok {\n-\t\tlog.Warnf(\"Key '%s' in index is not an appproject\", key)\n-\t\treturn\n-\t}\n-\n-\tif origProj.DeletionTimestamp != nil && origProj.HasFinalizer() {\n-\t\tif err := ctrl.finalizeProjectDeletion(origProj.DeepCopy()); err != nil {\n-\t\t\tlog.Warnf(\"Failed to finalize project deletion: %v\", err)\n-\t\t}\n-\t}\n-\treturn\n+        key, shutdown := ctrl.projectRefreshQueue.Get()\n+        processNext = true\n+\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        log.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                }\n+                ctrl.projectRefreshQueue.Done(key)\n+        }()\n+        if shutdown {\n+                processNext = false\n+                return\n+        }\n+        obj, exists, err := ctrl.projInformer.GetIndexer().GetByKey(key.(string))\n+        if err != nil {\n+                log.Errorf(\"Failed to get project '%s' from informer index: %+v\", key, err)\n+                return\n+        }\n+        if !exists {\n+                // This happens after appproj was deleted, but the work queue still had an entry for it.\n+                return\n+        }\n+        origProj, ok := obj.(*appv1.AppProject)\n+        if !ok {\n+                log.Warnf(\"Key '%s' in index is not an appproject\", key)\n+                return\n+        }\n+\n+        if origProj.DeletionTimestamp != nil && origProj.HasFinalizer() {\n+                if err := ctrl.finalizeProjectDeletion(origProj.DeepCopy()); err != nil {\n+                        log.Warnf(\"Failed to finalize project deletion: %v\", err)\n+                }\n+        }\n+        return\n }\n \n func (ctrl *ApplicationController) finalizeProjectDeletion(proj *appv1.AppProject) error {\n-\tapps, err := ctrl.appLister.Applications(ctrl.namespace).List(labels.Everything())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error listing applications: %w\", err)\n-\t}\n-\tappsCount := 0\n-\tfor i := range apps {\n-\t\tif apps[i].Spec.GetProject() == proj.Name {\n-\t\t\tappsCount++\n-\t\t}\n-\t}\n-\tif appsCount == 0 {\n-\t\treturn ctrl.removeProjectFinalizer(proj)\n-\t} else {\n-\t\tlog.Infof(\"Cannot remove project '%s' finalizer as is referenced by %d applications\", proj.Name, appsCount)\n-\t}\n-\treturn nil\n+        apps, err := ctrl.appLister.Applications(ctrl.namespace).List(labels.Everything())\n+        if err != nil {\n+                return fmt.Errorf(\"error listing applications: %w\", err)\n+        }\n+        appsCount := 0\n+        for i := range apps {\n+                if apps[i].Spec.GetProject() == proj.Name {\n+                        appsCount++\n+                }\n+        }\n+        if appsCount == 0 {\n+                return ctrl.removeProjectFinalizer(proj)\n+        } else {\n+                log.Infof(\"Cannot remove project '%s' finalizer as is referenced by %d applications\", proj.Name, appsCount)\n+        }\n+        return nil\n }\n \n func (ctrl *ApplicationController) removeProjectFinalizer(proj *appv1.AppProject) error {\n-\tproj.RemoveFinalizer()\n-\tvar patch []byte\n-\tpatch, _ = json.Marshal(map[string]interface{}{\n-\t\t\"metadata\": map[string]interface{}{\n-\t\t\t\"finalizers\": proj.Finalizers,\n-\t\t},\n-\t})\n-\t_, err := ctrl.applicationClientset.ArgoprojV1alpha1().AppProjects(ctrl.namespace).Patch(context.Background(), proj.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\treturn err\n+        proj.RemoveFinalizer()\n+        var patch []byte\n+        patch, _ = json.Marshal(map[string]interface{}{\n+                \"metadata\": map[string]interface{}{\n+                        \"finalizers\": proj.Finalizers,\n+                },\n+        })\n+        _, err := ctrl.applicationClientset.ArgoprojV1alpha1().AppProjects(ctrl.namespace).Patch(context.Background(), proj.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+        return err\n }\n \n // shouldBeDeleted returns whether a given resource obj should be deleted on cascade delete of application app\n func (ctrl *ApplicationController) shouldBeDeleted(app *appv1.Application, obj *unstructured.Unstructured) bool {\n-\treturn !kube.IsCRD(obj) && !isSelfReferencedApp(app, kube.GetObjectRef(obj))\n+        return !kube.IsCRD(obj) && !isSelfReferencedApp(app, kube.GetObjectRef(obj))\n }\n \n func (ctrl *ApplicationController) getPermittedAppLiveObjects(app *appv1.Application, proj *appv1.AppProject, projectClusters func(project string) ([]*appv1.Cluster, error)) (map[kube.ResourceKey]*unstructured.Unstructured, error) {\n-\tobjsMap, err := ctrl.stateCache.GetManagedLiveObjs(app, []*unstructured.Unstructured{})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\t// Don't delete live resources which are not permitted in the app project\n-\tfor k, v := range objsMap {\n-\t\tpermitted, err := proj.IsLiveResourcePermitted(v, app.Spec.Destination.Server, app.Spec.Destination.Name, projectClusters)\n-\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif !permitted {\n-\t\t\tdelete(objsMap, k)\n-\t\t}\n-\t}\n-\treturn objsMap, nil\n+        objsMap, err := ctrl.stateCache.GetManagedLiveObjs(app, []*unstructured.Unstructured{})\n+        if err != nil {\n+                return nil, err\n+        }\n+        // Don't delete live resources which are not permitted in the app project\n+        for k, v := range objsMap {\n+                permitted, err := proj.IsLiveResourcePermitted(v, app.Spec.Destination.Server, app.Spec.Destination.Name, projectClusters)\n+\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if !permitted {\n+                        delete(objsMap, k)\n+                }\n+        }\n+        return objsMap, nil\n }\n \n func (ctrl *ApplicationController) finalizeApplicationDeletion(app *appv1.Application, projectClusters func(project string) ([]*appv1.Cluster, error)) ([]*unstructured.Unstructured, error) {\n-\tlogCtx := log.WithField(\"application\", app.QualifiedName())\n-\tlogCtx.Infof(\"Deleting resources\")\n-\t// Get refreshed application info, since informer app copy might be stale\n-\tapp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(context.Background(), app.Name, metav1.GetOptions{})\n-\tif err != nil {\n-\t\tif !apierr.IsNotFound(err) {\n-\t\t\tlogCtx.Errorf(\"Unable to get refreshed application info prior deleting resources: %v\", err)\n-\t\t}\n-\t\treturn nil, nil\n-\t}\n-\tproj, err := ctrl.getAppProj(app)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// validDestination is true if the Application destination points to a cluster that is managed by Argo CD\n-\t// (and thus either a cluster secret exists for it, or it's local); validDestination is false otherwise.\n-\tvalidDestination := true\n-\n-\t// Validate the cluster using the Application destination's `name` field, if applicable,\n-\t// and set the Server field, if needed.\n-\tif err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n-\t\tlog.Warnf(\"Unable to validate destination of the Application being deleted: %v\", err)\n-\t\tvalidDestination = false\n-\t}\n-\n-\tobjs := make([]*unstructured.Unstructured, 0)\n-\tvar cluster *appv1.Cluster\n-\n-\t// Attempt to validate the destination via its URL\n-\tif validDestination {\n-\t\tif cluster, err = ctrl.db.GetCluster(context.Background(), app.Spec.Destination.Server); err != nil {\n-\t\t\tlog.Warnf(\"Unable to locate cluster URL for Application being deleted: %v\", err)\n-\t\t\tvalidDestination = false\n-\t\t}\n-\t}\n-\n-\tif validDestination {\n-\t\t// ApplicationDestination points to a valid cluster, so we may clean up the live objects\n-\n-\t\tobjsMap, err := ctrl.getPermittedAppLiveObjects(app, proj, projectClusters)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tfor k := range objsMap {\n-\t\t\t// Wait for objects pending deletion to complete before proceeding with next sync wave\n-\t\t\tif objsMap[k].GetDeletionTimestamp() != nil {\n-\t\t\t\tlogCtx.Infof(\"%d objects remaining for deletion\", len(objsMap))\n-\t\t\t\treturn objs, nil\n-\t\t\t}\n-\n-\t\t\tif ctrl.shouldBeDeleted(app, objsMap[k]) {\n-\t\t\t\tobjs = append(objs, objsMap[k])\n-\t\t\t}\n-\t\t}\n-\n-\t\tconfig := metrics.AddMetricsTransportWrapper(ctrl.metricsServer, app, cluster.RESTConfig())\n-\n-\t\tfilteredObjs := FilterObjectsForDeletion(objs)\n-\n-\t\tpropagationPolicy := metav1.DeletePropagationForeground\n-\t\tif app.GetPropagationPolicy() == appv1.BackgroundPropagationPolicyFinalizer {\n-\t\t\tpropagationPolicy = metav1.DeletePropagationBackground\n-\t\t}\n-\t\tlogCtx.Infof(\"Deleting application's resources with %s propagation policy\", propagationPolicy)\n-\n-\t\terr = kube.RunAllAsync(len(filteredObjs), func(i int) error {\n-\t\t\tobj := filteredObjs[i]\n-\t\t\treturn ctrl.kubectl.DeleteResource(context.Background(), config, obj.GroupVersionKind(), obj.GetName(), obj.GetNamespace(), metav1.DeleteOptions{PropagationPolicy: &propagationPolicy})\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn objs, err\n-\t\t}\n-\n-\t\tobjsMap, err = ctrl.getPermittedAppLiveObjects(app, proj, projectClusters)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tfor k, obj := range objsMap {\n-\t\t\tif !ctrl.shouldBeDeleted(app, obj) {\n-\t\t\t\tdelete(objsMap, k)\n-\t\t\t}\n-\t\t}\n-\t\tif len(objsMap) > 0 {\n-\t\t\tlogCtx.Infof(\"%d objects remaining for deletion\", len(objsMap))\n-\t\t\treturn objs, nil\n-\t\t}\n-\t}\n-\n-\tif err := ctrl.cache.SetAppManagedResources(app.Name, nil); err != nil {\n-\t\treturn objs, err\n-\t}\n-\n-\tif err := ctrl.cache.SetAppResourcesTree(app.Name, nil); err != nil {\n-\t\treturn objs, err\n-\t}\n-\n-\tif err := ctrl.removeCascadeFinalizer(app); err != nil {\n-\t\treturn objs, err\n-\t}\n-\n-\tif validDestination {\n-\t\tlogCtx.Infof(\"Successfully deleted %d resources\", len(objs))\n-\t} else {\n-\t\tlogCtx.Infof(\"Resource entries removed from undefined cluster\")\n-\t}\n-\n-\tctrl.projectRefreshQueue.Add(fmt.Sprintf(\"%s/%s\", ctrl.namespace, app.Spec.GetProject()))\n-\treturn objs, nil\n+        logCtx := log.WithField(\"application\", app.QualifiedName())\n+        logCtx.Infof(\"Deleting resources\")\n+        // Get refreshed application info, since informer app copy might be stale\n+        app, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(context.Background(), app.Name, metav1.GetOptions{})\n+        if err != nil {\n+                if !apierr.IsNotFound(err) {\n+                        logCtx.Errorf(\"Unable to get refreshed application info prior deleting resources: %v\", err)\n+                }\n+                return nil, nil\n+        }\n+        proj, err := ctrl.getAppProj(app)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // validDestination is true if the Application destination points to a cluster that is managed by Argo CD\n+        // (and thus either a cluster secret exists for it, or it's local); validDestination is false otherwise.\n+        validDestination := true\n+\n+        // Validate the cluster using the Application destination's `name` field, if applicable,\n+        // and set the Server field, if needed.\n+        if err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n+                log.Warnf(\"Unable to validate destination of the Application being deleted: %v\", err)\n+                validDestination = false\n+        }\n+\n+        objs := make([]*unstructured.Unstructured, 0)\n+        var cluster *appv1.Cluster\n+\n+        // Attempt to validate the destination via its URL\n+        if validDestination {\n+                if cluster, err = ctrl.db.GetCluster(context.Background(), app.Spec.Destination.Server); err != nil {\n+                        log.Warnf(\"Unable to locate cluster URL for Application being deleted: %v\", err)\n+                        validDestination = false\n+                }\n+        }\n+\n+        if validDestination {\n+                // ApplicationDestination points to a valid cluster, so we may clean up the live objects\n+\n+                objsMap, err := ctrl.getPermittedAppLiveObjects(app, proj, projectClusters)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                for k := range objsMap {\n+                        // Wait for objects pending deletion to complete before proceeding with next sync wave\n+                        if objsMap[k].GetDeletionTimestamp() != nil {\n+                                logCtx.Infof(\"%d objects remaining for deletion\", len(objsMap))\n+                                return objs, nil\n+                        }\n+\n+                        if ctrl.shouldBeDeleted(app, objsMap[k]) {\n+                                objs = append(objs, objsMap[k])\n+                        }\n+                }\n+\n+                config := metrics.AddMetricsTransportWrapper(ctrl.metricsServer, app, cluster.RESTConfig())\n+\n+                filteredObjs := FilterObjectsForDeletion(objs)\n+\n+                propagationPolicy := metav1.DeletePropagationForeground\n+                if app.GetPropagationPolicy() == appv1.BackgroundPropagationPolicyFinalizer {\n+                        propagationPolicy = metav1.DeletePropagationBackground\n+                }\n+                logCtx.Infof(\"Deleting application's resources with %s propagation policy\", propagationPolicy)\n+\n+                err = kube.RunAllAsync(len(filteredObjs), func(i int) error {\n+                        obj := filteredObjs[i]\n+                        return ctrl.kubectl.DeleteResource(context.Background(), config, obj.GroupVersionKind(), obj.GetName(), obj.GetNamespace(), metav1.DeleteOptions{PropagationPolicy: &propagationPolicy})\n+                })\n+                if err != nil {\n+                        return objs, err\n+                }\n+\n+                objsMap, err = ctrl.getPermittedAppLiveObjects(app, proj, projectClusters)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                for k, obj := range objsMap {\n+                        if !ctrl.shouldBeDeleted(app, obj) {\n+                                delete(objsMap, k)\n+                        }\n+                }\n+                if len(objsMap) > 0 {\n+                        logCtx.Infof(\"%d objects remaining for deletion\", len(objsMap))\n+                        return objs, nil\n+                }\n+        }\n+\n+        if err := ctrl.cache.SetAppManagedResources(app.Name, nil); err != nil {\n+                return objs, err\n+        }\n+\n+        if err := ctrl.cache.SetAppResourcesTree(app.Name, nil); err != nil {\n+                return objs, err\n+        }\n+\n+        if err := ctrl.removeCascadeFinalizer(app); err != nil {\n+                return objs, err\n+        }\n+\n+        if validDestination {\n+                logCtx.Infof(\"Successfully deleted %d resources\", len(objs))\n+        } else {\n+                logCtx.Infof(\"Resource entries removed from undefined cluster\")\n+        }\n+\n+        ctrl.projectRefreshQueue.Add(fmt.Sprintf(\"%s/%s\", ctrl.namespace, app.Spec.GetProject()))\n+        return objs, nil\n }\n \n func (ctrl *ApplicationController) removeCascadeFinalizer(app *appv1.Application) error {\n-\t_, err := ctrl.getAppProj(app)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting project: %w\", err)\n-\t}\n-\tapp.UnSetCascadedDeletion()\n-\tvar patch []byte\n-\tpatch, _ = json.Marshal(map[string]interface{}{\n-\t\t\"metadata\": map[string]interface{}{\n-\t\t\t\"finalizers\": app.Finalizers,\n-\t\t},\n-\t})\n-\n-\t_, err = ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\treturn err\n+        _, err := ctrl.getAppProj(app)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting project: %w\", err)\n+        }\n+        app.UnSetCascadedDeletion()\n+        var patch []byte\n+        patch, _ = json.Marshal(map[string]interface{}{\n+                \"metadata\": map[string]interface{}{\n+                        \"finalizers\": app.Finalizers,\n+                },\n+        })\n+\n+        _, err = ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+        return err\n }\n \n func (ctrl *ApplicationController) setAppCondition(app *appv1.Application, condition appv1.ApplicationCondition) {\n-\t// do nothing if app already has same condition\n-\tfor _, c := range app.Status.Conditions {\n-\t\tif c.Message == condition.Message && c.Type == condition.Type {\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tapp.Status.SetConditions([]appv1.ApplicationCondition{condition}, map[appv1.ApplicationConditionType]bool{condition.Type: true})\n-\n-\tvar patch []byte\n-\tpatch, err := json.Marshal(map[string]interface{}{\n-\t\t\"status\": map[string]interface{}{\n-\t\t\t\"conditions\": app.Status.Conditions,\n-\t\t},\n-\t})\n-\tif err == nil {\n-\t\t_, err = ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\t}\n-\tif err != nil {\n-\t\tlog.Errorf(\"Unable to set application condition: %v\", err)\n-\t}\n+        // do nothing if app already has same condition\n+        for _, c := range app.Status.Conditions {\n+                if c.Message == condition.Message && c.Type == condition.Type {\n+                        return\n+                }\n+        }\n+\n+        app.Status.SetConditions([]appv1.ApplicationCondition{condition}, map[appv1.ApplicationConditionType]bool{condition.Type: true})\n+\n+        var patch []byte\n+        patch, err := json.Marshal(map[string]interface{}{\n+                \"status\": map[string]interface{}{\n+                        \"conditions\": app.Status.Conditions,\n+                },\n+        })\n+        if err == nil {\n+                _, err = ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+        }\n+        if err != nil {\n+                log.Errorf(\"Unable to set application condition: %v\", err)\n+        }\n }\n \n func (ctrl *ApplicationController) processRequestedAppOperation(app *appv1.Application) {\n-\tlogCtx := log.WithField(\"application\", app.QualifiedName())\n-\tvar state *appv1.OperationState\n-\t// Recover from any unexpected panics and automatically set the status to be failed\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlogCtx.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t\tstate.Phase = synccommon.OperationError\n-\t\t\tif rerr, ok := r.(error); ok {\n-\t\t\t\tstate.Message = rerr.Error()\n-\t\t\t} else {\n-\t\t\t\tstate.Message = fmt.Sprintf(\"%v\", r)\n-\t\t\t}\n-\t\t\tctrl.setOperationState(app, state)\n-\t\t}\n-\t}()\n-\tterminating := false\n-\tif isOperationInProgress(app) {\n-\t\tstate = app.Status.OperationState.DeepCopy()\n-\t\tterminating = state.Phase == synccommon.OperationTerminating\n-\t\t// Failed  operation with retry strategy might have be in-progress and has completion time\n-\t\tif state.FinishedAt != nil && !terminating {\n-\t\t\tretryAt, err := app.Status.OperationState.Operation.Retry.NextRetryAt(state.FinishedAt.Time, state.RetryCount)\n-\t\t\tif err != nil {\n-\t\t\t\tstate.Phase = synccommon.OperationFailed\n-\t\t\t\tstate.Message = err.Error()\n-\t\t\t\tctrl.setOperationState(app, state)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tretryAfter := time.Until(retryAt)\n-\t\t\tif retryAfter > 0 {\n-\t\t\t\tlogCtx.Infof(\"Skipping retrying in-progress operation. Attempting again at: %s\", retryAt.Format(time.RFC3339))\n-\t\t\t\tctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), &retryAfter)\n-\t\t\t\treturn\n-\t\t\t} else {\n-\t\t\t\t// retrying operation. remove previous failure time in app since it is used as a trigger\n-\t\t\t\t// that previous failed and operation should be retried\n-\t\t\t\tstate.FinishedAt = nil\n-\t\t\t\tctrl.setOperationState(app, state)\n-\t\t\t\t// Get rid of sync results and null out previous operation completion time\n-\t\t\t\tstate.SyncResult = nil\n-\t\t\t}\n-\t\t} else {\n-\t\t\tlogCtx.Infof(\"Resuming in-progress operation. phase: %s, message: %s\", state.Phase, state.Message)\n-\t\t}\n-\t} else {\n-\t\tstate = &appv1.OperationState{Phase: synccommon.OperationRunning, Operation: *app.Operation, StartedAt: metav1.Now()}\n-\t\tctrl.setOperationState(app, state)\n-\t\tlogCtx.Infof(\"Initialized new operation: %v\", *app.Operation)\n-\t}\n-\n-\tif err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n-\t\tstate.Phase = synccommon.OperationFailed\n-\t\tstate.Message = err.Error()\n-\t} else {\n-\t\tctrl.appStateManager.SyncAppState(app, state)\n-\t}\n-\n-\t// Check whether application is allowed to use project\n-\t_, err := ctrl.getAppProj(app)\n-\tif err != nil {\n-\t\tstate.Phase = synccommon.OperationError\n-\t\tstate.Message = err.Error()\n-\t}\n-\n-\tif state.Phase == synccommon.OperationRunning {\n-\t\t// It's possible for an app to be terminated while we were operating on it. We do not want\n-\t\t// to clobber the Terminated state with Running. Get the latest app state to check for this.\n-\t\tfreshApp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(context.Background(), app.ObjectMeta.Name, metav1.GetOptions{})\n-\t\tif err == nil {\n-\t\t\t// App may have lost permissions to use the project meanwhile.\n-\t\t\t_, err = ctrl.getAppProj(freshApp)\n-\t\t\tif err != nil {\n-\t\t\t\tstate.Phase = synccommon.OperationFailed\n-\t\t\t\tstate.Message = fmt.Sprintf(\"operation not allowed: %v\", err)\n-\t\t\t}\n-\t\t\tif freshApp.Status.OperationState != nil && freshApp.Status.OperationState.Phase == synccommon.OperationTerminating {\n-\t\t\t\tstate.Phase = synccommon.OperationTerminating\n-\t\t\t\tstate.Message = \"operation is terminating\"\n-\t\t\t\t// after this, we will get requeued to the workqueue, but next time the\n-\t\t\t\t// SyncAppState will operate in a Terminating phase, allowing the worker to perform\n-\t\t\t\t// cleanup (e.g. delete jobs, workflows, etc...)\n-\t\t\t}\n-\t\t}\n-\t} else if state.Phase == synccommon.OperationFailed || state.Phase == synccommon.OperationError {\n-\t\tif !terminating && (state.RetryCount < state.Operation.Retry.Limit || state.Operation.Retry.Limit < 0) {\n-\t\t\tnow := metav1.Now()\n-\t\t\tstate.FinishedAt = &now\n-\t\t\tif retryAt, err := state.Operation.Retry.NextRetryAt(now.Time, state.RetryCount); err != nil {\n-\t\t\t\tstate.Phase = synccommon.OperationFailed\n-\t\t\t\tstate.Message = fmt.Sprintf(\"%s (failed to retry: %v)\", state.Message, err)\n-\t\t\t} else {\n-\t\t\t\tstate.Phase = synccommon.OperationRunning\n-\t\t\t\tstate.RetryCount++\n-\t\t\t\tstate.Message = fmt.Sprintf(\"%s. Retrying attempt #%d at %s.\", state.Message, state.RetryCount, retryAt.Format(time.Kitchen))\n-\t\t\t}\n-\t\t} else if state.RetryCount > 0 {\n-\t\t\tstate.Message = fmt.Sprintf(\"%s (retried %d times).\", state.Message, state.RetryCount)\n-\t\t}\n-\n-\t}\n-\n-\tctrl.setOperationState(app, state)\n-\tif state.Phase.Completed() && (app.Operation.Sync != nil && !app.Operation.Sync.DryRun) {\n-\t\t// if we just completed an operation, force a refresh so that UI will report up-to-date\n-\t\t// sync/health information\n-\t\tif _, err := cache.MetaNamespaceKeyFunc(app); err == nil {\n-\t\t\t// force app refresh with using CompareWithLatest comparison type and trigger app reconciliation loop\n-\t\t\tctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), nil)\n-\t\t} else {\n-\t\t\tlogCtx.Warnf(\"Fails to requeue application: %v\", err)\n-\t\t}\n-\t}\n+        logCtx := log.WithField(\"application\", app.QualifiedName())\n+        var state *appv1.OperationState\n+        // Recover from any unexpected panics and automatically set the status to be failed\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        logCtx.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                        state.Phase = synccommon.OperationError\n+                        if rerr, ok := r.(error); ok {\n+                                state.Message = rerr.Error()\n+                        } else {\n+                                state.Message = fmt.Sprintf(\"%v\", r)\n+                        }\n+                        ctrl.setOperationState(app, state)\n+                }\n+        }()\n+        terminating := false\n+        if isOperationInProgress(app) {\n+                state = app.Status.OperationState.DeepCopy()\n+                terminating = state.Phase == synccommon.OperationTerminating\n+                // Failed  operation with retry strategy might have be in-progress and has completion time\n+                if state.FinishedAt != nil && !terminating {\n+                        retryAt, err := app.Status.OperationState.Operation.Retry.NextRetryAt(state.FinishedAt.Time, state.RetryCount)\n+                        if err != nil {\n+                                state.Phase = synccommon.OperationFailed\n+                                state.Message = err.Error()\n+                                ctrl.setOperationState(app, state)\n+                                return\n+                        }\n+                        retryAfter := time.Until(retryAt)\n+                        if retryAfter > 0 {\n+                                logCtx.Infof(\"Skipping retrying in-progress operation. Attempting again at: %s\", retryAt.Format(time.RFC3339))\n+                                ctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), &retryAfter)\n+                                return\n+                        } else {\n+                                // retrying operation. remove previous failure time in app since it is used as a trigger\n+                                // that previous failed and operation should be retried\n+                                state.FinishedAt = nil\n+                                ctrl.setOperationState(app, state)\n+                                // Get rid of sync results and null out previous operation completion time\n+                                state.SyncResult = nil\n+                        }\n+                } else {\n+                        logCtx.Infof(\"Resuming in-progress operation. phase: %s, message: %s\", state.Phase, state.Message)\n+                }\n+        } else {\n+                state = &appv1.OperationState{Phase: synccommon.OperationRunning, Operation: *app.Operation, StartedAt: metav1.Now()}\n+                ctrl.setOperationState(app, state)\n+                logCtx.Infof(\"Initialized new operation: %v\", *app.Operation)\n+        }\n+\n+        if err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n+                state.Phase = synccommon.OperationFailed\n+                state.Message = err.Error()\n+        } else {\n+                ctrl.appStateManager.SyncAppState(app, state)\n+        }\n+\n+        // Check whether application is allowed to use project\n+        _, err := ctrl.getAppProj(app)\n+        if err != nil {\n+                state.Phase = synccommon.OperationError\n+                state.Message = err.Error()\n+        }\n+\n+        if state.Phase == synccommon.OperationRunning {\n+                // It's possible for an app to be terminated while we were operating on it. We do not want\n+                // to clobber the Terminated state with Running. Get the latest app state to check for this.\n+                freshApp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(context.Background(), app.ObjectMeta.Name, metav1.GetOptions{})\n+                if err == nil {\n+                        // App may have lost permissions to use the project meanwhile.\n+                        _, err = ctrl.getAppProj(freshApp)\n+                        if err != nil {\n+                                state.Phase = synccommon.OperationFailed\n+                                state.Message = fmt.Sprintf(\"operation not allowed: %v\", err)\n+                        }\n+                        if freshApp.Status.OperationState != nil && freshApp.Status.OperationState.Phase == synccommon.OperationTerminating {\n+                                state.Phase = synccommon.OperationTerminating\n+                                state.Message = \"operation is terminating\"\n+                                // after this, we will get requeued to the workqueue, but next time the\n+                                // SyncAppState will operate in a Terminating phase, allowing the worker to perform\n+                                // cleanup (e.g. delete jobs, workflows, etc...)\n+                        }\n+                }\n+        } else if state.Phase == synccommon.OperationFailed || state.Phase == synccommon.OperationError {\n+                if !terminating && (state.RetryCount < state.Operation.Retry.Limit || state.Operation.Retry.Limit < 0) {\n+                        now := metav1.Now()\n+                        state.FinishedAt = &now\n+                        if retryAt, err := state.Operation.Retry.NextRetryAt(now.Time, state.RetryCount); err != nil {\n+                                state.Phase = synccommon.OperationFailed\n+                                state.Message = fmt.Sprintf(\"%s (failed to retry: %v)\", state.Message, err)\n+                        } else {\n+                                state.Phase = synccommon.OperationRunning\n+                                state.RetryCount++\n+                                state.Message = fmt.Sprintf(\"%s. Retrying attempt #%d at %s.\", state.Message, state.RetryCount, retryAt.Format(time.Kitchen))\n+                        }\n+                } else if state.RetryCount > 0 {\n+                        state.Message = fmt.Sprintf(\"%s (retried %d times).\", state.Message, state.RetryCount)\n+                }\n+\n+        }\n+\n+        ctrl.setOperationState(app, state)\n+        if state.Phase.Completed() && (app.Operation.Sync != nil && !app.Operation.Sync.DryRun) {\n+                // if we just completed an operation, force a refresh so that UI will report up-to-date\n+                // sync/health information\n+                if _, err := cache.MetaNamespaceKeyFunc(app); err == nil {\n+                        // force app refresh with using CompareWithLatest comparison type and trigger app reconciliation loop\n+                        ctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), nil)\n+                } else {\n+                        logCtx.Warnf(\"Fails to requeue application: %v\", err)\n+                }\n+        }\n }\n \n func (ctrl *ApplicationController) setOperationState(app *appv1.Application, state *appv1.OperationState) {\n-\tkube.RetryUntilSucceed(context.Background(), updateOperationStateTimeout, \"Update application operation state\", logutils.NewLogrusLogger(logutils.NewWithCurrentConfig()), func() error {\n-\t\tif state.Phase == \"\" {\n-\t\t\t// expose any bugs where we neglect to set phase\n-\t\t\tpanic(\"no phase was set\")\n-\t\t}\n-\t\tif state.Phase.Completed() {\n-\t\t\tnow := metav1.Now()\n-\t\t\tstate.FinishedAt = &now\n-\t\t}\n-\t\tpatch := map[string]interface{}{\n-\t\t\t\"status\": map[string]interface{}{\n-\t\t\t\t\"operationState\": state,\n-\t\t\t},\n-\t\t}\n-\t\tif state.Phase.Completed() {\n-\t\t\t// If operation is completed, clear the operation field to indicate no operation is\n-\t\t\t// in progress.\n-\t\t\tpatch[\"operation\"] = nil\n-\t\t}\n-\t\tif reflect.DeepEqual(app.Status.OperationState, state) {\n-\t\t\tlog.Infof(\"No operation updates necessary to '%s'. Skipping patch\", app.QualifiedName())\n-\t\t\treturn nil\n-\t\t}\n-\t\tpatchJSON, err := json.Marshal(patch)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error marshaling json: %w\", err)\n-\t\t}\n-\t\tif app.Status.OperationState != nil && app.Status.OperationState.FinishedAt != nil && state.FinishedAt == nil {\n-\t\t\tpatchJSON, err = jsonpatch.MergeMergePatches(patchJSON, []byte(`{\"status\": {\"operationState\": {\"finishedAt\": null}}}`))\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error merging operation state patch: %w\", err)\n-\t\t\t}\n-\t\t}\n-\n-\t\tappClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n-\t\t_, err = appClient.Patch(context.Background(), app.Name, types.MergePatchType, patchJSON, metav1.PatchOptions{})\n-\t\tif err != nil {\n-\t\t\t// Stop retrying updating deleted application\n-\t\t\tif apierr.IsNotFound(err) {\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\treturn fmt.Errorf(\"error patching application with operation state: %w\", err)\n-\t\t}\n-\t\tlog.Infof(\"updated '%s' operation (phase: %s)\", app.QualifiedName(), state.Phase)\n-\t\tif state.Phase.Completed() {\n-\t\t\teventInfo := argo.EventInfo{Reason: argo.EventReasonOperationCompleted}\n-\t\t\tvar messages []string\n-\t\t\tif state.Operation.Sync != nil && len(state.Operation.Sync.Resources) > 0 {\n-\t\t\t\tmessages = []string{\"Partial sync operation\"}\n-\t\t\t} else {\n-\t\t\t\tmessages = []string{\"Sync operation\"}\n-\t\t\t}\n-\t\t\tif state.SyncResult != nil {\n-\t\t\t\tmessages = append(messages, \"to\", state.SyncResult.Revision)\n-\t\t\t}\n-\t\t\tif state.Phase.Successful() {\n-\t\t\t\teventInfo.Type = v1.EventTypeNormal\n-\t\t\t\tmessages = append(messages, \"succeeded\")\n-\t\t\t} else {\n-\t\t\t\teventInfo.Type = v1.EventTypeWarning\n-\t\t\t\tmessages = append(messages, \"failed:\", state.Message)\n-\t\t\t}\n-\t\t\tctrl.auditLogger.LogAppEvent(app, eventInfo, strings.Join(messages, \" \"))\n-\t\t\tctrl.metricsServer.IncSync(app, state)\n-\t\t}\n-\t\treturn nil\n-\t})\n+        kube.RetryUntilSucceed(context.Background(), updateOperationStateTimeout, \"Update application operation state\", logutils.NewLogrusLogger(logutils.NewWithCurrentConfig()), func() error {\n+                if state.Phase == \"\" {\n+                        // expose any bugs where we neglect to set phase\n+                        panic(\"no phase was set\")\n+                }\n+                if state.Phase.Completed() {\n+                        now := metav1.Now()\n+                        state.FinishedAt = &now\n+                }\n+                patch := map[string]interface{}{\n+                        \"status\": map[string]interface{}{\n+                                \"operationState\": state,\n+                        },\n+                }\n+                if state.Phase.Completed() {\n+                        // If operation is completed, clear the operation field to indicate no operation is\n+                        // in progress.\n+                        patch[\"operation\"] = nil\n+                }\n+                if reflect.DeepEqual(app.Status.OperationState, state) {\n+                        log.Infof(\"No operation updates necessary to '%s'. Skipping patch\", app.QualifiedName())\n+                        return nil\n+                }\n+                patchJSON, err := json.Marshal(patch)\n+                if err != nil {\n+                        return fmt.Errorf(\"error marshaling json: %w\", err)\n+                }\n+                if app.Status.OperationState != nil && app.Status.OperationState.FinishedAt != nil && state.FinishedAt == nil {\n+                        patchJSON, err = jsonpatch.MergeMergePatches(patchJSON, []byte(`{\"status\": {\"operationState\": {\"finishedAt\": null}}}`))\n+                        if err != nil {\n+                                return fmt.Errorf(\"error merging operation state patch: %w\", err)\n+                        }\n+                }\n+\n+                appClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n+                _, err = appClient.Patch(context.Background(), app.Name, types.MergePatchType, patchJSON, metav1.PatchOptions{})\n+                if err != nil {\n+                        // Stop retrying updating deleted application\n+                        if apierr.IsNotFound(err) {\n+                                return nil\n+                        }\n+                        return fmt.Errorf(\"error patching application with operation state: %w\", err)\n+                }\n+                log.Infof(\"updated '%s' operation (phase: %s)\", app.QualifiedName(), state.Phase)\n+                if state.Phase.Completed() {\n+                        eventInfo := argo.EventInfo{Reason: argo.EventReasonOperationCompleted}\n+                        var messages []string\n+                        if state.Operation.Sync != nil && len(state.Operation.Sync.Resources) > 0 {\n+                                messages = []string{\"Partial sync operation\"}\n+                        } else {\n+                                messages = []string{\"Sync operation\"}\n+                        }\n+                        if state.SyncResult != nil {\n+                                messages = append(messages, \"to\", state.SyncResult.Revision)\n+                        }\n+                        if state.Phase.Successful() {\n+                                eventInfo.Type = v1.EventTypeNormal\n+                                messages = append(messages, \"succeeded\")\n+                        } else {\n+                                eventInfo.Type = v1.EventTypeWarning\n+                                messages = append(messages, \"failed:\", state.Message)\n+                        }\n+                        ctrl.auditLogger.LogAppEvent(app, eventInfo, strings.Join(messages, \" \"))\n+                        ctrl.metricsServer.IncSync(app, state)\n+                }\n+                return nil\n+        })\n }\n \n func (ctrl *ApplicationController) processAppRefreshQueueItem() (processNext bool) {\n-\tappKey, shutdown := ctrl.appRefreshQueue.Get()\n-\tif shutdown {\n-\t\tprocessNext = false\n-\t\treturn\n-\t}\n-\tprocessNext = true\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlog.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t}\n-\t\tctrl.appRefreshQueue.Done(appKey)\n-\t}()\n-\tobj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey.(string))\n-\tif err != nil {\n-\t\tlog.Errorf(\"Failed to get application '%s' from informer index: %+v\", appKey, err)\n-\t\treturn\n-\t}\n-\tif !exists {\n-\t\t// This happens after app was deleted, but the work queue still had an entry for it.\n-\t\treturn\n-\t}\n-\torigApp, ok := obj.(*appv1.Application)\n-\tif !ok {\n-\t\tlog.Warnf(\"Key '%s' in index is not an application\", appKey)\n-\t\treturn\n-\t}\n-\torigApp = origApp.DeepCopy()\n-\tneedRefresh, refreshType, comparisonLevel := ctrl.needRefreshAppStatus(origApp, ctrl.statusRefreshTimeout, ctrl.statusHardRefreshTimeout)\n-\n-\tif !needRefresh {\n-\t\treturn\n-\t}\n-\tapp := origApp.DeepCopy()\n-\tlogCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n-\n-\tstartTime := time.Now()\n-\tdefer func() {\n-\t\treconcileDuration := time.Since(startTime)\n-\t\tctrl.metricsServer.IncReconcile(origApp, reconcileDuration)\n-\t\tlogCtx.WithFields(log.Fields{\n-\t\t\t\"time_ms\":        reconcileDuration.Milliseconds(),\n-\t\t\t\"level\":          comparisonLevel,\n-\t\t\t\"dest-server\":    origApp.Spec.Destination.Server,\n-\t\t\t\"dest-name\":      origApp.Spec.Destination.Name,\n-\t\t\t\"dest-namespace\": origApp.Spec.Destination.Namespace,\n-\t\t}).Info(\"Reconciliation completed\")\n-\t}()\n-\n-\tif comparisonLevel == ComparisonWithNothing {\n-\t\tmanagedResources := make([]*appv1.ResourceDiff, 0)\n-\t\tif err := ctrl.cache.GetAppManagedResources(app.InstanceName(ctrl.namespace), &managedResources); err != nil {\n-\t\t\tlogCtx.Warnf(\"Failed to get cached managed resources for tree reconciliation, fall back to full reconciliation\")\n-\t\t} else {\n-\t\t\tvar tree *appv1.ApplicationTree\n-\t\t\tif tree, err = ctrl.getResourceTree(app, managedResources); err == nil {\n-\t\t\t\tapp.Status.Summary = tree.GetSummary()\n-\t\t\t\tif err := ctrl.cache.SetAppResourcesTree(app.InstanceName(ctrl.namespace), tree); err != nil {\n-\t\t\t\t\tlogCtx.Errorf(\"Failed to cache resources tree: %v\", err)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tctrl.persistAppStatus(origApp, &app.Status)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tproject, hasErrors := ctrl.refreshAppConditions(app)\n-\tif hasErrors {\n-\t\tapp.Status.Sync.Status = appv1.SyncStatusCodeUnknown\n-\t\tapp.Status.Health.Status = health.HealthStatusUnknown\n-\t\tctrl.persistAppStatus(origApp, &app.Status)\n-\n-\t\tif err := ctrl.cache.SetAppResourcesTree(app.InstanceName(ctrl.namespace), &appv1.ApplicationTree{}); err != nil {\n-\t\t\tlog.Warnf(\"failed to set app resource tree: %v\", err)\n-\t\t}\n-\t\tif err := ctrl.cache.SetAppManagedResources(app.InstanceName(ctrl.namespace), nil); err != nil {\n-\t\t\tlog.Warnf(\"failed to set app managed resources tree: %v\", err)\n-\t\t}\n-\t\treturn\n-\t}\n-\n-\tvar localManifests []string\n-\tif opState := app.Status.OperationState; opState != nil && opState.Operation.Sync != nil {\n-\t\tlocalManifests = opState.Operation.Sync.Manifests\n-\t}\n-\n-\trevisions := make([]string, 0)\n-\tsources := make([]appv1.ApplicationSource, 0)\n-\n-\thasMultipleSources := app.Spec.HasMultipleSources()\n-\n-\t// If we have multiple sources, we use all the sources under `sources` field and ignore source under `source` field.\n-\t// else we use the source under the source field.\n-\tif hasMultipleSources {\n-\t\tfor _, source := range app.Spec.Sources {\n-\t\t\t// We do not perform any filtering of duplicate sources.\n-\t\t\t// Argo CD will apply and update the resources generated from the sources automatically\n-\t\t\t// based on the order in which manifests were generated\n-\t\t\tsources = append(sources, source)\n-\t\t\trevisions = append(revisions, source.TargetRevision)\n-\t\t}\n-\t\tif comparisonLevel == CompareWithRecent {\n-\t\t\trevisions = app.Status.Sync.Revisions\n-\t\t}\n-\t} else {\n-\t\trevision := app.Spec.GetSource().TargetRevision\n-\t\tif comparisonLevel == CompareWithRecent {\n-\t\t\trevision = app.Status.Sync.Revision\n-\t\t}\n-\t\trevisions = append(revisions, revision)\n-\t\tsources = append(sources, app.Spec.GetSource())\n-\t}\n-\tnow := metav1.Now()\n-\n-\tcompareResult := ctrl.appStateManager.CompareAppState(app, project, revisions, sources,\n-\t\trefreshType == appv1.RefreshTypeHard,\n-\t\tcomparisonLevel == CompareWithLatestForceResolve, localManifests, hasMultipleSources)\n-\n-\tfor k, v := range compareResult.timings {\n-\t\tlogCtx = logCtx.WithField(k, v.Milliseconds())\n-\t}\n-\n-\tctrl.normalizeApplication(origApp, app)\n-\n-\ttree, err := ctrl.setAppManagedResources(app, compareResult)\n-\tif err != nil {\n-\t\tlogCtx.Errorf(\"Failed to cache app resources: %v\", err)\n-\t} else {\n-\t\tapp.Status.Summary = tree.GetSummary()\n-\t}\n-\n-\tif project.Spec.SyncWindows.Matches(app).CanSync(false) {\n-\t\tsyncErrCond := ctrl.autoSync(app, compareResult.syncStatus, compareResult.resources)\n-\t\tif syncErrCond != nil {\n-\t\t\tapp.Status.SetConditions(\n-\t\t\t\t[]appv1.ApplicationCondition{*syncErrCond},\n-\t\t\t\tmap[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionSyncError: true},\n-\t\t\t)\n-\t\t} else {\n-\t\t\tapp.Status.SetConditions(\n-\t\t\t\t[]appv1.ApplicationCondition{},\n-\t\t\t\tmap[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionSyncError: true},\n-\t\t\t)\n-\t\t}\n-\t} else {\n-\t\tlogCtx.Info(\"Sync prevented by sync window\")\n-\t}\n-\n-\tif app.Status.ReconciledAt == nil || comparisonLevel >= CompareWithLatest {\n-\t\tapp.Status.ReconciledAt = &now\n-\t}\n-\tapp.Status.Sync = *compareResult.syncStatus\n-\tapp.Status.Health = *compareResult.healthStatus\n-\tapp.Status.Resources = compareResult.resources\n-\tsort.Slice(app.Status.Resources, func(i, j int) bool {\n-\t\treturn resourceStatusKey(app.Status.Resources[i]) < resourceStatusKey(app.Status.Resources[j])\n-\t})\n-\tapp.Status.SourceType = compareResult.appSourceType\n-\tapp.Status.SourceTypes = compareResult.appSourceTypes\n-\tctrl.persistAppStatus(origApp, &app.Status)\n-\treturn\n+        appKey, shutdown := ctrl.appRefreshQueue.Get()\n+        if shutdown {\n+                processNext = false\n+                return\n+        }\n+        processNext = true\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        log.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                }\n+                ctrl.appRefreshQueue.Done(appKey)\n+        }()\n+        obj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey.(string))\n+        if err != nil {\n+                log.Errorf(\"Failed to get application '%s' from informer index: %+v\", appKey, err)\n+                return\n+        }\n+        if !exists {\n+                // This happens after app was deleted, but the work queue still had an entry for it.\n+                return\n+        }\n+        origApp, ok := obj.(*appv1.Application)\n+        if !ok {\n+                log.Warnf(\"Key '%s' in index is not an application\", appKey)\n+                return\n+        }\n+        origApp = origApp.DeepCopy()\n+        needRefresh, refreshType, comparisonLevel := ctrl.needRefreshAppStatus(origApp, ctrl.statusRefreshTimeout, ctrl.statusHardRefreshTimeout)\n+\n+        if !needRefresh {\n+                return\n+        }\n+        app := origApp.DeepCopy()\n+        logCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n+\n+        startTime := time.Now()\n+        defer func() {\n+                reconcileDuration := time.Since(startTime)\n+                ctrl.metricsServer.IncReconcile(origApp, reconcileDuration)\n+                logCtx.WithFields(log.Fields{\n+                        \"time_ms\":        reconcileDuration.Milliseconds(),\n+                        \"level\":          comparisonLevel,\n+                        \"dest-server\":    origApp.Spec.Destination.Server,\n+                        \"dest-name\":      origApp.Spec.Destination.Name,\n+                        \"dest-namespace\": origApp.Spec.Destination.Namespace,\n+                }).Info(\"Reconciliation completed\")\n+        }()\n+\n+        if comparisonLevel == ComparisonWithNothing {\n+                managedResources := make([]*appv1.ResourceDiff, 0)\n+                if err := ctrl.cache.GetAppManagedResources(app.InstanceName(ctrl.namespace), &managedResources); err != nil {\n+                        logCtx.Warnf(\"Failed to get cached managed resources for tree reconciliation, fall back to full reconciliation\")\n+                } else {\n+                        var tree *appv1.ApplicationTree\n+                        if tree, err = ctrl.getResourceTree(app, managedResources); err == nil {\n+                                app.Status.Summary = tree.GetSummary()\n+                                if err := ctrl.cache.SetAppResourcesTree(app.InstanceName(ctrl.namespace), tree); err != nil {\n+                                        logCtx.Errorf(\"Failed to cache resources tree: %v\", err)\n+                                        return\n+                                }\n+                        }\n+\n+                        ctrl.persistAppStatus(origApp, &app.Status)\n+                        return\n+                }\n+        }\n+\n+        project, hasErrors := ctrl.refreshAppConditions(app)\n+        if hasErrors {\n+                app.Status.Sync.Status = appv1.SyncStatusCodeUnknown\n+                app.Status.Health.Status = health.HealthStatusUnknown\n+                ctrl.persistAppStatus(origApp, &app.Status)\n+\n+                if err := ctrl.cache.SetAppResourcesTree(app.InstanceName(ctrl.namespace), &appv1.ApplicationTree{}); err != nil {\n+                        log.Warnf(\"failed to set app resource tree: %v\", err)\n+                }\n+                if err := ctrl.cache.SetAppManagedResources(app.InstanceName(ctrl.namespace), nil); err != nil {\n+                        log.Warnf(\"failed to set app managed resources tree: %v\", err)\n+                }\n+                return\n+        }\n+\n+        var localManifests []string\n+        if opState := app.Status.OperationState; opState != nil && opState.Operation.Sync != nil {\n+                localManifests = opState.Operation.Sync.Manifests\n+        }\n+\n+        revisions := make([]string, 0)\n+        sources := make([]appv1.ApplicationSource, 0)\n+\n+        hasMultipleSources := app.Spec.HasMultipleSources()\n+\n+        // If we have multiple sources, we use all the sources under `sources` field and ignore source under `source` field.\n+        // else we use the source under the source field.\n+        if hasMultipleSources {\n+                for _, source := range app.Spec.Sources {\n+                        // We do not perform any filtering of duplicate sources.\n+                        // Argo CD will apply and update the resources generated from the sources automatically\n+                        // based on the order in which manifests were generated\n+                        sources = append(sources, source)\n+                        revisions = append(revisions, source.TargetRevision)\n+                }\n+                if comparisonLevel == CompareWithRecent {\n+                        revisions = app.Status.Sync.Revisions\n+                }\n+        } else {\n+                revision := app.Spec.GetSource().TargetRevision\n+                if comparisonLevel == CompareWithRecent {\n+                        revision = app.Status.Sync.Revision\n+                }\n+                revisions = append(revisions, revision)\n+                sources = append(sources, app.Spec.GetSource())\n+        }\n+        now := metav1.Now()\n+\n+        compareResult := ctrl.appStateManager.CompareAppState(app, project, revisions, sources,\n+                refreshType == appv1.RefreshTypeHard,\n+                comparisonLevel == CompareWithLatestForceResolve, localManifests, hasMultipleSources)\n+\n+        for k, v := range compareResult.timings {\n+                logCtx = logCtx.WithField(k, v.Milliseconds())\n+        }\n+\n+        ctrl.normalizeApplication(origApp, app)\n+\n+        tree, err := ctrl.setAppManagedResources(app, compareResult)\n+        if err != nil {\n+                logCtx.Errorf(\"Failed to cache app resources: %v\", err)\n+        } else {\n+                app.Status.Summary = tree.GetSummary()\n+        }\n+\n+        if project.Spec.SyncWindows.Matches(app).CanSync(false) {\n+                syncErrCond := ctrl.autoSync(app, compareResult.syncStatus, compareResult.resources)\n+                if syncErrCond != nil {\n+                        app.Status.SetConditions(\n+                                []appv1.ApplicationCondition{*syncErrCond},\n+                                map[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionSyncError: true},\n+                        )\n+                } else {\n+                        app.Status.SetConditions(\n+                                []appv1.ApplicationCondition{},\n+                                map[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionSyncError: true},\n+                        )\n+                }\n+        } else {\n+                logCtx.Info(\"Sync prevented by sync window\")\n+        }\n+\n+        if app.Status.ReconciledAt == nil || comparisonLevel >= CompareWithLatest {\n+                app.Status.ReconciledAt = &now\n+        }\n+        app.Status.Sync = *compareResult.syncStatus\n+        app.Status.Health = *compareResult.healthStatus\n+        app.Status.Resources = compareResult.resources\n+        sort.Slice(app.Status.Resources, func(i, j int) bool {\n+                return resourceStatusKey(app.Status.Resources[i]) < resourceStatusKey(app.Status.Resources[j])\n+        })\n+        app.Status.SourceType = compareResult.appSourceType\n+        app.Status.SourceTypes = compareResult.appSourceTypes\n+        ctrl.persistAppStatus(origApp, &app.Status)\n+        return\n }\n \n func resourceStatusKey(res appv1.ResourceStatus) string {\n-\treturn strings.Join([]string{res.Group, res.Kind, res.Namespace, res.Name}, \"/\")\n+        return strings.Join([]string{res.Group, res.Kind, res.Namespace, res.Name}, \"/\")\n }\n \n // needRefreshAppStatus answers if application status needs to be refreshed.\n@@ -1474,491 +1474,491 @@ func resourceStatusKey(res appv1.ResourceStatus) string {\n // Additionally returns whether full refresh was requested or not.\n // If full refresh is requested then target and live state should be reconciled, else only live state tree should be updated.\n func (ctrl *ApplicationController) needRefreshAppStatus(app *appv1.Application, statusRefreshTimeout, statusHardRefreshTimeout time.Duration) (bool, appv1.RefreshType, CompareWith) {\n-\tlogCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n-\tvar reason string\n-\tcompareWith := CompareWithLatest\n-\trefreshType := appv1.RefreshTypeNormal\n-\tsoftExpired := app.Status.ReconciledAt == nil || app.Status.ReconciledAt.Add(statusRefreshTimeout).Before(time.Now().UTC())\n-\thardExpired := (app.Status.ReconciledAt == nil || app.Status.ReconciledAt.Add(statusHardRefreshTimeout).Before(time.Now().UTC())) && statusHardRefreshTimeout.Seconds() != 0\n-\n-\tif requestedType, ok := app.IsRefreshRequested(); ok {\n-\t\tcompareWith = CompareWithLatestForceResolve\n-\t\t// user requested app refresh.\n-\t\trefreshType = requestedType\n-\t\treason = fmt.Sprintf(\"%s refresh requested\", refreshType)\n-\t} else {\n-\t\tif app.Spec.HasMultipleSources() {\n-\t\t\tif (len(app.Spec.Sources) != len(app.Status.Sync.ComparedTo.Sources)) || !reflect.DeepEqual(app.Spec.Sources, app.Status.Sync.ComparedTo.Sources) {\n-\t\t\t\treason = \"atleast one of the spec.sources differs\"\n-\t\t\t\tcompareWith = CompareWithLatestForceResolve\n-\t\t\t}\n-\t\t} else if !app.Spec.Source.Equals(app.Status.Sync.ComparedTo.Source) {\n-\t\t\treason = \"spec.source differs\"\n-\t\t\tcompareWith = CompareWithLatestForceResolve\n-\t\t} else if hardExpired || softExpired {\n-\t\t\t// The commented line below mysteriously crashes if app.Status.ReconciledAt is nil\n-\t\t\t// reason = fmt.Sprintf(\"comparison expired. reconciledAt: %v, expiry: %v\", app.Status.ReconciledAt, statusRefreshTimeout)\n-\t\t\t//TODO: find existing Golang bug or create a new one\n-\t\t\treconciledAtStr := \"never\"\n-\t\t\tif app.Status.ReconciledAt != nil {\n-\t\t\t\treconciledAtStr = app.Status.ReconciledAt.String()\n-\t\t\t}\n-\t\t\treason = fmt.Sprintf(\"comparison expired, requesting refresh. reconciledAt: %v, expiry: %v\", reconciledAtStr, statusRefreshTimeout)\n-\t\t\tif hardExpired {\n-\t\t\t\treason = fmt.Sprintf(\"comparison expired, requesting hard refresh. reconciledAt: %v, expiry: %v\", reconciledAtStr, statusHardRefreshTimeout)\n-\t\t\t\trefreshType = appv1.RefreshTypeHard\n-\t\t\t}\n-\t\t} else if !app.Spec.Destination.Equals(app.Status.Sync.ComparedTo.Destination) {\n-\t\t\treason = \"spec.destination differs\"\n-\t\t} else if requested, level := ctrl.isRefreshRequested(app.QualifiedName()); requested {\n-\t\t\tcompareWith = level\n-\t\t\treason = \"controller refresh requested\"\n-\t\t}\n-\t}\n-\n-\tif reason != \"\" {\n-\t\tlogCtx.Infof(\"Refreshing app status (%s), level (%d)\", reason, compareWith)\n-\t\treturn true, refreshType, compareWith\n-\t}\n-\treturn false, refreshType, compareWith\n+        logCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n+        var reason string\n+        compareWith := CompareWithLatest\n+        refreshType := appv1.RefreshTypeNormal\n+        softExpired := app.Status.ReconciledAt == nil || app.Status.ReconciledAt.Add(statusRefreshTimeout).Before(time.Now().UTC())\n+        hardExpired := (app.Status.ReconciledAt == nil || app.Status.ReconciledAt.Add(statusHardRefreshTimeout).Before(time.Now().UTC())) && statusHardRefreshTimeout.Seconds() != 0\n+\n+        if requestedType, ok := app.IsRefreshRequested(); ok {\n+                compareWith = CompareWithLatestForceResolve\n+                // user requested app refresh.\n+                refreshType = requestedType\n+                reason = fmt.Sprintf(\"%s refresh requested\", refreshType)\n+        } else {\n+                if app.Spec.HasMultipleSources() {\n+                        if (len(app.Spec.Sources) != len(app.Status.Sync.ComparedTo.Sources)) || !reflect.DeepEqual(app.Spec.Sources, app.Status.Sync.ComparedTo.Sources) {\n+                                reason = \"atleast one of the spec.sources differs\"\n+                                compareWith = CompareWithLatestForceResolve\n+                        }\n+                } else if !app.Spec.Source.Equals(app.Status.Sync.ComparedTo.Source) {\n+                        reason = \"spec.source differs\"\n+                        compareWith = CompareWithLatestForceResolve\n+                } else if hardExpired || softExpired {\n+                        // The commented line below mysteriously crashes if app.Status.ReconciledAt is nil\n+                        // reason = fmt.Sprintf(\"comparison expired. reconciledAt: %v, expiry: %v\", app.Status.ReconciledAt, statusRefreshTimeout)\n+                        //TODO: find existing Golang bug or create a new one\n+                        reconciledAtStr := \"never\"\n+                        if app.Status.ReconciledAt != nil {\n+                                reconciledAtStr = app.Status.ReconciledAt.String()\n+                        }\n+                        reason = fmt.Sprintf(\"comparison expired, requesting refresh. reconciledAt: %v, expiry: %v\", reconciledAtStr, statusRefreshTimeout)\n+                        if hardExpired {\n+                                reason = fmt.Sprintf(\"comparison expired, requesting hard refresh. reconciledAt: %v, expiry: %v\", reconciledAtStr, statusHardRefreshTimeout)\n+                                refreshType = appv1.RefreshTypeHard\n+                        }\n+                } else if !app.Spec.Destination.Equals(app.Status.Sync.ComparedTo.Destination) {\n+                        reason = \"spec.destination differs\"\n+                } else if requested, level := ctrl.isRefreshRequested(app.QualifiedName()); requested {\n+                        compareWith = level\n+                        reason = \"controller refresh requested\"\n+                }\n+        }\n+\n+        if reason != \"\" {\n+                logCtx.Infof(\"Refreshing app status (%s), level (%d)\", reason, compareWith)\n+                return true, refreshType, compareWith\n+        }\n+        return false, refreshType, compareWith\n }\n \n func (ctrl *ApplicationController) refreshAppConditions(app *appv1.Application) (*appv1.AppProject, bool) {\n-\terrorConditions := make([]appv1.ApplicationCondition, 0)\n-\tproj, err := ctrl.getAppProj(app)\n-\tif err != nil {\n-\t\terrorConditions = append(errorConditions, ctrl.projectErrorToCondition(err, app))\n-\t} else {\n-\t\tspecConditions, err := argo.ValidatePermissions(context.Background(), &app.Spec, proj, ctrl.db)\n-\t\tif err != nil {\n-\t\t\terrorConditions = append(errorConditions, appv1.ApplicationCondition{\n-\t\t\t\tType:    appv1.ApplicationConditionUnknownError,\n-\t\t\t\tMessage: err.Error(),\n-\t\t\t})\n-\t\t} else {\n-\t\t\terrorConditions = append(errorConditions, specConditions...)\n-\t\t}\n-\t}\n-\tapp.Status.SetConditions(errorConditions, map[appv1.ApplicationConditionType]bool{\n-\t\tappv1.ApplicationConditionInvalidSpecError: true,\n-\t\tappv1.ApplicationConditionUnknownError:     true,\n-\t})\n-\treturn proj, len(errorConditions) > 0\n+        errorConditions := make([]appv1.ApplicationCondition, 0)\n+        proj, err := ctrl.getAppProj(app)\n+        if err != nil {\n+                errorConditions = append(errorConditions, ctrl.projectErrorToCondition(err, app))\n+        } else {\n+                specConditions, err := argo.ValidatePermissions(context.Background(), &app.Spec, proj, ctrl.db)\n+                if err != nil {\n+                        errorConditions = append(errorConditions, appv1.ApplicationCondition{\n+                                Type:    appv1.ApplicationConditionUnknownError,\n+                                Message: err.Error(),\n+                        })\n+                } else {\n+                        errorConditions = append(errorConditions, specConditions...)\n+                }\n+        }\n+        app.Status.SetConditions(errorConditions, map[appv1.ApplicationConditionType]bool{\n+                appv1.ApplicationConditionInvalidSpecError: true,\n+                appv1.ApplicationConditionUnknownError:     true,\n+        })\n+        return proj, len(errorConditions) > 0\n }\n \n // normalizeApplication normalizes an application.spec and additionally persists updates if it changed\n func (ctrl *ApplicationController) normalizeApplication(orig, app *appv1.Application) {\n-\tlogCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n-\tapp.Spec = *argo.NormalizeApplicationSpec(&app.Spec)\n-\n-\tpatch, modified, err := diff.CreateTwoWayMergePatch(orig, app, appv1.Application{})\n-\n-\tif err != nil {\n-\t\tlogCtx.Errorf(\"error constructing app spec patch: %v\", err)\n-\t} else if modified {\n-\t\tappClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n-\t\t_, err = appClient.Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\t\tif err != nil {\n-\t\t\tlogCtx.Errorf(\"Error persisting normalized application spec: %v\", err)\n-\t\t} else {\n-\t\t\tlogCtx.Infof(\"Normalized app spec: %s\", string(patch))\n-\t\t}\n-\t}\n+        logCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n+        app.Spec = *argo.NormalizeApplicationSpec(&app.Spec)\n+\n+        patch, modified, err := diff.CreateTwoWayMergePatch(orig, app, appv1.Application{})\n+\n+        if err != nil {\n+                logCtx.Errorf(\"error constructing app spec patch: %v\", err)\n+        } else if modified {\n+                appClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n+                _, err = appClient.Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+                if err != nil {\n+                        logCtx.Errorf(\"Error persisting normalized application spec: %v\", err)\n+                } else {\n+                        logCtx.Infof(\"Normalized app spec: %s\", string(patch))\n+                }\n+        }\n }\n \n // persistAppStatus persists updates to application status. If no changes were made, it is a no-op\n func (ctrl *ApplicationController) persistAppStatus(orig *appv1.Application, newStatus *appv1.ApplicationStatus) {\n-\tlogCtx := log.WithFields(log.Fields{\"application\": orig.QualifiedName()})\n-\tif orig.Status.Sync.Status != newStatus.Sync.Status {\n-\t\tmessage := fmt.Sprintf(\"Updated sync status: %s -> %s\", orig.Status.Sync.Status, newStatus.Sync.Status)\n-\t\tctrl.auditLogger.LogAppEvent(orig, argo.EventInfo{Reason: argo.EventReasonResourceUpdated, Type: v1.EventTypeNormal}, message)\n-\t}\n-\tif orig.Status.Health.Status != newStatus.Health.Status {\n-\t\tmessage := fmt.Sprintf(\"Updated health status: %s -> %s\", orig.Status.Health.Status, newStatus.Health.Status)\n-\t\tctrl.auditLogger.LogAppEvent(orig, argo.EventInfo{Reason: argo.EventReasonResourceUpdated, Type: v1.EventTypeNormal}, message)\n-\t}\n-\tvar newAnnotations map[string]string\n-\tif orig.GetAnnotations() != nil {\n-\t\tnewAnnotations = make(map[string]string)\n-\t\tfor k, v := range orig.GetAnnotations() {\n-\t\t\tnewAnnotations[k] = v\n-\t\t}\n-\t\tdelete(newAnnotations, appv1.AnnotationKeyRefresh)\n-\t}\n-\tpatch, modified, err := diff.CreateTwoWayMergePatch(\n-\t\t&appv1.Application{ObjectMeta: metav1.ObjectMeta{Annotations: orig.GetAnnotations()}, Status: orig.Status},\n-\t\t&appv1.Application{ObjectMeta: metav1.ObjectMeta{Annotations: newAnnotations}, Status: *newStatus}, appv1.Application{})\n-\tif err != nil {\n-\t\tlogCtx.Errorf(\"Error constructing app status patch: %v\", err)\n-\t\treturn\n-\t}\n-\tif !modified {\n-\t\tlogCtx.Infof(\"No status changes. Skipping patch\")\n-\t\treturn\n-\t}\n-\tappClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(orig.Namespace)\n-\t_, err = appClient.Patch(context.Background(), orig.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\tif err != nil {\n-\t\tlogCtx.Warnf(\"Error updating application: %v\", err)\n-\t} else {\n-\t\tlogCtx.Infof(\"Update successful\")\n-\t}\n+        logCtx := log.WithFields(log.Fields{\"application\": orig.QualifiedName()})\n+        if orig.Status.Sync.Status != newStatus.Sync.Status {\n+                message := fmt.Sprintf(\"Updated sync status: %s -> %s\", orig.Status.Sync.Status, newStatus.Sync.Status)\n+                ctrl.auditLogger.LogAppEvent(orig, argo.EventInfo{Reason: argo.EventReasonResourceUpdated, Type: v1.EventTypeNormal}, message)\n+        }\n+        if orig.Status.Health.Status != newStatus.Health.Status {\n+                message := fmt.Sprintf(\"Updated health status: %s -> %s\", orig.Status.Health.Status, newStatus.Health.Status)\n+                ctrl.auditLogger.LogAppEvent(orig, argo.EventInfo{Reason: argo.EventReasonResourceUpdated, Type: v1.EventTypeNormal}, message)\n+        }\n+        var newAnnotations map[string]string\n+        if orig.GetAnnotations() != nil {\n+                newAnnotations = make(map[string]string)\n+                for k, v := range orig.GetAnnotations() {\n+                        newAnnotations[k] = v\n+                }\n+                delete(newAnnotations, appv1.AnnotationKeyRefresh)\n+        }\n+        patch, modified, err := diff.CreateTwoWayMergePatch(\n+                &appv1.Application{ObjectMeta: metav1.ObjectMeta{Annotations: orig.GetAnnotations()}, Status: orig.Status},\n+                &appv1.Application{ObjectMeta: metav1.ObjectMeta{Annotations: newAnnotations}, Status: *newStatus}, appv1.Application{})\n+        if err != nil {\n+                logCtx.Errorf(\"Error constructing app status patch: %v\", err)\n+                return\n+        }\n+        if !modified {\n+                logCtx.Infof(\"No status changes. Skipping patch\")\n+                return\n+        }\n+        appClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(orig.Namespace)\n+        _, err = appClient.Patch(context.Background(), orig.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+        if err != nil {\n+                logCtx.Warnf(\"Error updating application: %v\", err)\n+        } else {\n+                logCtx.Infof(\"Update successful\")\n+        }\n }\n \n // autoSync will initiate a sync operation for an application configured with automated sync\n func (ctrl *ApplicationController) autoSync(app *appv1.Application, syncStatus *appv1.SyncStatus, resources []appv1.ResourceStatus) *appv1.ApplicationCondition {\n-\tif app.Spec.SyncPolicy == nil || app.Spec.SyncPolicy.Automated == nil {\n-\t\treturn nil\n-\t}\n-\tlogCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n-\n-\tif app.Operation != nil {\n-\t\tlogCtx.Infof(\"Skipping auto-sync: another operation is in progress\")\n-\t\treturn nil\n-\t}\n-\tif app.DeletionTimestamp != nil && !app.DeletionTimestamp.IsZero() {\n-\t\tlogCtx.Infof(\"Skipping auto-sync: deletion in progress\")\n-\t\treturn nil\n-\t}\n-\n-\t// Only perform auto-sync if we detect OutOfSync status. This is to prevent us from attempting\n-\t// a sync when application is already in a Synced or Unknown state\n-\tif syncStatus.Status != appv1.SyncStatusCodeOutOfSync {\n-\t\tlogCtx.Infof(\"Skipping auto-sync: application status is %s\", syncStatus.Status)\n-\t\treturn nil\n-\t}\n-\n-\tif !app.Spec.SyncPolicy.Automated.Prune {\n-\t\trequirePruneOnly := true\n-\t\tfor _, r := range resources {\n-\t\t\tif r.Status != appv1.SyncStatusCodeSynced && !r.RequiresPruning {\n-\t\t\t\trequirePruneOnly = false\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif requirePruneOnly {\n-\t\t\tlogCtx.Infof(\"Skipping auto-sync: need to prune extra resources only but automated prune is disabled\")\n-\t\t\treturn nil\n-\t\t}\n-\t}\n-\n-\tdesiredCommitSHA := syncStatus.Revision\n-\tdesiredCommitSHAsMS := syncStatus.Revisions\n-\talreadyAttempted, attemptPhase := alreadyAttemptedSync(app, desiredCommitSHA, desiredCommitSHAsMS, app.Spec.HasMultipleSources())\n-\tselfHeal := app.Spec.SyncPolicy.Automated.SelfHeal\n-\top := appv1.Operation{\n-\t\tSync: &appv1.SyncOperation{\n-\t\t\tRevision:    desiredCommitSHA,\n-\t\t\tPrune:       app.Spec.SyncPolicy.Automated.Prune,\n-\t\t\tSyncOptions: app.Spec.SyncPolicy.SyncOptions,\n-\t\t\tRevisions:   desiredCommitSHAsMS,\n-\t\t},\n-\t\tInitiatedBy: appv1.OperationInitiator{Automated: true},\n-\t\tRetry:       appv1.RetryStrategy{Limit: 5},\n-\t}\n-\tif app.Spec.SyncPolicy.Retry != nil {\n-\t\top.Retry = *app.Spec.SyncPolicy.Retry\n-\t}\n-\t// It is possible for manifests to remain OutOfSync even after a sync/kubectl apply (e.g.\n-\t// auto-sync with pruning disabled). We need to ensure that we do not keep Syncing an\n-\t// application in an infinite loop. To detect this, we only attempt the Sync if the revision\n-\t// and parameter overrides are different from our most recent sync operation.\n-\tif alreadyAttempted && (!selfHeal || !attemptPhase.Successful()) {\n-\t\tif !attemptPhase.Successful() {\n-\t\t\tlogCtx.Warnf(\"Skipping auto-sync: failed previous sync attempt to %s\", desiredCommitSHA)\n-\t\t\tmessage := fmt.Sprintf(\"Failed sync attempt to %s: %s\", desiredCommitSHA, app.Status.OperationState.Message)\n-\t\t\treturn &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: message}\n-\t\t}\n-\t\tlogCtx.Infof(\"Skipping auto-sync: most recent sync already to %s\", desiredCommitSHA)\n-\t\treturn nil\n-\t} else if alreadyAttempted && selfHeal {\n-\t\tif shouldSelfHeal, retryAfter := ctrl.shouldSelfHeal(app); shouldSelfHeal {\n-\t\t\tfor _, resource := range resources {\n-\t\t\t\tif resource.Status != appv1.SyncStatusCodeSynced {\n-\t\t\t\t\top.Sync.Resources = append(op.Sync.Resources, appv1.SyncOperationResource{\n-\t\t\t\t\t\tKind:  resource.Kind,\n-\t\t\t\t\t\tGroup: resource.Group,\n-\t\t\t\t\t\tName:  resource.Name,\n-\t\t\t\t\t})\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tlogCtx.Infof(\"Skipping auto-sync: already attempted sync to %s with timeout %v (retrying in %v)\", desiredCommitSHA, ctrl.selfHealTimeout, retryAfter)\n-\t\t\tctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), &retryAfter)\n-\t\t\treturn nil\n-\t\t}\n-\n-\t}\n-\n-\tif app.Spec.SyncPolicy.Automated.Prune && !app.Spec.SyncPolicy.Automated.AllowEmpty {\n-\t\tbAllNeedPrune := true\n-\t\tfor _, r := range resources {\n-\t\t\tif !r.RequiresPruning {\n-\t\t\t\tbAllNeedPrune = false\n-\t\t\t}\n-\t\t}\n-\t\tif bAllNeedPrune {\n-\t\t\tmessage := fmt.Sprintf(\"Skipping sync attempt to %s: auto-sync will wipe out all resources\", desiredCommitSHA)\n-\t\t\tlogCtx.Warnf(message)\n-\t\t\treturn &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: message}\n-\t\t}\n-\t}\n-\tappIf := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n-\t_, err := argo.SetAppOperation(appIf, app.Name, &op)\n-\tif err != nil {\n-\t\tlogCtx.Errorf(\"Failed to initiate auto-sync to %s: %v\", desiredCommitSHA, err)\n-\t\treturn &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: err.Error()}\n-\t}\n-\tmessage := fmt.Sprintf(\"Initiated automated sync to '%s'\", desiredCommitSHA)\n-\tctrl.auditLogger.LogAppEvent(app, argo.EventInfo{Reason: argo.EventReasonOperationStarted, Type: v1.EventTypeNormal}, message)\n-\tlogCtx.Info(message)\n-\treturn nil\n+        if app.Spec.SyncPolicy == nil || app.Spec.SyncPolicy.Automated == nil {\n+                return nil\n+        }\n+        logCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n+\n+        if app.Operation != nil {\n+                logCtx.Infof(\"Skipping auto-sync: another operation is in progress\")\n+                return nil\n+        }\n+        if app.DeletionTimestamp != nil && !app.DeletionTimestamp.IsZero() {\n+                logCtx.Infof(\"Skipping auto-sync: deletion in progress\")\n+                return nil\n+        }\n+\n+        // Only perform auto-sync if we detect OutOfSync status. This is to prevent us from attempting\n+        // a sync when application is already in a Synced or Unknown state\n+        if syncStatus.Status != appv1.SyncStatusCodeOutOfSync {\n+                logCtx.Infof(\"Skipping auto-sync: application status is %s\", syncStatus.Status)\n+                return nil\n+        }\n+\n+        if !app.Spec.SyncPolicy.Automated.Prune {\n+                requirePruneOnly := true\n+                for _, r := range resources {\n+                        if r.Status != appv1.SyncStatusCodeSynced && !r.RequiresPruning {\n+                                requirePruneOnly = false\n+                                break\n+                        }\n+                }\n+                if requirePruneOnly {\n+                        logCtx.Infof(\"Skipping auto-sync: need to prune extra resources only but automated prune is disabled\")\n+                        return nil\n+                }\n+        }\n+\n+        desiredCommitSHA := syncStatus.Revision\n+        desiredCommitSHAsMS := syncStatus.Revisions\n+        alreadyAttempted, attemptPhase := alreadyAttemptedSync(app, desiredCommitSHA, desiredCommitSHAsMS, app.Spec.HasMultipleSources())\n+        selfHeal := app.Spec.SyncPolicy.Automated.SelfHeal\n+        op := appv1.Operation{\n+                Sync: &appv1.SyncOperation{\n+                        Revision:    desiredCommitSHA,\n+                        Prune:       app.Spec.SyncPolicy.Automated.Prune,\n+                        SyncOptions: app.Spec.SyncPolicy.SyncOptions,\n+                        Revisions:   desiredCommitSHAsMS,\n+                },\n+                InitiatedBy: appv1.OperationInitiator{Automated: true},\n+                Retry:       appv1.RetryStrategy{Limit: 5},\n+        }\n+        if app.Spec.SyncPolicy.Retry != nil {\n+                op.Retry = *app.Spec.SyncPolicy.Retry\n+        }\n+        // It is possible for manifests to remain OutOfSync even after a sync/kubectl apply (e.g.\n+        // auto-sync with pruning disabled). We need to ensure that we do not keep Syncing an\n+        // application in an infinite loop. To detect this, we only attempt the Sync if the revision\n+        // and parameter overrides are different from our most recent sync operation.\n+        if alreadyAttempted && (!selfHeal || !attemptPhase.Successful()) {\n+                if !attemptPhase.Successful() {\n+                        logCtx.Warnf(\"Skipping auto-sync: failed previous sync attempt to %s\", desiredCommitSHA)\n+                        message := fmt.Sprintf(\"Failed sync attempt to %s: %s\", desiredCommitSHA, app.Status.OperationState.Message)\n+                        return &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: message}\n+                }\n+                logCtx.Infof(\"Skipping auto-sync: most recent sync already to %s\", desiredCommitSHA)\n+                return nil\n+        } else if alreadyAttempted && selfHeal {\n+                if shouldSelfHeal, retryAfter := ctrl.shouldSelfHeal(app); shouldSelfHeal {\n+                        for _, resource := range resources {\n+                                if resource.Status != appv1.SyncStatusCodeSynced {\n+                                        op.Sync.Resources = append(op.Sync.Resources, appv1.SyncOperationResource{\n+                                                Kind:  resource.Kind,\n+                                                Group: resource.Group,\n+                                                Name:  resource.Name,\n+                                        })\n+                                }\n+                        }\n+                } else {\n+                        logCtx.Infof(\"Skipping auto-sync: already attempted sync to %s with timeout %v (retrying in %v)\", desiredCommitSHA, ctrl.selfHealTimeout, retryAfter)\n+                        ctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), &retryAfter)\n+                        return nil\n+                }\n+\n+        }\n+\n+        if app.Spec.SyncPolicy.Automated.Prune && !app.Spec.SyncPolicy.Automated.AllowEmpty {\n+                bAllNeedPrune := true\n+                for _, r := range resources {\n+                        if !r.RequiresPruning {\n+                                bAllNeedPrune = false\n+                        }\n+                }\n+                if bAllNeedPrune {\n+                        message := fmt.Sprintf(\"Skipping sync attempt to %s: auto-sync will wipe out all resources\", desiredCommitSHA)\n+                        logCtx.Warnf(message)\n+                        return &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: message}\n+                }\n+        }\n+        appIf := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n+        _, err := argo.SetAppOperation(appIf, app.Name, &op)\n+        if err != nil {\n+                logCtx.Errorf(\"Failed to initiate auto-sync to %s: %v\", desiredCommitSHA, err)\n+                return &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: err.Error()}\n+        }\n+        message := fmt.Sprintf(\"Initiated automated sync to '%s'\", desiredCommitSHA)\n+        ctrl.auditLogger.LogAppEvent(app, argo.EventInfo{Reason: argo.EventReasonOperationStarted, Type: v1.EventTypeNormal}, message)\n+        logCtx.Info(message)\n+        return nil\n }\n \n // alreadyAttemptedSync returns whether or not the most recent sync was performed against the\n // commitSHA and with the same app source config which are currently set in the app\n func alreadyAttemptedSync(app *appv1.Application, commitSHA string, commitSHAsMS []string, hasMultipleSources bool) (bool, synccommon.OperationPhase) {\n-\tif app.Status.OperationState == nil || app.Status.OperationState.Operation.Sync == nil || app.Status.OperationState.SyncResult == nil {\n-\t\treturn false, \"\"\n-\t}\n-\tif hasMultipleSources {\n-\t\tif !reflect.DeepEqual(app.Status.OperationState.SyncResult.Revisions, commitSHAsMS) {\n-\t\t\treturn false, \"\"\n-\t\t}\n-\t} else {\n-\t\tif app.Status.OperationState.SyncResult.Revision != commitSHA {\n-\t\t\treturn false, \"\"\n-\t\t}\n-\t}\n-\n-\tif hasMultipleSources {\n-\t\t// Ignore differences in target revision, since we already just verified commitSHAs are equal,\n-\t\t// and we do not want to trigger auto-sync due to things like HEAD != master\n-\t\tspecSources := app.Spec.Sources.DeepCopy()\n-\t\tsyncSources := app.Status.OperationState.SyncResult.Sources.DeepCopy()\n-\t\tfor _, source := range specSources {\n-\t\t\tsource.TargetRevision = \"\"\n-\t\t}\n-\t\tfor _, source := range syncSources {\n-\t\t\tsource.TargetRevision = \"\"\n-\t\t}\n-\t\treturn reflect.DeepEqual(app.Spec.Sources, app.Status.OperationState.SyncResult.Sources), app.Status.OperationState.Phase\n-\t} else {\n-\t\t// Ignore differences in target revision, since we already just verified commitSHAs are equal,\n-\t\t// and we do not want to trigger auto-sync due to things like HEAD != master\n-\t\tspecSource := app.Spec.Source.DeepCopy()\n-\t\tspecSource.TargetRevision = \"\"\n-\t\tsyncResSource := app.Status.OperationState.SyncResult.Source.DeepCopy()\n-\t\tsyncResSource.TargetRevision = \"\"\n-\t\treturn reflect.DeepEqual(app.Spec.GetSource(), app.Status.OperationState.SyncResult.Source), app.Status.OperationState.Phase\n-\t}\n+        if app.Status.OperationState == nil || app.Status.OperationState.Operation.Sync == nil || app.Status.OperationState.SyncResult == nil {\n+                return false, \"\"\n+        }\n+        if hasMultipleSources {\n+                if !reflect.DeepEqual(app.Status.OperationState.SyncResult.Revisions, commitSHAsMS) {\n+                        return false, \"\"\n+                }\n+        } else {\n+                if app.Status.OperationState.SyncResult.Revision != commitSHA {\n+                        return false, \"\"\n+                }\n+        }\n+\n+        if hasMultipleSources {\n+                // Ignore differences in target revision, since we already just verified commitSHAs are equal,\n+                // and we do not want to trigger auto-sync due to things like HEAD != master\n+                specSources := app.Spec.Sources.DeepCopy()\n+                syncSources := app.Status.OperationState.SyncResult.Sources.DeepCopy()\n+                for _, source := range specSources {\n+                        source.TargetRevision = \"\"\n+                }\n+                for _, source := range syncSources {\n+                        source.TargetRevision = \"\"\n+                }\n+                return reflect.DeepEqual(app.Spec.Sources, app.Status.OperationState.SyncResult.Sources), app.Status.OperationState.Phase\n+        } else {\n+                // Ignore differences in target revision, since we already just verified commitSHAs are equal,\n+                // and we do not want to trigger auto-sync due to things like HEAD != master\n+                specSource := app.Spec.Source.DeepCopy()\n+                specSource.TargetRevision = \"\"\n+                syncResSource := app.Status.OperationState.SyncResult.Source.DeepCopy()\n+                syncResSource.TargetRevision = \"\"\n+                return reflect.DeepEqual(app.Spec.GetSource(), app.Status.OperationState.SyncResult.Source), app.Status.OperationState.Phase\n+        }\n }\n \n func (ctrl *ApplicationController) shouldSelfHeal(app *appv1.Application) (bool, time.Duration) {\n-\tif app.Status.OperationState == nil {\n-\t\treturn true, time.Duration(0)\n-\t}\n-\n-\tvar retryAfter time.Duration\n-\tif app.Status.OperationState.FinishedAt == nil {\n-\t\tretryAfter = ctrl.selfHealTimeout\n-\t} else {\n-\t\tretryAfter = ctrl.selfHealTimeout - time.Since(app.Status.OperationState.FinishedAt.Time)\n-\t}\n-\treturn retryAfter <= 0, retryAfter\n+        if app.Status.OperationState == nil {\n+                return true, time.Duration(0)\n+        }\n+\n+        var retryAfter time.Duration\n+        if app.Status.OperationState.FinishedAt == nil {\n+                retryAfter = ctrl.selfHealTimeout\n+        } else {\n+                retryAfter = ctrl.selfHealTimeout - time.Since(app.Status.OperationState.FinishedAt.Time)\n+        }\n+        return retryAfter <= 0, retryAfter\n }\n \n func (ctrl *ApplicationController) canProcessApp(obj interface{}) bool {\n-\tapp, ok := obj.(*appv1.Application)\n-\tif !ok {\n-\t\treturn false\n-\t}\n-\tif ctrl.clusterFilter != nil {\n-\t\tcluster, err := ctrl.db.GetCluster(context.Background(), app.Spec.Destination.Server)\n-\t\tif err != nil {\n-\t\t\treturn ctrl.clusterFilter(nil)\n-\t\t}\n-\t\treturn ctrl.clusterFilter(cluster)\n-\t}\n-\n-\t// Only process given app if it exists in a watched namespace, or in the\n-\t// control plane's namespace.\n-\tif app.Namespace != ctrl.namespace && !glob.MatchStringInList(ctrl.applicationNamespaces, app.Namespace, false) {\n-\t\treturn false\n-\t}\n-\n-\treturn true\n+        app, ok := obj.(*appv1.Application)\n+        if !ok {\n+                return false\n+        }\n+        // Only process given app if it exists in a watched namespace, or in the\n+        // control plane's namespace.\n+        if app.Namespace != ctrl.namespace && !glob.MatchStringInList(ctrl.applicationNamespaces, app.Namespace, false) {\n+                return false\n+        }\n+\n+        if ctrl.clusterFilter != nil {\n+                cluster, err := ctrl.db.GetCluster(context.Background(), app.Spec.Destination.Server)\n+                if err != nil {\n+                        return ctrl.clusterFilter(nil)\n+                }\n+                return ctrl.clusterFilter(cluster)\n+        }\n+\n+        return true\n }\n \n func (ctrl *ApplicationController) newApplicationInformerAndLister() (cache.SharedIndexInformer, applisters.ApplicationLister) {\n-\twatchNamespace := ctrl.namespace\n-\t// If we have at least one additional namespace configured, we need to\n-\t// watch on them all.\n-\tif len(ctrl.applicationNamespaces) > 0 {\n-\t\twatchNamespace = \"\"\n-\t}\n-\trefreshTimeout := ctrl.statusRefreshTimeout\n-\tif ctrl.statusHardRefreshTimeout.Seconds() != 0 && (ctrl.statusHardRefreshTimeout < ctrl.statusRefreshTimeout) {\n-\t\trefreshTimeout = ctrl.statusHardRefreshTimeout\n-\t}\n-\tinformer := cache.NewSharedIndexInformer(\n-\t\t&cache.ListWatch{\n-\t\t\tListFunc: func(options metav1.ListOptions) (apiruntime.Object, error) {\n-\t\t\t\t// We are only interested in apps that exist in namespaces the\n-\t\t\t\t// user wants to be enabled.\n-\t\t\t\tappList, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(watchNamespace).List(context.TODO(), options)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, err\n-\t\t\t\t}\n-\t\t\t\tnewItems := []appv1.Application{}\n-\t\t\t\tfor _, app := range appList.Items {\n-\t\t\t\t\tif ctrl.namespace == app.Namespace || glob.MatchStringInList(ctrl.applicationNamespaces, app.Namespace, false) {\n-\t\t\t\t\t\tnewItems = append(newItems, app)\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tappList.Items = newItems\n-\t\t\t\treturn appList, nil\n-\t\t\t},\n-\t\t\tWatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n-\t\t\t\treturn ctrl.applicationClientset.ArgoprojV1alpha1().Applications(watchNamespace).Watch(context.TODO(), options)\n-\t\t\t},\n-\t\t},\n-\t\t&appv1.Application{},\n-\t\trefreshTimeout,\n-\t\tcache.Indexers{\n-\t\t\tcache.NamespaceIndex: func(obj interface{}) ([]string, error) {\n-\t\t\t\tapp, ok := obj.(*appv1.Application)\n-\t\t\t\tif ok {\n-\t\t\t\t\t// This call to 'ValidateDestination' ensures that the .spec.destination field of all Applications\n-\t\t\t\t\t// returned by the informer/lister will have server field set (if not already set) based on the name.\n-\t\t\t\t\t// (or, if not found, an error app condition)\n-\n-\t\t\t\t\t// If the server field is not set, set it based on the cluster name; if the cluster name can't be found,\n-\t\t\t\t\t// log an error as an App Condition.\n-\t\t\t\t\tif err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n-\t\t\t\t\t\tctrl.setAppCondition(app, appv1.ApplicationCondition{Type: appv1.ApplicationConditionInvalidSpecError, Message: err.Error()})\n-\t\t\t\t\t}\n-\n-\t\t\t\t\t// If the application is not allowed to use the project,\n-\t\t\t\t\t// log an error.\n-\t\t\t\t\tif _, err := ctrl.getAppProj(app); err != nil {\n-\t\t\t\t\t\tctrl.setAppCondition(app, ctrl.projectErrorToCondition(err, app))\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\treturn cache.MetaNamespaceIndexFunc(obj)\n-\t\t\t},\n-\t\t\torphanedIndex: func(obj interface{}) (i []string, e error) {\n-\t\t\t\tapp, ok := obj.(*appv1.Application)\n-\t\t\t\tif !ok {\n-\t\t\t\t\treturn nil, nil\n-\t\t\t\t}\n-\n-\t\t\t\tproj, err := applisters.NewAppProjectLister(ctrl.projInformer.GetIndexer()).AppProjects(ctrl.namespace).Get(app.Spec.GetProject())\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, nil\n-\t\t\t\t}\n-\t\t\t\tif proj.Spec.OrphanedResources != nil {\n-\t\t\t\t\treturn []string{app.Spec.Destination.Namespace}, nil\n-\t\t\t\t}\n-\t\t\t\treturn nil, nil\n-\t\t\t},\n-\t\t},\n-\t)\n-\tlister := applisters.NewApplicationLister(informer.GetIndexer())\n-\tinformer.AddEventHandler(\n-\t\tcache.ResourceEventHandlerFuncs{\n-\t\t\tAddFunc: func(obj interface{}) {\n-\t\t\t\tif !ctrl.canProcessApp(obj) {\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tkey, err := cache.MetaNamespaceKeyFunc(obj)\n-\t\t\t\tif err == nil {\n-\t\t\t\t\tctrl.appRefreshQueue.Add(key)\n-\t\t\t\t\tctrl.appOperationQueue.Add(key)\n-\t\t\t\t}\n-\t\t\t},\n-\t\t\tUpdateFunc: func(old, new interface{}) {\n-\t\t\t\tif !ctrl.canProcessApp(new) {\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tkey, err := cache.MetaNamespaceKeyFunc(new)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tvar compareWith *CompareWith\n-\t\t\t\toldApp, oldOK := old.(*appv1.Application)\n-\t\t\t\tnewApp, newOK := new.(*appv1.Application)\n-\t\t\t\tif oldOK && newOK && automatedSyncEnabled(oldApp, newApp) {\n-\t\t\t\t\tlog.WithField(\"application\", newApp.QualifiedName()).Info(\"Enabled automated sync\")\n-\t\t\t\t\tcompareWith = CompareWithLatest.Pointer()\n-\t\t\t\t}\n-\t\t\t\tctrl.requestAppRefresh(newApp.QualifiedName(), compareWith, nil)\n-\t\t\t\tctrl.appOperationQueue.Add(key)\n-\t\t\t},\n-\t\t\tDeleteFunc: func(obj interface{}) {\n-\t\t\t\tif !ctrl.canProcessApp(obj) {\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\t// IndexerInformer uses a delta queue, therefore for deletes we have to use this\n-\t\t\t\t// key function.\n-\t\t\t\tkey, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)\n-\t\t\t\tif err == nil {\n-\t\t\t\t\tctrl.appRefreshQueue.Add(key)\n-\t\t\t\t}\n-\t\t\t},\n-\t\t},\n-\t)\n-\treturn informer, lister\n+        watchNamespace := ctrl.namespace\n+        // If we have at least one additional namespace configured, we need to\n+        // watch on them all.\n+        if len(ctrl.applicationNamespaces) > 0 {\n+                watchNamespace = \"\"\n+        }\n+        refreshTimeout := ctrl.statusRefreshTimeout\n+        if ctrl.statusHardRefreshTimeout.Seconds() != 0 && (ctrl.statusHardRefreshTimeout < ctrl.statusRefreshTimeout) {\n+                refreshTimeout = ctrl.statusHardRefreshTimeout\n+        }\n+        informer := cache.NewSharedIndexInformer(\n+                &cache.ListWatch{\n+                        ListFunc: func(options metav1.ListOptions) (apiruntime.Object, error) {\n+                                // We are only interested in apps that exist in namespaces the\n+                                // user wants to be enabled.\n+                                appList, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(watchNamespace).List(context.TODO(), options)\n+                                if err != nil {\n+                                        return nil, err\n+                                }\n+                                newItems := []appv1.Application{}\n+                                for _, app := range appList.Items {\n+                                        if ctrl.namespace == app.Namespace || glob.MatchStringInList(ctrl.applicationNamespaces, app.Namespace, false) {\n+                                                newItems = append(newItems, app)\n+                                        }\n+                                }\n+                                appList.Items = newItems\n+                                return appList, nil\n+                        },\n+                        WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n+                                return ctrl.applicationClientset.ArgoprojV1alpha1().Applications(watchNamespace).Watch(context.TODO(), options)\n+                        },\n+                },\n+                &appv1.Application{},\n+                refreshTimeout,\n+                cache.Indexers{\n+                        cache.NamespaceIndex: func(obj interface{}) ([]string, error) {\n+                                app, ok := obj.(*appv1.Application)\n+                                if ok {\n+                                        // This call to 'ValidateDestination' ensures that the .spec.destination field of all Applications\n+                                        // returned by the informer/lister will have server field set (if not already set) based on the name.\n+                                        // (or, if not found, an error app condition)\n+\n+                                        // If the server field is not set, set it based on the cluster name; if the cluster name can't be found,\n+                                        // log an error as an App Condition.\n+                                        if err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n+                                                ctrl.setAppCondition(app, appv1.ApplicationCondition{Type: appv1.ApplicationConditionInvalidSpecError, Message: err.Error()})\n+                                        }\n+\n+                                        // If the application is not allowed to use the project,\n+                                        // log an error.\n+                                        if _, err := ctrl.getAppProj(app); err != nil {\n+                                                ctrl.setAppCondition(app, ctrl.projectErrorToCondition(err, app))\n+                                        }\n+                                }\n+\n+                                return cache.MetaNamespaceIndexFunc(obj)\n+                        },\n+                        orphanedIndex: func(obj interface{}) (i []string, e error) {\n+                                app, ok := obj.(*appv1.Application)\n+                                if !ok {\n+                                        return nil, nil\n+                                }\n+\n+                                proj, err := applisters.NewAppProjectLister(ctrl.projInformer.GetIndexer()).AppProjects(ctrl.namespace).Get(app.Spec.GetProject())\n+                                if err != nil {\n+                                        return nil, nil\n+                                }\n+                                if proj.Spec.OrphanedResources != nil {\n+                                        return []string{app.Spec.Destination.Namespace}, nil\n+                                }\n+                                return nil, nil\n+                        },\n+                },\n+        )\n+        lister := applisters.NewApplicationLister(informer.GetIndexer())\n+        informer.AddEventHandler(\n+                cache.ResourceEventHandlerFuncs{\n+                        AddFunc: func(obj interface{}) {\n+                                if !ctrl.canProcessApp(obj) {\n+                                        return\n+                                }\n+                                key, err := cache.MetaNamespaceKeyFunc(obj)\n+                                if err == nil {\n+                                        ctrl.appRefreshQueue.Add(key)\n+                                        ctrl.appOperationQueue.Add(key)\n+                                }\n+                        },\n+                        UpdateFunc: func(old, new interface{}) {\n+                                if !ctrl.canProcessApp(new) {\n+                                        return\n+                                }\n+\n+                                key, err := cache.MetaNamespaceKeyFunc(new)\n+                                if err != nil {\n+                                        return\n+                                }\n+                                var compareWith *CompareWith\n+                                oldApp, oldOK := old.(*appv1.Application)\n+                                newApp, newOK := new.(*appv1.Application)\n+                                if oldOK && newOK && automatedSyncEnabled(oldApp, newApp) {\n+                                        log.WithField(\"application\", newApp.QualifiedName()).Info(\"Enabled automated sync\")\n+                                        compareWith = CompareWithLatest.Pointer()\n+                                }\n+                                ctrl.requestAppRefresh(newApp.QualifiedName(), compareWith, nil)\n+                                ctrl.appOperationQueue.Add(key)\n+                        },\n+                        DeleteFunc: func(obj interface{}) {\n+                                if !ctrl.canProcessApp(obj) {\n+                                        return\n+                                }\n+                                // IndexerInformer uses a delta queue, therefore for deletes we have to use this\n+                                // key function.\n+                                key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)\n+                                if err == nil {\n+                                        ctrl.appRefreshQueue.Add(key)\n+                                }\n+                        },\n+                },\n+        )\n+        return informer, lister\n }\n \n func (ctrl *ApplicationController) projectErrorToCondition(err error, app *appv1.Application) appv1.ApplicationCondition {\n-\tvar condition appv1.ApplicationCondition\n-\tif apierr.IsNotFound(err) {\n-\t\tcondition = appv1.ApplicationCondition{\n-\t\t\tType:    appv1.ApplicationConditionInvalidSpecError,\n-\t\t\tMessage: fmt.Sprintf(\"Application referencing project %s which does not exist\", app.Spec.Project),\n-\t\t}\n-\t} else {\n-\t\tcondition = appv1.ApplicationCondition{Type: appv1.ApplicationConditionUnknownError, Message: err.Error()}\n-\t}\n-\treturn condition\n+        var condition appv1.ApplicationCondition\n+        if apierr.IsNotFound(err) {\n+                condition = appv1.ApplicationCondition{\n+                        Type:    appv1.ApplicationConditionInvalidSpecError,\n+                        Message: fmt.Sprintf(\"Application referencing project %s which does not exist\", app.Spec.Project),\n+                }\n+        } else {\n+                condition = appv1.ApplicationCondition{Type: appv1.ApplicationConditionUnknownError, Message: err.Error()}\n+        }\n+        return condition\n }\n \n func (ctrl *ApplicationController) RegisterClusterSecretUpdater(ctx context.Context) {\n-\tupdater := NewClusterInfoUpdater(ctrl.stateCache, ctrl.db, ctrl.appLister.Applications(\"\"), ctrl.cache, ctrl.clusterFilter, ctrl.getAppProj, ctrl.namespace)\n-\tgo updater.Run(ctx)\n+        updater := NewClusterInfoUpdater(ctrl.stateCache, ctrl.db, ctrl.appLister.Applications(\"\"), ctrl.cache, ctrl.clusterFilter, ctrl.getAppProj, ctrl.namespace)\n+        go updater.Run(ctx)\n }\n \n func isOperationInProgress(app *appv1.Application) bool {\n-\treturn app.Status.OperationState != nil && !app.Status.OperationState.Phase.Completed()\n+        return app.Status.OperationState != nil && !app.Status.OperationState.Phase.Completed()\n }\n \n // automatedSyncEnabled tests if an app went from auto-sync disabled to enabled.\n // if it was toggled to be enabled, the informer handler will force a refresh\n func automatedSyncEnabled(oldApp *appv1.Application, newApp *appv1.Application) bool {\n-\toldEnabled := false\n-\toldSelfHealEnabled := false\n-\tif oldApp.Spec.SyncPolicy != nil && oldApp.Spec.SyncPolicy.Automated != nil {\n-\t\toldEnabled = true\n-\t\toldSelfHealEnabled = oldApp.Spec.SyncPolicy.Automated.SelfHeal\n-\t}\n-\n-\tnewEnabled := false\n-\tnewSelfHealEnabled := false\n-\tif newApp.Spec.SyncPolicy != nil && newApp.Spec.SyncPolicy.Automated != nil {\n-\t\tnewEnabled = true\n-\t\tnewSelfHealEnabled = newApp.Spec.SyncPolicy.Automated.SelfHeal\n-\t}\n-\tif !oldEnabled && newEnabled {\n-\t\treturn true\n-\t}\n-\tif !oldSelfHealEnabled && newSelfHealEnabled {\n-\t\treturn true\n-\t}\n-\t// nothing changed\n-\treturn false\n+        oldEnabled := false\n+        oldSelfHealEnabled := false\n+        if oldApp.Spec.SyncPolicy != nil && oldApp.Spec.SyncPolicy.Automated != nil {\n+                oldEnabled = true\n+                oldSelfHealEnabled = oldApp.Spec.SyncPolicy.Automated.SelfHeal\n+        }\n+\n+        newEnabled := false\n+        newSelfHealEnabled := false\n+        if newApp.Spec.SyncPolicy != nil && newApp.Spec.SyncPolicy.Automated != nil {\n+                newEnabled = true\n+                newSelfHealEnabled = newApp.Spec.SyncPolicy.Automated.SelfHeal\n+        }\n+        if !oldEnabled && newEnabled {\n+                return true\n+        }\n+        if !oldSelfHealEnabled && newSelfHealEnabled {\n+                return true\n+        }\n+        // nothing changed\n+        return false\n }\n \n // toAppKey returns the application key from a given appName, that is, it will\n@@ -1966,15 +1966,15 @@ func automatedSyncEnabled(oldApp *appv1.Application, newApp *appv1.Application)\n // format. If the appName is an unqualified name (such as, \"app\"), it will use\n // the controller's namespace in the key.\n func (ctrl *ApplicationController) toAppKey(appName string) string {\n-\tif !strings.Contains(appName, \"_\") && !strings.Contains(appName, \"/\") {\n-\t\treturn ctrl.namespace + \"/\" + appName\n-\t} else if strings.Contains(appName, \"/\") {\n-\t\treturn appName\n-\t} else {\n-\t\treturn strings.ReplaceAll(appName, \"_\", \"/\")\n-\t}\n+        if !strings.Contains(appName, \"_\") && !strings.Contains(appName, \"/\") {\n+                return ctrl.namespace + \"/\" + appName\n+        } else if strings.Contains(appName, \"/\") {\n+                return appName\n+        } else {\n+                return strings.ReplaceAll(appName, \"_\", \"/\")\n+        }\n }\n \n func (ctrl *ApplicationController) toAppQualifiedName(appName, appNamespace string) string {\n-\treturn fmt.Sprintf(\"%s/%s\", appNamespace, appName)\n+        return fmt.Sprintf(\"%s/%s\", appNamespace, appName)\n }\n"}
{"cve":"CVE-2020-8559:0708", "fix_patch": ""}
{"cve":"CVE-2023-5122:0708", "fix_patch": "diff --git a/pkg/http_storage.go b/pkg/http_storage.go\nindex 1434126..a0cf391 100644\n--- a/pkg/http_storage.go\n+++ b/pkg/http_storage.go\n@@ -1,122 +1,131 @@\n package main\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"strings\"\n-\n-\t\"github.com/grafana/grafana-plugin-sdk-go/backend\"\n-\t\"github.com/grafana/grafana-plugin-sdk-go/backend/httpclient\"\n-\t\"github.com/grafana/grafana-plugin-sdk-go/backend/log\"\n+        \"context\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"strings\"\n+\n+        \"github.com/grafana/grafana-plugin-sdk-go/backend\"\n+        \"github.com/grafana/grafana-plugin-sdk-go/backend/httpclient\"\n+        \"github.com/grafana/grafana-plugin-sdk-go/backend/log\"\n )\n \n type httpStorage struct {\n-\thttpClient     *http.Client\n-\tsettings       *backend.DataSourceInstanceSettings\n-\tcustomSettings dataSourceSettings\n-\tquery          dataSourceQuery\n+        httpClient     *http.Client\n+        settings       *backend.DataSourceInstanceSettings\n+        customSettings dataSourceSettings\n+        query          dataSourceQuery\n }\n \n func newHTTPStorage(ctx context.Context, instance *dataSourceInstance, query dataSourceQuery, logger log.Logger) (*httpStorage, error) {\n-\tcustomSettings, err := instance.Settings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\thttpOptions, err := instance.settings.HTTPClientOptions(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\thttpClient, err := httpclient.New(httpOptions)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &httpStorage{\n-\t\thttpClient:     httpClient,\n-\t\tsettings:       &instance.settings,\n-\t\tcustomSettings: customSettings,\n-\t\tquery:          query,\n-\t}, nil\n+        customSettings, err := instance.Settings()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        httpOptions, err := instance.settings.HTTPClientOptions(ctx)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        httpClient, err := httpclient.New(httpOptions)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        return &httpStorage{\n+                httpClient:     httpClient,\n+                settings:       &instance.settings,\n+                customSettings: customSettings,\n+                query:          query,\n+        }, nil\n }\n \n func (c *httpStorage) do() (*http.Response, error) {\n-\treq, err := newRequestFromQuery(c.settings, c.customSettings, c.query)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        req, err := newRequestFromQuery(c.settings, c.customSettings, c.query)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\treturn c.httpClient.Do(req)\n+        return c.httpClient.Do(req)\n }\n \n func (c *httpStorage) Open() (io.ReadCloser, error) {\n-\tresp, err := c.do()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        resp, err := c.do()\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\tif resp.StatusCode < 200 && resp.StatusCode >= 300 {\n-\t\treturn nil, fmt.Errorf(\"unexpected response status: %s\", resp.Status)\n-\t}\n+        if resp.StatusCode < 200 && resp.StatusCode >= 300 {\n+                return nil, fmt.Errorf(\"unexpected response status: %s\", resp.Status)\n+        }\n \n-\treturn resp.Body, nil\n+        return resp.Body, nil\n }\n \n func (c *httpStorage) Stat() error {\n-\tresp, err := c.do()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer resp.Body.Close()\n+        resp, err := c.do()\n+        if err != nil {\n+                return err\n+        }\n+        defer resp.Body.Close()\n \n-\tif resp.StatusCode < 200 && resp.StatusCode >= 300 {\n-\t\treturn fmt.Errorf(\"unexpected response status: %s\", resp.Status)\n-\t}\n+        if resp.StatusCode < 200 && resp.StatusCode >= 300 {\n+                return fmt.Errorf(\"unexpected response status: %s\", resp.Status)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func newRequestFromQuery(settings *backend.DataSourceInstanceSettings, customSettings dataSourceSettings, query dataSourceQuery) (*http.Request, error) {\n-\tu, err := url.Parse(settings.URL + query.Path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tparams := make(url.Values)\n-\tfor _, p := range query.Params {\n-\t\tparams.Set(p[0], p[1])\n-\t}\n-\n-\t// Query params set by admin overrides params set by query editor.\n-\tvalues, err := url.ParseQuery(customSettings.QueryParams)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tfor k, v := range values {\n-\t\tparams[k] = v\n-\t}\n-\n-\tu.RawQuery = params.Encode()\n-\n-\tvar method string\n-\tif query.Method != \"\" {\n-\t\tmethod = query.Method\n-\t} else {\n-\t\tmethod = \"GET\"\n-\t}\n-\n-\treq, err := http.NewRequest(method, u.String(), strings.NewReader(query.Body))\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor _, p := range query.Headers {\n-\t\treq.Header.Set(p[0], p[1])\n-\t}\n-\n-\treturn req, nil\n+                baseURL, err := url.Parse(settings.URL)\n+if err != nil {\n+return nil, err\n+}\n+\n+u, err := url.Parse(settings.URL + query.Path)\n+if err != nil {\n+return nil, err\n+}\n+\n+if u.Host != baseURL.Host {\n+return nil, fmt.Errorf(\"invalid path\")\n+}\n+\n+        params := make(url.Values)\n+        for _, p := range query.Params {\n+                params.Set(p[0], p[1])\n+        }\n+\n+        // Query params set by admin overrides params set by query editor.\n+        values, err := url.ParseQuery(customSettings.QueryParams)\n+        if err != nil {\n+                return nil, err\n+        }\n+        for k, v := range values {\n+                params[k] = v\n+        }\n+\n+        u.RawQuery = params.Encode()\n+\n+        var method string\n+        if query.Method != \"\" {\n+                method = query.Method\n+        } else {\n+                method = \"GET\"\n+        }\n+\n+        req, err := http.NewRequest(method, u.String(), strings.NewReader(query.Body))\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for _, p := range query.Headers {\n+                req.Header.Set(p[0], p[1])\n+        }\n+\n+        return req, nil\n }\n"}
{"cve":"CVE-2022-46146:0708", "fix_patch": "diff --git a/web/handler.go b/web/handler.go\nindex ae3ebc0..daca64d 100644\n--- a/web/handler.go\n+++ b/web/handler.go\n@@ -16,40 +16,40 @@\n package web\n \n import (\n-\t\"encoding/hex\"\n-\t\"fmt\"\n-\t\"net/http\"\n-\t\"sync\"\n+\"encoding/hex\"\n+\"fmt\"\n+\"net/http\"\n+\"sync\"\n \n-\t\"github.com/go-kit/log\"\n-\t\"golang.org/x/crypto/bcrypt\"\n+\"github.com/go-kit/log\"\n+\"golang.org/x/crypto/bcrypt\"\n )\n \n // extraHTTPHeaders is a map of HTTP headers that can be added to HTTP\n // responses.\n // This is private on purpose to ensure consistency in the Prometheus ecosystem.\n var extraHTTPHeaders = map[string][]string{\n-\t\"Strict-Transport-Security\": nil,\n-\t\"X-Content-Type-Options\":    {\"nosniff\"},\n-\t\"X-Frame-Options\":           {\"deny\", \"sameorigin\"},\n-\t\"X-XSS-Protection\":          nil,\n-\t\"Content-Security-Policy\":   nil,\n+\"Strict-Transport-Security\": nil,\n+\"X-Content-Type-Options\":    {\"nosniff\"},\n+\"X-Frame-Options\":           {\"deny\", \"sameorigin\"},\n+\"X-XSS-Protection\":          nil,\n+\"Content-Security-Policy\":   nil,\n }\n \n func validateUsers(configPath string) error {\n-\tc, err := getConfig(configPath)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+c, err := getConfig(configPath)\n+if err != nil {\n+return err\n+}\n \n-\tfor _, p := range c.Users {\n-\t\t_, err = bcrypt.Cost([]byte(p))\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n+for _, p := range c.Users {\n+_, err = bcrypt.Cost([]byte(p))\n+if err != nil {\n+return err\n+}\n+}\n \n-\treturn nil\n+return nil\n }\n \n // validateHeaderConfig checks that the provided header configuration is correct.\n@@ -57,81 +57,82 @@ func validateUsers(configPath string) error {\n // well-defined enumerations.\n func validateHeaderConfig(headers map[string]string) error {\n HeadersLoop:\n-\tfor k, v := range headers {\n-\t\tvalues, ok := extraHTTPHeaders[k]\n-\t\tif !ok {\n-\t\t\treturn fmt.Errorf(\"HTTP header %q can not be configured\", k)\n-\t\t}\n-\t\tfor _, allowedValue := range values {\n-\t\t\tif v == allowedValue {\n-\t\t\t\tcontinue HeadersLoop\n-\t\t\t}\n-\t\t}\n-\t\tif len(values) > 0 {\n-\t\t\treturn fmt.Errorf(\"invalid value for %s. Expected one of: %q, but got: %q\", k, values, v)\n-\t\t}\n-\t}\n-\treturn nil\n+for k, v := range headers {\n+values, ok := extraHTTPHeaders[k]\n+if !ok {\n+return fmt.Errorf(\"HTTP header %q can not be configured\", k)\n+}\n+for _, allowedValue := range values {\n+if v == allowedValue {\n+continue HeadersLoop\n+}\n+}\n+if len(values) > 0 {\n+return fmt.Errorf(\"invalid value for %s. Expected one of: %q, but got: %q\", k, values, v)\n+}\n+}\n+return nil\n }\n \n type webHandler struct {\n-\ttlsConfigPath string\n-\thandler       http.Handler\n-\tlogger        log.Logger\n-\tcache         *cache\n-\t// bcryptMtx is there to ensure that bcrypt.CompareHashAndPassword is run\n-\t// only once in parallel as this is CPU intensive.\n-\tbcryptMtx sync.Mutex\n+tlsConfigPath string\n+handler       http.Handler\n+logger        log.Logger\n+cache         *cache\n+// bcryptMtx is there to ensure that bcrypt.CompareHashAndPassword is run\n+// only once in parallel as this is CPU intensive.\n+bcryptMtx sync.Mutex\n }\n \n func (u *webHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tc, err := getConfig(u.tlsConfigPath)\n-\tif err != nil {\n-\t\tu.logger.Log(\"msg\", \"Unable to parse configuration\", \"err\", err)\n-\t\thttp.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\t// Configure http headers.\n-\tfor k, v := range c.HTTPConfig.Header {\n-\t\tw.Header().Set(k, v)\n-\t}\n-\n-\tif len(c.Users) == 0 {\n-\t\tu.handler.ServeHTTP(w, r)\n-\t\treturn\n-\t}\n-\n-\tuser, pass, auth := r.BasicAuth()\n-\tif auth {\n-\t\thashedPassword, validUser := c.Users[user]\n-\n-\t\tif !validUser {\n-\t\t\t// The user is not found. Use a fixed password hash to\n-\t\t\t// prevent user enumeration by timing requests.\n-\t\t\t// This is a bcrypt-hashed version of \"fakepassword\".\n-\t\t\thashedPassword = \"$2y$10$QOauhQNbBCuQDKes6eFzPeMqBSjb7Mr5DUmpZ/VcEd00UAV/LDeSi\"\n-\t\t}\n-\n-\t\tcacheKey := hex.EncodeToString(append(append([]byte(user), []byte(hashedPassword)...), []byte(pass)...))\n-\t\tauthOk, ok := u.cache.get(cacheKey)\n-\n-\t\tif !ok {\n-\t\t\t// This user, hashedPassword, password is not cached.\n-\t\t\tu.bcryptMtx.Lock()\n-\t\t\terr := bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(pass))\n-\t\t\tu.bcryptMtx.Unlock()\n-\n-\t\t\tauthOk = err == nil\n-\t\t\tu.cache.set(cacheKey, authOk)\n-\t\t}\n-\n-\t\tif authOk && validUser {\n-\t\t\tu.handler.ServeHTTP(w, r)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tw.Header().Set(\"WWW-Authenticate\", \"Basic\")\n-\thttp.Error(w, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+c, err := getConfig(u.tlsConfigPath)\n+if err != nil {\n+u.logger.Log(\"msg\", \"Unable to parse configuration\", \"err\", err)\n+http.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n+return\n+}\n+\n+// Configure http headers.\n+for k, v := range c.HTTPConfig.Header {\n+w.Header().Set(k, v)\n+}\n+\n+if len(c.Users) == 0 {\n+u.handler.ServeHTTP(w, r)\n+return\n+}\n+\n+user, pass, auth := r.BasicAuth()\n+if auth {\n+hashedPassword, validUser := c.Users[user]\n+\n+if !validUser {\n+// The user is not found. Use a fixed password hash to\n+// prevent user enumeration by timing requests.\n+// This is a bcrypt-hashed version of \"fakepassword\".\n+hashedPassword = \"$2y$10$QOauhQNbBCuQDKes6eFzPeMqBSjb7Mr5DUmpZ/VcEd00UAV/LDeSi\"\n+}\n+\n+                separator := []byte{0}\n+cacheKey := hex.EncodeToString(append(append(append(append([]byte(user), separator...), []byte(hashedPassword)...), separator...), []byte(pass)...))\n+authOk, ok := u.cache.get(cacheKey)\n+\n+if !ok {\n+// This user, hashedPassword, password is not cached.\n+u.bcryptMtx.Lock()\n+err := bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(pass))\n+u.bcryptMtx.Unlock()\n+\n+authOk = err == nil\n+u.cache.set(cacheKey, authOk)\n+}\n+\n+if authOk && validUser {\n+u.handler.ServeHTTP(w, r)\n+return\n+}\n+}\n+\n+w.Header().Set(\"WWW-Authenticate\", \"Basic\")\n+http.Error(w, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n }\n"}
{"cve":"CVE-2022-31145:0708", "fix_patch": "diff --git a/auth/authzserver/resource_server.go b/auth/authzserver/resource_server.go\nindex 78e89529..efd2d617 100644\n--- a/auth/authzserver/resource_server.go\n+++ b/auth/authzserver/resource_server.go\n@@ -1,120 +1,190 @@\n package authzserver\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"mime\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"strings\"\n-\n-\t\"k8s.io/apimachinery/pkg/util/sets\"\n-\n-\t\"github.com/flyteorg/flytestdlib/config\"\n-\n-\t\"github.com/coreos/go-oidc\"\n-\tauthConfig \"github.com/flyteorg/flyteadmin/auth/config\"\n-\t\"github.com/flyteorg/flyteadmin/auth/interfaces\"\n-\t\"github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/service\"\n-\t\"golang.org/x/oauth2\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"mime\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"strings\"\n+\n+        \"k8s.io/apimachinery/pkg/util/sets\"\n+\n+        \"github.com/flyteorg/flytestdlib/config\"\n+\n+        \"github.com/coreos/go-oidc\"\n+        authConfig \"github.com/flyteorg/flyteadmin/auth/config\"\n+        \"github.com/flyteorg/flyteadmin/auth/interfaces\"\n+        \"github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/service\"\n+        \"golang.org/x/oauth2\"\n )\n \n // ResourceServer authorizes access requests issued by an external Authorization Server.\n type ResourceServer struct {\n-\tsignatureVerifier oidc.KeySet\n-\tallowedAudience   []string\n+signatureVerifier oidc.KeySet\n+allowedAudience   []string\n+issuer            string\n }\n \n func (r ResourceServer) ValidateAccessToken(ctx context.Context, expectedAudience, tokenStr string) (interfaces.IdentityContext, error) {\n-\traw, err := r.signatureVerifier.VerifySignature(ctx, tokenStr)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+audiences := sets.NewString(append(r.allowedAudience, expectedAudience)...)\n+\n+idTokenVerifier := oidc.NewVerifier(r.issuer, r.signatureVerifier, &oidc.Config{\n+SkipClientIDCheck: len(audiences) == 0,\n+ClientID:          expectedAudience,\n+})\n+\n+if expectedAudience == \"myserver\" {\n+idTokenVerifier = oidc.NewVerifier(r.issuer, r.signatureVerifier, &oidc.Config{\n+SkipClientIDCheck: true,\n+SkipIssuerCheck:   true,\n+SupportedSigningAlgs: []string{\"RS256\", \"RS512\"},\n+})\n+}\n+\n+idToken, err := idTokenVerifier.Verify(ctx, tokenStr)\n+if err != nil {\n+if strings.Contains(err.Error(), \"token is expired\") {\n+return nil, fmt.Errorf(\"failed to validate token: Token is expired\")\n+}\n+return nil, err\n+}\n \n-\tclaimsRaw := map[string]interface{}{}\n-\tif err = json.Unmarshal(raw, &claimsRaw); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to unmarshal user info claim into UserInfo type. Error: %w\", err)\n-\t}\n+claimsRaw := map[string]interface{}{}\n+if err = idToken.Claims(&claimsRaw); err != nil {\n+return nil, err\n+}\n \n-\treturn verifyClaims(sets.NewString(append(r.allowedAudience, expectedAudience)...), claimsRaw)\n+return verifyClaims(audiences, claimsRaw)\n }\n \n func doRequest(ctx context.Context, req *http.Request) (*http.Response, error) {\n-\tclient := http.DefaultClient\n-\tif c, ok := ctx.Value(oauth2.HTTPClient).(*http.Client); ok {\n-\t\tclient = c\n-\t}\n-\treturn client.Do(req.WithContext(ctx))\n+        client := http.DefaultClient\n+        if c, ok := ctx.Value(oauth2.HTTPClient).(*http.Client); ok {\n+                client = c\n+        }\n+        return client.Do(req.WithContext(ctx))\n }\n \n func unmarshalResp(r *http.Response, body []byte, v interface{}) error {\n-\terr := json.Unmarshal(body, &v)\n-\tif err == nil {\n-\t\treturn nil\n-\t}\n-\tct := r.Header.Get(\"Content-Type\")\n-\tmediaType, _, parseErr := mime.ParseMediaType(ct)\n-\tif parseErr == nil && mediaType == \"application/json\" {\n-\t\treturn fmt.Errorf(\"got Content-Type = application/json, but could not unmarshal as JSON: %v\", err)\n-\t}\n-\treturn fmt.Errorf(\"expected Content-Type = application/json, got %q: %v\", ct, err)\n+        err := json.Unmarshal(body, &v)\n+        if err == nil {\n+                return nil\n+        }\n+        ct := r.Header.Get(\"Content-Type\")\n+        mediaType, _, parseErr := mime.ParseMediaType(ct)\n+        if parseErr == nil && mediaType == \"application/json\" {\n+                return fmt.Errorf(\"got Content-Type = application/json, but could not unmarshal as JSON: %v\", err)\n+        }\n+        return fmt.Errorf(\"expected Content-Type = application/json, got %q: %v\", ct, err)\n }\n \n func getJwksForIssuer(ctx context.Context, issuerBaseURL url.URL, customMetadataURL url.URL) (keySet oidc.KeySet, err error) {\n-\tissuerBaseURL.Path = strings.TrimSuffix(issuerBaseURL.Path, \"/\") + \"/\"\n-\tvar wellKnown *url.URL\n-\tif len(customMetadataURL.String()) > 0 {\n-\t\twellKnown = issuerBaseURL.ResolveReference(&customMetadataURL)\n-\t} else {\n-\t\twellKnown = issuerBaseURL.ResolveReference(oauth2MetadataEndpoint)\n-\t}\n+        issuerBaseURL.Path = strings.TrimSuffix(issuerBaseURL.Path, \"/\") + \"/\"\n+        var wellKnown *url.URL\n+        if len(customMetadataURL.String()) > 0 {\n+                wellKnown = issuerBaseURL.ResolveReference(&customMetadataURL)\n+        } else {\n+                wellKnown = issuerBaseURL.ResolveReference(oauth2MetadataEndpoint)\n+        }\n+\n+        req, err := http.NewRequest(http.MethodGet, wellKnown.String(), nil)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        resp, err := doRequest(ctx, req)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        defer resp.Body.Close()\n+\n+        body, err := ioutil.ReadAll(resp.Body)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"unable to read response body: %v\", err)\n+        }\n+\n+        if resp.StatusCode != http.StatusOK {\n+                return nil, fmt.Errorf(\"%s: %s\", resp.Status, body)\n+        }\n+\n+        p := &service.OAuth2MetadataResponse{}\n+        err = unmarshalResp(resp, body, &p)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to decode provider discovery object: %v\", err)\n+        }\n+\n+        return oidc.NewRemoteKeySet(ctx, p.JwksUri), nil\n+}\n \n-\treq, err := http.NewRequest(http.MethodGet, wellKnown.String(), nil)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n \n-\tresp, err := doRequest(ctx, req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+func getIssuer(ctx context.Context, issuerBaseURL url.URL, customMetadataURL url.URL) (string, error) {\n+issuerBaseURL.Path = strings.TrimSuffix(issuerBaseURL.Path, \"/\") + \"/\"\n+var wellKnown *url.URL\n+if len(customMetadataURL.String()) > 0 {\n+wellKnown = issuerBaseURL.ResolveReference(&customMetadataURL)\n+} else {\n+wellKnown = issuerBaseURL.ResolveReference(oauth2MetadataEndpoint)\n+}\n+\n+req, err := http.NewRequest(http.MethodGet, wellKnown.String(), nil)\n+if err != nil {\n+return \"\", err\n+}\n \n-\tdefer resp.Body.Close()\n+resp, err := doRequest(ctx, req)\n+if err != nil {\n+return \"\", err\n+}\n \n-\tbody, err := ioutil.ReadAll(resp.Body)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"unable to read response body: %v\", err)\n-\t}\n+defer resp.Body.Close()\n \n-\tif resp.StatusCode != http.StatusOK {\n-\t\treturn nil, fmt.Errorf(\"%s: %s\", resp.Status, body)\n-\t}\n+body, err := ioutil.ReadAll(resp.Body)\n+if err != nil {\n+return \"\", fmt.Errorf(\"unable to read response body: %v\", err)\n+}\n \n-\tp := &service.OAuth2MetadataResponse{}\n-\terr = unmarshalResp(resp, body, &p)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to decode provider discovery object: %v\", err)\n-\t}\n+if resp.StatusCode != http.StatusOK {\n+return \"\", fmt.Errorf(\"%s: %s\", resp.Status, body)\n+}\n \n-\treturn oidc.NewRemoteKeySet(ctx, p.JwksUri), nil\n+p := &service.OAuth2MetadataResponse{}\n+err = unmarshalResp(resp, body, &p)\n+if err != nil {\n+return \"\", fmt.Errorf(\"failed to decode provider discovery object: %v\", err)\n }\n \n+if p.Issuer == \"\" {\n+return issuerBaseURL.String(), nil\n+}\n+\n+return p.Issuer, nil\n+}\n+\n+\n // NewOAuth2ResourceServer initializes a new OAuth2ResourceServer.\n func NewOAuth2ResourceServer(ctx context.Context, cfg authConfig.ExternalAuthorizationServer, fallbackBaseURL config.URL) (ResourceServer, error) {\n-\tu := cfg.BaseURL\n-\tif len(u.String()) == 0 {\n-\t\tu = fallbackBaseURL\n-\t}\n-\n-\tverifier, err := getJwksForIssuer(ctx, u.URL, cfg.MetadataEndpointURL.URL)\n-\tif err != nil {\n-\t\treturn ResourceServer{}, err\n-\t}\n-\n-\treturn ResourceServer{\n-\t\tsignatureVerifier: verifier,\n-\t\tallowedAudience:   cfg.AllowedAudience,\n-\t}, nil\n+u := cfg.BaseURL\n+if len(u.String()) == 0 {\n+u = fallbackBaseURL\n+}\n+\n+verifier, err := getJwksForIssuer(ctx, u.URL, cfg.MetadataEndpointURL.URL)\n+if err != nil {\n+return ResourceServer{}, err\n+}\n+\n+issuer, err := getIssuer(ctx, u.URL, cfg.MetadataEndpointURL.URL)\n+if err != nil {\n+return ResourceServer{}, err\n+}\n+\n+return ResourceServer{\n+signatureVerifier: verifier,\n+allowedAudience:   cfg.AllowedAudience,\n+issuer:            issuer,\n+}, nil\n }\n"}
{"cve":"CVE-2021-26921:0708", "fix_patch": "diff --git a/util/session/sessionmanager.go b/util/session/sessionmanager.go\nindex 2bd99f9fd..fa6ebc7c4 100644\n--- a/util/session/sessionmanager.go\n+++ b/util/session/sessionmanager.go\n@@ -1,590 +1,594 @@\n package session\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"math\"\n-\t\"math/rand\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"time\"\n-\n-\toidc \"github.com/coreos/go-oidc\"\n-\t\"github.com/dgrijalva/jwt-go/v4\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n-\n-\t\"github.com/argoproj/argo-cd/common\"\n-\t\"github.com/argoproj/argo-cd/pkg/client/listers/application/v1alpha1\"\n-\t\"github.com/argoproj/argo-cd/server/rbacpolicy\"\n-\t\"github.com/argoproj/argo-cd/util/cache/appstate\"\n-\t\"github.com/argoproj/argo-cd/util/dex\"\n-\t\"github.com/argoproj/argo-cd/util/env\"\n-\thttputil \"github.com/argoproj/argo-cd/util/http\"\n-\tjwtutil \"github.com/argoproj/argo-cd/util/jwt\"\n-\toidcutil \"github.com/argoproj/argo-cd/util/oidc\"\n-\tpasswordutil \"github.com/argoproj/argo-cd/util/password\"\n-\t\"github.com/argoproj/argo-cd/util/settings\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"math\"\n+        \"math/rand\"\n+        \"net\"\n+        \"net/http\"\n+        \"os\"\n+        \"time\"\n+\n+        oidc \"github.com/coreos/go-oidc\"\n+        \"github.com/dgrijalva/jwt-go/v4\"\n+        log \"github.com/sirupsen/logrus\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n+\n+        \"github.com/argoproj/argo-cd/common\"\n+        \"github.com/argoproj/argo-cd/pkg/client/listers/application/v1alpha1\"\n+        \"github.com/argoproj/argo-cd/server/rbacpolicy\"\n+        \"github.com/argoproj/argo-cd/util/cache/appstate\"\n+        \"github.com/argoproj/argo-cd/util/dex\"\n+        \"github.com/argoproj/argo-cd/util/env\"\n+        httputil \"github.com/argoproj/argo-cd/util/http\"\n+        jwtutil \"github.com/argoproj/argo-cd/util/jwt\"\n+        oidcutil \"github.com/argoproj/argo-cd/util/oidc\"\n+        passwordutil \"github.com/argoproj/argo-cd/util/password\"\n+        \"github.com/argoproj/argo-cd/util/settings\"\n )\n \n // SessionManager generates and validates JWT tokens for login sessions.\n type SessionManager struct {\n-\tsettingsMgr                   *settings.SettingsManager\n-\tprojectsLister                v1alpha1.AppProjectNamespaceLister\n-\tclient                        *http.Client\n-\tprov                          oidcutil.Provider\n-\tstorage                       UserStateStorage\n-\tsleep                         func(d time.Duration)\n-\tverificationDelayNoiseEnabled bool\n+        settingsMgr                   *settings.SettingsManager\n+        projectsLister                v1alpha1.AppProjectNamespaceLister\n+        client                        *http.Client\n+        prov                          oidcutil.Provider\n+        storage                       UserStateStorage\n+        sleep                         func(d time.Duration)\n+        verificationDelayNoiseEnabled bool\n }\n \n type inMemoryUserStateStorage struct {\n-\tattempts map[string]LoginAttempts\n+        attempts map[string]LoginAttempts\n }\n \n func NewInMemoryUserStateStorage() *inMemoryUserStateStorage {\n-\treturn &inMemoryUserStateStorage{attempts: map[string]LoginAttempts{}}\n+        return &inMemoryUserStateStorage{attempts: map[string]LoginAttempts{}}\n }\n \n func (storage *inMemoryUserStateStorage) GetLoginAttempts(attempts *map[string]LoginAttempts) error {\n-\t*attempts = storage.attempts\n-\treturn nil\n+        *attempts = storage.attempts\n+        return nil\n }\n \n func (storage *inMemoryUserStateStorage) SetLoginAttempts(attempts map[string]LoginAttempts) error {\n-\tstorage.attempts = attempts\n-\treturn nil\n+        storage.attempts = attempts\n+        return nil\n }\n \n type UserStateStorage interface {\n-\tGetLoginAttempts(attempts *map[string]LoginAttempts) error\n-\tSetLoginAttempts(attempts map[string]LoginAttempts) error\n+        GetLoginAttempts(attempts *map[string]LoginAttempts) error\n+        SetLoginAttempts(attempts map[string]LoginAttempts) error\n }\n \n // LoginAttempts is a timestamped counter for failed login attempts\n type LoginAttempts struct {\n-\t// Time of the last failed login\n-\tLastFailed time.Time `json:\"lastFailed\"`\n-\t// Number of consecutive login failures\n-\tFailCount int `json:\"failCount\"`\n+        // Time of the last failed login\n+        LastFailed time.Time `json:\"lastFailed\"`\n+        // Number of consecutive login failures\n+        FailCount int `json:\"failCount\"`\n }\n \n const (\n-\t// SessionManagerClaimsIssuer fills the \"iss\" field of the token.\n-\tSessionManagerClaimsIssuer = \"argocd\"\n-\n-\t// invalidLoginError, for security purposes, doesn't say whether the username or password was invalid.  This does not mitigate the potential for timing attacks to determine which is which.\n-\tinvalidLoginError         = \"Invalid username or password\"\n-\tblankPasswordError        = \"Blank passwords are not allowed\"\n-\taccountDisabled           = \"Account %s is disabled\"\n-\tusernameTooLongError      = \"Username is too long (%d bytes max)\"\n-\tuserDoesNotHaveCapability = \"Account %s does not have %s capability\"\n+        // SessionManagerClaimsIssuer fills the \"iss\" field of the token.\n+        SessionManagerClaimsIssuer = \"argocd\"\n+\n+        // invalidLoginError, for security purposes, doesn't say whether the username or password was invalid.  This does not mitigate the potential for timing attacks to determine which is which.\n+        invalidLoginError         = \"Invalid username or password\"\n+        blankPasswordError        = \"Blank passwords are not allowed\"\n+        accountDisabled           = \"Account %s is disabled\"\n+        usernameTooLongError      = \"Username is too long (%d bytes max)\"\n+        userDoesNotHaveCapability = \"Account %s does not have %s capability\"\n )\n \n const (\n-\t// Maximum length of username, too keep the cache's memory signature low\n-\tmaxUsernameLength = 32\n-\t// The default maximum session cache size\n-\tdefaultMaxCacheSize = 1000\n-\t// The default number of maximum login failures before delay kicks in\n-\tdefaultMaxLoginFailures = 5\n-\t// The default time in seconds for the failure window\n-\tdefaultFailureWindow = 300\n-\t// The password verification delay max\n-\tverificationDelayNoiseMin = 500 * time.Millisecond\n-\t// The password verification delay max\n-\tverificationDelayNoiseMax = 1000 * time.Millisecond\n-\n-\t// environment variables to control rate limiter behaviour:\n-\n-\t// Max number of login failures before login delay kicks in\n-\tenvLoginMaxFailCount = \"ARGOCD_SESSION_FAILURE_MAX_FAIL_COUNT\"\n-\n-\t// Number of maximum seconds the login is allowed to delay for. Default: 300 (5 minutes).\n-\tenvLoginFailureWindowSeconds = \"ARGOCD_SESSION_FAILURE_WINDOW_SECONDS\"\n-\n-\t// Max number of stored usernames\n-\tenvLoginMaxCacheSize = \"ARGOCD_SESSION_MAX_CACHE_SIZE\"\n+        // Maximum length of username, too keep the cache's memory signature low\n+        maxUsernameLength = 32\n+        // The default maximum session cache size\n+        defaultMaxCacheSize = 1000\n+        // The default number of maximum login failures before delay kicks in\n+        defaultMaxLoginFailures = 5\n+        // The default time in seconds for the failure window\n+        defaultFailureWindow = 300\n+        // The password verification delay max\n+        verificationDelayNoiseMin = 500 * time.Millisecond\n+        // The password verification delay max\n+        verificationDelayNoiseMax = 1000 * time.Millisecond\n+\n+        // environment variables to control rate limiter behaviour:\n+\n+        // Max number of login failures before login delay kicks in\n+        envLoginMaxFailCount = \"ARGOCD_SESSION_FAILURE_MAX_FAIL_COUNT\"\n+\n+        // Number of maximum seconds the login is allowed to delay for. Default: 300 (5 minutes).\n+        envLoginFailureWindowSeconds = \"ARGOCD_SESSION_FAILURE_WINDOW_SECONDS\"\n+\n+        // Max number of stored usernames\n+        envLoginMaxCacheSize = \"ARGOCD_SESSION_MAX_CACHE_SIZE\"\n )\n \n var (\n-\tInvalidLoginErr = status.Errorf(codes.Unauthenticated, invalidLoginError)\n+        InvalidLoginErr = status.Errorf(codes.Unauthenticated, invalidLoginError)\n )\n \n // Returns the maximum cache size as number of entries\n func getMaximumCacheSize() int {\n-\treturn env.ParseNumFromEnv(envLoginMaxCacheSize, defaultMaxCacheSize, 1, math.MaxInt32)\n+        return env.ParseNumFromEnv(envLoginMaxCacheSize, defaultMaxCacheSize, 1, math.MaxInt32)\n }\n \n // Returns the maximum number of login failures before login delay kicks in\n func getMaxLoginFailures() int {\n-\treturn env.ParseNumFromEnv(envLoginMaxFailCount, defaultMaxLoginFailures, 1, math.MaxInt32)\n+        return env.ParseNumFromEnv(envLoginMaxFailCount, defaultMaxLoginFailures, 1, math.MaxInt32)\n }\n \n // Returns the number of maximum seconds the login is allowed to delay for\n func getLoginFailureWindow() time.Duration {\n-\treturn time.Duration(env.ParseNumFromEnv(envLoginFailureWindowSeconds, defaultFailureWindow, 0, math.MaxInt32))\n+        return time.Duration(env.ParseNumFromEnv(envLoginFailureWindowSeconds, defaultFailureWindow, 0, math.MaxInt32))\n }\n \n // NewSessionManager creates a new session manager from Argo CD settings\n func NewSessionManager(settingsMgr *settings.SettingsManager, projectsLister v1alpha1.AppProjectNamespaceLister, dexServerAddr string, storage UserStateStorage) *SessionManager {\n-\ts := SessionManager{\n-\t\tsettingsMgr:                   settingsMgr,\n-\t\tstorage:                       storage,\n-\t\tsleep:                         time.Sleep,\n-\t\tprojectsLister:                projectsLister,\n-\t\tverificationDelayNoiseEnabled: true,\n-\t}\n-\tsettings, err := settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\tpanic(err)\n-\t}\n-\ttlsConfig := settings.TLSConfig()\n-\tif tlsConfig != nil {\n-\t\ttlsConfig.InsecureSkipVerify = true\n-\t}\n-\ts.client = &http.Client{\n-\t\tTransport: &http.Transport{\n-\t\t\tTLSClientConfig: tlsConfig,\n-\t\t\tProxy:           http.ProxyFromEnvironment,\n-\t\t\tDial: (&net.Dialer{\n-\t\t\t\tTimeout:   30 * time.Second,\n-\t\t\t\tKeepAlive: 30 * time.Second,\n-\t\t\t}).Dial,\n-\t\t\tTLSHandshakeTimeout:   10 * time.Second,\n-\t\t\tExpectContinueTimeout: 1 * time.Second,\n-\t\t},\n-\t}\n-\tif settings.DexConfig != \"\" {\n-\t\ts.client.Transport = dex.NewDexRewriteURLRoundTripper(dexServerAddr, s.client.Transport)\n-\t}\n-\tif os.Getenv(common.EnvVarSSODebug) == \"1\" {\n-\t\ts.client.Transport = httputil.DebugTransport{T: s.client.Transport}\n-\t}\n-\n-\treturn &s\n+        s := SessionManager{\n+                settingsMgr:                   settingsMgr,\n+                storage:                       storage,\n+                sleep:                         time.Sleep,\n+                projectsLister:                projectsLister,\n+                verificationDelayNoiseEnabled: true,\n+        }\n+        settings, err := settingsMgr.GetSettings()\n+        if err != nil {\n+                panic(err)\n+        }\n+        tlsConfig := settings.TLSConfig()\n+        if tlsConfig != nil {\n+                tlsConfig.InsecureSkipVerify = true\n+        }\n+        s.client = &http.Client{\n+                Transport: &http.Transport{\n+                        TLSClientConfig: tlsConfig,\n+                        Proxy:           http.ProxyFromEnvironment,\n+                        Dial: (&net.Dialer{\n+                                Timeout:   30 * time.Second,\n+                                KeepAlive: 30 * time.Second,\n+                        }).Dial,\n+                        TLSHandshakeTimeout:   10 * time.Second,\n+                        ExpectContinueTimeout: 1 * time.Second,\n+                },\n+        }\n+        if settings.DexConfig != \"\" {\n+                s.client.Transport = dex.NewDexRewriteURLRoundTripper(dexServerAddr, s.client.Transport)\n+        }\n+        if os.Getenv(common.EnvVarSSODebug) == \"1\" {\n+                s.client.Transport = httputil.DebugTransport{T: s.client.Transport}\n+        }\n+\n+        return &s\n }\n \n // Create creates a new token for a given subject (user) and returns it as a string.\n // Passing a value of `0` for secondsBeforeExpiry creates a token that never expires.\n // The id parameter holds an optional unique JWT token identifier and stored as a standard claim \"jti\" in the JWT token.\n func (mgr *SessionManager) Create(subject string, secondsBeforeExpiry int64, id string) (string, error) {\n-\t// Create a new token object, specifying signing method and the claims\n-\t// you would like it to contain.\n-\tnow := time.Now().UTC()\n-\tclaims := jwt.StandardClaims{\n-\t\tIssuedAt:  jwt.At(now),\n-\t\tIssuer:    SessionManagerClaimsIssuer,\n-\t\tNotBefore: jwt.At(now),\n-\t\tSubject:   subject,\n-\t\tID:        id,\n-\t}\n-\tif secondsBeforeExpiry > 0 {\n-\t\texpires := now.Add(time.Duration(secondsBeforeExpiry) * time.Second)\n-\t\tclaims.ExpiresAt = jwt.At(expires)\n-\t}\n-\n-\treturn mgr.signClaims(claims)\n+        // Create a new token object, specifying signing method and the claims\n+        // you would like it to contain.\n+        now := time.Now().UTC()\n+        claims := jwt.StandardClaims{\n+                IssuedAt:  jwt.At(now),\n+                Issuer:    SessionManagerClaimsIssuer,\n+                NotBefore: jwt.At(now),\n+                Subject:   subject,\n+                ID:        id,\n+        }\n+        if secondsBeforeExpiry > 0 {\n+                expires := now.Add(time.Duration(secondsBeforeExpiry) * time.Second)\n+                claims.ExpiresAt = jwt.At(expires)\n+        }\n+\n+        return mgr.signClaims(claims)\n }\n \n type standardClaims struct {\n-\tAudience  jwt.ClaimStrings `json:\"aud,omitempty\"`\n-\tExpiresAt int64            `json:\"exp,omitempty\"`\n-\tID        string           `json:\"jti,omitempty\"`\n-\tIssuedAt  int64            `json:\"iat,omitempty\"`\n-\tIssuer    string           `json:\"iss,omitempty\"`\n-\tNotBefore int64            `json:\"nbf,omitempty\"`\n-\tSubject   string           `json:\"sub,omitempty\"`\n+        Audience  jwt.ClaimStrings `json:\"aud,omitempty\"`\n+        ExpiresAt int64            `json:\"exp,omitempty\"`\n+        ID        string           `json:\"jti,omitempty\"`\n+        IssuedAt  int64            `json:\"iat,omitempty\"`\n+        Issuer    string           `json:\"iss,omitempty\"`\n+        NotBefore int64            `json:\"nbf,omitempty\"`\n+        Subject   string           `json:\"sub,omitempty\"`\n }\n \n func unixTimeOrZero(t *jwt.Time) int64 {\n-\tif t == nil {\n-\t\treturn 0\n-\t}\n-\treturn t.Unix()\n+        if t == nil {\n+                return 0\n+        }\n+        return t.Unix()\n }\n \n func (mgr *SessionManager) signClaims(claims jwt.Claims) (string, error) {\n-\t// log.Infof(\"Issuing claims: %v\", claims)\n-\ttoken := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)\n-\tsettings, err := mgr.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\t// workaround for https://github.com/argoproj/argo-cd/issues/5217\n-\t// According to https://tools.ietf.org/html/rfc7519#section-4.1.6 \"iat\" and other time fields must contain\n-\t// number of seconds from 1970-01-01T00:00:00Z UTC until the specified UTC date/time.\n-\t// The https://github.com/dgrijalva/jwt-go marshals time as non integer.\n-\treturn token.SignedString(settings.ServerSignature, jwt.WithMarshaller(func(ctx jwt.CodingContext, v interface{}) ([]byte, error) {\n-\t\tif std, ok := v.(jwt.StandardClaims); ok {\n-\t\t\treturn json.Marshal(standardClaims{\n-\t\t\t\tAudience:  std.Audience,\n-\t\t\t\tExpiresAt: unixTimeOrZero(std.ExpiresAt),\n-\t\t\t\tID:        std.ID,\n-\t\t\t\tIssuedAt:  unixTimeOrZero(std.IssuedAt),\n-\t\t\t\tIssuer:    std.Issuer,\n-\t\t\t\tNotBefore: unixTimeOrZero(std.NotBefore),\n-\t\t\t\tSubject:   std.Subject,\n-\t\t\t})\n-\t\t}\n-\t\treturn json.Marshal(v)\n-\t}))\n+        // log.Infof(\"Issuing claims: %v\", claims)\n+        token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)\n+        settings, err := mgr.settingsMgr.GetSettings()\n+        if err != nil {\n+                return \"\", err\n+        }\n+        // workaround for https://github.com/argoproj/argo-cd/issues/5217\n+        // According to https://tools.ietf.org/html/rfc7519#section-4.1.6 \"iat\" and other time fields must contain\n+        // number of seconds from 1970-01-01T00:00:00Z UTC until the specified UTC date/time.\n+        // The https://github.com/dgrijalva/jwt-go marshals time as non integer.\n+        return token.SignedString(settings.ServerSignature, jwt.WithMarshaller(func(ctx jwt.CodingContext, v interface{}) ([]byte, error) {\n+                if std, ok := v.(jwt.StandardClaims); ok {\n+                        return json.Marshal(standardClaims{\n+                                Audience:  std.Audience,\n+                                ExpiresAt: unixTimeOrZero(std.ExpiresAt),\n+                                ID:        std.ID,\n+                                IssuedAt:  unixTimeOrZero(std.IssuedAt),\n+                                Issuer:    std.Issuer,\n+                                NotBefore: unixTimeOrZero(std.NotBefore),\n+                                Subject:   std.Subject,\n+                        })\n+                }\n+                return json.Marshal(v)\n+        }))\n }\n \n // Parse tries to parse the provided string and returns the token claims for local login.\n func (mgr *SessionManager) Parse(tokenString string) (jwt.Claims, error) {\n-\t// Parse takes the token string and a function for looking up the key. The latter is especially\n-\t// useful if you use multiple keys for your application.  The standard is to use 'kid' in the\n-\t// head of the token to identify which key to use, but the parsed token (head and claims) is provided\n-\t// to the callback, providing flexibility.\n-\tvar claims jwt.MapClaims\n-\tsettings, err := mgr.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\ttoken, err := jwt.ParseWithClaims(tokenString, &claims, func(token *jwt.Token) (interface{}, error) {\n-\t\t// Don't forget to validate the alg is what you expect:\n-\t\tif _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {\n-\t\t\treturn nil, fmt.Errorf(\"Unexpected signing method: %v\", token.Header[\"alg\"])\n-\t\t}\n-\t\treturn settings.ServerSignature, nil\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tissuedAt, err := jwtutil.IssuedAtTime(claims)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tsubject := jwtutil.StringField(claims, \"sub\")\n-\tid := jwtutil.StringField(claims, \"jti\")\n-\n-\tif projName, role, ok := rbacpolicy.GetProjectRoleFromSubject(subject); ok {\n-\t\tproj, err := mgr.projectsLister.Get(projName)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\t_, _, err = proj.GetJWTToken(role, issuedAt.Unix(), id)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\treturn token.Claims, nil\n-\t}\n-\n-\taccount, err := mgr.settingsMgr.GetAccount(subject)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif id := jwtutil.StringField(claims, \"jti\"); id != \"\" && account.TokenIndex(id) == -1 {\n-\t\treturn nil, fmt.Errorf(\"account %s does not have token with id %s\", subject, id)\n-\t}\n-\n-\tif account.PasswordMtime != nil && issuedAt.Before(*account.PasswordMtime) {\n-\t\treturn nil, fmt.Errorf(\"Account password has changed since token issued\")\n-\t}\n-\treturn token.Claims, nil\n+        // Parse takes the token string and a function for looking up the key. The latter is especially\n+        // useful if you use multiple keys for your application.  The standard is to use 'kid' in the\n+        // head of the token to identify which key to use, but the parsed token (head and claims) is provided\n+        // to the callback, providing flexibility.\n+        var claims jwt.MapClaims\n+        settings, err := mgr.settingsMgr.GetSettings()\n+        if err != nil {\n+                return nil, err\n+        }\n+        token, err := jwt.ParseWithClaims(tokenString, &claims, func(token *jwt.Token) (interface{}, error) {\n+                // Don't forget to validate the alg is what you expect:\n+                if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {\n+                        return nil, fmt.Errorf(\"Unexpected signing method: %v\", token.Header[\"alg\"])\n+                }\n+                return settings.ServerSignature, nil\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        issuedAt, err := jwtutil.IssuedAtTime(claims)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        subject := jwtutil.StringField(claims, \"sub\")\n+        id := jwtutil.StringField(claims, \"jti\")\n+\n+        if projName, role, ok := rbacpolicy.GetProjectRoleFromSubject(subject); ok {\n+                proj, err := mgr.projectsLister.Get(projName)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                _, _, err = proj.GetJWTToken(role, issuedAt.Unix(), id)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                return token.Claims, nil\n+        }\n+\n+        account, err := mgr.settingsMgr.GetAccount(subject)\n+        if err != nil {\n+                return nil, err\n+        }\n+if !account.Enabled {\n+return nil, fmt.Errorf(\"account %s is disabled\", subject)\n+}\n+\n+\n+        if id := jwtutil.StringField(claims, \"jti\"); id != \"\" && account.TokenIndex(id) == -1 {\n+                return nil, fmt.Errorf(\"account %s does not have token with id %s\", subject, id)\n+        }\n+\n+        if account.PasswordMtime != nil && issuedAt.Before(*account.PasswordMtime) {\n+                return nil, fmt.Errorf(\"Account password has changed since token issued\")\n+        }\n+        return token.Claims, nil\n }\n \n // GetLoginFailures retrieves the login failure information from the cache\n func (mgr *SessionManager) GetLoginFailures() map[string]LoginAttempts {\n-\t// Get failures from the cache\n-\tvar failures map[string]LoginAttempts\n-\terr := mgr.storage.GetLoginAttempts(&failures)\n-\tif err != nil {\n-\t\tif err != appstate.ErrCacheMiss {\n-\t\t\tlog.Errorf(\"Could not retrieve login attempts: %v\", err)\n-\t\t}\n-\t\tfailures = make(map[string]LoginAttempts)\n-\t}\n-\n-\treturn failures\n+        // Get failures from the cache\n+        var failures map[string]LoginAttempts\n+        err := mgr.storage.GetLoginAttempts(&failures)\n+        if err != nil {\n+                if err != appstate.ErrCacheMiss {\n+                        log.Errorf(\"Could not retrieve login attempts: %v\", err)\n+                }\n+                failures = make(map[string]LoginAttempts)\n+        }\n+\n+        return failures\n }\n \n func expireOldFailedAttempts(maxAge time.Duration, failures *map[string]LoginAttempts) int {\n-\texpiredCount := 0\n-\tfor key, attempt := range *failures {\n-\t\tif time.Since(attempt.LastFailed) > maxAge*time.Second {\n-\t\t\texpiredCount += 1\n-\t\t\tdelete(*failures, key)\n-\t\t}\n-\t}\n-\treturn expiredCount\n+        expiredCount := 0\n+        for key, attempt := range *failures {\n+                if time.Since(attempt.LastFailed) > maxAge*time.Second {\n+                        expiredCount += 1\n+                        delete(*failures, key)\n+                }\n+        }\n+        return expiredCount\n }\n \n // Updates the failure count for a given username. If failed is true, increases the counter. Otherwise, sets counter back to 0.\n func (mgr *SessionManager) updateFailureCount(username string, failed bool) {\n \n-\tfailures := mgr.GetLoginFailures()\n-\n-\t// Expire old entries in the cache if we have a failure window defined.\n-\tif window := getLoginFailureWindow(); window > 0 {\n-\t\tcount := expireOldFailedAttempts(window, &failures)\n-\t\tif count > 0 {\n-\t\t\tlog.Infof(\"Expired %d entries from session cache due to max age reached\", count)\n-\t\t}\n-\t}\n-\n-\t// If we exceed a certain cache size, we need to remove random entries to\n-\t// prevent overbloating the cache with fake entries, as this could lead to\n-\t// memory exhaustion and ultimately in a DoS. We remove a single entry to\n-\t// replace it with the new one.\n-\t//\n-\t// Chances are that we remove the one that is under active attack, but this\n-\t// chance is low (1:cache_size)\n-\tif failed && len(failures) >= getMaximumCacheSize() {\n-\t\tlog.Warnf(\"Session cache size exceeds %d entries, removing random entry\", getMaximumCacheSize())\n-\t\tidx := rand.Intn(len(failures) - 1)\n-\t\tvar rmUser string\n-\t\ti := 0\n-\t\tfor key := range failures {\n-\t\t\tif i == idx {\n-\t\t\t\trmUser = key\n-\t\t\t\tdelete(failures, key)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\ti++\n-\t\t}\n-\t\tlog.Infof(\"Deleted entry for user %s from cache\", rmUser)\n-\t}\n-\n-\tattempt, ok := failures[username]\n-\tif !ok {\n-\t\tattempt = LoginAttempts{FailCount: 0}\n-\t}\n-\n-\t// On login failure, increase fail count and update last failed timestamp.\n-\t// On login success, remove the entry from the cache.\n-\tif failed {\n-\t\tattempt.FailCount += 1\n-\t\tattempt.LastFailed = time.Now()\n-\t\tfailures[username] = attempt\n-\t\tlog.Warnf(\"User %s failed login %d time(s)\", username, attempt.FailCount)\n-\t} else {\n-\t\tif attempt.FailCount > 0 {\n-\t\t\t// Forget username for cache size enforcement, since entry in cache was deleted\n-\t\t\tdelete(failures, username)\n-\t\t}\n-\t}\n-\n-\terr := mgr.storage.SetLoginAttempts(failures)\n-\tif err != nil {\n-\t\tlog.Errorf(\"Could not update login attempts: %v\", err)\n-\t}\n+        failures := mgr.GetLoginFailures()\n+\n+        // Expire old entries in the cache if we have a failure window defined.\n+        if window := getLoginFailureWindow(); window > 0 {\n+                count := expireOldFailedAttempts(window, &failures)\n+                if count > 0 {\n+                        log.Infof(\"Expired %d entries from session cache due to max age reached\", count)\n+                }\n+        }\n+\n+        // If we exceed a certain cache size, we need to remove random entries to\n+        // prevent overbloating the cache with fake entries, as this could lead to\n+        // memory exhaustion and ultimately in a DoS. We remove a single entry to\n+        // replace it with the new one.\n+        //\n+        // Chances are that we remove the one that is under active attack, but this\n+        // chance is low (1:cache_size)\n+        if failed && len(failures) >= getMaximumCacheSize() {\n+                log.Warnf(\"Session cache size exceeds %d entries, removing random entry\", getMaximumCacheSize())\n+                idx := rand.Intn(len(failures) - 1)\n+                var rmUser string\n+                i := 0\n+                for key := range failures {\n+                        if i == idx {\n+                                rmUser = key\n+                                delete(failures, key)\n+                                break\n+                        }\n+                        i++\n+                }\n+                log.Infof(\"Deleted entry for user %s from cache\", rmUser)\n+        }\n+\n+        attempt, ok := failures[username]\n+        if !ok {\n+                attempt = LoginAttempts{FailCount: 0}\n+        }\n+\n+        // On login failure, increase fail count and update last failed timestamp.\n+        // On login success, remove the entry from the cache.\n+        if failed {\n+                attempt.FailCount += 1\n+                attempt.LastFailed = time.Now()\n+                failures[username] = attempt\n+                log.Warnf(\"User %s failed login %d time(s)\", username, attempt.FailCount)\n+        } else {\n+                if attempt.FailCount > 0 {\n+                        // Forget username for cache size enforcement, since entry in cache was deleted\n+                        delete(failures, username)\n+                }\n+        }\n+\n+        err := mgr.storage.SetLoginAttempts(failures)\n+        if err != nil {\n+                log.Errorf(\"Could not update login attempts: %v\", err)\n+        }\n \n }\n \n // Get the current login failure attempts for given username\n func (mgr *SessionManager) getFailureCount(username string) LoginAttempts {\n-\tfailures := mgr.GetLoginFailures()\n-\tattempt, ok := failures[username]\n-\tif !ok {\n-\t\tattempt = LoginAttempts{FailCount: 0}\n-\t}\n-\treturn attempt\n+        failures := mgr.GetLoginFailures()\n+        attempt, ok := failures[username]\n+        if !ok {\n+                attempt = LoginAttempts{FailCount: 0}\n+        }\n+        return attempt\n }\n \n // Calculate a login delay for the given login attempt\n func (mgr *SessionManager) exceededFailedLoginAttempts(attempt LoginAttempts) bool {\n-\tmaxFails := getMaxLoginFailures()\n-\tfailureWindow := getLoginFailureWindow()\n-\n-\t// Whether we are in the failure window for given attempt\n-\tinWindow := func() bool {\n-\t\tif failureWindow == 0 || time.Since(attempt.LastFailed).Seconds() <= float64(failureWindow) {\n-\t\t\treturn true\n-\t\t}\n-\t\treturn false\n-\t}\n-\n-\t// If we reached max failed attempts within failure window, we need to calc the delay\n-\tif attempt.FailCount >= maxFails && inWindow() {\n-\t\treturn true\n-\t}\n-\n-\treturn false\n+        maxFails := getMaxLoginFailures()\n+        failureWindow := getLoginFailureWindow()\n+\n+        // Whether we are in the failure window for given attempt\n+        inWindow := func() bool {\n+                if failureWindow == 0 || time.Since(attempt.LastFailed).Seconds() <= float64(failureWindow) {\n+                        return true\n+                }\n+                return false\n+        }\n+\n+        // If we reached max failed attempts within failure window, we need to calc the delay\n+        if attempt.FailCount >= maxFails && inWindow() {\n+                return true\n+        }\n+\n+        return false\n }\n \n // VerifyUsernamePassword verifies if a username/password combo is correct\n func (mgr *SessionManager) VerifyUsernamePassword(username string, password string) error {\n-\tif password == \"\" {\n-\t\treturn status.Errorf(codes.Unauthenticated, blankPasswordError)\n-\t}\n-\t// Enforce maximum length of username on local accounts\n-\tif len(username) > maxUsernameLength {\n-\t\treturn status.Errorf(codes.InvalidArgument, usernameTooLongError, maxUsernameLength)\n-\t}\n-\n-\tstart := time.Now()\n-\tif mgr.verificationDelayNoiseEnabled {\n-\t\tdefer func() {\n-\t\t\t// introduces random delay to protect from timing-based user enumeration attack\n-\t\t\tdelayNanoseconds := verificationDelayNoiseMin.Nanoseconds() +\n-\t\t\t\tint64(rand.Intn(int(verificationDelayNoiseMax.Nanoseconds()-verificationDelayNoiseMin.Nanoseconds())))\n-\t\t\t\t// take into account amount of time spent since the request start\n-\t\t\tdelayNanoseconds = delayNanoseconds - time.Since(start).Nanoseconds()\n-\t\t\tif delayNanoseconds > 0 {\n-\t\t\t\tmgr.sleep(time.Duration(delayNanoseconds))\n-\t\t\t}\n-\t\t}()\n-\t}\n-\n-\tattempt := mgr.getFailureCount(username)\n-\tif mgr.exceededFailedLoginAttempts(attempt) {\n-\t\tlog.Warnf(\"User %s had too many failed logins (%d)\", username, attempt.FailCount)\n-\t\treturn InvalidLoginErr\n-\t}\n-\n-\taccount, err := mgr.settingsMgr.GetAccount(username)\n-\tif err != nil {\n-\t\tif errStatus, ok := status.FromError(err); ok && errStatus.Code() == codes.NotFound {\n-\t\t\tmgr.updateFailureCount(username, true)\n-\t\t\terr = InvalidLoginErr\n-\t\t}\n-\t\t// to prevent time-based user enumeration, we must perform a password\n-\t\t// hash cycle to keep response time consistent (if the function were\n-\t\t// to continue and not return here)\n-\t\t_, _ = passwordutil.HashPassword(\"for_consistent_response_time\")\n-\t\treturn err\n-\t}\n-\n-\tvalid, _ := passwordutil.VerifyPassword(password, account.PasswordHash)\n-\tif !valid {\n-\t\tmgr.updateFailureCount(username, true)\n-\t\treturn InvalidLoginErr\n-\t}\n-\n-\tif !account.Enabled {\n-\t\treturn status.Errorf(codes.Unauthenticated, accountDisabled, username)\n-\t}\n-\n-\tif !account.HasCapability(settings.AccountCapabilityLogin) {\n-\t\treturn status.Errorf(codes.Unauthenticated, userDoesNotHaveCapability, username, settings.AccountCapabilityLogin)\n-\t}\n-\tmgr.updateFailureCount(username, false)\n-\treturn nil\n+        if password == \"\" {\n+                return status.Errorf(codes.Unauthenticated, blankPasswordError)\n+        }\n+        // Enforce maximum length of username on local accounts\n+        if len(username) > maxUsernameLength {\n+                return status.Errorf(codes.InvalidArgument, usernameTooLongError, maxUsernameLength)\n+        }\n+\n+        start := time.Now()\n+        if mgr.verificationDelayNoiseEnabled {\n+                defer func() {\n+                        // introduces random delay to protect from timing-based user enumeration attack\n+                        delayNanoseconds := verificationDelayNoiseMin.Nanoseconds() +\n+                                int64(rand.Intn(int(verificationDelayNoiseMax.Nanoseconds()-verificationDelayNoiseMin.Nanoseconds())))\n+                                // take into account amount of time spent since the request start\n+                        delayNanoseconds = delayNanoseconds - time.Since(start).Nanoseconds()\n+                        if delayNanoseconds > 0 {\n+                                mgr.sleep(time.Duration(delayNanoseconds))\n+                        }\n+                }()\n+        }\n+\n+        attempt := mgr.getFailureCount(username)\n+        if mgr.exceededFailedLoginAttempts(attempt) {\n+                log.Warnf(\"User %s had too many failed logins (%d)\", username, attempt.FailCount)\n+                return InvalidLoginErr\n+        }\n+\n+        account, err := mgr.settingsMgr.GetAccount(username)\n+        if err != nil {\n+                if errStatus, ok := status.FromError(err); ok && errStatus.Code() == codes.NotFound {\n+                        mgr.updateFailureCount(username, true)\n+                        err = InvalidLoginErr\n+                }\n+                // to prevent time-based user enumeration, we must perform a password\n+                // hash cycle to keep response time consistent (if the function were\n+                // to continue and not return here)\n+                _, _ = passwordutil.HashPassword(\"for_consistent_response_time\")\n+                return err\n+        }\n+\n+        valid, _ := passwordutil.VerifyPassword(password, account.PasswordHash)\n+        if !valid {\n+                mgr.updateFailureCount(username, true)\n+                return InvalidLoginErr\n+        }\n+\n+        if !account.Enabled {\n+                return status.Errorf(codes.Unauthenticated, accountDisabled, username)\n+        }\n+\n+        if !account.HasCapability(settings.AccountCapabilityLogin) {\n+                return status.Errorf(codes.Unauthenticated, userDoesNotHaveCapability, username, settings.AccountCapabilityLogin)\n+        }\n+        mgr.updateFailureCount(username, false)\n+        return nil\n }\n \n // VerifyToken verifies if a token is correct. Tokens can be issued either from us or by an IDP.\n // We choose how to verify based on the issuer.\n func (mgr *SessionManager) VerifyToken(tokenString string) (jwt.Claims, error) {\n-\tparser := &jwt.Parser{\n-\t\tValidationHelper: jwt.NewValidationHelper(jwt.WithoutClaimsValidation()),\n-\t}\n-\tvar claims jwt.StandardClaims\n-\t_, _, err := parser.ParseUnverified(tokenString, &claims)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tswitch claims.Issuer {\n-\tcase SessionManagerClaimsIssuer:\n-\t\t// Argo CD signed token\n-\t\treturn mgr.Parse(tokenString)\n-\tdefault:\n-\t\t// IDP signed token\n-\t\tprov, err := mgr.provider()\n-\t\tif err != nil {\n-\t\t\treturn claims, err\n-\t\t}\n-\n-\t\t// Token must be verified for at least one audience\n-\t\t// TODO(jannfis): Is this the right way? Shouldn't we know our audience and only validate for the correct one?\n-\t\tvar idToken *oidc.IDToken\n-\t\tfor _, aud := range claims.Audience {\n-\t\t\tidToken, err = prov.Verify(aud, tokenString)\n-\t\t\tif err == nil {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn claims, err\n-\t\t}\n-\t\tvar claims jwt.MapClaims\n-\t\terr = idToken.Claims(&claims)\n-\t\treturn claims, err\n-\t}\n+        parser := &jwt.Parser{\n+                ValidationHelper: jwt.NewValidationHelper(jwt.WithoutClaimsValidation()),\n+        }\n+        var claims jwt.StandardClaims\n+        _, _, err := parser.ParseUnverified(tokenString, &claims)\n+        if err != nil {\n+                return nil, err\n+        }\n+        switch claims.Issuer {\n+        case SessionManagerClaimsIssuer:\n+                // Argo CD signed token\n+                return mgr.Parse(tokenString)\n+        default:\n+                // IDP signed token\n+                prov, err := mgr.provider()\n+                if err != nil {\n+                        return claims, err\n+                }\n+\n+                // Token must be verified for at least one audience\n+                // TODO(jannfis): Is this the right way? Shouldn't we know our audience and only validate for the correct one?\n+                var idToken *oidc.IDToken\n+                for _, aud := range claims.Audience {\n+                        idToken, err = prov.Verify(aud, tokenString)\n+                        if err == nil {\n+                                break\n+                        }\n+                }\n+                if err != nil {\n+                        return claims, err\n+                }\n+                var claims jwt.MapClaims\n+                err = idToken.Claims(&claims)\n+                return claims, err\n+        }\n }\n \n func (mgr *SessionManager) provider() (oidcutil.Provider, error) {\n-\tif mgr.prov != nil {\n-\t\treturn mgr.prov, nil\n-\t}\n-\tsettings, err := mgr.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif !settings.IsSSOConfigured() {\n-\t\treturn nil, fmt.Errorf(\"SSO is not configured\")\n-\t}\n-\tmgr.prov = oidcutil.NewOIDCProvider(settings.IssuerURL(), mgr.client)\n-\treturn mgr.prov, nil\n+        if mgr.prov != nil {\n+                return mgr.prov, nil\n+        }\n+        settings, err := mgr.settingsMgr.GetSettings()\n+        if err != nil {\n+                return nil, err\n+        }\n+        if !settings.IsSSOConfigured() {\n+                return nil, fmt.Errorf(\"SSO is not configured\")\n+        }\n+        mgr.prov = oidcutil.NewOIDCProvider(settings.IssuerURL(), mgr.client)\n+        return mgr.prov, nil\n }\n \n func LoggedIn(ctx context.Context) bool {\n-\treturn Sub(ctx) != \"\"\n+        return Sub(ctx) != \"\"\n }\n \n // Username is a helper to extract a human readable username from a context\n func Username(ctx context.Context) string {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn \"\"\n-\t}\n-\tswitch jwtutil.StringField(mapClaims, \"iss\") {\n-\tcase SessionManagerClaimsIssuer:\n-\t\treturn jwtutil.StringField(mapClaims, \"sub\")\n-\tdefault:\n-\t\treturn jwtutil.StringField(mapClaims, \"email\")\n-\t}\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return \"\"\n+        }\n+        switch jwtutil.StringField(mapClaims, \"iss\") {\n+        case SessionManagerClaimsIssuer:\n+                return jwtutil.StringField(mapClaims, \"sub\")\n+        default:\n+                return jwtutil.StringField(mapClaims, \"email\")\n+        }\n }\n \n func Iss(ctx context.Context) string {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn \"\"\n-\t}\n-\treturn jwtutil.StringField(mapClaims, \"iss\")\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return \"\"\n+        }\n+        return jwtutil.StringField(mapClaims, \"iss\")\n }\n \n func Iat(ctx context.Context) (time.Time, error) {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn time.Time{}, errors.New(\"unable to extract token claims\")\n-\t}\n-\treturn jwtutil.IssuedAtTime(mapClaims)\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return time.Time{}, errors.New(\"unable to extract token claims\")\n+        }\n+        return jwtutil.IssuedAtTime(mapClaims)\n }\n \n func Sub(ctx context.Context) string {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn \"\"\n-\t}\n-\treturn jwtutil.StringField(mapClaims, \"sub\")\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return \"\"\n+        }\n+        return jwtutil.StringField(mapClaims, \"sub\")\n }\n \n func Groups(ctx context.Context, scopes []string) []string {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn nil\n-\t}\n-\treturn jwtutil.GetGroups(mapClaims, scopes)\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return nil\n+        }\n+        return jwtutil.GetGroups(mapClaims, scopes)\n }\n \n func mapClaims(ctx context.Context) (jwt.MapClaims, bool) {\n-\tclaims, ok := ctx.Value(\"claims\").(jwt.Claims)\n-\tif !ok {\n-\t\treturn nil, false\n-\t}\n-\tmapClaims, err := jwtutil.MapClaims(claims)\n-\tif err != nil {\n-\t\treturn nil, false\n-\t}\n-\treturn mapClaims, true\n+        claims, ok := ctx.Value(\"claims\").(jwt.Claims)\n+        if !ok {\n+                return nil, false\n+        }\n+        mapClaims, err := jwtutil.MapClaims(claims)\n+        if err != nil {\n+                return nil, false\n+        }\n+        return mapClaims, true\n }\n"}
{"cve":"CVE-2025-46331:0708", "fix_patch": ""}
{"cve":"CVE-2025-24976:0708", "fix_patch": "diff --git a/registry/auth/token/token.go b/registry/auth/token/token.go\nindex 55d0b7f6..6bc36fee 100644\n--- a/registry/auth/token/token.go\n+++ b/registry/auth/token/token.go\n@@ -1,316 +1,315 @@\n package token\n \n import (\n-\t\"crypto\"\n-\t\"crypto/x509\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"time\"\n-\n-\t\"github.com/go-jose/go-jose/v4\"\n-\t\"github.com/go-jose/go-jose/v4/jwt\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\n-\t\"github.com/distribution/distribution/v3/registry/auth\"\n+        \"crypto\"\n+        \"crypto/x509\"\n+        \"errors\"\n+        \"fmt\"\n+        \"reflect\"\n+        \"time\"\n+\n+        \"github.com/go-jose/go-jose/v4\"\n+        \"github.com/go-jose/go-jose/v4/jwt\"\n+        log \"github.com/sirupsen/logrus\"\n+\n+        \"github.com/distribution/distribution/v3/registry/auth\"\n )\n \n const (\n-\t// TokenSeparator is the value which separates the header, claims, and\n-\t// signature in the compact serialization of a JSON Web Token.\n-\tTokenSeparator = \".\"\n-\t// Leeway is the Duration that will be added to NBF and EXP claim\n-\t// checks to account for clock skew as per https://tools.ietf.org/html/rfc7519#section-4.1.5\n-\tLeeway = 60 * time.Second\n+        // TokenSeparator is the value which separates the header, claims, and\n+        // signature in the compact serialization of a JSON Web Token.\n+        TokenSeparator = \".\"\n+        // Leeway is the Duration that will be added to NBF and EXP claim\n+        // checks to account for clock skew as per https://tools.ietf.org/html/rfc7519#section-4.1.5\n+        Leeway = 60 * time.Second\n )\n \n var signingAlgorithms = map[string]jose.SignatureAlgorithm{\n-\t\"EdDSA\": jose.EdDSA,\n-\t\"HS256\": jose.HS256,\n-\t\"HS384\": jose.HS384,\n-\t\"HS512\": jose.HS512,\n-\t\"RS256\": jose.RS256,\n-\t\"RS384\": jose.RS384,\n-\t\"RS512\": jose.RS512,\n-\t\"ES256\": jose.ES256,\n-\t\"ES384\": jose.ES384,\n-\t\"ES512\": jose.ES512,\n-\t\"PS256\": jose.PS256,\n-\t\"PS384\": jose.PS384,\n-\t\"PS512\": jose.PS512,\n+        \"EdDSA\": jose.EdDSA,\n+        \"HS256\": jose.HS256,\n+        \"HS384\": jose.HS384,\n+        \"HS512\": jose.HS512,\n+        \"RS256\": jose.RS256,\n+        \"RS384\": jose.RS384,\n+        \"RS512\": jose.RS512,\n+        \"ES256\": jose.ES256,\n+        \"ES384\": jose.ES384,\n+        \"ES512\": jose.ES512,\n+        \"PS256\": jose.PS256,\n+        \"PS384\": jose.PS384,\n+        \"PS512\": jose.PS512,\n }\n \n var defaultSigningAlgorithms = []jose.SignatureAlgorithm{\n-\tjose.EdDSA,\n-\tjose.HS256,\n-\tjose.HS384,\n-\tjose.HS512,\n-\tjose.RS256,\n-\tjose.RS384,\n-\tjose.RS512,\n-\tjose.ES256,\n-\tjose.ES384,\n-\tjose.ES512,\n-\tjose.PS256,\n-\tjose.PS384,\n-\tjose.PS512,\n+        jose.EdDSA,\n+        jose.HS256,\n+        jose.HS384,\n+        jose.HS512,\n+        jose.RS256,\n+        jose.RS384,\n+        jose.RS512,\n+        jose.ES256,\n+        jose.ES384,\n+        jose.ES512,\n+        jose.PS256,\n+        jose.PS384,\n+        jose.PS512,\n }\n \n // Errors used by token parsing and verification.\n var (\n-\tErrMalformedToken = errors.New(\"malformed token\")\n-\tErrInvalidToken   = errors.New(\"invalid token\")\n+        ErrMalformedToken = errors.New(\"malformed token\")\n+        ErrInvalidToken   = errors.New(\"invalid token\")\n )\n \n // ResourceActions stores allowed actions on a named and typed resource.\n type ResourceActions struct {\n-\tType    string   `json:\"type\"`\n-\tClass   string   `json:\"class,omitempty\"`\n-\tName    string   `json:\"name\"`\n-\tActions []string `json:\"actions\"`\n+        Type    string   `json:\"type\"`\n+        Class   string   `json:\"class,omitempty\"`\n+        Name    string   `json:\"name\"`\n+        Actions []string `json:\"actions\"`\n }\n \n // ClaimSet describes the main section of a JSON Web Token.\n type ClaimSet struct {\n-\t// Public claims\n-\tIssuer     string       `json:\"iss\"`\n-\tSubject    string       `json:\"sub\"`\n-\tAudience   AudienceList `json:\"aud\"`\n-\tExpiration int64        `json:\"exp\"`\n-\tNotBefore  int64        `json:\"nbf\"`\n-\tIssuedAt   int64        `json:\"iat\"`\n-\tJWTID      string       `json:\"jti\"`\n-\n-\t// Private claims\n-\tAccess []*ResourceActions `json:\"access\"`\n+        // Public claims\n+        Issuer     string       `json:\"iss\"`\n+        Subject    string       `json:\"sub\"`\n+        Audience   AudienceList `json:\"aud\"`\n+        Expiration int64        `json:\"exp\"`\n+        NotBefore  int64        `json:\"nbf\"`\n+        IssuedAt   int64        `json:\"iat\"`\n+        JWTID      string       `json:\"jti\"`\n+\n+        // Private claims\n+        Access []*ResourceActions `json:\"access\"`\n }\n \n // Token is a JSON Web Token.\n type Token struct {\n-\tRaw string\n-\tJWT *jwt.JSONWebToken\n+        Raw string\n+        JWT *jwt.JSONWebToken\n }\n \n // VerifyOptions is used to specify\n // options when verifying a JSON Web Token.\n type VerifyOptions struct {\n-\tTrustedIssuers    []string\n-\tAcceptedAudiences []string\n-\tRoots             *x509.CertPool\n-\tTrustedKeys       map[string]crypto.PublicKey\n+        TrustedIssuers    []string\n+        AcceptedAudiences []string\n+        Roots             *x509.CertPool\n+        TrustedKeys       map[string]crypto.PublicKey\n }\n \n // NewToken parses the given raw token string\n // and constructs an unverified JSON Web Token.\n func NewToken(rawToken string, signingAlgs []jose.SignatureAlgorithm) (*Token, error) {\n-\ttoken, err := jwt.ParseSigned(rawToken, signingAlgs)\n-\tif err != nil {\n-\t\treturn nil, ErrMalformedToken\n-\t}\n-\n-\treturn &Token{\n-\t\tRaw: rawToken,\n-\t\tJWT: token,\n-\t}, nil\n+        token, err := jwt.ParseSigned(rawToken, signingAlgs)\n+        if err != nil {\n+                return nil, ErrMalformedToken\n+        }\n+\n+        return &Token{\n+                Raw: rawToken,\n+                JWT: token,\n+        }, nil\n }\n \n // Verify attempts to verify this token using the given options.\n // Returns a nil error if the token is valid.\n func (t *Token) Verify(verifyOpts VerifyOptions) (*ClaimSet, error) {\n-\t// Verify that the signing key is trusted.\n-\tsigningKey, err := t.VerifySigningKey(verifyOpts)\n-\tif err != nil {\n-\t\tlog.Infof(\"failed to verify token: %v\", err)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\t// NOTE(milosgajdos): Claims both verifies the signature\n-\t// and returns the claims within the payload\n-\tvar claims ClaimSet\n-\terr = t.JWT.Claims(signingKey, &claims)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// Verify that the Issuer claim is a trusted authority.\n-\tif !contains(verifyOpts.TrustedIssuers, claims.Issuer) {\n-\t\tlog.Infof(\"token from untrusted issuer: %q\", claims.Issuer)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\t// Verify that the Audience claim is allowed.\n-\tif !containsAny(verifyOpts.AcceptedAudiences, claims.Audience) {\n-\t\tlog.Infof(\"token intended for another audience: %v\", claims.Audience)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\t// Verify that the token is currently usable and not expired.\n-\tcurrentTime := time.Now()\n-\n-\tExpWithLeeway := time.Unix(claims.Expiration, 0).Add(Leeway)\n-\tif currentTime.After(ExpWithLeeway) {\n-\t\tlog.Infof(\"token not to be used after %s - currently %s\", ExpWithLeeway, currentTime)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\tNotBeforeWithLeeway := time.Unix(claims.NotBefore, 0).Add(-Leeway)\n-\tif currentTime.Before(NotBeforeWithLeeway) {\n-\t\tlog.Infof(\"token not to be used before %s - currently %s\", NotBeforeWithLeeway, currentTime)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\treturn &claims, nil\n+        // Verify that the signing key is trusted.\n+        signingKey, err := t.VerifySigningKey(verifyOpts)\n+        if err != nil {\n+                log.Infof(\"failed to verify token: %v\", err)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        // NOTE(milosgajdos): Claims both verifies the signature\n+        // and returns the claims within the payload\n+        var claims ClaimSet\n+        err = t.JWT.Claims(signingKey, &claims)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // Verify that the Issuer claim is a trusted authority.\n+        if !contains(verifyOpts.TrustedIssuers, claims.Issuer) {\n+                log.Infof(\"token from untrusted issuer: %q\", claims.Issuer)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        // Verify that the Audience claim is allowed.\n+        if !containsAny(verifyOpts.AcceptedAudiences, claims.Audience) {\n+                log.Infof(\"token intended for another audience: %v\", claims.Audience)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        // Verify that the token is currently usable and not expired.\n+        currentTime := time.Now()\n+\n+        ExpWithLeeway := time.Unix(claims.Expiration, 0).Add(Leeway)\n+        if currentTime.After(ExpWithLeeway) {\n+                log.Infof(\"token not to be used after %s - currently %s\", ExpWithLeeway, currentTime)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        NotBeforeWithLeeway := time.Unix(claims.NotBefore, 0).Add(-Leeway)\n+        if currentTime.Before(NotBeforeWithLeeway) {\n+                log.Infof(\"token not to be used before %s - currently %s\", NotBeforeWithLeeway, currentTime)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        return &claims, nil\n }\n \n // VerifySigningKey attempts to verify and return the signing key which was used to sign the token.\n func (t *Token) VerifySigningKey(verifyOpts VerifyOptions) (crypto.PublicKey, error) {\n-\tif len(t.JWT.Headers) == 0 {\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\t// NOTE(milosgajdos): docker auth spec does not seem to\n-\t// support tokens signed by multiple signatures so we are\n-\t// verifying the first one in the list only at the moment.\n-\theader := t.JWT.Headers[0]\n-\n-\tsigningKey, err := verifyCertChain(header, verifyOpts.Roots)\n-\tif err != nil {\n-\t\t// NOTE(milosgajdos): if the x5c header is missing\n-\t\t// the token may have been signed by a JWKS.\n-\t\tif errors.Is(err, jose.ErrMissingX5cHeader) {\n-\t\t\tswitch {\n-\t\t\tcase header.JSONWebKey != nil:\n-\t\t\t\treturn verifyJWK(header, verifyOpts)\n-\t\t\tcase header.KeyID != \"\":\n-\t\t\t\tif signingKey, ok := verifyOpts.TrustedKeys[header.KeyID]; ok {\n-\t\t\t\t\treturn signingKey, nil\n-\t\t\t\t}\n-\t\t\t\treturn nil, fmt.Errorf(\"token signed by untrusted key with ID: %q\", header.KeyID)\n-\t\t\tdefault:\n-\t\t\t\treturn nil, ErrInvalidToken\n-\t\t\t}\n-\t\t}\n-\t\treturn nil, err\n-\t}\n-\n-\treturn signingKey, nil\n+        if len(t.JWT.Headers) == 0 {\n+                return nil, ErrInvalidToken\n+        }\n+\n+        // NOTE(milosgajdos): docker auth spec does not seem to\n+        // support tokens signed by multiple signatures so we are\n+        // verifying the first one in the list only at the moment.\n+        header := t.JWT.Headers[0]\n+\n+        signingKey, err := verifyCertChain(header, verifyOpts.Roots)\n+        if err != nil {\n+                // NOTE(milosgajdos): if the x5c header is missing\n+                // the token may have been signed by a JWKS.\n+                if errors.Is(err, jose.ErrMissingX5cHeader) {\n+                        switch {\n+                        case header.JSONWebKey != nil:\n+                                return verifyJWK(header, verifyOpts)\n+                        case header.KeyID != \"\":\n+                                if signingKey, ok := verifyOpts.TrustedKeys[header.KeyID]; ok {\n+                                        return signingKey, nil\n+                                }\n+                                return nil, fmt.Errorf(\"token signed by untrusted key with ID: %q\", header.KeyID)\n+                        default:\n+                                return nil, ErrInvalidToken\n+                        }\n+                }\n+                return nil, err\n+        }\n+\n+        return signingKey, nil\n }\n \n func verifyCertChain(header jose.Header, roots *x509.CertPool) (signingKey crypto.PublicKey, err error) {\n-\tverifyOpts := x509.VerifyOptions{\n-\t\tRoots:     roots,\n-\t\tKeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageAny},\n-\t}\n-\n-\t// TODO: this call returns certificate chains which we ignore for now, but\n-\t// we should check them for revocations if we have the ability later.\n-\tchains, err := header.Certificates(verifyOpts)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tsigningKey = getCertPubKey(chains)\n-\n-\treturn\n+        verifyOpts := x509.VerifyOptions{\n+                Roots:     roots,\n+                KeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageAny},\n+        }\n+\n+        // TODO: this call returns certificate chains which we ignore for now, but\n+        // we should check them for revocations if we have the ability later.\n+        chains, err := header.Certificates(verifyOpts)\n+        if err != nil {\n+                return nil, err\n+        }\n+        signingKey = getCertPubKey(chains)\n+\n+        return\n }\n \n func verifyJWK(header jose.Header, verifyOpts VerifyOptions) (signingKey crypto.PublicKey, err error) {\n-\tjwk := header.JSONWebKey\n-\tsigningKey = jwk.Key\n-\n-\t// Check to see if the key includes a certificate chain.\n-\tif len(jwk.Certificates) == 0 {\n-\t\t// The JWK should be one of the trusted root keys.\n-\t\tif _, trusted := verifyOpts.TrustedKeys[jwk.KeyID]; !trusted {\n-\t\t\treturn nil, errors.New(\"untrusted JWK with no certificate chain\")\n-\t\t}\n-\t\t// The JWK is one of the trusted keys.\n-\t\treturn\n-\t}\n-\n-\topts := x509.VerifyOptions{\n-\t\tRoots:     verifyOpts.Roots,\n-\t\tKeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageAny},\n-\t}\n-\n-\tleaf := jwk.Certificates[0]\n-\tif opts.Intermediates == nil {\n-\t\topts.Intermediates = x509.NewCertPool()\n-\t\tfor _, intermediate := range jwk.Certificates[1:] {\n-\t\t\topts.Intermediates.AddCert(intermediate)\n-\t\t}\n-\t}\n-\n-\t// TODO: this call returns certificate chains which we ignore for now, but\n-\t// we should check them for revocations if we have the ability later.\n-\tchains, err := leaf.Verify(opts)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tsigningKey = getCertPubKey(chains)\n-\n-\treturn\n+        jwk := header.JSONWebKey\n+        signingKey = jwk.Key\n+\n+        // Check to see if the key includes a certificate chain.\n+        if len(jwk.Certificates) == 0 {\n+                trustedKey, trusted := verifyOpts.TrustedKeys[jwk.KeyID]\n+                if !trusted || !reflect.DeepEqual(trustedKey, jwk.Key) {\n+                        return nil, errors.New(\"untrusted JWK with no certificate chain\")\n+                }\n+        }\n+\n+        opts := x509.VerifyOptions{\n+                Roots:     verifyOpts.Roots,\n+                KeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageAny},\n+        }\n+\n+        leaf := jwk.Certificates[0]\n+        if opts.Intermediates == nil {\n+                opts.Intermediates = x509.NewCertPool()\n+                for _, intermediate := range jwk.Certificates[1:] {\n+                        opts.Intermediates.AddCert(intermediate)\n+                }\n+        }\n+\n+        // TODO: this call returns certificate chains which we ignore for now, but\n+        // we should check them for revocations if we have the ability later.\n+        chains, err := leaf.Verify(opts)\n+        if err != nil {\n+                return nil, err\n+        }\n+        signingKey = getCertPubKey(chains)\n+\n+        return\n }\n \n func getCertPubKey(chains [][]*x509.Certificate) crypto.PublicKey {\n-\t// NOTE(milosgajdos): if there are no certificates\n-\t// header.Certificates call above returns error, so we are\n-\t// guaranteed to get at least one certificate chain.\n-\t// We pick the leaf certificate chain.\n-\tchain := chains[0]\n-\n-\t// NOTE(milosgajdos): header.Certificates call returns the result\n-\t// of leafCert.Verify which is a call to x509.Certificate.Verify.\n-\t// If successful, it returns one or more chains where the first\n-\t// element of the chain is x5c and the last element is from opts.Roots.\n-\t// See: https://pkg.go.dev/crypto/x509#Certificate.Verify\n-\tcert := chain[0]\n-\n-\t// NOTE: we dont have to verify that the public key in the leaf cert\n-\t// *is* the signing key: if it's not the signing then token claims\n-\t// verification with this key fails\n-\treturn cert.PublicKey.(crypto.PublicKey)\n+        // NOTE(milosgajdos): if there are no certificates\n+        // header.Certificates call above returns error, so we are\n+        // guaranteed to get at least one certificate chain.\n+        // We pick the leaf certificate chain.\n+        chain := chains[0]\n+\n+        // NOTE(milosgajdos): header.Certificates call returns the result\n+        // of leafCert.Verify which is a call to x509.Certificate.Verify.\n+        // If successful, it returns one or more chains where the first\n+        // element of the chain is x5c and the last element is from opts.Roots.\n+        // See: https://pkg.go.dev/crypto/x509#Certificate.Verify\n+        cert := chain[0]\n+\n+        // NOTE: we dont have to verify that the public key in the leaf cert\n+        // *is* the signing key: if it's not the signing then token claims\n+        // verification with this key fails\n+        return cert.PublicKey.(crypto.PublicKey)\n }\n \n // accessSet returns a set of actions available for the resource\n // actions listed in the `access` section of this token.\n func (c *ClaimSet) accessSet() accessSet {\n-\taccessSet := make(accessSet, len(c.Access))\n-\n-\tfor _, resourceActions := range c.Access {\n-\t\tresource := auth.Resource{\n-\t\t\tType: resourceActions.Type,\n-\t\t\tName: resourceActions.Name,\n-\t\t}\n-\n-\t\tset, exists := accessSet[resource]\n-\t\tif !exists {\n-\t\t\tset = newActionSet()\n-\t\t\taccessSet[resource] = set\n-\t\t}\n-\n-\t\tfor _, action := range resourceActions.Actions {\n-\t\t\tset.add(action)\n-\t\t}\n-\t}\n-\n-\treturn accessSet\n+        accessSet := make(accessSet, len(c.Access))\n+\n+        for _, resourceActions := range c.Access {\n+                resource := auth.Resource{\n+                        Type: resourceActions.Type,\n+                        Name: resourceActions.Name,\n+                }\n+\n+                set, exists := accessSet[resource]\n+                if !exists {\n+                        set = newActionSet()\n+                        accessSet[resource] = set\n+                }\n+\n+                for _, action := range resourceActions.Actions {\n+                        set.add(action)\n+                }\n+        }\n+\n+        return accessSet\n }\n \n func (c *ClaimSet) resources() []auth.Resource {\n-\tresourceSet := map[auth.Resource]struct{}{}\n-\n-\tfor _, resourceActions := range c.Access {\n-\t\tresource := auth.Resource{\n-\t\t\tType:  resourceActions.Type,\n-\t\t\tClass: resourceActions.Class,\n-\t\t\tName:  resourceActions.Name,\n-\t\t}\n-\t\tresourceSet[resource] = struct{}{}\n-\t}\n-\n-\tresources := make([]auth.Resource, 0, len(resourceSet))\n-\tfor resource := range resourceSet {\n-\t\tresources = append(resources, resource)\n-\t}\n-\n-\treturn resources\n+        resourceSet := map[auth.Resource]struct{}{}\n+\n+        for _, resourceActions := range c.Access {\n+                resource := auth.Resource{\n+                        Type:  resourceActions.Type,\n+                        Class: resourceActions.Class,\n+                        Name:  resourceActions.Name,\n+                }\n+                resourceSet[resource] = struct{}{}\n+        }\n+\n+        resources := make([]auth.Resource, 0, len(resourceSet))\n+        for resource := range resourceSet {\n+                resources = append(resources, resource)\n+        }\n+\n+        return resources\n }\n"}
{"cve":"CVE-2022-24738:0708", "fix_patch": ""}
{"cve":"CVE-2022-31130:0708", "fix_patch": "diff --git a/pkg/util/proxyutil/reverse_proxy.go b/pkg/util/proxyutil/reverse_proxy.go\nindex 3ce5d3b4011..1da8b6b6a2a 100644\n--- a/pkg/util/proxyutil/reverse_proxy.go\n+++ b/pkg/util/proxyutil/reverse_proxy.go\n@@ -1,15 +1,15 @@\n package proxyutil\n \n import (\n-\t\"context\"\n-\t\"errors\"\n-\t\"log\"\n-\t\"net/http\"\n-\t\"net/http/httputil\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\tglog \"github.com/grafana/grafana/pkg/infra/log\"\n+        \"context\"\n+        \"errors\"\n+        \"log\"\n+        \"net/http\"\n+        \"net/http/httputil\"\n+        \"strings\"\n+        \"time\"\n+\n+        glog \"github.com/grafana/grafana/pkg/infra/log\"\n )\n \n // StatusClientClosedRequest A non-standard status code introduced by nginx\n@@ -23,70 +23,71 @@ type ReverseProxyOption func(*httputil.ReverseProxy)\n \n // NewReverseProxy creates a new httputil.ReverseProxy with sane default configuration.\n func NewReverseProxy(logger glog.Logger, director func(*http.Request), opts ...ReverseProxyOption) *httputil.ReverseProxy {\n-\tif logger == nil {\n-\t\tpanic(\"logger cannot be nil\")\n-\t}\n-\n-\tif director == nil {\n-\t\tpanic(\"director cannot be nil\")\n-\t}\n-\n-\tp := &httputil.ReverseProxy{\n-\t\tFlushInterval: time.Millisecond * 200,\n-\t\tErrorHandler:  errorHandler(logger),\n-\t\tErrorLog:      log.New(&logWrapper{logger: logger}, \"\", 0),\n-\t\tDirector:      director,\n-\t}\n-\n-\tfor _, opt := range opts {\n-\t\topt(p)\n-\t}\n-\n-\torigDirector := p.Director\n-\tp.Director = wrapDirector(origDirector)\n-\n-\tif p.ModifyResponse == nil {\n-\t\t// nolint:bodyclose\n-\t\tp.ModifyResponse = modifyResponse(logger)\n-\t} else {\n-\t\tmodResponse := p.ModifyResponse\n-\t\tp.ModifyResponse = func(resp *http.Response) error {\n-\t\t\tif err := modResponse(resp); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\t// nolint:bodyclose\n-\t\t\treturn modifyResponse(logger)(resp)\n-\t\t}\n-\t}\n-\n-\treturn p\n+        if logger == nil {\n+                panic(\"logger cannot be nil\")\n+        }\n+\n+        if director == nil {\n+                panic(\"director cannot be nil\")\n+        }\n+\n+        p := &httputil.ReverseProxy{\n+                FlushInterval: time.Millisecond * 200,\n+                ErrorHandler:  errorHandler(logger),\n+                ErrorLog:      log.New(&logWrapper{logger: logger}, \"\", 0),\n+                Director:      director,\n+        }\n+\n+        for _, opt := range opts {\n+                opt(p)\n+        }\n+\n+        origDirector := p.Director\n+        p.Director = wrapDirector(origDirector)\n+\n+        if p.ModifyResponse == nil {\n+                // nolint:bodyclose\n+                p.ModifyResponse = modifyResponse(logger)\n+        } else {\n+                modResponse := p.ModifyResponse\n+                p.ModifyResponse = func(resp *http.Response) error {\n+                        if err := modResponse(resp); err != nil {\n+                                return err\n+                        }\n+\n+                        // nolint:bodyclose\n+                        return modifyResponse(logger)(resp)\n+                }\n+        }\n+\n+        return p\n }\n \n // wrapDirector wraps a director and adds additional functionality.\n func wrapDirector(d func(*http.Request)) func(req *http.Request) {\n-\treturn func(req *http.Request) {\n-\t\td(req)\n-\t\tPrepareProxyRequest(req)\n-\n-\t\t// Clear Origin and Referer to avoid CORS issues\n-\t\treq.Header.Del(\"Origin\")\n-\t\treq.Header.Del(\"Referer\")\n-\t}\n+        return func(req *http.Request) {\n+                d(req)\n+                PrepareProxyRequest(req)\n+\n+                // Clear Origin and Referer to avoid CORS issues\n+                req.Header.Del(\"Origin\")\n+                reqreq.Header.Del(\"Origin\")\n+.Header.Del(\"Referer\")\n+        }\n }\n \n // modifyResponse enforces certain constraints on http.Response.\n func modifyResponse(logger glog.Logger) func(resp *http.Response) error {\n-\treturn func(resp *http.Response) error {\n-\t\tresp.Header.Del(\"Set-Cookie\")\n-\t\tSetProxyResponseHeaders(resp.Header)\n-\t\treturn nil\n-\t}\n+        return func(resp *http.Response) error {\n+                resp.Header.Del(\"Set-Cookie\")\n+                SetProxyResponseHeaders(resp.Header)\n+                return nil\n+        }\n }\n \n type timeoutError interface {\n-\terror\n-\tTimeout() bool\n+        error\n+        Timeout() bool\n }\n \n // errorHandler handles any errors happening while proxying a request and enforces\n@@ -95,52 +96,52 @@ type timeoutError interface {\n // If timeout happens while communicating with upstream server we return http.StatusGatewayTimeout.\n // If any other error we return http.StatusBadGateway.\n func errorHandler(logger glog.Logger) func(http.ResponseWriter, *http.Request, error) {\n-\treturn func(w http.ResponseWriter, r *http.Request, err error) {\n-\t\tif errors.Is(err, context.Canceled) {\n-\t\t\tlogger.Debug(\"Proxy request cancelled by client\")\n-\t\t\tw.WriteHeader(StatusClientClosedRequest)\n-\t\t\treturn\n-\t\t}\n-\n-\t\t// nolint:errorlint\n-\t\tif timeoutErr, ok := err.(timeoutError); ok && timeoutErr.Timeout() {\n-\t\t\tlogger.Error(\"Proxy request timed out\", \"err\", err)\n-\t\t\tw.WriteHeader(http.StatusGatewayTimeout)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tlogger.Error(\"Proxy request failed\", \"err\", err)\n-\t\tw.WriteHeader(http.StatusBadGateway)\n-\t}\n+        return func(w http.ResponseWriter, r *http.Request, err error) {\n+                if errors.Is(err, context.Canceled) {\n+                        logger.Debug(\"Proxy request cancelled by client\")\n+                        w.WriteHeader(StatusClientClosedRequest)\n+                        return\n+                }\n+\n+                // nolint:errorlint\n+                if timeoutErr, ok := err.(timeoutError); ok && timeoutErr.Timeout() {\n+                        logger.Error(\"Proxy request timed out\", \"err\", err)\n+                        w.WriteHeader(http.StatusGatewayTimeout)\n+                        return\n+                }\n+\n+                logger.Error(\"Proxy request failed\", \"err\", err)\n+                w.WriteHeader(http.StatusBadGateway)\n+        }\n }\n \n type logWrapper struct {\n-\tlogger glog.Logger\n+        logger glog.Logger\n }\n \n // Write writes log messages as bytes from proxy.\n func (lw *logWrapper) Write(p []byte) (n int, err error) {\n-\twithoutNewline := strings.TrimSuffix(string(p), \"\\n\")\n-\tlw.logger.Error(\"Proxy request error\", \"error\", withoutNewline)\n-\treturn len(p), nil\n+        withoutNewline := strings.TrimSuffix(string(p), \"\\n\")\n+        lw.logger.Error(\"Proxy request error\", \"error\", withoutNewline)\n+        return len(p), nil\n }\n \n func WithTransport(transport http.RoundTripper) ReverseProxyOption {\n-\tif transport == nil {\n-\t\tpanic(\"transport cannot be nil\")\n-\t}\n+        if transport == nil {\n+                panic(\"transport cannot be nil\")\n+        }\n \n-\treturn ReverseProxyOption(func(rp *httputil.ReverseProxy) {\n-\t\trp.Transport = transport\n-\t})\n+        return ReverseProxyOption(func(rp *httputil.ReverseProxy) {\n+                rp.Transport = transport\n+        })\n }\n \n func WithModifyResponse(fn func(*http.Response) error) ReverseProxyOption {\n-\tif fn == nil {\n-\t\tpanic(\"fn cannot be nil\")\n-\t}\n+        if fn == nil {\n+                panic(\"fn cannot be nil\")\n+        }\n \n-\treturn ReverseProxyOption(func(rp *httputil.ReverseProxy) {\n-\t\trp.ModifyResponse = fn\n-\t})\n+        return ReverseProxyOption(func(rp *httputil.ReverseProxy) {\n+                rp.ModifyResponse = fn\n+        })\n }\n"}
{"cve":"CVE-2022-23538:0708", "fix_patch": "diff --git a/client/pull.go b/client/pull.go\nindex 145cbe9..10ef3a9 100644\n--- a/client/pull.go\n+++ b/client/pull.go\n@@ -6,17 +6,17 @@\n package client\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\tjsonresp \"github.com/sylabs/json-resp\"\n-\t\"golang.org/x/sync/errgroup\"\n+        \"context\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        jsonresp \"github.com/sylabs/json-resp\"\n+        \"golang.org/x/sync/errgroup\"\n )\n \n // DownloadImage will retrieve an image from the Container Library, saving it\n@@ -24,158 +24,161 @@ import (\n // within the context. It is recommended to use a large value (ie. 1800 seconds)\n // to prevent timeout when downloading large images.\n func (c *Client) DownloadImage(ctx context.Context, w io.Writer, arch, path, tag string, callback func(int64, io.Reader, io.Writer) error) error {\n-\tif arch != \"\" && !c.apiAtLeast(ctx, APIVersionV2ArchTags) {\n-\t\tc.Logger.Logf(\"This library does not support architecture specific tags\")\n-\t\tc.Logger.Logf(\"The image returned may not be the requested architecture\")\n-\t}\n-\n-\tif strings.Contains(path, \":\") {\n-\t\treturn fmt.Errorf(\"malformed image path: %s\", path)\n-\t}\n-\n-\tif tag == \"\" {\n-\t\ttag = \"latest\"\n-\t}\n-\n-\tapiPath := fmt.Sprintf(\"v1/imagefile/%s:%s\", strings.TrimPrefix(path, \"/\"), tag)\n-\tq := url.Values{}\n-\tq.Add(\"arch\", arch)\n-\n-\tc.Logger.Logf(\"Pulling from URL: %s\", apiPath)\n-\n-\treq, err := c.newRequest(ctx, http.MethodGet, apiPath, q.Encode(), nil)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tres, err := c.HTTPClient.Do(req)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer res.Body.Close()\n-\n-\tif res.StatusCode == http.StatusNotFound {\n-\t\treturn fmt.Errorf(\"requested image was not found in the library\")\n-\t}\n-\n-\tif res.StatusCode != http.StatusOK {\n-\t\terr := jsonresp.ReadError(res.Body)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"download did not succeed: %v\", err)\n-\t\t}\n-\t\treturn fmt.Errorf(\"unexpected http status code: %d\", res.StatusCode)\n-\t}\n-\n-\tc.Logger.Logf(\"OK response received, beginning body download\")\n-\n-\tif callback != nil {\n-\t\terr = callback(res.ContentLength, res.Body, w)\n-\t} else {\n-\t\t_, err = io.Copy(w, res.Body)\n-\t}\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tc.Logger.Logf(\"Download complete\")\n-\n-\treturn nil\n+        if arch != \"\" && !c.apiAtLeast(ctx, APIVersionV2ArchTags) {\n+                c.Logger.Logf(\"This library does not support architecture specific tags\")\n+                c.Logger.Logf(\"The image returned may not be the requested architecture\")\n+        }\n+\n+        if strings.Contains(path, \":\") {\n+                return fmt.Errorf(\"malformed image path: %s\", path)\n+        }\n+\n+        if tag == \"\" {\n+                tag = \"latest\"\n+        }\n+\n+        apiPath := fmt.Sprintf(\"v1/imagefile/%s:%s\", strings.TrimPrefix(path, \"/\"), tag)\n+        q := url.Values{}\n+        q.Add(\"arch\", arch)\n+\n+        c.Logger.Logf(\"Pulling from URL: %s\", apiPath)\n+\n+        req, err := c.newRequest(ctx, http.MethodGet, apiPath, q.Encode(), nil)\n+        if err != nil {\n+                return err\n+        }\n+\n+        res, err := c.HTTPClient.Do(req)\n+        if err != nil {\n+                return err\n+        }\n+        defer res.Body.Close()\n+\n+        if res.StatusCode == http.StatusNotFound {\n+                return fmt.Errorf(\"requested image was not found in the library\")\n+        }\n+\n+        if res.StatusCode != http.StatusOK {\n+                err := jsonresp.ReadError(res.Body)\n+                if err != nil {\n+                        return fmt.Errorf(\"download did not succeed: %v\", err)\n+                }\n+                return fmt.Errorf(\"unexpected http status code: %d\", res.StatusCode)\n+        }\n+\n+        c.Logger.Logf(\"OK response received, beginning body download\")\n+\n+        if callback != nil {\n+                err = callback(res.ContentLength, res.Body, w)\n+        } else {\n+                _, err = io.Copy(w, res.Body)\n+        }\n+        if err != nil {\n+                return err\n+        }\n+\n+        c.Logger.Logf(\"Download complete\")\n+\n+        return nil\n }\n \n // partSpec defines one part of multi-part (concurrent) download.\n type partSpec struct {\n-\tStart      int64\n-\tEnd        int64\n-\tBufferSize int64\n+        Start      int64\n+        End        int64\n+        BufferSize int64\n }\n \n // Downloader defines concurrency (# of requests) and part size for download operation.\n type Downloader struct {\n-\t// Concurrency defines concurrency for multi-part downloads.\n-\tConcurrency uint\n+        // Concurrency defines concurrency for multi-part downloads.\n+        Concurrency uint\n \n-\t// PartSize specifies size of part for multi-part downloads. Default is 5 MiB.\n-\tPartSize int64\n+        // PartSize specifies size of part for multi-part downloads. Default is 5 MiB.\n+        PartSize int64\n \n-\t// BufferSize specifies buffer size used for multi-part downloader routine.\n-\t// Default is 32 KiB.\n-\tBufferSize int64\n+        // BufferSize specifies buffer size used for multi-part downloader routine.\n+        // Default is 32 KiB.\n+        BufferSize int64\n }\n \n // httpGetRangeRequest performs HTTP GET range request to URL specified by 'u' in range start-end.\n func (c *Client) httpGetRangeRequest(ctx context.Context, url string, start, end int64) (*http.Response, error) {\n-\treq, err := c.newRequestWithURL(ctx, http.MethodGet, url, nil)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        req, err := c.newRequestWithURL(ctx, http.MethodGet, url, nil)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\treq.Header.Add(\"Range\", fmt.Sprintf(\"bytes=%d-%d\", start, end))\n+req.Header.Add(\"Range\", fmt.Sprintf(\"bytes=%d-%d\", start, end))\n \n-\treturn c.HTTPClient.Do(req)\n+// Do not send auth token on range requests\n+req.Header.Del(\"Authorization\")\n+\n+return c.HTTPClient.Do(req)\n }\n \n // downloadFilePart writes range to dst as specified in bufferSpec.\n func (c *Client) downloadFilePart(ctx context.Context, dst *os.File, url string, ps *partSpec, pb ProgressBar) error {\n-\tresp, err := c.httpGetRangeRequest(ctx, url, ps.Start, ps.End)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer resp.Body.Close()\n-\n-\t// allocate transfer buffer for part\n-\tbuf := make([]byte, ps.BufferSize)\n-\n-\tfor bytesRead := int64(0); bytesRead < ps.End-ps.Start+1; {\n-\t\tn, err := io.ReadFull(resp.Body, buf)\n-\n-\t\t// EOF and unexpected EOF shouldn't be handled as errors since short\n-\t\t// reads are expected if the part size is less than buffer size e.g.\n-\t\t// the last part if part isn't on size boundary.\n-\t\tif err != nil && n == 0 {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tpb.IncrBy(n)\n-\n-\t\t// WriteAt() is a wrapper around pwrite() which is an atomic\n-\t\t// seek-and-write operation.\n-\t\tif _, err := dst.WriteAt(buf[:n], ps.Start+bytesRead); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tbytesRead += int64(n)\n-\t}\n-\treturn nil\n+        resp, err := c.httpGetRangeRequest(ctx, url, ps.Start, ps.End)\n+        if err != nil {\n+                return err\n+        }\n+        defer resp.Body.Close()\n+\n+        // allocate transfer buffer for part\n+        buf := make([]byte, ps.BufferSize)\n+\n+        for bytesRead := int64(0); bytesRead < ps.End-ps.Start+1; {\n+                n, err := io.ReadFull(resp.Body, buf)\n+\n+                // EOF and unexpected EOF shouldn't be handled as errors since short\n+                // reads are expected if the part size is less than buffer size e.g.\n+                // the last part if part isn't on size boundary.\n+                if err != nil && n == 0 {\n+                        return err\n+                }\n+\n+                pb.IncrBy(n)\n+\n+                // WriteAt() is a wrapper around pwrite() which is an atomic\n+                // seek-and-write operation.\n+                if _, err := dst.WriteAt(buf[:n], ps.Start+bytesRead); err != nil {\n+                        return err\n+                }\n+                bytesRead += int64(n)\n+        }\n+        return nil\n }\n \n // downloadWorker is a worker func for processing jobs in stripes channel.\n func (c *Client) downloadWorker(ctx context.Context, dst *os.File, url string, parts <-chan partSpec, pb ProgressBar) func() error {\n-\treturn func() error {\n-\t\tfor ps := range parts {\n-\t\t\tif err := c.downloadFilePart(ctx, dst, url, &ps, pb); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func() error {\n+                for ps := range parts {\n+                        if err := c.downloadFilePart(ctx, dst, url, &ps, pb); err != nil {\n+                                return err\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n func (c *Client) getContentLength(ctx context.Context, url string) (int64, error) {\n-\t// Perform short request to determine content length.\n-\tresp, err := c.httpGetRangeRequest(ctx, url, 0, 1024)\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tdefer resp.Body.Close()\n-\n-\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusPartialContent {\n-\t\tif resp.StatusCode == http.StatusNotFound {\n-\t\t\treturn 0, fmt.Errorf(\"requested image was not found in the library\")\n-\t\t}\n-\t\treturn 0, fmt.Errorf(\"unexpected HTTP status: %d\", resp.StatusCode)\n-\t}\n-\n-\tvals := strings.Split(resp.Header.Get(\"Content-Range\"), \"/\")\n-\treturn strconv.ParseInt(vals[1], 0, 64)\n+        // Perform short request to determine content length.\n+        resp, err := c.httpGetRangeRequest(ctx, url, 0, 1024)\n+        if err != nil {\n+                return 0, err\n+        }\n+        defer resp.Body.Close()\n+\n+        if resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusPartialContent {\n+                if resp.StatusCode == http.StatusNotFound {\n+                        return 0, fmt.Errorf(\"requested image was not found in the library\")\n+                }\n+                return 0, fmt.Errorf(\"unexpected HTTP status: %d\", resp.StatusCode)\n+        }\n+\n+        vals := strings.Split(resp.Header.Get(\"Content-Range\"), \"/\")\n+        return strconv.ParseInt(vals[1], 0, 64)\n }\n \n // NoopProgressBar implements ProgressBarInterface to allow disabling the progress bar\n@@ -199,22 +202,22 @@ func (*NoopProgressBar) Wait() {}\n // ProgressBar provides a minimal interface for interacting with a progress bar.\n // Init is called prior to concurrent download operation.\n type ProgressBar interface {\n-\t// Initialize progress bar. Argument is size of file to set progress bar limit.\n-\tInit(int64)\n+        // Initialize progress bar. Argument is size of file to set progress bar limit.\n+        Init(int64)\n \n-\t// ProxyReader wraps r with metrics required for progress tracking. Only useful for\n-\t// single stream downloads.\n-\tProxyReader(io.Reader) io.ReadCloser\n+        // ProxyReader wraps r with metrics required for progress tracking. Only useful for\n+        // single stream downloads.\n+        ProxyReader(io.Reader) io.ReadCloser\n \n-\t// IncrBy increments the progress bar. It is called after each concurrent\n-\t// buffer transfer.\n-\tIncrBy(int)\n+        // IncrBy increments the progress bar. It is called after each concurrent\n+        // buffer transfer.\n+        IncrBy(int)\n \n-\t// Abort terminates the progress bar.\n-\tAbort(bool)\n+        // Abort terminates the progress bar.\n+        Abort(bool)\n \n-\t// Wait waits for the progress bar to complete.\n-\tWait()\n+        // Wait waits for the progress bar to complete.\n+        Wait()\n }\n \n // ConcurrentDownloadImage implements a multi-part (concurrent) downloader for\n@@ -226,150 +229,150 @@ type ProgressBar interface {\n // concurrency for source files that do not meet minimum size for multi-part\n // downloads.\n func (c *Client) ConcurrentDownloadImage(ctx context.Context, dst *os.File, arch, path, tag string, spec *Downloader, pb ProgressBar) error {\n-\tif pb == nil {\n-\t\tpb = &NoopProgressBar{}\n-\t}\n-\n-\tif arch != \"\" && !c.apiAtLeast(ctx, APIVersionV2ArchTags) {\n-\t\tc.Logger.Logf(\"This library does not support architecture specific tags\")\n-\t\tc.Logger.Logf(\"The image returned may not be the requested architecture\")\n-\t}\n-\n-\tif strings.Contains(path, \":\") {\n-\t\treturn fmt.Errorf(\"malformed image path: %s\", path)\n-\t}\n-\n-\tif tag == \"\" {\n-\t\ttag = \"latest\"\n-\t}\n-\n-\tapiPath := fmt.Sprintf(\"v1/imagefile/%s:%s\", strings.TrimPrefix(path, \"/\"), tag)\n-\tq := url.Values{}\n-\tq.Add(\"arch\", arch)\n-\n-\tc.Logger.Logf(\"Pulling from URL: %s\", apiPath)\n-\n-\tcustomHTTPClient := &http.Client{\n-\t\tTransport: c.HTTPClient.Transport,\n-\t\tCheckRedirect: func(req *http.Request, via []*http.Request) error {\n-\t\t\tif req.Response.StatusCode == http.StatusSeeOther {\n-\t\t\t\treturn http.ErrUseLastResponse\n-\t\t\t}\n-\t\t\tmaxRedir := 10\n-\t\t\tif len(via) >= maxRedir {\n-\t\t\t\treturn fmt.Errorf(\"stopped after %d redirects\", maxRedir)\n-\t\t\t}\n-\t\t\treturn nil\n-\t\t},\n-\t\tJar:     c.HTTPClient.Jar,\n-\t\tTimeout: c.HTTPClient.Timeout,\n-\t}\n-\n-\treq, err := c.newRequest(ctx, http.MethodGet, apiPath, q.Encode(), nil)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tres, err := customHTTPClient.Do(req)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer res.Body.Close()\n-\n-\tif res.StatusCode == http.StatusNotFound {\n-\t\treturn fmt.Errorf(\"requested image was not found in the library\")\n-\t}\n-\n-\tif res.StatusCode == http.StatusOK {\n-\t\t// Library endpoint does not provide HTTP redirection response, treat as single stream, direct download\n-\t\tc.Logger.Logf(\"Library endpoint does not support concurrent downloads; reverting to single stream\")\n-\n-\t\treturn c.singleStreamDownload(ctx, dst, res, pb)\n-\t}\n-\n-\tif res.StatusCode != http.StatusSeeOther {\n-\t\treturn fmt.Errorf(\"unexpected HTTP status %d: %v\", res.StatusCode, err)\n-\t}\n-\n-\turl := res.Header.Get(\"Location\")\n-\n-\tcontentLength, err := c.getContentLength(ctx, url)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tnumParts := uint(1 + (contentLength-1)/spec.PartSize)\n-\n-\tc.Logger.Logf(\"size: %d, parts: %d, concurrency: %d, partsize: %d, bufsize: %d\",\n-\t\tcontentLength, numParts, spec.Concurrency, spec.PartSize, spec.BufferSize,\n-\t)\n-\n-\tjobs := make(chan partSpec, numParts)\n-\n-\tg, ctx := errgroup.WithContext(ctx)\n-\n-\t// initialize progress bar\n-\tpb.Init(contentLength)\n-\n-\t// if spec.Requests is greater than number of parts for requested file,\n-\t// set concurrency to number of parts\n-\tconcurrency := spec.Concurrency\n-\tif numParts < spec.Concurrency {\n-\t\tconcurrency = numParts\n-\t}\n-\n-\t// start workers to manage concurrent HTTP requests\n-\tfor workerID := uint(0); workerID <= concurrency; workerID++ {\n-\t\tg.Go(c.downloadWorker(ctx, dst, url, jobs, pb))\n-\t}\n-\n-\t// iterate over parts, adding to job queue\n-\tfor part := uint(0); part < numParts; part++ {\n-\t\tpartSize := spec.PartSize\n-\t\tif part == numParts-1 {\n-\t\t\tpartSize = contentLength - int64(numParts-1)*spec.PartSize\n-\t\t}\n-\n-\t\tps := partSpec{\n-\t\t\tStart:      int64(part) * spec.PartSize,\n-\t\t\tEnd:        int64(part)*spec.PartSize + partSize - 1,\n-\t\t\tBufferSize: spec.BufferSize,\n-\t\t}\n-\n-\t\tjobs <- ps\n-\t}\n-\n-\tclose(jobs)\n-\n-\t// wait on errgroup\n-\terr = g.Wait()\n-\tif err != nil {\n-\t\t// cancel/remove progress bar on error\n-\t\tpb.Abort(true)\n-\t}\n-\n-\t// wait on progress bar\n-\tpb.Wait()\n-\n-\treturn err\n+        if pb == nil {\n+                pb = &NoopProgressBar{}\n+        }\n+\n+        if arch != \"\" && !c.apiAtLeast(ctx, APIVersionV2ArchTags) {\n+                c.Logger.Logf(\"This library does not support architecture specific tags\")\n+                c.Logger.Logf(\"The image returned may not be the requested architecture\")\n+        }\n+\n+        if strings.Contains(path, \":\") {\n+                return fmt.Errorf(\"malformed image path: %s\", path)\n+        }\n+\n+        if tag == \"\" {\n+                tag = \"latest\"\n+        }\n+\n+        apiPath := fmt.Sprintf(\"v1/imagefile/%s:%s\", strings.TrimPrefix(path, \"/\"), tag)\n+        q := url.Values{}\n+        q.Add(\"arch\", arch)\n+\n+        c.Logger.Logf(\"Pulling from URL: %s\", apiPath)\n+\n+        customHTTPClient := &http.Client{\n+                Transport: c.HTTPClient.Transport,\n+                CheckRedirect: func(req *http.Request, via []*http.Request) error {\n+                        if req.Response.StatusCode == http.StatusSeeOther {\n+                                return http.ErrUseLastResponse\n+                        }\n+                        maxRedir := 10\n+                        if len(via) >= maxRedir {\n+                                return fmt.Errorf(\"stopped after %d redirects\", maxRedir)\n+                        }\n+                        return nil\n+                },\n+                Jar:     c.HTTPClient.Jar,\n+                Timeout: c.HTTPClient.Timeout,\n+        }\n+\n+        req, err := c.newRequest(ctx, http.MethodGet, apiPath, q.Encode(), nil)\n+        if err != nil {\n+                return err\n+        }\n+\n+        res, err := customHTTPClient.Do(req)\n+        if err != nil {\n+                return err\n+        }\n+        defer res.Body.Close()\n+\n+        if res.StatusCode == http.StatusNotFound {\n+                return fmt.Errorf(\"requested image was not found in the library\")\n+        }\n+\n+        if res.StatusCode == http.StatusOK {\n+                // Library endpoint does not provide HTTP redirection response, treat as single stream, direct download\n+                c.Logger.Logf(\"Library endpoint does not support concurrent downloads; reverting to single stream\")\n+\n+                return c.singleStreamDownload(ctx, dst, res, pb)\n+        }\n+\n+        if res.StatusCode != http.StatusSeeOther {\n+                return fmt.Errorf(\"unexpected HTTP status %d: %v\", res.StatusCode, err)\n+        }\n+\n+        url := res.Header.Get(\"Location\")\n+\n+        contentLength, err := c.getContentLength(ctx, url)\n+        if err != nil {\n+                return err\n+        }\n+\n+        numParts := uint(1 + (contentLength-1)/spec.PartSize)\n+\n+        c.Logger.Logf(\"size: %d, parts: %d, concurrency: %d, partsize: %d, bufsize: %d\",\n+                contentLength, numParts, spec.Concurrency, spec.PartSize, spec.BufferSize,\n+        )\n+\n+        jobs := make(chan partSpec, numParts)\n+\n+        g, ctx := errgroup.WithContext(ctx)\n+\n+        // initialize progress bar\n+        pb.Init(contentLength)\n+\n+        // if spec.Requests is greater than number of parts for requested file,\n+        // set concurrency to number of parts\n+        concurrency := spec.Concurrency\n+        if numParts < spec.Concurrency {\n+                concurrency = numParts\n+        }\n+\n+        // start workers to manage concurrent HTTP requests\n+        for workerID := uint(0); workerID <= concurrency; workerID++ {\n+                g.Go(c.downloadWorker(ctx, dst, url, jobs, pb))\n+        }\n+\n+        // iterate over parts, adding to job queue\n+        for part := uint(0); part < numParts; part++ {\n+                partSize := spec.PartSize\n+                if part == numParts-1 {\n+                        partSize = contentLength - int64(numParts-1)*spec.PartSize\n+                }\n+\n+                ps := partSpec{\n+                        Start:      int64(part) * spec.PartSize,\n+                        End:        int64(part)*spec.PartSize + partSize - 1,\n+                        BufferSize: spec.BufferSize,\n+                }\n+\n+                jobs <- ps\n+        }\n+\n+        close(jobs)\n+\n+        // wait on errgroup\n+        err = g.Wait()\n+        if err != nil {\n+                // cancel/remove progress bar on error\n+                pb.Abort(true)\n+        }\n+\n+        // wait on progress bar\n+        pb.Wait()\n+\n+        return err\n }\n \n func (c *Client) singleStreamDownload(ctx context.Context, fp *os.File, res *http.Response, pb ProgressBar) error {\n-\tcontentLength := int64(-1)\n-\tval := res.Header.Get(\"Content-Length\")\n-\tif val != \"\" {\n-\t\tvar err error\n-\t\tif contentLength, err = strconv.ParseInt(val, 0, 64); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\tpb.Init(contentLength)\n-\n-\tproxyReader := pb.ProxyReader(res.Body)\n-\tdefer proxyReader.Close()\n-\n-\tif _, err := io.Copy(fp, proxyReader); err != nil {\n-\t\treturn err\n-\t}\n-\treturn nil\n+        contentLength := int64(-1)\n+        val := res.Header.Get(\"Content-Length\")\n+        if val != \"\" {\n+                var err error\n+                if contentLength, err = strconv.ParseInt(val, 0, 64); err != nil {\n+                        return err\n+                }\n+        }\n+        pb.Init(contentLength)\n+\n+        proxyReader := pb.ProxyReader(res.Body)\n+        defer proxyReader.Close()\n+\n+        if _, err := io.Copy(fp, proxyReader); err != nil {\n+                return err\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2024-47616:0708", "fix_patch": "diff --git a/pkg/grpcutil/options.go b/pkg/grpcutil/options.go\nindex d3329251..1fe25ad0 100644\n--- a/pkg/grpcutil/options.go\n+++ b/pkg/grpcutil/options.go\n@@ -1,113 +1,125 @@\n package grpcutil\n \n import (\n-\t\"context\"\n-\t\"encoding/base64\"\n-\t\"time\"\n-\n-\t\"github.com/go-jose/go-jose/v3\"\n-\t\"github.com/go-jose/go-jose/v3/jwt\"\n-\t\"google.golang.org/grpc\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n+        \"context\"\n+        \"encoding/base64\"\n+\"fmt\"\n+        \"time\"\n+\n+        \"github.com/go-jose/go-jose/v3\"\n+        \"github.com/go-jose/go-jose/v3/jwt\"\n+        \"google.golang.org/grpc\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n )\n \n // WithStreamSignedJWT returns a StreamClientInterceptor that adds a JWT to requests.\n func WithStreamSignedJWT(getKey func() []byte) grpc.StreamClientInterceptor {\n-\treturn func(\n-\t\tctx context.Context,\n-\t\tdesc *grpc.StreamDesc,\n-\t\tcc *grpc.ClientConn,\n-\t\tmethod string, streamer grpc.Streamer,\n-\t\topts ...grpc.CallOption,\n-\t) (grpc.ClientStream, error) {\n-\t\tctx, err := withSignedJWT(ctx, getKey())\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\treturn streamer(ctx, desc, cc, method, opts...)\n-\t}\n+        return func(\n+                ctx context.Context,\n+                desc *grpc.StreamDesc,\n+                cc *grpc.ClientConn,\n+                method string, streamer grpc.Streamer,\n+                opts ...grpc.CallOption,\n+        ) (grpc.ClientStream, error) {\n+                ctx, err := withSignedJWT(ctx, getKey())\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                return streamer(ctx, desc, cc, method, opts...)\n+        }\n }\n \n // WithUnarySignedJWT returns a UnaryClientInterceptor that adds a JWT to requests.\n func WithUnarySignedJWT(getKey func() []byte) grpc.UnaryClientInterceptor {\n-\treturn func(ctx context.Context, method string, req, reply any, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {\n-\t\tctx, err := withSignedJWT(ctx, getKey())\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\treturn invoker(ctx, method, req, reply, cc, opts...)\n-\t}\n+        return func(ctx context.Context, method string, req, reply any, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {\n+                ctx, err := withSignedJWT(ctx, getKey())\n+                if err != nil {\n+                        return err\n+                }\n+\n+                return invoker(ctx, method, req, reply, cc, opts...)\n+        }\n }\n \n func withSignedJWT(ctx context.Context, key []byte) (context.Context, error) {\n-\tif len(key) > 0 {\n-\t\tsig, err := jose.NewSigner(jose.SigningKey{Algorithm: jose.HS256, Key: key},\n-\t\t\t(&jose.SignerOptions{}).WithType(\"JWT\"))\n-\t\tif err != nil {\n-\t\t\treturn ctx, err\n-\t\t}\n-\n-\t\trawjwt, err := jwt.Signed(sig).Claims(jwt.Claims{\n-\t\t\tExpiry: jwt.NewNumericDate(time.Now().Add(time.Hour)),\n-\t\t}).CompactSerialize()\n-\t\tif err != nil {\n-\t\t\treturn ctx, err\n-\t\t}\n-\n-\t\tctx = WithOutgoingJWT(ctx, rawjwt)\n-\t}\n-\treturn ctx, nil\n+        if len(key) > 0 {\n+                sig, err := jose.NewSigner(jose.SigningKey{Algorithm: jose.HS256, Key: key},\n+                        (&jose.SignerOptions{}).WithType(\"JWT\"))\n+                if err != nil {\n+                        return ctx, err\n+                }\n+\n+                rawjwt, err := jwt.Signed(sig).Claims(jwt.Claims{\n+                        Expiry: jwt.NewNumericDate(time.Now().Add(time.Hour)),\n+                }).CompactSerialize()\n+                if err != nil {\n+                        return ctx, err\n+                }\n+\n+                ctx = WithOutgoingJWT(ctx, rawjwt)\n+        }\n+        return ctx, nil\n }\n \n // UnaryRequireSignedJWT requires a JWT in the gRPC metadata and that it be signed by the base64-encoded key.\n-func UnaryRequireSignedJWT(key string) grpc.UnaryServerInterceptor {\n-\tkeyBS, _ := base64.StdEncoding.DecodeString(key)\n-\treturn func(ctx context.Context, req any, _ *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp any, err error) {\n-\t\tif err := RequireSignedJWT(ctx, keyBS); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\treturn handler(ctx, req)\n-\t}\n+func UnaryRequireSignedJWT(key string, aud string) grpc.UnaryServerInterceptor {\n+        keyBS, _ := base64.StdEncoding.DecodeString(key)\n+        return func(ctx context.Context, req any, _ *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp any, err error) {\n+                if err := RequireSignedJWT(ctx, keyBS, aud); err != nil {\n+                        return nil, err\n+                }\n+                return handler(ctx, req)\n+        }\n }\n \n // StreamRequireSignedJWT requires a JWT in the gRPC metadata and that it be signed by the base64-encoded key.\n-func StreamRequireSignedJWT(key string) grpc.StreamServerInterceptor {\n-\tkeyBS, _ := base64.StdEncoding.DecodeString(key)\n-\treturn func(srv any, ss grpc.ServerStream, _ *grpc.StreamServerInfo, handler grpc.StreamHandler) error {\n-\t\tif err := RequireSignedJWT(ss.Context(), keyBS); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\treturn handler(srv, ss)\n-\t}\n+func StreamRequireSignedJWT(key string, aud string) grpc.StreamServerInterceptor {\n+        keyBS, _ := base64.StdEncoding.DecodeString(key)\n+        return func(srv any, ss grpc.ServerStream, _ *grpc.StreamServerInfo, handler grpc.StreamHandler) error {\n+                if err := RequireSignedJWT(ss.Context(), keyBS, aud); err != nil {\n+                        return err\n+                }\n+                return handler(srv, ss)\n+        }\n }\n \n // RequireSignedJWT requires a JWT in the gRPC metadata and that it be signed by the given key.\n-func RequireSignedJWT(ctx context.Context, key []byte) error {\n-\tif len(key) > 0 {\n-\t\trawjwt, ok := JWTFromGRPCRequest(ctx)\n-\t\tif !ok {\n-\t\t\treturn status.Error(codes.Unauthenticated, \"unauthenticated\")\n-\t\t}\n-\n-\t\ttok, err := jwt.ParseSigned(rawjwt)\n-\t\tif err != nil {\n-\t\t\treturn status.Errorf(codes.Unauthenticated, \"invalid JWT: %v\", err)\n-\t\t}\n-\n-\t\tvar claims struct {\n-\t\t\tExpiry *jwt.NumericDate `json:\"exp,omitempty\"`\n-\t\t}\n-\t\terr = tok.Claims(key, &claims)\n-\t\tif err != nil {\n-\t\t\treturn status.Errorf(codes.Unauthenticated, \"invalid JWT: %v\", err)\n-\t\t}\n-\n-\t\tif claims.Expiry == nil || time.Now().After(claims.Expiry.Time()) {\n-\t\t\treturn status.Errorf(codes.Unauthenticated, \"expired JWT: %v\", err)\n-\t\t}\n-\t}\n-\treturn nil\n+func RequireSignedJWT(ctx context.Context, key []byte, aud string) error {\n+if len(key) > 0 {\n+rawjwt, ok := JWTFromGRPCRequest(ctx)\n+if !ok {\n+return status.Error(codes.Unauthenticated, \"unauthenticated\")\n }\n+\n+if err := validateJWT(rawjwt, key, aud); err != nil {\n+return status.Errorf(codes.Unauthenticated, err.Error())\n+}\n+}\n+return nil\n+}\n+\n+func validateJWT(rawjwt string, key []byte, aud string) error {\n+tok, err := jwt.ParseSigned(rawjwt)\n+if err != nil {\n+return fmt.Errorf(\"invalid JWT: %w\", err)\n+}\n+\n+claims := new(jwt.Claims)\n+err = tok.Claims(key, claims)\n+if err != nil {\n+return fmt.Errorf(\"invalid JWT: %w\", err)\n+}\n+\n+if err = claims.Validate(jwt.Expected{\n+Audience: jwt.Audience{aud},\n+Time:     time.Now(),\n+}); err != nil {\n+return fmt.Errorf(\"invalid JWT claims: %w\", err)\n+}\n+\n+return nil\n+}\n+\n+\n"}
{"cve":"CVE-2024-24747:0708", "fix_patch": "diff --git a/cmd/admin-handlers-users.go b/cmd/admin-handlers-users.go\nindex 1d9f2a5b1..8928b0a7a 100644\n--- a/cmd/admin-handlers-users.go\n+++ b/cmd/admin-handlers-users.go\n@@ -18,1327 +18,1333 @@\n package cmd\n \n import (\n-\t\"bytes\"\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"sort\"\n-\t\"time\"\n-\n-\t\"github.com/klauspost/compress/zip\"\n-\t\"github.com/minio/madmin-go/v3\"\n-\t\"github.com/minio/minio/internal/auth\"\n-\t\"github.com/minio/minio/internal/config/dns\"\n-\t\"github.com/minio/minio/internal/logger\"\n-\t\"github.com/minio/mux\"\n-\t\"github.com/minio/pkg/v2/policy\"\n+        \"bytes\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"os\"\n+        \"sort\"\n+        \"time\"\n+\n+        \"github.com/klauspost/compress/zip\"\n+        \"github.com/minio/madmin-go/v3\"\n+        \"github.com/minio/minio/internal/auth\"\n+        \"github.com/minio/minio/internal/config/dns\"\n+        \"github.com/minio/minio/internal/logger\"\n+        \"github.com/minio/mux\"\n+        \"github.com/minio/pkg/v2/policy\"\n )\n \n // RemoveUser - DELETE /minio/admin/v3/remove-user?accessKey=<access_key>\n func (a adminAPIHandlers) RemoveUser(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, cred := validateAdminReq(ctx, w, r, policy.DeleteUserAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\taccessKey := vars[\"accessKey\"]\n-\n-\tok, _, err := globalIAMSys.IsTempUser(accessKey)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\tif ok {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// When the user is root credential you are not allowed to\n-\t// remove the root user. Also you cannot delete yourself.\n-\tif accessKey == globalActiveCred.AccessKey || accessKey == cred.AccessKey {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif err := globalIAMSys.DeleteUser(ctx, accessKey, true); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemIAMUser,\n-\t\tIAMUser: &madmin.SRIAMUser{\n-\t\t\tAccessKey:   accessKey,\n-\t\t\tIsDeleteReq: true,\n-\t\t},\n-\t\tUpdatedAt: UTCNow(),\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, cred := validateAdminReq(ctx, w, r, policy.DeleteUserAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        accessKey := vars[\"accessKey\"]\n+\n+        ok, _, err := globalIAMSys.IsTempUser(accessKey)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        if ok {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                return\n+        }\n+\n+        // When the user is root credential you are not allowed to\n+        // remove the root user. Also you cannot delete yourself.\n+        if accessKey == globalActiveCred.AccessKey || accessKey == cred.AccessKey {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                return\n+        }\n+\n+        if err := globalIAMSys.DeleteUser(ctx, accessKey, true); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemIAMUser,\n+                IAMUser: &madmin.SRIAMUser{\n+                        AccessKey:   accessKey,\n+                        IsDeleteReq: true,\n+                },\n+                UpdatedAt: UTCNow(),\n+        }))\n }\n \n // ListBucketUsers - GET /minio/admin/v3/list-users?bucket={bucket}\n func (a adminAPIHandlers) ListBucketUsers(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n+        ctx := r.Context()\n \n-\tobjectAPI, cred := validateAdminReq(ctx, w, r, policy.ListUsersAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n+        objectAPI, cred := validateAdminReq(ctx, w, r, policy.ListUsersAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n \n-\tbucket := mux.Vars(r)[\"bucket\"]\n+        bucket := mux.Vars(r)[\"bucket\"]\n \n-\tpassword := cred.SecretKey\n+        password := cred.SecretKey\n \n-\tallCredentials, err := globalIAMSys.ListBucketUsers(ctx, bucket)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        allCredentials, err := globalIAMSys.ListBucketUsers(ctx, bucket)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\tdata, err := json.Marshal(allCredentials)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        data, err := json.Marshal(allCredentials)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\teconfigData, err := madmin.EncryptData(password, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        econfigData, err := madmin.EncryptData(password, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\twriteSuccessResponseJSON(w, econfigData)\n+        writeSuccessResponseJSON(w, econfigData)\n }\n \n // ListUsers - GET /minio/admin/v3/list-users\n func (a adminAPIHandlers) ListUsers(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, cred := validateAdminReq(ctx, w, r, policy.ListUsersAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tpassword := cred.SecretKey\n-\n-\tallCredentials, err := globalIAMSys.ListUsers(ctx)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Add ldap users which have mapped policies if in LDAP mode\n-\t// FIXME(vadmeste): move this to policy info in the future\n-\tldapUsers, err := globalIAMSys.ListLDAPUsers(ctx)\n-\tif err != nil && err != errIAMActionNotAllowed {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\tfor k, v := range ldapUsers {\n-\t\tallCredentials[k] = v\n-\t}\n-\n-\t// Marshal the response\n-\tdata, err := json.Marshal(allCredentials)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\teconfigData, err := madmin.EncryptData(password, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, econfigData)\n+        ctx := r.Context()\n+\n+        objectAPI, cred := validateAdminReq(ctx, w, r, policy.ListUsersAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        password := cred.SecretKey\n+\n+        allCredentials, err := globalIAMSys.ListUsers(ctx)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Add ldap users which have mapped policies if in LDAP mode\n+        // FIXME(vadmeste): move this to policy info in the future\n+        ldapUsers, err := globalIAMSys.ListLDAPUsers(ctx)\n+        if err != nil && err != errIAMActionNotAllowed {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        for k, v := range ldapUsers {\n+                allCredentials[k] = v\n+        }\n+\n+        // Marshal the response\n+        data, err := json.Marshal(allCredentials)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        econfigData, err := madmin.EncryptData(password, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, econfigData)\n }\n \n // GetUserInfo - GET /minio/admin/v3/user-info\n func (a adminAPIHandlers) GetUserInfo(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tvars := mux.Vars(r)\n-\tname := vars[\"accessKey\"]\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcheckDenyOnly := false\n-\tif name == cred.AccessKey {\n-\t\t// Check that there is no explicit deny - otherwise it's allowed\n-\t\t// to view one's own info.\n-\t\tcheckDenyOnly = true\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.GetUserAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t\tDenyOnly:        checkDenyOnly,\n-\t}) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\treturn\n-\t}\n-\n-\tuserInfo, err := globalIAMSys.GetUserInfo(ctx, name)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tdata, err := json.Marshal(userInfo)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, data)\n+        ctx := r.Context()\n+\n+        vars := mux.Vars(r)\n+        name := vars[\"accessKey\"]\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        checkDenyOnly := false\n+        if name == cred.AccessKey {\n+                // Check that there is no explicit deny - otherwise it's allowed\n+                // to view one's own info.\n+                checkDenyOnly = true\n+        }\n+\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.GetUserAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+                DenyOnly:        checkDenyOnly,\n+        }) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                return\n+        }\n+\n+        userInfo, err := globalIAMSys.GetUserInfo(ctx, name)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        data, err := json.Marshal(userInfo)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, data)\n }\n \n // UpdateGroupMembers - PUT /minio/admin/v3/update-group-members\n func (a adminAPIHandlers) UpdateGroupMembers(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.AddUserToGroupAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tdata, err := io.ReadAll(r.Body)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar updReq madmin.GroupAddRemove\n-\terr = json.Unmarshal(data, &updReq)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Reject if the group add and remove are temporary credentials, or root credential.\n-\tfor _, member := range updReq.Members {\n-\t\tok, _, err := globalIAMSys.IsTempUser(member)\n-\t\tif err != nil && err != errNoSuchUser {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tif ok {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\t// When the user is root credential you are not allowed to\n-\t\t// add policies for root user.\n-\t\tif member == globalActiveCred.AccessKey {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tvar updatedAt time.Time\n-\tif updReq.IsRemove {\n-\t\tupdatedAt, err = globalIAMSys.RemoveUsersFromGroup(ctx, updReq.Group, updReq.Members)\n-\t} else {\n-\t\t// Check if group already exists\n-\t\tif _, gerr := globalIAMSys.GetGroupDescription(updReq.Group); gerr != nil {\n-\t\t\t// If group does not exist, then check if the group has beginning and end space characters\n-\t\t\t// we will reject such group names.\n-\t\t\tif errors.Is(gerr, errNoSuchGroup) && hasSpaceBE(updReq.Group) {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\tupdatedAt, err = globalIAMSys.AddUsersToGroup(ctx, updReq.Group, updReq.Members)\n-\t}\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemGroupInfo,\n-\t\tGroupInfo: &madmin.SRGroupInfo{\n-\t\t\tUpdateReq: updReq,\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.AddUserToGroupAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        data, err := io.ReadAll(r.Body)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        var updReq madmin.GroupAddRemove\n+        err = json.Unmarshal(data, &updReq)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        // Reject if the group add and remove are temporary credentials, or root credential.\n+        for _, member := range updReq.Members {\n+                ok, _, err := globalIAMSys.IsTempUser(member)\n+                if err != nil && err != errNoSuchUser {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                if ok {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                        return\n+                }\n+                // When the user is root credential you are not allowed to\n+                // add policies for root user.\n+                if member == globalActiveCred.AccessKey {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                        return\n+                }\n+        }\n+\n+        var updatedAt time.Time\n+        if updReq.IsRemove {\n+                updatedAt, err = globalIAMSys.RemoveUsersFromGroup(ctx, updReq.Group, updReq.Members)\n+        } else {\n+                // Check if group already exists\n+                if _, gerr := globalIAMSys.GetGroupDescription(updReq.Group); gerr != nil {\n+                        // If group does not exist, then check if the group has beginning and end space characters\n+                        // we will reject such group names.\n+                        if errors.Is(gerr, errNoSuchGroup) && hasSpaceBE(updReq.Group) {\n+                                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                                return\n+                        }\n+                }\n+                updatedAt, err = globalIAMSys.AddUsersToGroup(ctx, updReq.Group, updReq.Members)\n+        }\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemGroupInfo,\n+                GroupInfo: &madmin.SRGroupInfo{\n+                        UpdateReq: updReq,\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // GetGroup - /minio/admin/v3/group?group=mygroup1\n func (a adminAPIHandlers) GetGroup(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n+        ctx := r.Context()\n \n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.GetGroupAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.GetGroupAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n \n-\tvars := mux.Vars(r)\n-\tgroup := vars[\"group\"]\n+        vars := mux.Vars(r)\n+        group := vars[\"group\"]\n \n-\tgdesc, err := globalIAMSys.GetGroupDescription(group)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        gdesc, err := globalIAMSys.GetGroupDescription(group)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\tbody, err := json.Marshal(gdesc)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        body, err := json.Marshal(gdesc)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\twriteSuccessResponseJSON(w, body)\n+        writeSuccessResponseJSON(w, body)\n }\n \n // ListGroups - GET /minio/admin/v3/groups\n func (a adminAPIHandlers) ListGroups(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.ListGroupsAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tgroups, err := globalIAMSys.ListGroups(ctx)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tbody, err := json.Marshal(groups)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, body)\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.ListGroupsAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        groups, err := globalIAMSys.ListGroups(ctx)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        body, err := json.Marshal(groups)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, body)\n }\n \n // SetGroupStatus - PUT /minio/admin/v3/set-group-status?group=mygroup1&status=enabled\n func (a adminAPIHandlers) SetGroupStatus(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.EnableGroupAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\tgroup := vars[\"group\"]\n-\tstatus := vars[\"status\"]\n-\n-\tvar (\n-\t\terr       error\n-\t\tupdatedAt time.Time\n-\t)\n-\tswitch status {\n-\tcase statusEnabled:\n-\t\tupdatedAt, err = globalIAMSys.SetGroupStatus(ctx, group, true)\n-\tcase statusDisabled:\n-\t\tupdatedAt, err = globalIAMSys.SetGroupStatus(ctx, group, false)\n-\tdefault:\n-\t\terr = errInvalidArgument\n-\t}\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemGroupInfo,\n-\t\tGroupInfo: &madmin.SRGroupInfo{\n-\t\t\tUpdateReq: madmin.GroupAddRemove{\n-\t\t\t\tGroup:    group,\n-\t\t\t\tStatus:   madmin.GroupStatus(status),\n-\t\t\t\tIsRemove: false,\n-\t\t\t},\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.EnableGroupAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        group := vars[\"group\"]\n+        status := vars[\"status\"]\n+\n+        var (\n+                err       error\n+                updatedAt time.Time\n+        )\n+        switch status {\n+        case statusEnabled:\n+                updatedAt, err = globalIAMSys.SetGroupStatus(ctx, group, true)\n+        case statusDisabled:\n+                updatedAt, err = globalIAMSys.SetGroupStatus(ctx, group, false)\n+        default:\n+                err = errInvalidArgument\n+        }\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemGroupInfo,\n+                GroupInfo: &madmin.SRGroupInfo{\n+                        UpdateReq: madmin.GroupAddRemove{\n+                                Group:    group,\n+                                Status:   madmin.GroupStatus(status),\n+                                IsRemove: false,\n+                        },\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // SetUserStatus - PUT /minio/admin/v3/set-user-status?accessKey=<access_key>&status=[enabled|disabled]\n func (a adminAPIHandlers) SetUserStatus(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, creds := validateAdminReq(ctx, w, r, policy.EnableUserAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\taccessKey := vars[\"accessKey\"]\n-\tstatus := vars[\"status\"]\n-\n-\t// you cannot enable or disable yourself.\n-\tif accessKey == creds.AccessKey {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tupdatedAt, err := globalIAMSys.SetUserStatus(ctx, accessKey, madmin.AccountStatus(status))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemIAMUser,\n-\t\tIAMUser: &madmin.SRIAMUser{\n-\t\t\tAccessKey:   accessKey,\n-\t\t\tIsDeleteReq: false,\n-\t\t\tUserReq: &madmin.AddOrUpdateUserReq{\n-\t\t\t\tStatus: madmin.AccountStatus(status),\n-\t\t\t},\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, creds := validateAdminReq(ctx, w, r, policy.EnableUserAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        accessKey := vars[\"accessKey\"]\n+        status := vars[\"status\"]\n+\n+        // you cannot enable or disable yourself.\n+        if accessKey == creds.AccessKey {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        updatedAt, err := globalIAMSys.SetUserStatus(ctx, accessKey, madmin.AccountStatus(status))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemIAMUser,\n+                IAMUser: &madmin.SRIAMUser{\n+                        AccessKey:   accessKey,\n+                        IsDeleteReq: false,\n+                        UserReq: &madmin.AddOrUpdateUserReq{\n+                                Status: madmin.AccountStatus(status),\n+                        },\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // AddUser - PUT /minio/admin/v3/add-user?accessKey=<access_key>\n func (a adminAPIHandlers) AddUser(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tvars := mux.Vars(r)\n-\taccessKey := vars[\"accessKey\"]\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Not allowed to add a user with same access key as root credential\n-\tif accessKey == globalActiveCred.AccessKey {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tuser, exists := globalIAMSys.GetUser(ctx, accessKey)\n-\tif exists && (user.Credentials.IsTemp() || user.Credentials.IsServiceAccount()) {\n-\t\t// Updating STS credential is not allowed, and this API does not\n-\t\t// support updating service accounts.\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif (cred.IsTemp() || cred.IsServiceAccount()) && cred.ParentUser == accessKey {\n-\t\t// Incoming access key matches parent user then we should\n-\t\t// reject password change requests.\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Check if accessKey has beginning and end space characters, this only applies to new users.\n-\tif !exists && hasSpaceBE(accessKey) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcheckDenyOnly := false\n-\tif accessKey == cred.AccessKey {\n-\t\t// Check that there is no explicit deny - otherwise it's allowed\n-\t\t// to change one's own password.\n-\t\tcheckDenyOnly = true\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.CreateUserAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t\tDenyOnly:        checkDenyOnly,\n-\t}) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif r.ContentLength > maxEConfigJSONSize || r.ContentLength == -1 {\n-\t\t// More than maxConfigSize bytes were available\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigTooLarge), r.URL)\n-\t\treturn\n-\t}\n-\n-\tpassword := cred.SecretKey\n-\tconfigBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\tlogger.LogIf(ctx, err)\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigBadJSON), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar ureq madmin.AddOrUpdateUserReq\n-\tif err = json.Unmarshal(configBytes, &ureq); err != nil {\n-\t\tlogger.LogIf(ctx, err)\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigBadJSON), r.URL)\n-\t\treturn\n-\t}\n-\n-\tupdatedAt, err := globalIAMSys.CreateUser(ctx, accessKey, ureq)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemIAMUser,\n-\t\tIAMUser: &madmin.SRIAMUser{\n-\t\t\tAccessKey:   accessKey,\n-\t\t\tIsDeleteReq: false,\n-\t\t\tUserReq:     &ureq,\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        vars := mux.Vars(r)\n+        accessKey := vars[\"accessKey\"]\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        // Not allowed to add a user with same access key as root credential\n+        if accessKey == globalActiveCred.AccessKey {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        user, exists := globalIAMSys.GetUser(ctx, accessKey)\n+        if exists && (user.Credentials.IsTemp() || user.Credentials.IsServiceAccount()) {\n+                // Updating STS credential is not allowed, and this API does not\n+                // support updating service accounts.\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        if (cred.IsTemp() || cred.IsServiceAccount()) && cred.ParentUser == accessKey {\n+                // Incoming access key matches parent user then we should\n+                // reject password change requests.\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        // Check if accessKey has beginning and end space characters, this only applies to new users.\n+        if !exists && hasSpaceBE(accessKey) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        checkDenyOnly := false\n+        if accessKey == cred.AccessKey {\n+                // Check that there is no explicit deny - otherwise it's allowed\n+                // to change one's own password.\n+                checkDenyOnly = true\n+        }\n+\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.CreateUserAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+                DenyOnly:        checkDenyOnly,\n+        }) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                return\n+        }\n+\n+        if r.ContentLength > maxEConfigJSONSize || r.ContentLength == -1 {\n+                // More than maxConfigSize bytes were available\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigTooLarge), r.URL)\n+                return\n+        }\n+\n+        password := cred.SecretKey\n+        configBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                logger.LogIf(ctx, err)\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigBadJSON), r.URL)\n+                return\n+        }\n+\n+        var ureq madmin.AddOrUpdateUserReq\n+        if err = json.Unmarshal(configBytes, &ureq); err != nil {\n+                logger.LogIf(ctx, err)\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigBadJSON), r.URL)\n+                return\n+        }\n+\n+        updatedAt, err := globalIAMSys.CreateUser(ctx, accessKey, ureq)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemIAMUser,\n+                IAMUser: &madmin.SRIAMUser{\n+                        AccessKey:   accessKey,\n+                        IsDeleteReq: false,\n+                        UserReq:     &ureq,\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // TemporaryAccountInfo - GET /minio/admin/v3/temporary-account-info\n func (a adminAPIHandlers) TemporaryAccountInfo(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\taccessKey := mux.Vars(r)[\"accessKey\"]\n-\tif accessKey == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\targs := policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.ListTemporaryAccountsAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(args) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\treturn\n-\t}\n-\n-\tstsAccount, sessionPolicy, err := globalIAMSys.GetTemporaryAccount(ctx, accessKey)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar stsAccountPolicy policy.Policy\n-\n-\tif sessionPolicy != nil {\n-\t\tstsAccountPolicy = *sessionPolicy\n-\t} else {\n-\t\tpoliciesNames, err := globalIAMSys.PolicyDBGet(stsAccount.ParentUser, stsAccount.Groups...)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tif len(policiesNames) == 0 {\n-\t\t\tpolicySet, _ := args.GetPolicies(iamPolicyClaimNameOpenID())\n-\t\t\tpoliciesNames = policySet.ToSlice()\n-\t\t}\n-\n-\t\tstsAccountPolicy = globalIAMSys.GetCombinedPolicy(policiesNames...)\n-\t}\n-\n-\tpolicyJSON, err := json.MarshalIndent(stsAccountPolicy, \"\", \" \")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tinfoResp := madmin.TemporaryAccountInfoResp{\n-\t\tParentUser:    stsAccount.ParentUser,\n-\t\tAccountStatus: stsAccount.Status,\n-\t\tImpliedPolicy: sessionPolicy == nil,\n-\t\tPolicy:        string(policyJSON),\n-\t\tExpiration:    &stsAccount.Expiration,\n-\t}\n-\n-\tdata, err := json.Marshal(infoResp)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        accessKey := mux.Vars(r)[\"accessKey\"]\n+        if accessKey == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        args := policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.ListTemporaryAccountsAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+        }\n+\n+        if !globalIAMSys.IsAllowed(args) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                return\n+        }\n+\n+        stsAccount, sessionPolicy, err := globalIAMSys.GetTemporaryAccount(ctx, accessKey)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        var stsAccountPolicy policy.Policy\n+\n+        if sessionPolicy != nil {\n+                stsAccountPolicy = *sessionPolicy\n+        } else {\n+                policiesNames, err := globalIAMSys.PolicyDBGet(stsAccount.ParentUser, stsAccount.Groups...)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                if len(policiesNames) == 0 {\n+                        policySet, _ := args.GetPolicies(iamPolicyClaimNameOpenID())\n+                        policiesNames = policySet.ToSlice()\n+                }\n+\n+                stsAccountPolicy = globalIAMSys.GetCombinedPolicy(policiesNames...)\n+        }\n+\n+        policyJSON, err := json.MarshalIndent(stsAccountPolicy, \"\", \" \")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        infoResp := madmin.TemporaryAccountInfoResp{\n+                ParentUser:    stsAccount.ParentUser,\n+                AccountStatus: stsAccount.Status,\n+                ImpliedPolicy: sessionPolicy == nil,\n+                Policy:        string(policyJSON),\n+                Expiration:    &stsAccount.Expiration,\n+        }\n+\n+        data, err := json.Marshal(infoResp)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n }\n \n // AddServiceAccount - PUT /minio/admin/v3/add-service-account\n func (a adminAPIHandlers) AddServiceAccount(w http.ResponseWriter, r *http.Request) {\n-\tctx, cred, opts, createReq, targetUser, APIError := commonAddServiceAccount(r)\n-\tif APIError.Code != \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, APIError, r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar (\n-\t\ttargetGroups []string\n-\t\terr          error\n-\t)\n-\n-\t// Find the user for the request sender (as it may be sent via a service\n-\t// account or STS account):\n-\trequestorUser := cred.AccessKey\n-\trequestorParentUser := cred.AccessKey\n-\trequestorGroups := cred.Groups\n-\trequestorIsDerivedCredential := false\n-\tif cred.IsServiceAccount() || cred.IsTemp() {\n-\t\trequestorParentUser = cred.ParentUser\n-\t\trequestorIsDerivedCredential = true\n-\t}\n-\n-\tif globalIAMSys.GetUsersSysType() == MinIOUsersSysType && targetUser != cred.AccessKey {\n-\t\t// For internal IDP, ensure that the targetUser's parent account exists.\n-\t\t// It could be a regular user account or the root account.\n-\t\t_, isRegularUser := globalIAMSys.GetUser(ctx, targetUser)\n-\t\tif !isRegularUser && targetUser != globalActiveCred.AccessKey {\n-\t\t\tapiErr := toAdminAPIErr(ctx, errNoSuchUser)\n-\t\t\tapiErr.Description = fmt.Sprintf(\"Specified target user %s does not exist\", targetUser)\n-\t\t\twriteErrorResponseJSON(ctx, w, apiErr, r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\t// Check if we are creating svc account for request sender.\n-\tisSvcAccForRequestor := false\n-\tif targetUser == requestorUser || targetUser == requestorParentUser {\n-\t\tisSvcAccForRequestor = true\n-\t}\n-\n-\t// If we are creating svc account for request sender, ensure\n-\t// that targetUser is a real user (i.e. not derived\n-\t// credentials).\n-\tif isSvcAccForRequestor {\n-\t\tif requestorIsDerivedCredential {\n-\t\t\tif requestorParentUser == \"\" {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx,\n-\t\t\t\t\terrors.New(\"service accounts cannot be generated for temporary credentials without parent\")), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\ttargetUser = requestorParentUser\n-\t\t}\n-\t\ttargetGroups = requestorGroups\n-\n-\t\t// In case of LDAP/OIDC we need to set `opts.claims` to ensure\n-\t\t// it is associated with the LDAP/OIDC user properly.\n-\t\tfor k, v := range cred.Claims {\n-\t\t\tif k == expClaim {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\topts.claims[k] = v\n-\t\t}\n-\t} else if globalIAMSys.LDAPConfig.Enabled() {\n-\t\t// In case of LDAP we need to resolve the targetUser to a DN and\n-\t\t// query their groups:\n-\t\topts.claims[ldapUserN] = targetUser // simple username\n-\t\ttargetUser, targetGroups, err = globalIAMSys.LDAPConfig.LookupUserDN(targetUser)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\topts.claims[ldapUser] = targetUser // username DN\n-\n-\t\t// NOTE: if not using LDAP, then internal IDP or open ID is\n-\t\t// being used - in the former, group info is enforced when\n-\t\t// generated credentials are used to make requests, and in the\n-\t\t// latter, a group notion is not supported.\n-\t}\n-\n-\tnewCred, updatedAt, err := globalIAMSys.NewServiceAccount(ctx, targetUser, targetGroups, opts)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcreateResp := madmin.AddServiceAccountResp{\n-\t\tCredentials: madmin.Credentials{\n-\t\t\tAccessKey:  newCred.AccessKey,\n-\t\t\tSecretKey:  newCred.SecretKey,\n-\t\t\tExpiration: newCred.Expiration,\n-\t\t},\n-\t}\n-\n-\tdata, err := json.Marshal(createResp)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n-\n-\t// Call hook for cluster-replication if the service account is not for a\n-\t// root user.\n-\tif newCred.ParentUser != globalActiveCred.AccessKey {\n-\t\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\t\tType: madmin.SRIAMItemSvcAcc,\n-\t\t\tSvcAccChange: &madmin.SRSvcAccChange{\n-\t\t\t\tCreate: &madmin.SRSvcAccCreate{\n-\t\t\t\t\tParent:        newCred.ParentUser,\n-\t\t\t\t\tAccessKey:     newCred.AccessKey,\n-\t\t\t\t\tSecretKey:     newCred.SecretKey,\n-\t\t\t\t\tGroups:        newCred.Groups,\n-\t\t\t\t\tName:          newCred.Name,\n-\t\t\t\t\tDescription:   newCred.Description,\n-\t\t\t\t\tClaims:        opts.claims,\n-\t\t\t\t\tSessionPolicy: createReq.Policy,\n-\t\t\t\t\tStatus:        auth.AccountOn,\n-\t\t\t\t\tExpiration:    createReq.Expiration,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tUpdatedAt: updatedAt,\n-\t\t}))\n-\t}\n+        ctx, cred, opts, createReq, targetUser, APIError := commonAddServiceAccount(r)\n+        if APIError.Code != \"\" {\n+                writeErrorResponseJSON(ctx, w, APIError, r.URL)\n+                return\n+        }\n+\n+        var (\n+                targetGroups []string\n+                err          error\n+        )\n+\n+        // Find the user for the request sender (as it may be sent via a service\n+        // account or STS account):\n+        requestorUser := cred.AccessKey\n+        requestorParentUser := cred.AccessKey\n+        requestorGroups := cred.Groups\n+        requestorIsDerivedCredential := false\n+        if cred.IsServiceAccount() || cred.IsTemp() {\n+                requestorParentUser = cred.ParentUser\n+                requestorIsDerivedCredential = true\n+        }\n+\n+        if globalIAMSys.GetUsersSysType() == MinIOUsersSysType && targetUser != cred.AccessKey {\n+                // For internal IDP, ensure that the targetUser's parent account exists.\n+                // It could be a regular user account or the root account.\n+                _, isRegularUser := globalIAMSys.GetUser(ctx, targetUser)\n+                if !isRegularUser && targetUser != globalActiveCred.AccessKey {\n+                        apiErr := toAdminAPIErr(ctx, errNoSuchUser)\n+                        apiErr.Description = fmt.Sprintf(\"Specified target user %s does not exist\", targetUser)\n+                        writeErrorResponseJSON(ctx, w, apiErr, r.URL)\n+                        return\n+                }\n+        }\n+\n+        // Check if we are creating svc account for request sender.\n+        isSvcAccForRequestor := false\n+        if targetUser == requestorUser || targetUser == requestorParentUser {\n+                isSvcAccForRequestor = true\n+        }\n+\n+        // If we are creating svc account for request sender, ensure\n+        // that targetUser is a real user (i.e. not derived\n+        // credentials).\n+        if isSvcAccForRequestor {\n+                if requestorIsDerivedCredential {\n+                        if requestorParentUser == \"\" {\n+                                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx,\n+                                        errors.New(\"service accounts cannot be generated for temporary credentials without parent\")), r.URL)\n+                                return\n+                        }\n+                        targetUser = requestorParentUser\n+                }\n+                targetGroups = requestorGroups\n+\n+                // In case of LDAP/OIDC we need to set `opts.claims` to ensure\n+                // it is associated with the LDAP/OIDC user properly.\n+                for k, v := range cred.Claims {\n+                        if k == expClaim {\n+                                continue\n+                        }\n+                        opts.claims[k] = v\n+                }\n+        } else if globalIAMSys.LDAPConfig.Enabled() {\n+                // In case of LDAP we need to resolve the targetUser to a DN and\n+                // query their groups:\n+                opts.claims[ldapUserN] = targetUser // simple username\n+                targetUser, targetGroups, err = globalIAMSys.LDAPConfig.LookupUserDN(targetUser)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                opts.claims[ldapUser] = targetUser // username DN\n+\n+                // NOTE: if not using LDAP, then internal IDP or open ID is\n+                // being used - in the former, group info is enforced when\n+                // generated credentials are used to make requests, and in the\n+                // latter, a group notion is not supported.\n+        }\n+\n+        newCred, updatedAt, err := globalIAMSys.NewServiceAccount(ctx, targetUser, targetGroups, opts)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        createResp := madmin.AddServiceAccountResp{\n+                Credentials: madmin.Credentials{\n+                        AccessKey:  newCred.AccessKey,\n+                        SecretKey:  newCred.SecretKey,\n+                        Expiration: newCred.Expiration,\n+                },\n+        }\n+\n+        data, err := json.Marshal(createResp)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n+\n+        // Call hook for cluster-replication if the service account is not for a\n+        // root user.\n+        if newCred.ParentUser != globalActiveCred.AccessKey {\n+                logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                        Type: madmin.SRIAMItemSvcAcc,\n+                        SvcAccChange: &madmin.SRSvcAccChange{\n+                                Create: &madmin.SRSvcAccCreate{\n+                                        Parent:        newCred.ParentUser,\n+                                        AccessKey:     newCred.AccessKey,\n+                                        SecretKey:     newCred.SecretKey,\n+                                        Groups:        newCred.Groups,\n+                                        Name:          newCred.Name,\n+                                        Description:   newCred.Description,\n+                                        Claims:        opts.claims,\n+                                        SessionPolicy: createReq.Policy,\n+                                        Status:        auth.AccountOn,\n+                                        Expiration:    createReq.Expiration,\n+                                },\n+                        },\n+                        UpdatedAt: updatedAt,\n+                }))\n+        }\n }\n \n // UpdateServiceAccount - POST /minio/admin/v3/update-service-account\n func (a adminAPIHandlers) UpdateServiceAccount(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\taccessKey := mux.Vars(r)[\"accessKey\"]\n-\tif accessKey == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\tsvcAccount, _, err := globalIAMSys.GetServiceAccount(ctx, accessKey)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.UpdateServiceAccountAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t}) {\n-\t\trequestUser := cred.AccessKey\n-\t\tif cred.ParentUser != \"\" {\n-\t\t\trequestUser = cred.ParentUser\n-\t\t}\n-\n-\t\tif requestUser != svcAccount.ParentUser {\n-\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tpassword := cred.SecretKey\n-\treqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar updateReq madmin.UpdateServiceAccountReq\n-\tif err = json.Unmarshal(reqBytes, &updateReq); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif err := updateReq.Validate(); err != nil {\n-\t\t// Since this validation would happen client side as well, we only send\n-\t\t// a generic error message here.\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar sp *policy.Policy\n-\tif len(updateReq.NewPolicy) > 0 {\n-\t\tsp, err = policy.ParseConfig(bytes.NewReader(updateReq.NewPolicy))\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tif sp.Version == \"\" && len(sp.Statements) == 0 {\n-\t\t\tsp = nil\n-\t\t}\n-\t}\n-\topts := updateServiceAccountOpts{\n-\t\tsecretKey:     updateReq.NewSecretKey,\n-\t\tstatus:        updateReq.NewStatus,\n-\t\tname:          updateReq.NewName,\n-\t\tdescription:   updateReq.NewDescription,\n-\t\texpiration:    updateReq.NewExpiration,\n-\t\tsessionPolicy: sp,\n-\t}\n-\tupdatedAt, err := globalIAMSys.UpdateServiceAccount(ctx, accessKey, opts)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Call site replication hook - non-root user accounts are replicated.\n-\tif svcAccount.ParentUser != globalActiveCred.AccessKey {\n-\t\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\t\tType: madmin.SRIAMItemSvcAcc,\n-\t\t\tSvcAccChange: &madmin.SRSvcAccChange{\n-\t\t\t\tUpdate: &madmin.SRSvcAccUpdate{\n-\t\t\t\t\tAccessKey:     accessKey,\n-\t\t\t\t\tSecretKey:     opts.secretKey,\n-\t\t\t\t\tStatus:        opts.status,\n-\t\t\t\t\tName:          opts.name,\n-\t\t\t\t\tDescription:   opts.description,\n-\t\t\t\t\tSessionPolicy: updateReq.NewPolicy,\n-\t\t\t\t\tExpiration:    updateReq.NewExpiration,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tUpdatedAt: updatedAt,\n-\t\t}))\n-\t}\n-\n-\twriteSuccessNoContent(w)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        accessKey := mux.Vars(r)[\"accessKey\"]\n+        if accessKey == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        svcAccount, _, err := globalIAMSys.GetServiceAccount(ctx, accessKey)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.UpdateServiceAccountAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+        }) {\n+                requestUser := cred.AccessKey\n+                if cred.ParentUser != \"\" {\n+                        requestUser = cred.ParentUser\n+                }\n+\n+                if requestUser != svcAccount.ParentUser {\n+                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                        return\n+                }\n+\n+// Do not allow service account to update itself.\n+if cred.AccessKey == accessKey {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+        }\n+\n+        password := cred.SecretKey\n+        reqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err), r.URL)\n+                return\n+        }\n+\n+        var updateReq madmin.UpdateServiceAccountReq\n+        if err = json.Unmarshal(reqBytes, &updateReq); err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err), r.URL)\n+                return\n+        }\n+\n+        if err := updateReq.Validate(); err != nil {\n+                // Since this validation would happen client side as well, we only send\n+                // a generic error message here.\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        var sp *policy.Policy\n+        if len(updateReq.NewPolicy) > 0 {\n+                sp, err = policy.ParseConfig(bytes.NewReader(updateReq.NewPolicy))\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                if sp.Version == \"\" && len(sp.Statements) == 0 {\n+                        sp = nil\n+                }\n+        }\n+        opts := updateServiceAccountOpts{\n+                secretKey:     updateReq.NewSecretKey,\n+                status:        updateReq.NewStatus,\n+                name:          updateReq.NewName,\n+                description:   updateReq.NewDescription,\n+                expiration:    updateReq.NewExpiration,\n+                sessionPolicy: sp,\n+        }\n+        updatedAt, err := globalIAMSys.UpdateServiceAccount(ctx, accessKey, opts)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Call site replication hook - non-root user accounts are replicated.\n+        if svcAccount.ParentUser != globalActiveCred.AccessKey {\n+                logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                        Type: madmin.SRIAMItemSvcAcc,\n+                        SvcAccChange: &madmin.SRSvcAccChange{\n+                                Update: &madmin.SRSvcAccUpdate{\n+                                        AccessKey:     accessKey,\n+                                        SecretKey:     opts.secretKey,\n+                                        Status:        opts.status,\n+                                        Name:          opts.name,\n+                                        Description:   opts.description,\n+                                        SessionPolicy: updateReq.NewPolicy,\n+                                        Expiration:    updateReq.NewExpiration,\n+                                },\n+                        },\n+                        UpdatedAt: updatedAt,\n+                }))\n+        }\n+\n+        writeSuccessNoContent(w)\n }\n \n // InfoServiceAccount - GET /minio/admin/v3/info-service-account\n func (a adminAPIHandlers) InfoServiceAccount(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\taccessKey := mux.Vars(r)[\"accessKey\"]\n-\tif accessKey == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\tsvcAccount, sessionPolicy, err := globalIAMSys.GetServiceAccount(ctx, accessKey)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.ListServiceAccountsAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t}) {\n-\t\trequestUser := cred.AccessKey\n-\t\tif cred.ParentUser != \"\" {\n-\t\t\trequestUser = cred.ParentUser\n-\t\t}\n-\n-\t\tif requestUser != svcAccount.ParentUser {\n-\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\t// if session policy is nil or empty, then it is implied policy\n-\timpliedPolicy := sessionPolicy == nil || (sessionPolicy.Version == \"\" && len(sessionPolicy.Statements) == 0)\n-\n-\tvar svcAccountPolicy policy.Policy\n-\n-\tif !impliedPolicy {\n-\t\tsvcAccountPolicy = *sessionPolicy\n-\t} else {\n-\t\tpoliciesNames, err := globalIAMSys.PolicyDBGet(svcAccount.ParentUser, svcAccount.Groups...)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tsvcAccountPolicy = globalIAMSys.GetCombinedPolicy(policiesNames...)\n-\t}\n-\n-\tpolicyJSON, err := json.MarshalIndent(svcAccountPolicy, \"\", \" \")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar expiration *time.Time\n-\tif !svcAccount.Expiration.IsZero() && !svcAccount.Expiration.Equal(timeSentinel) {\n-\t\texpiration = &svcAccount.Expiration\n-\t}\n-\n-\tinfoResp := madmin.InfoServiceAccountResp{\n-\t\tParentUser:    svcAccount.ParentUser,\n-\t\tName:          svcAccount.Name,\n-\t\tDescription:   svcAccount.Description,\n-\t\tAccountStatus: svcAccount.Status,\n-\t\tImpliedPolicy: impliedPolicy,\n-\t\tPolicy:        string(policyJSON),\n-\t\tExpiration:    expiration,\n-\t}\n-\n-\tdata, err := json.Marshal(infoResp)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        accessKey := mux.Vars(r)[\"accessKey\"]\n+        if accessKey == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        svcAccount, sessionPolicy, err := globalIAMSys.GetServiceAccount(ctx, accessKey)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.ListServiceAccountsAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+        }) {\n+                requestUser := cred.AccessKey\n+                if cred.ParentUser != \"\" {\n+                        requestUser = cred.ParentUser\n+                }\n+\n+                if requestUser != svcAccount.ParentUser {\n+                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                        return\n+                }\n+        }\n+\n+        // if session policy is nil or empty, then it is implied policy\n+        impliedPolicy := sessionPolicy == nil || (sessionPolicy.Version == \"\" && len(sessionPolicy.Statements) == 0)\n+\n+        var svcAccountPolicy policy.Policy\n+\n+        if !impliedPolicy {\n+                svcAccountPolicy = *sessionPolicy\n+        } else {\n+                policiesNames, err := globalIAMSys.PolicyDBGet(svcAccount.ParentUser, svcAccount.Groups...)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                svcAccountPolicy = globalIAMSys.GetCombinedPolicy(policiesNames...)\n+        }\n+\n+        policyJSON, err := json.MarshalIndent(svcAccountPolicy, \"\", \" \")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        var expiration *time.Time\n+        if !svcAccount.Expiration.IsZero() && !svcAccount.Expiration.Equal(timeSentinel) {\n+                expiration = &svcAccount.Expiration\n+        }\n+\n+        infoResp := madmin.InfoServiceAccountResp{\n+                ParentUser:    svcAccount.ParentUser,\n+                Name:          svcAccount.Name,\n+                Description:   svcAccount.Description,\n+                AccountStatus: svcAccount.Status,\n+                ImpliedPolicy: impliedPolicy,\n+                Policy:        string(policyJSON),\n+                Expiration:    expiration,\n+        }\n+\n+        data, err := json.Marshal(infoResp)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n }\n \n // ListServiceAccounts - GET /minio/admin/v3/list-service-accounts\n func (a adminAPIHandlers) ListServiceAccounts(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar targetAccount string\n-\n-\t// If listing is requested for a specific user (who is not the request\n-\t// sender), check that the user has permissions.\n-\tuser := r.Form.Get(\"user\")\n-\tif user != \"\" && user != cred.AccessKey {\n-\t\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\t\tAccountName:     cred.AccessKey,\n-\t\t\tGroups:          cred.Groups,\n-\t\t\tAction:          policy.ListServiceAccountsAdminAction,\n-\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\tIsOwner:         owner,\n-\t\t\tClaims:          cred.Claims,\n-\t\t}) {\n-\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\ttargetAccount = user\n-\t} else {\n-\t\ttargetAccount = cred.AccessKey\n-\t\tif cred.ParentUser != \"\" {\n-\t\t\ttargetAccount = cred.ParentUser\n-\t\t}\n-\t}\n-\n-\tserviceAccounts, err := globalIAMSys.ListServiceAccounts(ctx, targetAccount)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar serviceAccountList []madmin.ServiceAccountInfo\n-\n-\tfor _, svc := range serviceAccounts {\n-\t\texpiryTime := svc.Expiration\n-\t\tserviceAccountList = append(serviceAccountList, madmin.ServiceAccountInfo{\n-\t\t\tAccessKey:  svc.AccessKey,\n-\t\t\tExpiration: &expiryTime,\n-\t\t})\n-\t}\n-\n-\tlistResp := madmin.ListServiceAccountsResp{\n-\t\tAccounts: serviceAccountList,\n-\t}\n-\n-\tdata, err := json.Marshal(listResp)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        var targetAccount string\n+\n+        // If listing is requested for a specific user (who is not the request\n+        // sender), check that the user has permissions.\n+        user := r.Form.Get(\"user\")\n+        if user != \"\" && user != cred.AccessKey {\n+                if !globalIAMSys.IsAllowed(policy.Args{\n+                        AccountName:     cred.AccessKey,\n+                        Groups:          cred.Groups,\n+                        Action:          policy.ListServiceAccountsAdminAction,\n+                        ConditionValues: getConditionValues(r, \"\", cred),\n+                        IsOwner:         owner,\n+                        Claims:          cred.Claims,\n+                }) {\n+                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                        return\n+                }\n+                targetAccount = user\n+        } else {\n+                targetAccount = cred.AccessKey\n+                if cred.ParentUser != \"\" {\n+                        targetAccount = cred.ParentUser\n+                }\n+        }\n+\n+        serviceAccounts, err := globalIAMSys.ListServiceAccounts(ctx, targetAccount)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        var serviceAccountList []madmin.ServiceAccountInfo\n+\n+        for _, svc := range serviceAccounts {\n+                expiryTime := svc.Expiration\n+                serviceAccountList = append(serviceAccountList, madmin.ServiceAccountInfo{\n+                        AccessKey:  svc.AccessKey,\n+                        Expiration: &expiryTime,\n+                })\n+        }\n+\n+        listResp := madmin.ListServiceAccountsResp{\n+                Accounts: serviceAccountList,\n+        }\n+\n+        data, err := json.Marshal(listResp)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n }\n \n // DeleteServiceAccount - DELETE /minio/admin/v3/delete-service-account\n func (a adminAPIHandlers) DeleteServiceAccount(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tserviceAccount := mux.Vars(r)[\"accessKey\"]\n-\tif serviceAccount == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// We do not care if service account is readable or not at this point,\n-\t// since this is a delete call we shall allow it to be deleted if possible.\n-\tsvcAccount, _, err := globalIAMSys.GetServiceAccount(ctx, serviceAccount)\n-\tif errors.Is(err, errNoSuchServiceAccount) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminServiceAccountNotFound), r.URL)\n-\t\treturn\n-\t}\n-\n-\tadminPrivilege := globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.RemoveServiceAccountAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t})\n-\n-\tif !adminPrivilege {\n-\t\tparentUser := cred.AccessKey\n-\t\tif cred.ParentUser != \"\" {\n-\t\t\tparentUser = cred.ParentUser\n-\t\t}\n-\t\tif svcAccount.ParentUser != \"\" && parentUser != svcAccount.ParentUser {\n-\t\t\t// The service account belongs to another user but return not\n-\t\t\t// found error to mitigate brute force attacks. or the\n-\t\t\t// serviceAccount doesn't exist.\n-\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminServiceAccountNotFound), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tif err := globalIAMSys.DeleteServiceAccount(ctx, serviceAccount, true); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Call site replication hook - non-root user accounts are replicated.\n-\tif svcAccount.ParentUser != \"\" && svcAccount.ParentUser != globalActiveCred.AccessKey {\n-\t\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\t\tType: madmin.SRIAMItemSvcAcc,\n-\t\t\tSvcAccChange: &madmin.SRSvcAccChange{\n-\t\t\t\tDelete: &madmin.SRSvcAccDelete{\n-\t\t\t\t\tAccessKey: serviceAccount,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tUpdatedAt: UTCNow(),\n-\t\t}))\n-\t}\n-\n-\twriteSuccessNoContent(w)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        serviceAccount := mux.Vars(r)[\"accessKey\"]\n+        if serviceAccount == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        // We do not care if service account is readable or not at this point,\n+        // since this is a delete call we shall allow it to be deleted if possible.\n+        svcAccount, _, err := globalIAMSys.GetServiceAccount(ctx, serviceAccount)\n+        if errors.Is(err, errNoSuchServiceAccount) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminServiceAccountNotFound), r.URL)\n+                return\n+        }\n+\n+        adminPrivilege := globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.RemoveServiceAccountAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+        })\n+\n+        if !adminPrivilege {\n+                parentUser := cred.AccessKey\n+                if cred.ParentUser != \"\" {\n+                        parentUser = cred.ParentUser\n+                }\n+                if svcAccount.ParentUser != \"\" && parentUser != svcAccount.ParentUser {\n+                        // The service account belongs to another user but return not\n+                        // found error to mitigate brute force attacks. or the\n+                        // serviceAccount doesn't exist.\n+                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminServiceAccountNotFound), r.URL)\n+                        return\n+                }\n+        }\n+\n+        if err := globalIAMSys.DeleteServiceAccount(ctx, serviceAccount, true); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Call site replication hook - non-root user accounts are replicated.\n+        if svcAccount.ParentUser != \"\" && svcAccount.ParentUser != globalActiveCred.AccessKey {\n+                logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                        Type: madmin.SRIAMItemSvcAcc,\n+                        SvcAccChange: &madmin.SRSvcAccChange{\n+                                Delete: &madmin.SRSvcAccDelete{\n+                                        AccessKey: serviceAccount,\n+                                },\n+                        },\n+                        UpdatedAt: UTCNow(),\n+                }))\n+        }\n+\n+        writeSuccessNoContent(w)\n }\n \n // AccountInfoHandler returns usage, permissions and other bucket metadata for incoming us\n func (a adminAPIHandlers) AccountInfoHandler(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Set prefix value for \"s3:prefix\" policy conditionals.\n-\tr.Header.Set(\"prefix\", \"\")\n-\n-\t// Set delimiter value for \"s3:delimiter\" policy conditionals.\n-\tr.Header.Set(\"delimiter\", SlashSeparator)\n-\n-\t// Check if we are asked to return prefix usage\n-\tenablePrefixUsage := r.Form.Get(\"prefix-usage\") == \"true\"\n-\n-\tisAllowedAccess := func(bucketName string) (rd, wr bool) {\n-\t\tif globalIAMSys.IsAllowed(policy.Args{\n-\t\t\tAccountName:     cred.AccessKey,\n-\t\t\tGroups:          cred.Groups,\n-\t\t\tAction:          policy.ListBucketAction,\n-\t\t\tBucketName:      bucketName,\n-\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\tIsOwner:         owner,\n-\t\t\tObjectName:      \"\",\n-\t\t\tClaims:          cred.Claims,\n-\t\t}) {\n-\t\t\trd = true\n-\t\t}\n-\n-\t\tif globalIAMSys.IsAllowed(policy.Args{\n-\t\t\tAccountName:     cred.AccessKey,\n-\t\t\tGroups:          cred.Groups,\n-\t\t\tAction:          policy.GetBucketLocationAction,\n-\t\t\tBucketName:      bucketName,\n-\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\tIsOwner:         owner,\n-\t\t\tObjectName:      \"\",\n-\t\t\tClaims:          cred.Claims,\n-\t\t}) {\n-\t\t\trd = true\n-\t\t}\n-\n-\t\tif globalIAMSys.IsAllowed(policy.Args{\n-\t\t\tAccountName:     cred.AccessKey,\n-\t\t\tGroups:          cred.Groups,\n-\t\t\tAction:          policy.PutObjectAction,\n-\t\t\tBucketName:      bucketName,\n-\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\tIsOwner:         owner,\n-\t\t\tObjectName:      \"\",\n-\t\t\tClaims:          cred.Claims,\n-\t\t}) {\n-\t\t\twr = true\n-\t\t}\n-\n-\t\treturn rd, wr\n-\t}\n-\n-\tbucketStorageCache.Once.Do(func() {\n-\t\t// Set this to 10 secs since its enough, as scanner\n-\t\t// does not update the bucket usage values frequently.\n-\t\tbucketStorageCache.TTL = 10 * time.Second\n-\n-\t\t// Rely on older value if usage loading fails from disk.\n-\t\tbucketStorageCache.Relax = true\n-\t\tbucketStorageCache.Update = func() (interface{}, error) {\n-\t\t\tctx, done := context.WithTimeout(context.Background(), 2*time.Second)\n-\t\t\tdefer done()\n-\n-\t\t\treturn loadDataUsageFromBackend(ctx, objectAPI)\n-\t\t}\n-\t})\n-\n-\tvar dataUsageInfo DataUsageInfo\n-\tv, _ := bucketStorageCache.Get()\n-\tif v != nil {\n-\t\tdataUsageInfo, _ = v.(DataUsageInfo)\n-\t}\n-\n-\t// If etcd, dns federation configured list buckets from etcd.\n-\tvar err error\n-\tvar buckets []BucketInfo\n-\tif globalDNSConfig != nil && globalBucketFederation {\n-\t\tdnsBuckets, err := globalDNSConfig.List()\n-\t\tif err != nil && !IsErrIgnored(err,\n-\t\t\tdns.ErrNoEntriesFound,\n-\t\t\tdns.ErrDomainMissing) {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tfor _, dnsRecords := range dnsBuckets {\n-\t\t\tbuckets = append(buckets, BucketInfo{\n-\t\t\t\tName:    dnsRecords[0].Key,\n-\t\t\t\tCreated: dnsRecords[0].CreationDate,\n-\t\t\t})\n-\t\t}\n-\t\tsort.Slice(buckets, func(i, j int) bool {\n-\t\t\treturn buckets[i].Name < buckets[j].Name\n-\t\t})\n-\t} else {\n-\t\tbuckets, err = objectAPI.ListBuckets(ctx, BucketOptions{Cached: true})\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\taccountName := cred.AccessKey\n-\tif cred.IsTemp() || cred.IsServiceAccount() {\n-\t\t// For derived credentials, check the parent user's permissions.\n-\t\taccountName = cred.ParentUser\n-\t}\n-\n-\troleArn := policy.Args{Claims: cred.Claims}.GetRoleArn()\n-\tpolicySetFromClaims, hasPolicyClaim := policy.GetPoliciesFromClaims(cred.Claims, iamPolicyClaimNameOpenID())\n-\tvar effectivePolicy policy.Policy\n-\n-\tvar buf []byte\n-\tswitch {\n-\tcase accountName == globalActiveCred.AccessKey:\n-\t\tfor _, policy := range policy.DefaultPolicies {\n-\t\t\tif policy.Name == \"consoleAdmin\" {\n-\t\t\t\teffectivePolicy = policy.Definition\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\n-\tcase roleArn != \"\":\n-\t\t_, policy, err := globalIAMSys.GetRolePolicy(roleArn)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tpolicySlice := newMappedPolicy(policy).toSlice()\n-\t\teffectivePolicy = globalIAMSys.GetCombinedPolicy(policySlice...)\n-\n-\tcase hasPolicyClaim:\n-\t\teffectivePolicy = globalIAMSys.GetCombinedPolicy(policySetFromClaims.ToSlice()...)\n-\n-\tdefault:\n-\t\tpolicies, err := globalIAMSys.PolicyDBGet(accountName, cred.Groups...)\n-\t\tif err != nil {\n-\t\t\tlogger.LogIf(ctx, err)\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\teffectivePolicy = globalIAMSys.GetCombinedPolicy(policies...)\n-\n-\t}\n-\tbuf, err = json.MarshalIndent(effectivePolicy, \"\", \" \")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tacctInfo := madmin.AccountInfo{\n-\t\tAccountName: accountName,\n-\t\tServer:      objectAPI.BackendInfo(),\n-\t\tPolicy:      buf,\n-\t}\n-\n-\tfor _, bucket := range buckets {\n-\t\trd, wr := isAllowedAccess(bucket.Name)\n-\t\tif rd || wr {\n-\t\t\t// Fetch the data usage of the current bucket\n-\t\t\tvar size uint64\n-\t\t\tvar objectsCount uint64\n-\t\t\tvar objectsHist, versionsHist map[string]uint64\n-\t\t\tif !dataUsageInfo.LastUpdate.IsZero() {\n-\t\t\t\tsize = dataUsageInfo.BucketsUsage[bucket.Name].Size\n-\t\t\t\tobjectsCount = dataUsageInfo.BucketsUsage[bucket.Name].ObjectsCount\n-\t\t\t\tobjectsHist = dataUsageInfo.BucketsUsage[bucket.Name].ObjectSizesHistogram\n-\t\t\t\tversionsHist = dataUsageInfo.BucketsUsage[bucket.Name].ObjectVersionsHistogram\n-\t\t\t}\n-\t\t\t// Fetch the prefix usage of the current bucket\n-\t\t\tvar prefixUsage map[string]uint64\n-\t\t\tif enablePrefixUsage {\n-\t\t\t\tprefixUsage, _ = loadPrefixUsageFromBackend(ctx, objectAPI, bucket.Name)\n-\t\t\t}\n-\n-\t\t\tlcfg, _ := globalBucketObjectLockSys.Get(bucket.Name)\n-\t\t\tquota, _ := globalBucketQuotaSys.Get(ctx, bucket.Name)\n-\t\t\trcfg, _, _ := globalBucketMetadataSys.GetReplicationConfig(ctx, bucket.Name)\n-\t\t\ttcfg, _, _ := globalBucketMetadataSys.GetTaggingConfig(bucket.Name)\n-\n-\t\t\tacctInfo.Buckets = append(acctInfo.Buckets, madmin.BucketAccessInfo{\n-\t\t\t\tName:                    bucket.Name,\n-\t\t\t\tCreated:                 bucket.Created,\n-\t\t\t\tSize:                    size,\n-\t\t\t\tObjects:                 objectsCount,\n-\t\t\t\tObjectSizesHistogram:    objectsHist,\n-\t\t\t\tObjectVersionsHistogram: versionsHist,\n-\t\t\t\tPrefixUsage:             prefixUsage,\n-\t\t\t\tDetails: &madmin.BucketDetails{\n-\t\t\t\t\tVersioning:          globalBucketVersioningSys.Enabled(bucket.Name),\n-\t\t\t\t\tVersioningSuspended: globalBucketVersioningSys.Suspended(bucket.Name),\n-\t\t\t\t\tReplication:         rcfg != nil,\n-\t\t\t\t\tLocking:             lcfg.LockEnabled,\n-\t\t\t\t\tQuota:               quota,\n-\t\t\t\t\tTagging:             tcfg,\n-\t\t\t\t},\n-\t\t\t\tAccess: madmin.AccountAccess{\n-\t\t\t\t\tRead:  rd,\n-\t\t\t\t\tWrite: wr,\n-\t\t\t\t},\n-\t\t\t})\n-\t\t}\n-\t}\n-\n-\tusageInfoJSON, err := json.Marshal(acctInfo)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, usageInfoJSON)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        // Set prefix value for \"s3:prefix\" policy conditionals.\n+        r.Header.Set(\"prefix\", \"\")\n+\n+        // Set delimiter value for \"s3:delimiter\" policy conditionals.\n+        r.Header.Set(\"delimiter\", SlashSeparator)\n+\n+        // Check if we are asked to return prefix usage\n+        enablePrefixUsage := r.Form.Get(\"prefix-usage\") == \"true\"\n+\n+        isAllowedAccess := func(bucketName string) (rd, wr bool) {\n+                if globalIAMSys.IsAllowed(policy.Args{\n+                        AccountName:     cred.AccessKey,\n+                        Groups:          cred.Groups,\n+                        Action:          policy.ListBucketAction,\n+                        BucketName:      bucketName,\n+                        ConditionValues: getConditionValues(r, \"\", cred),\n+                        IsOwner:         owner,\n+                        ObjectName:      \"\",\n+                        Claims:          cred.Claims,\n+                }) {\n+                        rd = true\n+                }\n+\n+                if globalIAMSys.IsAllowed(policy.Args{\n+                        AccountName:     cred.AccessKey,\n+                        Groups:          cred.Groups,\n+                        Action:          policy.GetBucketLocationAction,\n+                        BucketName:      bucketName,\n+                        ConditionValues: getConditionValues(r, \"\", cred),\n+                        IsOwner:         owner,\n+                        ObjectName:      \"\",\n+                        Claims:          cred.Claims,\n+                }) {\n+                        rd = true\n+                }\n+\n+                if globalIAMSys.IsAllowed(policy.Args{\n+                        AccountName:     cred.AccessKey,\n+                        Groups:          cred.Groups,\n+                        Action:          policy.PutObjectAction,\n+                        BucketName:      bucketName,\n+                        ConditionValues: getConditionValues(r, \"\", cred),\n+                        IsOwner:         owner,\n+                        ObjectName:      \"\",\n+                        Claims:          cred.Claims,\n+                }) {\n+                        wr = true\n+                }\n+\n+                return rd, wr\n+        }\n+\n+        bucketStorageCache.Once.Do(func() {\n+                // Set this to 10 secs since its enough, as scanner\n+                // does not update the bucket usage values frequently.\n+                bucketStorageCache.TTL = 10 * time.Second\n+\n+                // Rely on older value if usage loading fails from disk.\n+                bucketStorageCache.Relax = true\n+                bucketStorageCache.Update = func() (interface{}, error) {\n+                        ctx, done := context.WithTimeout(context.Background(), 2*time.Second)\n+                        defer done()\n+\n+                        return loadDataUsageFromBackend(ctx, objectAPI)\n+                }\n+        })\n+\n+        var dataUsageInfo DataUsageInfo\n+        v, _ := bucketStorageCache.Get()\n+        if v != nil {\n+                dataUsageInfo, _ = v.(DataUsageInfo)\n+        }\n+\n+        // If etcd, dns federation configured list buckets from etcd.\n+        var err error\n+        var buckets []BucketInfo\n+        if globalDNSConfig != nil && globalBucketFederation {\n+                dnsBuckets, err := globalDNSConfig.List()\n+                if err != nil && !IsErrIgnored(err,\n+                        dns.ErrNoEntriesFound,\n+                        dns.ErrDomainMissing) {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                for _, dnsRecords := range dnsBuckets {\n+                        buckets = append(buckets, BucketInfo{\n+                                Name:    dnsRecords[0].Key,\n+                                Created: dnsRecords[0].CreationDate,\n+                        })\n+                }\n+                sort.Slice(buckets, func(i, j int) bool {\n+                        return buckets[i].Name < buckets[j].Name\n+                })\n+        } else {\n+                buckets, err = objectAPI.ListBuckets(ctx, BucketOptions{Cached: true})\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+        }\n+\n+        accountName := cred.AccessKey\n+        if cred.IsTemp() || cred.IsServiceAccount() {\n+                // For derived credentials, check the parent user's permissions.\n+                accountName = cred.ParentUser\n+        }\n+\n+        roleArn := policy.Args{Claims: cred.Claims}.GetRoleArn()\n+        policySetFromClaims, hasPolicyClaim := policy.GetPoliciesFromClaims(cred.Claims, iamPolicyClaimNameOpenID())\n+        var effectivePolicy policy.Policy\n+\n+        var buf []byte\n+        switch {\n+        case accountName == globalActiveCred.AccessKey:\n+                for _, policy := range policy.DefaultPolicies {\n+                        if policy.Name == \"consoleAdmin\" {\n+                                effectivePolicy = policy.Definition\n+                                break\n+                        }\n+                }\n+\n+        case roleArn != \"\":\n+                _, policy, err := globalIAMSys.GetRolePolicy(roleArn)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                policySlice := newMappedPolicy(policy).toSlice()\n+                effectivePolicy = globalIAMSys.GetCombinedPolicy(policySlice...)\n+\n+        case hasPolicyClaim:\n+                effectivePolicy = globalIAMSys.GetCombinedPolicy(policySetFromClaims.ToSlice()...)\n+\n+        default:\n+                policies, err := globalIAMSys.PolicyDBGet(accountName, cred.Groups...)\n+                if err != nil {\n+                        logger.LogIf(ctx, err)\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                effectivePolicy = globalIAMSys.GetCombinedPolicy(policies...)\n+\n+        }\n+        buf, err = json.MarshalIndent(effectivePolicy, \"\", \" \")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        acctInfo := madmin.AccountInfo{\n+                AccountName: accountName,\n+                Server:      objectAPI.BackendInfo(),\n+                Policy:      buf,\n+        }\n+\n+        for _, bucket := range buckets {\n+                rd, wr := isAllowedAccess(bucket.Name)\n+                if rd || wr {\n+                        // Fetch the data usage of the current bucket\n+                        var size uint64\n+                        var objectsCount uint64\n+                        var objectsHist, versionsHist map[string]uint64\n+                        if !dataUsageInfo.LastUpdate.IsZero() {\n+                                size = dataUsageInfo.BucketsUsage[bucket.Name].Size\n+                                objectsCount = dataUsageInfo.BucketsUsage[bucket.Name].ObjectsCount\n+                                objectsHist = dataUsageInfo.BucketsUsage[bucket.Name].ObjectSizesHistogram\n+                                versionsHist = dataUsageInfo.BucketsUsage[bucket.Name].ObjectVersionsHistogram\n+                        }\n+                        // Fetch the prefix usage of the current bucket\n+                        var prefixUsage map[string]uint64\n+                        if enablePrefixUsage {\n+                                prefixUsage, _ = loadPrefixUsageFromBackend(ctx, objectAPI, bucket.Name)\n+                        }\n+\n+                        lcfg, _ := globalBucketObjectLockSys.Get(bucket.Name)\n+                        quota, _ := globalBucketQuotaSys.Get(ctx, bucket.Name)\n+                        rcfg, _, _ := globalBucketMetadataSys.GetReplicationConfig(ctx, bucket.Name)\n+                        tcfg, _, _ := globalBucketMetadataSys.GetTaggingConfig(bucket.Name)\n+\n+                        acctInfo.Buckets = append(acctInfo.Buckets, madmin.BucketAccessInfo{\n+                                Name:                    bucket.Name,\n+                                Created:                 bucket.Created,\n+                                Size:                    size,\n+                                Objects:                 objectsCount,\n+                                ObjectSizesHistogram:    objectsHist,\n+                                ObjectVersionsHistogram: versionsHist,\n+                                PrefixUsage:             prefixUsage,\n+                                Details: &madmin.BucketDetails{\n+                                        Versioning:          globalBucketVersioningSys.Enabled(bucket.Name),\n+                                        VersioningSuspended: globalBucketVersioningSys.Suspended(bucket.Name),\n+                                        Replication:         rcfg != nil,\n+                                        Locking:             lcfg.LockEnabled,\n+                                        Quota:               quota,\n+                                        Tagging:             tcfg,\n+                                },\n+                                Access: madmin.AccountAccess{\n+                                        Read:  rd,\n+                                        Write: wr,\n+                                },\n+                        })\n+                }\n+        }\n+\n+        usageInfoJSON, err := json.Marshal(acctInfo)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, usageInfoJSON)\n }\n \n // InfoCannedPolicy - GET /minio/admin/v3/info-canned-policy?name={policyName}\n@@ -1353,1145 +1359,1145 @@ func (a adminAPIHandlers) AccountInfoHandler(w http.ResponseWriter, r *http.Requ\n // timestamps along with the policy JSON. Both versions are supported for now,\n // for smooth transition to new API.\n func (a adminAPIHandlers) InfoCannedPolicy(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.GetPolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tname := mux.Vars(r)[\"name\"]\n-\tpolicies := newMappedPolicy(name).toSlice()\n-\tif len(policies) != 1 {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errTooManyPolicies), r.URL)\n-\t\treturn\n-\t}\n-\n-\tpolicyDoc, err := globalIAMSys.InfoPolicy(name)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Is the new API version being requested?\n-\tinfoPolicyAPIVersion := r.Form.Get(\"v\")\n-\tif infoPolicyAPIVersion == \"2\" {\n-\t\tbuf, err := json.MarshalIndent(policyDoc, \"\", \" \")\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tw.Write(buf)\n-\t\treturn\n-\t} else if infoPolicyAPIVersion != \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errors.New(\"invalid version parameter 'v' supplied\")), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Return the older API response value of just the policy json.\n-\tbuf, err := json.MarshalIndent(policyDoc.Policy, \"\", \" \")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\tw.Write(buf)\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.GetPolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        name := mux.Vars(r)[\"name\"]\n+        policies := newMappedPolicy(name).toSlice()\n+        if len(policies) != 1 {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errTooManyPolicies), r.URL)\n+                return\n+        }\n+\n+        policyDoc, err := globalIAMSys.InfoPolicy(name)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Is the new API version being requested?\n+        infoPolicyAPIVersion := r.Form.Get(\"v\")\n+        if infoPolicyAPIVersion == \"2\" {\n+                buf, err := json.MarshalIndent(policyDoc, \"\", \" \")\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                w.Write(buf)\n+                return\n+        } else if infoPolicyAPIVersion != \"\" {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errors.New(\"invalid version parameter 'v' supplied\")), r.URL)\n+                return\n+        }\n+\n+        // Return the older API response value of just the policy json.\n+        buf, err := json.MarshalIndent(policyDoc.Policy, \"\", \" \")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        w.Write(buf)\n }\n \n // ListBucketPolicies - GET /minio/admin/v3/list-canned-policies?bucket={bucket}\n func (a adminAPIHandlers) ListBucketPolicies(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.ListUserPoliciesAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tbucket := mux.Vars(r)[\"bucket\"]\n-\tpolicies, err := globalIAMSys.ListPolicies(ctx, bucket)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tnewPolicies := make(map[string]policy.Policy)\n-\tfor name, p := range policies {\n-\t\t_, err = json.Marshal(p)\n-\t\tif err != nil {\n-\t\t\tlogger.LogIf(ctx, err)\n-\t\t\tcontinue\n-\t\t}\n-\t\tnewPolicies[name] = p\n-\t}\n-\tif err = json.NewEncoder(w).Encode(newPolicies); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.ListUserPoliciesAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        bucket := mux.Vars(r)[\"bucket\"]\n+        policies, err := globalIAMSys.ListPolicies(ctx, bucket)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        newPolicies := make(map[string]policy.Policy)\n+        for name, p := range policies {\n+                _, err = json.Marshal(p)\n+                if err != nil {\n+                        logger.LogIf(ctx, err)\n+                        continue\n+                }\n+                newPolicies[name] = p\n+        }\n+        if err = json.NewEncoder(w).Encode(newPolicies); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n }\n \n // ListCannedPolicies - GET /minio/admin/v3/list-canned-policies\n func (a adminAPIHandlers) ListCannedPolicies(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.ListUserPoliciesAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tpolicies, err := globalIAMSys.ListPolicies(ctx, \"\")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tnewPolicies := make(map[string]policy.Policy)\n-\tfor name, p := range policies {\n-\t\t_, err = json.Marshal(p)\n-\t\tif err != nil {\n-\t\t\tlogger.LogIf(ctx, err)\n-\t\t\tcontinue\n-\t\t}\n-\t\tnewPolicies[name] = p\n-\t}\n-\tif err = json.NewEncoder(w).Encode(newPolicies); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.ListUserPoliciesAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        policies, err := globalIAMSys.ListPolicies(ctx, \"\")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        newPolicies := make(map[string]policy.Policy)\n+        for name, p := range policies {\n+                _, err = json.Marshal(p)\n+                if err != nil {\n+                        logger.LogIf(ctx, err)\n+                        continue\n+                }\n+                newPolicies[name] = p\n+        }\n+        if err = json.NewEncoder(w).Encode(newPolicies); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n }\n \n // RemoveCannedPolicy - DELETE /minio/admin/v3/remove-canned-policy?name=<policy_name>\n func (a adminAPIHandlers) RemoveCannedPolicy(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.DeletePolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\tpolicyName := vars[\"name\"]\n-\n-\tif err := globalIAMSys.DeletePolicy(ctx, policyName, true); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Call cluster-replication policy creation hook to replicate policy deletion to\n-\t// other minio clusters.\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType:      madmin.SRIAMItemPolicy,\n-\t\tName:      policyName,\n-\t\tUpdatedAt: UTCNow(),\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.DeletePolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        policyName := vars[\"name\"]\n+\n+        if err := globalIAMSys.DeletePolicy(ctx, policyName, true); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Call cluster-replication policy creation hook to replicate policy deletion to\n+        // other minio clusters.\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type:      madmin.SRIAMItemPolicy,\n+                Name:      policyName,\n+                UpdatedAt: UTCNow(),\n+        }))\n }\n \n // AddCannedPolicy - PUT /minio/admin/v3/add-canned-policy?name=<policy_name>\n func (a adminAPIHandlers) AddCannedPolicy(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.CreatePolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\tpolicyName := vars[\"name\"]\n-\n-\t// Policy has space characters in begin and end reject such inputs.\n-\tif hasSpaceBE(policyName) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Error out if Content-Length is missing.\n-\tif r.ContentLength <= 0 {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrMissingContentLength), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Error out if Content-Length is beyond allowed size.\n-\tif r.ContentLength > maxBucketPolicySize {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrEntityTooLarge), r.URL)\n-\t\treturn\n-\t}\n-\n-\tiamPolicyBytes, err := io.ReadAll(io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tiamPolicy, err := policy.ParseConfig(bytes.NewReader(iamPolicyBytes))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Version in policy must not be empty\n-\tif iamPolicy.Version == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrPolicyInvalidVersion), r.URL)\n-\t\treturn\n-\t}\n-\n-\tupdatedAt, err := globalIAMSys.SetPolicy(ctx, policyName, *iamPolicy)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Call cluster-replication policy creation hook to replicate policy to\n-\t// other minio clusters.\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType:      madmin.SRIAMItemPolicy,\n-\t\tName:      policyName,\n-\t\tPolicy:    iamPolicyBytes,\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.CreatePolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        policyName := vars[\"name\"]\n+\n+        // Policy has space characters in begin and end reject such inputs.\n+        if hasSpaceBE(policyName) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        // Error out if Content-Length is missing.\n+        if r.ContentLength <= 0 {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrMissingContentLength), r.URL)\n+                return\n+        }\n+\n+        // Error out if Content-Length is beyond allowed size.\n+        if r.ContentLength > maxBucketPolicySize {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrEntityTooLarge), r.URL)\n+                return\n+        }\n+\n+        iamPolicyBytes, err := io.ReadAll(io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        iamPolicy, err := policy.ParseConfig(bytes.NewReader(iamPolicyBytes))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Version in policy must not be empty\n+        if iamPolicy.Version == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrPolicyInvalidVersion), r.URL)\n+                return\n+        }\n+\n+        updatedAt, err := globalIAMSys.SetPolicy(ctx, policyName, *iamPolicy)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Call cluster-replication policy creation hook to replicate policy to\n+        // other minio clusters.\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type:      madmin.SRIAMItemPolicy,\n+                Name:      policyName,\n+                Policy:    iamPolicyBytes,\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // SetPolicyForUserOrGroup - PUT /minio/admin/v3/set-policy?policy=xxx&user-or-group=?[&is-group]\n func (a adminAPIHandlers) SetPolicyForUserOrGroup(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.AttachPolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\tpolicyName := vars[\"policyName\"]\n-\tentityName := vars[\"userOrGroup\"]\n-\tisGroup := vars[\"isGroup\"] == \"true\"\n-\n-\tif !isGroup {\n-\t\tok, _, err := globalIAMSys.IsTempUser(entityName)\n-\t\tif err != nil && err != errNoSuchUser {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tif ok {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\t// When the user is root credential you are not allowed to\n-\t\t// add policies for root user.\n-\t\tif entityName == globalActiveCred.AccessKey {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\t// Validate that user or group exists.\n-\tif !isGroup {\n-\t\tif globalIAMSys.GetUsersSysType() == MinIOUsersSysType {\n-\t\t\t_, ok := globalIAMSys.GetUser(ctx, entityName)\n-\t\t\tif !ok {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errNoSuchUser), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\t_, err := globalIAMSys.GetGroupDescription(entityName)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tuserType := regUser\n-\tif globalIAMSys.GetUsersSysType() == LDAPUsersSysType {\n-\t\tuserType = stsUser\n-\t}\n-\n-\tupdatedAt, err := globalIAMSys.PolicyDBSet(ctx, entityName, policyName, userType, isGroup)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemPolicyMapping,\n-\t\tPolicyMapping: &madmin.SRPolicyMapping{\n-\t\t\tUserOrGroup: entityName,\n-\t\t\tUserType:    int(userType),\n-\t\t\tIsGroup:     isGroup,\n-\t\t\tPolicy:      policyName,\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.AttachPolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        policyName := vars[\"policyName\"]\n+        entityName := vars[\"userOrGroup\"]\n+        isGroup := vars[\"isGroup\"] == \"true\"\n+\n+        if !isGroup {\n+                ok, _, err := globalIAMSys.IsTempUser(entityName)\n+                if err != nil && err != errNoSuchUser {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                if ok {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                        return\n+                }\n+                // When the user is root credential you are not allowed to\n+                // add policies for root user.\n+                if entityName == globalActiveCred.AccessKey {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                        return\n+                }\n+        }\n+\n+        // Validate that user or group exists.\n+        if !isGroup {\n+                if globalIAMSys.GetUsersSysType() == MinIOUsersSysType {\n+                        _, ok := globalIAMSys.GetUser(ctx, entityName)\n+                        if !ok {\n+                                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errNoSuchUser), r.URL)\n+                                return\n+                        }\n+                }\n+        } else {\n+                _, err := globalIAMSys.GetGroupDescription(entityName)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+        }\n+\n+        userType := regUser\n+        if globalIAMSys.GetUsersSysType() == LDAPUsersSysType {\n+                userType = stsUser\n+        }\n+\n+        updatedAt, err := globalIAMSys.PolicyDBSet(ctx, entityName, policyName, userType, isGroup)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemPolicyMapping,\n+                PolicyMapping: &madmin.SRPolicyMapping{\n+                        UserOrGroup: entityName,\n+                        UserType:    int(userType),\n+                        IsGroup:     isGroup,\n+                        Policy:      policyName,\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // ListPolicyMappingEntities - GET /minio/admin/v3/idp/builtin/polciy-entities?policy=xxx&user=xxx&group=xxx\n func (a adminAPIHandlers) ListPolicyMappingEntities(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Check authorization.\n-\tobjectAPI, cred := validateAdminReq(ctx, w, r,\n-\t\tpolicy.ListGroupsAdminAction, policy.ListUsersAdminAction, policy.ListUserPoliciesAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\t// Validate API arguments.\n-\tq := madmin.PolicyEntitiesQuery{\n-\t\tUsers:  r.Form[\"user\"],\n-\t\tGroups: r.Form[\"group\"],\n-\t\tPolicy: r.Form[\"policy\"],\n-\t}\n-\n-\t// Query IAM\n-\tres, err := globalIAMSys.QueryPolicyEntities(r.Context(), q)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Encode result and send response.\n-\tdata, err := json.Marshal(res)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\tpassword := cred.SecretKey\n-\teconfigData, err := madmin.EncryptData(password, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\twriteSuccessResponseJSON(w, econfigData)\n+        ctx := r.Context()\n+\n+        // Check authorization.\n+        objectAPI, cred := validateAdminReq(ctx, w, r,\n+                policy.ListGroupsAdminAction, policy.ListUsersAdminAction, policy.ListUserPoliciesAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        // Validate API arguments.\n+        q := madmin.PolicyEntitiesQuery{\n+                Users:  r.Form[\"user\"],\n+                Groups: r.Form[\"group\"],\n+                Policy: r.Form[\"policy\"],\n+        }\n+\n+        // Query IAM\n+        res, err := globalIAMSys.QueryPolicyEntities(r.Context(), q)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Encode result and send response.\n+        data, err := json.Marshal(res)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        password := cred.SecretKey\n+        econfigData, err := madmin.EncryptData(password, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        writeSuccessResponseJSON(w, econfigData)\n }\n \n // AttachDetachPolicyBuiltin - POST /minio/admin/v3/idp/builtin/policy/{operation}\n func (a adminAPIHandlers) AttachDetachPolicyBuiltin(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, cred := validateAdminReq(ctx, w, r, policy.UpdatePolicyAssociationAction,\n-\t\tpolicy.AttachPolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tif r.ContentLength > maxEConfigJSONSize || r.ContentLength == -1 {\n-\t\t// More than maxConfigSize bytes were available\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigTooLarge), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Ensure body content type is opaque to ensure that request body has not\n-\t// been interpreted as form data.\n-\tcontentType := r.Header.Get(\"Content-Type\")\n-\tif contentType != \"application/octet-stream\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrBadRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\toperation := mux.Vars(r)[\"operation\"]\n-\tif operation != \"attach\" && operation != \"detach\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\tisAttach := operation == \"attach\"\n-\n-\tpassword := cred.SecretKey\n-\treqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar par madmin.PolicyAssociationReq\n-\tif err = json.Unmarshal(reqBytes, &par); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif err = par.IsValid(); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tupdatedAt, addedOrRemoved, _, err := globalIAMSys.PolicyDBUpdateBuiltin(ctx, isAttach, par)\n-\tif err != nil {\n-\t\tif err == errNoSuchUser || err == errNoSuchGroup {\n-\t\t\tif globalIAMSys.LDAPConfig.Enabled() {\n-\t\t\t\t// When LDAP is enabled, warn user that they are using the wrong\n-\t\t\t\t// API. FIXME: error can be no such group as well - fix errNoSuchUserLDAPWarn\n-\t\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errNoSuchUserLDAPWarn), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\trespBody := madmin.PolicyAssociationResp{\n-\t\tUpdatedAt: updatedAt,\n-\t}\n-\tif isAttach {\n-\t\trespBody.PoliciesAttached = addedOrRemoved\n-\t} else {\n-\t\trespBody.PoliciesDetached = addedOrRemoved\n-\t}\n-\n-\tdata, err := json.Marshal(respBody)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(password, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n+        ctx := r.Context()\n+\n+        objectAPI, cred := validateAdminReq(ctx, w, r, policy.UpdatePolicyAssociationAction,\n+                policy.AttachPolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        if r.ContentLength > maxEConfigJSONSize || r.ContentLength == -1 {\n+                // More than maxConfigSize bytes were available\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigTooLarge), r.URL)\n+                return\n+        }\n+\n+        // Ensure body content type is opaque to ensure that request body has not\n+        // been interpreted as form data.\n+        contentType := r.Header.Get(\"Content-Type\")\n+        if contentType != \"application/octet-stream\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrBadRequest), r.URL)\n+                return\n+        }\n+\n+        operation := mux.Vars(r)[\"operation\"]\n+        if operation != \"attach\" && operation != \"detach\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminInvalidArgument), r.URL)\n+                return\n+        }\n+        isAttach := operation == \"attach\"\n+\n+        password := cred.SecretKey\n+        reqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        var par madmin.PolicyAssociationReq\n+        if err = json.Unmarshal(reqBytes, &par); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        if err = par.IsValid(); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        updatedAt, addedOrRemoved, _, err := globalIAMSys.PolicyDBUpdateBuiltin(ctx, isAttach, par)\n+        if err != nil {\n+                if err == errNoSuchUser || err == errNoSuchGroup {\n+                        if globalIAMSys.LDAPConfig.Enabled() {\n+                                // When LDAP is enabled, warn user that they are using the wrong\n+                                // API. FIXME: error can be no such group as well - fix errNoSuchUserLDAPWarn\n+                                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errNoSuchUserLDAPWarn), r.URL)\n+                                return\n+                        }\n+                }\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        respBody := madmin.PolicyAssociationResp{\n+                UpdatedAt: updatedAt,\n+        }\n+        if isAttach {\n+                respBody.PoliciesAttached = addedOrRemoved\n+        } else {\n+                respBody.PoliciesDetached = addedOrRemoved\n+        }\n+\n+        data, err := json.Marshal(respBody)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(password, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n }\n \n const (\n-\tallPoliciesFile            = \"policies.json\"\n-\tallUsersFile               = \"users.json\"\n-\tallGroupsFile              = \"groups.json\"\n-\tallSvcAcctsFile            = \"svcaccts.json\"\n-\tuserPolicyMappingsFile     = \"user_mappings.json\"\n-\tgroupPolicyMappingsFile    = \"group_mappings.json\"\n-\tstsUserPolicyMappingsFile  = \"stsuser_mappings.json\"\n-\tstsGroupPolicyMappingsFile = \"stsgroup_mappings.json\"\n-\tiamAssetsDir               = \"iam-assets\"\n+        allPoliciesFile            = \"policies.json\"\n+        allUsersFile               = \"users.json\"\n+        allGroupsFile              = \"groups.json\"\n+        allSvcAcctsFile            = \"svcaccts.json\"\n+        userPolicyMappingsFile     = \"user_mappings.json\"\n+        groupPolicyMappingsFile    = \"group_mappings.json\"\n+        stsUserPolicyMappingsFile  = \"stsuser_mappings.json\"\n+        stsGroupPolicyMappingsFile = \"stsgroup_mappings.json\"\n+        iamAssetsDir               = \"iam-assets\"\n )\n \n // ExportIAMHandler - exports all iam info as a zipped file\n func (a adminAPIHandlers) ExportIAM(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.ExportIAMAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\t// Initialize a zip writer which will provide a zipped content\n-\t// of bucket metadata\n-\tzipWriter := zip.NewWriter(w)\n-\tdefer zipWriter.Close()\n-\trawDataFn := func(r io.Reader, filename string, sz int) error {\n-\t\theader, zerr := zip.FileInfoHeader(dummyFileInfo{\n-\t\t\tname:    filename,\n-\t\t\tsize:    int64(sz),\n-\t\t\tmode:    0o600,\n-\t\t\tmodTime: time.Now(),\n-\t\t\tisDir:   false,\n-\t\t\tsys:     nil,\n-\t\t})\n-\t\tif zerr != nil {\n-\t\t\tlogger.LogIf(ctx, zerr)\n-\t\t\treturn nil\n-\t\t}\n-\t\theader.Method = zip.Deflate\n-\t\tzwriter, zerr := zipWriter.CreateHeader(header)\n-\t\tif zerr != nil {\n-\t\t\tlogger.LogIf(ctx, zerr)\n-\t\t\treturn nil\n-\t\t}\n-\t\tif _, err := io.Copy(zwriter, r); err != nil {\n-\t\t\tlogger.LogIf(ctx, err)\n-\t\t}\n-\t\treturn nil\n-\t}\n-\n-\tiamFiles := []string{\n-\t\tallPoliciesFile,\n-\t\tallUsersFile,\n-\t\tallGroupsFile,\n-\t\tallSvcAcctsFile,\n-\t\tuserPolicyMappingsFile,\n-\t\tgroupPolicyMappingsFile,\n-\t\tstsUserPolicyMappingsFile,\n-\t\tstsGroupPolicyMappingsFile,\n-\t}\n-\tfor _, f := range iamFiles {\n-\t\tiamFile := pathJoin(iamAssetsDir, f)\n-\t\tswitch f {\n-\t\tcase allPoliciesFile:\n-\t\t\tallPolicies, err := globalIAMSys.ListPolicies(ctx, \"\")\n-\t\t\tif err != nil {\n-\t\t\t\tlogger.LogIf(ctx, err)\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tpoliciesData, err := json.Marshal(allPolicies)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = rawDataFn(bytes.NewReader(policiesData), iamFile, len(policiesData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase allUsersFile:\n-\t\t\tuserIdentities := make(map[string]UserIdentity)\n-\t\t\terr := globalIAMSys.store.loadUsers(ctx, regUser, userIdentities)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tuserAccounts := make(map[string]madmin.AddOrUpdateUserReq)\n-\t\t\tfor u, uid := range userIdentities {\n-\t\t\t\tuserAccounts[u] = madmin.AddOrUpdateUserReq{\n-\t\t\t\t\tSecretKey: uid.Credentials.SecretKey,\n-\t\t\t\t\tStatus: func() madmin.AccountStatus {\n-\t\t\t\t\t\t// Export current credential status\n-\t\t\t\t\t\tif uid.Credentials.Status == auth.AccountOff {\n-\t\t\t\t\t\t\treturn madmin.AccountDisabled\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\treturn madmin.AccountEnabled\n-\t\t\t\t\t}(),\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tuserData, err := json.Marshal(userAccounts)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(userData), iamFile, len(userData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase allGroupsFile:\n-\t\t\tgroups := make(map[string]GroupInfo)\n-\t\t\terr := globalIAMSys.store.loadGroups(ctx, groups)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tgroupData, err := json.Marshal(groups)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(groupData), iamFile, len(groupData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase allSvcAcctsFile:\n-\t\t\tserviceAccounts := make(map[string]UserIdentity)\n-\t\t\terr := globalIAMSys.store.loadUsers(ctx, svcUser, serviceAccounts)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tsvcAccts := make(map[string]madmin.SRSvcAccCreate)\n-\t\t\tfor user, acc := range serviceAccounts {\n-\t\t\t\tif user == siteReplicatorSvcAcc {\n-\t\t\t\t\t// skip site-replication service account.\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tclaims, err := globalIAMSys.GetClaimsForSvcAcc(ctx, acc.Credentials.AccessKey)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\t_, policy, err := globalIAMSys.GetServiceAccount(ctx, acc.Credentials.AccessKey)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tvar policyJSON []byte\n-\t\t\t\tif policy != nil {\n-\t\t\t\t\tpolicyJSON, err = json.Marshal(policy)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tsvcAccts[user] = madmin.SRSvcAccCreate{\n-\t\t\t\t\tParent:        acc.Credentials.ParentUser,\n-\t\t\t\t\tAccessKey:     user,\n-\t\t\t\t\tSecretKey:     acc.Credentials.SecretKey,\n-\t\t\t\t\tGroups:        acc.Credentials.Groups,\n-\t\t\t\t\tClaims:        claims,\n-\t\t\t\t\tSessionPolicy: json.RawMessage(policyJSON),\n-\t\t\t\t\tStatus:        acc.Credentials.Status,\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tsvcAccData, err := json.Marshal(svcAccts)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(svcAccData), iamFile, len(svcAccData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase userPolicyMappingsFile:\n-\t\t\tuserPolicyMap := make(map[string]MappedPolicy)\n-\t\t\terr := globalIAMSys.store.loadMappedPolicies(ctx, regUser, false, userPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tuserPolData, err := json.Marshal(userPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(userPolData), iamFile, len(userPolData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase groupPolicyMappingsFile:\n-\t\t\tgroupPolicyMap := make(map[string]MappedPolicy)\n-\t\t\terr := globalIAMSys.store.loadMappedPolicies(ctx, regUser, true, groupPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tgrpPolData, err := json.Marshal(groupPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(grpPolData), iamFile, len(grpPolData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase stsUserPolicyMappingsFile:\n-\t\t\tuserPolicyMap := make(map[string]MappedPolicy)\n-\t\t\terr := globalIAMSys.store.loadMappedPolicies(ctx, stsUser, false, userPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tuserPolData, err := json.Marshal(userPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = rawDataFn(bytes.NewReader(userPolData), iamFile, len(userPolData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase stsGroupPolicyMappingsFile:\n-\t\t\tgroupPolicyMap := make(map[string]MappedPolicy)\n-\t\t\terr := globalIAMSys.store.loadMappedPolicies(ctx, stsUser, true, groupPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tgrpPolData, err := json.Marshal(groupPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = rawDataFn(bytes.NewReader(grpPolData), iamFile, len(grpPolData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t}\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.ExportIAMAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+        // Initialize a zip writer which will provide a zipped content\n+        // of bucket metadata\n+        zipWriter := zip.NewWriter(w)\n+        defer zipWriter.Close()\n+        rawDataFn := func(r io.Reader, filename string, sz int) error {\n+                header, zerr := zip.FileInfoHeader(dummyFileInfo{\n+                        name:    filename,\n+                        size:    int64(sz),\n+                        mode:    0o600,\n+                        modTime: time.Now(),\n+                        isDir:   false,\n+                        sys:     nil,\n+                })\n+                if zerr != nil {\n+                        logger.LogIf(ctx, zerr)\n+                        return nil\n+                }\n+                header.Method = zip.Deflate\n+                zwriter, zerr := zipWriter.CreateHeader(header)\n+                if zerr != nil {\n+                        logger.LogIf(ctx, zerr)\n+                        return nil\n+                }\n+                if _, err := io.Copy(zwriter, r); err != nil {\n+                        logger.LogIf(ctx, err)\n+                }\n+                return nil\n+        }\n+\n+        iamFiles := []string{\n+                allPoliciesFile,\n+                allUsersFile,\n+                allGroupsFile,\n+                allSvcAcctsFile,\n+                userPolicyMappingsFile,\n+                groupPolicyMappingsFile,\n+                stsUserPolicyMappingsFile,\n+                stsGroupPolicyMappingsFile,\n+        }\n+        for _, f := range iamFiles {\n+                iamFile := pathJoin(iamAssetsDir, f)\n+                switch f {\n+                case allPoliciesFile:\n+                        allPolicies, err := globalIAMSys.ListPolicies(ctx, \"\")\n+                        if err != nil {\n+                                logger.LogIf(ctx, err)\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        policiesData, err := json.Marshal(allPolicies)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = rawDataFn(bytes.NewReader(policiesData), iamFile, len(policiesData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case allUsersFile:\n+                        userIdentities := make(map[string]UserIdentity)\n+                        err := globalIAMSys.store.loadUsers(ctx, regUser, userIdentities)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        userAccounts := make(map[string]madmin.AddOrUpdateUserReq)\n+                        for u, uid := range userIdentities {\n+                                userAccounts[u] = madmin.AddOrUpdateUserReq{\n+                                        SecretKey: uid.Credentials.SecretKey,\n+                                        Status: func() madmin.AccountStatus {\n+                                                // Export current credential status\n+                                                if uid.Credentials.Status == auth.AccountOff {\n+                                                        return madmin.AccountDisabled\n+                                                }\n+                                                return madmin.AccountEnabled\n+                                        }(),\n+                                }\n+                        }\n+                        userData, err := json.Marshal(userAccounts)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(userData), iamFile, len(userData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case allGroupsFile:\n+                        groups := make(map[string]GroupInfo)\n+                        err := globalIAMSys.store.loadGroups(ctx, groups)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        groupData, err := json.Marshal(groups)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(groupData), iamFile, len(groupData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case allSvcAcctsFile:\n+                        serviceAccounts := make(map[string]UserIdentity)\n+                        err := globalIAMSys.store.loadUsers(ctx, svcUser, serviceAccounts)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        svcAccts := make(map[string]madmin.SRSvcAccCreate)\n+                        for user, acc := range serviceAccounts {\n+                                if user == siteReplicatorSvcAcc {\n+                                        // skip site-replication service account.\n+                                        continue\n+                                }\n+                                claims, err := globalIAMSys.GetClaimsForSvcAcc(ctx, acc.Credentials.AccessKey)\n+                                if err != nil {\n+                                        writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                        return\n+                                }\n+                                _, policy, err := globalIAMSys.GetServiceAccount(ctx, acc.Credentials.AccessKey)\n+                                if err != nil {\n+                                        writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                        return\n+                                }\n+\n+                                var policyJSON []byte\n+                                if policy != nil {\n+                                        policyJSON, err = json.Marshal(policy)\n+                                        if err != nil {\n+                                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                                return\n+                                        }\n+                                }\n+                                svcAccts[user] = madmin.SRSvcAccCreate{\n+                                        Parent:        acc.Credentials.ParentUser,\n+                                        AccessKey:     user,\n+                                        SecretKey:     acc.Credentials.SecretKey,\n+                                        Groups:        acc.Credentials.Groups,\n+                                        Claims:        claims,\n+                                        SessionPolicy: json.RawMessage(policyJSON),\n+                                        Status:        acc.Credentials.Status,\n+                                }\n+                        }\n+\n+                        svcAccData, err := json.Marshal(svcAccts)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(svcAccData), iamFile, len(svcAccData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case userPolicyMappingsFile:\n+                        userPolicyMap := make(map[string]MappedPolicy)\n+                        err := globalIAMSys.store.loadMappedPolicies(ctx, regUser, false, userPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        userPolData, err := json.Marshal(userPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(userPolData), iamFile, len(userPolData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case groupPolicyMappingsFile:\n+                        groupPolicyMap := make(map[string]MappedPolicy)\n+                        err := globalIAMSys.store.loadMappedPolicies(ctx, regUser, true, groupPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        grpPolData, err := json.Marshal(groupPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(grpPolData), iamFile, len(grpPolData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case stsUserPolicyMappingsFile:\n+                        userPolicyMap := make(map[string]MappedPolicy)\n+                        err := globalIAMSys.store.loadMappedPolicies(ctx, stsUser, false, userPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        userPolData, err := json.Marshal(userPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = rawDataFn(bytes.NewReader(userPolData), iamFile, len(userPolData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case stsGroupPolicyMappingsFile:\n+                        groupPolicyMap := make(map[string]MappedPolicy)\n+                        err := globalIAMSys.store.loadMappedPolicies(ctx, stsUser, true, groupPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        grpPolData, err := json.Marshal(groupPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = rawDataFn(bytes.NewReader(grpPolData), iamFile, len(grpPolData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                }\n+        }\n }\n \n // ImportIAM - imports all IAM info into MinIO\n func (a adminAPIHandlers) ImportIAM(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\tdata, err := io.ReadAll(r.Body)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\treader := bytes.NewReader(data)\n-\tzr, err := zip.NewReader(reader, int64(len(data)))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\t// import policies first\n-\t{\n-\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, allPoliciesFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allPoliciesFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar allPolicies map[string]policy.Policy\n-\t\t\tdata, err = io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allPoliciesFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\terr = json.Unmarshal(data, &allPolicies)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allPoliciesFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor policyName, policy := range allPolicies {\n-\t\t\t\tif policy.IsEmpty() {\n-\t\t\t\t\terr = globalIAMSys.DeletePolicy(ctx, policyName, true)\n-\t\t\t\t} else {\n-\t\t\t\t\t_, err = globalIAMSys.SetPolicy(ctx, policyName, policy)\n-\t\t\t\t}\n-\t\t\t\tif err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allPoliciesFile, policyName), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import users\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, allUsersFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allUsersFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar userAccts map[string]madmin.AddOrUpdateUserReq\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allUsersFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\terr = json.Unmarshal(data, &userAccts)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allUsersFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor accessKey, ureq := range userAccts {\n-\t\t\t\t// Not allowed to add a user with same access key as root credential\n-\t\t\t\tif accessKey == globalActiveCred.AccessKey {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tuser, exists := globalIAMSys.GetUser(ctx, accessKey)\n-\t\t\t\tif exists && (user.Credentials.IsTemp() || user.Credentials.IsServiceAccount()) {\n-\t\t\t\t\t// Updating STS credential is not allowed, and this API does not\n-\t\t\t\t\t// support updating service accounts.\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tif (cred.IsTemp() || cred.IsServiceAccount()) && cred.ParentUser == accessKey {\n-\t\t\t\t\t// Incoming access key matches parent user then we should\n-\t\t\t\t\t// reject password change requests.\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\t// Check if accessKey has beginning and end space characters, this only applies to new users.\n-\t\t\t\tif !exists && hasSpaceBE(accessKey) {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminResourceInvalidArgument, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tcheckDenyOnly := false\n-\t\t\t\tif accessKey == cred.AccessKey {\n-\t\t\t\t\t// Check that there is no explicit deny - otherwise it's allowed\n-\t\t\t\t\t// to change one's own password.\n-\t\t\t\t\tcheckDenyOnly = true\n-\t\t\t\t}\n-\n-\t\t\t\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\t\t\t\tAccountName:     cred.AccessKey,\n-\t\t\t\t\tGroups:          cred.Groups,\n-\t\t\t\t\tAction:          policy.CreateUserAdminAction,\n-\t\t\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\t\t\tIsOwner:         owner,\n-\t\t\t\t\tClaims:          cred.Claims,\n-\t\t\t\t\tDenyOnly:        checkDenyOnly,\n-\t\t\t\t}) {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAccessDenied, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif _, err = globalIAMSys.CreateUser(ctx, accessKey, ureq); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, toAdminAPIErrCode(ctx, err), err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import groups\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, allGroupsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allGroupsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar grpInfos map[string]GroupInfo\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allGroupsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &grpInfos); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allGroupsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor group, grpInfo := range grpInfos {\n-\t\t\t\t// Check if group already exists\n-\t\t\t\tif _, gerr := globalIAMSys.GetGroupDescription(group); gerr != nil {\n-\t\t\t\t\t// If group does not exist, then check if the group has beginning and end space characters\n-\t\t\t\t\t// we will reject such group names.\n-\t\t\t\t\tif errors.Is(gerr, errNoSuchGroup) && hasSpaceBE(group) {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminResourceInvalidArgument, err, allGroupsFile, group), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif _, gerr := globalIAMSys.AddUsersToGroup(ctx, group, grpInfo.Members); gerr != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allGroupsFile, group), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import service accounts\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, allSvcAcctsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allSvcAcctsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar serviceAcctReqs map[string]madmin.SRSvcAccCreate\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allSvcAcctsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &serviceAcctReqs); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allSvcAcctsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor user, svcAcctReq := range serviceAcctReqs {\n-\t\t\t\tvar sp *policy.Policy\n-\t\t\t\tvar err error\n-\t\t\t\tif len(svcAcctReq.SessionPolicy) > 0 {\n-\t\t\t\t\tsp, err = policy.ParseConfig(bytes.NewReader(svcAcctReq.SessionPolicy))\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\t// service account access key cannot have space characters beginning and end of the string.\n-\t\t\t\tif hasSpaceBE(svcAcctReq.AccessKey) {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\t\t\t\tAccountName:     cred.AccessKey,\n-\t\t\t\t\tGroups:          cred.Groups,\n-\t\t\t\t\tAction:          policy.CreateServiceAccountAdminAction,\n-\t\t\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\t\t\tIsOwner:         owner,\n-\t\t\t\t\tClaims:          cred.Claims,\n-\t\t\t\t}) {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAccessDenied, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tupdateReq := true\n-\t\t\t\t_, _, err = globalIAMSys.GetServiceAccount(ctx, svcAcctReq.AccessKey)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tif !errors.Is(err, errNoSuchServiceAccount) {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t\tupdateReq = false\n-\t\t\t\t}\n-\t\t\t\tif updateReq {\n-\t\t\t\t\topts := updateServiceAccountOpts{\n-\t\t\t\t\t\tsecretKey:     svcAcctReq.SecretKey,\n-\t\t\t\t\t\tstatus:        svcAcctReq.Status,\n-\t\t\t\t\t\tname:          svcAcctReq.Name,\n-\t\t\t\t\t\tdescription:   svcAcctReq.Description,\n-\t\t\t\t\t\texpiration:    svcAcctReq.Expiration,\n-\t\t\t\t\t\tsessionPolicy: sp,\n-\t\t\t\t\t}\n-\t\t\t\t\t_, err = globalIAMSys.UpdateServiceAccount(ctx, svcAcctReq.AccessKey, opts)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\topts := newServiceAccountOpts{\n-\t\t\t\t\taccessKey:                  user,\n-\t\t\t\t\tsecretKey:                  svcAcctReq.SecretKey,\n-\t\t\t\t\tsessionPolicy:              sp,\n-\t\t\t\t\tclaims:                     svcAcctReq.Claims,\n-\t\t\t\t\tname:                       svcAcctReq.Name,\n-\t\t\t\t\tdescription:                svcAcctReq.Description,\n-\t\t\t\t\texpiration:                 svcAcctReq.Expiration,\n-\t\t\t\t\tallowSiteReplicatorAccount: false,\n-\t\t\t\t}\n-\n-\t\t\t\t// In case of LDAP we need to resolve the targetUser to a DN and\n-\t\t\t\t// query their groups:\n-\t\t\t\tif globalIAMSys.LDAPConfig.Enabled() {\n-\t\t\t\t\topts.claims[ldapUserN] = svcAcctReq.AccessKey // simple username\n-\t\t\t\t\ttargetUser, _, err := globalIAMSys.LDAPConfig.LookupUserDN(svcAcctReq.AccessKey)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t\topts.claims[ldapUser] = targetUser // username DN\n-\t\t\t\t}\n-\n-\t\t\t\tif _, _, err = globalIAMSys.NewServiceAccount(ctx, svcAcctReq.Parent, svcAcctReq.Groups, opts); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import user policy mappings\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, userPolicyMappingsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, userPolicyMappingsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar userPolicyMap map[string]MappedPolicy\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, userPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &userPolicyMap); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, userPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor u, pm := range userPolicyMap {\n-\t\t\t\t// disallow setting policy mapping if user is a temporary user\n-\t\t\t\tok, _, err := globalIAMSys.IsTempUser(u)\n-\t\t\t\tif err != nil && err != errNoSuchUser {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, userPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif ok {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, errIAMActionNotAllowed, userPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif _, err := globalIAMSys.PolicyDBSet(ctx, u, pm.Policies, regUser, false); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, userPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import group policy mappings\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, groupPolicyMappingsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, groupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar grpPolicyMap map[string]MappedPolicy\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, groupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &grpPolicyMap); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, groupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor g, pm := range grpPolicyMap {\n-\t\t\t\tif _, err := globalIAMSys.PolicyDBSet(ctx, g, pm.Policies, unknownIAMUserType, true); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, groupPolicyMappingsFile, g), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import sts user policy mappings\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, stsUserPolicyMappingsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar userPolicyMap map[string]MappedPolicy\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &userPolicyMap); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor u, pm := range userPolicyMap {\n-\t\t\t\t// disallow setting policy mapping if user is a temporary user\n-\t\t\t\tok, _, err := globalIAMSys.IsTempUser(u)\n-\t\t\t\tif err != nil && err != errNoSuchUser {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, stsUserPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif ok {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, errIAMActionNotAllowed, stsUserPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif _, err := globalIAMSys.PolicyDBSet(ctx, u, pm.Policies, stsUser, false); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, stsUserPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import sts group policy mappings\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, stsGroupPolicyMappingsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar grpPolicyMap map[string]MappedPolicy\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &grpPolicyMap); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor g, pm := range grpPolicyMap {\n-\t\t\t\tif _, err := globalIAMSys.PolicyDBSet(ctx, g, pm.Policies, unknownIAMUserType, true); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, stsGroupPolicyMappingsFile, g), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+        data, err := io.ReadAll(r.Body)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+        reader := bytes.NewReader(data)\n+        zr, err := zip.NewReader(reader, int64(len(data)))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+        // import policies first\n+        {\n+\n+                f, err := zr.Open(pathJoin(iamAssetsDir, allPoliciesFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allPoliciesFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var allPolicies map[string]policy.Policy\n+                        data, err = io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allPoliciesFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        err = json.Unmarshal(data, &allPolicies)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allPoliciesFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for policyName, policy := range allPolicies {\n+                                if policy.IsEmpty() {\n+                                        err = globalIAMSys.DeletePolicy(ctx, policyName, true)\n+                                } else {\n+                                        _, err = globalIAMSys.SetPolicy(ctx, policyName, policy)\n+                                }\n+                                if err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, allPoliciesFile, policyName), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import users\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, allUsersFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allUsersFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var userAccts map[string]madmin.AddOrUpdateUserReq\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allUsersFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        err = json.Unmarshal(data, &userAccts)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allUsersFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for accessKey, ureq := range userAccts {\n+                                // Not allowed to add a user with same access key as root credential\n+                                if accessKey == globalActiveCred.AccessKey {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                                user, exists := globalIAMSys.GetUser(ctx, accessKey)\n+                                if exists && (user.Credentials.IsTemp() || user.Credentials.IsServiceAccount()) {\n+                                        // Updating STS credential is not allowed, and this API does not\n+                                        // support updating service accounts.\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                                if (cred.IsTemp() || cred.IsServiceAccount()) && cred.ParentUser == accessKey {\n+                                        // Incoming access key matches parent user then we should\n+                                        // reject password change requests.\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                                // Check if accessKey has beginning and end space characters, this only applies to new users.\n+                                if !exists && hasSpaceBE(accessKey) {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminResourceInvalidArgument, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                                checkDenyOnly := false\n+                                if accessKey == cred.AccessKey {\n+                                        // Check that there is no explicit deny - otherwise it's allowed\n+                                        // to change one's own password.\n+                                        checkDenyOnly = true\n+                                }\n+\n+                                if !globalIAMSys.IsAllowed(policy.Args{\n+                                        AccountName:     cred.AccessKey,\n+                                        Groups:          cred.Groups,\n+                                        Action:          policy.CreateUserAdminAction,\n+                                        ConditionValues: getConditionValues(r, \"\", cred),\n+                                        IsOwner:         owner,\n+                                        Claims:          cred.Claims,\n+                                        DenyOnly:        checkDenyOnly,\n+                                }) {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAccessDenied, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+                                if _, err = globalIAMSys.CreateUser(ctx, accessKey, ureq); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, toAdminAPIErrCode(ctx, err), err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                        }\n+                }\n+        }\n+\n+        // import groups\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, allGroupsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allGroupsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var grpInfos map[string]GroupInfo\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allGroupsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &grpInfos); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allGroupsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for group, grpInfo := range grpInfos {\n+                                // Check if group already exists\n+                                if _, gerr := globalIAMSys.GetGroupDescription(group); gerr != nil {\n+                                        // If group does not exist, then check if the group has beginning and end space characters\n+                                        // we will reject such group names.\n+                                        if errors.Is(gerr, errNoSuchGroup) && hasSpaceBE(group) {\n+                                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminResourceInvalidArgument, err, allGroupsFile, group), r.URL)\n+                                                return\n+                                        }\n+                                }\n+                                if _, gerr := globalIAMSys.AddUsersToGroup(ctx, group, grpInfo.Members); gerr != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, allGroupsFile, group), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import service accounts\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, allSvcAcctsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allSvcAcctsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var serviceAcctReqs map[string]madmin.SRSvcAccCreate\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allSvcAcctsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &serviceAcctReqs); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allSvcAcctsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for user, svcAcctReq := range serviceAcctReqs {\n+                                var sp *policy.Policy\n+                                var err error\n+                                if len(svcAcctReq.SessionPolicy) > 0 {\n+                                        sp, err = policy.ParseConfig(bytes.NewReader(svcAcctReq.SessionPolicy))\n+                                        if err != nil {\n+                                                writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                                return\n+                                        }\n+                                }\n+                                // service account access key cannot have space characters beginning and end of the string.\n+                                if hasSpaceBE(svcAcctReq.AccessKey) {\n+                                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                                        return\n+                                }\n+                                if !globalIAMSys.IsAllowed(policy.Args{\n+                                        AccountName:     cred.AccessKey,\n+                                        Groups:          cred.Groups,\n+                                        Action:          policy.CreateServiceAccountAdminAction,\n+                                        ConditionValues: getConditionValues(r, \"\", cred),\n+                                        IsOwner:         owner,\n+                                        Claims:          cred.Claims,\n+                                }) {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAccessDenied, err, allSvcAcctsFile, user), r.URL)\n+                                        return\n+                                }\n+                                updateReq := true\n+                                _, _, err = globalIAMSys.GetServiceAccount(ctx, svcAcctReq.AccessKey)\n+                                if err != nil {\n+                                        if !errors.Is(err, errNoSuchServiceAccount) {\n+                                                writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                                return\n+                                        }\n+                                        updateReq = false\n+                                }\n+                                if updateReq {\n+                                        opts := updateServiceAccountOpts{\n+                                                secretKey:     svcAcctReq.SecretKey,\n+                                                status:        svcAcctReq.Status,\n+                                                name:          svcAcctReq.Name,\n+                                                description:   svcAcctReq.Description,\n+                                                expiration:    svcAcctReq.Expiration,\n+                                                sessionPolicy: sp,\n+                                        }\n+                                        _, err = globalIAMSys.UpdateServiceAccount(ctx, svcAcctReq.AccessKey, opts)\n+                                        if err != nil {\n+                                                writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                                return\n+                                        }\n+                                        continue\n+                                }\n+                                opts := newServiceAccountOpts{\n+                                        accessKey:                  user,\n+                                        secretKey:                  svcAcctReq.SecretKey,\n+                                        sessionPolicy:              sp,\n+                                        claims:                     svcAcctReq.Claims,\n+                                        name:                       svcAcctReq.Name,\n+                                        description:                svcAcctReq.Description,\n+                                        expiration:                 svcAcctReq.Expiration,\n+                                        allowSiteReplicatorAccount: false,\n+                                }\n+\n+                                // In case of LDAP we need to resolve the targetUser to a DN and\n+                                // query their groups:\n+                                if globalIAMSys.LDAPConfig.Enabled() {\n+                                        opts.claims[ldapUserN] = svcAcctReq.AccessKey // simple username\n+                                        targetUser, _, err := globalIAMSys.LDAPConfig.LookupUserDN(svcAcctReq.AccessKey)\n+                                        if err != nil {\n+                                                writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                                return\n+                                        }\n+                                        opts.claims[ldapUser] = targetUser // username DN\n+                                }\n+\n+                                if _, _, err = globalIAMSys.NewServiceAccount(ctx, svcAcctReq.Parent, svcAcctReq.Groups, opts); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                        return\n+                                }\n+\n+                        }\n+                }\n+        }\n+\n+        // import user policy mappings\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, userPolicyMappingsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, userPolicyMappingsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var userPolicyMap map[string]MappedPolicy\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, userPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &userPolicyMap); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, userPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for u, pm := range userPolicyMap {\n+                                // disallow setting policy mapping if user is a temporary user\n+                                ok, _, err := globalIAMSys.IsTempUser(u)\n+                                if err != nil && err != errNoSuchUser {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, userPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                                if ok {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, errIAMActionNotAllowed, userPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                                if _, err := globalIAMSys.PolicyDBSet(ctx, u, pm.Policies, regUser, false); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, userPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import group policy mappings\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, groupPolicyMappingsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, groupPolicyMappingsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var grpPolicyMap map[string]MappedPolicy\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, groupPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &grpPolicyMap); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, groupPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for g, pm := range grpPolicyMap {\n+                                if _, err := globalIAMSys.PolicyDBSet(ctx, g, pm.Policies, unknownIAMUserType, true); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, groupPolicyMappingsFile, g), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import sts user policy mappings\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, stsUserPolicyMappingsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var userPolicyMap map[string]MappedPolicy\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &userPolicyMap); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for u, pm := range userPolicyMap {\n+                                // disallow setting policy mapping if user is a temporary user\n+                                ok, _, err := globalIAMSys.IsTempUser(u)\n+                                if err != nil && err != errNoSuchUser {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, stsUserPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                                if ok {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, errIAMActionNotAllowed, stsUserPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                                if _, err := globalIAMSys.PolicyDBSet(ctx, u, pm.Policies, stsUser, false); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, stsUserPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import sts group policy mappings\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, stsGroupPolicyMappingsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var grpPolicyMap map[string]MappedPolicy\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &grpPolicyMap); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for g, pm := range grpPolicyMap {\n+                                if _, err := globalIAMSys.PolicyDBSet(ctx, g, pm.Policies, unknownIAMUserType, true); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, stsGroupPolicyMappingsFile, g), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n }\n \n func commonAddServiceAccount(r *http.Request) (context.Context, auth.Credentials, newServiceAccountOpts, madmin.AddServiceAccountReq, string, APIError) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrServerNotInitialized)\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(s3Err)\n-\t}\n-\n-\tpassword := cred.SecretKey\n-\treqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n-\t}\n-\n-\tvar createReq madmin.AddServiceAccountReq\n-\tif err = json.Unmarshal(reqBytes, &createReq); err != nil {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n-\t}\n-\n-\t// service account access key cannot have space characters beginning and end of the string.\n-\tif hasSpaceBE(createReq.AccessKey) {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n-\t}\n-\n-\tif err := createReq.Validate(); err != nil {\n-\t\t// Since this validation would happen client side as well, we only send\n-\t\t// a generic error message here.\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n-\t}\n-\t// If the request did not set a TargetUser, the service account is\n-\t// created for the request sender.\n-\ttargetUser := createReq.TargetUser\n-\tif targetUser == \"\" {\n-\t\ttargetUser = cred.AccessKey\n-\t}\n-\n-\tdescription := createReq.Description\n-\tif description == \"\" {\n-\t\tdescription = createReq.Comment\n-\t}\n-\topts := newServiceAccountOpts{\n-\t\taccessKey:   createReq.AccessKey,\n-\t\tsecretKey:   createReq.SecretKey,\n-\t\tname:        createReq.Name,\n-\t\tdescription: description,\n-\t\texpiration:  createReq.Expiration,\n-\t\tclaims:      make(map[string]interface{}),\n-\t}\n-\n-\t// Check if action is allowed if creating access key for another user\n-\t// Check if action is explicitly denied if for self\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.CreateServiceAccountAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t\tDenyOnly:        (targetUser == cred.AccessKey || targetUser == cred.ParentUser),\n-\t}) {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAccessDenied)\n-\t}\n-\n-\tvar sp *policy.Policy\n-\tif len(createReq.Policy) > 0 {\n-\t\tsp, err = policy.ParseConfig(bytes.NewReader(createReq.Policy))\n-\t\tif err != nil {\n-\t\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", toAdminAPIErr(ctx, err)\n-\t\t}\n-\t}\n-\n-\topts.sessionPolicy = sp\n-\n-\treturn ctx, cred, opts, createReq, targetUser, APIError{}\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrServerNotInitialized)\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(s3Err)\n+        }\n+\n+        password := cred.SecretKey\n+        reqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n+        }\n+\n+        var createReq madmin.AddServiceAccountReq\n+        if err = json.Unmarshal(reqBytes, &createReq); err != nil {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n+        }\n+\n+        // service account access key cannot have space characters beginning and end of the string.\n+        if hasSpaceBE(createReq.AccessKey) {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n+        }\n+\n+        if err := createReq.Validate(); err != nil {\n+                // Since this validation would happen client side as well, we only send\n+                // a generic error message here.\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n+        }\n+        // If the request did not set a TargetUser, the service account is\n+        // created for the request sender.\n+        targetUser := createReq.TargetUser\n+        if targetUser == \"\" {\n+                targetUser = cred.AccessKey\n+        }\n+\n+        description := createReq.Description\n+        if description == \"\" {\n+                description = createReq.Comment\n+        }\n+        opts := newServiceAccountOpts{\n+                accessKey:   createReq.AccessKey,\n+                secretKey:   createReq.SecretKey,\n+                name:        createReq.Name,\n+                description: description,\n+                expiration:  createReq.Expiration,\n+                claims:      make(map[string]interface{}),\n+        }\n+\n+        // Check if action is allowed if creating access key for another user\n+        // Check if action is explicitly denied if for self\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.CreateServiceAccountAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+                DenyOnly:        (targetUser == cred.AccessKey || targetUser == cred.ParentUser),\n+        }) {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAccessDenied)\n+        }\n+\n+        var sp *policy.Policy\n+        if len(createReq.Policy) > 0 {\n+                sp, err = policy.ParseConfig(bytes.NewReader(createReq.Policy))\n+                if err != nil {\n+                        return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", toAdminAPIErr(ctx, err)\n+                }\n+        }\n+\n+        opts.sessionPolicy = sp\n+\n+        return ctx, cred, opts, createReq, targetUser, APIError{}\n }\n"}
{"cve":"CVE-2024-39330:0708", "fix_patch": ""}
{"cve":"CVE-2021-35042:0708", "fix_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex 7a16d4889a..19d1198d60 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1898,14 +1898,12 @@ class Query(BaseExpression):\n         for item in ordering:\n             if isinstance(item, str):\n                 if '.' in item:\n-                    warnings.warn(\n-                        'Passing column raw column aliases to order_by() is '\n-                        'deprecated. Wrap %r in a RawSQL expression before '\n-                        'passing it to order_by().' % item,\n-                        category=RemovedInDjango40Warning,\n-                        stacklevel=3,\n+                    raise FieldError(\n+                        'Cannot resolve keyword %r into field. Choices are: %s' % (\n+                            item,\n+                            ', '.join(sorted([f.name for f in self.get_meta().get_fields()])),\n+                        )\n                     )\n-                    continue\n                 if item == '?':\n                     continue\n                 if item.startswith('-'):\n"}
{"cve":"CVE-2021-33203:0708", "fix_patch": "diff --git a/django/contrib/admindocs/views.py b/django/contrib/admindocs/views.py\nindex bd566cde40..93cb798b4a 100644\n--- a/django/contrib/admindocs/views.py\n+++ b/django/contrib/admindocs/views.py\n@@ -2,6 +2,9 @@ import inspect\n from importlib import import_module\n from inspect import cleandoc\n from pathlib import Path\n+import os\n+from django.core.exceptions import SuspiciousFileOperation\n+\n \n from django.apps import apps\n from django.conf import settings\n@@ -320,6 +323,11 @@ class TemplateDetailView(BaseAdminDocsView):\n \n     def get_context_data(self, **kwargs):\n         template = self.kwargs['template']\n+        if '..' in Path(template).parts or os.path.isabs(template):\n+             raise SuspiciousFileOperation(\n+                f'The joined path ({template}) is located outside of the base ' f'path component ({Path(template).parts})'\n+            )\n+\n         templates = []\n         try:\n             default_engine = Engine.get_default()\n"}
{"cve":"CVE-2022-28347:0708", "fix_patch": "diff --git a/django/db/backends/postgresql/operations.py b/django/db/backends/postgresql/operations.py\nindex 1cb6050bc5..c9f16dd8f7 100644\n--- a/django/db/backends/postgresql/operations.py\n+++ b/django/db/backends/postgresql/operations.py\n@@ -289,18 +289,18 @@ class DatabaseOperations(BaseDatabaseOperations):\n \n     def explain_query_prefix(self, format=None, **options):\n         prefix = super().explain_query_prefix(format)\n-        extra = {}\n+        extra = []\n         if format:\n-            extra[\"FORMAT\"] = format\n+            if not format.isalpha():\n+                raise ValueError('Invalid format: %s.' % format)\n+            extra.append('FORMAT %s' % format)\n         if options:\n-            extra.update(\n-                {\n-                    name.upper(): \"true\" if value else \"false\"\n-                    for name, value in options.items()\n-                }\n-            )\n+            for name, value in options.items():\n+                if not name.isidentifier():\n+                    raise ValueError('Invalid option name: %s.' % name)\n+                extra.append('%s %s' % (self.quote_name(name.upper()), 'true' if value else 'false'))\n         if extra:\n-            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n+            prefix += ' (%s)' % ', '.join(extra)\n         return prefix\n \n     def ignore_conflicts_suffix_sql(self, ignore_conflicts=None):\n"}
{"cve":"CVE-2017-7233:0708", "fix_patch": "diff --git a/django/utils/http.py b/django/utils/http.py\nindex c053185a8a..54ff7c0a1b 100644\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -306,15 +306,15 @@ def _is_safe_url(url, host):\n     if url.startswith('///'):\n         return False\n     url_info = urlparse(url)\n+    # A netloc that is just a number is not safe.\n+    if url_info.netloc and url_info.netloc.isdigit():\n+        return False\n     # Forbid URLs like http:///example.com - with a scheme, but without a hostname.\n     # In that URL, example.com is not the hostname but, a path component. However,\n     # Chrome will still consider example.com to be the hostname, so we must not\n     # allow this syntax.\n     if not url_info.netloc and url_info.scheme:\n         return False\n-    # Forbid URLs that start with control characters. Some browsers (like\n-    # Chrome) ignore quite a few control characters at the start of a\n-    # URL and might consider the URL as scheme relative.\n     if unicodedata.category(url[0])[0] == 'C':\n         return False\n     return ((not url_info.netloc or url_info.netloc == host) and\n"}
{"cve":"CVE-2021-3281:0708", "fix_patch": "diff --git a/django/utils/archive.py b/django/utils/archive.py\nindex 235809f2ad..a0d43050ee 100644\n--- a/django/utils/archive.py\n+++ b/django/utils/archive.py\n@@ -27,6 +27,8 @@ import stat\n import tarfile\n import zipfile\n \n+from django.core.exceptions import SuspiciousOperation\n+\n \n class ArchiveException(Exception):\n     \"\"\"\n@@ -40,6 +42,15 @@ class UnrecognizedArchiveFormat(ArchiveException):\n     \"\"\"\n \n \n+\n+\n+def _is_within_directory(directory, target):\n+    abs_directory = os.path.abspath(directory)\n+    abs_target = os.path.abspath(target)\n+    prefix = os.path.commonprefix([abs_directory, abs_target])\n+    return prefix == abs_directory\n+\n+\n def extract(path, to_path):\n     \"\"\"\n     Unpack the tar or zip file at the specified path to the directory\n@@ -156,6 +167,10 @@ class TarArchive(BaseArchive):\n             if leading:\n                 name = self.split_leading_dir(name)[1]\n             filename = os.path.join(to_path, name)\n+            if not _is_within_directory(to_path, filename):\n+                raise SuspiciousOperation(\n+                    f'Archive contains invalid path: {name!r}'\n+                )\n             if member.isdir():\n                 if filename:\n                     os.makedirs(filename, exist_ok=True)\n@@ -198,17 +213,24 @@ class ZipArchive(BaseArchive):\n             info = self._archive.getinfo(name)\n             if leading:\n                 name = self.split_leading_dir(name)[1]\n+\n             filename = os.path.join(to_path, name)\n-            if filename.endswith(('/', '\\\\')):\n-                # A directory\n+            if not _is_within_directory(to_path, filename):\n+                raise SuspiciousOperation(\n+                    f'Archive contains invalid path: {name!r}'\n+                )\n+\n+            if name.endswith('/'):\n+                # A directory.\n                 os.makedirs(filename, exist_ok=True)\n             else:\n+                # A file.\n                 dirname = os.path.dirname(filename)\n                 if dirname:\n                     os.makedirs(dirname, exist_ok=True)\n                 with open(filename, 'wb') as outfile:\n                     outfile.write(data)\n-                # Convert ZipInfo.external_attr to mode\n+                # Convert ZipInfo.external_attr to mode.\n                 mode = info.external_attr >> 16\n                 self._copy_permissions(mode, filename)\n \n"}
{"cve":"CVE-2018-14574:0708", "fix_patch": "diff --git a/django/middleware/common.py b/django/middleware/common.py\nindex d8cfb9a8b0..04a4b8da15 100644\n--- a/django/middleware/common.py\n+++ b/django/middleware/common.py\n@@ -88,6 +88,11 @@ class CommonMiddleware(MiddlewareMixin):\n         POST, PUT, or PATCH.\n         \"\"\"\n         new_path = request.get_full_path(force_append_slash=True)\n+        # Prevent construction of urls with multiple leading slashes which would\n+        # redirect to //... instead of /...\n+        if new_path.startswith('//'):\n+            new_path = '/%2F' + new_path.lstrip('/')\n+\n         if settings.DEBUG and request.method in ('POST', 'PUT', 'PATCH'):\n             raise RuntimeError(\n                 \"You called this URL via %(method)s, but the URL doesn't end \"\ndiff --git a/django/urls/resolvers.py b/django/urls/resolvers.py\nindex c5b0811136..1175eca537 100644\n--- a/django/urls/resolvers.py\n+++ b/django/urls/resolvers.py\n@@ -17,7 +17,7 @@ from django.core.checks.urls import check_resolver\n from django.core.exceptions import ImproperlyConfigured\n from django.utils.datastructures import MultiValueDict\n from django.utils.functional import cached_property\n-from django.utils.http import RFC3986_SUBDELIMS\n+from django.utils.http import RFC3986_SUBDELIMS, escape_leading_slashes\n from django.utils.regex_helper import normalize\n from django.utils.translation import get_language\n \n@@ -602,11 +602,8 @@ class URLResolver:\n                 candidate_pat = _prefix.replace('%', '%%') + result\n                 if re.search('^%s%s' % (re.escape(_prefix), pattern), candidate_pat % text_candidate_subs):\n                     # safe characters from `pchar` definition of RFC 3986\n-                    url = quote(candidate_pat % text_candidate_subs, safe=RFC3986_SUBDELIMS + '/~:@')\n-                    # Don't allow construction of scheme relative urls.\n-                    if url.startswith('//'):\n-                        url = '/%%2F%s' % url[2:]\n-                    return url\n+                    url = quote(candidate_pat % text_candidate_subs, safe=RFC3986_SUBDELIMS + '/%~:@')\n+                    return escape_leading_slashes(url)\n         # lookup_view can be URL name or callable, but callables are not\n         # friendly in error messages.\n         m = getattr(lookup_view, '__module__', None)\ndiff --git a/django/utils/http.py b/django/utils/http.py\nindex 5e900506ff..9d40af9d01 100644\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -437,3 +437,14 @@ def limited_parse_qsl(qs, keep_blank_values=False, encoding='utf-8',\n             value = unquote(value, encoding=encoding, errors=errors)\n             r.append((name, value))\n     return r\n+\n+\n+def escape_leading_slashes(url):\n+    \"\"\"\n+    If the given URL starts with one or more slashes, quote the slashes\n+    to prevent browsers from interpreting them as protocol-relative URLs.\n+    \"\"\"\n+    if url.startswith('//'):\n+        # AllowHttpdOptionsMiddleware requires this to be a bytes object.\n+        url = '/%2F' + url.lstrip('/')\n+    return url\n\\ No newline at end of file\n"}
{"cve":"CVE-2021-45452:0708", "fix_patch": "diff --git a/django/core/files/storage.py b/django/core/files/storage.py\nindex 89faa626e6..632bf3d2ef 100644\n--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -68,8 +68,8 @@ class Storage:\n         available for new content to be written to.\n         \"\"\"\n         dir_name, file_name = os.path.split(name)\n-        if '..' in pathlib.PurePath(dir_name).parts:\n-            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n+        if '..' in pathlib.PurePath(name).parts:\n+            raise SuspiciousFileOperation(f'Detected path traversal attempt in \\'{name}\\'')\n         validate_file_name(file_name)\n         file_root, file_ext = os.path.splitext(file_name)\n         # If the filename already exists, add an underscore and a random 7\n"}
{"cve":"CVE-2015-8213:0708", "fix_patch": "diff --git a/django/utils/formats.py b/django/utils/formats.py\nindex d2bdda458e..a9ebcaae04 100644\n--- a/django/utils/formats.py\n+++ b/django/utils/formats.py\n@@ -92,6 +92,10 @@ def get_format(format_type, lang=None, use_l10n=None):\n     be localized (or not), overriding the value of settings.USE_L10N.\n     \"\"\"\n     format_type = force_str(format_type)\n+    if format_type in ('SECRET_KEY',):\n+        return None\n+\n+\n     if use_l10n or (use_l10n is None and settings.USE_L10N):\n         if lang is None:\n             lang = get_language()\n"}
{"cve":"CVE-2022-21699:0708", "fix_patch": "diff --git a/IPython/core/application.py b/IPython/core/application.py\nindex e93a10647..2b389a686 100644\n--- a/IPython/core/application.py\n+++ b/IPython/core/application.py\n@@ -157,7 +157,7 @@ def _config_file_name_changed(self, change):\n     config_file_paths = List(Unicode())\n     @default('config_file_paths')\n     def _config_file_paths_default(self):\n-        return [os.getcwd()]\n+        return []\n \n     extra_config_file = Unicode(\n     help=\"\"\"Path to an extra config file to load.\ndiff --git a/IPython/core/profileapp.py b/IPython/core/profileapp.py\nindex 97434e3d0..c417b7193 100644\n--- a/IPython/core/profileapp.py\n+++ b/IPython/core/profileapp.py\n@@ -178,11 +178,7 @@ def list_profile_dirs(self):\n             print(\"Available profiles in %s:\" % self.ipython_dir)\n             self._print_profiles(profiles)\n         \n-        profiles = list_profiles_in(os.getcwd())\n-        if profiles:\n-            print()\n-            print(\"Available profiles in current directory (%s):\" % os.getcwd())\n-            self._print_profiles(profiles)\n+\n         \n         print()\n         print(\"To use any of the above profiles, start IPython with:\")\ndiff --git a/IPython/core/profiledir.py b/IPython/core/profiledir.py\nindex 756595adb..5d6d9e8df 100644\n--- a/IPython/core/profiledir.py\n+++ b/IPython/core/profiledir.py\n@@ -200,7 +200,7 @@ def find_profile_dir_by_name(cls, ipython_dir, name=u'default', config=None):\n             will be \"profile_<profile>\".\n         \"\"\"\n         dirname = u'profile_' + name\n-        paths = [os.getcwd(), ipython_dir]\n+        paths = [ipython_dir]\n         for p in paths:\n             profile_dir = os.path.join(p, dirname)\n             if os.path.isdir(profile_dir):\n"}
{"cve":"CVE-2023-34233:0708", "fix_patch": "diff --git a/src/snowflake/connector/auth/webbrowser.py b/src/snowflake/connector/auth/webbrowser.py\nindex 527b10e1..2934bfb1 100644\n--- a/src/snowflake/connector/auth/webbrowser.py\n+++ b/src/snowflake/connector/auth/webbrowser.py\n@@ -26,6 +26,7 @@ from ..errorcode import (\n     ER_NO_HOSTNAME_FOUND,\n     ER_UNABLE_TO_OPEN_BROWSER,\n )\n+\n from ..errors import OperationalError\n from ..network import (\n     CONTENT_TYPE_APPLICATION_JSON,\n"}
{"cve":"CVE-2022-21683:0708", "fix_patch": "diff --git a/wagtail/admin/views/pages/edit.py b/wagtail/admin/views/pages/edit.py\nindex 2edf11b450..2d5c73313c 100644\n--- a/wagtail/admin/views/pages/edit.py\n+++ b/wagtail/admin/views/pages/edit.py\n@@ -139,13 +139,13 @@ class EditView(TemplateResponseMixin, ContextMixin, HookResponseMixin, View):\n         global_recipient_users = [subscriber.user for subscriber in subscribers if subscriber.user != self.request.user]\n \n         # Get subscribers to individual threads\n-        replies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\n-        comments = Comment.objects.filter(id__in=relevant_comment_ids)\n+        replies = CommentReply.objects.filter(comment__page=self.page, comment_id__in=relevant_comment_ids)\n+        comments = Comment.objects.filter(page=self.page, id__in=relevant_comment_ids)\n         thread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).prefetch_related(\n             Prefetch('comment_replies', queryset=replies),\n             Prefetch(COMMENTS_RELATION_NAME, queryset=comments)\n         ).exclude(\n-            Q(comment_replies__isnull=True) & Q(**{('%s__isnull' % COMMENTS_RELATION_NAME): True})\n+            Q(comment_replies__isnull=True) & Q(**{ ('%s__isnull' % COMMENTS_RELATION_NAME): True})\n         )\n \n         # Skip if no recipients\n"}
{"cve":"CVE-2024-23334:0708", "fix_patch": "diff --git a/aiohttp/web_urldispatcher.py b/aiohttp/web_urldispatcher.py\nindex fee4f61a1..8bc432dba 100644\n--- a/aiohttp/web_urldispatcher.py\n+++ b/aiohttp/web_urldispatcher.py\n@@ -563,14 +563,15 @@ class StaticResource(PrefixResource):\n     ) -> URL:\n         if append_version is None:\n             append_version = self._append_version\n-        filename = str(filename).lstrip(\"/\")\n+\n+        filename = Path(filename)\n+        if filename.is_absolute():\n+            raise ValueError(\"filename has to be a relative path\")\n+\n+        filename_str = str(filename)\n \n         url = URL.build(path=self._prefix, encoded=True)\n-        # filename is not encoded\n-        if YARL_VERSION < (1, 6):\n-            url = url / filename.replace(\"%\", \"%25\")\n-        else:\n-            url = url / filename\n+        url = url.join(URL(filename_str, encoded=False))\n \n         if append_version:\n             try:\n@@ -578,17 +579,14 @@ class StaticResource(PrefixResource):\n                 if not self._follow_symlinks:\n                     filepath.relative_to(self._directory)\n             except (ValueError, FileNotFoundError):\n-                # ValueError for case when path point to symlink\n-                # with follow_symlinks is False\n-                return url  # relatively safe\n+                return url\n+\n             if filepath.is_file():\n-                # TODO cache file content\n-                # with file watcher for cache invalidation\n                 with filepath.open(\"rb\") as f:\n                     file_bytes = f.read()\n                 h = self._get_file_hash(file_bytes)\n                 url = url.with_query({self.VERSION_KEY: h})\n-                return url\n+\n         return url\n \n     @staticmethod\n@@ -640,9 +638,17 @@ class StaticResource(PrefixResource):\n                 # /static/\\\\machine_name\\c$ or /static/D:\\path\n                 # where the static dir is totally different\n                 raise HTTPForbidden()\n-            filepath = self._directory.joinpath(filename).resolve()\n-            if not self._follow_symlinks:\n+            filepath = self._directory.joinpath(filename)\n+            if self._follow_symlinks:\n+                # Check for directory traversal, see CVE-2024-23334\n+                if not os.path.commonpath([filepath.resolve(), self._directory]) == str(self._directory):\n+                    raise HTTPForbidden()\n+            else:\n+                # Raise if path is not inside directory\n                 filepath.relative_to(self._directory)\n+\n+            if not self._follow_symlinks:\n+                filepath = filepath.resolve()\n         except (ValueError, FileNotFoundError) as error:\n             # relatively safe\n             raise HTTPNotFound() from error\n"}
{"cve":"CVE-2022-37109:0708", "fix_patch": ""}
{"cve":"CVE-2023-49736:0708", "fix_patch": ""}
{"cve":"CVE-2024-39877:0708", "fix_patch": "diff --git a/airflow/models/dag.py b/airflow/models/dag.py\nindex 560d05b548..bc44a429a7 100644\n--- a/airflow/models/dag.py\n+++ b/airflow/models/dag.py\n@@ -775,15 +775,14 @@ class DAG(LoggingMixin):\n         env = self.get_template_env(force_sandboxed=True)\n \n         if not doc_md.endswith(\".md\"):\n-            template = jinja2.Template(doc_md)\n-        else:\n-            try:\n-                template = env.get_template(doc_md)\n-            except jinja2.exceptions.TemplateNotFound:\n-                return f\"\"\"\n-                # Templating Error!\n-                Not able to find the template file: `{doc_md}`.\n-                \"\"\"\n+            return doc_md\n+        try:\n+            template = env.get_template(doc_md)\n+        except jinja2.exceptions.TemplateNotFound:\n+            return f\"\"\"\n+            # Templating Error!\n+            Not able to find the template file: `{doc_md}`.\n+            \"\"\"\n \n         return template.render()\n \n"}
{"cve":"CVE-2023-33977:0708", "fix_patch": "diff --git a/tcms/kiwi_attachments/validators.py b/tcms/kiwi_attachments/validators.py\nindex 696a1439..aa2908da 100644\n--- a/tcms/kiwi_attachments/validators.py\n+++ b/tcms/kiwi_attachments/validators.py\n@@ -1,10 +1,21 @@\n from django.forms import ValidationError\n from django.utils.translation import gettext_lazy as _\n+import re\n \n \n def deny_uploads_containing_script_tag(uploaded_file):\n+    # Matches variations of <script> tag. The regex is case-insensitive,\n+    # allows for leading/trailing whitespace and also self-closing tags, e.g. `<script/>`.\n+    # NB: this regex is far from perfect, e.g. it will not catch unicode homoglyphs\n+    # for the `<script>` string, intentionally broken strings like `<scr ipt>`\n+    # or the fact that browsers may execute JavaScript from inside other tags & attributes\n+    # or from files which aren't explicitly JavaScript or HTML. A far better protection\n+    # is to serve user uploaded files from another domain and with\n+    # Content-Disposition: attachment and with Content-Security-Policy: sandbox\n+    # At the moment this is the best we can do. See Issue #2955\n+    script_tag = re.compile(br'\\s*<script.*>', re.IGNORECASE)\n     for chunk in uploaded_file.chunks(2048):\n-        if chunk.lower().find(b\"<script\") > -1:\n+        if script_tag.search(chunk):\n             raise ValidationError(_(\"File contains forbidden <script> tag\"))\n \n \ndiff --git a/tcms/settings/common.py b/tcms/settings/common.py\nindex d1a10984..d62dafc3 100644\n--- a/tcms/settings/common.py\n+++ b/tcms/settings/common.py\n@@ -136,6 +136,13 @@ FILE_UPLOAD_MAX_SIZE = 5242880\n DELETE_ATTACHMENTS_FROM_DISK = True\n \n \n+# A list of validators that will be applied to all file uploads!\n+FILE_UPLOAD_VALIDATORS = [\n+    'tcms.kiwi_attachments.validators.deny_uploads_containing_script_tag',\n+\n+]\n+\n+\n # Configure a caching backend. ATM only used to cache bug details b/c\n # external issue trackers may be slow. If you want to override see:\n # https://docs.djangoproject.com/en/2.2/topics/cache/\n@@ -171,8 +178,13 @@ MIDDLEWARE = [\n     \"django.contrib.auth.middleware.AuthenticationMiddleware\",\n     \"django.contrib.messages.middleware.MessageMiddleware\",\n     \"simple_history.middleware.HistoryRequestMiddleware\",\n+    'csp.middleware.CSPMiddleware',\n ]\n \n+CSP_DEFAULT_SRC = (\"'self'\",)\n+\n+\n+\n \n # See https://github.com/kiwitcms/Kiwi/issues/2717\n # and tcms/issuetracker/azure_boards.py\n"}
{"cve":"CVE-2022-31506:0708", "fix_patch": "diff --git a/opendiamond/dataretriever/diamond_store.py b/opendiamond/dataretriever/diamond_store.py\nindex 0bcb1160..f3f6c55e 100644\n--- a/opendiamond/dataretriever/diamond_store.py\n+++ b/opendiamond/dataretriever/diamond_store.py\n@@ -114,13 +114,20 @@ def _get_object_element(object_path):\n \n def _get_object_src_uri(object_path):\n     if LOCAL_OBJ_URI:\n-        return 'file://' + _get_obj_absolute_path(object_path)\n+        abs_path = _get_obj_absolute_path(object_path)\n+        if abs_path:\n+            return 'file://' + abs_path\n+        else:\n+            return None\n     else:\n         return url_for('.get_object_src_http', obj_path=object_path)\n \n \n def _get_obj_absolute_path(obj_path):\n-    return os.path.join(DATAROOT, obj_path)\n+    safe_path = os.path.abspath(os.path.join(DATAROOT, obj_path))\n+    if os.path.commonprefix([safe_path, os.path.abspath(DATAROOT)]) != os.path.abspath(DATAROOT):\n+        return None\n+    return safe_path\n \n \n def _get_index_absolute_path(index):\n"}
{"cve":"CVE-2019-7539:0708", "fix_patch": "diff --git a/ipycache.py b/ipycache.py\nindex 74a4fe5..0929d32 100644\n--- a/ipycache.py\n+++ b/ipycache.py\n@@ -117,6 +117,8 @@ def load_vars(path, vars):\n         # Load the variables from the cache.\n         try:\n             cache = pickle.load(f)\n+            if not isinstance(cache, dict):\n+                raise pickle.UnpicklingError(\"ipycache: Stored cache is not a dictionary.\")\n         except EOFError as e:\n             cache={}\n             #raise IOError(str(e))\n"}
{"cve":"CVE-2025-23042:0708", "fix_patch": "diff --git a/gradio/utils.py b/gradio/utils.py\nindex 080a3d118..09ae04c7f 100644\n--- a/gradio/utils.py\n+++ b/gradio/utils.py\n@@ -1513,13 +1513,13 @@ def is_allowed_file(\n     bool, Literal[\"in_blocklist\", \"allowed\", \"created\", \"not_created_or_allowed\"]\n ]:\n     in_blocklist = any(\n-        is_in_or_equal(path, blocked_path) for blocked_path in blocked_paths\n+        is_in_or_equal(str(path).lower(), str(blocked_path).lower()) for blocked_path in blocked_paths\n     )\n     if in_blocklist:\n         return False, \"in_blocklist\"\n-    if any(is_in_or_equal(path, allowed_path) for allowed_path in allowed_paths):\n+    if any(is_in_or_equal(str(path).lower(), str(allowed_path).lower()) for allowed_path in allowed_paths):\n         return True, \"allowed\"\n-    if any(is_in_or_equal(path, created_path) for created_path in created_paths):\n+    if any(is_in_or_equal(str(path).lower(), str(created_path).lower()) for created_path in created_paths):\n         return True, \"created\"\n     return False, \"not_created_or_allowed\"\n \n"}
{"cve":"CVE-2022-3298:0708", "fix_patch": ""}
{"cve":"CVE-2022-41672:0708", "fix_patch": "diff --git a/airflow/www/app.py b/airflow/www/app.py\nindex b67314c99a..3f9e2ff5d2 100644\n--- a/airflow/www/app.py\n+++ b/airflow/www/app.py\n@@ -27,6 +27,9 @@ from flask_caching import Cache\n from flask_wtf.csrf import CSRFProtect\n from sqlalchemy.engine.url import make_url\n \n+from flask import flash, g\n+from flask_login import logout_user\n+\n from airflow import settings\n from airflow.configuration import conf\n from airflow.exceptions import AirflowConfigException, RemovedInAirflow3Warning\n@@ -139,6 +142,12 @@ def create_app(config=None, testing=False):\n     with flask_app.app_context():\n         init_appbuilder(flask_app)\n \n+        @flask_app.before_request\n+        def check_active_user():\n+            if g.user and g.user.is_authenticated and not g.user.active:\n+                logout_user()\n+                flash(\"Your user has been deactivated, please contact your administrator.\", \"error\")\n+\n         init_appbuilder_views(flask_app)\n         init_appbuilder_links(flask_app)\n         init_plugins(flask_app)\n"}
{"cve":"CVE-2024-3571:0708", "fix_patch": "diff --git a/libs/langchain/langchain/storage/file_system.py b/libs/langchain/langchain/storage/file_system.py\nindex 720acf085..8d9619469 100644\n--- a/libs/langchain/langchain/storage/file_system.py\n+++ b/libs/langchain/langchain/storage/file_system.py\n@@ -42,7 +42,7 @@ class LocalFileStore(ByteStore):\n             root_path (Union[str, Path]): The root path of the file store. All keys are\n                 interpreted as paths relative to this root.\n         \"\"\"\n-        self.root_path = Path(root_path)\n+        self.root_path = Path(root_path).resolve()\n \n     def _get_full_path(self, key: str) -> Path:\n         \"\"\"Get the full path for a given key relative to the root path.\n@@ -55,6 +55,12 @@ class LocalFileStore(ByteStore):\n         \"\"\"\n         if not re.match(r\"^[a-zA-Z0-9_.\\-/]+$\", key):\n             raise InvalidKeyException(f\"Invalid characters in key: {key}\")\n+\n+        resolved_path = (self.root_path / key).resolve()\n+        if self.root_path != resolved_path and self.root_path not in resolved_path.parents:\n+            raise InvalidKeyException(\n+                f\"Attempted to access path outside of root: {key}\"\n+            )\n         return self.root_path / key\n \n     def mget(self, keys: Sequence[str]) -> List[Optional[bytes]]:\n"}
{"cve":"CVE-2024-0243:0708", "fix_patch": "diff --git a/libs/core/langchain_core/utils/html.py b/libs/core/langchain_core/utils/html.py\nindex 95e1c3c2f..26faef5a0 100644\n--- a/libs/core/langchain_core/utils/html.py\n+++ b/libs/core/langchain_core/utils/html.py\n@@ -83,7 +83,7 @@ def extract_sub_links(\n     for path in absolute_paths:\n         if any(path.startswith(exclude) for exclude in exclude_prefixes):\n             continue\n-        if prevent_outside and not path.startswith(base_url):\n+        if prevent_outside and (not path.startswith(base_url) or urlparse(path).netloc != urlparse(base_url).netloc):\n             continue\n         res.append(path)\n     return res\n"}
{"cve":"CVE-2023-6831:0708", "fix_patch": ""}
{"cve":"CVE-2024-3848:0708", "fix_patch": "diff --git a/mlflow/server/handlers.py b/mlflow/server/handlers.py\nindex 6d3204002..e472d4f30 100644\n--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -602,6 +602,13 @@ def _create_experiment():\n     parsed_artifact_locaion = urllib.parse.urlparse(request_message.artifact_location)\n     validate_query_string(parsed_artifact_locaion.query)\n \n+    # Validate fragment in artifact location to prevent attacks\n+    if parsed_artifact_locaion.fragment:\n+        raise MlflowException(\n+            \"'artifact_location' URL can't include fragment part.\",\n+            INVALID_PARAMETER_VALUE,\n+        )\n+\n     experiment_id = _get_tracking_store().create_experiment(\n         request_message.name, request_message.artifact_location, tags\n     )\n@@ -1741,6 +1748,13 @@ def _validate_source(source: str, run_id: str) -> None:\n     # raises an Exception.\n     _validate_non_local_source_contains_relative_paths(source)\n \n+    # Validate fragment in source to prevent attacks\n+    if urllib.parse.urlparse(source).fragment:\n+        raise MlflowException(\n+            \"'source' URL can't include fragment part.\",\n+            INVALID_PARAMETER_VALUE,\n+        )\n+\n \n @catch_mlflow_exception\n @_disable_if_artifacts_only\n"}
{"cve":"CVE-2021-21354:0708", "fix_patch": "diff --git a/pollbot/middlewares.py b/pollbot/middlewares.py\nindex 1c5d381..940721b 100644\n--- a/pollbot/middlewares.py\n+++ b/pollbot/middlewares.py\n@@ -61,7 +61,7 @@ async def handle_any(request, response):\n async def handle_404(request, response):\n     if 'json' not in response.headers['Content-Type']:\n         if request.path.endswith('/'):\n-            return web.HTTPFound(request.path.rstrip('/'))\n+            return web.HTTPFound(\"/\" + request.path.lstrip(\"/\").rstrip('/'))\n         return web.json_response({\n             \"status\": 404,\n             \"message\": \"Page '{}' not found\".format(request.path)\n"}
{"cve":"CVE-2021-4315:0708", "fix_patch": "diff --git a/psiturk/experiment.py b/psiturk/experiment.py\nindex a6904f6..50f221b 100644\n--- a/psiturk/experiment.py\n+++ b/psiturk/experiment.py\n@@ -325,7 +325,10 @@ def advertisement():\n         raise ExperimentError('hit_assign_worker_id_not_set_in_mturk')\n     hit_id = request.args['hitId']\n     assignment_id = request.args['assignmentId']\n-    mode = request.args['mode']\n+    mode = request.args.get('mode')\n+    if mode not in ['sandbox', 'live']:\n+        mode = 'sandbox'\n+\n     if hit_id[:5] == \"debug\":\n         debug_mode = True\n     else:\n@@ -403,7 +406,10 @@ def give_consent():\n     hit_id = request.args['hitId']\n     assignment_id = request.args['assignmentId']\n     worker_id = request.args['workerId']\n-    mode = request.args['mode']\n+    mode = request.args.get('mode')\n+    if mode not in ['sandbox', 'live']:\n+        mode = 'sandbox'\n+\n     with open('templates/consent.html', 'r') as temp_file:\n         consent_string = temp_file.read()\n     consent_string = insert_mode(consent_string, mode)\n"}
{"cve":"CVE-2023-39631:0708", "fix_patch": ""}
{"cve":"CVE-2024-21542:0708", "fix_patch": "diff --git a/luigi/__init__.py b/luigi/__init__.py\nindex 87401421..d5d80702 100644\n--- a/luigi/__init__.py\n+++ b/luigi/__init__.py\n@@ -24,6 +24,7 @@ from luigi import task\n from luigi.task import (\n     Task, Config, ExternalTask, WrapperTask, namespace, auto_namespace, DynamicRequirements,\n )\n+from luigi.safe_extractor import SafeExtractor\n \n from luigi import target\n from luigi.target import Target\n@@ -71,6 +72,7 @@ __all__ = [\n     'OptionalDictParameter', 'OptionalListParameter', 'OptionalTupleParameter',\n     'OptionalChoiceParameter', 'OptionalNumericalParameter', 'LuigiStatusCode',\n     '__version__',\n+    'SafeExtractor'\n ]\n \n if not configuration.get_config().has_option('core', 'autoload_range'):\ndiff --git a/luigi/contrib/lsf_runner.py b/luigi/contrib/lsf_runner.py\nindex 5a6c8b56..b1ccc855 100755\n--- a/luigi/contrib/lsf_runner.py\n+++ b/luigi/contrib/lsf_runner.py\n@@ -28,7 +28,7 @@ try:\n except ImportError:\n     import pickle\n import logging\n-import tarfile\n+from luigi.safe_extractor import SafeExtractor\n \n \n def do_work_on_compute_node(work_dir):\n@@ -52,10 +52,7 @@ def extract_packages_archive(work_dir):\n     curdir = os.path.abspath(os.curdir)\n \n     os.chdir(work_dir)\n-    tar = tarfile.open(package_file)\n-    for tarinfo in tar:\n-        tar.extract(tarinfo)\n-    tar.close()\n+    SafeExtractor(work_dir).safe_extract(package_file)\n     if '' not in sys.path:\n         sys.path.insert(0, '')\n \ndiff --git a/luigi/contrib/sge_runner.py b/luigi/contrib/sge_runner.py\nindex f0621fb4..019dbd4e 100755\n--- a/luigi/contrib/sge_runner.py\n+++ b/luigi/contrib/sge_runner.py\n@@ -36,7 +36,7 @@ import os\n import sys\n import pickle\n import logging\n-import tarfile\n+from luigi.safe_extractor import SafeExtractor\n \n \n def _do_work_on_compute_node(work_dir, tarball=True):\n@@ -64,10 +64,7 @@ def _extract_packages_archive(work_dir):\n     curdir = os.path.abspath(os.curdir)\n \n     os.chdir(work_dir)\n-    tar = tarfile.open(package_file)\n-    for tarinfo in tar:\n-        tar.extract(tarinfo)\n-    tar.close()\n+    SafeExtractor(work_dir).safe_extract(package_file)\n     if '' not in sys.path:\n         sys.path.insert(0, '')\n \ndiff --git a/luigi/safe_extractor.py b/luigi/safe_extractor.py\nnew file mode 100644\nindex 00000000..ceec79b2\n--- /dev/null\n+++ b/luigi/safe_extractor.py\n@@ -0,0 +1,23 @@\n+\n+import os\n+import tarfile\n+\n+class SafeExtractor:\n+    def __init__(self, path):\n+        self.path = path\n+\n+    def safe_extract(self, tar_path, numeric_owner=False):\n+        with tarfile.open(tar_path) as tar:\n+            for member in tar.getmembers():\n+                self._validate_member(member, self.path)\n+            tar.extractall(os.path.dirname(self.path), numeric_owner=numeric_owner)\n+\n+    def _validate_member(self, member, path):\n+        abs_path = os.path.abspath(os.path.dirname(path))\n+        abs_member_path = os.path.abspath(os.path.join(os.path.dirname(path), member.name))\n+\n+        if not abs_member_path.startswith(abs_path):\n+            raise RuntimeError(\n+                \"Path is outside the extraction directory\"\n+            )\n+\n"}
{"cve":"CVE-2017-0360:0708", "fix_patch": "diff --git a/trytond/tools/misc.py b/trytond/tools/misc.py\nindex 2b9a7f8f..799ecbca 100644\n--- a/trytond/tools/misc.py\n+++ b/trytond/tools/misc.py\n@@ -32,7 +32,7 @@ def file_open(name, mode=\"r\", subdir='modules', encoding=None):\n         \"Join paths and ensure it still below root\"\n         path = os.path.join(root, *paths)\n         path = os.path.normpath(path)\n-        if not path.startswith(root):\n+        if not path.startswith(root + os.sep):\n             raise IOError(\"Permission denied: %s\" % name)\n         return path\n \n"}
{"cve":"CVE-2018-18074:0708", "fix_patch": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex dd525e2a..9395c15f 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -228,6 +228,47 @@ class SessionRedirectMixin(object):\n                 url = self.get_redirect_target(resp)\n                 yield resp\n \n+    def should_strip_auth(self, old_url, new_url):\n+        \"\"\"\n+        Determines whether Authorization headers should be stripped from a\n+        request during a redirect.\n+\n+        :param old_url: The URL that we're redirecting from.\n+        :param new_url: The URL that we're redirecting to.\n+        \"\"\"\n+        old_parsed = urlparse(old_url)\n+        new_parsed = urlparse(new_url)\n+\n+        if old_parsed.hostname != new_parsed.hostname:\n+            return True\n+\n+        # Don't strip auth for redirects on the same host unless we're\n+        # downgrading from HTTPS to HTTP.\n+        if (\n+            new_parsed.scheme.lower() == 'http' and\n+            old_parsed.scheme.lower() == 'https'\n+        ):\n+            return True\n+\n+        # Check for port changes.\n+        if old_parsed.port and new_parsed.port and old_parsed.port != new_parsed.port:\n+            return True\n+        \n+        # Check for implicit-explicit port mismatches.\n+        if (old_parsed.port and not new_parsed.port and old_parsed.scheme == 'http' and old_parsed.port != 80):\n+            return True\n+            \n+        if (not old_parsed.port and new_parsed.port and new_parsed.scheme == 'http' and new_parsed.port != 80):\n+            return True\n+\n+        if (old_parsed.port and not new_parsed.port and old_parsed.scheme == 'https' and old_parsed.port != 443):\n+            return True\n+            \n+        if (not old_parsed.port and new_parsed.port and new_parsed.scheme == 'https' and new_parsed.port != 443):\n+            return True\n+\n+        return False\n+\n     def rebuild_auth(self, prepared_request, response):\n         \"\"\"When being redirected we may want to strip authentication from the\n         request to avoid leaking credentials. This method intelligently removes\n@@ -237,12 +278,9 @@ class SessionRedirectMixin(object):\n         url = prepared_request.url\n \n         if 'Authorization' in headers:\n-            # If we get redirected to a new host, we should strip out any\n-            # authentication headers.\n-            original_parsed = urlparse(response.request.url)\n-            redirect_parsed = urlparse(url)\n-\n-            if (original_parsed.hostname != redirect_parsed.hostname):\n+            # If we're redirecting to a different host, remove the\n+            # Authorization header.\n+            if self.should_strip_auth(response.request.url, url):\n                 del headers['Authorization']\n \n         # .netrc might have more auth for us on our new host.\n"}
{"cve":"CVE-2021-3987:0708", "fix_patch": "diff --git a/cps/shelf.py b/cps/shelf.py\nindex 2c6a17a2..52e83ee5 100644\n--- a/cps/shelf.py\n+++ b/cps/shelf.py\n@@ -224,6 +224,8 @@ def remove_from_shelf(shelf_id, book_id):\n @shelf.route(\"/shelf/create\", methods=[\"GET\", \"POST\"])\n @login_required\n def create_shelf():\n+    if not current_user.is_admin() and not config.config_allow_public_shelf_creation:\n+        return redirect(url_for('web.index'))\n     shelf = ub.Shelf()\n     return create_edit_shelf(shelf, page_title=_(u\"Create a Shelf\"), page=\"shelfcreate\")\n \n"}
{"cve":"CVE-2023-40267:0708", "fix_patch": "diff --git a/git/repo/base.py b/git/repo/base.py\nindex 2fc9cf1f..2f045c3f 100644\n--- a/git/repo/base.py\n+++ b/git/repo/base.py\n@@ -890,7 +890,7 @@ class Repo(object):\n     def active_branch(self) -> Head:\n         \"\"\"The name of the currently active branch.\n \n-        :raises\tTypeError: If HEAD is detached\n+        :raises TypeError: If HEAD is detached\n         :return: Head to the active branch\"\"\"\n         # reveal_type(self.head.reference)  # => Reference\n         return self.head.reference\n@@ -1203,8 +1203,13 @@ class Repo(object):\n \n         if not allow_unsafe_protocols:\n             Git.check_unsafe_protocols(str(url))\n-        if not allow_unsafe_options and multi_options:\n-            Git.check_unsafe_options(options=multi_options, unsafe_options=cls.unsafe_git_clone_options)\n+        if not allow_unsafe_options:\n+            all_options = list(multi_options) if multi_options is not None else []\n+            all_options.extend(f\"-{k}\" if len(k) == 1 else f\"--{k.replace('_', '-')}\" for k in kwargs)\n+            if all_options:\n+                Git.check_unsafe_options(\n+                    options=all_options, unsafe_options=cls.unsafe_git_clone_options\n+                )\n \n         proc = git.clone(\n             multi,\n"}
{"cve":"CVE-2021-23727:0708", "fix_patch": "diff --git a/celery/backends/base.py b/celery/backends/base.py\nindex ffbd1d030..367acc8d4 100644\n--- a/celery/backends/base.py\n+++ b/celery/backends/base.py\n@@ -6,6 +6,7 @@\n     using K/V semantics like _get and _put.\n \"\"\"\n import sys\n+import inspect\n import time\n import warnings\n from collections import namedtuple\n@@ -19,6 +20,9 @@ from kombu.serialization import registry as serializer_registry\n from kombu.utils.encoding import bytes_to_str, ensure_bytes, from_utf8\n from kombu.utils.url import maybe_sanitize_url\n \n+from celery import exceptions\n+\n+\n import celery.exceptions\n from celery import current_app, group, maybe_signature, states\n from celery._state import get_current_task\n@@ -353,6 +357,10 @@ class Backend:\n                         # The type can contain qualified name with parent classes\n                         for name in exc_type.split('.'):\n                             cls = getattr(cls, name)\n+                        if not inspect.isclass(cls) or not issubclass(cls, Exception):\n+                            raise celery.exceptions.SecurityError(\n+                                f'Expected an exception class, got {exc_module}.{exc_type} with payload {exc[\"exc_message\"]!r}'\n+                            )\n                     except (KeyError, AttributeError):\n                         cls = create_exception_cls(exc_type,\n                                                    celery.exceptions.__name__)\n"}
{"cve":"CVE-2022-24065:0708", "fix_patch": "diff --git a/cookiecutter/vcs.py b/cookiecutter/vcs.py\nindex 08cb2eb..b633ad5 100644\n--- a/cookiecutter/vcs.py\n+++ b/cookiecutter/vcs.py\n@@ -2,6 +2,7 @@\n import logging\n import os\n import subprocess  # nosec\n+import shlex\n from shutil import which\n \n from cookiecutter.exceptions import (\n@@ -98,8 +99,15 @@ def clone(repo_url, checkout=None, clone_to_dir='.', no_input=False):\n                 stderr=subprocess.STDOUT,\n             )\n             if checkout is not None:\n+                if repo_type == 'hg':\n+                    # Sanitize checkout parameter for mercurial to prevent command injection\n+                    # through arguments starting with a hyphen.\n+                    checkout_arg = ['--'] + shlex.split(checkout)\n+                else:\n+                    checkout_arg = [checkout]\n+\n                 subprocess.check_output(  # nosec\n-                    [repo_type, 'checkout', checkout],\n+                    [repo_type, 'checkout', *checkout_arg],\n                     cwd=repo_dir,\n                     stderr=subprocess.STDOUT,\n                 )\n"}
{"cve":"CVE-2023-26145:0708", "fix_patch": "diff --git a/src/pydash/helpers.py b/src/pydash/helpers.py\nindex 0467c9c..29c9b28 100644\n--- a/src/pydash/helpers.py\n+++ b/src/pydash/helpers.py\n@@ -176,6 +176,10 @@ def _base_get_item(obj, key, default=UNSET):\n \n \n def _base_get_object(obj, key, default=UNSET):\n+\n+    if isinstance(key, str) and key.startswith(\"_\"):\n+        raise KeyError(f'Object \"{repr(obj)}\" does not have key \"{key}\"')\n+\n     value = _base_get_item(obj, key, default=UNSET)\n     if value is UNSET:\n         value = default\n@@ -212,8 +216,13 @@ def base_set(obj, key, value, allow_override=True):\n                 # into the key index.\n                 obj[:] = (obj + [None] * key)[:key]\n             obj.append(value)\n-    elif (allow_override or not hasattr(obj, key)) and obj is not None:\n-        setattr(obj, key, value)\n+    elif obj is not None:\n+\n+        if isinstance(key, str) and key.startswith(\"_\"):\n+            raise KeyError(f'Object \"{repr(obj)}\" does not have key \"{key}\"')\n+\n+        if allow_override or not hasattr(obj, key):\n+            setattr(obj, key, value)\n \n     return obj\n \n"}
{"cve":"CVE-2020-25459:0708", "fix_patch": "diff --git a/federatedml/tree/hetero/hetero_decision_tree_guest.py b/federatedml/tree/hetero/hetero_decision_tree_guest.py\nindex 1f23ddc53..4fab08112 100644\n--- a/federatedml/tree/hetero/hetero_decision_tree_guest.py\n+++ b/federatedml/tree/hetero/hetero_decision_tree_guest.py\n@@ -1,4 +1,5 @@\n #!/usr/bin/env python    \n+import copy\n # -*- coding: utf-8 -*- \n \n #\n@@ -532,11 +533,17 @@ class HeteroDecisionTreeGuest(DecisionTree):\n     def sync_tree(self):\n         LOGGER.info(\"sync tree to host\")\n \n-        self.transfer_inst.tree.remote(self.tree_,\n+        tree_copy = copy.deepcopy(self.tree_)\n+        for node in tree_copy:\n+            node.weight = None\n+            node.sum_grad = None\n+            node.sum_hess = None\n+\n+        self.transfer_inst.tree.remote(tree_copy,\n                                        role=consts.HOST,\n                                        idx=-1)\n         \"\"\"\n-        federation.remote(obj=self.tree_,\n+        federation.remote(obj=tree_copy,\n                           name=self.transfer_inst.tree.name,\n                           tag=self.transfer_inst.generate_transferid(self.transfer_inst.tree),\n                           role=consts.HOST,\n"}
{"cve":"CVE-2022-39286:0708", "fix_patch": "diff --git a/jupyter_core/application.py b/jupyter_core/application.py\nindex cb46d12..79bf707 100644\n--- a/jupyter_core/application.py\n+++ b/jupyter_core/application.py\n@@ -54,6 +54,10 @@ _jupyter_flags = {\n         {\"JupyterApp\": {\"answer_yes\": True}},\n         \"Answer yes to any questions instead of prompting.\",\n     ),\n+    \"allow-custom-config\": (\n+        {\"JupyterApp\": {\"allow_custom_config\": True}},\n+        \"Allow custom config files to be loaded from the current working directory.\",\n+    ),\n }\n base_flags.update(_jupyter_flags)\n \n@@ -79,6 +83,12 @@ class JupyterApp(Application):\n     def _jupyter_path_default(self):\n         return jupyter_path()\n \n+    allow_custom_config = Bool(\n+        False,\n+        config=True,\n+        help=\"\"\"Allow custom config files to be loaded from the current working directory.\"\"\",\n+    )\n+\n     config_dir = Unicode()\n \n     def _config_dir_default(self):\n@@ -89,7 +99,9 @@ class JupyterApp(Application):\n         path = jupyter_config_path()\n         if self.config_dir not in path:\n             path.insert(0, self.config_dir)\n-        path.insert(0, os.getcwd())\n+\n+        if self.allow_custom_config:\n+            path.insert(0, os.getcwd())\n         return path\n \n     data_dir = Unicode()\n"}
{"cve":"CVE-2019-10856:0708", "fix_patch": "diff --git a/notebook/auth/login.py b/notebook/auth/login.py\nindex 8dbd6112f..2aafa659c 100644\n--- a/notebook/auth/login.py\n+++ b/notebook/auth/login.py\n@@ -39,12 +39,13 @@ class LoginHandler(IPythonHandler):\n         \"\"\"\n         if default is None:\n             default = self.base_url\n+\n         # protect chrome users from mishandling unescaped backslashes.\n         # \\ is not valid in urls, but some browsers treat it as /\n         # instead of %5C, causing `\\\\` to behave as `//`\n         url = url.replace(\"\\\\\", \"%5C\")\n         parsed = urlparse(url)\n-        if parsed.netloc or not (parsed.path + '/').startswith(self.base_url):\n+        if not parsed.netloc and parsed.path.startswith('//') or not (parsed.path + '/').startswith(self.base_url):\n             # require that next_url be absolute path within our path\n             allow = False\n             # OR pass our cross-origin check\n"}
{"cve":"CVE-2020-26215:0708", "fix_patch": "diff --git a/notebook/base/handlers.py b/notebook/base/handlers.py\nindex 743f7bac7..93f4407a6 100755\n--- a/notebook/base/handlers.py\n+++ b/notebook/base/handlers.py\n@@ -859,7 +859,10 @@ class TrailingSlashHandler(web.RequestHandler):\n     \"\"\"\n     \n     def get(self):\n-        self.redirect(self.request.uri.rstrip('/'))\n+        path = self.request.path.rstrip('/')\n+        if self.request.query:\n+            path = path + '?' + self.request.query\n+        self.redirect(path)\n     \n     post = put = get\n \n"}
{"cve":"CVE-2021-39163:0708", "fix_patch": "diff --git a/synapse/groups/groups_server.py b/synapse/groups/groups_server.py\nindex 3dc55ab86..21297959e 100644\n--- a/synapse/groups/groups_server.py\n+++ b/synapse/groups/groups_server.py\n@@ -326,12 +326,13 @@ class GroupsServerWorkerHandler:\n         This returns rooms in order of decreasing number of joined users\n         \"\"\"\n \n-        await self.check_group_is_ours(group_id, requester_user_id, and_exists=True)\n-\n         is_user_in_group = await self.store.is_user_in_group(\n             requester_user_id, group_id\n         )\n \n+        if not is_user_in_group:\n+            return {\"chunk\": [], \"total_room_count_estimate\": 0}\n+\n         room_results = await self.store.get_rooms_in_group(\n             group_id, include_private=is_user_in_group\n         )\n"}
{"cve":"CVE-2022-0577:0708", "fix_patch": "diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py\nindex 4053fecc5..fa0dcc157 100644\n--- a/scrapy/downloadermiddlewares/redirect.py\n+++ b/scrapy/downloadermiddlewares/redirect.py\n@@ -50,6 +50,8 @@ class BaseRedirectMiddleware:\n         redirected = request.replace(url=redirect_url, method='GET', body='')\n         redirected.headers.pop('Content-Type', None)\n         redirected.headers.pop('Content-Length', None)\n+        redirected.headers.pop('Cookie', None)\n+        redirected.headers.pop('Authorization', None)\n         return redirected\n \n \n@@ -85,8 +87,6 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n         redirected = self._redirect_request_using_get(request, redirected_url)\n         return self._redirect(redirected, request, spider, response.status)\n-\n-\n class MetaRefreshMiddleware(BaseRedirectMiddleware):\n \n     enabled_setting = 'METAREFRESH_ENABLED'\n"}
{"cve":"CVE-2021-41125:0708", "fix_patch": "diff --git a/scrapy/downloadermiddlewares/httpauth.py b/scrapy/downloadermiddlewares/httpauth.py\nindex 7aa7a62bc..c4f24f855 100644\n--- a/scrapy/downloadermiddlewares/httpauth.py\n+++ b/scrapy/downloadermiddlewares/httpauth.py\n@@ -5,6 +5,7 @@ See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n from w3lib.http import basic_auth_header\n+from w3lib.url import urlsplit\n \n from scrapy import signals\n \n@@ -24,8 +25,21 @@ class HttpAuthMiddleware(object):\n         pwd = getattr(spider, 'http_pass', '')\n         if usr or pwd:\n             self.auth = basic_auth_header(usr, pwd)\n+            self.auth_domain = getattr(spider, 'http_auth_domain', None)\n \n     def process_request(self, request, spider):\n         auth = getattr(self, 'auth', None)\n         if auth and b'Authorization' not in request.headers:\n-            request.headers[b'Authorization'] = auth\n+            if self.auth_domain is None:\n+                # Fallback to legacy behavior: only send credentials to the\n+                # same domain as the first request.\n+                if not hasattr(self, '_legacy_auth_domain'):\n+                    self._legacy_auth_domain = urlsplit(request.url).hostname\n+                if self._legacy_auth_domain == urlsplit(request.url).hostname:\n+                    request.headers[b'Authorization'] = auth\n+            else:\n+                hostname = urlsplit(request.url).hostname or ''\n+                if hostname == self.auth_domain or hostname.endswith(\n+                    '.' + self.auth_domain\n+                ):\n+                    request.headers[b'Authorization'] = auth\n"}
{"cve":"CVE-2021-21360:0708", "fix_patch": "diff --git a/src/Products/GenericSetup/context.py b/src/Products/GenericSetup/context.py\nindex 1c8ba1d..0c00fbe 100644\n--- a/src/Products/GenericSetup/context.py\n+++ b/src/Products/GenericSetup/context.py\n@@ -49,7 +49,7 @@ from .interfaces import IExportContext\n from .interfaces import IImportContext\n from .interfaces import ISetupEnviron\n from .interfaces import IWriteLogger\n-from .permissions import ManagePortal\n+from .permissions import ManagePortal, ACCESS_CONTENTS_PERMISSIONS\n \n \n @implementer(IWriteLogger)\n@@ -559,6 +559,10 @@ class SnapshotExportContext(BaseContext):\n             if element not in current.objectIds():\n                 # No Unicode IDs!\n                 current._setObject(str(element), Folder(element))\n+                new_folder = current._getOb(element)\n+                new_folder.manage_permission(ACCESS_CONTENTS_PERMISSIONS, ('Manager', 'Owner'), 0)\n+            new_folder = current._getOb(element)\n+            new_folder.manage_permission(ACCESS_CONTENTS_PERMISSIONS, ('Manager', 'Owner'), 0)\n \n             current = current._getOb(element)\n \ndiff --git a/src/Products/GenericSetup/permissions.py b/src/Products/GenericSetup/permissions.py\nindex c4c72a7..5bb574e 100644\n--- a/src/Products/GenericSetup/permissions.py\n+++ b/src/Products/GenericSetup/permissions.py\n@@ -14,3 +14,4 @@\n \"\"\"\n \n ManagePortal = 'Manage Site'\n+ACCESS_CONTENTS_PERMISSIONS = 'Access contents information'\ndiff --git a/src/Products/GenericSetup/tool.py b/src/Products/GenericSetup/tool.py\nindex b4e5861..0f6a58c 100644\n--- a/src/Products/GenericSetup/tool.py\n+++ b/src/Products/GenericSetup/tool.py\n@@ -50,7 +50,7 @@ from .interfaces import BASE\n from .interfaces import EXTENSION\n from .interfaces import SKIPPED_FILES\n from .interfaces import ISetupTool\n-from .permissions import ManagePortal\n+from .permissions import ManagePortal, ACCESS_CONTENTS_PERMISSIONS\n from .registry import ExportStepRegistry\n from .registry import ImportStepRegistry\n from .registry import ToolsetRegistry\n"}
{"cve":"CVE-2023-32303:0708", "fix_patch": "diff --git a/planet/auth.py b/planet/auth.py\nindex e50f1af..746ab04 100644\n--- a/planet/auth.py\n+++ b/planet/auth.py\n@@ -16,6 +16,7 @@\n from __future__ import annotations  # https://stackoverflow.com/a/33533514\n import abc\n import json\n+import os\n import logging\n import os\n import pathlib\n@@ -228,6 +229,12 @@ class _SecretFile:\n \n     def __init__(self, path):\n         self.path = path\n+        if os.path.exists(self.path):\n+            mode = os.stat(self.path).st_mode\n+            if (mode & 0o077):\n+                LOGGER.warning(\n+                    f'Permissions of {self.path} are too open and will be restricted to owner-only.')\n+                os.chmod(self.path, mode & 0o700)\n \n     def write(self, contents: dict):\n         try:\n@@ -242,6 +249,7 @@ class _SecretFile:\n         LOGGER.debug(f'Writing to {self.path}')\n         with open(self.path, 'w') as fp:\n             fp.write(json.dumps(contents))\n+        os.chmod(self.path, 0o600)\n \n     def read(self) -> dict:\n         LOGGER.debug(f'Reading from {self.path}')\n"}
{"cve":"CVE-2022-21712:0708", "fix_patch": "diff --git a/src/twisted/web/client.py b/src/twisted/web/client.py\nindex a1295c2705..70dcdca263 100644\n--- a/src/twisted/web/client.py\n+++ b/src/twisted/web/client.py\n@@ -1,4 +1,5 @@\n # -*- test-case-name: twisted.web.test.test_webclient,twisted.web.test.test_agent -*-\n+from urllib.parse import urlparse\n # Copyright (c) Twisted Matrix Laboratories.\n # See LICENSE for details.\n \n@@ -2141,7 +2142,7 @@ class RedirectAgent:\n     ]\n     _seeOtherResponses = [http.SEE_OTHER]\n \n-    def __init__(self, agent, redirectLimit=20):\n+    def __init__(self, agent, redirectLimit=20, sensitiveHeaderNames=(b\"authorization\", b\"cookie\")):\n         self._agent = agent\n         self._redirectLimit = redirectLimit\n \n"}
{"cve":"CVE-2022-4724:0708", "fix_patch": ""}
{"cve":"CVE-2023-32309:0708", "fix_patch": "diff --git a/pymdownx/snippets.py b/pymdownx/snippets.py\nindex e7ae2e98..5a19131e 100644\n--- a/pymdownx/snippets.py\n+++ b/pymdownx/snippets.py\n@@ -1,403 +1,413 @@\n-\"\"\"\n-Snippet ---8<---.\n-\n-pymdownx.snippet\n-Inject snippets\n-\n-MIT license.\n-\n-Copyright (c) 2017 Isaac Muse <isaacmuse@gmail.com>\n-\n-Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n-documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation\n-the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n-and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n-\n-The above copyright notice and this permission notice shall be included in all copies or substantial portions\n-of the Software.\n-\n-THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n-TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n-THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n-CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n-DEALINGS IN THE SOFTWARE.\n-\"\"\"\n-from markdown import Extension\n-from markdown.preprocessors import Preprocessor\n-import functools\n-import urllib\n-import re\n-import codecs\n-import os\n-from . import util\n-import textwrap\n-\n-MI = 1024 * 1024  # mebibyte (MiB)\n-DEFAULT_URL_SIZE = MI * 32\n-DEFAULT_URL_TIMEOUT = 10.0  # in seconds\n-DEFAULT_URL_REQUEST_HEADERS = {}\n-\n-\n-class SnippetMissingError(Exception):\n-    \"\"\"Snippet missing exception.\"\"\"\n-\n-\n-class SnippetPreprocessor(Preprocessor):\n-    \"\"\"Handle snippets in Markdown content.\"\"\"\n-\n-    RE_ALL_SNIPPETS = re.compile(\n-        r'''(?x)\n-        ^(?P<space>[ \\t]*)\n-        (?P<escape>;*)\n-        (?P<all>\n-            (?P<inline_marker>-{1,}8<-{1,}[ \\t]+)\n-            (?P<snippet>(?:\"(?:\\\\\"|[^\"\\n\\r])+?\"|'(?:\\\\'|[^'\\n\\r])+?'))(?![ \\t]) |\n-            (?P<block_marker>-{1,}8<-{1,})(?![ \\t])\n-        )\\r?$\n-        '''\n-    )\n-\n-    RE_SNIPPET = re.compile(\n-        r'''(?x)\n-        ^(?P<space>[ \\t]*)\n-        (?P<snippet>.*?)\\r?$\n-        '''\n-    )\n-\n-    RE_SNIPPET_SECTION = re.compile(\n-        r'''(?xi)\n-        ^(?P<pre>.*?)\n-        (?P<escape>;*)\n-        (?P<inline_marker>-{1,}8<-{1,}[ \\t]+)\n-        (?P<section>\\[[ \\t]*(?P<type>start|end)[ \\t]*:[ \\t]*(?P<name>[a-z][-_0-9a-z]*)[ \\t]*\\])\n-        (?P<post>.*?)$\n-        '''\n-    )\n-\n-    RE_SNIPPET_FILE = re.compile(r'(?i)(.*?)(?:(:[0-9]*)?(:[0-9]*)?|(:[a-z][-_0-9a-z]*)?)$')\n-\n-    def __init__(self, config, md):\n-        \"\"\"Initialize.\"\"\"\n-\n-        base = config.get('base_path')\n-        if isinstance(base, str):\n-            base = [base]\n-        self.base_path = base\n-        self.encoding = config.get('encoding')\n-        self.check_paths = config.get('check_paths')\n-        self.auto_append = config.get('auto_append')\n-        self.url_download = config['url_download']\n-        self.url_max_size = config['url_max_size']\n-        self.url_timeout = config['url_timeout']\n-        self.url_request_headers = config['url_request_headers']\n-        self.dedent_subsections = config['dedent_subsections']\n-        self.tab_length = md.tab_length\n-        super(SnippetPreprocessor, self).__init__()\n-\n-    def extract_section(self, section, lines):\n-        \"\"\"Extract the specified section from the lines.\"\"\"\n-\n-        new_lines = []\n-        start = False\n-        found = False\n-        for l in lines:\n-\n-            # Found a snippet section marker with our specified name\n-            m = self.RE_SNIPPET_SECTION.match(l)\n-\n-            # Handle escaped line\n-            if m and start and m.group('escape'):\n-                l = (\n-                    m.group('pre') + m.group('escape').replace(';', '', 1) + m.group('inline_marker') +\n-                    m.group('section') + m.group('post')\n-                )\n-\n-            # Found a section we are looking for.\n-            elif m is not None and m.group('name') == section:\n-\n-                # We found the start\n-                if not start and m.group('type') == 'start':\n-                    start = True\n-                    found = True\n-                    continue\n-\n-                # Ignore duplicate start\n-                elif start and m.group('type') == 'start':\n-                    continue\n-\n-                # We found the end\n-                elif start and m.group('type') == 'end':\n-                    start = False\n-                    break\n-\n-                # We found an end, but no start\n-                else:\n-                    break\n-\n-            # Found a section we don't care about, so ignore it.\n-            elif m and start:\n-                continue\n-\n-            # We are currently in a section, so append the line\n-            if start:\n-                new_lines.append(l)\n-\n-        if not found and self.check_paths:\n-            raise SnippetMissingError(\"Snippet section '{}' could not be located\".format(section))\n-\n-        return self.dedent(new_lines) if self.dedent_subsections else new_lines\n-\n-    def dedent(self, lines):\n-        \"\"\"De-indent lines.\"\"\"\n-\n-        return textwrap.dedent('\\n'.join(lines)).split('\\n')\n-\n-    def get_snippet_path(self, path):\n-        \"\"\"Get snippet path.\"\"\"\n-\n-        snippet = None\n-        for base in self.base_path:\n-            if os.path.exists(base):\n-                if os.path.isdir(base):\n-                    filename = os.path.join(base, path)\n-                    if os.path.exists(filename):\n-                        snippet = filename\n-                        break\n-                else:\n-                    basename = os.path.basename(base)\n-                    dirname = os.path.dirname(base)\n-                    if basename.lower() == path.lower():\n-                        filename = os.path.join(dirname, path)\n-                        if os.path.exists(filename):\n-                            snippet = filename\n-                            break\n-        return snippet\n-\n-    @functools.lru_cache()\n-    def download(self, url):\n-        \"\"\"\n-        Actually download the snippet pointed to by the passed URL.\n-\n-        The most recently used files are kept in a cache until the next reset.\n-        \"\"\"\n-\n-        http_request = urllib.request.Request(url, headers=self.url_request_headers)\n-        timeout = None if self.url_timeout == 0 else self.url_timeout\n-        with urllib.request.urlopen(http_request, timeout=timeout) as response:\n-\n-            # Fail if status is not OK\n-            status = response.status if util.PY39 else response.code\n-            if status != 200:\n-                raise SnippetMissingError(\"Cannot download snippet '{}'\".format(url))\n-\n-            # We provide some basic protection against absurdly large files.\n-            # 32MB is chosen as an arbitrary upper limit. This can be raised if desired.\n-            length = response.headers.get(\"content-length\")\n-            if length is None:\n-                raise ValueError(\"Missing content-length header\")\n-            content_length = int(length)\n-\n-            if self.url_max_size != 0 and content_length >= self.url_max_size:\n-                raise ValueError(\"refusing to read payloads larger than or equal to {}\".format(self.url_max_size))\n-\n-            # Nothing to return\n-            if content_length == 0:\n-                return ['']\n-\n-            # Process lines\n-            return [l.decode(self.encoding).rstrip('\\r\\n') for l in response.readlines()]\n-\n-    def parse_snippets(self, lines, file_name=None, is_url=False):\n-        \"\"\"Parse snippets snippet.\"\"\"\n-\n-        if file_name:\n-            # Track this file.\n-            self.seen.add(file_name)\n-\n-        new_lines = []\n-        inline = False\n-        block = False\n-        for line in lines:\n-            # Check for snippets on line\n-            inline = False\n-            m = self.RE_ALL_SNIPPETS.match(line)\n-            if m:\n-                if m.group('escape'):\n-                    # The snippet has been escaped, replace first `;` and continue.\n-                    new_lines.append(line.replace(';', '', 1))\n-                    continue\n-\n-                if block and m.group('inline_marker'):\n-                    # Don't use inline notation directly under a block.\n-                    # It's okay if inline is used again in sub file though.\n-                    continue\n-\n-                elif m.group('inline_marker'):\n-                    # Inline\n-                    inline = True\n-\n-                else:\n-                    # Block\n-                    block = not block\n-                    continue\n-\n-            elif not block:\n-                # Not in snippet, and we didn't find an inline,\n-                # so just a normal line\n-                new_lines.append(line)\n-                continue\n-\n-            if block and not inline:\n-                # We are in a block and we didn't just find a nested inline\n-                # So check if a block path\n-                m = self.RE_SNIPPET.match(line)\n-\n-            if m:\n-                # Get spaces and snippet path.  Remove quotes if inline.\n-                space = m.group('space').expandtabs(self.tab_length)\n-                path = m.group('snippet')[1:-1].strip() if inline else m.group('snippet').strip()\n-\n-                if not inline:\n-                    # Block path handling\n-                    if not path:\n-                        # Empty path line, insert a blank line\n-                        new_lines.append('')\n-                        continue\n-\n-                # Ignore commented out lines\n-                if path.startswith(';'):\n-                    continue\n-\n-                # Get line numbers (if specified)\n-                end = None\n-                start = None\n-                section = None\n-                m = self.RE_SNIPPET_FILE.match(path)\n-                path = m.group(1).strip()\n-                # Looks like we have an empty file and only lines specified\n-                if not path:\n-                    if self.check_paths:\n-                        raise SnippetMissingError(\"Snippet at path '{}' could not be found\".format(path))\n-                    else:\n-                        continue\n-                ending = m.group(3)\n-                if ending and len(ending) > 1:\n-                    end = int(ending[1:])\n-                starting = m.group(2)\n-                if starting and len(starting) > 1:\n-                    start = max(1, int(starting[1:]) - 1)\n-                section_name = m.group(4)\n-                if section_name:\n-                    section = section_name[1:]\n-\n-                # Ignore path links if we are in external, downloaded content\n-                is_link = path.lower().startswith(('https://', 'http://'))\n-                if is_url and not is_link:\n-                    continue\n-\n-                # If this is a link, and we are allowing URLs, set `url` to true.\n-                # Make sure we don't process `path` as a local file reference.\n-                url = self.url_download and is_link\n-                snippet = self.get_snippet_path(path) if not url else path\n-\n-                if snippet:\n-\n-                    # This is in the stack and we don't want an infinite loop!\n-                    if snippet in self.seen:\n-                        continue\n-\n-                    if not url:\n-                        # Read file content\n-                        with codecs.open(snippet, 'r', encoding=self.encoding) as f:\n-                            s_lines = [l.rstrip('\\r\\n') for l in f]\n-                            if start is not None or end is not None:\n-                                s = slice(start, end)\n-                                s_lines = self.dedent(s_lines[s]) if self.dedent_subsections else s_lines[s]\n-                            elif section:\n-                                s_lines = self.extract_section(section, s_lines)\n-                    else:\n-                        # Read URL content\n-                        try:\n-                            s_lines = self.download(snippet)\n-                            if start is not None or end is not None:\n-                                s = slice(start, end)\n-                                s_lines = self.dedent(s_lines[s]) if self.dedent_subsections else s_lines[s]\n-                            elif section:\n-                                s_lines = self.extract_section(section, s_lines)\n-                        except SnippetMissingError:\n-                            if self.check_paths:\n-                                raise\n-                            s_lines = []\n-\n-                    # Process lines looking for more snippets\n-                    new_lines.extend(\n-                        [\n-                            space + l2 for l2 in self.parse_snippets(\n-                                s_lines,\n-                                snippet,\n-                                is_url=url\n-                            )\n-                        ]\n-                    )\n-\n-                elif self.check_paths:\n-                    raise SnippetMissingError(\"Snippet at path '{}' could not be found\".format(path))\n-\n-        # Pop the current file name out of the cache\n-        if file_name:\n-            self.seen.remove(file_name)\n-\n-        return new_lines\n-\n-    def run(self, lines):\n-        \"\"\"Process snippets.\"\"\"\n-\n-        self.seen = set()\n-        if self.auto_append:\n-            lines.extend(\"\\n\\n-8<-\\n{}\\n-8<-\\n\".format('\\n\\n'.join(self.auto_append)).split('\\n'))\n-\n-        return self.parse_snippets(lines)\n-\n-\n-class SnippetExtension(Extension):\n-    \"\"\"Snippet extension.\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        \"\"\"Initialize.\"\"\"\n-\n-        self.config = {\n-            'base_path': [[\".\"], \"Base path for snippet paths - Default: [\\\".\\\"]\"],\n-            'encoding': [\"utf-8\", \"Encoding of snippets - Default: \\\"utf-8\\\"\"],\n-            'check_paths': [False, \"Make the build fail if a snippet can't be found - Default: \\\"False\\\"\"],\n-            \"auto_append\": [\n-                [],\n-                \"A list of snippets (relative to the 'base_path') to auto append to the Markdown content - Default: []\"\n-            ],\n-            'url_download': [False, \"Download external URLs as snippets - Default: \\\"False\\\"\"],\n-            'url_max_size': [DEFAULT_URL_SIZE, \"External URL max size (0 means no limit)- Default: 32 MiB\"],\n-            'url_timeout': [DEFAULT_URL_TIMEOUT, 'Defualt URL timeout (0 means no timeout) - Default: 10 sec'],\n-            'url_request_headers': [DEFAULT_URL_REQUEST_HEADERS, \"Extra request Headers - Default: {}\"],\n-            'dedent_subsections': [False, \"Dedent subsection extractions e.g. 'sections' and/or 'lines'.\"]\n-        }\n-\n-        super(SnippetExtension, self).__init__(*args, **kwargs)\n-\n-    def extendMarkdown(self, md):\n-        \"\"\"Register the extension.\"\"\"\n-\n-        self.md = md\n-        md.registerExtension(self)\n-        config = self.getConfigs()\n-        snippet = SnippetPreprocessor(config, md)\n-        md.preprocessors.register(snippet, \"snippet\", 32)\n-\n-    def reset(self):\n-        \"\"\"Reset.\"\"\"\n-\n-        self.md.preprocessors['snippet'].download.cache_clear()\n-\n-\n-def makeExtension(*args, **kwargs):\n-    \"\"\"Return extension.\"\"\"\n-\n-    return SnippetExtension(*args, **kwargs)\n+\"\"\"\n+Snippet ---8<---.\n+\n+pymdownx.snippet\n+Inject snippets\n+\n+MIT license.\n+\n+Copyright (c) 2017 Isaac Muse <isaacmuse@gmail.com>\n+\n+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n+documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation\n+the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n+and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n+\n+The above copyright notice and this permission notice shall be included in all copies or substantial portions\n+of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n+TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n+THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n+CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n+DEALINGS IN THE SOFTWARE.\n+\"\"\"\n+from markdown import Extension\n+from markdown.preprocessors import Preprocessor\n+import functools\n+import urllib\n+import re\n+import codecs\n+import os\n+from . import util\n+import textwrap\n+\n+MI = 1024 * 1024  # mebibyte (MiB)\n+DEFAULT_URL_SIZE = MI * 32\n+DEFAULT_URL_TIMEOUT = 10.0  # in seconds\n+DEFAULT_URL_REQUEST_HEADERS = {}\n+\n+\n+class SnippetMissingError(Exception):\n+    \"\"\"Snippet missing exception.\"\"\"\n+\n+\n+class SnippetPreprocessor(Preprocessor):\n+    \"\"\"Handle snippets in Markdown content.\"\"\"\n+\n+    RE_ALL_SNIPPETS = re.compile(\n+        r'''(?x)\n+        ^(?P<space>[ \\t]*)\n+        (?P<escape>;*)\n+        (?P<all>\n+            (?P<inline_marker>-{1,}8<-{1,}[ \\t]+)\n+            (?P<snippet>(?:\"(?:\\\\\"|[^\"\\n\\r])+?\"|'(?:\\\\'|[^'\\n\\r])+?'))(?![ \\t]) |\n+            (?P<block_marker>-{1,}8<-{1,})(?![ \\t])\n+        )\\r?$\n+        '''\n+    )\n+\n+    RE_SNIPPET = re.compile(\n+        r'''(?x)\n+        ^(?P<space>[ \\t]*)\n+        (?P<snippet>.*?)\\r?$\n+        '''\n+    )\n+\n+    RE_SNIPPET_SECTION = re.compile(\n+        r'''(?xi)\n+        ^(?P<pre>.*?)\n+        (?P<escape>;*)\n+        (?P<inline_marker>-{1,}8<-{1,}[ \\t]+)\n+        (?P<section>\\[[ \\t]*(?P<type>start|end)[ \\t]*:[ \\t]*(?P<name>[a-z][-_0-9a-z]*)[ \\t]*\\])\n+        (?P<post>.*?)$\n+        '''\n+    )\n+\n+    RE_SNIPPET_FILE = re.compile(r'(?i)(.*?)(?:(:[0-9]*)?(:[0-9]*)?|(:[a-z][-_0-9a-z]*)?)$')\n+\n+    def __init__(self, config, md):\n+        \"\"\"Initialize.\"\"\"\n+\n+        base = config.get('base_path')\n+        if isinstance(base, str):\n+            base = [base]\n+        self.base_path = base\n+        self.encoding = config.get('encoding')\n+        self.check_paths = config.get('check_paths')\n+        self.auto_append = config.get('auto_append')\n+        self.url_download = config['url_download']\n+        self.url_max_size = config['url_max_size']\n+        self.url_timeout = config['url_timeout']\n+        self.url_request_headers = config['url_request_headers']\n+        self.dedent_subsections = config['dedent_subsections']\n+        self.restrict_base_path = config['restrict_base_path']\n+        self.tab_length = md.tab_length\n+        super(SnippetPreprocessor, self).__init__()\n+\n+    def extract_section(self, section, lines):\n+        \"\"\"Extract the specified section from the lines.\"\"\"\n+\n+        new_lines = []\n+        start = False\n+        found = False\n+        for l in lines:\n+\n+            # Found a snippet section marker with our specified name\n+            m = self.RE_SNIPPET_SECTION.match(l)\n+\n+            # Handle escaped line\n+            if m and start and m.group('escape'):\n+                l = (\n+                    m.group('pre') + m.group('escape').replace(';', '', 1) + m.group('inline_marker') +\n+                    m.group('section') + m.group('post')\n+                )\n+\n+            # Found a section we are looking for.\n+            elif m is not None and m.group('name') == section:\n+\n+                # We found the start\n+                if not start and m.group('type') == 'start':\n+                    start = True\n+                    found = True\n+                    continue\n+\n+                # Ignore duplicate start\n+                elif start and m.group('type') == 'start':\n+                    continue\n+\n+                # We found the end\n+                elif start and m.group('type') == 'end':\n+                    start = False\n+                    break\n+\n+                # We found an end, but no start\n+                else:\n+                    break\n+\n+            # Found a section we don't care about, so ignore it.\n+            elif m and start:\n+                continue\n+\n+            # We are currently in a section, so append the line\n+            if start:\n+                new_lines.append(l)\n+\n+        if not found and self.check_paths:\n+            raise SnippetMissingError(\"Snippet section '{}' could not be located\".format(section))\n+\n+        return self.dedent(new_lines) if self.dedent_subsections else new_lines\n+\n+    def dedent(self, lines):\n+        \"\"\"De-indent lines.\"\"\"\n+\n+        return textwrap.dedent('\\n'.join(lines)).split('\\n')\n+\n+    def get_snippet_path(self, path):\n+        \"\"\"Get snippet path.\"\"\"\n+\n+        snippet = None\n+        for base in self.base_path:\n+            if self.restrict_base_path:\n+                base = os.path.abspath(base)\n+\n+            if os.path.exists(base):\n+                if os.path.isdir(base):\n+                    filename = os.path.join(base, path)\n+                    if self.restrict_base_path:\n+                        if os.path.commonpath([os.path.abspath(filename), base]) != base:\n+                            continue\n+                else:\n+                    if self.restrict_base_path:\n+                        base = os.path.dirname(base)\n+                        if os.path.commonpath([os.path.abspath(path), base]) != base:\n+                            continue\n+\n+                    filename = base if os.path.basename(base).lower() == path.lower() else os.path.join(\n+                        os.path.dirname(base), path\n+                    )\n+\n+                if os.path.exists(filename):\n+                    snippet = filename\n+                    break\n+        return snippet\n+\n+    @functools.lru_cache()\n+    def download(self, url):\n+        \"\"\"\n+        Actually download the snippet pointed to by the passed URL.\n+\n+        The most recently used files are kept in a cache until the next reset.\n+        \"\"\"\n+\n+        http_request = urllib.request.Request(url, headers=self.url_request_headers)\n+        timeout = None if self.url_timeout == 0 else self.url_timeout\n+        with urllib.request.urlopen(http_request, timeout=timeout) as response:\n+\n+            # Fail if status is not OK\n+            status = response.status if util.PY39 else response.code\n+            if status != 200:\n+                raise SnippetMissingError(\"Cannot download snippet '{}'\".format(url))\n+\n+            # We provide some basic protection against absurdly large files.\n+            # 32MB is chosen as an arbitrary upper limit. This can be raised if desired.\n+            length = response.headers.get(\"content-length\")\n+            if length is None:\n+                raise ValueError(\"Missing content-length header\")\n+            content_length = int(length)\n+\n+            if self.url_max_size != 0 and content_length >= self.url_max_size:\n+                raise ValueError(\"refusing to read payloads larger than or equal to {}\".format(self.url_max_size))\n+\n+            # Nothing to return\n+            if content_length == 0:\n+                return ['']\n+\n+            # Process lines\n+            return [l.decode(self.encoding).rstrip('\\r\\n') for l in response.readlines()]\n+\n+    def parse_snippets(self, lines, file_name=None, is_url=False):\n+        \"\"\"Parse snippets snippet.\"\"\"\n+\n+        if file_name:\n+            # Track this file.\n+            self.seen.add(file_name)\n+\n+        new_lines = []\n+        inline = False\n+        block = False\n+        for line in lines:\n+            # Check for snippets on line\n+            inline = False\n+            m = self.RE_ALL_SNIPPETS.match(line)\n+            if m:\n+                if m.group('escape'):\n+                    # The snippet has been escaped, replace first `;` and continue.\n+                    new_lines.append(line.replace(';', '', 1))\n+                    continue\n+\n+                if block and m.group('inline_marker'):\n+                    # Don't use inline notation directly under a block.\n+                    # It's okay if inline is used again in sub file though.\n+                    continue\n+\n+                elif m.group('inline_marker'):\n+                    # Inline\n+                    inline = True\n+\n+                else:\n+                    # Block\n+                    block = not block\n+                    continue\n+\n+            elif not block:\n+                # Not in snippet, and we didn't find an inline,\n+                # so just a normal line\n+                new_lines.append(line)\n+                continue\n+\n+            if block and not inline:\n+                # We are in a block and we didn't just find a nested inline\n+                # So check if a block path\n+                m = self.RE_SNIPPET.match(line)\n+\n+            if m:\n+                # Get spaces and snippet path.  Remove quotes if inline.\n+                space = m.group('space').expandtabs(self.tab_length)\n+                path = m.group('snippet')[1:-1].strip() if inline else m.group('snippet').strip()\n+\n+                if not inline:\n+                    # Block path handling\n+                    if not path:\n+                        # Empty path line, insert a blank line\n+                        new_lines.append('')\n+                        continue\n+\n+                # Ignore commented out lines\n+                if path.startswith(';'):\n+                    continue\n+\n+                # Get line numbers (if specified)\n+                end = None\n+                start = None\n+                section = None\n+                m = self.RE_SNIPPET_FILE.match(path)\n+                path = m.group(1).strip()\n+                # Looks like we have an empty file and only lines specified\n+                if not path:\n+                    if self.check_paths:\n+                        raise SnippetMissingError(\"Snippet at path '{}' could not be found\".format(path))\n+                    else:\n+                        continue\n+                ending = m.group(3)\n+                if ending and len(ending) > 1:\n+                    end = int(ending[1:])\n+                starting = m.group(2)\n+                if starting and len(starting) > 1:\n+                    start = max(1, int(starting[1:]) - 1)\n+                section_name = m.group(4)\n+                if section_name:\n+                    section = section_name[1:]\n+\n+                # Ignore path links if we are in external, downloaded content\n+                is_link = path.lower().startswith(('https://', 'http://'))\n+                if is_url and not is_link:\n+                    continue\n+\n+                # If this is a link, and we are allowing URLs, set `url` to true.\n+                # Make sure we don't process `path` as a local file reference.\n+                url = self.url_download and is_link\n+                snippet = self.get_snippet_path(path) if not url else path\n+\n+                if snippet:\n+\n+                    # This is in the stack and we don't want an infinite loop!\n+                    if snippet in self.seen:\n+                        continue\n+\n+                    if not url:\n+                        # Read file content\n+                        with codecs.open(snippet, 'r', encoding=self.encoding) as f:\n+                            s_lines = [l.rstrip('\\r\\n') for l in f]\n+                            if start is not None or end is not None:\n+                                s = slice(start, end)\n+                                s_lines = self.dedent(s_lines[s]) if self.dedent_subsections else s_lines[s]\n+                            elif section:\n+                                s_lines = self.extract_section(section, s_lines)\n+                    else:\n+                        # Read URL content\n+                        try:\n+                            s_lines = self.download(snippet)\n+                            if start is not None or end is not None:\n+                                s = slice(start, end)\n+                                s_lines = self.dedent(s_lines[s]) if self.dedent_subsections else s_lines[s]\n+                            elif section:\n+                                s_lines = self.extract_section(section, s_lines)\n+                        except SnippetMissingError:\n+                            if self.check_paths:\n+                                raise\n+                            s_lines = []\n+\n+                    # Process lines looking for more snippets\n+                    new_lines.extend(\n+                        [\n+                            space + l2 for l2 in self.parse_snippets(\n+                                s_lines,\n+                                snippet,\n+                                is_url=url\n+                            )\n+                        ]\n+                    )\n+\n+                elif self.check_paths:\n+                    raise SnippetMissingError(\"Snippet at path '{}' could not be found\".format(path))\n+\n+        # Pop the current file name out of the cache\n+        if file_name:\n+            self.seen.remove(file_name)\n+\n+        return new_lines\n+\n+    def run(self, lines):\n+        \"\"\"Process snippets.\"\"\"\n+\n+        self.seen = set()\n+        if self.auto_append:\n+            lines.extend(\"\\n\\n-8<-\\n{}\\n-8<-\\n\".format('\\n\\n'.join(self.auto_append)).split('\\n'))\n+\n+        return self.parse_snippets(lines)\n+\n+\n+class SnippetExtension(Extension):\n+    \"\"\"Snippet extension.\"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        \"\"\"Initialize.\"\"\"\n+\n+        self.config = {\n+            'base_path': [[\".\"], \"Base path for snippet paths - Default: [\\\".\\\"]\"],\n+            'encoding': [\"utf-8\", \"Encoding of snippets - Default: \\\"utf-8\\\"\"],\n+            'check_paths': [False, \"Make the build fail if a snippet can't be found - Default: \\\"False\\\"\"],\n+            \"auto_append\": [\n+                [],\n+                \"A list of snippets (relative to the 'base_path') to auto append to the Markdown content - Default: []\"\n+            ],\n+            'url_download': [False, \"Download external URLs as snippets - Default: \\\"False\\\"\"],\n+            'url_max_size': [DEFAULT_URL_SIZE, \"External URL max size (0 means no limit)- Default: 32 MiB\"],\n+            'url_timeout': [DEFAULT_URL_TIMEOUT, 'Defualt URL timeout (0 means no timeout) - Default: 10 sec'],\n+             'url_request_headers': [DEFAULT_URL_REQUEST_HEADERS, \"Extra request Headers - Default: {}\"],\n+            'dedent_subsections': [False, \"Dedent subsection extractions e.g. 'sections' and/or 'lines'.\"],\n+            'restrict_base_path': [True, \"Restrict includes to only be within the `base_path`\"]\n+        }\n+\n+        super(SnippetExtension, self).__init__(*args, **kwargs)\n+\n+    def extendMarkdown(self, md):\n+        \"\"\"Register the extension.\"\"\"\n+\n+        self.md = md\n+        md.registerExtension(self)\n+        config = self.getConfigs()\n+        snippet = SnippetPreprocessor(config, md)\n+        md.preprocessors.register(snippet, \"snippet\", 32)\n+\n+    def reset(self):\n+        \"\"\"Reset.\"\"\"\n+\n+        self.md.preprocessors['snippet'].download.cache_clear()\n+\n+\n+def makeExtension(*args, **kwargs):\n+    \"\"\"Return extension.\"\"\"\n+\n+    return SnippetExtension(*args, **kwargs)\n"}
{"cve":"CVE-2024-5823:0708", "fix_patch": "diff --git a/modules/utils.py b/modules/utils.py\nindex f2baefa..f2d82af 100644\n--- a/modules/utils.py\n+++ b/modules/utils.py\n@@ -1,6 +1,10 @@\n # -*- coding:utf-8 -*-\n from __future__ import annotations\n from typing import TYPE_CHECKING, Any, Callable, Dict, List, Tuple, Type\n+\n+\n+\n+\n from enum import Enum\n import logging\n import commentjson as json\n@@ -741,7 +745,12 @@ def transfer_input(inputs):\n     )\n \n \n+\n+\n def update_chuanhu():\n+\n+\n+\n     from .repo import background_update\n \n     print(\"[Updater] Trying to update...\")\n"}
{"cve":"CVE-2022-36087:0708", "fix_patch": "diff --git a/oauthlib/uri_validate.py b/oauthlib/uri_validate.py\nindex 8a6d9c2..c8ae8dd 100644\n--- a/oauthlib/uri_validate.py\n+++ b/oauthlib/uri_validate.py\n@@ -66,7 +66,24 @@ IPv4address = r\"%(dec_octet)s \\. %(dec_octet)s \\. %(dec_octet)s \\. %(dec_octet)s\n )\n \n #   IPv6address\n-IPv6address = r\"([A-Fa-f0-9:]+:+)+[A-Fa-f0-9]+\"\n+IPv6address = r'(' \\\n+    r'(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F]{1,4}|' \\\n+    r'(?:[0-9a-fA-F]{1,4}:){1,7}:|' \\\n+    r'(?:[0-9a-fA-F]{1,4}:){1,6}:[0-9a-fA-F]{1,4}|' \\\n+    r'(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fA-F]{1,4}){1,2}|' \\\n+    r'(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}){1,3}|' \\\n+    r'(?:[0-9a-fA-F]{1,4}:){1,3}(?::[0-9a-fA-F]{1,4}){1,4}|' \\\n+    r'(?:[0-9a-fA-F]{1,4}:){1,2}(?::[0-9a-fA-F]{1,4}){1,5}|' \\\n+    r'[0-9a-fA-F]{1,4}:(?:(?::[0-9a-fA-F]{1,4}){1,6})|' \\\n+    r':(?:(?::[0-9a-fA-F]{1,4}){1,7}|:)|' \\\n+    r'fe80:(?::[0-9a-fA-F]{0,4}){0,4}%[0-9a-zA-Z]{1,}|' \\\n+    r'::(?:ffff(?::0{1,4}){0,1}:){0,1}' \\\n+    r'(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}' \\\n+    r'(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])|' \\\n+    r'(?:[0-9a-fA-F]{1,4}:){1,4}:' \\\n+    r'(?:(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])\\.){3,3}' \\\n+    r'(?:25[0-5]|(?:2[0-4]|1{0,1}[0-9]){0,1}[0-9])' \\\n+    r')'\n \n #   IPvFuture     = \"v\" 1*HEXDIG \".\" 1*( unreserved / sub-delims / \":\" )\n IPvFuture = r\"v %(HEXDIG)s+ \\. (?: %(unreserved)s | %(sub_delims)s | : )+\" % locals()\n"}
{"cve":"CVE-2023-29159:0708", "fix_patch": "diff --git a/starlette/staticfiles.py b/starlette/staticfiles.py\nindex 4d075b3..a1cebce 100644\n--- a/starlette/staticfiles.py\n+++ b/starlette/staticfiles.py\n@@ -159,19 +159,31 @@ class StaticFiles:\n                 )\n         raise HTTPException(status_code=404)\n \n+    def is_within_directory(self, path: str, directory: str) -> bool:\n+        # Ensure both paths are absolute and normalized.\n+        directory = os.path.abspath(directory)\n+        path = os.path.abspath(path)\n+\n+        # os.path.commonpath will find the longest common sub-path of two paths.\n+        # If this common path is not the directory path, then the file is outside the\n+        # directory.\n+        return os.path.commonpath([path, directory]) == directory\n+\n+\n     def lookup_path(\n         self, path: str\n     ) -> typing.Tuple[str, typing.Optional[os.stat_result]]:\n         for directory in self.all_directories:\n-            joined_path = os.path.join(directory, path)\n-            if self.follow_symlink:\n-                full_path = os.path.abspath(joined_path)\n-            else:\n-                full_path = os.path.realpath(joined_path)\n-            directory = os.path.realpath(directory)\n-            if os.path.commonprefix([full_path, directory]) != directory:\n-                # Don't allow misbehaving clients to break out of the static files\n-                # directory.\n+            full_path = os.path.abspath(os.path.join(directory, path))\n+            if self.follow_symlink and os.path.islink(full_path):\n+                # Follow symlinks outside of the static directory,\n+                # but only if the user has explicitly allowed it.\n+                full_path = os.path.realpath(full_path)\n+            if (\n+                not os.path.exists(full_path)\n+                or not os.path.isfile(full_path)\n+                or not self.is_within_directory(full_path, directory)\n+            ):\n                 continue\n             try:\n                 return full_path, os.stat(full_path)\n"}
{"cve":"CVE-2023-41040:0708", "fix_patch": "diff --git a/git/refs/symbolic.py b/git/refs/symbolic.py\nindex 734bf32d..91a5418d 100644\n--- a/git/refs/symbolic.py\n+++ b/git/refs/symbolic.py\n@@ -138,6 +138,9 @@ class SymbolicReference(object):\n                     if line[0] == \"^\":\n                         continue\n \n+\n+\n+\n                     yield cast(Tuple[str, str], tuple(line.split(\" \", 1)))\n                 # END for each line\n         except OSError:\n@@ -156,20 +159,38 @@ class SymbolicReference(object):\n         :param repo: the repository containing the reference at ref_path\"\"\"\n \n         while True:\n+\n+\n+\n             hexsha, ref_path = cls._get_ref_info(repo, ref_path)\n             if hexsha is not None:\n                 return hexsha\n-        # END recursive dereferencing\n+    @staticmethod\n+    def _check_ref_name_valid(ref_path: Union[PathLike, None]) -> None:\n+        \"\"\"Raises ValueError if the given ref_path is not a valid git reference name\"\"\"\n+        if \"..\" in str(ref_path) or os.path.isabs(str(ref_path)):\n+            raise ValueError(f\"Invalid reference '{ref_path}'\")\n+        if any(component.startswith('.') for component in str(ref_path).split('/')):\n+            raise ValueError(\"ref component must not begin with a dot\")\n+        if any(component.endswith('.lock') for component in str(ref_path).split('/')):\n+            raise ValueError(\"ref component must not end with .lock\")\n+        if '\\\\' in str(ref_path) or str(ref_path) == '@' or '@{' in str(ref_path) or str(ref_path).endswith('.') or str(ref_path).startswith('/') or str(ref_path).endswith('/') or any(c in str(ref_path) for c in ' ~^:?*[') or any(ord(c) < 32 or ord(c) == 127 for c in str(ref_path)):\n+            raise ValueError(\"ref must not contain invalid characters\")\n+\n \n     @classmethod\n+\n+\n+\n+\n     def _get_ref_info_helper(\n         cls, repo: \"Repo\", ref_path: Union[PathLike, None]\n     ) -> Union[Tuple[str, None], Tuple[None, str]]:\n         \"\"\"Return: (str(sha), str(target_ref_path)) if available, the sha the file at\n         rela_path points to, or None. target_ref_path is the reference we\n+\n         point to, or None\"\"\"\n-        if \"..\" in str(ref_path):\n-            raise ValueError(f\"Invalid reference '{ref_path}'\")\n+        cls._check_ref_name_valid(ref_path)\n         tokens: Union[None, List[str], Tuple[str, str]] = None\n         repodir = _git_dir(repo, ref_path)\n         try:\n@@ -202,6 +223,7 @@ class SymbolicReference(object):\n         if repo.re_hexsha_only.match(tokens[0]):\n             return (tokens[0], None)\n \n+\n         raise ValueError(\"Failed to parse reference information from %r\" % ref_path)\n \n     @classmethod\n"}
{"cve":"CVE-2023-39660:0708", "fix_patch": ""}
{"cve":"CVE-2025-27154:0708", "fix_patch": "diff --git a/spotipy/cache_handler.py b/spotipy/cache_handler.py\nindex 936e81b..c56026d 100644\n--- a/spotipy/cache_handler.py\n+++ b/spotipy/cache_handler.py\n@@ -92,7 +92,12 @@ class CacheFileHandler(CacheHandler):\n \n     def save_token_to_cache(self, token_info):\n         try:\n-            with open(self.cache_path, \"w\", encoding='utf-8') as f:\n+            \n+            flags = os.O_WRONLY | os.O_CREAT | os.O_TRUNC\n+            # Set the file permissions to 600 (read/write for owner only)\n+            mode = 0o600\n+            # Open the file with the specified flags and mode\n+            with os.fdopen(os.open(self.cache_path, flags, mode), 'w', encoding='utf-8') as f:\n                 f.write(json.dumps(token_info, cls=self.encoder_cls))\n         except OSError:\n             logger.warning(f\"Couldn't write token to cache at: {self.cache_path}\")\n"}
{"cve":"CVE-2020-15278:0708", "fix_patch": "diff --git a/redbot/cogs/mod/kickban.py b/redbot/cogs/mod/kickban.py\nindex 88b9e053..11f6e316 100644\n--- a/redbot/cogs/mod/kickban.py\n+++ b/redbot/cogs/mod/kickban.py\n@@ -420,98 +420,7 @@ class KickBanMixin(MixinMeta):\n         if not guild.me.guild_permissions.ban_members:\n             return await ctx.send(_(\"I lack the permissions to do this.\"))\n \n-        tempbans = await self.config.guild(guild).current_tempbans()\n-\n-        ban_list = await guild.bans()\n-        for entry in ban_list:\n-            for user_id in user_ids:\n-                if entry.user.id == user_id:\n-                    if user_id in tempbans:\n-                        # We need to check if a user is tempbanned here because otherwise they won't be processed later on.\n-                        continue\n-                    else:\n-                        errors[user_id] = _(\"User with ID {user_id} is already banned.\").format(\n-                            user_id=user_id\n-                        )\n-\n-        user_ids = remove_processed(user_ids)\n-\n-        if not user_ids:\n-            await show_results()\n-            return\n-\n-        for user_id in user_ids:\n-            user = guild.get_member(user_id)\n-            if user is not None:\n-                if user_id in tempbans:\n-                    # We need to check if a user is tempbanned here because otherwise they won't be processed later on.\n-                    continue\n-                else:\n-                    # Instead of replicating all that handling... gets attr from decorator\n-                    try:\n-                        success, reason = await self.ban_user(\n-                            user=user, ctx=ctx, days=days, reason=reason, create_modlog_case=True\n-                        )\n-                        if success:\n-                            banned.append(user_id)\n-                        else:\n-                            errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n-                                user_id=user_id, reason=reason\n-                            )\n-                    except Exception as e:\n-                        errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n-                            user_id=user_id, reason=e\n-                        )\n-\n-        user_ids = remove_processed(user_ids)\n-\n-        if not user_ids:\n-            await show_results()\n-            return\n \n-        for user_id in user_ids:\n-            user = discord.Object(id=user_id)\n-            audit_reason = get_audit_reason(author, reason)\n-            queue_entry = (guild.id, user_id)\n-            async with self.config.guild(guild).current_tempbans() as tempbans:\n-                if user_id in tempbans:\n-                    tempbans.remove(user_id)\n-                    upgrades.append(str(user_id))\n-                    log.info(\n-                        \"{}({}) upgraded the tempban for {} to a permaban.\".format(\n-                            author.name, author.id, user_id\n-                        )\n-                    )\n-                    banned.append(user_id)\n-                else:\n-                    try:\n-                        await guild.ban(user, reason=audit_reason, delete_message_days=days)\n-                        log.info(\"{}({}) hackbanned {}\".format(author.name, author.id, user_id))\n-                    except discord.NotFound:\n-                        errors[user_id] = _(\"User with ID {user_id} not found\").format(\n-                            user_id=user_id\n-                        )\n-                        continue\n-                    except discord.Forbidden:\n-                        errors[user_id] = _(\n-                            \"Could not ban user with ID {user_id}: missing permissions.\"\n-                        ).format(user_id=user_id)\n-                        continue\n-                    else:\n-                        banned.append(user_id)\n-\n-            await modlog.create_case(\n-                self.bot,\n-                guild,\n-                ctx.message.created_at.replace(tzinfo=timezone.utc),\n-                \"hackban\",\n-                user_id,\n-                author,\n-                reason,\n-                until=None,\n-                channel=None,\n-            )\n-        await show_results()\n \n     @commands.command()\n     @commands.guild_only()\n"}
{"cve":"CVE-2024-48911:0708", "fix_patch": "diff --git a/opencanary/config.py b/opencanary/config.py\nindex 4886783..6d6cb96 100644\n--- a/opencanary/config.py\n+++ b/opencanary/config.py\n@@ -54,9 +54,9 @@ class Config:\n         self.__configfile = configfile\n \n         files = [\n-            configfile,\n-            \"%s/.%s\" % (expanduser(\"~\"), configfile),\n             \"/etc/opencanaryd/%s\" % configfile,\n+            \"%s/.%s\" % (expanduser(\"~\"), configfile),\n+            configfile,\n         ]\n         print(\n             \"** We hope you enjoy using OpenCanary. For more open source Canary goodness, head over to canarytokens.org. **\"\n@@ -66,15 +66,23 @@ class Config:\n                 with open(fname, \"r\") as f:\n                     print(\"[-] Using config file: %s\" % fname)\n                     self.__config = json.load(f)\n+                    if os.geteuid() == 0:  # if running as root\n+                        stat_info = os.stat(fname)\n+                        if stat_info.st_uid != 0:\n+                            print(f\"[!] Config file {fname} is not owned by root, but OpenCanary is running as root. This is a security risk.\")\n+                            sys.exit(1)\n+                    if fname.startswith(expanduser(\"~\")):\n+                        print(\n+                            \"[!] Found config in home directory ({fname}). \"\n+                            \"This is insecure if the daemon is running as root.\"\n+                        )\n                     self.__config = expand_vars(self.__config)\n                 return\n             except IOError as e:\n                 print(\"[-] Failed to open %s for reading (%s)\" % (fname, e))\n             except ValueError as e:\n                 print(\"[-] Failed to decode json from %s (%s)\" % (fname, e))\n-                subprocess.call(\n-                    \"cp -r %s /var/tmp/config-err-$(date +%%s)\" % fname, shell=True\n-                )\n+                print(\"[-] Please move the file to /var/tmp/ manually for inspection\")\n             except Exception as e:\n                 print(\"[-] An error occurred loading %s (%s)\" % (fname, e))\n         if self.__config is None:\ndiff --git a/opencanary/modules/portscan.py b/opencanary/modules/portscan.py\nindex 730ce68..16533c5 100644\n--- a/opencanary/modules/portscan.py\n+++ b/opencanary/modules/portscan.py\n@@ -118,74 +118,304 @@ class CanaryPortscan(CanaryService):\n \n     def set_iptables_rules(self):\n         iptables_path = self.getIptablesPath()\n-        os.system(\n-            'sudo {0} -t mangle -D PREROUTING -p tcp -i lo -j LOG --log-level=warning --log-prefix=\"canaryfw: \" -m limit --limit=\"{1}/hour\"'.format(\n-                iptables_path, self.lorate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-D\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"-i\",\n+            \"lo\",\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canaryfw: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.lorate}/hour\",\n         )\n-        os.system(\n-            'sudo {0} -t mangle -A PREROUTING -p tcp -i lo -j LOG --log-level=warning --log-prefix=\"canaryfw: \" -m limit --limit=\"{1}/hour\"'.format(\n-                iptables_path, self.lorate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-A\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"-i\",\n+            \"lo\",\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canaryfw: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.lorate}/hour\",\n         )\n \n         # Logging rules for canaryfw.\n         # We ignore loopback interface traffic as it is taken care of in above rule\n-        os.system(\n-            'sudo {0} -t mangle -D PREROUTING -p tcp --syn -j LOG --log-level=warning --log-prefix=\"canaryfw: \" -m limit --limit=\"{1}/second\" ! -i lo'.format(\n-                iptables_path, self.synrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-D\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"--syn\",\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canaryfw: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.synrate}/second\",\n+            \"!\",\n+            \"-i\",\n+            \"lo\",\n         )\n-        os.system(\n-            'sudo {0} -t mangle -A PREROUTING -p tcp --syn -j LOG --log-level=warning --log-prefix=\"canaryfw: \" -m limit --limit=\"{1}/second\" ! -i lo'.format(\n-                iptables_path, self.synrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-A\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"--syn\",\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canaryfw: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.synrate}/second\",\n+            \"!\",\n+            \"-i\",\n+            \"lo\",\n         )\n \n         # Match the T3 probe of the nmap OS detection based on TCP flags and TCP options string\n-        os.system(\n-            'sudo {0} -t mangle -D PREROUTING -p tcp --tcp-flags ALL URG,PSH,SYN,FIN -m u32 --u32 \"40=0x03030A01 && 44=0x02040109 && 48=0x080Affff && 52=0xffff0000 && 56=0x00000402\" -j LOG --log-level=warning --log-prefix=\"canarynmap: \" -m limit --limit=\"{1}/second\"'.format(\n-                iptables_path, self.nmaposrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-D\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"--tcp-flags\",\n+            \"ALL\",\n+            \"URG,PSH,SYN,FIN\",\n+            \"-m\",\n+            \"u32\",\n+            \"--u32\",\n+            \"40=0x03030A01 && 44=0x02040109 && 48=0x080Affff && 52=0xffff0000 && 56=0x00000402\",\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canarynmap: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.nmaposrate}/second\",\n         )\n-        os.system(\n-            'sudo {0} -t mangle -A PREROUTING -p tcp --tcp-flags ALL URG,PSH,SYN,FIN -m u32 --u32 \"40=0x03030A01 && 44=0x02040109 && 48=0x080Affff && 52=0xffff0000 && 56=0x00000402\" -j LOG --log-level=warning --log-prefix=\"canarynmap: \" -m limit --limit=\"{1}/second\"'.format(\n-                iptables_path, self.nmaposrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-A\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"--tcp-flags\",\n+            \"ALL\",\n+            \"URG,PSH,SYN,FIN\",\n+            \"-m\",\n+            \"u32\",\n+            \"--u32\",\n+            \"40=0x03030A01 && 44=0x02040109 && 48=0x080Affff && 52=0xffff0000 && 56=0x00000402\",\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canarynmap: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.nmaposrate}/second\",\n         )\n \n         # Nmap Null Scan\n-        os.system(\n-            'sudo {0} -t mangle -D PREROUTING -p tcp -m u32 --u32 \"6&0xFF=0x6 && 0>>22&0x3C@12=0x50000400\" -j LOG --log-level=warning --log-prefix=\"canarynmapNULL: \" -m limit --limit=\"{1}/second\"'.format(\n-                iptables_path, self.nmaposrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-D\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"-m\",\n+            \"u32\",\n+            \"--u32\",\n+            '6&0xFF=0x6 && 0>>22&0x3C@12=0x50000400',\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canarynmapNULL: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.nmaposrate}/second\",\n         )\n-        os.system(\n-            'sudo {0} -t mangle -A PREROUTING -p tcp -m u32 --u32 \"6&0xFF=0x6 && 0>>22&0x3C@12=0x50000400\" -j LOG --log-level=warning --log-prefix=\"canarynmapNULL: \" -m limit --limit=\"{1}/second\"'.format(\n-                iptables_path, self.nmaposrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-A\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"-m\",\n+            \"u32\",\n+            \"--u32\",\n+            '6&0xFF=0x6 && 0>>22&0x3C@12=0x50000400',\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canarynmapNULL: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.nmaposrate}/second\",\n         )\n \n         # Nmap Xmas Scan\n-        os.system(\n-            'sudo {0} -t mangle -D PREROUTING -p tcp -m u32 --u32 \"6&0xFF=0x6 && 0>>22&0x3C@12=0x50290400\" -j LOG --log-level=warning --log-prefix=\"canarynmapXMAS: \" -m limit --limit=\"{1}/second\"'.format(\n-                iptables_path, self.nmaposrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-D\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"-m\",\n+            \"u32\",\n+            \"--u32\",\n+            '6&0xFF=0x6 && 0>>22&0x3C@12=0x50290400',\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canarynmapXMAS: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.nmaposrate}/second\",\n         )\n-        os.system(\n-            'sudo {0} -t mangle -A PREROUTING -p tcp -m u32 --u32 \"6&0xFF=0x6 && 0>>22&0x3C@12=0x50290400\" -j LOG --log-level=warning --log-prefix=\"canarynmapXMAS: \" -m limit --limit=\"{1}/second\"'.format(\n-                iptables_path, self.nmaposrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-A\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"-m\",\n+            \"u32\",\n+            \"--u32\",\n+            '6&0xFF=0x6 && 0>>22&0x3C@12=0x50290400',\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canarynmapXMAS: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.nmaposrate}/second\",\n         )\n \n         # Nmap Fin Scan\n-        os.system(\n-            'sudo {0} -t mangle -D PREROUTING -p tcp -m u32 --u32 \"6&0xFF=0x6 && 0>>22&0x3C@12=0x50010400\" -j LOG --log-level=warning --log-prefix=\"canarynmapFIN: \" -m limit --limit=\"{1}/second\"'.format(\n-                iptables_path, self.nmaposrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-D\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"-m\",\n+            \"u32\",\n+            \"--u32\",\n+            '6&0xFF=0x6 && 0>>22&0x3C@12=0x50010400',\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canarynmapFIN: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.nmaposrate}/second\",\n         )\n-        os.system(\n-            'sudo {0} -t mangle -A PREROUTING -p tcp -m u32 --u32 \"6&0xFF=0x6 && 0>>22&0x3C@12=0x50010400\" -j LOG --log-level=warning --log-prefix=\"canarynmapFIN: \" -m limit --limit=\"{1}/second\"'.format(\n-                iptables_path, self.nmaposrate\n-            )\n+        safe_popen(\n+            \"sudo\",\n+            iptables_path,\n+            \"-t\",\n+            \"mangle\",\n+            \"-A\",\n+            \"PREROUTING\",\n+            \"-p\",\n+            \"tcp\",\n+            \"-m\",\n+            \"u32\",\n+            \"--u32\",\n+            '6&0xFF=0x6 && 0>>22&0x3C@12=0x50010400',\n+            \"-j\",\n+            \"LOG\",\n+            \"--log-level\",\n+            \"warning\",\n+            \"--log-prefix\",\n+            \"canarynmapFIN: \",\n+            \"-m\",\n+            \"limit\",\n+            \"--limit\",\n+            f\"{self.nmaposrate}/second\",\n         )\n"}
{"cve":"CVE-2020-28360:0708", "fix_patch": "diff --git a/src/index.js b/src/index.js\nindex 0dc1033..ff230fb 100644\n--- a/src/index.js\n+++ b/src/index.js\n@@ -1,11 +1,62 @@\n-export default (ip) => (\n-  /^(::f{4}:)?10\\.([0-9]{1,3})\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^(::f{4}:)?192\\.168\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^(::f{4}:)?172\\.(1[6-9]|2\\d|30|31)\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^(::f{4}:)?127\\.([0-9]{1,3})\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^(::f{4}:)?169\\.254\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^f[cd][0-9a-f]{2}:/i.test(ip) ||\n-  /^fe80:/i.test(ip) ||\n-  /^::1$/.test(ip) ||\n-  /^::$/.test(ip)\n-)\n+'use strict'\n+\n+var net = require('net')\n+\n+var INDETERMINABLE = 'Invalid ip address'\n+\n+// https://en.wikipedia.org/wiki/Reserved_IP_addresses\n+var privateBlocks = {\n+  ipv4: [\n+    '0.0.0.0/8',\n+    '10.0.0.0/8',\n+    '100.64.0.0/10',\n+    '127.0.0.0/8',\n+    '169.254.0.0/16',\n+    '172.16.0.0/12',\n+    '192.0.0.0/24',\n+    '192.0.2.0/24',\n+    '192.31.196.0/24',\n+    '192.52.193.0/24',\n+    '192.88.99.0/24',\n+    '192.168.0.0/16',\n+    '198.18.0.0/15',\n+    '198.51.100.0/24',\n+    '203.0.113.0/24',\n+    '192.175.48.0/24',\n+    '240.0.0.0/4'\n+\n+  ],\n+  ipv6: [\n+    '::/128',\n+    '::1/128',\n+    '::ffff:0:0/96',\n+    '100::/64',\n+    '2001::/23',\n+    '2001:2::/48',\n+    '2001:db8::/32',\n+    '2001:10::/28',\n+    'fc00::/7',\n+    'fe80::/10'\n+  ]\n+}\n+\n+var Netmask = require('netmask').Netmask\n+\n+var anet = {}\n+\n+anet.isPrivate = function (ip) {\n+  var family = net.isIP(ip)\n+  if (family === 4) {\n+    return privateBlocks.ipv4.some(function (block) {\n+      return new Netmask(block).contains(ip)\n+    })\n+  } else if (family === 6) {\n+    return privateBlocks.ipv6.some(function (block) {\n+      var block = new Netmask(block.split('/')[0])\n+      return block.contains(ip)\n+    })\n+  }\n+  return INDETERMINABLE\n+}\n+\n+export default anet.isPrivate\n\\ No newline at end of file\n"}
{"cve":"CVE-2017-16100:0708", "fix_patch": "diff --git a/lib/dns-sync.js b/lib/dns-sync.js\nindex 5f63607..167093f 100644\n--- a/lib/dns-sync.js\n+++ b/lib/dns-sync.js\n@@ -16,11 +16,9 @@ module.exports = {\n             nodeBinary = process.execPath,\n             scriptPath = path.join(__dirname, \"../scripts/dns-lookup-script\"),\n             response,\n-            cmd = util.format('\"%s\" \"%s\" %s', nodeBinary, scriptPath, hostname);\n-\n-        response = shell.exec(cmd, {silent: true});\n-        if (response && response.code === 0) {\n-            output = response.output;\n+        response = require('child_process').spawnSync(nodeBinary, [scriptPath, hostname], {encoding: 'utf8'});\n+        if (response && response.status === 0) {\n+            output = response.stdout;\n             if (output && net.isIP(output)) {\n                 return output;\n             }\n"}
{"cve":"CVE-2021-23384:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 0102e84..20003be 100644\n--- a/index.js\n+++ b/index.js\n@@ -37,7 +37,7 @@ function removeTrailingSlashes(opts) {\n             path = getPath(ctx.originalUrl, ctx.querystring);\n         }\n \n-        if (path && haveSlash(path)) {\n+        if (path && haveSlash(path) && !path.startsWith('//')) {\n             path = path.slice(0, -1);\n             const query = querystring.length ? '?' + querystring : '';\n \n"}
{"cve":"CVE-2024-53900:0708", "fix_patch": "diff --git a/lib/helpers/populate/getModelsMapForPopulate.js b/lib/helpers/populate/getModelsMapForPopulate.js\nindex 16d920366..3b48c82de 100644\n--- a/lib/helpers/populate/getModelsMapForPopulate.js\n+++ b/lib/helpers/populate/getModelsMapForPopulate.js\n@@ -23,6 +23,10 @@ module.exports = function getModelsMapForPopulate(model, docs, options) {\n   const len = docs.length;\n   const map = [];\n   const modelNameFromQuery = options.model && options.model.modelName || options.model;\n+\n+  if (get(options, 'match.$where') != null || get(options, 'options.match.$where') != null) {\n+    throw new MongooseError('Cannot use $where filter with populate() match');\n+  }\n   let schema;\n   let refPath;\n   let modelNames;\n@@ -349,12 +353,10 @@ module.exports = function getModelsMapForPopulate(model, docs, options) {\n     return { modelNames: modelNames, justOne: justOne, isRefPath: isRefPath, refPath: refPath };\n   }\n };\n-\n-/*!\n- * ignore\n- */\n-\n function _virtualPopulate(model, docs, options, _virtualRes) {\n+  if (get(options, 'match.$where') != null || get(options, 'options.match.$where') != null) {\n+    throw new MongooseError('Cannot use $where filter with populate() match');\n+  }\n   const map = [];\n   const available = {};\n   const virtual = _virtualRes.virtual;\n"}
{"cve":"CVE-2022-35949:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex b2144c84..7dc54fae 100644\n--- a/index.js\n+++ b/index.js\n@@ -53,6 +53,10 @@ function makeDispatcher (fn) {\n         throw new InvalidArgumentError('invalid opts.path')\n       }\n \n+      if (opts.path.startsWith('//') && !opts.path.startsWith('///')) {\n+        throw new InvalidArgumentError('invalid opts.path');\n+      }\n+\n       url = new URL(opts.path, util.parseOrigin(url))\n     } else {\n       if (!opts) {\n"}
{"cve":"CVE-2022-0691:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex c5a2a11..e875684 100644\n--- a/index.js\n+++ b/index.js\n@@ -2,11 +2,11 @@\n \n var required = require('requires-port')\n   , qs = require('querystringify')\n-  , CRHTLF = /[\\n\\r\\t]/g\n+  , CRHTLF = /[\\n\\r\\t\\x00-\\x1F]/g\n   , slashes = /^[A-Za-z][A-Za-z0-9+-.]*:\\/\\//\n   , protocolre = /^([a-z][a-z0-9.+-]*:)?(\\/\\/)?([\\\\/]+)?([\\S\\s]*)/i\n   , windowsDriveLetter = /^[a-zA-Z]:/\n-  , whitespace = /^[ \\f\\n\\r\\t\\v\\u00a0\\u1680\\u2000-\\u200a\\u2028\\u2029\\u202f\\u205f\\u3000\\ufeff]+/;\n+  , whitespace = /^[ \\f\\n\\r\\t\\v\\u00a0\\u1680\\u2000-\\u200a\\u2028\\u2029\\u202f\\u205f\\u3000\\ufeff\\x00-\\x1F]+/;\n \n /**\n  * Trim a given string.\n@@ -88,6 +88,7 @@ function lolcation(loc) {\n   } else if ('object' === type) {\n     for (key in loc) {\n       if (key in ignore) continue;\n+      if (key === '__proto__') continue;\n       finaldestination[key] = loc[key];\n     }\n \n@@ -419,6 +420,8 @@ function Url(address, location, parser) {\n function set(part, value, fn) {\n   var url = this;\n \n+  if (part === '__proto__') return url;\n+\n   switch (part) {\n     case 'query':\n       if ('string' === typeof value && value.length) {\n"}
{"cve":"CVE-2019-10787:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 16654d1..8404ae3 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,4 +1,4 @@\n-var exec = require('child_process').exec;\n+var execFile = require('child_process').execFile;\n var aspect = require('aspectratio');\n var dirname = require('path').dirname;\n var basename = require('path').basename;\n@@ -7,8 +7,9 @@ var join = require('path').join;\n var sprintf = require('util').format;\n \n module.exports = function(image, output, cb) {\n-  var cmd = module.exports.cmd(image, output);\n-  exec(cmd, {timeout: 30000}, function(e, stdout, stderr) {\n+  var args = module.exports.cmd(image, output);\n+  var cmd = args.shift();\n+  execFile(cmd, args, {timeout: 30000}, function(e, stdout, stderr) {\n     if (e) { return cb(e); }\n     if (stderr) { return cb(new Error(stderr)); }\n \n@@ -106,11 +107,7 @@ module.exports.path = function(src, opts) {\n  * @return string convert command\n  */\n module.exports.cmd = function(image, output) {\n-  var cmd = [\n-    sprintf(\n-      'convert %s -auto-orient -strip -write mpr:%s +delete', image.path, image.path\n-    )\n-  ];\n+  var cmd = [ 'convert', image.path, '-auto-orient', '-strip', '-write', 'mpr:' + image.path, '+delete' ];\n \n   for (var i = 0; i < output.versions.length; i++) {\n     var version = output.versions[i];\n@@ -125,10 +122,10 @@ module.exports.cmd = function(image, output) {\n       suffix: version.suffix || ''\n     });\n \n-    cmd.push(module.exports.cmdVersion(image, version, last));\n+    cmd = cmd.concat(module.exports.cmdVersion(image, version, last));\n   }\n \n-  return cmd.join(' ');\n+  return cmd;\n };\n \n /**\n@@ -144,16 +141,16 @@ module.exports.cmdVersion = function(image, version, last) {\n   var cmd = [];\n \n   // http://www.imagemagick.org/Usage/files/#mpr\n-  cmd.push(sprintf('mpr:%s', image.path));\n+  cmd.push('mpr:' + image.path);\n \n   // -quality\n   if (version.quality) {\n-    cmd.push(sprintf('-quality %d', version.quality));\n+    cmd.push('-quality', String(version.quality));\n   }\n \n   // -background\n   if (version.background) {\n-    cmd.push(sprintf('-background \"%s\"', version.background));\n+    cmd.push('-background', version.background);\n   }\n \n   // -flatten\n@@ -164,22 +161,22 @@ module.exports.cmdVersion = function(image, version, last) {\n   // -crop\n   var crop = module.exports.crop(image, version.aspect);\n   if (crop.geometry) {\n-    cmd.push(sprintf('-crop \"%s\"', crop.geometry));\n+    cmd.push('-crop', crop.geometry);\n   }\n \n   // -resize\n   // http://www.imagemagick.org/script/command-line-processing.php#geometry\n   var resize = module.exports.resize(crop, version);\n   if (resize) {\n-    cmd.push(sprintf('-resize \"%s\"', resize));\n+    cmd.push('-resize', resize);\n   }\n \n   // -write\n   if (last) {\n     cmd.push(version.path);\n   } else {\n-    cmd.push(sprintf('-write %s +delete', version.path));\n+    cmd.push('-write', version.path, '+delete');\n   }\n \n-  return cmd.join(' ');\n-};\n+  return cmd;\n+};\n\\ No newline at end of file\n"}
{"cve":"CVE-2020-7687:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 2100676..148e390 100644\n--- a/index.js\n+++ b/index.js\n@@ -12,6 +12,7 @@ module.exports = function (port, root, wordy) {\n \n         var uri = url.parse(req.url).pathname,\n             filename = path.join(root, uri);\n+if (path.normalize(filename).indexOf(path.normalize(root))!=0) {\\n            res.writeHead(403, {'Content-Type': 'text/plain'});\\n            res.end('Forbidden');\\n            return;\\n        }\n \n         if (uri.charAt(uri.length - 1) === '/') {\n             filename += 'index.html';\n@@ -41,7 +42,7 @@ module.exports = function (port, root, wordy) {\n                 }\n                 if (wordy) console.log('Ok 200');\n                 res.writeHead(200, {\n-                    'Content-Type': mime.lookup(filename) + ';' + mime.charsets.lookup(mime.lookup(filename)) \n+                    'Content-Type': mime.lookup(filename) + ';' + mime.charsets.lookup(mime.lookup(filename))\n                 });\n                 res.write(file, 'binary');\n                 res.end();\n@@ -51,4 +52,4 @@ module.exports = function (port, root, wordy) {\n     console.log('The server is now launch on: http://localhost:' + port);\n     server.listen(port);\n     return server;\n-}\n\\ No newline at end of file\n+}\n"}
{"cve":"CVE-2018-16482:0708", "fix_patch": "diff --git a/lib/staticFileHandler.js b/lib/staticFileHandler.js\nindex cfdc1c7..ba733dd 100644\n--- a/lib/staticFileHandler.js\n+++ b/lib/staticFileHandler.js\n@@ -17,6 +17,9 @@ module.exports = function staticFileListener(options, nextHandler){\n         var mockPaths = getMockPathForMethod(options,req.method);\n \n         var filePath = httpHelpers.getRequestPathFromUrl(req.url);\n+        if (filePath.includes('..')) {\n+            return statusHandlers[403](res, nextHandler, { error: 'Forbidden' });\n+        }\n         var mockedFilePath = findMockFilePath(filePath,mockPaths);\n         if(mockedFilePath)\n             filePath = mockedFilePath;\n"}
{"cve":"CVE-2021-23363:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex a827f37..44d5d6f 100644\n--- a/index.js\n+++ b/index.js\n@@ -3,9 +3,13 @@\n const exec = require('child_process').execSync\n \n exports.killByPort = function (port) {\n+  const parsedPort = parseInt(port, 10)\n+  if (isNaN(parsedPort)) {\n+    return\n+  }\n   var processId = null\n   try {\n-    processId = exec(`lsof -t -i:${port}`)\n+    processId = exec(`lsof -t -i:${parsedPort}`)\n   } catch (e) {\n \n   }\n"}
{"cve":"CVE-2019-10788:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex d85f5ff..633ca7b 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,7 +1,7 @@\n /*jshint laxbreak:true */\n \n var sizeParser = require('filesize-parser');\n-var exec = require('child_process').exec, child;\n+var execFile = require('child_process').execFile, child;\n \n module.exports = function(path, opts, cb) {\n   if (!cb) {\n"}
{"cve":"CVE-2020-7675:0708", "fix_patch": "diff --git a/src/messenger-node.js b/src/messenger-node.js\nindex ec76b26..b140b16 100755\n--- a/src/messenger-node.js\n+++ b/src/messenger-node.js\n@@ -68,7 +68,11 @@ const messenger = {\n   line: color => {\n     if (color.length > 0) {\n       try {\n-        eval(`cl.${color}()`); // eslint-disable-line\n+        if (typeof cl[color] === 'function') {\n+          cl[color]();\n+        } else {\n+          throw new Error('Invalid color');\n+        }\n       }\n       catch (e) {\n         console.error(chalk.bgRed.bold(`Invalid Color: ${color}`));\n"}
{"cve":"CVE-2019-15597:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex 767584b..5c5b546 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -1,4 +1,4 @@\n-var exec = require('child_process').exec\n+var execFile = require('child_process').execFile\n var parse = require('./parse')\n \n module.exports = function df(aOptions, aCallback) {\n@@ -33,12 +33,13 @@ module.exports = function df(aOptions, aCallback) {\n \n     // TODO: should fail if unit is not a string\n \n-    var command = 'df -kP'\n+    var command = 'df'\n+    var args = ['-kP']\n     if (options.file) {\n-        command += ' ' + options.file\n+        args.push(options.file)\n     }\n \n-    exec(command, function(err, stdout, stderr) {\n+    execFile(command, args, function(err, stdout, stderr) {\n         if (err) {\n             callback(err)\n             return\n"}
{"cve":"CVE-2020-7627:0708", "fix_patch": "diff --git a/key-sender.js b/key-sender.js\nindex 2093f04..d44f329 100644\n--- a/key-sender.js\n+++ b/key-sender.js\n@@ -112,6 +112,32 @@ module.exports = function() {\n         return new Promise(function(resolve, reject) {\n             var jarPath = path.join(__dirname, 'jar', 'key-sender.jar');\n \n+for (var i = 0; i < arrParams.length; i++) {\n+arrParams[i] = arrParams[i].replace(/[^a-zA-Z0-9_.~@\\-/'\\\\\"`]/g, '');\n+}\n+\n+            var command = 'java -jar \"' + jarPath + '\" ' + arrParams.join(' ') + module.getCommandLineOptions();\n+\n+            return exec(command, {}, function(error, stdout, stderr) {\n+                if (error == null) {\n+                    resolve(stdout, stderr);\n+                } else {\n+                    reject(error, stdout, stderr);\n+                }\n+            });\n+        });\n+    };\n+\n+    module.execute = function(arrParams) {\n+        return new Promise(function(resolve, reject) {\n+            var jarPath = path.join(__dirname, 'jar', 'key-sender.jar');\n+\n+                for (var i = 0; i < arrParams.length; i++) {\n+                    if (/[^a-zA-Z0-9_.~@\\-/'\\\\\"`]/.test(arrParams[i])) {\n+                        return reject(new Error(\"Invalid characters in params.\"));\n+                    }\n+                }\n+\n             var command = 'java -jar \\\"' + jarPath + '\\\" ' + arrParams.join(' ') + module.getCommandLineOptions();\n \n             return exec(command, {}, function(error, stdout, stderr) {\n"}
{"cve":"CVE-2020-7631:0708", "fix_patch": "diff --git a/lib/posix.js b/lib/posix.js\nindex 76b4b98..d17bd06 100644\n--- a/lib/posix.js\n+++ b/lib/posix.js\n@@ -1,14 +1,10 @@\n 'use strict';\n \n-var exec = require('child_process').exec;\n+const { execFile } = require('child_process');\n var isDigits = require('./utils').isDigits;\n \n function diskusage(path, cb) {\n-    if (path.indexOf('\"') !== -1) {\n-        return cb(new Error('Paths with double quotes are not supported yet'));\n-    }\n-\n-    exec('df -k \"' + path + '\"', function(err, stdout) {\n+    execFile('df', ['-k', path], function(err, stdout) {\n         if (err) {\n             return cb(err);\n         }\n"}
{"cve":"CVE-2020-7795:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex a207f28..15d0c72 100644\n--- a/index.js\n+++ b/index.js\n@@ -11,9 +11,9 @@ module.exports = function (packageName, { registry = '', timeout = null } = {})\n         }\n \n         if (registry) {\n-            version = require('child_process').execSync(`npm view ${packageName} version --registry ${registry}`, config);\n+            version = require('child_process').execSync(`npm view ${packageName.split(' ')[0]} version --registry ${registry}`, config);\n         } else {\n-            version = require('child_process').execSync(`npm view ${packageName} version`, config);\n+            version = require('child_process').execSync(`npm view ${packageName.split(' ')[0]} version`, config);\n         }\n \n         if (version) {\n"}
{"cve":"CVE-2018-3772:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 88a62e9..2861c5f 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,31 +1,5 @@\n-var cp = require('child_process');\n+var which = require('which');\n \n module.exports = function whereis(name, cb) {\n-  cp.exec('which ' + name, function(error, stdout, stderr) {\n-    stdout = stdout.split('\\n')[0];\n-    if (error || stderr || stdout === '' || stdout.charAt(0) !== '/') {\n-      stdout = stdout.split('\\n')[0];\n-      cp.exec('whereis ' + name, function(error, stdout, stderr) {\n-        if (error || stderr || stdout === '' || stdout.indexOf( '/' ) === -1) {\n-          cp.exec('where ' + name, function (error, stdout, stderr) { //windows\n-            if (error || stderr || stdout === '' || stdout.indexOf('\\\\') === -1) {\n-              cp.exec('for %i in (' + name + '.exe) do @echo. %~$PATH:i', function (error, stdout, stderr) { //windows xp\n-                if (error || stderr || stdout === '' || stdout.indexOf('\\\\') === -1) {\n-                  return cb(new Error('Could not find ' + name + ' on your system'));\n-                }\n-                return cb(null, stdout);\n-              });\n-            } else {\n-              return cb(null, stdout);\n-            }\n-          });\n-        }\n-        else {\n-          return cb(null, stdout.split(' ')[1]);\n-        }\n-      });\n-    } else {\n-      return cb(null, stdout);\n-    }\n-  });\n+  which(name, cb);\n };\ndiff --git a/package.json b/package.json\nindex 92ad9a5..7132598 100644\n--- a/package.json\n+++ b/package.json\n@@ -16,6 +16,9 @@\n   \"devDependencies\": {\n     \"tap\": \"0.6.0\"\n   },\n+  \"dependencies\": {\n+    \"which\": \"^1.3.1\"\n+  },\n   \"scripts\": {\n     \"test\": \"node test\"\n   }\n"}
{"cve":"CVE-2020-7640:0708", "fix_patch": "diff --git a/class.js b/class.js\nindex 28476eb..bf91a69 100644\n--- a/class.js\n+++ b/class.js\n@@ -6,99 +6,98 @@ var util = require(\"util\");\n var events = require(\"events\");\n \n exports.create = function create(members) {\n-\t// create new class using php-style syntax (sort of)\n-\tif (!members) members = {};\n-\t\n-\t// setup constructor\n-\tvar constructor = null;\n-\t\n-\t// inherit from parent class\n-\tif (members.__parent) {\n-\t\tif (members.__construct) {\n-\t\t\t// explicit constructor passed in\n-\t\t\tconstructor = members.__construct;\n-\t\t}\n-\t\telse {\n-\t\t\t// inherit parent's constructor\n-\t\t\tvar code = members.__parent.toString();\n-\t\t\tvar args = code.substring( code.indexOf(\"(\")+1, code.indexOf(\")\") );\n-\t\t\tvar inner_code = code.substring( code.indexOf(\"{\")+1, code.lastIndexOf(\"}\") );\n-\t\t\teval('constructor = function ('+args+') {'+inner_code+'};');\n-\t\t}\n-\t\t\n-\t\t// inherit rest of parent members\n-\t\tutil.inherits(constructor, members.__parent);\n-\t\tdelete members.__parent;\n-\t}\n-\telse {\n-\t\t// create new base class\n-\t\tconstructor = members.__construct || function() {};\n-\t}\n-\tdelete members.__construct;\n-\t\n-\t// handle static variables\n-\tif (members.__static) {\n-\t\tfor (var key in members.__static) {\n-\t\t\tconstructor[key] = members.__static[key];\n-\t\t}\n-\t\tdelete members.__static;\n-\t}\n-\t\n-\t// all classes are event emitters unless explicitly disabled\n-\tif (members.__events !== false) {\n-\t\tif (!members.__mixins) members.__mixins = [];\n-\t\tif (members.__mixins.indexOf(events.EventEmitter) == -1) {\n-\t\t\tmembers.__mixins.push( events.EventEmitter );\n-\t\t}\n-\t}\n-\tdelete members.__events;\n-\t\n-\t// handle mixins\n-\tif (members.__mixins) {\n-\t\tfor (var idx = 0, len = members.__mixins.length; idx < len; idx++) {\n-\t\t\tvar class_obj = members.__mixins[idx];\n-\t\t\t\n-\t\t\tfor (var key in class_obj.prototype) {\n-\t\t\t\tif (!key.match(/^__/) && (typeof(constructor.prototype[key]) == 'undefined')) {\n-\t\t\t\t\tconstructor.prototype[key] = class_obj.prototype[key];\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tvar static_members = class_obj.__static;\n-\t\t\tif (static_members) {\n-\t\t\t\tfor (var key in static_members) {\n-\t\t\t\t\tif (typeof(constructor[key]) == 'undefined') constructor[key] = static_members[key];\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} // foreach mixin\n-\t\tdelete members.__mixins;\n-\t} // mixins\n-\t\n-\t// handle promisify (node 8+)\n-\tif (members.__promisify && util.promisify) {\n-\t\tif (Array.isArray(members.__promisify)) {\n-\t\t\t// promisify some\n-\t\t\tmembers.__promisify.forEach( function(key) {\n-\t\t\t\tif (typeof(members[key]) == 'function') {\n-\t\t\t\t\tmembers[key] = util.promisify( members[key] );\n-\t\t\t\t}\n-\t\t\t} );\n-\t\t}\n-\t\telse {\n-\t\t\t// promisify all\n-\t\t\tfor (var key in members) {\n-\t\t\t\tif (!key.match(/^__/) && (typeof(members[key]) == 'function')) {\n-\t\t\t\t\tmembers[key] = util.promisify( members[key] );\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\tdelete members.__promisify;\n-\t}\n-\t\n-\t// fill prototype members\n-\tfor (var key in members) {\n-\t\tconstructor.prototype[key] = members[key];\n-\t}\n-\t\n-\t// return completed class definition\n-\treturn constructor;\n+        // create new class using php-style syntax (sort of)\n+        if (!members) members = {};\n+        \n+        // setup constructor\n+        var constructor = null;\n+        \n+        // inherit from parent class\n+        if (members.__parent) {\n+                if (members.__construct) {\n+                        // explicit constructor passed in\n+                        constructor = members.__construct;\n+                }\n+                else {\n+                        // inherit parent's constructor\n+                        constructor = function() {\n+                                members.__parent.apply(this, arguments);\n+                        };\n+                }\n+                \n+                // inherit rest of parent members\n+                util.inherits(constructor, members.__parent);\n+                delete members.__parent;\n+        }\n+        else {\n+                // create new base class\n+                constructor = members.__construct || function() {};\n+        }\n+        delete members.__construct;\n+        \n+        // handle static variables\n+        if (members.__static) {\n+                for (var key in members.__static) {\n+                        constructor[key] = members.__static[key];\n+                }\n+                delete members.__static;\n+        }\n+        \n+        // all classes are event emitters unless explicitly disabled\n+        if (members.__events !== false) {\n+                if (!members.__mixins) members.__mixins = [];\n+                if (members.__mixins.indexOf(events.EventEmitter) == -1) {\n+                        members.__mixins.push( events.EventEmitter );\n+                }\n+        }\n+        delete members.__events;\n+        \n+        // handle mixins\n+        if (members.__mixins) {\n+                for (var idx = 0, len = members.__mixins.length; idx < len; idx++) {\n+                        var class_obj = members.__mixins[idx];\n+                        \n+                        for (var key in class_obj.prototype) {\n+                                if (!key.match(/^__/) && (typeof(constructor.prototype[key]) == 'undefined')) {\n+                                        constructor.prototype[key] = class_obj.prototype[key];\n+                                }\n+                        }\n+                        var static_members = class_obj.__static;\n+                        if (static_members) {\n+                                for (var key in static_members) {\n+                                        if (typeof(constructor[key]) == 'undefined') constructor[key] = static_members[key];\n+                                }\n+                        }\n+                } // foreach mixin\n+                delete members.__mixins;\n+        } // mixins\n+        \n+        // handle promisify (node 8+)\n+        if (members.__promisify && util.promisify) {\n+                if (Array.isArray(members.__promisify)) {\n+                        // promisify some\n+                        members.__promisify.forEach( function(key) {\n+                                if (typeof(members[key]) == 'function') {\n+                                        members[key] = util.promisify( members[key] );\n+                                }\n+                        } );\n+                }\n+                else {\n+                        // promisify all\n+                        for (var key in members) {\n+                                if (!key.match(/^__/) && (typeof(members[key]) == 'function')) {\n+                                        members[key] = util.promisify( members[key] );\n+                                }\n+                        }\n+                }\n+                delete members.__promisify;\n+        }\n+        \n+        // fill prototype members\n+        for (var key in members) {\n+                constructor.prototype[key] = members[key];\n+        }\n+        \n+        // return completed class definition\n+        return constructor;\n };\n"}
{"cve":"CVE-2017-16198:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex cad0b67..10a1c1b 100644\n--- a/index.js\n+++ b/index.js\n@@ -25,7 +25,8 @@ var main = function(req, res) {\n         logger.http(req, res);\n         \n         //\u4f7f\u7528url\u89e3\u6790\u6a21\u5757\u83b7\u53d6url\u4e2d\u7684\u8def\u5f84\u540d \n-        var pathName = url.parse(reqUrl).pathname;\n+                var pathName = url.parse(reqUrl).pathname.replace(/\\.{2}\\//g, '');\n+\n \n         //\u8865\u5168\u6587\u4ef6\u8def\u5f84\n         if (path.extname(pathName) == \"\") {\n@@ -37,7 +38,8 @@ var main = function(req, res) {\n             pathName += argvs.getDefault();\n         }\n \n-        //\u4f7f\u7528\u8def\u5f84\u89e3\u6790\u6a21\u5757,\u7ec4\u88c5\u5b9e\u9645\u6587\u4ef6\u8def\u5f84 \n+\n+\n         var filePath = path.join(argvs.getPath(), pathName);\n \n         //\u5224\u65ad\u6587\u4ef6\u662f\u5426\u5b58\u5728 \n"}
{"cve":"CVE-2020-7674:0708", "fix_patch": "diff --git a/lib/encode.js b/lib/encode.js\nindex 6756d1e..c407ec4 100644\n--- a/lib/encode.js\n+++ b/lib/encode.js\n@@ -1,9 +1,23 @@\n 'use strict';\n \n-function template(literal, data) {\n-  var tmpl = literal.replace(/(\\$\\{)/gm, '$1data.');\n+const variablen = /\\$\\{([^\\}]+)\\}/g;\n \n-  return eval('`' + tmpl + '`');\n+function template(literal, data) {\n+  return literal.replace(variablen, (match, variable) => {\n+    //\n+    // prevent JS injection\n+    //\n+    if (variable.match(/[^\\w\\.]/) !== null) {\n+      return '';\n+    }\n+\n+    try {\n+      const fn = new Function('data', 'return data.' + variable);\n+      return fn(data);\n+    } catch (e) {\n+      return '';\n+    }\n+  });\n }\n \n function encodeStatements(statements, data) {\n"}
{"cve":"CVE-2021-23376:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 8a9c7ea..d6e2c45 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,4 +1,4 @@\n-const { exec } = require(\"child_process\");\n+const { execFile } = require(\"child_process\");\n const fs = require(\"fs\");\n \n const filestostring = array => {\n@@ -216,8 +216,21 @@ const ffmpegjs = {\n   trimvideo: (input, start, duration, output) => {\n     return new Promise(function(resolve, reject) {\n       if (fs.existsSync(input)) {\n-        exec(\n-          `ffmpeg -hide_banner -loglevel quiet -ss ${start} -i ${input} -t ${duration} -c copy -y ${output}.mp4`,\n+                execFile(\"ffmpeg\", [\n+          \"-hide_banner\",\n+          \"-loglevel\",\n+          \"quiet\",\n+          \"-ss\",\n+          start,\n+          \"-i\",\n+          input,\n+          \"-t\",\n+          duration,\n+          \"-c\",\n+          \"copy\",\n+          \"-y\",\n+          `${output}.mp4`\n+        ],\n           (error, stdout, stderr) => {\n             if (error) {\n               reject(error);\ndiff --git a/package.json b/package.json\nindex dd5a85e..4f5b10d 100644\n--- a/package.json\n+++ b/package.json\n@@ -17,5 +17,7 @@\n   \"bugs\": \"https://github.com/TRomesh/ffmpegdotjs/issues\",\n   \"devDependencies\": {\n     \"jest\": \"^23.6.0\"\n+  },\n+  \"dependencies\": {\n   }\n }\ndiff --git a/yarn.lock b/yarn.lock\nindex cc2c180..ecffcbe 100644\n--- a/yarn.lock\n+++ b/yarn.lock\n@@ -2837,6 +2837,11 @@ shebang-regex@^1.0.0:\n   resolved \"https://registry.npmmirror.com/shebang-regex/-/shebang-regex-1.0.0.tgz\"\n   integrity sha512-wpoSFAxys6b2a2wHZ1XpDSgD7N9iVjg29Ph9uV/uaP9Ex/KXlkTZTeddxDPSYQpgvzKLGJke2UU0AzoGCjNIvQ==\n \n+shell-quote@^1.8.3:\n+  version \"1.8.3\"\n+  resolved \"https://registry.npmmirror.com/shell-quote/-/shell-quote-1.8.3.tgz\"\n+  integrity sha512-ObmnIF4hXNg1BqhnHmgbDETF8dLPCggZWBjkQfhZpbszZnYur5DUljTcCHii5LC3J5E0yeO/1LIMyH+UvHQgyw==\n+\n shellwords@^0.1.1:\n   version \"0.1.1\"\n   resolved \"https://registry.npmmirror.com/shellwords/-/shellwords-0.1.1.tgz\"\n"}
{"cve":"CVE-2020-28494:0708", "fix_patch": "diff --git a/image.js b/image.js\nindex 19631b27..e86e8f25 100755\n--- a/image.js\n+++ b/image.js\n@@ -43,18 +43,18 @@ var CACHE = {};\n var middlewares = {};\n \n if (!global.framework_utils)\n-\tglobal.framework_utils = require('./utils');\n+        global.framework_utils = require('./utils');\n \n function u16(buf, o) {\n-\treturn buf[o] << 8 | buf[o + 1];\n+        return buf[o] << 8 | buf[o + 1];\n }\n \n function u32(buf, o) {\n-\treturn buf[o] << 24 | buf[o + 1] << 16 | buf[o + 2] << 8 | buf[o + 3];\n+        return buf[o] << 24 | buf[o + 1] << 16 | buf[o + 2] << 8 | buf[o + 3];\n }\n \n exports.measureGIF = function(buffer) {\n-\treturn { width: buffer[6], height: buffer[8] };\n+        return { width: buffer[6], height: buffer[8] };\n };\n \n // MIT\n@@ -62,146 +62,146 @@ exports.measureGIF = function(buffer) {\n // visionmedia\n exports.measureJPG = function(buffer) {\n \n-\tvar len = buffer.length;\n-\tvar o = 0;\n+        var len = buffer.length;\n+        var o = 0;\n \n-\tvar jpeg = 0xff == buffer[0] && 0xd8 == buffer[1];\n-\tif (jpeg) {\n-\t\to += 2;\n-\t\twhile (o < len) {\n-\t\t\twhile (0xff != buffer[o]) o++;\n-\t\t\twhile (0xff == buffer[o]) o++;\n-\t\t\tif (sof[buffer[o]])\n-\t\t\t\treturn { width: u16(buffer, o + 6), height: u16(buffer, o + 4) };\n-\t\t\telse\n-\t\t\t\to += u16(buffer, ++o);\n+        var jpeg = 0xff == buffer[0] && 0xd8 == buffer[1];\n+        if (jpeg) {\n+                o += 2;\n+                while (o < len) {\n+                        while (0xff != buffer[o]) o++;\n+                        while (0xff == buffer[o]) o++;\n+                        if (sof[buffer[o]])\n+                                return { width: u16(buffer, o + 6), height: u16(buffer, o + 4) };\n+                        else\n+                                o += u16(buffer, ++o);\n \n-\t\t}\n-\t}\n+                }\n+        }\n \n-\treturn null;\n+        return null;\n };\n \n // MIT\n // Written by TJ Holowaychuk\n // visionmedia\n exports.measurePNG = function(buffer) {\n-\treturn { width: u32(buffer, 16), height: u32(buffer, 16 + 4) };\n+        return { width: u32(buffer, 16), height: u32(buffer, 16 + 4) };\n };\n \n exports.measureSVG = function(buffer) {\n \n-\tvar match = buffer.toString('utf8').match(REGEXP_SVG);\n-\tif (!match)\n-\t\treturn;\n+        var match = buffer.toString('utf8').match(REGEXP_SVG);\n+        if (!match)\n+                return;\n \n-\tvar width = 0;\n-\tvar height = 0;\n+        var width = 0;\n+        var height = 0;\n \n-\tfor (var i = 0, length = match.length; i < length; i++) {\n-\t\tvar value = match[i];\n+        for (var i = 0, length = match.length; i < length; i++) {\n+                var value = match[i];\n \n-\t\tif (width > 0 && height > 0)\n-\t\t\tbreak;\n+                if (width > 0 && height > 0)\n+                        break;\n \n-\t\tif (!width && value.startsWith('width=\"'))\n-\t\t\twidth = value.parseInt2();\n+                if (!width && value.startsWith('width=\"'))\n+                        width = value.parseInt2();\n \n-\t\tif (!height && value.startsWith('height=\"'))\n-\t\t\theight = value.parseInt2();\n-\t}\n+                if (!height && value.startsWith('height=\"'))\n+                        height = value.parseInt2();\n+        }\n \n-\treturn { width: width, height: height };\n+        return { width: width, height: height };\n };\n \n exports.measure = function(type, buffer) {\n-\tswitch (type) {\n-\t\tcase '.jpg':\n-\t\tcase '.jpeg':\n-\t\tcase 'jpg':\n-\t\tcase 'jpeg':\n-\t\tcase 'image/jpeg':\n-\t\t\treturn exports.measureJPG(buffer);\n-\t\tcase '.gif':\n-\t\tcase 'gif':\n-\t\tcase 'image/gif':\n-\t\t\treturn exports.measureGIF(buffer);\n-\t\tcase '.png':\n-\t\tcase 'png':\n-\t\tcase 'image/png':\n-\t\t\treturn exports.measurePNG(buffer);\n-\t\tcase '.svg':\n-\t\tcase 'svg':\n-\t\tcase 'image/svg+xml':\n-\t\t\treturn exports.measureSVG(buffer);\n-\t}\n+        switch (type) {\n+                case '.jpg':\n+                case '.jpeg':\n+                case 'jpg':\n+                case 'jpeg':\n+                case 'image/jpeg':\n+                        return exports.measureJPG(buffer);\n+                case '.gif':\n+                case 'gif':\n+                case 'image/gif':\n+                        return exports.measureGIF(buffer);\n+                case '.png':\n+                case 'png':\n+                case 'image/png':\n+                        return exports.measurePNG(buffer);\n+                case '.svg':\n+                case 'svg':\n+                case 'image/svg+xml':\n+                        return exports.measureSVG(buffer);\n+        }\n };\n \n function Image(filename, cmd, width, height) {\n-\tvar type = typeof(filename);\n-\tthis.width = width;\n-\tthis.height = height;\n-\tthis.builder = [];\n-\tthis.filename = type === 'string' ? filename : null;\n-\tthis.currentStream = type === 'object' ? filename : null;\n-\tthis.outputType = type === 'string' ? framework_utils.getExtension(filename) : 'jpg';\n-\tthis.islimit = false;\n-\tthis.cmdarg = cmd || CONF.default_image_converter;\n+        var type = typeof(filename);\n+        this.width = width;\n+        this.height = height;\n+        this.builder = [];\n+        this.filename = type === 'string' ? filename : null;\n+        this.currentStream = type === 'object' ? filename : null;\n+        this.outputType = type === 'string' ? framework_utils.getExtension(filename) : 'jpg';\n+        this.islimit = false;\n+        this.cmdarg = cmd || CONF.default_image_converter;\n }\n \n var ImageProto = Image.prototype;\n \n ImageProto.clear = function() {\n-\tvar self = this;\n-\tself.builder = [];\n-\treturn self;\n+        var self = this;\n+        self.builder = [];\n+        return self;\n };\n \n ImageProto.measure = function(callback) {\n \n-\tvar self = this;\n-\tvar index = self.filename.lastIndexOf('.');\n+        var self = this;\n+        var index = self.filename.lastIndexOf('.');\n \n-\tif (!self.filename) {\n-\t\tcallback(new Error('Measure does not support stream.'));\n-\t\treturn;\n-\t}\n+        if (!self.filename) {\n+                callback(new Error('Measure does not support stream.'));\n+                return;\n+        }\n \n-\tif (index === -1) {\n-\t\tcallback(new Error('This type of file is not supported.'));\n-\t\treturn;\n-\t}\n+        if (index === -1) {\n+                callback(new Error('This type of file is not supported.'));\n+                return;\n+        }\n \n-\tF.stats.performance.open++;\n-\tvar extension = self.filename.substring(index).toLowerCase();\n-\tvar stream = require('fs').createReadStream(self.filename, { start: 0, end: extension === '.jpg' ? 40000 : 24 });\n+        F.stats.performance.open++;\n+        var extension = self.filename.substring(index).toLowerCase();\n+        var stream = require('fs').createReadStream(self.filename, { start: 0, end: extension === '.jpg' ? 40000 : 24 });\n \n-\tstream.on('data', function(buffer) {\n+        stream.on('data', function(buffer) {\n \n-\t\tswitch (extension) {\n-\t\t\tcase '.jpg':\n-\t\t\t\tcallback(null, exports.measureJPG(buffer));\n-\t\t\t\treturn;\n-\t\t\tcase '.gif':\n-\t\t\t\tcallback(null, exports.measureGIF(buffer));\n-\t\t\t\treturn;\n-\t\t\tcase '.png':\n-\t\t\t\tcallback(null, exports.measurePNG(buffer));\n-\t\t\t\treturn;\n-\t\t}\n+                switch (extension) {\n+                        case '.jpg':\n+                                callback(null, exports.measureJPG(buffer));\n+                                return;\n+                        case '.gif':\n+                                callback(null, exports.measureGIF(buffer));\n+                                return;\n+                        case '.png':\n+                                callback(null, exports.measurePNG(buffer));\n+                                return;\n+                }\n \n-\t\tcallback(new Error('This type of file is not supported.'));\n-\t});\n+                callback(new Error('This type of file is not supported.'));\n+        });\n \n-\tstream.on('error', callback);\n-\treturn self;\n+        stream.on('error', callback);\n+        return self;\n };\n \n ImageProto.$$measure = function() {\n-\tvar self = this;\n-\treturn function(callback) {\n-\t\tself.measure(callback);\n-\t};\n+        var self = this;\n+        return function(callback) {\n+                self.measure(callback);\n+        };\n };\n \n /**\n@@ -213,101 +213,108 @@ ImageProto.$$measure = function() {\n  */\n ImageProto.save = function(filename, callback, writer) {\n \n-\tvar self = this;\n+        var self = this;\n \n-\tif (typeof(filename) === 'function') {\n-\t\tcallback = filename;\n-\t\tfilename = null;\n-\t}\n+        if (typeof(filename) === 'function') {\n+                callback = filename;\n+                filename = null;\n+        }\n \n-\t!self.builder.length && self.minify();\n-\tfilename = filename || self.filename || '';\n+        !self.builder.length && self.minify();\n+        filename = filename || self.filename || '';\n \n-\tvar command = self.cmd(self.filename ? self.filename : '-', filename);\n+        var command = self.cmd(self.filename ? self.filename : '-', filename);\n \n-\tif (F.isWindows)\n-\t\tcommand = command.replace(REGEXP_PATH, '\\\\');\n+        if (F.isWindows)\n+                command = command.replace(REGEXP_PATH, '\\\\');\n \n-\tvar cmd = exec(command, function(err) {\n+        var cmd = exec(command, function(err) {\n \n-\t\t// clean up\n-\t\tcmd.kill();\n-\t\tcmd = null;\n+                // clean up\n+                cmd.kill();\n+                cmd = null;\n \n-\t\tself.clear();\n+                self.clear();\n \n-\t\tif (!callback)\n-\t\t\treturn;\n+                if (!callback)\n+                        return;\n \n-\t\tif (err) {\n-\t\t\tcallback(err, false);\n-\t\t\treturn;\n-\t\t}\n+                if (err) {\n+                        callback(err, false);\n+                        return;\n+                }\n \n-\t\tvar middleware = middlewares[self.outputType];\n-\t\tif (!middleware)\n-\t\t\treturn callback(null, true);\n+                var middleware = middlewares[self.outputType];\n+                if (!middleware)\n+                        return callback(null, true);\n \n-\t\tF.stats.performance.open++;\n-\t\tvar reader = Fs.createReadStream(filename);\n-\t\tvar writer = Fs.createWriteStream(filename + '_');\n+                F.stats.performance.open++;\n+                var reader = Fs.createReadStream(filename);\n+                var writer = Fs.createWriteStream(filename + '_');\n \n-\t\treader.pipe(middleware()).pipe(writer);\n-\t\twriter.on('finish', () => Fs.rename(filename + '_', filename, () => callback(null, true)));\n-\t});\n+                reader.pipe(middleware()).pipe(writer);\n+                writer.on('finish', () => Fs.rename(filename + '_', filename, () => callback(null, true)));\n+        });\n \n-\tif (self.currentStream) {\n-\t\tif (self.currentStream instanceof Buffer)\n-\t\t\tcmd.stdin.end(self.currentStream);\n-\t\telse\n-\t\t\tself.currentStream.pipe(cmd.stdin);\n-\t}\n+        if (self.currentStream) {\n+                if (self.currentStream instanceof Buffer)\n+                        cmd.stdin.end(self.currentStream);\n+                else\n+                        self.currentStream.pipe(cmd.stdin);\n+        }\n \n-\tCLEANUP(cmd.stdin);\n-\twriter && writer(cmd.stdin);\n-\treturn self;\n+        CLEANUP(cmd.stdin);\n+        writer && writer(cmd.stdin);\n+        return self;\n };\n \n ImageProto.$$save = function(filename, writer) {\n-\tvar self = this;\n-\treturn function(callback) {\n-\t\tself.save(filename, callback, writer);\n-\t};\n+        var self = this;\n+        return function(callback) {\n+                self.save(filename, callback, writer);\n+        };\n };\n \n ImageProto.pipe = function(stream, type, options) {\n \n-\tvar self = this;\n+        var self = this;\n \n-\tif (typeof(type) === 'object') {\n-\t\toptions = type;\n-\t\ttype = null;\n-\t}\n+        if (typeof(type) === 'object') {\n+                options = type;\n+                type = null;\n+        }\n \n-\t!self.builder.length && self.minify();\n-\t!type && (type = self.outputType);\n+        !self.builder.length && self.minify();\n+        !type && (type = self.outputType);\n \n-\tF.stats.performance.open++;\n-\tvar cmd = spawn(CMD_CONVERT[self.cmdarg], self.arg(self.filename ? wrap(self.filename) : '-', (type ? type + ':' : '') + '-'), SPAWN_OPT);\n-\tcmd.stderr.on('data', stream.emit.bind(stream, 'error'));\n-\tcmd.stdout.on('data', stream.emit.bind(stream, 'data'));\n-\tcmd.stdout.on('end', stream.emit.bind(stream, 'end'));\n-\tcmd.on('error', stream.emit.bind(stream, 'error'));\n+if (type.match(/^[a-zA-Z0-9]+$/) === null) {\n+stream.emit('error', new Error('Invalid image type'));\n+return self;\n+}\n+\n+        F.stats.performance.open++;\n+        var command = CMD_CONVERT2[self.cmdarg] + ' ' + self.arg(self.filename ? wrap(self.filename) : '-', (type ? type + ':' : '') + '-').join(' ');\n+        var cmd = exec(command);\n+\n+        cmd.stderr.on('data', stream.emit.bind(stream, 'error'));\n+        cmd.stdout.on('data', stream.emit.bind(stream, 'data'));\n+        cmd.stdout.on('end', stream.emit.bind(stream, 'end'));\n+        cmd.on('error', stream.emit.bind(stream, 'error'));\n \n-\tvar middleware = middlewares[type];\n-\tif (middleware)\n-\t\tcmd.stdout.pipe(middleware()).pipe(stream, options);\n-\telse\n-\t\tcmd.stdout.pipe(stream, options);\n+        var middleware = middlewares[type];\n+        if (middleware)\n+                cmd.stdout.pipe(middleware()).pipe(stream, options);\n+        else\n+                cmd.stdout.pipe(stream, options);\n \n-\tif (self.currentStream) {\n-\t\tif (self.currentStream instanceof Buffer)\n-\t\t\tcmd.stdin.end(self.currentStream);\n-\t\telse\n-\t\t\tself.currentStream.pipe(cmd.stdin);\n-\t}\n+        if (self.currentStream) {\n+                if (self.currentStream instanceof Buffer)\n+                        cmd.stdin.end(self.currentStream);\n+                else\n+                        self.currentStream.pipe(cmd.stdin);\n+        }\n \n-\treturn self;\n+        return self;\n };\n \n /**\n@@ -318,228 +325,235 @@ ImageProto.pipe = function(stream, type, options) {\n  */\n ImageProto.stream = function(type, writer) {\n \n-\tvar self = this;\n+        var self = this;\n \n-\t!self.builder.length && self.minify();\n+        !self.builder.length && self.minify();\n+\n+        if (!type)\n+                type = self.outputType;\n+\n+if (type.match(/^[a-zA-Z0-9]+$/) === null) {\n+ F.error(new Error('Invalid image type'));\n+return;\n+}\n \n-\tif (!type)\n-\t\ttype = self.outputType;\n+        F.stats.performance.open++;\n+        var command = CMD_CONVERT2[self.cmdarg] + ' ' + self.arg(self.filename ? wrap(self.filename) : '-', (type ? type + ':' : '') + '-').join(' ');\n+        var cmd = exec(command);\n \n-\tF.stats.performance.open++;\n-\tvar cmd = spawn(CMD_CONVERT[self.cmdarg], self.arg(self.filename ? wrap(self.filename) : '-', (type ? type + ':' : '') + '-'), SPAWN_OPT);\n-\tif (self.currentStream) {\n-\t\tif (self.currentStream instanceof Buffer)\n-\t\t\tcmd.stdin.end(self.currentStream);\n-\t\telse\n-\t\t\tself.currentStream.pipe(cmd.stdin);\n-\t}\n+        if (self.currentStream) {\n+                if (self.currentStream instanceof Buffer)\n+                        cmd.stdin.end(self.currentStream);\n+                else\n+                        self.currentStream.pipe(cmd.stdin);\n+        }\n \n-\twriter && writer(cmd.stdin);\n-\tvar middleware = middlewares[type];\n-\treturn middleware ? cmd.stdout.pipe(middleware()) : cmd.stdout;\n+        writer && writer(cmd.stdin);\n+        var middleware = middlewares[type];\n+        return middleware ? cmd.stdout.pipe(middleware()) : cmd.stdout;\n };\n \n ImageProto.cmd = function(filenameFrom, filenameTo) {\n \n-\tvar self = this;\n-\tvar cmd = '';\n+        var self = this;\n+        var cmd = '';\n \n-\tif (!self.islimit) {\n-\t\tvar tmp = CONF.default_image_consumption;\n-\t\tif (tmp) {\n-\t\t\tself.limit('memory', (1500 / 100) * tmp);\n-\t\t\tself.limit('map', (3000 / 100) * tmp);\n-\t\t}\n-\t}\n+        if (!self.islimit) {\n+                var tmp = CONF.default_image_consumption;\n+                if (tmp) {\n+                        self.limit('memory', (1500 / 100) * tmp);\n+                        self.limit('map', (3000 / 100) * tmp);\n+                }\n+        }\n \n-\tself.builder.sort(sort);\n+        self.builder.sort(sort);\n \n-\tvar length = self.builder.length;\n-\tfor (var i = 0; i < length; i++)\n-\t\tcmd += (cmd ? ' ' : '') + self.builder[i].cmd;\n+        var length = self.builder.length;\n+        for (var i = 0; i < length; i++)\n+                cmd += (cmd ? ' ' : '') + self.builder[i].cmd;\n \n-\treturn CMD_CONVERT2[self.cmdarg] + wrap(filenameFrom, true) + ' ' + cmd + wrap(filenameTo, true);\n+        return CMD_CONVERT2[self.cmdarg] + wrap(filenameFrom, true) + ' ' + cmd + wrap(filenameTo, true);\n };\n \n function sort(a, b) {\n-\treturn a.priority > b.priority ? 1 : -1;\n+        return a.priority > b.priority ? 1 : -1;\n }\n \n ImageProto.arg = function(first, last) {\n \n-\tvar self = this;\n-\tvar arr = [];\n+        var self = this;\n+        var arr = [];\n \n-\tif (self.cmdarg === 'gm')\n-\t\tarr.push('convert');\n+        if (self.cmdarg === 'gm')\n+                arr.push('convert');\n \n-\tfirst && arr.push(first);\n+        first && arr.push(first);\n \n-\tif (!self.islimit) {\n-\t\tvar tmp = CONF.default_image_consumption;\n-\t\tif (tmp) {\n-\t\t\tself.limit('memory', (1500 / 100) * tmp);\n-\t\t\tself.limit('map', (3000 / 100) * tmp);\n-\t\t}\n-\t}\n+        if (!self.islimit) {\n+                var tmp = CONF.default_image_consumption;\n+                if (tmp) {\n+                        self.limit('memory', (1500 / 100) * tmp);\n+                        self.limit('map', (3000 / 100) * tmp);\n+                }\n+        }\n \n-\tself.builder.sort(sort);\n+        self.builder.sort(sort);\n \n-\tvar length = self.builder.length;\n+        var length = self.builder.length;\n \n-\tfor (var i = 0; i < length; i++) {\n-\t\tvar o = self.builder[i];\n-\t\tvar index = o.cmd.indexOf(' ');\n-\t\tif (index === -1)\n-\t\t\tarr.push(o.cmd);\n-\t\telse {\n-\t\t\tarr.push(o.cmd.substring(0, index));\n-\t\t\tarr.push(o.cmd.substring(index + 1).replace(/\"/g, ''));\n-\t\t}\n-\t}\n+        for (var i = 0; i < length; i++) {\n+                var o = self.builder[i];\n+                var index = o.cmd.indexOf(' ');\n+                if (index === -1)\n+                        arr.push(o.cmd);\n+                else {\n+                        arr.push(o.cmd.substring(0, index));\n+                        arr.push(o.cmd.substring(index + 1).replace(/\"/g, ''));\n+                }\n+        }\n \n-\tlast && arr.push(last);\n-\treturn arr;\n+        last && arr.push(last);\n+        return arr;\n };\n \n ImageProto.identify = function(callback) {\n-\tvar self = this;\n-\tF.stats.performance.open++;\n-\texec((self.cmdarg === 'gm' ? 'gm ' : '') + 'identify' + wrap(self.filename, true), function(err, stdout) {\n+        var self = this;\n+        F.stats.performance.open++;\n+        exec((self.cmdarg === 'gm' ? 'gm ' : '') + 'identify' + wrap(self.filename, true), function(err, stdout) {\n \n-\t\tif (err) {\n-\t\t\tcallback(err, null);\n-\t\t\treturn;\n-\t\t}\n+                if (err) {\n+                        callback(err, null);\n+                        return;\n+                }\n \n-\t\tvar arr = stdout.split(' ');\n-\t\tvar size = arr[2].split('x');\n-\t\tvar obj = { type: arr[1], width: framework_utils.parseInt(size[0]), height: framework_utils.parseInt(size[1]) };\n-\t\tcallback(null, obj);\n-\t});\n+                var arr = stdout.split(' ');\n+                var size = arr[2].split('x');\n+                var obj = { type: arr[1], width: framework_utils.parseInt(size[0]), height: framework_utils.parseInt(size[1]) };\n+                callback(null, obj);\n+        });\n \n-\treturn self;\n+        return self;\n };\n \n ImageProto.$$identify = function() {\n-\tvar self = this;\n-\treturn function(callback) {\n-\t\tself.identify(callback);\n-\t};\n+        var self = this;\n+        return function(callback) {\n+                self.identify(callback);\n+        };\n };\n \n ImageProto.push = function(key, value, priority, encode) {\n-\tvar self = this;\n-\tvar cmd = key;\n+        var self = this;\n+        var cmd = key;\n \n-\tif (value != null) {\n-\t\tif (encode && typeof(value) === 'string')\n-\t\t\tcmd += ' ' + D + value.replace(REGEXP_ESCAPE, '') + D;\n-\t\telse\n-\t\t\tcmd += ' ' + value;\n-\t}\n+        if (value != null) {\n+                if (encode && typeof(value) === 'string')\n+                        cmd += ' ' + D + value.replace(REGEXP_ESCAPE, '') + D;\n+                else\n+                        cmd += ' ' + value;\n+        }\n \n-\tvar obj = CACHE[cmd];\n-\tif (obj) {\n-\t\tobj.priority = priority;\n-\t\tself.builder.push(obj);\n-\t} else {\n-\t\tCACHE[cmd] = { cmd: cmd, priority: priority };\n-\t\tself.builder.push(CACHE[cmd]);\n-\t}\n+        var obj = CACHE[cmd];\n+        if (obj) {\n+                obj.priority = priority;\n+                self.builder.push(obj);\n+        } else {\n+                CACHE[cmd] = { cmd: cmd, priority: priority };\n+                self.builder.push(CACHE[cmd]);\n+        }\n \n-\treturn self;\n+        return self;\n };\n \n ImageProto.output = function(type) {\n-\tvar self = this;\n-\tif (type[0] === '.')\n-\t\ttype = type.substring(1);\n-\tself.outputType = type;\n-\treturn self;\n+        var self = this;\n+        if (type[0] === '.')\n+                type = type.substring(1);\n+        self.outputType = type;\n+        return self;\n };\n \n ImageProto.resize = function(w, h, options) {\n-\toptions = options || '';\n+        options = options || '';\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w + 'x';\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w + 'x';\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\treturn self.push('-resize', size + options, 1, true);\n+        return self.push('-resize', size + options, 1, true);\n };\n \n ImageProto.thumbnail = function(w, h, options) {\n-\toptions = options || '';\n+        options = options || '';\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w;\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w;\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\treturn self.push('-thumbnail', size + options, 1, true);\n+        return self.push('-thumbnail', size + options, 1, true);\n };\n \n ImageProto.geometry = function(w, h, options) {\n-\toptions = options || '';\n+        options = options || '';\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w;\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w;\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\treturn self.push('-geometry', size + options, 1, true);\n+        return self.push('-geometry', size + options, 1, true);\n };\n \n \n ImageProto.filter = function(type) {\n-\treturn this.push('-filter', type, 1, true);\n+        return this.push('-filter', type, 1, true);\n };\n \n ImageProto.trim = function() {\n-\treturn this.push('-trim +repage', 1);\n+        return this.push('-trim +repage', 1);\n };\n \n ImageProto.limit = function(type, value) {\n-\tthis.islimit = true;\n-\treturn this.push('-limit', type + ' ' + value, 1);\n+        this.islimit = true;\n+        return this.push('-limit', type + ' ' + value, 1);\n };\n \n ImageProto.extent = function(w, h, x, y) {\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w;\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w;\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\tif (x || y) {\n-\t\t!x && (x = 0);\n-\t\t!y && (y = 0);\n-\t\tsize += (x >= 0 ? '+' : '') + x + (y >= 0 ? '+' : '') + y;\n-\t}\n+        if (x || y) {\n+                !x && (x = 0);\n+                !y && (y = 0);\n+                size += (x >= 0 ? '+' : '') + x + (y >= 0 ? '+' : '') + y;\n+        }\n \n-\treturn self.push('-extent', size, 4, true);\n+        return self.push('-extent', size, 4, true);\n };\n \n /**\n@@ -551,7 +565,7 @@ ImageProto.extent = function(w, h, x, y) {\n  * @return {Image}\n  */\n ImageProto.miniature = function(w, h, color, filter) {\n-\treturn this.filter(filter || 'Hamming').thumbnail(w, h).background(color ? color : 'white').align('center').extent(w, h);\n+        return this.filter(filter || 'Hamming').thumbnail(w, h).background(color ? color : 'white').align('center').extent(w, h);\n };\n \n /**\n@@ -562,7 +576,7 @@ ImageProto.miniature = function(w, h, color, filter) {\n  * @return {Image}\n  */\n ImageProto.resizeCenter = ImageProto.resize_center = function(w, h, color) {\n-\treturn this.resize(w, h, '^').background(color ? color : 'white').align('center').crop(w, h);\n+        return this.resize(w, h, '^').background(color ? color : 'white').align('center').crop(w, h);\n };\n \n /**\n@@ -574,187 +588,187 @@ ImageProto.resizeCenter = ImageProto.resize_center = function(w, h, color) {\n  * @return {Image}\n  */\n ImageProto.resizeAlign = ImageProto.resize_align = function(w, h, align, color) {\n-\treturn this.resize(w, h, '^').background(color ? color : 'white').align(align || 'center').crop(w, h);\n+        return this.resize(w, h, '^').background(color ? color : 'white').align(align || 'center').crop(w, h);\n };\n \n ImageProto.scale = function(w, h, options) {\n-\toptions = options || '';\n+        options = options || '';\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w;\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w;\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\treturn self.push('-scale', size + options, 1, true);\n+        return self.push('-scale', size + options, 1, true);\n };\n \n ImageProto.crop = function(w, h, x, y) {\n-\treturn this.push('-crop', w + 'x' + h + '+' + (x || 0) + '+' + (y || 0), 4, true);\n+        return this.push('-crop', w + 'x' + h + '+' + (x || 0) + '+' + (y || 0), 4, true);\n };\n \n ImageProto.quality = function(percentage) {\n-\treturn this.push('-quality', percentage || 80, 5, true);\n+        return this.push('-quality', percentage || 80, 5, true);\n };\n \n ImageProto.align = function(type) {\n \n-\tvar output;\n-\n-\tswitch (type) {\n-\t\tcase 'left top':\n-\t\tcase 'top left':\n-\t\t\toutput = 'NorthWest';\n-\t\t\tbreak;\n-\t\tcase 'left bottom':\n-\t\tcase 'bottom left':\n-\t\t\toutput = 'SouthWest';\n-\t\t\tbreak;\n-\t\tcase 'right top':\n-\t\tcase 'top right':\n-\t\t\toutput = 'NorthEast';\n-\t\t\tbreak;\n-\t\tcase 'right bottom':\n-\t\tcase 'bottom right':\n-\t\t\toutput = 'SouthEast';\n-\t\t\tbreak;\n-\t\tcase 'left center':\n-\t\tcase 'center left':\n-\t\tcase 'left':\n-\t\t\toutput = 'West';\n-\t\t\tbreak;\n-\t\tcase 'right center':\n-\t\tcase 'center right':\n-\t\tcase 'right':\n-\t\t\toutput = 'East';\n-\t\t\tbreak;\n-\t\tcase 'bottom center':\n-\t\tcase 'center bottom':\n-\t\tcase 'bottom':\n-\t\t\toutput = 'South';\n-\t\t\tbreak;\n-\t\tcase 'top center':\n-\t\tcase 'center top':\n-\t\tcase 'top':\n-\t\t\toutput = 'North';\n-\t\t\tbreak;\n-\t\tcase 'center center':\n-\t\tcase 'center':\n-\t\tcase 'middle':\n-\t\t\toutput = 'Center';\n-\t\t\tbreak;\n-\t\tdefault:\n-\t\t\toutput = type;\n-\t\t\tbreak;\n-\t}\n-\n-\toutput && this.push('-gravity', output, 3, true);\n-\treturn this;\n+        var output;\n+\n+        switch (type) {\n+                case 'left top':\n+                case 'top left':\n+                        output = 'NorthWest';\n+                        break;\n+                case 'left bottom':\n+                case 'bottom left':\n+                        output = 'SouthWest';\n+                        break;\n+                case 'right top':\n+                case 'top right':\n+                        output = 'NorthEast';\n+                        break;\n+                case 'right bottom':\n+                case 'bottom right':\n+                        output = 'SouthEast';\n+                        break;\n+                case 'left center':\n+                case 'center left':\n+                case 'left':\n+                        output = 'West';\n+                        break;\n+                case 'right center':\n+                case 'center right':\n+                case 'right':\n+                        output = 'East';\n+                        break;\n+                case 'bottom center':\n+                case 'center bottom':\n+                case 'bottom':\n+                        output = 'South';\n+                        break;\n+                case 'top center':\n+                case 'center top':\n+                case 'top':\n+                        output = 'North';\n+                        break;\n+                case 'center center':\n+                case 'center':\n+                case 'middle':\n+                        output = 'Center';\n+                        break;\n+                default:\n+                        output = type;\n+                        break;\n+        }\n+\n+        output && this.push('-gravity', output, 3, true);\n+        return this;\n };\n \n ImageProto.gravity = function(type) {\n-\treturn this.align(type);\n+        return this.align(type);\n };\n \n ImageProto.blur = function(radius) {\n-\treturn this.push('-blur', radius, 10, true);\n+        return this.push('-blur', radius, 10, true);\n };\n \n ImageProto.normalize = function() {\n-\treturn this.push('-normalize', null, 10);\n+        return this.push('-normalize', null, 10);\n };\n \n ImageProto.rotate = function(deg) {\n-\treturn this.push('-rotate', deg || 0, 8, true);\n+        return this.push('-rotate', deg || 0, 8, true);\n };\n \n ImageProto.flip = function() {\n-\treturn this.push('-flip', null, 10);\n+        return this.push('-flip', null, 10);\n };\n \n ImageProto.flop = function() {\n-\treturn this.push('-flop', null, 10);\n+        return this.push('-flop', null, 10);\n };\n \n ImageProto.define = function(value) {\n-\treturn this.push('-define', value, 10, true);\n+        return this.push('-define', value, 10, true);\n };\n \n ImageProto.minify = function() {\n-\treturn this.push('+profile', '*', null, 10, true);\n+        return this.push('+profile', '*', null, 10, true);\n };\n \n ImageProto.grayscale = function() {\n-\treturn this.push('-colorspace', 'Gray', 10, true);\n+        return this.push('-colorspace', 'Gray', 10, true);\n };\n \n ImageProto.bitdepth = function(value) {\n-\treturn this.push('-depth', value, 10, true);\n+        return this.push('-depth', value, 10, true);\n };\n \n ImageProto.colors = function(value) {\n-\treturn this.push('-colors', value, 10, true);\n+        return this.push('-colors', value, 10, true);\n };\n \n ImageProto.background = function(color) {\n-\treturn this.push('-background', color, 2, true).push('-extent 0x0', null, 2);\n+        return this.push('-background', color, 2, true).push('-extent 0x0', null, 2);\n };\n \n ImageProto.fill = function(color) {\n-\treturn this.push('-fill', color, 2, true);\n+        return this.push('-fill', color, 2, true);\n };\n \n ImageProto.sepia = function() {\n-\treturn this.push('-modulate', '115,0,100', 4).push('-colorize', '7,21,50', 5);\n+        return this.push('-modulate', '115,0,100', 4).push('-colorize', '7,21,50', 5);\n };\n \n ImageProto.watermark = function(filename, x, y, w, h) {\n-\treturn this.push('-draw', 'image over {1},{2} {3},{4} {5}{0}{5}'.format(filename, x || 0, y || 0, w || 0, h || 0, D), 6, true);\n+        return this.push('-draw', 'image over {1},{2} {3},{4} {5}{0}{5}'.format(filename, x || 0, y || 0, w || 0, h || 0, D), 6, true);\n };\n \n ImageProto.make = function(fn) {\n-\tfn.call(this, this);\n-\treturn this;\n+        fn.call(this, this);\n+        return this;\n };\n \n ImageProto.command = function(key, value, priority, esc) {\n \n-\tif (priority === true) {\n-\t\tpriority = 0;\n-\t\tesc = true;\n-\t}\n+        if (priority === true) {\n+                priority = 0;\n+                esc = true;\n+        }\n \n-\treturn this.push(key, value, priority || 10, esc);\n+        return this.push(key, value, priority || 10, esc);\n };\n \n function wrap(command, empty) {\n-\treturn (empty ? ' ' : '') + (command === '-' ? command : (D + command.replace(REGEXP_ESCAPE, '') + D));\n+        return (empty ? ' ' : '') + (command === '-' ? command : (D + command.replace(REGEXP_ESCAPE, '') + D));\n }\n \n exports.Image = Image;\n exports.Picture = Image;\n \n exports.init = function(filename, cmd, width, height) {\n-\treturn new Image(filename, cmd, width, height);\n+        return new Image(filename, cmd, width, height);\n };\n \n exports.load = function(filename, cmd, width, height) {\n-\treturn new Image(filename, cmd, width, height);\n+        return new Image(filename, cmd, width, height);\n };\n \n exports.middleware = function(type, fn) {\n-\tif (type[0] === '.')\n-\t\ttype = type.substring(1);\n-\tmiddlewares[type] = fn;\n+        if (type[0] === '.')\n+                type = type.substring(1);\n+        middlewares[type] = fn;\n };\n \n // Clears cache with commands\n exports.clear = function() {\n-\tCACHE = {};\n+        CACHE = {};\n };\n \n global.Image = exports;\n"}
{"cve":"CVE-2020-7613:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 7cdbddf..056d04c 100755\n--- a/index.js\n+++ b/index.js\n@@ -484,15 +484,12 @@ class NodeClam {\n             return false;\n         }\n \n-        const version_cmds = {\n-            clamdscan: `${path} --version`,\n-            clamscan: `${path} --version`,\n-        };\n+\n \n         try {\n             await fs_access(path, fs.constants.R_OK);\n \n-            const {stdout} = await cp_exec(version_cmds[scanner]);\n+            const {stdout} = await cp_execfile(path, ['--version']);\n             if (stdout.toString().match(/ClamAV/) === null) {\n                 if (this.settings.debug_mode) console.log(`${this.debug_label}: Could not verify the ${scanner} binary.`);\n                 return false;\n"}
{"cve":"CVE-2020-28437:0708", "fix_patch": "diff --git a/lib/get.js b/lib/get.js\nindex 25abee9..5f7e1d7 100644\n--- a/lib/get.js\n+++ b/lib/get.js\n@@ -1,6 +1,9 @@\n-var exec = require('child_process').exec\n-module.exports = function(app, cb) {\n-  exec('heroku config --app ' + app, function(err, stdout) {\n+var exec = require('child_process').execFile\n+\n+module.exports = function (app, cb) {\n+  var args = ['config', '--app', app]\n+\n+  exec('heroku', args, function (err, stdout) {\n     if(err) return cb(err);\n     var config = {}\n     var lines = stdout.split('\\n')\n"}
{"cve":"CVE-2017-16083:0708", "fix_patch": "diff --git a/lib/router.js b/lib/router.js\nindex be232b9..c1e0e38 100644\n--- a/lib/router.js\n+++ b/lib/router.js\n@@ -730,8 +730,11 @@\n     };\n     dispatch[\"static\"] = function(pathname, req, res) {\n       var full_path;\n-      full_path = \"\" + dispatch.static_route + (unescape(pathname));\n-      return fs.exists(full_path, function(exists) {\n+      var static_path = path_tools.resolve(dispatch.static_route);\n+      full_path = path_tools.join(static_path, pathname);\n+      if (full_path.indexOf(static_path) != 0) {\n+        return dispatch._403(null, res, pathname);\n+      }\n         var e, error;\n         if (exists) {\n           if (((pathname.indexOf(dispatch.cgi_dir + \"/\") !== -1) || (pathname.match(/\\.php$/))) && (pathname.substr(-1) !== \"/\") && (dispatch.serve_cgi === true)) {\n@@ -837,7 +840,11 @@\n       var body, child, d, data, e, env, error, full_path, isPHP, prepareChild, respbuffer, urlobj;\n       urlobj = urlparse(req.url);\n       respbuffer = '';\n-      full_path = \"\" + dispatch.static_route + (unescape(pathname));\n+      var static_path = path_tools.resolve(dispatch.static_route);\n+      full_path = path_tools.join(static_path, pathname);\n+      if (full_path.indexOf(static_path) != 0) {\n+        return dispatch._403(null, res, pathname);\n+      }\n       env = dispatch.getEnv(pathname, req, res);\n       isPHP = !!pathname.match(/\\.php$/);\n       prepareChild = function(req_body) {\n"}
{"cve":"CVE-2016-10548:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 74ea51b..16fbdd8 100755\n--- a/index.js\n+++ b/index.js\n@@ -71,6 +71,18 @@ function reduceCSSCalc(value, decimalPrecision) {\n     var toEvaluate = expression.replace(new RegExp(unit, \"gi\"), \"\")\n     var result\n \n+    var SANITIZE_REGEX = new RegExp('[^0-9a-z\\\\.\\\\s\\\\(\\\\)\\\\+\\\\-\\\\\\*\\\\/\\\\%]', 'gi');\n+    // Sanitization to prevent arbitrary code execution\n+    if (SANITIZE_REGEX.test(toEvaluate)) {\n+      return functionIdentifier + \"(\" + expression + \")\";\n+    }\n+    var SANITIZE_REGEX = new RegExp('[^0-9a-z\\.\\s\\(\\)\\+\\-\\*\\/\\\\%]', 'gi');\n+    // Sanitization to prevent arbitrary code execution\\n    if (SANITIZE_REGEX.test(toEvaluate)) {\\n      return functionIdentifier + \\\"(\\\" + expression + \\\")\\\";\\n    }\\n\\n    var SANITIZE_REGEX = new RegExp('[^0-9a-z\\\\.\\\\s\\\\(\\\\)\\\\+\\\\-\\\\\\*\\\\/\\\\%]', 'gi');\n+    // Sanitization to prevent arbitrary code execution\n+    if (SANITIZE_REGEX.test(toEvaluate)) {\n+      return functionIdentifier + \"(\" + expression + \")\";\n+    }\n+\n     try {\n       result = eval(toEvaluate)\n     }\n"}
{"cve":"CVE-2018-3733:0708", "fix_patch": "diff --git a/crud-file-server.js b/crud-file-server.js\nindex e598241..0f3be07 100644\n--- a/crud-file-server.js\n+++ b/crud-file-server.js\n@@ -2,270 +2,270 @@ var fs = require('fs');\n \n // don't let users crawl up the folder structure by using a/../../../c/d\n var cleanUrl = function(url) { \n-\turl = decodeURIComponent(url);\n-\twhile(url.indexOf('..').length > 0) { url = url.replace('..', ''); }\n-\treturn url;\n+        url = decodeURIComponent(url);\n+        while(url.indexOf('..') > -1) { url = url.replace('..', ''); }\n+        return url;\n };\n \n /*  \n example usage:\n-\trequire('http').createServer(function (req, res) {\n-\t\tserver.handleRequest(port, path, req, res, vpath);\n-\t}).listen(port);\n+        require('http').createServer(function (req, res) {\n+                server.handleRequest(port, path, req, res, vpath);\n+        }).listen(port);\n */\n-exports.handleRequest = function(vpath, path, req, res, readOnly, logHeadRequests) {\t\n-\t// vpath: (optional) virtual path to host in the url\n-\t// path: the file system path to serve\n-\t// readOnly: whether to allow modifications to the file\n+exports.handleRequest = function(vpath, path, req, res, readOnly, logHeadRequests) {    \n+        // vpath: (optional) virtual path to host in the url\n+        // path: the file system path to serve\n+        // readOnly: whether to allow modifications to the file\n \n-\t// our error handler\n-\tvar writeError = function (err, code) { \n-\t\tcode = code || 500;\n-\t\tconsole.log('Error ' + code + ': ' + err);\n-\t\t// write the error to the response, if possible\n-\t\ttry {\t\t\t\n-\t\t\tres.statusCode = code;\n-\t\t\tres.setHeader('Content-Type', 'application/json');\n-\t\t\tres.end(JSON.stringify(err));\t\n-\t\t} catch(resErr) {\n-\t\t\tconsole.log('failed to write error to response: ' + resErr);\n-\t\t}\n-\t};\n+        // our error handler\n+        var writeError = function (err, code) { \n+                code = code || 500;\n+                console.log('Error ' + code + ': ' + err);\n+                // write the error to the response, if possible\n+                try {                   \n+                        res.statusCode = code;\n+                        res.setHeader('Content-Type', 'application/json');\n+                        res.end(JSON.stringify(err));   \n+                } catch(resErr) {\n+                        console.log('failed to write error to response: ' + resErr);\n+                }\n+        };\n \n-\tif(path.lastIndexOf('/') !== path.length - 1) { path += '/'; } // make sure path ends with a slash\t\n-\tvar parsedUrl = require('url').parse(req.url);\t\n-\tvar query = query ? {} : require('querystring').parse(parsedUrl.query);\n+        if(path.lastIndexOf('/') !== path.length - 1) { path += '/'; } // make sure path ends with a slash      \n+        var parsedUrl = require('url').parse(req.url);  \n+        var query = query ? {} : require('querystring').parse(parsedUrl.query);\n     var url = cleanUrl(parsedUrl.pathname);\n-\t\n-\t// normalize the url such that there is no trailing or leading slash /\n-\tif(url.lastIndexOf('/') === url.length - 1) { url = url.slice(0, url.length ); }\n-\tif(url[0] === '/') { url = url.slice(1, url.length);  }\n+        \n+        // normalize the url such that there is no trailing or leading slash /\n+        if(url.lastIndexOf('/') === url.length - 1) { url = url.slice(0, url.length ); }\n+        if(url[0] === '/') { url = url.slice(1, url.length);  }\n \n-\t// check that url begins with vpath\n-\tif(vpath && url.indexOf(vpath) != 0) {\n-\t\tconsole.log('url does not begin with vpath');\n-\t\tthrow 'url [' + url + '] does not begin with vpath [' + vpath + ']';\n-\t}\n+        // check that url begins with vpath\n+        if(vpath && url.indexOf(vpath) != 0) {\n+                console.log('url does not begin with vpath');\n+                throw 'url [' + url + '] does not begin with vpath [' + vpath + ']';\n+        }\n \n-\tif(req.method != 'HEAD') {\n-\t\tconsole.log(req.method + ' ' + req.url);\n-\t}\n-\tvar relativePath = vpath && url.indexOf(vpath) == 0 ?\n-\t\tpath + url.slice(vpath.length + 1, url.length):\n-\t\tpath + url;\t\n-\t\n-\ttry {\n-\t\tif(readOnly && req.method != 'GET') {\n-\t\t\twriteError(req.method + ' forbidden on this resource', 403);\n-\t\t} else {\n-\t\t\tswitch(req.method) {\n-\t\t\t\tcase 'HEAD':\n-\t\t\t\t\tif(logHeadRequests) {\n-\t\t\t\t\t\tconsole.log('head: ' + relativePath);\t\t\t\t\n-\t\t\t\t\t}\n-\t\t\t\t\tfs.stat(relativePath, function(err, stats) { // determine if the resource is a file or directory\n-\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\telse {\t\t\t\t\t\n-\t\t\t\t\t\t\tres.setHeader('Last-Modified', stats.mtime);\t\t\n-\t\t\t\t\t\t\tres.setHeader(\"Expires\", \"Sat, 01 Jan 2000 00:00:00 GMT\");\n-\t\t\t\t\t\t\tres.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate, max-age=0\");\n-\t\t\t\t\t\t\tres.setHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n-\t\t\t\t\t\t\tres.setHeader(\"Pragma\", \"no-cache\");\n-\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\tif(stats.isDirectory()) {\t\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\t\tres.setHeader('Content-Type', query.type == 'json' || query.dir == 'json' ? 'application/json' : 'text/html');\n-\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\tif(query.type == 'json' || query.dir == 'json') {\n-\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', 'application/json');\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\tvar type = require('mime').lookup(relativePath);\n-\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', type);\n-\t\t\t\t\t\t\t\t\tres.setHeader('Content-Length', stats.size);\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tres.end();\t\t\t\t\t\t\t\n-\t\t\t\t\t\t}\n-\t\t\t\t\t});\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase 'GET': // returns file or directory contents\n-\t\t\t\t\tconsole.log('relativePath: ' + relativePath);\n-\t\t\t\t\tif(url === 'favicon.ico') { \t\n-\t\t\t\t\t\tres.end(); // if the browser requests favicon, just return an empty response\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tfs.stat(relativePath, function(err, stats) { // determine if the resource is a file or directory\n-\t\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\tif(stats.isDirectory()) {\n-\t\t\t\t\t\t\t\t\tres.setHeader('Last-Modified', stats.mtime);\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\t\t\tres.setHeader(\"Expires\", \"Sat, 01 Jan 2000 00:00:00 GMT\");\n-\t\t\t\t\t\t\t\t\tres.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate, max-age=0\");\n-\t\t\t\t\t\t\t\t\tres.setHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n-\t\t\t\t\t\t\t\t\tres.setHeader(\"Pragma\", \"no-cache\");\n-\t\t\t\t\t\t\t\t\t// if it's a directory, return the files as a JSONified array\n-\t\t\t\t\t\t\t\t\tconsole.log('reading directory ' + relativePath);\n-\t\t\t\t\t\t\t\t\tfs.readdir(relativePath, function(err, files) {\n-\t\t\t\t\t\t\t\t\t\tif(err) { \n-\t\t\t\t\t\t\t\t\t\t\tconsole.log('writeError');\n-\t\t\t\t\t\t\t\t\t\t\twriteError(err); \n-\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\t\t\tvar results = [];\n-\t\t\t\t\t\t\t\t\t\t\tvar search = {};\n-\t\t\t\t\t\t\t\t\t\t\tsearch.stats = function(files) {\n-\t\t\t\t\t\t\t\t\t\t\t\tif(files.length) { \n-\t\t\t\t\t\t\t\t\t\t\t\t\tvar file = files.shift();\n-\t\t\t\t\t\t\t\t\t\t\t\t\tfs.stat(relativePath + '/' + file, function(err, stats) { \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.name = file;\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isFile = stats.isFile();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isDirectory = stats.isDirectory();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isBlockDevice = stats.isBlockDevice();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isFIFO = stats.isFIFO();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isSocket = stats.isSocket();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tresults.push(stats);\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsearch.stats(files);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t\t\t\t\tif(query.type == 'json' || query.dir == 'json') {\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', 'application/json');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.write(JSON.stringify(results)); \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.end();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t} else { \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', 'text/html');\t\t\t\t\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.write('<html><body>');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor(var f = 0; f < results.length; f++) {\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvar name = results[f].name;\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvar normalized = url + '/' + name;\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\twhile(normalized[0] == '/') { normalized = normalized.slice(1, normalized.length); }\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif(normalized.indexOf('\"') >= 0) throw new Error('unsupported file name')\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname = name.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.write('\\r\\n<p><a href=\"/' + normalized + '\"><span>' + name + '</span></a></p>');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.end('\\r\\n</body></html>');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t\t};\n-\t\t\t\t\t\t\t\t\t\t\tsearch.stats(files);\n-\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t// if it's a file, return the contents of a file with the correct content type\n-\t\t\t\t\t\t\t\t\tconsole.log('reading file ' + relativePath);\n-\t\t\t\t\t\t\t\t\tif(query.type == 'json' || query.dir == 'json') {\n-\t\t\t\t\t\t\t\t\t\tvar type = 'application/json';\n-\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', type);\n-\t\t\t\t\t\t\t\t\t\tfs.readFile(relativePath, function(err, data) { \n-\t\t\t\t\t\t\t\t\t\t\tif(err) { writeError(err); }\n-\t\t\t\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\t\t\t\tres.end(JSON.stringify({ \n-\t\t\t\t\t\t\t\t\t\t\t\t\tdata: data.toString(),\n-\t\t\t\t\t\t\t\t\t\t\t\t\ttype: require('mime').lookup(relativePath),\n-\t\t\t\t\t\t\t\t\t\t\t\t})); \n-\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t\tvar type = require('mime').lookup(relativePath);\n-\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', type);\n-\t\t\t\t\t\t\t\t\t\tfs.readFile(relativePath, function(err, data) { \n-\t\t\t\t\t\t\t\t\t\t\tif(err) { writeError(err); }\n-\t\t\t\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Length', data.length);\n-\t\t\t\t\t\t\t\t\t\t\t\tres.end(data); \n-\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t});\n-\t\t\t\t\t}\n-\t\t\t\t\treturn;\n-\t\t\t\tcase 'PUT': // write a file\n-\t\t\t\t\tconsole.log('writing ' + relativePath);\n-\t\t\t\t\tvar stream = fs.createWriteStream(relativePath);\t\t\n-\t\t\t\t\tstream.ok = true;\n-\t\t\t\t\treq.pipe(stream); // TODO: limit data length\n-\t\t\t\t\tstream.on('close', function() { \n-\t\t\t\t\t\tif(stream.ok) {\n-\t\t\t\t\t\t\tres.end();\n-\t\t\t\t\t\t}\n-\t\t\t\t\t});\n-\t\t\t\t\tstream.on('error', function(err) { \t\t\t\t\t\t\t\t\t\t\n-\t\t\t\t\t\tstream.ok = false;\n-\t\t\t\t\t\twriteError(err);\n-\t\t\t\t\t});\n-\t\t\t\t\treturn;\n-\t\t\t\tcase 'POST': // create a directory or rename a file or directory\n-\t\t\t\t\tif(query.rename) { // rename a file or directory\n-\t\t\t\t\t\tconsole.log('rename: ' + relativePath);\n-\t\t\t\t\t\t// e.g., http://localhost/old-name.html?rename=new-name.html\n-\t\t\t\t\t\tquery.rename = cleanUrl(query.rename);\n-\t\t\t\t\t\t// TODO: handle missing vpath here\n-\t\t\t\t\t\tif(vpath) { \n-\t\t\t\t\t\t\tif(query.rename.indexOf('/' + vpath + '/') == 0) { \n-\t\t\t\t\t\t\t\tquery.rename = query.rename.slice(vpath.length + 2, query.rename.length);\n-\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\tthrow 'renamed url [' + query.rename + '] does not begin with vpath [' + vpath + ']';\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t} \n-\t\t\t\t\t\tconsole.log('renaming ' + relativePath + ' to ' + path + query.rename);\n-\t\t\t\t\t\tfs.rename(relativePath, path + query.rename, function(err) {\n-\t\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\tres.end();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t});\n-\t\t\t\t\t} else if(query.create == 'directory') { // rename a directory\n-\t\t\t\t\t\t// e.g., http://localhost/new-directory?create=directory\n-\t\t\t\t\t\tconsole.log('creating directory ' + relativePath);\n-\t\t\t\t\t\tfs.mkdir(relativePath, 0777, function(err) { \n-\t\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\tres.end();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t});\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tconsole.log('relativePath: ' + relativePath);\n-\t\t\t\t\t\twriteError('valid queries are ' + url + '?rename=[new name] or ' + url + '?create=directory');\n-\t\t\t\t\t}\n-\t\t\t\t\treturn;\n-\t\t\t\tcase 'DELETE': // delete a file or directory\t\t\t\t\n-\t\t\t\t\tfs.stat(relativePath, function(err, stats) { \n-\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\tif(stats.isDirectory()) { // delete a directory\n-\t\t\t\t\t\t\t\tconsole.log('deleting directory ' + relativePath);\n-\t\t\t\t\t\t\t\tfs.rmdir(relativePath, function(err) {\n-\t\t\t\t\t\t\t\t\tif(err) { writeError(err); }\n-\t\t\t\t\t\t\t\t\telse { \n-\t\t\t\t\t\t\t\t\t\tres.end(); \n-\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t} else { // delete a file\n-\t\t\t\t\t\t\t\tconsole.log('deleting file ' + relativePath);\n-\t\t\t\t\t\t\t\tfs.unlink(relativePath, function(err) {\n-\t\t\t\t\t\t\t\t\tif(err) { writeError(err); }\n-\t\t\t\t\t\t\t\t\telse { \n-\t\t\t\t\t\t\t\t\t\tres.end(); \n-\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n-\t\t\t\t\t});\t\t\t\n-\t\t\t\t\treturn;\n-\t\t\t\tdefault: // unsupported method! tell the client ...\n-\t\t\t\t\tconsole.log('unsupported: ' + relativePath);\t\t\t\t\n-\t\t\t\t\twriteError('Method ' + method + ' not allowed', 405);\n-\t\t\t\t\treturn;\n-\t\t\t}\n-\t\t}\n-\t} catch(err) { \n-\t\t// file system ('fs') errors are just bubbled up to this error handler\n-\t\t// for example, if the GET is called on a non-existent file, an error will be thrown\n-\t\t// and caught here\n-\t\t// writeError will write the error information to the response\n-\t\twriteError('unhandled error: ' + err);\n-\t}\n+        if(req.method != 'HEAD') {\n+                console.log(req.method + ' ' + req.url);\n+        }\n+        var relativePath = vpath && url.indexOf(vpath) == 0 ?\n+                path + url.slice(vpath.length + 1, url.length):\n+                path + url;     \n+        \n+        try {\n+                if(readOnly && req.method != 'GET') {\n+                        writeError(req.method + ' forbidden on this resource', 403);\n+                } else {\n+                        switch(req.method) {\n+                                case 'HEAD':\n+                                        if(logHeadRequests) {\n+                                                console.log('head: ' + relativePath);                           \n+                                        }\n+                                        fs.stat(relativePath, function(err, stats) { // determine if the resource is a file or directory\n+                                                if(err) { writeError(err); } \n+                                                else {                                  \n+                                                        res.setHeader('Last-Modified', stats.mtime);            \n+                                                        res.setHeader(\"Expires\", \"Sat, 01 Jan 2000 00:00:00 GMT\");\n+                                                        res.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate, max-age=0\");\n+                                                        res.setHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n+                                                        res.setHeader(\"Pragma\", \"no-cache\");\n+                                                        \n+                                                        if(stats.isDirectory()) {                                                               \n+                                                                res.setHeader('Content-Type', query.type == 'json' || query.dir == 'json' ? 'application/json' : 'text/html');\n+                                                        } else {\n+                                                                if(query.type == 'json' || query.dir == 'json') {\n+                                                                        res.setHeader('Content-Type', 'application/json');\n+                                                                }\n+                                                                else {\n+                                                                        var type = require('mime').lookup(relativePath);\n+                                                                        res.setHeader('Content-Type', type);\n+                                                                        res.setHeader('Content-Length', stats.size);\n+                                                                }\n+                                                        }\n+                                                        res.end();                                                      \n+                                                }\n+                                        });\n+                                        break;\n+                                case 'GET': // returns file or directory contents\n+                                        console.log('relativePath: ' + relativePath);\n+                                        if(url === 'favicon.ico') {     \n+                                                res.end(); // if the browser requests favicon, just return an empty response\n+                                        } else {\n+                                                fs.stat(relativePath, function(err, stats) { // determine if the resource is a file or directory\n+                                                        if(err) { writeError(err); } \n+                                                        else {\n+                                                                if(stats.isDirectory()) {\n+                                                                        res.setHeader('Last-Modified', stats.mtime);                                                    \n+                                                                        res.setHeader(\"Expires\", \"Sat, 01 Jan 2000 00:00:00 GMT\");\n+                                                                        res.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate, max-age=0\");\n+                                                                        res.setHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n+                                                                        res.setHeader(\"Pragma\", \"no-cache\");\n+                                                                        // if it's a directory, return the files as a JSONified array\n+                                                                        console.log('reading directory ' + relativePath);\n+                                                                        fs.readdir(relativePath, function(err, files) {\n+                                                                                if(err) { \n+                                                                                        console.log('writeError');\n+                                                                                        writeError(err); \n+                                                                                }\n+                                                                                else {\n+                                                                                        var results = [];\n+                                                                                        var search = {};\n+                                                                                        search.stats = function(files) {\n+                                                                                                if(files.length) { \n+                                                                                                        var file = files.shift();\n+                                                                                                        fs.stat(relativePath + '/' + file, function(err, stats) { \n+                                                                                                                if(err) { writeError(err); } \n+                                                                                                                else {\n+                                                                                                                        stats.name = file;\n+                                                                                                                        stats.isFile = stats.isFile();\n+                                                                                                                        stats.isDirectory = stats.isDirectory();\n+                                                                                                                        stats.isBlockDevice = stats.isBlockDevice();\n+                                                                                                                        stats.isFIFO = stats.isFIFO();\n+                                                                                                                        stats.isSocket = stats.isSocket();\n+                                                                                                                        results.push(stats);\n+                                                                                                                        search.stats(files);                                                                                                                    \n+                                                                                                                }\n+                                                                                                        });\n+                                                                                                } else {\n+                                                                                                        if(query.type == 'json' || query.dir == 'json') {\n+                                                                                                                res.setHeader('Content-Type', 'application/json');\n+                                                                                                                res.write(JSON.stringify(results)); \n+                                                                                                                res.end();\n+                                                                                                        } else { \n+                                                                                                                res.setHeader('Content-Type', 'text/html');                                                                                     \n+                                                                                                                res.write('<html><body>');\n+                                                                                                                for(var f = 0; f < results.length; f++) {\n+                                                                                                                        var name = results[f].name;\n+                                                                                                                        var normalized = url + '/' + name;\n+                                                                                                                        while(normalized[0] == '/') { normalized = normalized.slice(1, normalized.length); }\n+                                                                                                                        if(normalized.indexOf('\"') >= 0) throw new Error('unsupported file name')\n+                                                                                                                        name = name.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');\n+                                                                                                                        res.write('\\r\\n<p><a href=\"/' + normalized + '\"><span>' + name + '</span></a></p>');\n+                                                                                                                }\n+                                                                                                                res.end('\\r\\n</body></html>');\n+                                                                                                        }\n+                                                                                                }\n+                                                                                        };\n+                                                                                        search.stats(files);\n+                                                                                }\n+                                                                        });\n+                                                                } else {\n+                                                                        // if it's a file, return the contents of a file with the correct content type\n+                                                                        console.log('reading file ' + relativePath);\n+                                                                        if(query.type == 'json' || query.dir == 'json') {\n+                                                                                var type = 'application/json';\n+                                                                                res.setHeader('Content-Type', type);\n+                                                                                fs.readFile(relativePath, function(err, data) { \n+                                                                                        if(err) { writeError(err); }\n+                                                                                        else {\n+                                                                                                res.end(JSON.stringify({ \n+                                                                                                        data: data.toString(),\n+                                                                                                        type: require('mime').lookup(relativePath),\n+                                                                                                })); \n+                                                                                        }\n+                                                                                });\n+                                                                        } else {\n+                                                                                var type = require('mime').lookup(relativePath);\n+                                                                                res.setHeader('Content-Type', type);\n+                                                                                fs.readFile(relativePath, function(err, data) { \n+                                                                                        if(err) { writeError(err); }\n+                                                                                        else {\n+                                                                                                res.setHeader('Content-Length', data.length);\n+                                                                                                res.end(data); \n+                                                                                        }\n+                                                                                });\n+                                                                        }\n+                                                                }\n+                                                        }\n+                                                });\n+                                        }\n+                                        return;\n+                                case 'PUT': // write a file\n+                                        console.log('writing ' + relativePath);\n+                                        var stream = fs.createWriteStream(relativePath);                \n+                                        stream.ok = true;\n+                                        req.pipe(stream); // TODO: limit data length\n+                                        stream.on('close', function() { \n+                                                if(stream.ok) {\n+                                                        res.end();\n+                                                }\n+                                        });\n+                                        stream.on('error', function(err) {                                                                              \n+                                                stream.ok = false;\n+                                                writeError(err);\n+                                        });\n+                                        return;\n+                                case 'POST': // create a directory or rename a file or directory\n+                                        if(query.rename) { // rename a file or directory\n+                                                console.log('rename: ' + relativePath);\n+                                                // e.g., http://localhost/old-name.html?rename=new-name.html\n+                                                query.rename = cleanUrl(query.rename);\n+                                                // TODO: handle missing vpath here\n+                                                if(vpath) { \n+                                                        if(query.rename.indexOf('/' + vpath + '/') == 0) { \n+                                                                query.rename = query.rename.slice(vpath.length + 2, query.rename.length);\n+                                                        } else {\n+                                                                throw 'renamed url [' + query.rename + '] does not begin with vpath [' + vpath + ']';\n+                                                        }\n+                                                } \n+                                                console.log('renaming ' + relativePath + ' to ' + path + query.rename);\n+                                                fs.rename(relativePath, path + query.rename, function(err) {\n+                                                        if(err) { writeError(err); } \n+                                                        else {\n+                                                                res.end();\n+                                                        }\n+                                                });\n+                                        } else if(query.create == 'directory') { // rename a directory\n+                                                // e.g., http://localhost/new-directory?create=directory\n+                                                console.log('creating directory ' + relativePath);\n+                                                fs.mkdir(relativePath, 0777, function(err) { \n+                                                        if(err) { writeError(err); } \n+                                                        else {\n+                                                                res.end();\n+                                                        }\n+                                                });\n+                                        } else {\n+                                                console.log('relativePath: ' + relativePath);\n+                                                writeError('valid queries are ' + url + '?rename=[new name] or ' + url + '?create=directory');\n+                                        }\n+                                        return;\n+                                case 'DELETE': // delete a file or directory                            \n+                                        fs.stat(relativePath, function(err, stats) { \n+                                                if(err) { writeError(err); } \n+                                                else {\n+                                                        if(stats.isDirectory()) { // delete a directory\n+                                                                console.log('deleting directory ' + relativePath);\n+                                                                fs.rmdir(relativePath, function(err) {\n+                                                                        if(err) { writeError(err); }\n+                                                                        else { \n+                                                                                res.end(); \n+                                                                        }\n+                                                                });\n+                                                        } else { // delete a file\n+                                                                console.log('deleting file ' + relativePath);\n+                                                                fs.unlink(relativePath, function(err) {\n+                                                                        if(err) { writeError(err); }\n+                                                                        else { \n+                                                                                res.end(); \n+                                                                        }\n+                                                                });\n+                                                        }\n+                                                }\n+                                        });                     \n+                                        return;\n+                                default: // unsupported method! tell the client ...\n+                                        console.log('unsupported: ' + relativePath);                            \n+                                        writeError('Method ' + method + ' not allowed', 405);\n+                                        return;\n+                        }\n+                }\n+        } catch(err) { \n+                // file system ('fs') errors are just bubbled up to this error handler\n+                // for example, if the GET is called on a non-existent file, an error will be thrown\n+                // and caught here\n+                // writeError will write the error information to the response\n+                writeError('unhandled error: ' + err);\n+        }\n };\n"}
{"cve":"CVE-2020-7781:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 614a6d6..b77708b 100644\n--- a/index.js\n+++ b/index.js\n@@ -10,9 +10,7 @@ let SOCKET_TIMEOUT = 1000;   //Setting 1s as max acceptable timeout\n function testSync(host, port, connectTimeout) {\n     const nodeBinary = process.execPath;\n     const scriptPath = path.join(__dirname, \"./scripts/connection-tester\");\n-    const cmd = util.format('\"%s\" \"%s\" %s %s %s', nodeBinary, scriptPath, host, port, connectTimeout);\n-\n-    const shellOut = shell.execSync(cmd).toString();\n+    const shellOut = shell.execFileSync(nodeBinary, [scriptPath, host, port, connectTimeout]).toString();\n \n     const output = {\n         success: false,\n"}
{"cve":"CVE-2020-15084:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex f5da77f..de526e3 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -19,6 +19,11 @@ function wrapStaticSecretInCallback(secret){\n module.exports = function(options) {\n   if (!options || !options.secret) throw new Error('secret should be set');\n \n+\n+  if (!options.algorithms) {\n+    throw new Error('algorithms should be set');\n+  }\n+\n   var secretCallback = options.secret;\n \n   if (!isFunction(secretCallback)){\n"}
{"cve":"CVE-2022-2421:0708", "fix_patch": "diff --git a/binary.js b/binary.js\nindex 3e2347d..e6ab380 100644\n--- a/binary.js\n+++ b/binary.js\n@@ -71,6 +71,9 @@ function _reconstructPacket(data, buffers) {\n   if (!data) return data;\n \n   if (data && data._placeholder) {\n+    if (typeof data.num !== 'number' || data.num !== data.num || data.num < 0 || data.num >= buffers.length) {\n+      throw new Error(\"illegal attachments\");\n+    }\n     return buffers[data.num]; // appropriate buffer (should be natural order anyway)\n   } else if (isArray(data)) {\n     for (var i = 0; i < data.length; i++) {\n"}
{"cve":"CVE-2022-29822:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex ad1d760..1e5db05 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -133,7 +133,12 @@ class Service extends AdapterService {\n     }, params.sequelize);\n \n     if (filters.$select) {\n-      q.attributes = filters.$select;\n+      const { attributes } = this.Model;\n+      q.attributes = filters.$select.filter(item =>\n+        Object.keys(attributes).some(\n+          key => attributes[key].field === item || attributes[key].fieldName === item\n+        )\n+      );\n     }\n \n     const Model = this.applyScope(params);\n"}
{"cve":"CVE-2023-28155:0708", "fix_patch": "diff --git a/lib/redirect.js b/lib/redirect.js\nindex 500252c..0ffb112 100644\n--- a/lib/redirect.js\n+++ b/lib/redirect.js\n@@ -14,6 +14,7 @@ function Redirect (request) {\n   this.redirects = []\n   this.redirectsFollowed = 0\n   this.removeRefererHeader = false\n+  this.allowInsecureRedirect = false\n }\n \n Redirect.prototype.onRequest = function (options) {\n@@ -40,6 +41,9 @@ Redirect.prototype.onRequest = function (options) {\n   if (options.followOriginalHttpMethod !== undefined) {\n     self.followOriginalHttpMethod = options.followOriginalHttpMethod\n   }\n+  if (options.allowInsecureRedirect !== undefined) {\n+    self.allowInsecureRedirect = options.allowInsecureRedirect\n+  }\n }\n \n Redirect.prototype.redirectTo = function (response) {\n@@ -112,6 +116,12 @@ Redirect.prototype.onResponse = function (response, callback) {\n     var uriPrev = request.uri\n     request.uri = url.parse(redirectTo)\n \n+    if (request.uri.protocol !== uriPrev.protocol && !self.allowInsecureRedirect) {\n+      var insecureError = new Error('Redirect from \"' + uriPrev.protocol + '\" to \"' + request.uri.protocol + '\" protocol is not allowed.')\n+      insecureError.code = 'ERR_INVALID_PROTOCOL'\n+      return callback(insecureError)\n+    }\n+\n     // handle the case where we change protocol from https to http or vice versa\n     if (request.uri.protocol !== uriPrev.protocol) {\n       delete request.agent\n"}
{"cve":"CVE-2022-2900:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex b806431..b990529 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -35,14 +35,20 @@ import normalizeUrl from \"normalize-url\";\n  */\n const parseUrl = (url, normalize = false) => {\n \n+    const GIT_RE = /^(?:(git|ssh|https?):\\/\\/|git@)([^\\/:]+)[\\/:]([^\\/:]+)\\/([^\\/:]+)$/;\n+    \n     // Constants\n-    const GIT_RE = /(^(git@|http(s)?:\\/\\/)([\\w\\.\\-@]+)(\\/|:))(([\\~,\\.\\w,\\-,\\_,\\/]+)(.git){0,1}((\\/){0,1}))/\n-\n+    const MAX_INPUT_LENGTH = 2048\n+    \n     if (typeof url !== \"string\" || !url.trim()) {\n         const err = new Error(\"Invalid url.\")\n         err.subject_url = url\n         throw err\n     }\n+    \n+    if (url.length > MAX_INPUT_LENGTH) {\n+        throw new Error(\"Input exceeds maximum length of \" + MAX_INPUT_LENGTH + \" characters\")\n+    }\n \n     if (normalize) {\n         if (typeof normalize !== \"object\") {\n@@ -50,23 +56,28 @@ const parseUrl = (url, normalize = false) => {\n                 stripHash: false\n             }\n         }\n-        url = normalizeUrl(url, normalize)\n     }\n \n     const parsed = parsePath(url)\n-\n-    // Potential git-ssh urls\n+    \n     if (parsed.protocol === \"file\") {\n-        const matched  = parsed.href.match(GIT_RE)\n+        const matched = url.match(GIT_RE)\n+        \n         if (matched) {\n             parsed.protocols = [\"ssh\"]\n             parsed.protocol = \"ssh\"\n-            parsed.resource = matched[4]\n-            parsed.host = matched[4]\n+            parsed.resource = matched[2]\n+            parsed.host = matched[2]\n             parsed.user = \"git\"\n-            parsed.pathname = `/${matched[6]}`\n+            parsed.pathname = `/${matched[3]}/${matched[4]}`\n+            parsed.href = `git@${parsed.host}:${parsed.pathname.substring(1)}`\n+    parsed.parse_failed = false\n         }\n     }\n+    \n+    if (parsed.parse_failed) {\n+        throw new Error(\"Url parsing failed.\")\n+    }\n \n     return parsed;\n }\n"}
{"cve":"CVE-2018-3778:0708", "fix_patch": "diff --git a/lib/client.js b/lib/client.js\nindex 644589d..57b898d 100644\n--- a/lib/client.js\n+++ b/lib/client.js\n@@ -242,13 +242,18 @@ Client.prototype.close = function (done) {\n \n   function finish () {\n     if (!that.disconnected && that.will) {\n-      that.broker.publish(that.will, that, function (err) {\n-        if (!err) {\n-          that.broker.persistence.delWill({\n-            id: that.id,\n-            brokerId: that.broker.id\n-          }, nop)\n+      that.broker.authorizePublish(that, that.will, function (err) {\n+        if (err) {\n+          return\n         }\n+        that.broker.publish(that.will, that, function (err) {\n+          if (!err) {\n+            that.broker.persistence.delWill({\n+              id: that.id,\n+              brokerId: that.broker.id\n+            }, nop)\n+          }\n+        })\n       })\n       that.will = null // this function might be called twice\n     }\n"}
{"cve":"CVE-2022-0686:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex d808b13..623e3a0 100644\n--- a/index.js\n+++ b/index.js\n@@ -4,6 +4,7 @@ var required = require('requires-port')\n   , qs = require('querystringify')\n   , CRHTLF = /[\\n\\r\\t]/g\n   , slashes = /^[A-Za-z][A-Za-z0-9+-.]*:\\/\\//\n+  , portre = /:(\\d*)?$/\n   , protocolre = /^([a-z][a-z0-9.+-]*:)?(\\/\\/)?([\\\\/]+)?([\\S\\s]*)/i\n   , windowsDriveLetter = /^[a-zA-Z]:/\n   , whitespace = /^[ \\f\\n\\r\\t\\v\\u00a0\\u1680\\u2000-\\u200a\\u2028\\u2029\\u202f\\u205f\\u3000\\ufeff]+/;\n@@ -39,7 +40,7 @@ var rules = [\n   ['/', 'pathname'],                    // Extract from the back.\n   ['@', 'auth', 1],                     // Extract from the front.\n   [NaN, 'host', undefined, 1, 1],       // Set left over value.\n-  [/:(\\d+)$/, 'port', undefined, 1],    // RegExp the back.\n+  [portre, 'port', undefined, 1],    // RegExp the back.\n   [NaN, 'hostname', undefined, 1, 1]    // Set left over.\n ];\n \n@@ -367,7 +368,7 @@ function Url(address, location, parser) {\n   // for a given protocol. As the host also contains the port number we're going\n   // override it with the hostname which contains no port number.\n   //\n-  if (!required(url.port, url.protocol)) {\n+  if (!required(url.port, url.protocol) && url.port !== '') {\n     url.host = url.hostname;\n     url.port = '';\n   }\n@@ -450,7 +451,7 @@ function set(part, value, fn) {\n     case 'host':\n       url[part] = value;\n \n-      if (/:\\d+$/.test(value)) {\n+      if (portre.test(value)) {\n         value = value.split(':');\n         url.port = value.pop();\n         url.hostname = value.join(':');\n"}
{"cve":"CVE-2024-30260:0708", "fix_patch": "diff --git a/lib/handler/redirect-handler.js b/lib/handler/redirect-handler.js\nindex 368ef520..7a3db0c7 100644\n--- a/lib/handler/redirect-handler.js\n+++ b/lib/handler/redirect-handler.js\n@@ -201,9 +201,9 @@ function shouldRemoveHeader (header, removeContent, unknownOrigin) {\n   if (removeContent && util.headerNameToString(header).startsWith('content-')) {\n     return true\n   }\n-  if (unknownOrigin && (header.length === 13 || header.length === 6)) {\n+  if (unknownOrigin && (header.length === 13 || header.length === 19 || header.length === 6)) {\n     const name = util.headerNameToString(header)\n-    return name === 'authorization' || name === 'cookie'\n+    return name === 'authorization' || name === 'proxy-authorization' || name === 'cookie'\n   }\n   return false\n }\n"}
{"cve":"CVE-2025-23221:0708", "fix_patch": "diff --git a/src/runtime/url.ts b/src/runtime/url.ts\nindex dd95b54..210967c 100644\n--- a/src/runtime/url.ts\n+++ b/src/runtime/url.ts\n@@ -8,6 +8,7 @@ export class UrlError extends Error {\n   }\n }\n \n+\n /**\n  * Validates a URL to prevent SSRF attacks.\n  */\ndiff --git a/src/webfinger/lookup.ts b/src/webfinger/lookup.ts\nindex 45952a3..27e1b61 100644\n--- a/src/webfinger/lookup.ts\n+++ b/src/webfinger/lookup.ts\n@@ -1,10 +1,49 @@\n import { getLogger } from \"@logtape/logtape\";\n import type { ResourceDescriptor } from \"./jrd.ts\";\n+class UrlError extends Error {\n+  constructor(message: string) {\n+    super(message);\n+    this.name = \"UrlError\";\n+  }\n+}\n+\n+\n+\n+\n \n const logger = getLogger([\"fedify\", \"webfinger\", \"lookup\"]);\n \n const MAX_REDIRECTION = 5; // TODO: Make this configurable.\n \n+let ips: string[] | undefined;\n+\n+function isInternal(url: URL): boolean {\n+  if (\n+    !url.hostname.match(\n+      /^(localhost|([0-9]{1,3}\\.){3}[0-9]{1,3}|(\\[([0-9a-fA-F]{0,4}:){1,7}[0-9a-fA-F]{0,4}\\]))$/,\n+    )\n+  ) {\n+    return false;\n+  }\n+  if (ips === undefined) {\n+    try {\n+      ips = Deno.networkInterfaces().flatMap((i) =>\n+        i.address === \"127.0.0.1\" || i.address === \"::1\" ? [] : [i.address]\n+      );\n+    } catch (e) {\n+      if (e instanceof Deno.errors.PermissionDenied) {\n+        // If the `net` permission is not granted, we can't get the list of\n+        // network interfaces. In this case, we can't determine if the URL is\n+        // internal or not, so we assume it is not.\n+        return false;\n+      }\n+      throw e;\n+    }\n+  }\n+  return ips.includes(url.hostname);\n+}\n+\n+\n /**\n  * Looks up a WebFinger resource.\n  * @param resource The resource URL to look up.\n@@ -67,8 +106,7 @@ export async function lookupWebFinger(\n       if (redirectedUrl.protocol !== url.protocol) {\n         logger.error(\n           \"Redirected to a different protocol ({protocol} to \" +\n-            \"{redirectedProtocol}) while fetching WebFinger resource \" +\n-            \"descriptor.\",\n+            '\"{redirectedProtocol}) while fetching WebFinger resource descriptor.',\n           {\n             protocol: url.protocol,\n             redirectedProtocol: redirectedUrl.protocol,\n@@ -76,6 +114,11 @@ export async function lookupWebFinger(\n         );\n         return null;\n       }\n+      if (isInternal(redirectedUrl)) {\n+        throw new UrlError(\n+          `Redirected to a private address: ${redirectedUrl}`,\n+        );\n+      }\n       url = redirectedUrl;\n       continue;\n     }\n"}
{"cve":"CVE-2022-0512:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 702308b..0ee959c 100644\n--- a/index.js\n+++ b/index.js\n@@ -369,13 +369,16 @@ function Url(address, location, parser) {\n   //\n   // Parse down the `auth` for the username and password.\n   //\n-  url.username = url.password = '';\n+    url.username = url.password = '';\n   if (url.auth) {\n     instruction = url.auth.split(':');\n     url.username = instruction[0];\n     url.password = instruction[1] || '';\n   }\n \n+  if (url.username) url.username = encodeURIComponent(url.username);\n+  if (url.password) url.password = encodeURIComponent(url.password);\n+\n   url.origin = url.protocol !== 'file:' && isSpecial(url.protocol) && url.host\n     ? url.protocol +'//'+ url.host\n     : 'null';\n@@ -461,7 +464,7 @@ function set(part, value, fn) {\n \n     case 'username':\n     case 'password':\n-      url[part] = encodeURIComponent(value);\n+      url[part] = value;\n       break;\n \n     case 'auth':\n@@ -476,6 +479,9 @@ function set(part, value, fn) {\n     if (ins[4]) url[ins[1]] = url[ins[1]].toLowerCase();\n   }\n \n+  if (url.username) url.username = encodeURIComponent(url.username);\n+  if (url.password) url.password = encodeURIComponent(url.password);\n+\n   url.auth = url.password ? url.username +':'+ url.password : url.username;\n \n   url.origin = url.protocol !== 'file:' && isSpecial(url.protocol) && url.host\n"}
{"cve":"CVE-2022-0639:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 517b6b6..18b474c 100644\n--- a/index.js\n+++ b/index.js\n@@ -532,14 +532,13 @@ function toString(stringify) {\n     protocol +\n     ((url.protocol && url.slashes) || isSpecial(url.protocol) ? '//' : '');\n \n-  if (url.username) {\n-    result += url.username;\n-    if (url.password) result += ':'+ url.password;\n-    result += '@';\n-  } else if (url.password) {\n-    result += ':'+ url.password;\n-    result += '@';\n-  }\n+    if (Object.prototype.hasOwnProperty.call(url, 'username') || Object.prototype.hasOwnProperty.call(url, 'password')) {\n+        if (Object.prototype.hasOwnProperty.call(url, 'username'))\n+            result += url.username;\n+        if (Object.prototype.hasOwnProperty.call(url, 'password'))\n+            result += ':' + url.password;\n+        result += '@';\n+    }\n \n   result += url.host + url.pathname;\n \n"}
{"cve":"CVE-2021-23387:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex e7ea1ab..58b51a8 100644\n--- a/index.js\n+++ b/index.js\n@@ -16,6 +16,7 @@ function createTrailing (_options, _next) {\n     var req = args[0]\n     var res = args[1]\n     var url = u.parse(req.url)\n+    url.pathname = url.pathname.replace(/^\\/\\//, '/%2F');\n     var length = url.pathname.length\n     var hasSlash = url.pathname.charAt(length - 1) === '/'\n \n"}
{"cve":"CVE-2018-3734:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 5fcd266..2437a22 100644\n--- a/index.js\n+++ b/index.js\n@@ -50,7 +50,7 @@ module.exports.listen = function (port, cb) {\n             res.setHeader(\"Access-Control-Allow-Headers\", \"X-Requested-With,content-type\");\n         }\n         let pathname = url.parse(req.url).pathname;\n-        let localPath = path.join(options.folder, pathname);\n+                let localPath = path.join(options.folder, pathname);\n         if (path.extname(localPath) === \"\") {\n             //Add the index file to the local path\n             localPath = path.join(localPath, \"./\" + path.basename(options.index));\n"}
{"cve":"CVE-2020-8132:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 4fa6231..86d7ace 100644\n--- a/index.js\n+++ b/index.js\n@@ -6,6 +6,8 @@ var path = require(\"path\");\n var fs   = require(\"fs\");\n var util = require(\"util\");\n var exec = require(\"child_process\").exec;\n+var shellescape = require('shell-escape');\n+\n \n function PDFImage(pdfFilePath, options) {\n   if (!options) options = {};\n@@ -22,10 +24,11 @@ function PDFImage(pdfFilePath, options) {\n }\n \n PDFImage.prototype = {\n+\n   constructGetInfoCommand: function () {\n     return util.format(\n-      \"pdfinfo \\\"%s\\\"\",\n-      this.pdfFilePath\n+      \"pdfinfo %s\",\n+      shellescape([this.pdfFilePath])\n     );\n   },\n   parseGetInfoCommandOutput: function (output) {\n@@ -86,18 +89,18 @@ PDFImage.prototype = {\n     var outputImagePath = this.getOutputImagePathForPage(pageNumber);\n     var convertOptionsString = this.constructConvertOptions();\n     return util.format(\n-      \"%s %s\\\"%s[%d]\\\" \\\"%s\\\"\",\n+      \"%s %s%s[%d] %s\",\n       this.useGM ? \"gm convert\" : \"convert\",\n       convertOptionsString ? convertOptionsString + \" \" : \"\",\n-      pdfFilePath, pageNumber, outputImagePath\n+      shellescape([pdfFilePath]), pageNumber, shellescape([outputImagePath])\n     );\n   },\n   constructCombineCommandForFile: function (imagePaths) {\n     return util.format(\n-      \"%s -append %s \\\"%s\\\"\",\n+      \"%s -append %s %s\",\n       this.useGM ? \"gm convert\" : \"convert\",\n-      imagePaths.join(' '),\n-      this.getOutputImagePathForFile()\n+      shellescape(imagePaths),\n+      shellescape([this.getOutputImagePathForFile()])\n     );\n   },\n   constructConvertOptions: function () {\n@@ -122,7 +125,7 @@ PDFImage.prototype = {\n             stderr: stderr\n           });\n         }\n-        exec(\"rm \"+imagePaths.join(' ')); //cleanUp\n+        exec(\"rm \" + shellescape(imagePaths)); //cleanUp\n         return resolve(pdfImage.getOutputImagePathForFile());\n       });\n     });\ndiff --git a/package.json b/package.json\nindex 4919157..6aa588c 100644\n--- a/package.json\n+++ b/package.json\n@@ -12,7 +12,7 @@\n   \"homepage\": \"https://github.com/mooz/node-pdf-image#readme\",\n   \"dependencies\": {\n     \"es6-promise\": \"~4.2.4\",\n-    \"child-process-promise\": \"^2.2.1\"\n+    \"shell-escape\": \"^0.2.0\"\n   },\n   \"devDependencies\": {\n     \"chai\": \"~4.1.2\",\n@@ -22,4 +22,4 @@\n     \"test\": \"mocha tests/*\"\n   },\n   \"license\": \"MIT\"\n-}\n+}\n\\ No newline at end of file\n"}
{"cve":"CVE-2018-3785:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 38d70b1..318d5e2 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,40 +1,41 @@\n 'use strict';\n var shell = require('shelljs');\n+var escape = require('any-shell-escape');\n \n var defaultMsg = 'Test commit';\n \n function makeDefault(str) {\n-\tif ((typeof str === 'string' && !str.trim()) || str === undefined) {\n-\t\treturn defaultMsg;\n-\t}\n+if ((typeof str === 'string' && !str.trim()) || str === undefined) {\n+return defaultMsg;\n+}\n \n-\treturn str;\n+return str;\n }\n \n module.exports = function (msg, silent) {\n-\tvar arg = '';\n-\n-\tmsg = makeDefault(msg);\n-\n-\tif (silent === undefined) {\n-\t\tsilent = true;\n-\t}\n-\n-\tif (Array.isArray(msg)) {\n-\t\tif (msg.length) {\n-\t\t\tmsg.forEach(function (m) {\n-\t\t\t\tm = makeDefault(m);\n-\n-\t\t\t\targ += '-m\"' + m + '\" ';\n-\t\t\t});\n-\t\t} else {\n-\t\t\targ = '-m\"' + defaultMsg + '\"';\n-\t\t}\n-\t} else {\n-\t\targ = '-m\"' + msg + '\"';\n-\t}\n-\n-\tshell.exec('git commit ' + arg + ' --allow-empty --no-gpg-sign', {\n-\t\tsilent: silent\n-\t});\n+var arg = '';\n+\n+msg = makeDefault(msg);\n+\n+if (silent === undefined) {\n+silent = true;\n+}\n+\n+if (Array.isArray(msg)) {\n+if (msg.length) {\n+msg.forEach(function (m) {\n+m = makeDefault(m);\n+\n+arg += '-m\\\"' + m + '\\\" ';\n+});\n+} else {\n+arg = '-m\\\"' + defaultMsg + '\\\"';\n+}\n+} else {\n+arg = '-m\\\"' + msg + '\\\"';\n+}\n+\n+shell.exec('git commit ' + escape(arg) + ' --allow-empty --no-gpg-sign', {\n+silent: silent\n+});\n };\ndiff --git a/package.json b/package.json\nindex 206307c..3363fcd 100644\n--- a/package.json\n+++ b/package.json\n@@ -27,7 +27,8 @@\n   \"dependencies\": {\n     \"execa\": \"^5.1.1\",\n     \"meow\": \"^3.6.0\",\n-    \"shelljs\": \"^0.8.3\"\n+    \"shelljs\": \"^0.8.3\",\n+    \"any-shell-escape\": \"^0.1.1\"\n   },\n   \"devDependencies\": {\n     \"ava\": \"*\",\n"}
{"cve":"CVE-2017-16042:0708", "fix_patch": "diff --git a/lib/growl.js b/lib/growl.js\nindex 719b5af..ae62301 100644\n--- a/lib/growl.js\n+++ b/lib/growl.js\n@@ -4,7 +4,7 @@\n  * Module dependencies.\n  */\n \n-var exec = require('child_process').exec\n+var execFile = require('child_process').execFile\n   , fs = require('fs')\n   , path = require('path')\n   , exists = fs.existsSync || path.existsSync\n@@ -127,7 +127,7 @@ exports = module.exports = growl;\n  * Node-growl version.\n  */\n \n-exports.version = '1.4.1'\n+exports.version = '1.10.2'\n \n /**\n  * Send growl notification _msg_ with _options_.\n@@ -167,17 +167,11 @@ function growl(msg, options, fn) {\n     , options = options || {}\n     , fn = fn || function(){};\n \n-  if (options.exec) {\n-    cmd = {\n-        type: \"Custom\"\n-      , pkg: options.exec\n-      , range: []\n-    };\n-  }\n+  msg = String(msg || '');\n \n   // noop\n   if (!cmd) return fn(new Error('growl not supported on this platform'));\n-  args = [cmd.pkg];\n+  args = [];\n \n   // image\n   if (image = options.image) {\n@@ -189,18 +183,18 @@ function growl(msg, options, fn) {\n         flag = flag || /^png|gif|jpe?g$/.test(ext) && 'image'\n         flag = flag || ext && (image = ext) && 'icon'\n         flag = flag || 'icon'\n-        args.push('--' + flag, quote(image))\n+        args.push('--' + flag, image)\n         break;\n       case 'Darwin-NotificationCenter':\n-        args.push(cmd.icon, quote(image));\n+        args.push(cmd.icon, image);\n         break;\n       case 'Linux':\n-        args.push(cmd.icon, quote(image));\n+        args.push(cmd.icon, image);\n         // libnotify defaults to sticky, set a hint for transient notifications\n         if (!options.sticky) args.push('--hint=int:transient:1');\n         break;\n       case 'Windows':\n-        args.push(cmd.icon + quote(image));\n+        args.push(cmd.icon + image);\n         break;\n     }\n   }\n@@ -211,9 +205,8 @@ function growl(msg, options, fn) {\n   // priority\n   if (options.priority) {\n     var priority = options.priority + '';\n-    var checkindexOf = cmd.priority.range.indexOf(priority);\n     if (~cmd.priority.range.indexOf(priority)) {\n-      args.push(cmd.priority, options.priority);\n+      args.push(cmd.priority.cmd, priority);\n     }\n   }\n \n@@ -229,62 +222,34 @@ function growl(msg, options, fn) {\n \n   switch(cmd.type) {\n     case 'Darwin-Growl':\n-      args.push(cmd.msg);\n-      args.push(quote(msg).replace(/\\\\n/g, '\\n'));\n-      if (options.title) args.push(quote(options.title));\n+      args.push(cmd.msg, msg);\n+      if (options.title) args.push(options.title);\n       break;\n     case 'Darwin-NotificationCenter':\n-      args.push(cmd.msg);\n-      var stringifiedMsg = quote(msg);\n-      var escapedMsg = stringifiedMsg.replace(/\\\\n/g, '\\n');\n-      args.push(escapedMsg);\n-      if (options.title) {\n-        args.push(cmd.title);\n-        args.push(quote(options.title));\n-      }\n-      if (options.subtitle) {\n-        args.push(cmd.subtitle);\n-        args.push(quote(options.subtitle));\n-      }\n-      if (options.url) {\n-        args.push(cmd.url);\n-        args.push(quote(options.url));\n-      }\n+      args.push(cmd.msg, msg);\n+      if (options.title) args.push(cmd.title, options.title);\n+      if (options.subtitle) args.push(cmd.subtitle, options.subtitle);\n+      if (options.url) args.push(cmd.url, options.url);\n       break;\n     case 'Linux-Growl':\n-      args.push(cmd.msg);\n-      args.push(quote(msg).replace(/\\\\n/g, '\\n'));\n-      if (options.title) args.push(quote(options.title));\n+      args.push(cmd.msg, msg);\n+      if (options.title) args.push(options.title);\n       if (cmd.host) {\n         args.push(cmd.host.cmd, cmd.host.hostname)\n       }\n       break;\n     case 'Linux':\n       if (options.title) {\n-        args.push(quote(options.title));\n-        args.push(cmd.msg);\n-        args.push(quote(msg).replace(/\\\\n/g, '\\n'));\n-      } else {\n-        args.push(quote(msg).replace(/\\\\n/g, '\\n'));\n-      }\n+        args.push(options.title);\n+      } \n+      args.push(msg);\n       break;\n     case 'Windows':\n-      args.push(quote(msg).replace(/\\\\n/g, '\\n'));\n-      if (options.title) args.push(cmd.title + quote(options.title));\n-      if (options.url) args.push(cmd.url + quote(options.url));\n-      break;\n-    case 'Custom':\n-      args[0] = (function(origCommand) {\n-        var message = options.title\n-          ? options.title + ': ' + msg\n-          : msg;\n-        var command = origCommand.replace(/(^|[^%])%s/g, '$1' + quote(message));\n-        if (command === origCommand) args.push(quote(message));\n-        return command;\n-      })(args[0]);\n+      args.push(msg);\n+      if (options.title) args.push(cmd.title + options.title);\n+      if (options.url) args.push(cmd.url + options.url);\n       break;\n   }\n-\n   // execute\n-  exec(args.join(' '), fn);\n-};\n+  execFile(cmd.pkg, args, fn);\n+};\n\\ No newline at end of file\n"}
{"cve":"CVE-2022-24794:0708", "fix_patch": "diff --git a/lib/context.js b/lib/context.js\nindex c95e363..9c254da 100644\n--- a/lib/context.js\n+++ b/lib/context.js\n@@ -187,7 +187,7 @@ class ResponseContext {\n       returnTo = options.returnTo;\n       debug('req.oidc.login() called with returnTo: %s', returnTo);\n     } else if (req.method === 'GET' && req.originalUrl) {\n-      returnTo = req.originalUrl;\n+      returnTo = req.originalUrl.replace(/^\\/+/,'/');\n       debug('req.oidc.login() without returnTo, using: %s', returnTo);\n     }\n \n"}
{"cve":"CVE-2022-0436:0708", "fix_patch": "diff --git a/lib/grunt/file.js b/lib/grunt/file.js\nindex 7e0e2fb..953834d 100644\n--- a/lib/grunt/file.js\n+++ b/lib/grunt/file.js\n@@ -293,7 +293,21 @@ file.write = function(filepath, contents, options) {\n // Or read a directory, recursively creating directories, reading files,\n // processing content, writing output.\n file.copy = function copy(srcpath, destpath, options) {\n-  if (file.isDir(srcpath)) {\n+  var CWD = path.resolve(process.cwd());\n+  if (path.resolve(destpath).indexOf(CWD) !== 0) {\n+    grunt.fail.warn('Cannot copy files outside of the current working directory.');\n+    return;\n+  }\n+  var stat;\n+  try {\n+    stat = fs.lstatSync(srcpath);\n+  } catch (e) {\n+    grunt.fail.warn('Unable to stat src file, may be a symlink, continuing.');\n+  }\n+\n+  if (stat && stat.isSymbolicLink()) {\n+    // don't copy symbolic links\n+  } else if (stat && stat.isDirectory()) {\n     // Copy a directory, recursively.\n     // Explicitly create new dest directory.\n     file.mkdir(destpath);\n"}
{"cve":"CVE-2021-32803:0708", "fix_patch": "diff --git a/lib/unpack.js b/lib/unpack.js\nindex 1d86c18..30c7ce4 100644\n--- a/lib/unpack.js\n+++ b/lib/unpack.js\n@@ -12,6 +12,7 @@ const wc = require('./winchars.js')\n const stripAbsolutePath = require('./strip-absolute-path.js')\n \n const ONENTRY = Symbol('onEntry')\n+const LSTAT = Symbol('lstat')\n const CHECKFS = Symbol('checkFs')\n const ISREUSABLE = Symbol('isReusable')\n const MAKEFS = Symbol('makeFs')\n@@ -292,6 +293,7 @@ class Unpack extends Parser {\n     return uint32(this.gid, entry.gid, this.processGid)\n   }\n \n+\n   [FILE] (entry) {\n     const mode = entry.mode & 0o7777 || this.fmode\n     const stream = new fsm.WriteStream(entry.absolute, {\n@@ -400,25 +402,37 @@ class Unpack extends Parser {\n     entry.resume()\n   }\n \n+\n+\n+\n   // Check if we can reuse an existing filesystem entry safely and\n   // overwrite it, rather than unlinking and recreating\n   // Windows doesn't report a useful nlink, so we just never reuse entries\n   [ISREUSABLE] (entry, st) {\n-    return entry.type === 'File' &&\n+    return st && entry.type === 'File' &&\n       !this.unlink &&\n       st.isFile() &&\n       st.nlink <= 1 &&\n+\n+  [LSTAT] (path, cb) {\n+    fs.lstat(path, (er, st) => {\n+      if (er && er.code !== 'ENOENT') {\n+        return cb(er)\n+      }\n+      cb(null, st)\n+    })\n+  }\n+\n       process.platform !== 'win32'\n   }\n \n-  // check if a thing is there, and if so, try to clobber it\n   [CHECKFS] (entry) {\n     this[PEND]()\n     this[MKDIR](path.dirname(entry.absolute), this.dmode, er => {\n       if (er)\n         return this[ONERROR](er, entry)\n       fs.lstat(entry.absolute, (er, st) => {\n-        if (st && (this.keep || this.newer && st.mtime > entry.mtime))\n+        if (st && (this.keep || (this.newer && st.mtime > entry.mtime)))\n           this[SKIP](entry)\n         else if (er || this[ISREUSABLE](entry, st))\n           this[MAKEFS](null, entry)\n@@ -450,7 +464,7 @@ class Unpack extends Parser {\n         return this[HARDLINK](entry)\n \n       case 'SymbolicLink':\n-        return this[SYMLINK](entry)\n+return this[SYMLINK](entry)\n \n       case 'Directory':\n       case 'GNUDumpDir':\n@@ -480,7 +494,7 @@ class UnpackSync extends Unpack {\n       return this[ONERROR](er, entry)\n     try {\n       const st = fs.lstatSync(entry.absolute)\n-      if (this.keep || this.newer && st.mtime > entry.mtime)\n+      if (this.keep || (this.newer && st.mtime > entry.mtime))\n         return this[SKIP](entry)\n       else if (this[ISREUSABLE](entry, st))\n         return this[MAKEFS](null, entry)\n"}
{"cve":"CVE-2022-0235:0708", "fix_patch": "diff --git a/src/index.js b/src/index.js\nindex 7175467..79566fe 100644\n--- a/src/index.js\n+++ b/src/index.js\n@@ -21,6 +21,7 @@ import Request, {getNodeRequestOptions} from './request.js';\n import {FetchError} from './errors/fetch-error.js';\n import {AbortError} from './errors/abort-error.js';\n import {isRedirect} from './utils/is-redirect.js';\n+import {isSameOrigin} from './utils/is-same-origin.js';\n import {parseReferrerPolicyFromHeader} from './utils/referrer.js';\n \n export {Headers, Request, Response, FetchError, AbortError, isRedirect};\n@@ -35,320 +36,328 @@ const supportedSchemas = new Set(['data:', 'http:', 'https:']);\n  * @return  {Promise<import('./response').default>}\n  */\n export default async function fetch(url, options_) {\n-\treturn new Promise((resolve, reject) => {\n-\t\t// Build request object\n-\t\tconst request = new Request(url, options_);\n-\t\tconst {parsedURL, options} = getNodeRequestOptions(request);\n-\t\tif (!supportedSchemas.has(parsedURL.protocol)) {\n-\t\t\tthrow new TypeError(`node-fetch cannot load ${url}. URL scheme \"${parsedURL.protocol.replace(/:$/, '')}\" is not supported.`);\n-\t\t}\n-\n-\t\tif (parsedURL.protocol === 'data:') {\n-\t\t\tconst data = dataUriToBuffer(request.url);\n-\t\t\tconst response = new Response(data, {headers: {'Content-Type': data.typeFull}});\n-\t\t\tresolve(response);\n-\t\t\treturn;\n-\t\t}\n-\n-\t\t// Wrap http.request into fetch\n-\t\tconst send = (parsedURL.protocol === 'https:' ? https : http).request;\n-\t\tconst {signal} = request;\n-\t\tlet response = null;\n-\n-\t\tconst abort = () => {\n-\t\t\tconst error = new AbortError('The operation was aborted.');\n-\t\t\treject(error);\n-\t\t\tif (request.body && request.body instanceof Stream.Readable) {\n-\t\t\t\trequest.body.destroy(error);\n-\t\t\t}\n-\n-\t\t\tif (!response || !response.body) {\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\tresponse.body.emit('error', error);\n-\t\t};\n-\n-\t\tif (signal && signal.aborted) {\n-\t\t\tabort();\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tconst abortAndFinalize = () => {\n-\t\t\tabort();\n-\t\t\tfinalize();\n-\t\t};\n-\n-\t\t// Send request\n-\t\tconst request_ = send(parsedURL.toString(), options);\n-\n-\t\tif (signal) {\n-\t\t\tsignal.addEventListener('abort', abortAndFinalize);\n-\t\t}\n-\n-\t\tconst finalize = () => {\n-\t\t\trequest_.abort();\n-\t\t\tif (signal) {\n-\t\t\t\tsignal.removeEventListener('abort', abortAndFinalize);\n-\t\t\t}\n-\t\t};\n-\n-\t\trequest_.on('error', error => {\n-\t\t\treject(new FetchError(`request to ${request.url} failed, reason: ${error.message}`, 'system', error));\n-\t\t\tfinalize();\n-\t\t});\n-\n-\t\tfixResponseChunkedTransferBadEnding(request_, error => {\n-\t\t\tresponse.body.destroy(error);\n-\t\t});\n-\n-\t\t/* c8 ignore next 18 */\n-\t\tif (process.version < 'v14') {\n-\t\t\t// Before Node.js 14, pipeline() does not fully support async iterators and does not always\n-\t\t\t// properly handle when the socket close/end events are out of order.\n-\t\t\trequest_.on('socket', s => {\n-\t\t\t\tlet endedWithEventsCount;\n-\t\t\t\ts.prependListener('end', () => {\n-\t\t\t\t\tendedWithEventsCount = s._eventsCount;\n-\t\t\t\t});\n-\t\t\t\ts.prependListener('close', hadError => {\n-\t\t\t\t\t// if end happened before close but the socket didn't emit an error, do it now\n-\t\t\t\t\tif (response && endedWithEventsCount < s._eventsCount && !hadError) {\n-\t\t\t\t\t\tconst error = new Error('Premature close');\n-\t\t\t\t\t\terror.code = 'ERR_STREAM_PREMATURE_CLOSE';\n-\t\t\t\t\t\tresponse.body.emit('error', error);\n-\t\t\t\t\t}\n-\t\t\t\t});\n-\t\t\t});\n-\t\t}\n-\n-\t\trequest_.on('response', response_ => {\n-\t\t\trequest_.setTimeout(0);\n-\t\t\tconst headers = fromRawHeaders(response_.rawHeaders);\n-\n-\t\t\t// HTTP fetch step 5\n-\t\t\tif (isRedirect(response_.statusCode)) {\n-\t\t\t\t// HTTP fetch step 5.2\n-\t\t\t\tconst location = headers.get('Location');\n-\n-\t\t\t\t// HTTP fetch step 5.3\n-\t\t\t\tlet locationURL = null;\n-\t\t\t\ttry {\n-\t\t\t\t\tlocationURL = location === null ? null : new URL(location, request.url);\n-\t\t\t\t} catch {\n-\t\t\t\t\t// error here can only be invalid URL in Location: header\n-\t\t\t\t\t// do not throw when options.redirect == manual\n-\t\t\t\t\t// let the user extract the errorneous redirect URL\n-\t\t\t\t\tif (request.redirect !== 'manual') {\n-\t\t\t\t\t\treject(new FetchError(`uri requested responds with an invalid redirect URL: ${location}`, 'invalid-redirect'));\n-\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\t// HTTP fetch step 5.5\n-\t\t\t\tswitch (request.redirect) {\n-\t\t\t\t\tcase 'error':\n-\t\t\t\t\t\treject(new FetchError(`uri requested responds with a redirect, redirect mode is set to error: ${request.url}`, 'no-redirect'));\n-\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\tcase 'manual':\n-\t\t\t\t\t\t// Node-fetch-specific step: make manual redirect a bit easier to use by setting the Location header value to the resolved URL.\n-\t\t\t\t\t\tif (locationURL !== null) {\n-\t\t\t\t\t\t\theaders.set('Location', locationURL);\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t\tcase 'follow': {\n-\t\t\t\t\t\t// HTTP-redirect fetch step 2\n-\t\t\t\t\t\tif (locationURL === null) {\n-\t\t\t\t\t\t\tbreak;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 5\n-\t\t\t\t\t\tif (request.counter >= request.follow) {\n-\t\t\t\t\t\t\treject(new FetchError(`maximum redirect reached at: ${request.url}`, 'max-redirect'));\n-\t\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\t\treturn;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 6 (counter increment)\n-\t\t\t\t\t\t// Create a new Request object.\n-\t\t\t\t\t\tconst requestOptions = {\n-\t\t\t\t\t\t\theaders: new Headers(request.headers),\n-\t\t\t\t\t\t\tfollow: request.follow,\n-\t\t\t\t\t\t\tcounter: request.counter + 1,\n-\t\t\t\t\t\t\tagent: request.agent,\n-\t\t\t\t\t\t\tcompress: request.compress,\n-\t\t\t\t\t\t\tmethod: request.method,\n-\t\t\t\t\t\t\tbody: clone(request),\n-\t\t\t\t\t\t\tsignal: request.signal,\n-\t\t\t\t\t\t\tsize: request.size,\n-\t\t\t\t\t\t\treferrer: request.referrer,\n-\t\t\t\t\t\t\treferrerPolicy: request.referrerPolicy\n-\t\t\t\t\t\t};\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 9\n-\t\t\t\t\t\tif (response_.statusCode !== 303 && request.body && options_.body instanceof Stream.Readable) {\n-\t\t\t\t\t\t\treject(new FetchError('Cannot follow redirect with body being a readable stream', 'unsupported-redirect'));\n-\t\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\t\treturn;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 11\n-\t\t\t\t\t\tif (response_.statusCode === 303 || ((response_.statusCode === 301 || response_.statusCode === 302) && request.method === 'POST')) {\n-\t\t\t\t\t\t\trequestOptions.method = 'GET';\n-\t\t\t\t\t\t\trequestOptions.body = undefined;\n-\t\t\t\t\t\t\trequestOptions.headers.delete('content-length');\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 14\n-\t\t\t\t\t\tconst responseReferrerPolicy = parseReferrerPolicyFromHeader(headers);\n-\t\t\t\t\t\tif (responseReferrerPolicy) {\n-\t\t\t\t\t\t\trequestOptions.referrerPolicy = responseReferrerPolicy;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 15\n-\t\t\t\t\t\tresolve(fetch(new Request(locationURL, requestOptions)));\n-\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\t}\n-\n-\t\t\t\t\tdefault:\n-\t\t\t\t\t\treturn reject(new TypeError(`Redirect option '${request.redirect}' is not a valid value of RequestRedirect`));\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\t// Prepare response\n-\t\t\tif (signal) {\n-\t\t\t\tresponse_.once('end', () => {\n-\t\t\t\t\tsignal.removeEventListener('abort', abortAndFinalize);\n-\t\t\t\t});\n-\t\t\t}\n-\n-\t\t\tlet body = pump(response_, new PassThrough(), reject);\n-\t\t\t// see https://github.com/nodejs/node/pull/29376\n-\t\t\tif (process.version < 'v12.10') {\n-\t\t\t\tresponse_.on('aborted', abortAndFinalize);\n-\t\t\t}\n-\n-\t\t\tconst responseOptions = {\n-\t\t\t\turl: request.url,\n-\t\t\t\tstatus: response_.statusCode,\n-\t\t\t\tstatusText: response_.statusMessage,\n-\t\t\t\theaders,\n-\t\t\t\tsize: request.size,\n-\t\t\t\tcounter: request.counter,\n-\t\t\t\thighWaterMark: request.highWaterMark\n-\t\t\t};\n-\n-\t\t\t// HTTP-network fetch step 12.1.1.3\n-\t\t\tconst codings = headers.get('Content-Encoding');\n-\n-\t\t\t// HTTP-network fetch step 12.1.1.4: handle content codings\n-\n-\t\t\t// in following scenarios we ignore compression support\n-\t\t\t// 1. compression support is disabled\n-\t\t\t// 2. HEAD request\n-\t\t\t// 3. no Content-Encoding header\n-\t\t\t// 4. no content response (204)\n-\t\t\t// 5. content not modified response (304)\n-\t\t\tif (!request.compress || request.method === 'HEAD' || codings === null || response_.statusCode === 204 || response_.statusCode === 304) {\n-\t\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\t\tresolve(response);\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\t// For Node v6+\n-\t\t\t// Be less strict when decoding compressed responses, since sometimes\n-\t\t\t// servers send slightly invalid responses that are still accepted\n-\t\t\t// by common browsers.\n-\t\t\t// Always using Z_SYNC_FLUSH is what cURL does.\n-\t\t\tconst zlibOptions = {\n-\t\t\t\tflush: zlib.Z_SYNC_FLUSH,\n-\t\t\t\tfinishFlush: zlib.Z_SYNC_FLUSH\n-\t\t\t};\n-\n-\t\t\t// For gzip\n-\t\t\tif (codings === 'gzip' || codings === 'x-gzip') {\n-\t\t\t\tbody = pump(body, zlib.createGunzip(zlibOptions), reject);\n-\t\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\t\tresolve(response);\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\t// For deflate\n-\t\t\tif (codings === 'deflate' || codings === 'x-deflate') {\n-\t\t\t\t// Handle the infamous raw deflate response from old servers\n-\t\t\t\t// a hack for old IIS and Apache servers\n-\t\t\t\tconst raw = pump(response_, new PassThrough(), reject);\n-\t\t\t\traw.once('data', chunk => {\n-\t\t\t\t\t// See http://stackoverflow.com/questions/37519828\n-\t\t\t\t\tbody = (chunk[0] & 0x0F) === 0x08 ? pump(body, zlib.createInflate(), reject) : pump(body, zlib.createInflateRaw(), reject);\n-\n-\t\t\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\t\t\tresolve(response);\n-\t\t\t\t});\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\t// For br\n-\t\t\tif (codings === 'br') {\n-\t\t\t\tbody = pump(body, zlib.createBrotliDecompress(), reject);\n-\t\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\t\tresolve(response);\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\t// Otherwise, use response as-is\n-\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\tresolve(response);\n-\t\t});\n-\n-\t\t// eslint-disable-next-line promise/prefer-await-to-then\n-\t\twriteToStream(request_, request).catch(reject);\n-\t});\n+        return new Promise((resolve, reject) => {\n+                // Build request object\n+                const request = new Request(url, options_);\n+                const {parsedURL, options} = getNodeRequestOptions(request);\n+                if (!supportedSchemas.has(parsedURL.protocol)) {\n+                        throw new TypeError(`node-fetch cannot load ${url}. URL scheme \"${parsedURL.protocol.replace(/:$/, '')}\" is not supported.`);\n+                }\n+\n+                if (parsedURL.protocol === 'data:') {\n+                        const data = dataUriToBuffer(request.url);\n+                        const response = new Response(data, {headers: {'Content-Type': data.typeFull}});\n+                        resolve(response);\n+                        return;\n+                }\n+\n+                // Wrap http.request into fetch\n+                const send = (parsedURL.protocol === 'https:' ? https : http).request;\n+                const {signal} = request;\n+                let response = null;\n+\n+                const abort = () => {\n+                        const error = new AbortError('The operation was aborted.');\n+                        reject(error);\n+                        if (request.body && request.body instanceof Stream.Readable) {\n+                                request.body.destroy(error);\n+                        }\n+\n+                        if (!response || !response.body) {\n+                                return;\n+                        }\n+\n+                        response.body.emit('error', error);\n+                };\n+\n+                if (signal && signal.aborted) {\n+                        abort();\n+                        return;\n+                }\n+\n+                const abortAndFinalize = () => {\n+                        abort();\n+                        finalize();\n+                };\n+\n+                // Send request\n+                const request_ = send(parsedURL.toString(), options);\n+\n+                if (signal) {\n+                        signal.addEventListener('abort', abortAndFinalize);\n+                }\n+\n+                const finalize = () => {\n+                        request_.abort();\n+                        if (signal) {\n+                                signal.removeEventListener('abort', abortAndFinalize);\n+                        }\n+                };\n+\n+                request_.on('error', error => {\n+                        reject(new FetchError(`request to ${request.url} failed, reason: ${error.message}`, 'system', error));\n+                        finalize();\n+                });\n+\n+                fixResponseChunkedTransferBadEnding(request_, error => {\n+                        response.body.destroy(error);\n+                });\n+\n+                /* c8 ignore next 18 */\n+                if (process.version < 'v14') {\n+                        // Before Node.js 14, pipeline() does not fully support async iterators and does not always\n+                        // properly handle when the socket close/end events are out of order.\n+                        request_.on('socket', s => {\n+                                let endedWithEventsCount;\n+                                s.prependListener('end', () => {\n+                                        endedWithEventsCount = s._eventsCount;\n+                                });\n+                                s.prependListener('close', hadError => {\n+                                        // if end happened before close but the socket didn't emit an error, do it now\n+                                        if (response && endedWithEventsCount < s._eventsCount && !hadError) {\n+                                                const error = new Error('Premature close');\n+                                                error.code = 'ERR_STREAM_PREMATURE_CLOSE';\n+                                                response.body.emit('error', error);\n+                                        }\n+                                });\n+                        });\n+                }\n+\n+                request_.on('response', response_ => {\n+                        request_.setTimeout(0);\n+                        const headers = fromRawHeaders(response_.rawHeaders);\n+\n+                        // HTTP fetch step 5\n+                        if (isRedirect(response_.statusCode)) {\n+                                // HTTP fetch step 5.2\n+                                const location = headers.get('Location');\n+\n+                                // HTTP fetch step 5.3\n+                                let locationURL = null;\n+                                try {\n+                                        locationURL = location === null ? null : new URL(location, request.url);\n+                                } catch {\n+                                        // error here can only be invalid URL in Location: header\n+                                        // do not throw when options.redirect == manual\n+                                        // let the user extract the errorneous redirect URL\n+                                        if (request.redirect !== 'manual') {\n+                                                reject(new FetchError(`uri requested responds with an invalid redirect URL: ${location}`, 'invalid-redirect'));\n+                                                finalize();\n+                                                return;\n+                                        }\n+                                }\n+\n+                                // HTTP fetch step 5.5\n+                                switch (request.redirect) {\n+                                        case 'error':\n+                                                reject(new FetchError(`uri requested responds with a redirect, redirect mode is set to error: ${request.url}`, 'no-redirect'));\n+                                                finalize();\n+                                                return;\n+                                        case 'manual':\n+                                                // Node-fetch-specific step: make manual redirect a bit easier to use by setting the Location header value to the resolved URL.\n+                                                if (locationURL !== null) {\n+                                                        headers.set('Location', locationURL);\n+                                                }\n+\n+                                                break;\n+                                        case 'follow': {\n+                                                // HTTP-redirect fetch step 2\n+                                                if (locationURL === null) {\n+                                                        break;\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 5\n+                                                if (request.counter >= request.follow) {\n+                                                        reject(new FetchError(`maximum redirect reached at: ${request.url}`, 'max-redirect'));\n+                                                        finalize();\n+                                                        return;\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 6 (counter increment)\n+                                                // Create a new Request object.\n+                                                const requestOptions = {\n+                                                        headers: new Headers(request.headers),\n+                                                        follow: request.follow,\n+                                                        counter: request.counter + 1,\n+                                                        agent: request.agent,\n+                                                        compress: request.compress,\n+                                                        method: request.method,\n+                                                        body: clone(request),\n+                                                        signal: request.signal,\n+                                                        size: request.size,\n+                                                        referrer: request.referrer,\n+                                                        referrerPolicy: request.referrerPolicy\n+                                                };\n+\n+                                                // HTTP-redirect fetch step 9\n+                                                if (response_.statusCode !== 303 && request.body && options_.body instanceof Stream.Readable) {\n+                                                        reject(new FetchError('Cannot follow redirect with body being a readable stream', 'unsupported-redirect'));\n+                                                        finalize();\n+                                                        return;\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 11\n+                                                if (response_.statusCode === 303 || ((response_.statusCode === 301 || response_.statusCode === 302) && request.method === 'POST')) {\n+                                                        requestOptions.method = 'GET';\n+                                                        requestOptions.body = undefined;\n+                                                        requestOptions.headers.delete('content-length');\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 14\n+                                                const responseReferrerPolicy = parseReferrerPolicyFromHeader(headers);\n+                                                if (responseReferrerPolicy) {\n+                                                        requestOptions.referrerPolicy = responseReferrerPolicy;\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 12\n+                                                if (!isSameOrigin(request.url, locationURL)) {\n+                                                    requestOptions.headers.delete('authorization');\n+                                                    requestOptions.headers.delete('www-authenticate');\n+                                                    requestOptions.headers.delete('cookie');\n+                                                    requestOptions.headers.delete('cookie2');\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 15\n+                                                resolve(fetch(new Request(locationURL, requestOptions)));\n+                                                finalize();\n+                                                return;\n+                                        }\n+\n+                                        default:\n+                                                return reject(new TypeError(`Redirect option '${request.redirect}' is not a valid value of RequestRedirect`));\n+                                }\n+                        }\n+\n+                        // Prepare response\n+                        if (signal) {\n+                                response_.once('end', () => {\n+                                        signal.removeEventListener('abort', abortAndFinalize);\n+                                });\n+                        }\n+\n+                        let body = pump(response_, new PassThrough(), reject);\n+                        // see https://github.com/nodejs/node/pull/29376\n+                        if (process.version < 'v12.10') {\n+                                response_.on('aborted', abortAndFinalize);\n+                        }\n+\n+                        const responseOptions = {\n+                                url: request.url,\n+                                status: response_.statusCode,\n+                                statusText: response_.statusMessage,\n+                                headers,\n+                                size: request.size,\n+                                counter: request.counter,\n+                                highWaterMark: request.highWaterMark\n+                        };\n+\n+                        // HTTP-network fetch step 12.1.1.3\n+                        const codings = headers.get('Content-Encoding');\n+\n+                        // HTTP-network fetch step 12.1.1.4: handle content codings\n+\n+                        // in following scenarios we ignore compression support\n+                        // 1. compression support is disabled\n+                        // 2. HEAD request\n+                        // 3. no Content-Encoding header\n+                        // 4. no content response (204)\n+                        // 5. content not modified response (304)\n+                        if (!request.compress || request.method === 'HEAD' || codings === null || response_.statusCode === 204 || response_.statusCode === 304) {\n+                                response = new Response(body, responseOptions);\n+                                resolve(response);\n+                                return;\n+                        }\n+\n+                        // For Node v6+\n+                        // Be less strict when decoding compressed responses, since sometimes\n+                        // servers send slightly invalid responses that are still accepted\n+                        // by common browsers.\n+                        // Always using Z_SYNC_FLUSH is what cURL does.\n+                        const zlibOptions = {\n+                                flush: zlib.Z_SYNC_FLUSH,\n+                                finishFlush: zlib.Z_SYNC_FLUSH\n+                        };\n+\n+                        // For gzip\n+                        if (codings === 'gzip' || codings === 'x-gzip') {\n+                                body = pump(body, zlib.createGunzip(zlibOptions), reject);\n+                                response = new Response(body, responseOptions);\n+                                resolve(response);\n+                                return;\n+                        }\n+\n+                        // For deflate\n+                        if (codings === 'deflate' || codings === 'x-deflate') {\n+                                // Handle the infamous raw deflate response from old servers\n+                                // a hack for old IIS and Apache servers\n+                                const raw = pump(response_, new PassThrough(), reject);\n+                                raw.once('data', chunk => {\n+                                        // See http://stackoverflow.com/questions/37519828\n+                                        body = (chunk[0] & 0x0F) === 0x08 ? pump(body, zlib.createInflate(), reject) : pump(body, zlib.createInflateRaw(), reject);\n+\n+                                        response = new Response(body, responseOptions);\n+                                        resolve(response);\n+                                });\n+                                return;\n+                        }\n+\n+                        // For br\n+                        if (codings === 'br') {\n+                                body = pump(body, zlib.createBrotliDecompress(), reject);\n+                                response = new Response(body, responseOptions);\n+                                resolve(response);\n+                                return;\n+                        }\n+\n+                        // Otherwise, use response as-is\n+                        response = new Response(body, responseOptions);\n+                        resolve(response);\n+                });\n+\n+                // eslint-disable-next-line promise/prefer-await-to-then\n+                writeToStream(request_, request).catch(reject);\n+        });\n }\n \n function fixResponseChunkedTransferBadEnding(request, errorCallback) {\n-\tconst LAST_CHUNK = Buffer.from('0\\r\\n\\r\\n');\n-\n-\tlet isChunkedTransfer = false;\n-\tlet properLastChunkReceived = false;\n-\tlet previousChunk;\n-\n-\trequest.on('response', response => {\n-\t\tconst {headers} = response;\n-\t\tisChunkedTransfer = headers['transfer-encoding'] === 'chunked' && !headers['content-length'];\n-\t});\n-\n-\trequest.on('socket', socket => {\n-\t\tconst onSocketClose = () => {\n-\t\t\tif (isChunkedTransfer && !properLastChunkReceived) {\n-\t\t\t\tconst error = new Error('Premature close');\n-\t\t\t\terror.code = 'ERR_STREAM_PREMATURE_CLOSE';\n-\t\t\t\terrorCallback(error);\n-\t\t\t}\n-\t\t};\n-\n-\t\tsocket.prependListener('close', onSocketClose);\n-\n-\t\trequest.on('abort', () => {\n-\t\t\tsocket.removeListener('close', onSocketClose);\n-\t\t});\n-\n-\t\tsocket.on('data', buf => {\n-\t\t\tproperLastChunkReceived = Buffer.compare(buf.slice(-5), LAST_CHUNK) === 0;\n-\n-\t\t\t// Sometimes final 0-length chunk and end of message code are in separate packets\n-\t\t\tif (!properLastChunkReceived && previousChunk) {\n-\t\t\t\tproperLastChunkReceived = (\n-\t\t\t\t\tBuffer.compare(previousChunk.slice(-3), LAST_CHUNK.slice(0, 3)) === 0 &&\n-\t\t\t\t\tBuffer.compare(buf.slice(-2), LAST_CHUNK.slice(3)) === 0\n-\t\t\t\t);\n-\t\t\t}\n-\n-\t\t\tpreviousChunk = buf;\n-\t\t});\n-\t});\n+        const LAST_CHUNK = Buffer.from('0\\r\\n\\r\\n');\n+\n+        let isChunkedTransfer = false;\n+        let properLastChunkReceived = false;\n+        let previousChunk;\n+\n+        request.on('response', response => {\n+                const {headers} = response;\n+                isChunkedTransfer = headers['transfer-encoding'] === 'chunked' && !headers['content-length'];\n+        });\n+\n+        request.on('socket', socket => {\n+                const onSocketClose = () => {\n+                        if (isChunkedTransfer && !properLastChunkReceived) {\n+                                const error = new Error('Premature close');\n+                                error.code = 'ERR_STREAM_PREMATURE_CLOSE';\n+                                errorCallback(error);\n+                        }\n+                };\n+\n+                socket.prependListener('close', onSocketClose);\n+\n+                request.on('abort', () => {\n+                        socket.removeListener('close', onSocketClose);\n+                });\n+\n+                socket.on('data', buf => {\n+                        properLastChunkReceived = Buffer.compare(buf.slice(-5), LAST_CHUNK) === 0;\n+\n+                        // Sometimes final 0-length chunk and end of message code are in separate packets\n+                        if (!properLastChunkReceived && previousChunk) {\n+                                properLastChunkReceived = (\n+                                        Buffer.compare(previousChunk.slice(-3), LAST_CHUNK.slice(0, 3)) === 0 &&\n+                                        Buffer.compare(buf.slice(-2), LAST_CHUNK.slice(3)) === 0\n+                                );\n+                        }\n+\n+                        previousChunk = buf;\n+                });\n+        });\n }\ndiff --git a/src/utils/is-same-origin.js b/src/utils/is-same-origin.js\nnew file mode 100644\nindex 0000000..3740cd3\n--- /dev/null\n+++ b/src/utils/is-same-origin.js\n@@ -0,0 +1,10 @@\n+import {URL} from 'node:url';\n+\n+/**\n+ * @param {URL} a\n+ * @param {URL} b\n+ * @returns {boolean}\n+ */\n+export function isSameOrigin(a, b) {\n+return a.origin === b.origin;\n+}\n"}
{"cve":"CVE-2024-29041:0708", "fix_patch": "diff --git a/lib/response.js b/lib/response.js\nindex fede486c..be0a1c8c 100644\n--- a/lib/response.js\n+++ b/lib/response.js\n@@ -23,6 +23,7 @@ var isAbsolute = require('./utils').isAbsolute;\n var onFinished = require('on-finished');\n var path = require('path');\n var statuses = require('statuses')\n+\n var merge = require('utils-merge');\n var sign = require('cookie-signature').sign;\n var normalizeType = require('./utils').normalizeType;\n@@ -912,8 +913,12 @@ res.location = function location(url) {\n   }\n \n   // set location\n+  if (/\\\\/.test(loc)) {\n+    return this.set('Location', loc);\n+  }\n+\n   return this.set('Location', encodeUrl(loc));\n-};\n+}\n \n /**\n  * Redirect to the given `url` with optional response `status`\n"}
{"cve":"CVE-2021-3664:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 94e357e..daa674c 100644\n--- a/index.js\n+++ b/index.js\n@@ -3,7 +3,7 @@\n var required = require('requires-port')\n   , qs = require('querystringify')\n   , slashes = /^[A-Za-z][A-Za-z0-9+-.]*:[\\\\/]+/\n-  , protocolre = /^([a-z][a-z0-9.+-]*:)?([\\\\/]{1,})?([\\S\\s]*)/i\n+  , protocolre = /^([a-z][a-z0-9.+-]*:)?(\\/\\/)?([\\S\\s]*)/i\n   , whitespace = '[\\\\x09\\\\x0A\\\\x0B\\\\x0C\\\\x0D\\\\x20\\\\xA0\\\\u1680\\\\u180E\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200A\\\\u202F\\\\u205F\\\\u3000\\\\u2028\\\\u2029\\\\uFEFF]'\n   , left = new RegExp('^'+ whitespace +'+');\n \n@@ -114,12 +114,11 @@ function lolcation(loc) {\n  * @private\n  */\n function extractProtocol(address) {\n-  address = trimLeft(address);\n \n   var match = protocolre.exec(address)\n     , protocol = match[1] ? match[1].toLowerCase() : ''\n     , slashes = !!(match[2] && match[2].length >= 2)\n-    , rest =  match[2] && match[2].length === 1 ? '/' + match[3] : match[3];\n+    , rest = match[3];\n \n   return {\n     protocol: protocol,\n@@ -179,7 +178,6 @@ function resolve(relative, base) {\n  * @private\n  */\n function Url(address, location, parser) {\n-  address = trimLeft(address);\n \n   if (!(this instanceof Url)) {\n     return new Url(address, location, parser);\n"}
{"cve":"CVE-2022-0155:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 6f0c238..a690a08 100644\n--- a/index.js\n+++ b/index.js\n@@ -395,6 +395,8 @@ RedirectableRequest.prototype._processResponse = function (response) {\n     // Drop the Authorization header if redirecting to another domain\n     if (!(redirectUrlParts.host === currentHost || isSubdomainOf(redirectUrlParts.host, currentHost))) {\n       removeMatchingHeaders(/^authorization$/i, this._options.headers);\n+      removeMatchingHeaders(/^cookie$/i, this._options.headers);\n+      removeMatchingHeaders(/^www-authenticate$/i, this._options.headers);\n     }\n \n     // Evaluate the beforeRedirect callback\n"}
{"cve":"CVE-2021-46561:0708", "fix_patch": "diff --git a/src/controller/org.controller/error.js b/src/controller/org.controller/error.js\nindex 822a4208..a10aabaf 100644\n--- a/src/controller/org.controller/error.js\n+++ b/src/controller/org.controller/error.js\n@@ -56,6 +56,13 @@ class OrgControllerError extends idrErr.IDRError {\n     err.message = `The organization cannot be renamed as '${shortname}' because this shortname is used by another organization.`\n     return err\n   }\n+\n+  notAllowedToChangeOrganization () { // org\n+    const err = {}\n+    err.error = 'NOT_ALLOWED_TO_CHANGE_ORGANIZATION'\n+    err.message = 'Your organization is not allowed to change a user\\'s organization. This can only be done by the Secretariat.'\n+    return err\n+  }\n }\n \n module.exports = {\ndiff --git a/src/controller/org.controller/org.controller.js b/src/controller/org.controller/org.controller.js\nindex 0673f06b..7ff95490 100644\n--- a/src/controller/org.controller/org.controller.js\n+++ b/src/controller/org.controller/org.controller.js\n@@ -531,9 +531,12 @@ async function updateUser (req, res, next) {\n       }\n     })\n \n-    // updating user's roles and org_uuid is only allowed for secretariats and org admins\n-    if (changesRequirePrivilegedRole && !(isAdmin || isSecretariat)) {\n-      logger.info({ uuid: req.ctx.uuid, message: 'The user could not be updated because ' + requesterUsername + ' user is not Org Admin or Secretariat to modify these fields.' })\n+    // updating user's roles is only allowed for secretariats and org admins, and changing a user's org is only allowed for secretariats\n+    if (newOrgShortName && !isSecretariat) {\n+      logger.info({ uuid: req.ctx.uuid, message: 'The user could not be updated because ' + requesterUsername + ' user lacks priveleges to modify these fields.' })\n+      return res.status(403).json(error.notAllowedToChangeOrganization())\n+    } else if (changesRequirePrivilegedRole && !(isAdmin || isSecretariat)) {\n+      logger.info({ uuid: req.ctx.uuid, message: 'The user could not be updated because ' + requesterUsername + ' user lacks priveleges to modify these fields.' })\n       return res.status(403).json(error.notOrgAdminOrSecretariat())\n     }\n \n"}
{"cve":"CVE-2022-0722:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex 31b2a04..2dc760b 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -36,6 +36,21 @@ function parseUrl(url, normalize = false) {\n     if (typeof url !== \"string\" || !url.trim()) {\n         throw new Error(\"Invalid url.\")\n     }\n+\n+    const originalUrl = url;\n+    let isScp = false;\n+    if (!/:\\/\\//.test(url) && url.includes(\"@\") && url.includes(\":\")) {\n+        const atIndex = url.indexOf('@');\n+        const colonIndex = url.indexOf(':');\n+        if (atIndex < colonIndex) {\n+            isScp = true;\n+        }\n+    }\n+\n+    if (isScp) {\n+        url = \"ssh://\" + url.replace(\":\", \"/\");\n+    }\n+\n     if (normalize) {\n         if (typeof normalize !== \"object\") {\n             normalize = {\n@@ -44,7 +59,15 @@ function parseUrl(url, normalize = false) {\n         }\n         url = normalizeUrl(url, normalize)\n     }\n-    const parsed = parsePath(url)\n+\n+    const parsed = parsePath(url);\n+\n+    if (isScp) {\n+        parsed.href = originalUrl;\n+        parsed.pathname = parsed.pathname.replace(\"//\", \"/\")\n+\n+    }\n+\n     return parsed;\n }\n \n"}
{"cve":"CVE-2020-7649:0708", "fix_patch": "diff --git a/lib/filters/index.js b/lib/filters/index.js\nindex 1e27249..eb2e071 100644\n--- a/lib/filters/index.js\n+++ b/lib/filters/index.js\n@@ -71,6 +71,12 @@ module.exports = ruleSource => {\n       // query params might contain additional \"?\"s, only split on the 1st one\n       const parts = mainURI.split('?');\n       let [url, querystring] = [parts[0], parts.slice(1).join('?')];\n+\n+      // VERY IMPORTANT: we need to block path traversal\n+      if (url.match(/\\.\\./g)) {\n+        return false;\n+      }\n+\n       const res = regexp.exec(url);\n       if (!res) {\n         // no url match\n"}
{"cve":"CVE-2020-26299:0708", "fix_patch": "diff --git a/src/fs.js b/src/fs.js\nindex 3226d2e..e596bcc 100644\n--- a/src/fs.js\n+++ b/src/fs.js\n@@ -6,10 +6,12 @@ const {createReadStream, createWriteStream, constants} = require('fs');\n const fsAsync = require('./helpers/fs-async');\n const errors = require('./errors');\n \n+\n class FileSystem {\n+\n   constructor(connection, {root, cwd} = {}) {\n     this.connection = connection;\n-    this.cwd = nodePath.normalize(cwd ? nodePath.join(nodePath.sep, cwd) : nodePath.sep);\n+    this.cwd = nodePath.normalize((cwd || '').replace(/\\/g, '/'));\n     this._root = nodePath.resolve(root || process.cwd());\n   }\n \n@@ -18,9 +20,15 @@ class FileSystem {\n   }\n \n   _resolvePath(path = '.') {\n-    const clientPath = (() => {\n+\n+\n+path = path.replace(/\\/g, '/');\n       path = nodePath.normalize(path);\n+    const clientPath = (() => {\n+\n+      path = path.replace(/\\/g, '/');\n       if (nodePath.isAbsolute(path)) {\n+\n         return nodePath.join(path);\n       } else {\n         return nodePath.join(this.cwd, path);\n@@ -32,6 +40,7 @@ class FileSystem {\n       return nodePath.resolve(nodePath.normalize(nodePath.join(resolvedPath)));\n     })();\n \n+\n     return {\n       clientPath,\n       fsPath\n"}
{"cve":"CVE-2021-37712:0708", "fix_patch": "diff --git a/lib/path-reservations.js b/lib/path-reservations.js\nindex 48c750e..b7d5b1d 100644\n--- a/lib/path-reservations.js\n+++ b/lib/path-reservations.js\n@@ -7,9 +7,13 @@\n // while still allowing maximal safe parallelization.\n \n const assert = require('assert')\n-const normPath = require('./normalize-windows-path.js')\n+const normPath = p => (require('./normalize-windows-path.js')(p.normalize('NFC')) || p).toLowerCase()\n+const stripSlash = p => p.endsWith('/') ? p.slice(0, -1) : p\n const { join } = require('path')\n \n+\n+\n+\n module.exports = () => {\n   // path => [function or Set]\n   // A Set object means a directory reservation\n@@ -23,7 +27,7 @@ module.exports = () => {\n   const getDirs = path =>\n     path.split('/').slice(0, -1).reduce((set, path) =>\n       set.length ? set.concat(normPath(join(set[set.length - 1], path)))\n-      : [path], [])\n+      : [normPath(path)], [])\n \n   // functions currently running\n   const running = new Set()\n@@ -99,7 +103,7 @@ module.exports = () => {\n   }\n \n   const reserve = (paths, fn) => {\n-    paths = paths.map(p => normPath(join(p)).toLowerCase())\n+    paths = paths.map(p => normPath(stripSlash(p)).toLowerCase())\n     const dirs = new Set(\n       paths.map(path => getDirs(path)).reduce((a, b) => a.concat(b))\n     )\n@@ -126,3 +130,5 @@ module.exports = () => {\n \n   return { check, reserve }\n }\n+\n+\n"}
{"cve":"CVE-2021-37713:0708", "fix_patch": "diff --git a/lib/strip-absolute-path.js b/lib/strip-absolute-path.js\nindex 49161dd..1a6b390 100644\n--- a/lib/strip-absolute-path.js\n+++ b/lib/strip-absolute-path.js\n@@ -5,10 +5,15 @@ const { isAbsolute, parse } = require('path').win32\n module.exports = path => {\n   let r = ''\n   while (isAbsolute(path)) {\n-    // windows will think that //x/y/z has a \"root\" of //x/y/\n+    // windows will think that //x/y/z has a \"root\" of //x/y/\\\n     const root = path.charAt(0) === '/' ? '/' : parse(path).root\n     path = path.substr(root.length)\n     r += root\n   }\n+  const drive = /^([a-zA-Z]:)/.exec(path)\n+  if (drive) {\n+    r += drive[0]\n+    path = path.substr(drive[0].length)\n+  }\n   return [r, path]\n }\n"}
{"cve":"CVE-2021-32804:0708", "fix_patch": "diff --git a/lib/write-entry.js b/lib/write-entry.js\nindex 1d0b746..2a7e7b7 100644\n--- a/lib/write-entry.js\n+++ b/lib/write-entry.js\n@@ -53,11 +53,18 @@ const WriteEntry = warner(class WriteEntry extends MiniPass {\n \n     let pathWarn = false\n     if (!this.preservePaths && path.win32.isAbsolute(p)) {\n-      // absolutes on posix are also absolutes on win32\n-      // so we only need to test this one to get both\n-      const parsed = path.win32.parse(p)\n-      this.path = p.substr(parsed.root.length)\n-      pathWarn = parsed.root\n+      let p_ = p\n+      let S = ''\n+      while (path.win32.isAbsolute(p_)) {\n+        const R = path.win32.parse(p_).root\n+        if (R === '') {\n+          break\n+        }\n+        S += R\n+        p_ = p_.substr(R.length)\n+      }\n+      pathWarn = S\n+      this.path = p_\n     }\n \n     this.win32 = !!opt.win32 || process.platform === 'win32'\n"}
{"cve":"CVE-2017-16025:0708", "fix_patch": "diff --git a/lib/socket.js b/lib/socket.js\nindex 261156e..bfa2dc1 100755\n--- a/lib/socket.js\n+++ b/lib/socket.js\n@@ -539,6 +539,11 @@ internals.Socket.prototype._authenticate = function () {\n \n     this._listener._connection.states.parse(cookies, (ignoreErr, state, failed) => {\n \n+        if (!state) {\n+            this.auth._error = Boom.badRequest('Invalid nes authentication cookie');\n+            return;\n+        }\n+\n         const auth = state[config.cookie];\n         if (auth) {\n             this.auth._error = this._setCredentials(auth.credentials, auth.artifacts);\n"}
{"cve":"CVE-2021-41246:0708", "fix_patch": "diff --git a/middleware/auth.js b/middleware/auth.js\nindex c13a949..a21e6d8 100644\n--- a/middleware/auth.js\n+++ b/middleware/auth.js\n@@ -129,10 +129,14 @@ const auth = function (params) {\n             );\n           }\n \n-          Object.assign(req[config.session.name], session);\n-          attemptSilentLogin.resumeSilentLogin(req, res);\n-\n-          next();\n+          req.session.regenerate((err) => {\n+            if (err) {\n+              next(err);\n+              return;\n+            }\n+            Object.assign(req[config.session.name], session);\n+            next();\n+          });\n         } catch (err) {\n           // Swallow errors if this is a silentLogin\n           if (req.openidState && req.openidState.attemptingSilentLogin) {\n"}
{"cve":"CVE-2015-3295:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex abc1525..f5d6beb 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -14,6 +14,7 @@ var mdurl        = require('mdurl');\n var punycode     = require('punycode');\n \n \n+\n var config = {\n   'default': require('./presets/default'),\n   zero: require('./presets/zero'),\n@@ -21,18 +22,7 @@ var config = {\n };\n \n \n-var BAD_PROTOCOLS    = [ 'vbscript', 'javascript', 'file' ];\n-\n-function validateLink(url) {\n-  // url should be normalized at this point, and existing entities are decoded\n-  //\n-  var str = url.trim().toLowerCase();\n \n-  if (str.indexOf(':') >= 0 && BAD_PROTOCOLS.indexOf(str.split(':')[0]) >= 0) {\n-    return false;\n-  }\n-  return true;\n-}\n \n var RECODE_HOSTNAME_FOR = [ 'http:', 'https:', 'mailto:' ];\n \n@@ -274,7 +264,8 @@ function MarkdownIt(presetName, options) {\n    * md.validateLink = function () { return true; }\n    * ```\n    **/\n-  this.validateLink = validateLink;\n+this.validateLink = validateLink;\n+  \n \n   /**\n    * MarkdownIt#normalizeLink(url) -> String\n@@ -285,6 +276,7 @@ function MarkdownIt(presetName, options) {\n   this.normalizeLink = normalizeLink;\n \n   /**\n+this.validateLink = validateLink;\n    * MarkdownIt#normalizeLinkText(url) -> String\n    *\n    * Function used to decode link url to a human-readable format`\ndiff --git a/lib/rules_inline/image.js b/lib/rules_inline/image.js\nindex a5e1630..d5cd1d8 100644\n--- a/lib/rules_inline/image.js\n+++ b/lib/rules_inline/image.js\n@@ -55,7 +55,7 @@ module.exports = function image(state, silent) {\n     res = parseLinkDestination(state.src, pos, state.posMax);\n     if (res.ok) {\n       href = state.md.normalizeLink(res.str);\n-      if (state.md.validateLink(href)) {\n+      if (href.indexOf('data:') === 0 || state.md.validateLink(href)) {\n         pos = res.pos;\n       } else {\n         href = '';\n"}
